[{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2478810798","pull_request_review_id":3400618958,"id":2478810798,"node_id":"PRRC_kwDOKSXUF86Tv6au","diff_hunk":"@@ -1,6 +1,6 @@\n <Project Sdk=\"Microsoft.NET.Sdk\">\n   <PropertyGroup>\n-\t  <TargetFrameworks>net8.0;net7.0;net6.0;net462</TargetFrameworks>\n+\t  <TargetFrameworks>net8.0;net462</TargetFrameworks>","path":"src/AiDotNet.csproj","commit_id":"2d302b10bff6968f46f6215569850fe3574e2cef","original_commit_id":"2d302b10bff6968f46f6215569850fe3574e2cef","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**Unresolved CI/CD pipeline error requires attention.**\n\nThe pipeline is failing with `NETSDK1129: The 'Publish' target is not supported without specifying a target framework`. When a project targets multiple frameworks (net8.0;net462), the publish step must specify a target framework or use `/p:TargetFramework=net8.0` (or net462).\n\n\nThis needs to be fixed in your CI/CD configuration (GitHub Actions workflow). Update the publish step to explicitly specify the target framework:\n\n```yaml\n# In your workflow file, update the publish step:\ndotnet publish -c Release -f net8.0  # or net462, or both in separate steps\n```\n\nAlternatively, if you need to publish for both frameworks, create separate publish steps for each.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/AiDotNet.csproj around line 3, the project targets multiple frameworks\n(net8.0;net462) which causes the CI error NETSDK1129 during publish; update your\nGitHub Actions workflow publish step to explicitly specify a target framework\n(for example use dotnet publish -c Release -f net8.0 or dotnet publish -c\nRelease /p:TargetFramework=net8.0), or create separate publish steps‚Äîone for\nnet8.0 and one for net462‚Äîso each publish invocation targets a single framework.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-10-30T16:42:18Z","updated_at":"2025-10-30T16:42:20Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2478810798","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2478810798"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2478810798"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2478810798/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":3,"original_line":3,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":4,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2478810807","pull_request_review_id":3400618958,"id":2478810807,"node_id":"PRRC_kwDOKSXUF86Tv6a3","diff_hunk":"@@ -310,6 +325,179 @@ public TOutput Predict(TInput newData)\n         return NormalizationInfo.Normalizer.Denormalize(normalizedPredictions, NormalizationInfo.YParams);\n     }\n \n+    /// <summary>\n+    /// Trains the underlying model using input data and expected output.\n+    /// </summary>\n+    /// <param name=\"input\">Input training data.</param>\n+    /// <param name=\"expectedOutput\">Expected output values.</param>\n+    /// <exception cref=\"InvalidOperationException\">Thrown when the Model is not initialized.</exception>\n+    public void Train(TInput input, TOutput expectedOutput)\n+    {\n+        if (Model == null)\n+        {\n+            throw new InvalidOperationException(\"Model is not initialized.\");\n+        }\n+        Model.Train(input, expectedOutput);\n+    }\n+\n+    /// <summary>\n+    /// Gets the parameters of the underlying model.\n+    /// </summary>\n+    /// <returns>A vector containing the model parameters.</returns>\n+    /// <exception cref=\"InvalidOperationException\">Thrown when the Model is not initialized.</exception>\n+    public Vector<T> GetParameters()\n+    {\n+        if (Model == null)\n+        {\n+            throw new InvalidOperationException(\"Model is not initialized.\");\n+        }\n+\n+        return Model.GetParameters();\n+    }\n+\n+    /// <summary>\n+    /// Sets the parameters of the underlying model.\n+    /// </summary>\n+    /// <param name=\"parameters\">The parameter vector to set.</param>\n+    /// <exception cref=\"InvalidOperationException\">Thrown when the Model is not initialized.</exception>\n+    public void SetParameters(Vector<T> parameters)\n+    {\n+        if (Model == null)\n+        {\n+            throw new InvalidOperationException(\"Model is not initialized.\");\n+        }\n+\n+        Model.SetParameters(parameters);\n+    }\n+\n+    /// <summary>\n+    /// Gets the number of parameters in the underlying model.\n+    /// </summary>\n+    public int ParameterCount\n+    {\n+        get\n+        {\n+            if (Model == null)\n+            {\n+                return 0;\n+            }\n+\n+            return Model.ParameterCount;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Creates a new instance with the specified parameters.\n+    /// </summary>\n+    /// <param name=\"parameters\">The parameter vector to use.</param>\n+    /// <returns>A new PredictionModelResult with updated parameters.</returns>\n+    /// <exception cref=\"InvalidOperationException\">Thrown when the Model is not initialized.</exception>\n+    public IFullModel<T, TInput, TOutput> WithParameters(Vector<T> parameters)\n+    {\n+        if (Model == null)\n+        {\n+            throw new InvalidOperationException(\"Model is not initialized.\");\n+        }\n+\n+        var newModel = Model.WithParameters(parameters);\n+        return new PredictionModelResult<T, TInput, TOutput>(newModel, OptimizationResult, NormalizationInfo);\n+    }","path":"src/Models/Results/PredictionModelResult.cs","commit_id":"2d302b10bff6968f46f6215569850fe3574e2cef","original_commit_id":"2d302b10bff6968f46f6215569850fe3574e2cef","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**Keep OptimizationResult aligned with the returned model.**  \nLine 397 reuses the existing `OptimizationResult` when creating the new wrapper, so `OptimizationResult.BestSolution` (and any cached metrics) still reference the original model while `Model` now points to `newModel`. That breaks the invariant established in Line 212 and also means edits to the new result mutate the source result via the shared reference. Please clone/update the optimization result before returning, e.g.:\n\n```diff\n-        var newModel = Model.WithParameters(parameters);\n-        return new PredictionModelResult<T, TInput, TOutput>(newModel, OptimizationResult, NormalizationInfo);\n+        var newModel = Model.WithParameters(parameters);\n+        var clonedOptimization = OptimizationResult.DeepCopy();\n+        clonedOptimization.BestSolution = newModel;\n+        var clonedNormalization = NormalizationInfo.DeepCopy();\n+        return new PredictionModelResult<T, TInput, TOutput>(newModel, clonedOptimization, clonedNormalization);\n```\n\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/Models/Results/PredictionModelResult.cs around lines 395 to 404, the\nmethod returns a new PredictionModelResult wrapping newModel but reuses the\noriginal OptimizationResult, leaving OptimizationResult.BestSolution (and any\ncached metrics) pointing at the old model and sharing mutable state; create a\nnew/cloned OptimizationResult that has its BestSolution updated to reference\nnewModel (and copy any cached metrics/state) instead of reusing the original\nreference, then pass that cloned/updated OptimizationResult into the new\nPredictionModelResult to preserve the invariant and avoid mutating the source\nresult.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits cc389b7 to 5018b6b","created_at":"2025-10-30T16:42:18Z","updated_at":"2025-11-01T18:02:23Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2478810807","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2478810807"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2478810807"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2478810807/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":419,"original_start_line":395,"start_side":"RIGHT","line":440,"original_line":440,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":139,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2478810819","pull_request_review_id":3400618958,"id":2478810819,"node_id":"PRRC_kwDOKSXUF86Tv6bD","diff_hunk":"@@ -314,8 +314,30 @@ public Vector<T> Recall(Vector<T> input, int maxIterations = 100)\n     /// </remarks>\n     public override void UpdateParameters(Vector<T> parameters)\n     {\n-        // Hopfield networks typically don't use gradient-based updates\n-        throw new InvalidOperationException(\"Hopfield networks do not support gradient-based parameter updates. Use the Train(List<Vector<T>> patterns) method instead.\");\n+        int expectedLength = _size * _size;\n+\n+        if (parameters.Length != expectedLength)\n+        {\n+            throw new ArgumentException($\"Parameter vector length mismatch. Expected {expectedLength} parameters but got {parameters.Length}.\", nameof(parameters));\n+        }\n+\n+        int paramIndex = 0;\n+\n+        for (int i = 0; i < _size; i++)\n+        {\n+            for (int j = 0; j < _size; j++)\n+            {\n+                if (i == j)\n+                {\n+                    _weights[i, j] = NumOps.Zero;\n+                    paramIndex++;\n+                }\n+                else\n+                {\n+                    _weights[i, j] = parameters[paramIndex++];\n+                }\n+            }","path":"src/NeuralNetworks/HopfieldNetwork.cs","commit_id":"2d302b10bff6968f46f6215569850fe3574e2cef","original_commit_id":"2d302b10bff6968f46f6215569850fe3574e2cef","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**Preserve Hopfield symmetry when applying parameters.**\n\nLines 326-339 write `_weights[i, j]` straight from the vector, so a caller can set `w_ij` different from `w_ji`. Hopfield recall and the energy calculation assume a symmetric weight matrix with zero diagonal; breaking that invariant leads to unpredictable dynamics and stalled convergence. Please mirror each off-diagonal assignment (or validate equality) and only consume each unique pair once.\n\n\nApply this diff to enforce symmetry and expect only the independent off-diagonal parameters:\n\n```diff\n-        int expectedLength = _size * _size;\n+        int expectedLength = (_size * (_size - 1)) / 2;\n@@\n-        for (int i = 0; i < _size; i++)\n-        {\n-            for (int j = 0; j < _size; j++)\n-            {\n-                if (i == j)\n-                {\n-                    _weights[i, j] = NumOps.Zero;\n-                    paramIndex++;\n-                }\n-                else\n-                {\n-                    _weights[i, j] = parameters[paramIndex++];\n-                }\n-            }\n-        }\n+        for (int i = 0; i < _size; i++)\n+        {\n+            _weights[i, i] = NumOps.Zero;\n+        }\n+\n+        for (int i = 0; i < _size; i++)\n+        {\n+            for (int j = i + 1; j < _size; j++)\n+            {\n+                if (paramIndex >= parameters.Length)\n+                {\n+                    throw new ArgumentException($\"Parameter vector is missing values for symmetric pair ({i},{j}).\", nameof(parameters));\n+                }\n+\n+                T value = parameters[paramIndex++];\n+                _weights[i, j] = value;\n+                _weights[j, i] = value;\n+            }\n+        }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        int expectedLength = (_size * (_size - 1)) / 2;\n\n        if (parameters.Length != expectedLength)\n        {\n            throw new ArgumentException($\"Parameter vector length mismatch. Expected {expectedLength} parameters but got {parameters.Length}.\", nameof(parameters));\n        }\n\n        int paramIndex = 0;\n\n        for (int i = 0; i < _size; i++)\n        {\n            _weights[i, i] = NumOps.Zero;\n        }\n\n        for (int i = 0; i < _size; i++)\n        {\n            for (int j = i + 1; j < _size; j++)\n            {\n                if (paramIndex >= parameters.Length)\n                {\n                    throw new ArgumentException($\"Parameter vector is missing values for symmetric pair ({i},{j}).\", nameof(parameters));\n                }\n\n                T value = parameters[paramIndex++];\n                _weights[i, j] = value;\n                _weights[j, i] = value;\n            }\n        }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/HopfieldNetwork.cs around lines 317 to 339, the code\nwrites each _weights[i,j] directly from the parameter vector allowing w_ij !=\nw_ji; change to expect only the independent off-diagonal parameters\n(expectedLength = _size * (_size - 1) / 2), iterate i from 0.._size-1 and j from\ni+1.._size-1, set _weights[i,i] = NumOps.Zero for diagonals, and for each\noff-diagonal consume one parameter and assign it to both _weights[i,j] and\n_weights[j,i] while incrementing paramIndex once per pair so symmetry is\npreserved and the diagonal remains zero.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits cc389b7 to 6de38f8","created_at":"2025-10-30T16:42:18Z","updated_at":"2025-10-31T14:33:24Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2478810819","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2478810819"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2478810819"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2478810819/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":317,"start_side":"RIGHT","line":null,"original_line":339,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":28,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2478810835","pull_request_review_id":3400618958,"id":2478810835,"node_id":"PRRC_kwDOKSXUF86Tv6bT","diff_hunk":"@@ -296,7 +296,7 @@ private List<Genome<T>> InitializePopulation()\n     /// - Assigns random weights to these connections\n     /// \n     /// For example, if you have 3 inputs and 2 outputs:\n-    /// - You'll have 6 connections (3 inputs √ó 2 outputs)\n+    /// - You'll have 6 connections (3 inputs √Ø¬ø¬Ω 2 outputs)","path":"src/NeuralNetworks/NEAT.cs","commit_id":"2d302b10bff6968f46f6215569850fe3574e2cef","original_commit_id":"2d302b10bff6968f46f6215569850fe3574e2cef","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üü° Minor_\n\n**Fix character encoding issue in documentation.**\n\nThe multiplication symbol appears as ÔøΩ (a character encoding issue). This should be √ó or just \"x\" for clarity.\n\nApply this diff to fix the encoding:\n\n```diff\n-    /// - You'll have 6 connections (3 inputs ÔøΩ 2 outputs)\n+    /// - You'll have 6 connections (3 inputs √ó 2 outputs)\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    /// - You'll have 6 connections (3 inputs √ó 2 outputs)\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/NEAT.cs around line 299, the documentation comment\ncontains a bad character encoding for the multiplication symbol (\"ÔøΩ\"); replace\nthat character with a proper multiplication symbol (\"√ó\") or simply \"x\" (e.g.,\nchange \"3 inputs ÔøΩ 2 outputs\" to \"3 inputs √ó 2 outputs\" or \"3 inputs x 2\noutputs\") and ensure the file is saved using UTF-8 encoding so the symbol\nrenders correctly.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits 8fbbfa2 to 94a43dd","created_at":"2025-10-30T16:42:18Z","updated_at":"2025-10-31T12:42:51Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2478810835","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2478810835"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2478810835"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2478810835/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":null,"original_line":299,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":5,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2478810844","pull_request_review_id":3400618958,"id":2478810844,"node_id":"PRRC_kwDOKSXUF86Tv6bc","diff_hunk":"@@ -450,7 +450,33 @@ public Tensor<T> GetHiddenLayerActivation(Tensor<T> visibleLayer)\n     /// </remarks>\n     public override void UpdateParameters(Vector<T> parameters)\n     {\n-        // This method is not typically used in RBMs\n+        int weightCount = HiddenSize * VisibleSize;\n+        int expectedLength = weightCount + VisibleSize + HiddenSize;\n+\n+        if (parameters.Length != expectedLength)\n+        {\n+            throw new ArgumentException($\"Parameter vector length mismatch. Expected {expectedLength} parameters but got {parameters.Length}.\", nameof(parameters));\n+        }\n+\n+        int paramIndex = 0;\n+\n+        for (int i = 0; i < HiddenSize; i++)\n+        {\n+            for (int j = 0; j < VisibleSize; j++)\n+            {\n+                _weights[i, j] = parameters[paramIndex++];\n+            }\n+        }\n+\n+        for (int i = 0; i < VisibleSize; i++)\n+        {\n+            _visibleBiases[i] = parameters[paramIndex++];\n+        }\n+\n+        for (int i = 0; i < HiddenSize; i++)\n+        {\n+            _hiddenBiases[i] = parameters[paramIndex++];\n+        }","path":"src/NeuralNetworks/RestrictedBoltzmannMachine.cs","commit_id":"2d302b10bff6968f46f6215569850fe3574e2cef","original_commit_id":"2d302b10bff6968f46f6215569850fe3574e2cef","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n<details>\n<summary>üß© Analysis chain</summary>\n\n**Verify parameter ordering consistency with parameter extraction.**\n\nThe implementation logic is correct and properly validates the input vector length. However, ensure that the parameter ordering (weights ‚Üí visible biases ‚Üí hidden biases) matches the order used by any parameter extraction method in the base class.\n\n\n\n\nRun the following script to check for parameter extraction methods and verify ordering consistency:\n\n---\n\nüèÅ Script executed:\n\n```shell\n#!/bin/bash\n# Description: Verify parameter ordering consistency between UpdateParameters and GetParameters.\n\n# Search for GetParameters method in base class and related parameter extraction logic\necho \"=== Searching for GetParameters method ===\"\nast-grep --pattern $'class NeuralNetworkBase<$_> {\n  $$$\n  GetParameters($$$) {\n    $$$\n  }\n  $$$\n}'\n\necho -e \"\\n=== Searching for parameter extraction patterns in base class ===\"\nrg -n -A 10 \"GetParameters|ExtractParameters|ToParameterVector\" --type=cs -g '*NeuralNetworkBase*'\n\necho -e \"\\n=== Searching for other UpdateParameters implementations for comparison ===\"\nrg -n -B 2 -A 15 \"override void UpdateParameters\\(Vector\" --type=cs -g '*{EchoStateNetwork,HopfieldNetwork}*'\n```\n\nLength of output: 8058\n\n---\n\nI need to search the actual codebase directly for the RestrictedBoltzmannMachine.GetParameters implementation to verify the parameter ordering matches UpdateParameters.\n\n---\n\nüèÅ Script executed:\n\n```shell\n#!/bin/bash\n# Search for GetParameters method in RestrictedBoltzmannMachine class\necho \"=== Searching for GetParameters in RestrictedBoltzmannMachine.cs ===\"\nrg -n -B 2 -A 20 \"override.*GetParameters|public.*GetParameters\" src/NeuralNetworks/RestrictedBoltzmannMachine.cs\n\necho -e \"\\n=== Checking if RBM overrides GetParameters or inherits from base ===\"\ngrep -n \"GetParameters\" src/NeuralNetworks/RestrictedBoltzmannMachine.cs || echo \"No GetParameters found in RBM file\"\n\necho -e \"\\n=== Searching serialization logic to verify parameter order (lines 976-982 mentioned) ===\"\nrg -n -B 2 -A 10 \"SerializeNetworkSpecificData\\|GetParameters\" src/NeuralNetworks/RestrictedBoltzmannMachine.cs | head -50\n```\n\nLength of output: 525\n\n---\n\nüèÅ Script executed:\n\n```shell\n#!/bin/bash\n# Find RestrictedBoltzmannMachine file and search for GetParameters\necho \"=== Finding RestrictedBoltzmannMachine file ===\"\nfd -e cs \"RestrictedBoltzmannMachine\" \n\necho -e \"\\n=== Searching for GetParameters in RBM file ===\"\nfd -e cs \"RestrictedBoltzmannMachine\" --exec rg -n -B 2 -A 20 \"GetParameters\" {} \\;\n\necho -e \"\\n=== Searching serialization/parameter extraction at lines ~976-982 ===\"\nfd -e cs \"RestrictedBoltzmannMachine\" --exec rg -n -B 2 -A 10 \"SerializeNetworkSpecificData\\|GetParameters|ToParameterVector\" {} \\; | head -80\n```\n\nLength of output: 217\n\n---\n\nüèÅ Script executed:\n\n```shell\n#!/bin/bash\n# Check RBM class structure and whether it's layer-based\necho \"=== Checking RBM class definition and base class ===\"\nrg -n -A 5 \"^class RestrictedBoltzmannMachine\" src/NeuralNetworks/RestrictedBoltzmannMachine.cs\n\necho -e \"\\n=== Searching for SerializeNetworkSpecificData in RBM ===\"\nrg -n -B 2 -A 25 \"SerializeNetworkSpecificData\" src/NeuralNetworks/RestrictedBoltzmannMachine.cs\n\necho -e \"\\n=== Searching for parameter-related methods (ToParameterArray, GetParameterCount, etc) ===\"\nrg -n \"ParameterCount|ToParameter|FromParameter\" src/NeuralNetworks/RestrictedBoltzmannMachine.cs\n\necho -e \"\\n=== Check what fields are in RBM (_weights, _biases, etc) ===\"\nrg -n \"private.*_weights|private.*_biases|private.*_hiddenBiases|private.*_visibleBiases\" src/NeuralNetworks/RestrictedBoltzmannMachine.cs\n```\n\nLength of output: 1329\n\n---\n\nüèÅ Script executed:\n\n```shell\n#!/bin/bash\n# Check RBM's Layers property and how it's initialized\necho \"=== Searching for Layers property/field in RBM ===\"\nrg -n \"Layers|layers\" src/NeuralNetworks/RestrictedBoltzmannMachine.cs | head -20\n\necho -e \"\\n=== Checking RBM constructor to see if Layers is populated ===\"\nrg -n -B 5 -A 30 \"public RestrictedBoltzmannMachine\\(\" src/NeuralNetwork/RestrictedBoltzmannMachine.cs | head -80\n\necho -e \"\\n=== Check ParameterCount property in RBM ===\"\nrg -n -B 2 -A 5 \"ParameterCount\" src/NeuralNetworks/RestrictedBoltzmannMachine.cs\n\necho -e \"\\n=== Search for class definition to see full inheritance ===\"\nrg -n \"class RestrictedBoltzmannMachine\" src/NeuralNetworks/RestrictedBoltzmannMachine.cs\n```\n\nLength of output: 1632\n\n---\n\nüèÅ Script executed:\n\n```shell\n#!/bin/bash\n# Fix typo and search for ParameterCount and deserialization\necho \"=== Check ParameterCount property in RBM (corrected path) ===\"\nrg -n \"ParameterCount\" src/NeuralNetworks/RestrictedBoltzmannMachine.cs\n\necho -e \"\\n=== Search for DeserializeNetworkSpecificData in RBM ===\"\nrg -n -B 2 -A 25 \"DeserializeNetworkSpecificData\" src/NeuralNetworks/RestrictedBoltzmannMachine.cs\n\necho -e \"\\n=== Search for SetParameters in base class to see how it works ===\"\nrg -n -B 5 -A 30 \"public virtual void SetParameters\" src/NeuralNetworks/NeuralNetworkBase.cs | head -100\n\necho -e \"\\n=== Search for Layers property in base class ===\"\nrg -n -B 2 -A 5 \"protected.*List.*Layers|public.*IList.*Layers\" src/NeuralNetworks/NeuralNetworkBase.cs\n```\n\nLength of output: 3045\n\n---\n\n\n\n</details>\n\n**RBM parameter handling violates base class contract - critical design flaw.**\n\nThe verification reveals a critical inconsistency: RBM.UpdateParameters unpacks parameters directly into fields (weights ‚Üí visible biases ‚Üí hidden biases), but RBM inherits GetParameters and SetParameters from the base class, which operate on the Layers collection. Since RBM.InitializeLayers() is empty (by design), the inherited methods are incompatible with UpdateParameters.\n\nSpecifically:\n- RBM.UpdateParameters expects parameters in order: weights ‚Üí visible biases ‚Üí hidden biases\n- Base.GetParameters iterates through empty Layers collection ‚Üí returns empty/incorrect vector\n- Base.SetParameters iterates through empty Layers collection ‚Üí doesn't update RBM fields\n- Base class documentation states parameters \"should be in the same format as returned by GetParameters\"\n\n**Fix required**: RBM must override GetParameters and SetParameters to work with direct fields, not the Layers collection.\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits 36ecbde to 56d3dd2","created_at":"2025-10-30T16:42:18Z","updated_at":"2025-11-02T03:23:47Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2478810844","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2478810844"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2478810844"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2478810844/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":490,"original_start_line":453,"start_side":"RIGHT","line":516,"original_line":479,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":40,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2478810853","pull_request_review_id":3400618958,"id":2478810853,"node_id":"PRRC_kwDOKSXUF86Tv6bl","diff_hunk":"@@ -1,4 +1,4 @@\n-using Newtonsoft.Json;\n+Ôªøusing Newtonsoft.Json;","path":"src/Optimizers/BFGSOptimizer.cs","commit_id":"2d302b10bff6968f46f6215569850fe3574e2cef","original_commit_id":"2d302b10bff6968f46f6215569850fe3574e2cef","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üü° Minor_\n\n**Remove BOM character from file start.**\n\nLine 1 contains a zero-width byte-order mark (BOM) before the using directive that should be stripped for code cleanliness and compatibility.\n\n\nApply this fix by ensuring the file is saved with UTF-8 encoding (without BOM) in your editor. Most editors allow you to set this in encoding/file properties.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/Optimizers/BFGSOptimizer.cs around line 1 there is a leading BOM\n(zero-width byte-order mark) before the using directive; remove the BOM by\nre-saving the file as UTF-8 without BOM in your editor or IDE (or run a tool to\nstrip the BOM) so the file starts directly with the first character of the\nsource code.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-10-30T16:42:19Z","updated_at":"2025-10-30T16:42:21Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2478810853","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2478810853"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2478810853"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2478810853/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":1,"original_line":1,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":2,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2478810859","pull_request_review_id":3400618958,"id":2478810859,"node_id":"PRRC_kwDOKSXUF86Tv6br","diff_hunk":"@@ -0,0 +1,440 @@\n+namespace AiDotNet.TimeSeries;\n+\n+/// <summary>\n+/// Represents a single block in the N-BEATS architecture.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations (e.g., float, double).</typeparam>\n+/// <remarks>\n+/// <para>\n+/// Each N-BEATS block consists of:\n+/// 1. A stack of fully connected layers (the \"theta\" network)\n+/// 2. A basis expansion layer for generating backcast (reconstruction of input)\n+/// 3. A basis expansion layer for generating forecast (prediction of future)\n+/// </para>\n+/// <para>\n+/// The block architecture implements a doubly residual stacking principle:\n+/// - Backcast residual: Input minus backcast is passed to the next block\n+/// - Forecast addition: Forecasts from all blocks are summed for the final prediction\n+/// </para>\n+/// <para><b>For Beginners:</b> A block is the basic building unit of N-BEATS. Think of it like\n+/// a specialized predictor that:\n+/// 1. Looks at the input time series\n+/// 2. Tries to reconstruct what it saw (backcast)\n+/// 3. Predicts the future (forecast)\n+/// 4. Passes the \"leftover\" patterns it couldn't explain to the next block\n+///\n+/// Multiple blocks work together, with each one focusing on different aspects of the data.\n+/// </para>\n+/// </remarks>\n+public class NBEATSBlock<T>\n+{\n+    private readonly INumericOperations<T> _numOps;\n+    private readonly int _lookbackWindow;\n+    private readonly int _forecastHorizon;\n+    private readonly int _hiddenLayerSize;\n+    private readonly int _numHiddenLayers;\n+    private readonly int _thetaSizeBackcast;\n+    private readonly int _thetaSizeForecast;\n+    private readonly bool _useInterpretableBasis;\n+    private readonly int _polynomialDegree;\n+\n+    /// <summary>\n+    /// Weights for the fully connected layers (theta network).\n+    /// </summary>\n+    private List<Matrix<T>> _fcWeights;\n+\n+    /// <summary>\n+    /// Biases for the fully connected layers (theta network).\n+    /// </summary>\n+    private List<Vector<T>> _fcBiases;\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters in the block.\n+    /// </summary>\n+    public int ParameterCount\n+    {\n+        get\n+        {\n+            int count = 0;\n+            foreach (var weight in _fcWeights)\n+            {\n+                count += weight.Rows * weight.Cols;\n+            }\n+            foreach (var bias in _fcBiases)\n+            {\n+                count += bias.Length;\n+            }\n+            return count;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Initializes a new instance of the NBEATSBlock class.\n+    /// </summary>\n+    /// <param name=\"lookbackWindow\">The number of historical time steps used as input.</param>\n+    /// <param name=\"forecastHorizon\">The number of future time steps to predict.</param>\n+    /// <param name=\"hiddenLayerSize\">The size of hidden layers in the fully connected network.</param>\n+    /// <param name=\"numHiddenLayers\">The number of hidden layers.</param>\n+    /// <param name=\"thetaSizeBackcast\">The size of the theta vector for backcast basis expansion.</param>\n+    /// <param name=\"thetaSizeForecast\">The size of the theta vector for forecast basis expansion.</param>\n+    /// <param name=\"useInterpretableBasis\">Whether to use interpretable basis functions.</param>\n+    /// <param name=\"polynomialDegree\">The polynomial degree for trend basis (if interpretable).</param>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This creates a new block with specific parameters:\n+    /// - lookbackWindow: How far back in time the block looks\n+    /// - forecastHorizon: How far forward in time the block predicts\n+    /// - hiddenLayerSize: How many neurons in each hidden layer (bigger = more capacity)\n+    /// - numHiddenLayers: How many hidden layers (deeper = more complex patterns)\n+    /// - useInterpretableBasis: Whether to use human-understandable basis functions\n+    /// </para>\n+    /// </remarks>\n+    public NBEATSBlock(\n+        int lookbackWindow,\n+        int forecastHorizon,\n+        int hiddenLayerSize,\n+        int numHiddenLayers,\n+        int thetaSizeBackcast,\n+        int thetaSizeForecast,\n+        bool useInterpretableBasis,\n+        int polynomialDegree = 3)\n+    {\n+        if (lookbackWindow <= 0)\n+        {\n+            throw new ArgumentException(\"Lookback window must be positive.\", nameof(lookbackWindow));\n+        }\n+        if (forecastHorizon <= 0)\n+        {\n+            throw new ArgumentException(\"Forecast horizon must be positive.\", nameof(forecastHorizon));\n+        }\n+        if (hiddenLayerSize <= 0)\n+        {\n+            throw new ArgumentException(\"Hidden layer size must be positive.\", nameof(hiddenLayerSize));\n+        }\n+        if (numHiddenLayers <= 0)\n+        {\n+            throw new ArgumentException(\"Number of hidden layers must be positive.\", nameof(numHiddenLayers));\n+        }\n+\n+        _numOps = MathHelper.GetNumericOperations<T>();\n+        _lookbackWindow = lookbackWindow;\n+        _forecastHorizon = forecastHorizon;\n+        _hiddenLayerSize = hiddenLayerSize;\n+        _numHiddenLayers = numHiddenLayers;\n+        _thetaSizeBackcast = thetaSizeBackcast;\n+        _thetaSizeForecast = thetaSizeForecast;\n+        _useInterpretableBasis = useInterpretableBasis;\n+        _polynomialDegree = polynomialDegree;\n+\n+        _fcWeights = new List<Matrix<T>>();\n+        _fcBiases = new List<Vector<T>>();\n+\n+        InitializeWeights();\n+    }\n+\n+    /// <summary>\n+    /// Initializes the weights and biases for the fully connected layers.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Uses Xavier/Glorot initialization to set initial weights, which helps with\n+    /// training stability by keeping the scale of gradients roughly the same across layers.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This sets up the initial random values for all the\n+    /// weights and biases in the block. Good initialization is important for the model\n+    /// to learn effectively. We use a special technique (Xavier initialization) that\n+    /// has been proven to work well for neural networks.\n+    /// </para>\n+    /// </remarks>\n+    private void InitializeWeights()\n+    {\n+        var random = new Random(42);\n+\n+        // First layer: lookbackWindow -> hiddenLayerSize\n+        int inputSize = _lookbackWindow;\n+        double stddev = Math.Sqrt(2.0 / (inputSize + _hiddenLayerSize));\n+        var weight = new Matrix<T>(_hiddenLayerSize, inputSize);\n+        for (int i = 0; i < weight.Rows; i++)\n+        {\n+            for (int j = 0; j < weight.Cols; j++)\n+            {\n+                weight[i, j] = _numOps.FromDouble(random.NextDouble() * stddev * 2 - stddev);\n+            }\n+        }\n+        _fcWeights.Add(weight);\n+        _fcBiases.Add(new Vector<T>(_hiddenLayerSize));\n+\n+        // Hidden layers: hiddenLayerSize -> hiddenLayerSize\n+        for (int layer = 1; layer < _numHiddenLayers; layer++)\n+        {\n+            stddev = Math.Sqrt(2.0 / (_hiddenLayerSize + _hiddenLayerSize));\n+            weight = new Matrix<T>(_hiddenLayerSize, _hiddenLayerSize);\n+            for (int i = 0; i < weight.Rows; i++)\n+            {\n+                for (int j = 0; j < weight.Cols; j++)\n+                {\n+                    weight[i, j] = _numOps.FromDouble(random.NextDouble() * stddev * 2 - stddev);\n+                }\n+            }\n+            _fcWeights.Add(weight);\n+            _fcBiases.Add(new Vector<T>(_hiddenLayerSize));\n+        }\n+\n+        // Output layer for backcast theta: hiddenLayerSize -> thetaSizeBackcast\n+        stddev = Math.Sqrt(2.0 / (_hiddenLayerSize + _thetaSizeBackcast));\n+        weight = new Matrix<T>(_thetaSizeBackcast, _hiddenLayerSize);\n+        for (int i = 0; i < weight.Rows; i++)\n+        {\n+            for (int j = 0; j < weight.Cols; j++)\n+            {\n+                weight[i, j] = _numOps.FromDouble(random.NextDouble() * stddev * 2 - stddev);\n+            }\n+        }\n+        _fcWeights.Add(weight);\n+        _fcBiases.Add(new Vector<T>(_thetaSizeBackcast));\n+\n+        // Output layer for forecast theta: hiddenLayerSize -> thetaSizeForecast\n+        stddev = Math.Sqrt(2.0 / (_hiddenLayerSize + _thetaSizeForecast));\n+        weight = new Matrix<T>(_thetaSizeForecast, _hiddenLayerSize);\n+        for (int i = 0; i < weight.Rows; i++)\n+        {\n+            for (int j = 0; j < weight.Cols; j++)\n+            {\n+                weight[i, j] = _numOps.FromDouble(random.NextDouble() * stddev * 2 - stddev);\n+            }\n+        }\n+        _fcWeights.Add(weight);","path":"src/TimeSeries/NBEATSBlock.cs","commit_id":"2d302b10bff6968f46f6215569850fe3574e2cef","original_commit_id":"2d302b10bff6968f46f6215569850fe3574e2cef","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Fix Matrix dimension property usage**\n\n`Matrix<T>` in this codebase exposes `Columns`, not `Cols`. Every reference to `.Cols` currently fails to compile, which is why the pipeline reports `Matrix<T>` lacking a `Cols` member. Replace `.Cols` with `.Columns` throughout the block (parameter counting, weight initialization, forward pass loops, etc.) so the project builds.\n\n```diff\n-                count += weight.Rows * weight.Cols;\n+                count += weight.Rows * weight.Columns;\n...\n-            for (int j = 0; j < weight.Cols; j++)\n+            for (int j = 0; j < weight.Columns; j++)\n...\n-                for (int j = 0; j < _fcWeights[layer].Cols; j++)\n+                for (int j = 0; j < _fcWeights[layer].Columns; j++)\n```\n\nPlease apply the same replacement for every remaining occurrence in this file.\n\n<details>\n<summary>üß∞ Tools</summary>\n\n<details>\n<summary>ü™õ GitHub Check: Build All Frameworks</summary>\n\n[failure] 173-173: \n'Matrix<T>' does not contain a definition for 'Cols' and no accessible extension method 'Cols' accepting a first argument of type 'Matrix<T>' could be found (are you missing a using directive or an assembly reference?)\n\n---\n\n[failure] 158-158: \n'Matrix<T>' does not contain a definition for 'Cols' and no accessible extension method 'Cols' accepting a first argument of type 'Matrix<T>' could be found (are you missing a using directive or an assembly reference?)\n\n---\n\n[failure] 61-61: \n'Matrix<T>' does not contain a definition for 'Cols' and no accessible extension method 'Cols' accepting a first argument of type 'Matrix<T>' could be found (are you missing a using directive or an assembly reference?)\n\n</details>\n\n</details>\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/TimeSeries/NBEATSBlock.cs around lines 59 to 205 the code references\nMatrix<T>.Cols which does not exist; replace every occurrence of .Cols with\n.Columns (including the parameter counting loops, weight initialization loops,\nforward-pass loops and any other usages in this file) so the code compiles and\nuses the correct Matrix<T> property.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits 8fbbfa2 to 94a43dd","created_at":"2025-10-30T16:42:19Z","updated_at":"2025-10-31T12:42:50Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2478810859","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2478810859"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2478810859"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2478810859/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":59,"original_start_line":59,"start_side":"RIGHT","line":205,"original_line":205,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":205,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2478810873","pull_request_review_id":3400618958,"id":2478810873,"node_id":"PRRC_kwDOKSXUF86Tv6b5","diff_hunk":"@@ -0,0 +1,533 @@\n+namespace AiDotNet.TimeSeries;\n+\n+/// <summary>\n+/// Implements the N-BEATS (Neural Basis Expansion Analysis for Time Series) model for forecasting.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations (e.g., float, double).</typeparam>\n+/// <remarks>\n+/// <para>\n+/// N-BEATS is a deep neural architecture based on backward and forward residual links and\n+/// a very deep stack of fully-connected layers. The architecture has the following key features:\n+/// </para>\n+/// <list type=\"bullet\">\n+/// <item>Doubly residual stacking: Each block produces a backcast (reconstruction) and forecast</item>\n+/// <item>Hierarchical decomposition: Multiple stacks focus on different aspects (trend, seasonality)</item>\n+/// <item>Interpretability: Can use polynomial and Fourier basis for explainable forecasts</item>\n+/// <item>No manual feature engineering: Learns directly from raw time series data</item>\n+/// </list>\n+/// <para>\n+/// The original paper: Oreshkin et al., \"N-BEATS: Neural basis expansion analysis for\n+/// interpretable time series forecasting\" (ICLR 2020).\n+/// </para>\n+/// <para><b>For Beginners:</b> N-BEATS is a state-of-the-art neural network for time series\n+/// forecasting that automatically learns patterns from your data. Unlike traditional methods\n+/// that require you to manually specify trends and seasonality, N-BEATS figures these out\n+/// on its own.\n+///\n+/// Key advantages:\n+/// - No need for manual feature engineering (the model learns what's important)\n+/// - Can capture complex, non-linear patterns\n+/// - Provides interpretable components (trend, seasonality) when configured to do so\n+/// - Works well for both short-term and long-term forecasting\n+///\n+/// The model works by stacking many \"blocks\" together, where each block tries to:\n+/// 1. Understand what patterns are in the input (backcast)\n+/// 2. Predict the future based on those patterns (forecast)\n+/// 3. Pass the unexplained patterns to the next block\n+///\n+/// This allows the model to decompose complex time series into simpler components.\n+/// </para>\n+/// </remarks>\n+public class NBEATSModel<T> : TimeSeriesModelBase<T>\n+{\n+    private readonly NBEATSModelOptions<T> _options;\n+    private readonly List<NBEATSBlock<T>> _blocks;\n+    private readonly INumericOperations<T> _numOps;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the NBEATSModel class.\n+    /// </summary>\n+    /// <param name=\"options\">Configuration options for the N-BEATS model. If null, default options are used.</param>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This creates a new N-BEATS model with the specified configuration.\n+    /// The options control things like:\n+    /// - How far back to look (lookback window)\n+    /// - How far forward to predict (forecast horizon)\n+    /// - How complex the model should be (number of stacks, blocks, layer sizes)\n+    /// - Whether to use interpretable components\n+    ///\n+    /// If you don't provide options, sensible defaults will be used.\n+    /// </para>\n+    /// </remarks>\n+    public NBEATSModel(NBEATSModelOptions<T>? options = null) : base(options ?? new NBEATSModelOptions<T>())\n+    {\n+        _options = options ?? new NBEATSModelOptions<T>();\n+        _numOps = MathHelper.GetNumericOperations<T>();\n+        _blocks = new List<NBEATSBlock<T>>();\n+\n+        // Validate options\n+        ValidateNBEATSOptions();\n+\n+        // Initialize blocks\n+        InitializeBlocks();\n+    }\n+\n+    /// <summary>\n+    /// Validates the N-BEATS specific options.\n+    /// </summary>\n+    private void ValidateNBEATSOptions()\n+    {\n+        if (_options.LookbackWindow <= 0)\n+        {\n+            throw new ArgumentException(\"Lookback window must be positive.\", nameof(_options.LookbackWindow));\n+        }\n+\n+        if (_options.ForecastHorizon <= 0)\n+        {\n+            throw new ArgumentException(\"Forecast horizon must be positive.\", nameof(_options.ForecastHorizon));\n+        }\n+\n+        if (_options.NumStacks <= 0)\n+        {\n+            throw new ArgumentException(\"Number of stacks must be positive.\", nameof(_options.NumStacks));\n+        }\n+\n+        if (_options.NumBlocksPerStack <= 0)\n+        {\n+            throw new ArgumentException(\"Number of blocks per stack must be positive.\", nameof(_options.NumBlocksPerStack));\n+        }\n+\n+        if (_options.HiddenLayerSize <= 0)\n+        {\n+            throw new ArgumentException(\"Hidden layer size must be positive.\", nameof(_options.HiddenLayerSize));\n+        }\n+\n+        if (_options.NumHiddenLayers <= 0)\n+        {\n+            throw new ArgumentException(\"Number of hidden layers must be positive.\", nameof(_options.NumHiddenLayers));\n+        }\n+\n+        if (_options.PolynomialDegree < 1)\n+        {\n+            throw new ArgumentException(\"Polynomial degree must be at least 1.\", nameof(_options.PolynomialDegree));\n+        }\n+\n+        if (_options.Epochs <= 0)\n+        {\n+            throw new ArgumentException(\"Number of epochs must be positive.\", nameof(_options.Epochs));\n+        }\n+\n+        if (_options.BatchSize <= 0)\n+        {\n+            throw new ArgumentException(\"Batch size must be positive.\", nameof(_options.BatchSize));\n+        }\n+\n+        if (_options.LearningRate <= 0)\n+        {\n+            throw new ArgumentException(\"Learning rate must be positive.\", nameof(_options.LearningRate));\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Initializes all blocks in the N-BEATS architecture.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This creates all the individual blocks that make up\n+    /// the N-BEATS model. The number of blocks is determined by NumStacks * NumBlocksPerStack.\n+    ///\n+    /// Each block is initialized with the same architecture but different random weights,\n+    /// allowing them to learn different aspects of the time series.\n+    /// </para>\n+    /// </remarks>\n+    private void InitializeBlocks()\n+    {\n+        _blocks.Clear();\n+\n+        // Calculate theta sizes for basis expansion\n+        int thetaSizeBackcast;\n+        int thetaSizeForecast;\n+\n+        if (_options.UseInterpretableBasis)\n+        {\n+            // For polynomial basis, theta size is polynomial degree + 1\n+            thetaSizeBackcast = _options.PolynomialDegree + 1;\n+            thetaSizeForecast = _options.PolynomialDegree + 1;\n+        }\n+        else\n+        {\n+            // For generic basis, theta size matches the output length\n+            thetaSizeBackcast = _options.LookbackWindow;\n+            thetaSizeForecast = _options.ForecastHorizon;\n+        }\n+\n+        // Create all blocks\n+        int totalBlocks = _options.NumStacks * _options.NumBlocksPerStack;\n+        for (int i = 0; i < totalBlocks; i++)\n+        {\n+            var block = new NBEATSBlock<T>(\n+                _options.LookbackWindow,\n+                _options.ForecastHorizon,\n+                _options.HiddenLayerSize,\n+                _options.NumHiddenLayers,\n+                thetaSizeBackcast,\n+                thetaSizeForecast,\n+                _options.UseInterpretableBasis,\n+                _options.PolynomialDegree\n+            );\n+            _blocks.Add(block);\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs the core training logic for the N-BEATS model.\n+    /// </summary>\n+    /// <param name=\"x\">The input features matrix where each row is a historical window.</param>\n+    /// <param name=\"y\">The target values vector where each element is the corresponding forecast target.</param>\n+    /// <remarks>\n+    /// <para>\n+    /// Training uses a simple gradient descent approach with mean squared error loss.\n+    /// The model iterates through the training data for the specified number of epochs,\n+    /// updating parameters to minimize prediction error.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is where the model actually learns from your data.\n+    ///\n+    /// The training process:\n+    /// 1. The model makes predictions on your training data\n+    /// 2. It calculates how far off the predictions are (the error)\n+    /// 3. It adjusts its internal parameters to reduce this error\n+    /// 4. It repeats this process many times (epochs) until it learns the patterns\n+    ///\n+    /// Note: This is a simplified training implementation. A production version would\n+    /// include more sophisticated optimization, regularization, and validation.\n+    /// </para>\n+    /// </remarks>\n+    protected override void TrainCore(Matrix<T> x, Vector<T> y)\n+    {\n+        // For simplicity, we'll implement a basic training loop\n+        // A full implementation would use more sophisticated optimization\n+\n+        int numSamples = x.Rows;\n+\n+        // Simple gradient descent training for demonstration\n+        for (int epoch = 0; epoch < _options.Epochs; epoch++)\n+        {\n+            T totalLoss = _numOps.Zero;\n+\n+            // Process each sample\n+            for (int sampleIdx = 0; sampleIdx < numSamples; sampleIdx++)\n+            {\n+                Vector<T> input = x.GetRow(sampleIdx);\n+\n+                // Forward pass through all blocks\n+                Vector<T> residual = input.Clone();\n+                Vector<T> aggregatedForecast = new Vector<T>(_options.ForecastHorizon);\n+\n+                for (int blockIdx = 0; blockIdx < _blocks.Count; blockIdx++)\n+                {\n+                    var (backcast, forecast) = _blocks[blockIdx].Forward(residual);\n+\n+                    // Update residual for next block\n+                    for (int i = 0; i < residual.Length; i++)\n+                    {\n+                        residual[i] = _numOps.Subtract(residual[i], backcast[i]);\n+                    }\n+\n+                    // Accumulate forecast\n+                    for (int i = 0; i < aggregatedForecast.Length; i++)\n+                    {\n+                        aggregatedForecast[i] = _numOps.Add(aggregatedForecast[i], forecast[i]);\n+                    }\n+                }\n+\n+                // Calculate loss (simplified - just the first forecast step for now)\n+                T target = y[sampleIdx];\n+                T prediction = aggregatedForecast[0];\n+                T error = _numOps.Subtract(prediction, target);\n+                T loss = _numOps.Multiply(error, error);\n+                totalLoss = _numOps.Add(totalLoss, loss);\n+            }\n+\n+            // Average loss for this epoch\n+            T avgLoss = _numOps.Divide(totalLoss, _numOps.FromDouble(numSamples));\n+\n+            // Print progress every 10 epochs\n+            if (epoch % 10 == 0)\n+            {\n+                Console.WriteLine($\"Epoch {epoch}/{_options.Epochs}, Loss: {avgLoss}\");\n+            }\n+        }\n+\n+        // Store the final parameters\n+        ModelParameters = GetParameters();\n+    }","path":"src/TimeSeries/NBEATSModel.cs","commit_id":"2d302b10bff6968f46f6215569850fe3574e2cef","original_commit_id":"2d302b10bff6968f46f6215569850fe3574e2cef","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Critical: TrainCore doesn't actually perform training.**\n\nThe current implementation only computes forward passes and calculates loss, but never updates model parameters. There is no:\n- Gradient computation (backpropagation)\n- Parameter updates based on gradients\n- Actual optimization step\n\nThis means the model will not learn from training data - parameters remain at their random initialization values. Line 261 stores parameters, but they are unchanged from initialization.\n\nThe comment mentions \"simplified training\" but this is not training at all. A minimal implementation needs at least basic gradient descent with parameter updates.\n\n\n\n\nDo you want me to help implement a basic gradient descent training loop, or open an issue to track implementing proper backpropagation?\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commit 33506ba","created_at":"2025-10-30T16:42:19Z","updated_at":"2025-11-02T14:33:38Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2478810873","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2478810873"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2478810873"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2478810873/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":204,"original_start_line":204,"start_side":"RIGHT","line":319,"original_line":262,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":262,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2478810880","pull_request_review_id":3400618958,"id":2478810880,"node_id":"PRRC_kwDOKSXUF86Tv6cA","diff_hunk":"@@ -0,0 +1,533 @@\n+namespace AiDotNet.TimeSeries;\n+\n+/// <summary>\n+/// Implements the N-BEATS (Neural Basis Expansion Analysis for Time Series) model for forecasting.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations (e.g., float, double).</typeparam>\n+/// <remarks>\n+/// <para>\n+/// N-BEATS is a deep neural architecture based on backward and forward residual links and\n+/// a very deep stack of fully-connected layers. The architecture has the following key features:\n+/// </para>\n+/// <list type=\"bullet\">\n+/// <item>Doubly residual stacking: Each block produces a backcast (reconstruction) and forecast</item>\n+/// <item>Hierarchical decomposition: Multiple stacks focus on different aspects (trend, seasonality)</item>\n+/// <item>Interpretability: Can use polynomial and Fourier basis for explainable forecasts</item>\n+/// <item>No manual feature engineering: Learns directly from raw time series data</item>\n+/// </list>\n+/// <para>\n+/// The original paper: Oreshkin et al., \"N-BEATS: Neural basis expansion analysis for\n+/// interpretable time series forecasting\" (ICLR 2020).\n+/// </para>\n+/// <para><b>For Beginners:</b> N-BEATS is a state-of-the-art neural network for time series\n+/// forecasting that automatically learns patterns from your data. Unlike traditional methods\n+/// that require you to manually specify trends and seasonality, N-BEATS figures these out\n+/// on its own.\n+///\n+/// Key advantages:\n+/// - No need for manual feature engineering (the model learns what's important)\n+/// - Can capture complex, non-linear patterns\n+/// - Provides interpretable components (trend, seasonality) when configured to do so\n+/// - Works well for both short-term and long-term forecasting\n+///\n+/// The model works by stacking many \"blocks\" together, where each block tries to:\n+/// 1. Understand what patterns are in the input (backcast)\n+/// 2. Predict the future based on those patterns (forecast)\n+/// 3. Pass the unexplained patterns to the next block\n+///\n+/// This allows the model to decompose complex time series into simpler components.\n+/// </para>\n+/// </remarks>\n+public class NBEATSModel<T> : TimeSeriesModelBase<T>\n+{\n+    private readonly NBEATSModelOptions<T> _options;\n+    private readonly List<NBEATSBlock<T>> _blocks;\n+    private readonly INumericOperations<T> _numOps;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the NBEATSModel class.\n+    /// </summary>\n+    /// <param name=\"options\">Configuration options for the N-BEATS model. If null, default options are used.</param>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This creates a new N-BEATS model with the specified configuration.\n+    /// The options control things like:\n+    /// - How far back to look (lookback window)\n+    /// - How far forward to predict (forecast horizon)\n+    /// - How complex the model should be (number of stacks, blocks, layer sizes)\n+    /// - Whether to use interpretable components\n+    ///\n+    /// If you don't provide options, sensible defaults will be used.\n+    /// </para>\n+    /// </remarks>\n+    public NBEATSModel(NBEATSModelOptions<T>? options = null) : base(options ?? new NBEATSModelOptions<T>())\n+    {\n+        _options = options ?? new NBEATSModelOptions<T>();\n+        _numOps = MathHelper.GetNumericOperations<T>();\n+        _blocks = new List<NBEATSBlock<T>>();\n+\n+        // Validate options\n+        ValidateNBEATSOptions();\n+\n+        // Initialize blocks\n+        InitializeBlocks();\n+    }\n+\n+    /// <summary>\n+    /// Validates the N-BEATS specific options.\n+    /// </summary>\n+    private void ValidateNBEATSOptions()\n+    {\n+        if (_options.LookbackWindow <= 0)\n+        {\n+            throw new ArgumentException(\"Lookback window must be positive.\", nameof(_options.LookbackWindow));\n+        }\n+\n+        if (_options.ForecastHorizon <= 0)\n+        {\n+            throw new ArgumentException(\"Forecast horizon must be positive.\", nameof(_options.ForecastHorizon));\n+        }\n+\n+        if (_options.NumStacks <= 0)\n+        {\n+            throw new ArgumentException(\"Number of stacks must be positive.\", nameof(_options.NumStacks));\n+        }\n+\n+        if (_options.NumBlocksPerStack <= 0)\n+        {\n+            throw new ArgumentException(\"Number of blocks per stack must be positive.\", nameof(_options.NumBlocksPerStack));\n+        }\n+\n+        if (_options.HiddenLayerSize <= 0)\n+        {\n+            throw new ArgumentException(\"Hidden layer size must be positive.\", nameof(_options.HiddenLayerSize));\n+        }\n+\n+        if (_options.NumHiddenLayers <= 0)\n+        {\n+            throw new ArgumentException(\"Number of hidden layers must be positive.\", nameof(_options.NumHiddenLayers));\n+        }\n+\n+        if (_options.PolynomialDegree < 1)\n+        {\n+            throw new ArgumentException(\"Polynomial degree must be at least 1.\", nameof(_options.PolynomialDegree));\n+        }\n+\n+        if (_options.Epochs <= 0)\n+        {\n+            throw new ArgumentException(\"Number of epochs must be positive.\", nameof(_options.Epochs));\n+        }\n+\n+        if (_options.BatchSize <= 0)\n+        {\n+            throw new ArgumentException(\"Batch size must be positive.\", nameof(_options.BatchSize));\n+        }\n+\n+        if (_options.LearningRate <= 0)\n+        {\n+            throw new ArgumentException(\"Learning rate must be positive.\", nameof(_options.LearningRate));\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Initializes all blocks in the N-BEATS architecture.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This creates all the individual blocks that make up\n+    /// the N-BEATS model. The number of blocks is determined by NumStacks * NumBlocksPerStack.\n+    ///\n+    /// Each block is initialized with the same architecture but different random weights,\n+    /// allowing them to learn different aspects of the time series.\n+    /// </para>\n+    /// </remarks>\n+    private void InitializeBlocks()\n+    {\n+        _blocks.Clear();\n+\n+        // Calculate theta sizes for basis expansion\n+        int thetaSizeBackcast;\n+        int thetaSizeForecast;\n+\n+        if (_options.UseInterpretableBasis)\n+        {\n+            // For polynomial basis, theta size is polynomial degree + 1\n+            thetaSizeBackcast = _options.PolynomialDegree + 1;\n+            thetaSizeForecast = _options.PolynomialDegree + 1;\n+        }\n+        else\n+        {\n+            // For generic basis, theta size matches the output length\n+            thetaSizeBackcast = _options.LookbackWindow;\n+            thetaSizeForecast = _options.ForecastHorizon;\n+        }\n+\n+        // Create all blocks\n+        int totalBlocks = _options.NumStacks * _options.NumBlocksPerStack;\n+        for (int i = 0; i < totalBlocks; i++)\n+        {\n+            var block = new NBEATSBlock<T>(\n+                _options.LookbackWindow,\n+                _options.ForecastHorizon,\n+                _options.HiddenLayerSize,\n+                _options.NumHiddenLayers,\n+                thetaSizeBackcast,\n+                thetaSizeForecast,\n+                _options.UseInterpretableBasis,\n+                _options.PolynomialDegree\n+            );\n+            _blocks.Add(block);\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs the core training logic for the N-BEATS model.\n+    /// </summary>\n+    /// <param name=\"x\">The input features matrix where each row is a historical window.</param>\n+    /// <param name=\"y\">The target values vector where each element is the corresponding forecast target.</param>\n+    /// <remarks>\n+    /// <para>\n+    /// Training uses a simple gradient descent approach with mean squared error loss.\n+    /// The model iterates through the training data for the specified number of epochs,\n+    /// updating parameters to minimize prediction error.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is where the model actually learns from your data.\n+    ///\n+    /// The training process:\n+    /// 1. The model makes predictions on your training data\n+    /// 2. It calculates how far off the predictions are (the error)\n+    /// 3. It adjusts its internal parameters to reduce this error\n+    /// 4. It repeats this process many times (epochs) until it learns the patterns\n+    ///\n+    /// Note: This is a simplified training implementation. A production version would\n+    /// include more sophisticated optimization, regularization, and validation.\n+    /// </para>\n+    /// </remarks>\n+    protected override void TrainCore(Matrix<T> x, Vector<T> y)\n+    {\n+        // For simplicity, we'll implement a basic training loop\n+        // A full implementation would use more sophisticated optimization\n+\n+        int numSamples = x.Rows;\n+\n+        // Simple gradient descent training for demonstration\n+        for (int epoch = 0; epoch < _options.Epochs; epoch++)\n+        {\n+            T totalLoss = _numOps.Zero;\n+\n+            // Process each sample\n+            for (int sampleIdx = 0; sampleIdx < numSamples; sampleIdx++)\n+            {\n+                Vector<T> input = x.GetRow(sampleIdx);\n+\n+                // Forward pass through all blocks\n+                Vector<T> residual = input.Clone();\n+                Vector<T> aggregatedForecast = new Vector<T>(_options.ForecastHorizon);\n+\n+                for (int blockIdx = 0; blockIdx < _blocks.Count; blockIdx++)\n+                {\n+                    var (backcast, forecast) = _blocks[blockIdx].Forward(residual);\n+\n+                    // Update residual for next block\n+                    for (int i = 0; i < residual.Length; i++)\n+                    {\n+                        residual[i] = _numOps.Subtract(residual[i], backcast[i]);\n+                    }\n+\n+                    // Accumulate forecast\n+                    for (int i = 0; i < aggregatedForecast.Length; i++)\n+                    {\n+                        aggregatedForecast[i] = _numOps.Add(aggregatedForecast[i], forecast[i]);\n+                    }\n+                }\n+\n+                // Calculate loss (simplified - just the first forecast step for now)\n+                T target = y[sampleIdx];\n+                T prediction = aggregatedForecast[0];\n+                T error = _numOps.Subtract(prediction, target);\n+                T loss = _numOps.Multiply(error, error);\n+                totalLoss = _numOps.Add(totalLoss, loss);\n+            }\n+\n+            // Average loss for this epoch\n+            T avgLoss = _numOps.Divide(totalLoss, _numOps.FromDouble(numSamples));\n+\n+            // Print progress every 10 epochs\n+            if (epoch % 10 == 0)\n+            {\n+                Console.WriteLine($\"Epoch {epoch}/{_options.Epochs}, Loss: {avgLoss}\");\n+            }","path":"src/TimeSeries/NBEATSModel.cs","commit_id":"2d302b10bff6968f46f6215569850fe3574e2cef","original_commit_id":"2d302b10bff6968f46f6215569850fe3574e2cef","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**Replace Console.WriteLine with a proper logging mechanism.**\n\nDirect console output in a library class is problematic because:\n- Users cannot control or redirect output\n- Doesn't work in non-console applications (web services, GUI apps)\n- Tight coupling to console\n\nConsider using `ILogger<T>`, a progress callback parameter, or a configurable progress reporter instead.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/TimeSeries/NBEATSModel.cs around lines 254 to 257, replace the direct\nConsole.WriteLine call with a configurable logging/progress mechanism: add an\nILogger<NBEATSModel> field injected via the class constructor (or accept an\noptional IProgress<string>/IProgress<double> or a progress callback parameter),\nstore it on the instance, and use logger.LogInformation(...) (or\nprogress.Report(...)) where the Console.WriteLine currently is; ensure the new\ndependency is optional for backward compatibility (fall back to no-op logger or\nskip reporting) and update constructor/signature and any callers accordingly.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-10-30T16:42:19Z","updated_at":"2025-10-30T16:42:21Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2478810880","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2478810880"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2478810880"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2478810880/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":254,"start_side":"RIGHT","line":null,"original_line":257,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":257,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2478810884","pull_request_review_id":3400618958,"id":2478810884,"node_id":"PRRC_kwDOKSXUF86Tv6cE","diff_hunk":"@@ -0,0 +1,533 @@\n+namespace AiDotNet.TimeSeries;\n+\n+/// <summary>\n+/// Implements the N-BEATS (Neural Basis Expansion Analysis for Time Series) model for forecasting.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations (e.g., float, double).</typeparam>\n+/// <remarks>\n+/// <para>\n+/// N-BEATS is a deep neural architecture based on backward and forward residual links and\n+/// a very deep stack of fully-connected layers. The architecture has the following key features:\n+/// </para>\n+/// <list type=\"bullet\">\n+/// <item>Doubly residual stacking: Each block produces a backcast (reconstruction) and forecast</item>\n+/// <item>Hierarchical decomposition: Multiple stacks focus on different aspects (trend, seasonality)</item>\n+/// <item>Interpretability: Can use polynomial and Fourier basis for explainable forecasts</item>\n+/// <item>No manual feature engineering: Learns directly from raw time series data</item>\n+/// </list>\n+/// <para>\n+/// The original paper: Oreshkin et al., \"N-BEATS: Neural basis expansion analysis for\n+/// interpretable time series forecasting\" (ICLR 2020).\n+/// </para>\n+/// <para><b>For Beginners:</b> N-BEATS is a state-of-the-art neural network for time series\n+/// forecasting that automatically learns patterns from your data. Unlike traditional methods\n+/// that require you to manually specify trends and seasonality, N-BEATS figures these out\n+/// on its own.\n+///\n+/// Key advantages:\n+/// - No need for manual feature engineering (the model learns what's important)\n+/// - Can capture complex, non-linear patterns\n+/// - Provides interpretable components (trend, seasonality) when configured to do so\n+/// - Works well for both short-term and long-term forecasting\n+///\n+/// The model works by stacking many \"blocks\" together, where each block tries to:\n+/// 1. Understand what patterns are in the input (backcast)\n+/// 2. Predict the future based on those patterns (forecast)\n+/// 3. Pass the unexplained patterns to the next block\n+///\n+/// This allows the model to decompose complex time series into simpler components.\n+/// </para>\n+/// </remarks>\n+public class NBEATSModel<T> : TimeSeriesModelBase<T>\n+{\n+    private readonly NBEATSModelOptions<T> _options;\n+    private readonly List<NBEATSBlock<T>> _blocks;\n+    private readonly INumericOperations<T> _numOps;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the NBEATSModel class.\n+    /// </summary>\n+    /// <param name=\"options\">Configuration options for the N-BEATS model. If null, default options are used.</param>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This creates a new N-BEATS model with the specified configuration.\n+    /// The options control things like:\n+    /// - How far back to look (lookback window)\n+    /// - How far forward to predict (forecast horizon)\n+    /// - How complex the model should be (number of stacks, blocks, layer sizes)\n+    /// - Whether to use interpretable components\n+    ///\n+    /// If you don't provide options, sensible defaults will be used.\n+    /// </para>\n+    /// </remarks>\n+    public NBEATSModel(NBEATSModelOptions<T>? options = null) : base(options ?? new NBEATSModelOptions<T>())\n+    {\n+        _options = options ?? new NBEATSModelOptions<T>();\n+        _numOps = MathHelper.GetNumericOperations<T>();\n+        _blocks = new List<NBEATSBlock<T>>();\n+\n+        // Validate options\n+        ValidateNBEATSOptions();\n+\n+        // Initialize blocks\n+        InitializeBlocks();\n+    }\n+\n+    /// <summary>\n+    /// Validates the N-BEATS specific options.\n+    /// </summary>\n+    private void ValidateNBEATSOptions()\n+    {\n+        if (_options.LookbackWindow <= 0)\n+        {\n+            throw new ArgumentException(\"Lookback window must be positive.\", nameof(_options.LookbackWindow));\n+        }\n+\n+        if (_options.ForecastHorizon <= 0)\n+        {\n+            throw new ArgumentException(\"Forecast horizon must be positive.\", nameof(_options.ForecastHorizon));\n+        }\n+\n+        if (_options.NumStacks <= 0)\n+        {\n+            throw new ArgumentException(\"Number of stacks must be positive.\", nameof(_options.NumStacks));\n+        }\n+\n+        if (_options.NumBlocksPerStack <= 0)\n+        {\n+            throw new ArgumentException(\"Number of blocks per stack must be positive.\", nameof(_options.NumBlocksPerStack));\n+        }\n+\n+        if (_options.HiddenLayerSize <= 0)\n+        {\n+            throw new ArgumentException(\"Hidden layer size must be positive.\", nameof(_options.HiddenLayerSize));\n+        }\n+\n+        if (_options.NumHiddenLayers <= 0)\n+        {\n+            throw new ArgumentException(\"Number of hidden layers must be positive.\", nameof(_options.NumHiddenLayers));\n+        }\n+\n+        if (_options.PolynomialDegree < 1)\n+        {\n+            throw new ArgumentException(\"Polynomial degree must be at least 1.\", nameof(_options.PolynomialDegree));\n+        }\n+\n+        if (_options.Epochs <= 0)\n+        {\n+            throw new ArgumentException(\"Number of epochs must be positive.\", nameof(_options.Epochs));\n+        }\n+\n+        if (_options.BatchSize <= 0)\n+        {\n+            throw new ArgumentException(\"Batch size must be positive.\", nameof(_options.BatchSize));\n+        }\n+\n+        if (_options.LearningRate <= 0)\n+        {\n+            throw new ArgumentException(\"Learning rate must be positive.\", nameof(_options.LearningRate));\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Initializes all blocks in the N-BEATS architecture.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This creates all the individual blocks that make up\n+    /// the N-BEATS model. The number of blocks is determined by NumStacks * NumBlocksPerStack.\n+    ///\n+    /// Each block is initialized with the same architecture but different random weights,\n+    /// allowing them to learn different aspects of the time series.\n+    /// </para>\n+    /// </remarks>\n+    private void InitializeBlocks()\n+    {\n+        _blocks.Clear();\n+\n+        // Calculate theta sizes for basis expansion\n+        int thetaSizeBackcast;\n+        int thetaSizeForecast;\n+\n+        if (_options.UseInterpretableBasis)\n+        {\n+            // For polynomial basis, theta size is polynomial degree + 1\n+            thetaSizeBackcast = _options.PolynomialDegree + 1;\n+            thetaSizeForecast = _options.PolynomialDegree + 1;\n+        }\n+        else\n+        {\n+            // For generic basis, theta size matches the output length\n+            thetaSizeBackcast = _options.LookbackWindow;\n+            thetaSizeForecast = _options.ForecastHorizon;\n+        }\n+\n+        // Create all blocks\n+        int totalBlocks = _options.NumStacks * _options.NumBlocksPerStack;\n+        for (int i = 0; i < totalBlocks; i++)\n+        {\n+            var block = new NBEATSBlock<T>(\n+                _options.LookbackWindow,\n+                _options.ForecastHorizon,\n+                _options.HiddenLayerSize,\n+                _options.NumHiddenLayers,\n+                thetaSizeBackcast,\n+                thetaSizeForecast,\n+                _options.UseInterpretableBasis,\n+                _options.PolynomialDegree\n+            );\n+            _blocks.Add(block);\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs the core training logic for the N-BEATS model.\n+    /// </summary>\n+    /// <param name=\"x\">The input features matrix where each row is a historical window.</param>\n+    /// <param name=\"y\">The target values vector where each element is the corresponding forecast target.</param>\n+    /// <remarks>\n+    /// <para>\n+    /// Training uses a simple gradient descent approach with mean squared error loss.\n+    /// The model iterates through the training data for the specified number of epochs,\n+    /// updating parameters to minimize prediction error.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is where the model actually learns from your data.\n+    ///\n+    /// The training process:\n+    /// 1. The model makes predictions on your training data\n+    /// 2. It calculates how far off the predictions are (the error)\n+    /// 3. It adjusts its internal parameters to reduce this error\n+    /// 4. It repeats this process many times (epochs) until it learns the patterns\n+    ///\n+    /// Note: This is a simplified training implementation. A production version would\n+    /// include more sophisticated optimization, regularization, and validation.\n+    /// </para>\n+    /// </remarks>\n+    protected override void TrainCore(Matrix<T> x, Vector<T> y)\n+    {\n+        // For simplicity, we'll implement a basic training loop\n+        // A full implementation would use more sophisticated optimization\n+\n+        int numSamples = x.Rows;\n+\n+        // Simple gradient descent training for demonstration\n+        for (int epoch = 0; epoch < _options.Epochs; epoch++)\n+        {\n+            T totalLoss = _numOps.Zero;\n+\n+            // Process each sample\n+            for (int sampleIdx = 0; sampleIdx < numSamples; sampleIdx++)\n+            {\n+                Vector<T> input = x.GetRow(sampleIdx);\n+\n+                // Forward pass through all blocks\n+                Vector<T> residual = input.Clone();\n+                Vector<T> aggregatedForecast = new Vector<T>(_options.ForecastHorizon);\n+\n+                for (int blockIdx = 0; blockIdx < _blocks.Count; blockIdx++)\n+                {\n+                    var (backcast, forecast) = _blocks[blockIdx].Forward(residual);\n+\n+                    // Update residual for next block\n+                    for (int i = 0; i < residual.Length; i++)\n+                    {\n+                        residual[i] = _numOps.Subtract(residual[i], backcast[i]);\n+                    }\n+\n+                    // Accumulate forecast\n+                    for (int i = 0; i < aggregatedForecast.Length; i++)\n+                    {\n+                        aggregatedForecast[i] = _numOps.Add(aggregatedForecast[i], forecast[i]);\n+                    }\n+                }\n+\n+                // Calculate loss (simplified - just the first forecast step for now)\n+                T target = y[sampleIdx];\n+                T prediction = aggregatedForecast[0];\n+                T error = _numOps.Subtract(prediction, target);\n+                T loss = _numOps.Multiply(error, error);\n+                totalLoss = _numOps.Add(totalLoss, loss);\n+            }\n+\n+            // Average loss for this epoch\n+            T avgLoss = _numOps.Divide(totalLoss, _numOps.FromDouble(numSamples));\n+\n+            // Print progress every 10 epochs\n+            if (epoch % 10 == 0)\n+            {\n+                Console.WriteLine($\"Epoch {epoch}/{_options.Epochs}, Loss: {avgLoss}\");\n+            }\n+        }\n+\n+        // Store the final parameters\n+        ModelParameters = GetParameters();\n+    }\n+\n+    /// <summary>\n+    /// Predicts a single value based on the provided input vector.\n+    /// </summary>\n+    /// <param name=\"input\">The input vector containing the lookback window of historical values.</param>\n+    /// <returns>The predicted value for the next time step.</returns>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This method takes a window of historical values and\n+    /// predicts the next value. It runs the input through all the blocks in the model,\n+    /// each block contributing to the final prediction.\n+    /// </para>\n+    /// </remarks>\n+    public override T PredictSingle(Vector<T> input)\n+    {\n+        if (input.Length != _options.LookbackWindow)\n+        {\n+            throw new ArgumentException(\n+                $\"Input length ({input.Length}) must match lookback window ({_options.LookbackWindow}).\",\n+                nameof(input));\n+        }\n+\n+        Vector<T> residual = input.Clone();\n+        Vector<T> aggregatedForecast = new Vector<T>(_options.ForecastHorizon);\n+\n+        // Forward pass through all blocks\n+        for (int blockIdx = 0; blockIdx < _blocks.Count; blockIdx++)\n+        {\n+            var (backcast, forecast) = _blocks[blockIdx].Forward(residual);\n+\n+            // Update residual for next block\n+            for (int i = 0; i < residual.Length; i++)\n+            {\n+                residual[i] = _numOps.Subtract(residual[i], backcast[i]);\n+            }\n+\n+            // Accumulate forecast\n+            for (int i = 0; i < aggregatedForecast.Length; i++)\n+            {\n+                aggregatedForecast[i] = _numOps.Add(aggregatedForecast[i], forecast[i]);\n+            }\n+        }\n+\n+        // Return the first forecast step\n+        return aggregatedForecast[0];\n+    }\n+\n+    /// <summary>\n+    /// Generates forecasts for multiple future time steps.\n+    /// </summary>\n+    /// <param name=\"input\">The input vector containing the lookback window of historical values.</param>\n+    /// <returns>A vector of forecasted values for all forecast horizon steps.</returns>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This method predicts multiple future time steps at once.\n+    /// Unlike PredictSingle which only returns the next value, this returns all values\n+    /// up to the forecast horizon.\n+    ///\n+    /// For example, if your forecast horizon is 7, this will predict the next 7 time steps.\n+    /// </para>\n+    /// </remarks>\n+    public Vector<T> ForecastHorizon(Vector<T> input)\n+    {\n+        if (input.Length != _options.LookbackWindow)\n+        {\n+            throw new ArgumentException(\n+                $\"Input length ({input.Length}) must match lookback window ({_options.LookbackWindow}).\",\n+                nameof(input));\n+        }\n+\n+        Vector<T> residual = input.Clone();\n+        Vector<T> aggregatedForecast = new Vector<T>(_options.ForecastHorizon);\n+\n+        // Forward pass through all blocks\n+        for (int blockIdx = 0; blockIdx < _blocks.Count; blockIdx++)\n+        {\n+            var (backcast, forecast) = _blocks[blockIdx].Forward(residual);\n+\n+            // Update residual for next block\n+            for (int i = 0; i < residual.Length; i++)\n+            {\n+                residual[i] = _numOps.Subtract(residual[i], backcast[i]);\n+            }\n+\n+            // Accumulate forecast\n+            for (int i = 0; i < aggregatedForecast.Length; i++)\n+            {\n+                aggregatedForecast[i] = _numOps.Add(aggregatedForecast[i], forecast[i]);\n+            }\n+        }\n+\n+        return aggregatedForecast;\n+    }\n+\n+    /// <summary>\n+    /// Serializes model-specific data to the binary writer.\n+    /// </summary>\n+    /// <param name=\"writer\">The binary writer to write to.</param>\n+    protected override void SerializeCore(BinaryWriter writer)\n+    {\n+        // Write N-BEATS specific options\n+        writer.Write(_options.NumStacks);\n+        writer.Write(_options.NumBlocksPerStack);\n+        writer.Write(_options.PolynomialDegree);\n+        writer.Write(_options.LookbackWindow);\n+        writer.Write(_options.ForecastHorizon);\n+        writer.Write(_options.HiddenLayerSize);\n+        writer.Write(_options.NumHiddenLayers);\n+        writer.Write(_options.LearningRate);\n+        writer.Write(_options.Epochs);\n+        writer.Write(_options.BatchSize);\n+        writer.Write(_options.ShareWeightsInStack);\n+        writer.Write(_options.UseInterpretableBasis);\n+\n+        // Write all block parameters\n+        writer.Write(_blocks.Count);\n+        foreach (var block in _blocks)\n+        {\n+            Vector<T> blockParams = block.GetParameters();\n+            writer.Write(blockParams.Length);\n+            for (int i = 0; i < blockParams.Length; i++)\n+            {\n+                writer.Write(Convert.ToDouble(blockParams[i]));\n+            }\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Deserializes model-specific data from the binary reader.\n+    /// </summary>\n+    /// <param name=\"reader\">The binary reader to read from.</param>\n+    protected override void DeserializeCore(BinaryReader reader)\n+    {\n+        // Read N-BEATS specific options\n+        _options.NumStacks = reader.ReadInt32();\n+        _options.NumBlocksPerStack = reader.ReadInt32();\n+        _options.PolynomialDegree = reader.ReadInt32();\n+        _options.LookbackWindow = reader.ReadInt32();\n+        _options.ForecastHorizon = reader.ReadInt32();\n+        _options.HiddenLayerSize = reader.ReadInt32();\n+        _options.NumHiddenLayers = reader.ReadInt32();\n+        _options.LearningRate = reader.ReadDouble();\n+        _options.Epochs = reader.ReadInt32();\n+        _options.BatchSize = reader.ReadInt32();\n+        _options.ShareWeightsInStack = reader.ReadBoolean();\n+        _options.UseInterpretableBasis = reader.ReadBoolean();\n+\n+        // Reinitialize blocks with loaded options\n+        InitializeBlocks();\n+\n+        // Read all block parameters\n+        int blockCount = reader.ReadInt32();\n+        if (blockCount != _blocks.Count)\n+        {\n+            throw new InvalidOperationException(\n+                $\"Block count mismatch. Expected {_blocks.Count}, but serialized data contains {blockCount}.\");\n+        }\n+\n+        for (int i = 0; i < blockCount; i++)\n+        {\n+            int paramCount = reader.ReadInt32();\n+            Vector<T> blockParams = new Vector<T>(paramCount);\n+            for (int j = 0; j < paramCount; j++)\n+            {\n+                blockParams[j] = NumOps.FromDouble(reader.ReadDouble());\n+            }\n+            _blocks[i].SetParameters(blockParams);\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Gets metadata about the N-BEATS model.\n+    /// </summary>\n+    /// <returns>A ModelMetadata object containing information about the model.</returns>\n+    public override ModelMetadata<T> GetModelMetadata()\n+    {\n+        return new ModelMetadata<T>\n+        {\n+            ModelName = \"N-BEATS\",\n+            ModelType = \"Time Series Forecasting\",\n+            Description = \"Neural Basis Expansion Analysis for Interpretable Time Series Forecasting\",\n+            ParameterCount = ParameterCount,\n+            InputDimension = _options.LookbackWindow,\n+            OutputDimension = _options.ForecastHorizon,\n+            TrainingMetrics = LastEvaluationMetrics,\n+            Hyperparameters = new Dictionary<string, object>\n+            {\n+                { \"NumStacks\", _options.NumStacks },\n+                { \"NumBlocksPerStack\", _options.NumBlocksPerStack },\n+                { \"PolynomialDegree\", _options.PolynomialDegree },\n+                { \"LookbackWindow\", _options.LookbackWindow },\n+                { \"ForecastHorizon\", _options.ForecastHorizon },\n+                { \"HiddenLayerSize\", _options.HiddenLayerSize },\n+                { \"NumHiddenLayers\", _options.NumHiddenLayers },\n+                { \"UseInterpretableBasis\", _options.UseInterpretableBasis }\n+            }\n+        };\n+    }","path":"src/TimeSeries/NBEATSModel.cs","commit_id":"2d302b10bff6968f46f6215569850fe3574e2cef","original_commit_id":"2d302b10bff6968f46f6215569850fe3574e2cef","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n<details>\n<summary>üß© Analysis chain</summary>\n\n**Critical: GetModelMetadata has compilation errors.**\n\nThe code attempts to set properties that don't exist on `ModelMetadata<T>`:\n- `ModelName` (line 439)\n- `ModelType` (line 440) - also expects an enum, not a string\n- `Description` (line 441)\n- `ParameterCount` (line 442)\n- `InputDimension` (line 443)\n- `OutputDimension` (line 444)\n- `TrainingMetrics` (line 445)\n- `Hyperparameters` (line 446)\n\nThis prevents the code from compiling, as confirmed by the pipeline failure.\n\n\n\n\nPlease verify the actual definition of `ModelMetadata<T>` and update this method to match its available properties. Run the following script to examine the ModelMetadata type:\n\n---\n\nüèÅ Script executed:\n\n```shell\n#!/bin/bash\n# Find and examine the ModelMetadata definition\nfd -e cs ModelMetadata | head -5 | xargs -I {} sh -c 'echo \"=== {} ===\" && cat {}'\n```\n\nLength of output: 15787\n\n---\n\n\n\n</details>\n\n**Critical: GetModelMetadata has compilation errors‚Äîproperty names and types don't match ModelMetadata<T> definition.**\n\nThe following assignments will fail because the properties don't exist on `ModelMetadata<T>`:\n- `ModelName` ‚Üí should be `Name`\n- `ParameterCount`, `InputDimension`, `OutputDimension`, `TrainingMetrics` ‚Üí no equivalents\n- `InputDimension` ‚Üí should be `FeatureCount`\n- `Hyperparameters` ‚Üí should use `Properties` or `AdditionalInfo` dictionary\n- `ModelType` ‚Üí property exists but you're assigning a string instead of a `ModelType` enum value\n\nUpdate this method to use the correct properties: `Name`, `Description`, `FeatureCount`, `Complexity`, `ModelType` (enum), and `AdditionalInfo` or `Properties` dictionaries for hyperparameters.\n\n<details>\n<summary>üß∞ Tools</summary>\n\n<details>\n<summary>ü™õ GitHub Actions: Build</summary>\n\n[error] 439-439: CS0117: 'ModelMetadata<T>' does not contain a definition for 'ModelName'\n\n</details>\n<details>\n<summary>ü™õ GitHub Check: Build All Frameworks</summary>\n\n[failure] 446-446: \n'ModelMetadata<T>' does not contain a definition for 'Hyperparameters'\n\n---\n\n[failure] 445-445: \n'ModelMetadata<T>' does not contain a definition for 'TrainingMetrics'\n\n---\n\n[failure] 444-444: \n'ModelMetadata<T>' does not contain a definition for 'OutputDimension'\n\n---\n\n[failure] 443-443: \n'ModelMetadata<T>' does not contain a definition for 'InputDimension'\n\n---\n\n[failure] 442-442: \n'ModelMetadata<T>' does not contain a definition for 'ParameterCount'\n\n---\n\n[failure] 440-440: \nCannot implicitly convert type 'string' to 'AiDotNet.Enums.ModelType'\n\n---\n\n[failure] 439-439: \n'ModelMetadata<T>' does not contain a definition for 'ModelName'\n\n</details>\n\n</details>\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/TimeSeries/NBEATSModel.cs around lines 435-458, the GetModelMetadata\nmethod uses non-existent properties (ModelName, ParameterCount, InputDimension,\nOutputDimension, TrainingMetrics, Hyperparameters) and assigns a string to\nModelType; update it to populate the actual ModelMetadata<T> members: set Name =\n\"N-BEATS\", Description = the existing description, FeatureCount =\n_options.LookbackWindow, Complexity = ParameterCount (or an appropriate numeric\ncomplexity value), ModelType = ModelType.TimeSeriesForecasting (use the enum),\nand move all hyperparameters plus LastEvaluationMetrics into the metadata's\nAdditionalInfo (or Properties) dictionary as key/value pairs (e.g., \"NumStacks\":\n_options.NumStacks, ..., \"TrainingMetrics\": LastEvaluationMetrics). Ensure types\nmatch the ModelMetadata<T> definition and remove or stop referencing the\nnon-existent properties.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits 8fbbfa2 to 94a43dd","created_at":"2025-10-30T16:42:19Z","updated_at":"2025-10-31T12:42:57Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2478810884","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2478810884"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2478810884"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2478810884/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":492,"original_start_line":435,"start_side":"RIGHT","line":521,"original_line":464,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":458,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2478810890","pull_request_review_id":3400618958,"id":2478810890,"node_id":"PRRC_kwDOKSXUF86Tv6cK","diff_hunk":"@@ -47,18 +47,66 @@ protected override IFullModel<T, Matrix<T>, Vector<T>> TransferSameDomain(\n     /// Transfers a Neural Network model to a target domain with a different feature space.\n     /// </summary>\n     /// <remarks>\n-    /// NOTE: This implementation requires source domain data to properly train the feature mapper.\n-    /// The current API limitations prevent passing source data, so this method will throw\n-    /// InvalidOperationException. Users should use the public Transfer() method that accepts source data.\n+    /// <para>\n+    /// This method performs cross-domain transfer when source and target domains have different\n+    /// feature spaces. It requires a pre-trained feature mapper to be set via SetFeatureMapper().\n+    /// </para>\n+    /// <para>\n+    /// <b>Limitations:</b> Without access to source domain data, this method cannot:\n+    /// 1. Train the feature mapper (must be pre-trained)\n+    /// 2. Perform optimal knowledge distillation (uses model predictions on mapped data)\n+    /// 3. Validate feature space compatibility\n+    /// </para>\n+    /// <para>\n+    /// <b>Recommendation:</b> For best results, use the public Transfer() method that accepts\n+    /// both source and target domain data, which enables proper feature mapper training and\n+    /// knowledge distillation.\n+    /// </para>\n     /// </remarks>\n+    /// <exception cref=\"InvalidOperationException\">\n+    /// Thrown when FeatureMapper is null or not trained.\n+    /// </exception>\n     protected override IFullModel<T, Matrix<T>, Vector<T>> TransferCrossDomain(\n         IFullModel<T, Matrix<T>, Vector<T>> sourceModel,\n         Matrix<T> targetData,\n         Vector<T> targetLabels)\n     {\n-        throw new InvalidOperationException(\n-            \"Cross-domain transfer cannot be performed directly through this protected method due to the need for source domain data. \" +\n-            \"Please use the public 'Transfer(sourceModel, sourceData, targetData, targetLabels)' method which accepts both source and target domain data for feature mapping and transfer.\");\n+        // Validate that feature mapper is available and trained\n+        if (FeatureMapper == null)\n+        {\n+            throw new InvalidOperationException(\n+                \"Cross-domain transfer requires a feature mapper. Use SetFeatureMapper() before transfer. \" +\n+                \"Alternatively, use the public Transfer() method with source data for automatic feature mapping.\");\n+        }\n+\n+        if (!FeatureMapper.IsTrained)\n+        {\n+            throw new InvalidOperationException(\n+                \"Feature mapper must be trained before cross-domain transfer. \" +\n+                \"Either train the mapper using FeatureMapper.Train(sourceData, targetData) or \" +\n+                \"use the public Transfer() method with source data for automatic training.\");\n+        }\n+\n+        // Get source model's feature dimension\n+        int sourceFeatures = sourceModel.GetActiveFeatureIndices().Count();\n+\n+        // Map target data to source feature space\n+        Matrix<T> mappedTargetData = FeatureMapper.MapToSource(targetData, sourceFeatures);\n+\n+        // Use source model to generate soft labels on mapped data\n+        // This provides knowledge distillation without requiring source domain data\n+        Vector<T> softLabels = sourceModel.Predict(mappedTargetData);\n+\n+        // Combine soft labels with true labels\n+        // trueWeight of 0.7 means combinedLabels = 0.7 * trueLabels + 0.3 * softLabels\n+        Vector<T> combinedLabels = CombineLabels(softLabels, targetLabels, 0.7);\n+\n+        // Create and train a new model on the target domain\n+        // Use original targetData (not mapped) since the model should learn in target feature space\n+        var targetModel = sourceModel.DeepCopy();\n+        targetModel.Train(targetData, combinedLabels);","path":"src/TransferLearning/Algorithms/TransferNeuralNetwork.cs","commit_id":"2d302b10bff6968f46f6215569850fe3574e2cef","original_commit_id":"2d302b10bff6968f46f6215569850fe3574e2cef","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Cross-domain NN training still uses source input dimensionality**\n\n`sourceModel.DeepCopy()` keeps the original input layer shape. When `targetData` really has a different feature count (the entire reason to come down this path), the very first forward pass in `Train(...)` hits a matrix-dimension mismatch and blows up. Either build a fresh target-domain network sized to `targetData.Columns`, or keep training in the mapped source space (e.g., train on `mappedTargetData`) and wrap inference with the mapper. Right now the advertised cross-domain scenario is unusable.\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits 36ecbde to bf7f155","created_at":"2025-10-30T16:42:19Z","updated_at":"2025-11-02T03:36:12Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2478810890","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2478810890"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2478810890"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2478810890/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":106,"start_side":"RIGHT","line":null,"original_line":107,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":67,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2478846529","pull_request_review_id":3400667851,"id":2478846529,"node_id":"PRRC_kwDOKSXUF86TwDJB","diff_hunk":"@@ -0,0 +1,406 @@\n+namespace AiDotNet.NeuralNetworks.Layers;\n+\n+/// <summary>\n+/// Wraps an existing layer with LoRA functionality, allowing parameter-efficient fine-tuning.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// The LoRAAdapter wraps an existing layer (called the base layer) and adds a LoRA layer in parallel.\n+/// During forward pass, both the base layer and LoRA layer process the input, and their outputs are\n+/// summed. The base layer's parameters can be frozen while only the LoRA parameters are trained.\n+/// </para>\n+/// <para><b>For Beginners:</b> This adapter lets you add LoRA to an existing layer without modifying it.\n+/// Think of it like adding a \"correction layer\" that learns what adjustments are needed:\n+///\n+/// - The base layer keeps its original weights (optionally frozen)\n+/// - The LoRA layer learns a small correction\n+/// - The final output is: original_output + lora_correction\n+///\n+/// This is incredibly useful for fine-tuning pre-trained models:\n+/// 1. Load a pre-trained model\n+/// 2. Wrap its layers with LoRAAdapter\n+/// 3. Freeze the base layers\n+/// 4. Train only the small LoRA corrections\n+/// 5. Achieve similar results with 100x fewer trainable parameters!\n+/// </para>\n+/// </remarks>\n+public class LoRAAdapter<T> : LayerBase<T>\n+{\n+    /// <summary>\n+    /// The base layer being adapted.\n+    /// </summary>\n+    private readonly ILayer<T> _baseLayer;\n+\n+    /// <summary>\n+    /// The LoRA layer that provides the adaptation.\n+    /// </summary>\n+    private readonly LoRALayer<T> _loraLayer;\n+\n+    /// <summary>\n+    /// Whether the base layer's parameters are frozen (not trainable).\n+    /// </summary>\n+    private readonly bool _freezeBaseLayer;\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters.\n+    /// </summary>\n+    /// <remarks>\n+    /// If the base layer is frozen, this returns only the LoRA parameter count.\n+    /// Otherwise, it returns the sum of base and LoRA parameters.\n+    /// </remarks>\n+    public override int ParameterCount => _freezeBaseLayer ? _loraLayer.ParameterCount : (_baseLayer.ParameterCount + _loraLayer.ParameterCount);\n+\n+    /// <summary>\n+    /// Gets whether this adapter supports training.\n+    /// </summary>\n+    public override bool SupportsTraining => true;\n+\n+    /// <summary>\n+    /// Initializes a new LoRA adapter wrapping an existing layer.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with LoRA.</param>\n+    /// <param name=\"rank\">The rank of the LoRA decomposition.</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when the base layer doesn't have compatible dimensions.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This creates an adapter that adds LoRA to an existing layer.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to make more efficient to fine-tune\n+    /// - rank: How much compression (lower = fewer parameters, less flexibility)\n+    /// - alpha: How strong the LoRA adaptation is\n+    /// - freezeBaseLayer: Whether to lock the original layer's weights (usually true for efficiency)\n+    ///\n+    /// Example: If you have a dense layer with 1000x1000 weights, wrapping it with rank=8 LoRA\n+    /// (frozen) reduces trainable parameters from 1,000,000 to just 16,000!\n+    /// </para>\n+    /// </remarks>\n+    public LoRAAdapter(ILayer<T> baseLayer, int rank, double alpha = -1, bool freezeBaseLayer = true)\n+        : base(baseLayer.GetInputShape(), baseLayer.GetOutputShape())\n+    {\n+        _baseLayer = baseLayer ?? throw new ArgumentNullException(nameof(baseLayer));\n+        _freezeBaseLayer = freezeBaseLayer;\n+\n+        // Validate base layer has single-dimensional input/output\n+        if (baseLayer.GetInputShape().Length != 1 || baseLayer.GetOutputShape().Length != 1)\n+        {\n+            throw new ArgumentException(\"LoRAAdapter currently only supports layers with 1D input/output shapes\");\n+        }\n+\n+        int inputSize = baseLayer.GetInputShape()[0];\n+        int outputSize = baseLayer.GetOutputShape()[0];\n+\n+        // Create the LoRA layer\n+        _loraLayer = new LoRALayer<T>(inputSize, outputSize, rank, alpha);\n+\n+        // Initialize parameters\n+        Parameters = new Vector<T>(ParameterCount);\n+        UpdateParametersFromLayers();\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass through both base and LoRA layers.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Sum of base layer output and LoRA output.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The forward pass computes: output = base_layer(input) + lora_layer(input)\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This runs the input through both the original layer and the\n+    /// LoRA correction layer, then adds their outputs together. The result is the original\n+    /// behavior plus the learned adaptation.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        // Forward through base layer\n+        Tensor<T> baseOutput = _baseLayer.Forward(input);\n+\n+        // Forward through LoRA layer\n+        Tensor<T> loraOutput = _loraLayer.Forward(input);\n+\n+        // Sum the outputs\n+        Tensor<T> result = new Tensor<T>(baseOutput.Shape);\n+        for (int i = 0; i < baseOutput.Length; i++)\n+        {\n+            result[i] = NumOps.Add(baseOutput[i], loraOutput[i]);\n+        }\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Performs the backward pass through both layers.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient flowing back from the next layer.</param>\n+    /// <returns>Gradient to pass to the previous layer.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The backward pass propagates gradients through both the LoRA layer and (if not frozen)\n+    /// the base layer. The input gradients from both paths are summed.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> During learning, this figures out how to improve both layers:\n+    /// - Always updates the LoRA layer (that's what we're training)\n+    /// - Only updates the base layer if it's not frozen\n+    /// - Combines the gradients from both paths to tell earlier layers how to improve\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        // Backward through LoRA layer\n+        Tensor<T> loraInputGrad = _loraLayer.Backward(outputGradient);\n+\n+        // Backward through base layer (if not frozen)\n+        Tensor<T> baseInputGrad;\n+        if (_freezeBaseLayer)\n+        {\n+            // If frozen, still need to compute input gradients but don't update base layer parameters\n+            baseInputGrad = _baseLayer.Backward(outputGradient);\n+        }\n+        else\n+        {\n+            baseInputGrad = _baseLayer.Backward(outputGradient);\n+        }","path":"src/NeuralNetworks/Layers/LoRAAdapter.cs","commit_id":"7c9015291c4fb91b4f6cc8324d1f0aefcf1887eb","original_commit_id":"09b41f2d4d707931c166cf708a09b339cd29b9d3","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_üõ†Ô∏è Refactor suggestion_ | _üü† Major_\n\n**Remove redundant conditional branch.**\n\nBoth branches of the if-else statement execute identical code (`_baseLayer.Backward(outputGradient)`). The comment suggests the frozen case should \"not update base layer parameters,\" but both branches call Backward, which computes parameter gradients. The distinction should be in `UpdateParameters` (line 192), not here.\n\n\n\nApply this diff to simplify:\n\n```diff\n-        // Backward through base layer (if not frozen)\n-        Tensor<T> baseInputGrad;\n-        if (_freezeBaseLayer)\n-        {\n-            // If frozen, still need to compute input gradients but don't update base layer parameters\n-            baseInputGrad = _baseLayer.Backward(outputGradient);\n-        }\n-        else\n-        {\n-            baseInputGrad = _baseLayer.Backward(outputGradient);\n-        }\n+        // Backward through base layer\n+        // Note: Input gradients are always computed; base parameter updates are skipped in UpdateParameters if frozen\n+        Tensor<T> baseInputGrad = _baseLayer.Backward(outputGradient);\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/Layers/LoRAAdapter.cs around lines 159 to 167, the if-else\nis redundant because both branches call _baseLayer.Backward(outputGradient);\nremove the conditional and the duplicated call and replace with a single call:\nbaseInputGrad = _baseLayer.Backward(outputGradient); keep the explanatory\ncomment minimal or move the \"do not update base layer parameters when frozen\"\nnote to the UpdateParameters section (around line 192) where parameter updates\nare actually controlled.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits 429c649 to 562da58","created_at":"2025-10-30T16:54:37Z","updated_at":"2025-10-30T20:13:10Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2478846529","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2478846529"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2478846529"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2478846529/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":159,"start_side":"RIGHT","line":null,"original_line":167,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":167,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2478846537","pull_request_review_id":3400667851,"id":2478846537,"node_id":"PRRC_kwDOKSXUF86TwDJJ","diff_hunk":"@@ -0,0 +1,571 @@\n+namespace AiDotNet.NeuralNetworks.Layers;\n+\n+/// <summary>\n+/// Implements Low-Rank Adaptation (LoRA) layer for parameter-efficient fine-tuning of neural networks.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// LoRA works by decomposing weight updates into two low-rank matrices A and B, where the actual update\n+/// is computed as B * A. This dramatically reduces the number of trainable parameters compared to\n+/// fine-tuning all weights directly.\n+/// </para>\n+/// <para><b>For Beginners:</b> LoRA is a technique that makes it much cheaper to adapt large neural networks\n+/// to new tasks. Instead of updating all the weights in a layer (which can be millions of parameters),\n+/// LoRA adds two small matrices that work together to approximate the needed changes.\n+///\n+/// Think of it like this:\n+/// - Traditional fine-tuning: Adjusting every single knob on a massive control panel\n+/// - LoRA: Using just a few master controls that influence many knobs at once\n+///\n+/// The key insight is that the changes needed for fine-tuning often lie in a \"low-rank\" space,\n+/// meaning we don't need full freedom to adjust every parameter independently.\n+///\n+/// Key parameters:\n+/// - Rank (r): Controls how many \"master controls\" you have. Higher rank = more flexibility but more parameters\n+/// - Alpha: A scaling factor that controls how much influence the LoRA adaptation has\n+///\n+/// For example, adapting a layer with 1000x1000 weights (1M parameters) using LoRA with rank=8 only\n+/// requires 8x1000 + 8x1000 = 16,000 parameters (98.4% reduction!).\n+/// </para>\n+/// </remarks>\n+public class LoRALayer<T> : LayerBase<T>\n+{\n+    /// <summary>\n+    /// Low-rank matrix A with dimensions (inputSize √ó rank).\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Matrix A is the first part of the low-rank decomposition. It projects the input from\n+    /// inputSize dimensions down to rank dimensions. This matrix is initialized with random values\n+    /// and trained during fine-tuning.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is the first of two small matrices that work together.\n+    /// Think of it as compressing the input data into a smaller representation before expanding it again.\n+    /// </para>\n+    /// </remarks>\n+    private Matrix<T> _loraA;\n+\n+    /// <summary>\n+    /// Low-rank matrix B with dimensions (rank √ó outputSize).\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Matrix B is the second part of the low-rank decomposition. It projects from the rank dimensions\n+    /// back up to outputSize dimensions. This matrix is initialized to zero so that at the start of\n+    /// training, the LoRA layer has no effect on the base model's behavior.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is the second matrix that expands the compressed data back\n+    /// to full size. It starts at zero so the adapted model initially behaves exactly like the original.\n+    /// </para>\n+    /// </remarks>\n+    private Matrix<T> _loraB;\n+\n+    /// <summary>\n+    /// The rank of the low-rank decomposition.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// The rank determines the dimensionality of the intermediate representation. Lower ranks mean\n+    /// fewer parameters but less expressiveness. Typical values range from 1 to 64, with 8 being\n+    /// a common choice.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> The rank is like the number of \"compression channels\" you use.\n+    /// Higher rank = more flexibility but more parameters to train. It's a trade-off between\n+    /// efficiency and capability.\n+    /// </para>\n+    /// </remarks>\n+    private readonly int _rank;\n+\n+    /// <summary>\n+    /// Scaling factor for the LoRA contribution.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Alpha controls how much the LoRA adaptation influences the final output. The actual scaling\n+    /// applied is alpha/rank, which helps normalize the contribution across different rank values.\n+    /// Typical values for alpha are in the range of the rank (e.g., alpha = 16 with rank = 8).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This controls how strongly the LoRA adaptation affects the output.\n+    /// It's like a volume knob for the adaptations. The formula alpha/rank automatically adjusts\n+    /// so that different rank values produce similar strength adaptations.\n+    /// </para>\n+    /// </remarks>\n+    private readonly T _alpha;\n+\n+    /// <summary>\n+    /// Computed scaling factor (alpha / rank) used during forward pass.\n+    /// </summary>\n+    private readonly T _scaling;\n+\n+    /// <summary>\n+    /// Gradients for matrix A computed during backpropagation.\n+    /// </summary>\n+    private Matrix<T>? _loraAGradient;\n+\n+    /// <summary>\n+    /// Gradients for matrix B computed during backpropagation.\n+    /// </summary>\n+    private Matrix<T>? _loraBGradient;\n+\n+    /// <summary>\n+    /// Stored input from the forward pass, needed for gradient computation.\n+    /// </summary>\n+    private Tensor<T>? _lastInput;\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters (elements in A and B matrices).\n+    /// </summary>\n+    public override int ParameterCount => (_loraA.Rows * _loraA.Columns) + (_loraB.Rows * _loraB.Columns);\n+\n+    /// <summary>\n+    /// Gets whether this layer supports training (always true for LoRA).\n+    /// </summary>\n+    public override bool SupportsTraining => true;\n+\n+    /// <summary>\n+    /// Initializes a new LoRA layer with the specified dimensions and hyperparameters.\n+    /// </summary>\n+    /// <param name=\"inputSize\">The number of input features.</param>\n+    /// <param name=\"outputSize\">The number of output features.</param>\n+    /// <param name=\"rank\">The rank of the low-rank decomposition (must be positive and less than min(inputSize, outputSize)).</param>\n+    /// <param name=\"alpha\">The scaling factor for LoRA contributions (typically similar to rank value).</param>\n+    /// <param name=\"activationFunction\">Optional activation function to apply after the LoRA transformation.</param>\n+    /// <exception cref=\"ArgumentException\">Thrown when rank is invalid.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// The LoRA matrices are initialized as follows:\n+    /// - Matrix A: Random values from a Gaussian distribution (similar to Kaiming initialization)\n+    /// - Matrix B: Zero initialization (so LoRA starts with no effect)\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This creates a new LoRA layer. You specify the input and output sizes\n+    /// (which should match the layer you're adapting), the rank (how much compression), and alpha\n+    /// (how strong the adaptation is).\n+    ///\n+    /// The initialization is carefully chosen:\n+    /// - Matrix A gets random values (so training can start moving in useful directions)\n+    /// - Matrix B starts at zero (so initially, LoRA doesn't change anything)\n+    /// </para>\n+    /// </remarks>\n+    public LoRALayer(int inputSize, int outputSize, int rank, double alpha = -1, IActivationFunction<T>? activationFunction = null)\n+        : base(new[] { inputSize }, new[] { outputSize }, activationFunction ?? new IdentityActivation<T>())\n+    {\n+        if (rank <= 0)\n+        {\n+            throw new ArgumentException(\"Rank must be positive\", nameof(rank));\n+        }\n+\n+        if (rank > Math.Min(inputSize, outputSize))\n+        {\n+            throw new ArgumentException($\"Rank ({rank}) cannot exceed min(inputSize, outputSize) = {Math.Min(inputSize, outputSize)}\", nameof(rank));\n+        }\n+\n+        _rank = rank;\n+\n+        // Default alpha to rank if not specified\n+        _alpha = alpha > 0 ? NumOps.FromDouble(alpha) : NumOps.FromDouble(rank);\n+        _scaling = NumOps.Divide(_alpha, NumOps.FromDouble(rank));\n+\n+        // Initialize LoRA matrices\n+        // Matrix A: Random initialization (Gaussian with std = 1/sqrt(rank))\n+        _loraA = new Matrix<T>(inputSize, rank);\n+        T stddev = NumOps.Sqrt(NumOps.Divide(NumOps.One, NumOps.FromDouble(rank)));\n+        for (int i = 0; i < _loraA.Rows; i++)\n+        {\n+            for (int j = 0; j < _loraA.Columns; j++)\n+            {\n+                // Box-Muller transform for Gaussian random numbers\n+                double u1 = Random.NextDouble();\n+                double u2 = Random.NextDouble();\n+                double randStdNormal = Math.Sqrt(-2.0 * Math.Log(u1)) * Math.Sin(2.0 * Math.PI * u2);\n+                _loraA[i, j] = NumOps.Multiply(NumOps.FromDouble(randStdNormal), stddev);\n+            }\n+        }\n+\n+        // Matrix B: Zero initialization (so LoRA has no effect initially)\n+        _loraB = new Matrix<T>(rank, outputSize);\n+        for (int i = 0; i < _loraB.Rows; i++)\n+        {\n+            for (int j = 0; j < _loraB.Columns; j++)\n+            {\n+                _loraB[i, j] = NumOps.Zero;\n+            }\n+        }\n+\n+        // Initialize parameter vector\n+        Parameters = new Vector<T>(ParameterCount);\n+        UpdateParametersFromMatrices();\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass through the LoRA layer.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor of shape [batchSize, inputSize].</param>\n+    /// <returns>Output tensor of shape [batchSize, outputSize].</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The forward pass computes: output = input * A * B * scaling\n+    /// where scaling = alpha / rank.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This processes data through the LoRA layer. The input is:\n+    /// 1. Multiplied by matrix A (compressing to rank dimensions)\n+    /// 2. Multiplied by matrix B (expanding back to output dimensions)\n+    /// 3. Scaled by alpha/rank (controlling the strength)\n+    ///\n+    /// The result represents the adaptation that gets added to the base layer's output.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        _lastInput = input.Clone();\n+\n+        // Get batch size and validate input shape\n+        int batchSize = input.Shape[0];\n+        int inputSize = input.Shape.Length > 1 ? input.Shape[1] : input.Length;\n+\n+        if (inputSize != _loraA.Rows)\n+        {\n+            throw new ArgumentException($\"Input size {inputSize} does not match expected input size {_loraA.Rows}\");\n+        }\n+\n+        // Convert input to matrix [batchSize, inputSize]\n+        Matrix<T> inputMatrix = new Matrix<T>(batchSize, inputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                inputMatrix[i, j] = input[i * inputSize + j];\n+            }\n+        }\n+\n+        // Compute: input * A (result: [batchSize, rank])\n+        Matrix<T> intermediate = inputMatrix.Multiply(_loraA);\n+\n+        // Compute: intermediate * B (result: [batchSize, outputSize])\n+        Matrix<T> output = intermediate.Multiply(_loraB);\n+\n+        // Apply scaling\n+        output = output.Multiply(_scaling);\n+\n+        // Convert back to tensor\n+        Vector<T> outputData = new Vector<T>(batchSize * _loraB.Columns);\n+        int idx = 0;\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < _loraB.Columns; j++)\n+            {\n+                outputData[idx++] = output[i, j];\n+            }\n+        }\n+\n+        Tensor<T> result = new Tensor<T>(new[] { batchSize, _loraB.Columns }, outputData);\n+\n+        // Apply activation if specified\n+        if (ScalarActivation != null)\n+        {\n+            result = ApplyActivation(result);\n+        }\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Performs the backward pass through the LoRA layer.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient flowing back from the next layer.</param>\n+    /// <returns>Gradient to pass to the previous layer.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The backward pass computes gradients for both LoRA matrices and propagates gradients back to the input.\n+    /// Gradients are computed as:\n+    /// - dL/dB = A^T * input^T * outputGradient * scaling\n+    /// - dL/dA = input^T * outputGradient * B^T * scaling\n+    /// - dL/dinput = outputGradient * B^T * A^T * scaling\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is where learning happens! The backward pass:\n+    /// 1. Figures out how to adjust matrix A and B to reduce error\n+    /// 2. Passes gradients back to earlier layers so they can learn too\n+    ///\n+    /// It uses calculus (specifically, the chain rule) to figure out how each parameter\n+    /// contributed to the error.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (_lastInput == null)\n+        {\n+            throw new InvalidOperationException(\"Forward pass must be called before backward pass\");\n+        }\n+\n+        // Get dimensions\n+        int batchSize = _lastInput.Shape[0];\n+        int inputSize = _lastInput.Shape.Length > 1 ? _lastInput.Shape[1] : _lastInput.Length;\n+\n+        // Apply activation gradient if needed\n+        if (ScalarActivation != null)\n+        {\n+            // Need to get the pre-activation output for derivative calculation\n+            // For now, we'll pass the gradient through without modification\n+            // A full implementation would require storing pre-activation values\n+            outputGradient = ApplyActivationDerivative(_lastInput, outputGradient);\n+        }","path":"src/NeuralNetworks/Layers/LoRALayer.cs","commit_id":"8d5a1b6564c1a2bc1b334f1863e6003a6fd142b2","original_commit_id":"09b41f2d4d707931c166cf708a09b339cd29b9d3","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**Incomplete activation derivative implementation.**\n\nThe comment explicitly notes \"A full implementation would require storing pre-activation values,\" yet the code calls `ApplyActivationDerivative(_lastInput, outputGradient)`. If the activation is not IdentityActivation, this may produce incorrect gradients since `_lastInput` is the pre-forward input, not the pre-activation output.\n\n\n\nEither:\n1. Store pre-activation values during forward pass and use them here, or\n2. Document that activations are not fully supported yet and throw an exception if a non-identity activation is provided, or\n3. Remove activation support entirely until properly implemented\n\n```diff\n+        // Ensure activation is identity or not set\n+        if (ScalarActivation != null && !(ScalarActivation is IdentityActivation<T>))\n+        {\n+            throw new NotSupportedException(\"Non-identity activation functions are not yet fully supported in LoRALayer\");\n+        }\n+\n         // Apply activation gradient if needed\n         if (ScalarActivation != null)\n         {\n-            // Need to get the pre-activation output for derivative calculation\n-            // For now, we'll pass the gradient through without modification\n-            // A full implementation would require storing pre-activation values\n             outputGradient = ApplyActivationDerivative(_lastInput, outputGradient);\n         }\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/Layers/LoRALayer.cs around lines 305-311, the code\nincorrectly passes _lastInput to ApplyActivationDerivative (which is the\npre-forward input) instead of the pre-activation outputs; update the forward\npass to capture and store the pre-activation values into a new field (e.g.\n_lastPreActivation) immediately before applying ScalarActivation, then change\nthis backprop code to call ApplyActivationDerivative(_lastPreActivation,\noutputGradient); also add null checks and clear/validate _lastPreActivation on\nreset, or alternatively throw a clear NotSupportedException when a non-identity\nactivation is configured until the pre-activation capture is implemented.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits cc389b7 to 6de38f8","created_at":"2025-10-30T16:54:37Z","updated_at":"2025-10-31T14:33:23Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2478846537","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2478846537"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2478846537"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2478846537/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":305,"start_side":"RIGHT","line":null,"original_line":311,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":311,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2478846544","pull_request_review_id":3400667851,"id":2478846544,"node_id":"PRRC_kwDOKSXUF86TwDJQ","diff_hunk":"@@ -0,0 +1,391 @@\n+using AiDotNet.ActivationFunctions;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.NeuralNetworks.Layers;\n+using Xunit;\n+\n+namespace AiDotNetTests.UnitTests.NeuralNetworks\n+{\n+    public class LoRAAdapterTests\n+    {\n+        [Fact]\n+        public void Constructor_WithValidBaseLayer_InitializesCorrectly()\n+        {\n+            // Arrange\n+            var baseLayer = new DenseLayer<double>(10, 5);","path":"tests/UnitTests/NeuralNetworks/LoRAAdapterTests.cs","commit_id":"562da582066321f6906ec3b4c8a7dac6cc73ae66","original_commit_id":"09b41f2d4d707931c166cf708a09b339cd29b9d3","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Fix ambiguous DenseLayer constructor calls.**\n\nThe pipeline fails because `DenseLayer<T>` has two constructors with nullable activation function parameters, making the two-argument call ambiguous. Cast `null` to the appropriate activation function type or pass an explicit activation parameter.\n\n\n\nApply this diff to fix the compilation errors:\n\n```diff\n-            var baseLayer = new DenseLayer<double>(10, 5);\n+            var baseLayer = new DenseLayer<double>(10, 5, (IActivationFunction<double>?)null);\n```\n\nApply this pattern to all occurrences throughout the file (lines 14, 38, 53, 68, 84, 108, 136, 156, 188, 202, 226, 239, 256, 297, 311, 327, 328, 349, 363, 380).\n\n\nAlso applies to: 38-38, 53-53, 68-68, 84-84, 108-108, 136-136, 156-156, 188-188, 202-202, 226-226, 239-239, 256-256, 297-297, 311-311, 327-328, 349-349, 363-363, 380-380\n\n<details>\n<summary>üß∞ Tools</summary>\n\n<details>\n<summary>ü™õ GitHub Actions: Build</summary>\n\n[error] 14-14: CS0121: The call is ambiguous between the following methods or properties: 'DenseLayer<T>.DenseLayer(int, int, IActivationFunction<T>?)' and 'DenseLayer<T>.DenseLayer(int, int, IVectorActivationFunction<T>?)'\n\n</details>\n<details>\n<summary>ü™õ GitHub Check: Build All Frameworks</summary>\n\n[failure] 14-14: \nThe call is ambiguous between the following methods or properties: 'DenseLayer<T>.DenseLayer(int, int, IActivationFunction<T>?)' and 'DenseLayer<T>.DenseLayer(int, int, IVectorActivationFunction<T>?)'\n\n</details>\n\n</details>\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn tests/UnitTests/NeuralNetworks/LoRAAdapterTests.cs around lines 14, 38, 53,\n68, 84, 108, 136, 156, 188, 202, 226, 239, 256, 297, 311, 327-328, 349, 363, 380\nthe two-argument DenseLayer<double> constructor call is ambiguous because both\noverloads accept a nullable activation parameter; fix each occurrence by either\ncasting the null to the activation delegate type (e.g.,\n(Func<double,double>?)null) or by passing an explicit activation parameter (like\nnull as IActivation<double> or an actual activation), replacing lines with the\nsame constructor call pattern so the compiler can resolve the correct overload.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits 8fbbfa2 to 94a43dd","created_at":"2025-10-30T16:54:37Z","updated_at":"2025-10-31T12:43:02Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2478846544","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2478846544"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2478846544"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2478846544/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":null,"original_line":14,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":14,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2479403703","pull_request_review_id":3401474369,"id":2479403703,"node_id":"PRRC_kwDOKSXUF86TyLK3","diff_hunk":"@@ -0,0 +1,398 @@\n+namespace AiDotNet.NeuralNetworks.Layers;\n+\n+/// <summary>\n+/// Wraps an existing layer with LoRA functionality, allowing parameter-efficient fine-tuning.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// The LoRAAdapter wraps an existing layer (called the base layer) and adds a LoRA layer in parallel.\n+/// During forward pass, both the base layer and LoRA layer process the input, and their outputs are\n+/// summed. The base layer's parameters can be frozen while only the LoRA parameters are trained.\n+/// </para>\n+/// <para><b>For Beginners:</b> This adapter lets you add LoRA to an existing layer without modifying it.\n+/// Think of it like adding a \"correction layer\" that learns what adjustments are needed:\n+///\n+/// - The base layer keeps its original weights (optionally frozen)\n+/// - The LoRA layer learns a small correction\n+/// - The final output is: original_output + lora_correction\n+///\n+/// This is incredibly useful for fine-tuning pre-trained models:\n+/// 1. Load a pre-trained model\n+/// 2. Wrap its layers with LoRAAdapter\n+/// 3. Freeze the base layers\n+/// 4. Train only the small LoRA corrections\n+/// 5. Achieve similar results with 100x fewer trainable parameters!\n+/// </para>\n+/// </remarks>\n+public class LoRAAdapter<T> : LayerBase<T>\n+{\n+    /// <summary>\n+    /// The base layer being adapted.\n+    /// </summary>\n+    private readonly ILayer<T> _baseLayer;\n+\n+    /// <summary>\n+    /// The LoRA layer that provides the adaptation.\n+    /// </summary>\n+    private readonly LoRALayer<T> _loraLayer;\n+\n+    /// <summary>\n+    /// Whether the base layer's parameters are frozen (not trainable).\n+    /// </summary>\n+    private readonly bool _freezeBaseLayer;\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters.\n+    /// </summary>\n+    /// <remarks>\n+    /// If the base layer is frozen, this returns only the LoRA parameter count.\n+    /// Otherwise, it returns the sum of base and LoRA parameters.\n+    /// </remarks>\n+    public override int ParameterCount => _freezeBaseLayer ? _loraLayer.ParameterCount : (_baseLayer.ParameterCount + _loraLayer.ParameterCount);\n+\n+    /// <summary>\n+    /// Gets whether this adapter supports training.\n+    /// </summary>\n+    public override bool SupportsTraining => true;\n+\n+    /// <summary>\n+    /// Initializes a new LoRA adapter wrapping an existing layer.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with LoRA.</param>\n+    /// <param name=\"rank\">The rank of the LoRA decomposition.</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when the base layer doesn't have compatible dimensions.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This creates an adapter that adds LoRA to an existing layer.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to make more efficient to fine-tune\n+    /// - rank: How much compression (lower = fewer parameters, less flexibility)\n+    /// - alpha: How strong the LoRA adaptation is\n+    /// - freezeBaseLayer: Whether to lock the original layer's weights (usually true for efficiency)\n+    ///\n+    /// Example: If you have a dense layer with 1000x1000 weights, wrapping it with rank=8 LoRA\n+    /// (frozen) reduces trainable parameters from 1,000,000 to just 16,000!\n+    /// </para>\n+    /// </remarks>\n+    public LoRAAdapter(ILayer<T> baseLayer, int rank, double alpha = -1, bool freezeBaseLayer = true)\n+        : base(baseLayer.GetInputShape(), baseLayer.GetOutputShape())\n+    {\n+        _baseLayer = baseLayer ?? throw new ArgumentNullException(nameof(baseLayer));\n+        _freezeBaseLayer = freezeBaseLayer;\n+\n+        // Validate base layer has single-dimensional input/output\n+        if (baseLayer.GetInputShape().Length != 1 || baseLayer.GetOutputShape().Length != 1)\n+        {\n+            throw new ArgumentException(\"LoRAAdapter currently only supports layers with 1D input/output shapes\");\n+        }\n+\n+        int inputSize = baseLayer.GetInputShape()[0];\n+        int outputSize = baseLayer.GetOutputShape()[0];\n+\n+        // Create the LoRA layer\n+        _loraLayer = new LoRALayer<T>(inputSize, outputSize, rank, alpha);\n+\n+        // Initialize parameters\n+        Parameters = new Vector<T>(ParameterCount);\n+        UpdateParametersFromLayers();\n+    }\n+","path":"src/NeuralNetworks/Layers/LoRAAdapter.cs","commit_id":"5018b6bdca277af9a122f41988c3e03baf40cc83","original_commit_id":"562da582066321f6906ec3b4c8a7dac6cc73ae66","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**Null baseLayer hits `NullReferenceException` before intended guard.**\n\nBecause the base constructor call evaluates `baseLayer.GetInputShape()`/`GetOutputShape()` before the body runs, a null `baseLayer` will throw `NullReferenceException` instead of the advertised `ArgumentNullException`. Please perform the null check inside the initializer before dereferencing.\n\n\n\n```diff\n-    public LoRAAdapter(ILayer<T> baseLayer, int rank, double alpha = -1, bool freezeBaseLayer = true)\n-        : base(baseLayer.GetInputShape(), baseLayer.GetOutputShape())\n+    public LoRAAdapter(ILayer<T> baseLayer, int rank, double alpha = -1, bool freezeBaseLayer = true)\n+        : base(\n+            (baseLayer ?? throw new ArgumentNullException(nameof(baseLayer))).GetInputShape(),\n+            (baseLayer ?? throw new ArgumentNullException(nameof(baseLayer))).GetOutputShape())\n```\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/Layers/LoRAAdapter.cs around lines 81 to 103, the\nbase(...) call dereferences baseLayer before the constructor body so a null\nbaseLayer throws NullReferenceException; change the constructor initializer to\nguard the dereferences by using null-coalescing throw expressions for the\nGetInputShape()/GetOutputShape() calls (so ArgumentNullException is thrown if\nbaseLayer is null), keep the body assignment to _baseLayer and other logic\nunchanged, and ensure the rest of initialization uses the validated baseLayer.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits cc389b7 to 98c4115","created_at":"2025-10-30T20:17:22Z","updated_at":"2025-10-31T12:55:33Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2479403703","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2479403703"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2479403703"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2479403703/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":81,"start_side":"RIGHT","line":null,"original_line":103,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":103,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2483820485","pull_request_review_id":3407620231,"id":2483820485,"node_id":"PRRC_kwDOKSXUF86UDBfF","diff_hunk":"@@ -0,0 +1,575 @@\n+namespace AiDotNet.NeuralNetworks.Layers;\n+\n+/// <summary>\n+/// Implements Low-Rank Adaptation (LoRA) layer for parameter-efficient fine-tuning of neural networks.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// LoRA works by decomposing weight updates into two low-rank matrices A and B, where the actual update\n+/// is computed as B * A. This dramatically reduces the number of trainable parameters compared to\n+/// fine-tuning all weights directly.\n+/// </para>\n+/// <para><b>For Beginners:</b> LoRA is a technique that makes it much cheaper to adapt large neural networks\n+/// to new tasks. Instead of updating all the weights in a layer (which can be millions of parameters),\n+/// LoRA adds two small matrices that work together to approximate the needed changes.\n+///\n+/// Think of it like this:\n+/// - Traditional fine-tuning: Adjusting every single knob on a massive control panel\n+/// - LoRA: Using just a few master controls that influence many knobs at once\n+///\n+/// The key insight is that the changes needed for fine-tuning often lie in a \"low-rank\" space,\n+/// meaning we don't need full freedom to adjust every parameter independently.\n+///\n+/// Key parameters:\n+/// - Rank (r): Controls how many \"master controls\" you have. Higher rank = more flexibility but more parameters\n+/// - Alpha: A scaling factor that controls how much influence the LoRA adaptation has\n+///\n+/// For example, adapting a layer with 1000x1000 weights (1M parameters) using LoRA with rank=8 only\n+/// requires 8x1000 + 8x1000 = 16,000 parameters (98.4% reduction!).\n+/// </para>\n+/// </remarks>\n+public class LoRALayer<T> : LayerBase<T>\n+{\n+    /// <summary>\n+    /// Low-rank matrix A with dimensions (inputSize √ó rank).\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Matrix A is the first part of the low-rank decomposition. It projects the input from\n+    /// inputSize dimensions down to rank dimensions. This matrix is initialized with random values\n+    /// and trained during fine-tuning.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is the first of two small matrices that work together.\n+    /// Think of it as compressing the input data into a smaller representation before expanding it again.\n+    /// </para>\n+    /// </remarks>\n+    private Matrix<T> _loraA;\n+\n+    /// <summary>\n+    /// Low-rank matrix B with dimensions (rank √ó outputSize).\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Matrix B is the second part of the low-rank decomposition. It projects from the rank dimensions\n+    /// back up to outputSize dimensions. This matrix is initialized to zero so that at the start of\n+    /// training, the LoRA layer has no effect on the base model's behavior.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is the second matrix that expands the compressed data back\n+    /// to full size. It starts at zero so the adapted model initially behaves exactly like the original.\n+    /// </para>\n+    /// </remarks>\n+    private Matrix<T> _loraB;\n+\n+    /// <summary>\n+    /// The rank of the low-rank decomposition.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// The rank determines the dimensionality of the intermediate representation. Lower ranks mean\n+    /// fewer parameters but less expressiveness. Typical values range from 1 to 64, with 8 being\n+    /// a common choice.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> The rank is like the number of \"compression channels\" you use.\n+    /// Higher rank = more flexibility but more parameters to train. It's a trade-off between\n+    /// efficiency and capability.\n+    /// </para>\n+    /// </remarks>\n+    private readonly int _rank;\n+\n+    /// <summary>\n+    /// Scaling factor for the LoRA contribution.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Alpha controls how much the LoRA adaptation influences the final output. The actual scaling\n+    /// applied is alpha/rank, which helps normalize the contribution across different rank values.\n+    /// Typical values for alpha are in the range of the rank (e.g., alpha = 16 with rank = 8).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This controls how strongly the LoRA adaptation affects the output.\n+    /// It's like a volume knob for the adaptations. The formula alpha/rank automatically adjusts\n+    /// so that different rank values produce similar strength adaptations.\n+    /// </para>\n+    /// </remarks>\n+    private readonly T _alpha;\n+\n+    /// <summary>\n+    /// Computed scaling factor (alpha / rank) used during forward pass.\n+    /// </summary>\n+    private readonly T _scaling;\n+\n+    /// <summary>\n+    /// Gradients for matrix A computed during backpropagation.\n+    /// </summary>\n+    private Matrix<T>? _loraAGradient;\n+\n+    /// <summary>\n+    /// Gradients for matrix B computed during backpropagation.\n+    /// </summary>\n+    private Matrix<T>? _loraBGradient;\n+\n+    /// <summary>\n+    /// Stored input from the forward pass, needed for gradient computation.\n+    /// </summary>\n+    private Tensor<T>? _lastInput;\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters (elements in A and B matrices).\n+    /// </summary>\n+    public override int ParameterCount => (_loraA.Rows * _loraA.Columns) + (_loraB.Rows * _loraB.Columns);\n+\n+    /// <summary>\n+    /// Gets whether this layer supports training (always true for LoRA).\n+    /// </summary>\n+    public override bool SupportsTraining => true;\n+\n+    /// <summary>\n+    /// Initializes a new LoRA layer with the specified dimensions and hyperparameters.\n+    /// </summary>\n+    /// <param name=\"inputSize\">The number of input features.</param>\n+    /// <param name=\"outputSize\">The number of output features.</param>\n+    /// <param name=\"rank\">The rank of the low-rank decomposition (must be positive and less than min(inputSize, outputSize)).</param>\n+    /// <param name=\"alpha\">The scaling factor for LoRA contributions (typically similar to rank value).</param>\n+    /// <param name=\"activationFunction\">Optional activation function to apply after the LoRA transformation.</param>\n+    /// <exception cref=\"ArgumentException\">Thrown when rank is invalid.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// The LoRA matrices are initialized as follows:\n+    /// - Matrix A: Random values from a Gaussian distribution (similar to Kaiming initialization)\n+    /// - Matrix B: Zero initialization (so LoRA starts with no effect)\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This creates a new LoRA layer. You specify the input and output sizes\n+    /// (which should match the layer you're adapting), the rank (how much compression), and alpha\n+    /// (how strong the adaptation is).\n+    ///\n+    /// The initialization is carefully chosen:\n+    /// - Matrix A gets random values (so training can start moving in useful directions)\n+    /// - Matrix B starts at zero (so initially, LoRA doesn't change anything)\n+    /// </para>\n+    /// </remarks>\n+    public LoRALayer(int inputSize, int outputSize, int rank, double alpha = -1, IActivationFunction<T>? activationFunction = null)\n+        : base(new[] { inputSize }, new[] { outputSize }, activationFunction ?? new IdentityActivation<T>())\n+    {\n+        if (rank <= 0)\n+        {\n+            throw new ArgumentException(\"Rank must be positive\", nameof(rank));\n+        }\n+\n+        if (rank > Math.Min(inputSize, outputSize))\n+        {\n+            throw new ArgumentException($\"Rank ({rank}) cannot exceed min(inputSize, outputSize) = {Math.Min(inputSize, outputSize)}\", nameof(rank));\n+        }\n+\n+        _rank = rank;\n+\n+        // Default alpha to rank if not specified\n+        _alpha = alpha > 0 ? NumOps.FromDouble(alpha) : NumOps.FromDouble(rank);\n+        _scaling = NumOps.Divide(_alpha, NumOps.FromDouble(rank));\n+\n+        // Initialize LoRA matrices\n+        // Matrix A: Random initialization (Gaussian with std = 1/sqrt(rank))\n+        _loraA = new Matrix<T>(inputSize, rank);\n+        T stddev = NumOps.Sqrt(NumOps.Divide(NumOps.One, NumOps.FromDouble(rank)));\n+        for (int i = 0; i < _loraA.Rows; i++)\n+        {\n+            for (int j = 0; j < _loraA.Columns; j++)\n+            {\n+                // Box-Muller transform for Gaussian random numbers\n+                double u1 = Random.NextDouble();\n+                double u2 = Random.NextDouble();","path":"src/NeuralNetworks/Layers/LoRALayer.cs","commit_id":"8d5a1b6564c1a2bc1b334f1863e6003a6fd142b2","original_commit_id":"5018b6bdca277af9a122f41988c3e03baf40cc83","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"Using the static `Random` class without seeding creates non-deterministic initialization, which can cause issues with reproducibility in tests and training. Consider injecting a seeded Random instance or using a seeded static Random field to ensure consistent initialization across runs.","created_at":"2025-11-01T18:06:37Z","updated_at":"2025-11-01T18:06:38Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2483820485","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2483820485"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2483820485"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2483820485/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":183,"original_start_line":178,"start_side":"RIGHT","line":184,"original_line":179,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":179,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2483820490","pull_request_review_id":3407620231,"id":2483820490,"node_id":"PRRC_kwDOKSXUF86UDBfK","diff_hunk":"@@ -0,0 +1,575 @@\n+namespace AiDotNet.NeuralNetworks.Layers;\n+\n+/// <summary>\n+/// Implements Low-Rank Adaptation (LoRA) layer for parameter-efficient fine-tuning of neural networks.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// LoRA works by decomposing weight updates into two low-rank matrices A and B, where the actual update\n+/// is computed as B * A. This dramatically reduces the number of trainable parameters compared to\n+/// fine-tuning all weights directly.\n+/// </para>\n+/// <para><b>For Beginners:</b> LoRA is a technique that makes it much cheaper to adapt large neural networks\n+/// to new tasks. Instead of updating all the weights in a layer (which can be millions of parameters),\n+/// LoRA adds two small matrices that work together to approximate the needed changes.\n+///\n+/// Think of it like this:\n+/// - Traditional fine-tuning: Adjusting every single knob on a massive control panel\n+/// - LoRA: Using just a few master controls that influence many knobs at once\n+///\n+/// The key insight is that the changes needed for fine-tuning often lie in a \"low-rank\" space,\n+/// meaning we don't need full freedom to adjust every parameter independently.\n+///\n+/// Key parameters:\n+/// - Rank (r): Controls how many \"master controls\" you have. Higher rank = more flexibility but more parameters\n+/// - Alpha: A scaling factor that controls how much influence the LoRA adaptation has\n+///\n+/// For example, adapting a layer with 1000x1000 weights (1M parameters) using LoRA with rank=8 only\n+/// requires 8x1000 + 8x1000 = 16,000 parameters (98.4% reduction!).\n+/// </para>\n+/// </remarks>\n+public class LoRALayer<T> : LayerBase<T>\n+{\n+    /// <summary>\n+    /// Low-rank matrix A with dimensions (inputSize √ó rank).\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Matrix A is the first part of the low-rank decomposition. It projects the input from\n+    /// inputSize dimensions down to rank dimensions. This matrix is initialized with random values\n+    /// and trained during fine-tuning.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is the first of two small matrices that work together.\n+    /// Think of it as compressing the input data into a smaller representation before expanding it again.\n+    /// </para>\n+    /// </remarks>\n+    private Matrix<T> _loraA;\n+\n+    /// <summary>\n+    /// Low-rank matrix B with dimensions (rank √ó outputSize).\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Matrix B is the second part of the low-rank decomposition. It projects from the rank dimensions\n+    /// back up to outputSize dimensions. This matrix is initialized to zero so that at the start of\n+    /// training, the LoRA layer has no effect on the base model's behavior.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is the second matrix that expands the compressed data back\n+    /// to full size. It starts at zero so the adapted model initially behaves exactly like the original.\n+    /// </para>\n+    /// </remarks>\n+    private Matrix<T> _loraB;\n+\n+    /// <summary>\n+    /// The rank of the low-rank decomposition.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// The rank determines the dimensionality of the intermediate representation. Lower ranks mean\n+    /// fewer parameters but less expressiveness. Typical values range from 1 to 64, with 8 being\n+    /// a common choice.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> The rank is like the number of \"compression channels\" you use.\n+    /// Higher rank = more flexibility but more parameters to train. It's a trade-off between\n+    /// efficiency and capability.\n+    /// </para>\n+    /// </remarks>\n+    private readonly int _rank;\n+\n+    /// <summary>\n+    /// Scaling factor for the LoRA contribution.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Alpha controls how much the LoRA adaptation influences the final output. The actual scaling\n+    /// applied is alpha/rank, which helps normalize the contribution across different rank values.\n+    /// Typical values for alpha are in the range of the rank (e.g., alpha = 16 with rank = 8).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This controls how strongly the LoRA adaptation affects the output.\n+    /// It's like a volume knob for the adaptations. The formula alpha/rank automatically adjusts\n+    /// so that different rank values produce similar strength adaptations.\n+    /// </para>\n+    /// </remarks>\n+    private readonly T _alpha;\n+\n+    /// <summary>\n+    /// Computed scaling factor (alpha / rank) used during forward pass.\n+    /// </summary>\n+    private readonly T _scaling;\n+\n+    /// <summary>\n+    /// Gradients for matrix A computed during backpropagation.\n+    /// </summary>\n+    private Matrix<T>? _loraAGradient;\n+\n+    /// <summary>\n+    /// Gradients for matrix B computed during backpropagation.\n+    /// </summary>\n+    private Matrix<T>? _loraBGradient;\n+\n+    /// <summary>\n+    /// Stored input from the forward pass, needed for gradient computation.\n+    /// </summary>\n+    private Tensor<T>? _lastInput;\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters (elements in A and B matrices).\n+    /// </summary>\n+    public override int ParameterCount => (_loraA.Rows * _loraA.Columns) + (_loraB.Rows * _loraB.Columns);\n+\n+    /// <summary>\n+    /// Gets whether this layer supports training (always true for LoRA).\n+    /// </summary>\n+    public override bool SupportsTraining => true;\n+\n+    /// <summary>\n+    /// Initializes a new LoRA layer with the specified dimensions and hyperparameters.\n+    /// </summary>\n+    /// <param name=\"inputSize\">The number of input features.</param>\n+    /// <param name=\"outputSize\">The number of output features.</param>\n+    /// <param name=\"rank\">The rank of the low-rank decomposition (must be positive and less than min(inputSize, outputSize)).</param>\n+    /// <param name=\"alpha\">The scaling factor for LoRA contributions (typically similar to rank value).</param>\n+    /// <param name=\"activationFunction\">Optional activation function to apply after the LoRA transformation.</param>\n+    /// <exception cref=\"ArgumentException\">Thrown when rank is invalid.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// The LoRA matrices are initialized as follows:\n+    /// - Matrix A: Random values from a Gaussian distribution (similar to Kaiming initialization)\n+    /// - Matrix B: Zero initialization (so LoRA starts with no effect)\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This creates a new LoRA layer. You specify the input and output sizes\n+    /// (which should match the layer you're adapting), the rank (how much compression), and alpha\n+    /// (how strong the adaptation is).\n+    ///\n+    /// The initialization is carefully chosen:\n+    /// - Matrix A gets random values (so training can start moving in useful directions)\n+    /// - Matrix B starts at zero (so initially, LoRA doesn't change anything)\n+    /// </para>\n+    /// </remarks>\n+    public LoRALayer(int inputSize, int outputSize, int rank, double alpha = -1, IActivationFunction<T>? activationFunction = null)\n+        : base(new[] { inputSize }, new[] { outputSize }, activationFunction ?? new IdentityActivation<T>())\n+    {\n+        if (rank <= 0)\n+        {\n+            throw new ArgumentException(\"Rank must be positive\", nameof(rank));\n+        }\n+\n+        if (rank > Math.Min(inputSize, outputSize))\n+        {\n+            throw new ArgumentException($\"Rank ({rank}) cannot exceed min(inputSize, outputSize) = {Math.Min(inputSize, outputSize)}\", nameof(rank));\n+        }\n+\n+        _rank = rank;\n+\n+        // Default alpha to rank if not specified\n+        _alpha = alpha > 0 ? NumOps.FromDouble(alpha) : NumOps.FromDouble(rank);\n+        _scaling = NumOps.Divide(_alpha, NumOps.FromDouble(rank));\n+\n+        // Initialize LoRA matrices\n+        // Matrix A: Random initialization (Gaussian with std = 1/sqrt(rank))\n+        _loraA = new Matrix<T>(inputSize, rank);\n+        T stddev = NumOps.Sqrt(NumOps.Divide(NumOps.One, NumOps.FromDouble(rank)));\n+        for (int i = 0; i < _loraA.Rows; i++)\n+        {\n+            for (int j = 0; j < _loraA.Columns; j++)\n+            {\n+                // Box-Muller transform for Gaussian random numbers\n+                double u1 = Random.NextDouble();\n+                double u2 = Random.NextDouble();\n+                double randStdNormal = Math.Sqrt(-2.0 * Math.Log(u1)) * Math.Sin(2.0 * Math.PI * u2);\n+                _loraA[i, j] = NumOps.Multiply(NumOps.FromDouble(randStdNormal), stddev);\n+            }\n+        }\n+\n+        // Matrix B: Zero initialization (so LoRA has no effect initially)\n+        _loraB = new Matrix<T>(rank, outputSize);\n+        for (int i = 0; i < _loraB.Rows; i++)\n+        {\n+            for (int j = 0; j < _loraB.Columns; j++)\n+            {\n+                _loraB[i, j] = NumOps.Zero;\n+            }\n+        }\n+\n+        // Initialize parameter vector\n+        Parameters = new Vector<T>(ParameterCount);\n+        UpdateParametersFromMatrices();\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass through the LoRA layer.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor of shape [batchSize, inputSize].</param>\n+    /// <returns>Output tensor of shape [batchSize, outputSize].</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The forward pass computes: output = input * A * B * scaling\n+    /// where scaling = alpha / rank.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This processes data through the LoRA layer. The input is:\n+    /// 1. Multiplied by matrix A (compressing to rank dimensions)\n+    /// 2. Multiplied by matrix B (expanding back to output dimensions)\n+    /// 3. Scaled by alpha/rank (controlling the strength)\n+    ///\n+    /// The result represents the adaptation that gets added to the base layer's output.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        _lastInput = input.Clone();\n+\n+        // Get batch size and validate input shape\n+        int batchSize = input.Shape[0];\n+        int inputSize = input.Shape.Length > 1 ? input.Shape[1] : input.Length;\n+\n+        if (inputSize != _loraA.Rows)\n+        {\n+            throw new ArgumentException($\"Input size {inputSize} does not match expected input size {_loraA.Rows}\");\n+        }\n+\n+        // Convert input to matrix [batchSize, inputSize]\n+        Matrix<T> inputMatrix = new Matrix<T>(batchSize, inputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                inputMatrix[i, j] = input[i * inputSize + j];\n+            }\n+        }\n+\n+        // Compute: input * A (result: [batchSize, rank])\n+        Matrix<T> intermediate = inputMatrix.Multiply(_loraA);\n+\n+        // Compute: intermediate * B (result: [batchSize, outputSize])\n+        Matrix<T> output = intermediate.Multiply(_loraB);\n+\n+        // Apply scaling\n+        output = output.Multiply(_scaling);\n+\n+        // Convert back to tensor\n+        Vector<T> outputData = new Vector<T>(batchSize * _loraB.Columns);\n+        int idx = 0;\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < _loraB.Columns; j++)\n+            {\n+                outputData[idx++] = output[i, j];\n+            }\n+        }\n+\n+        Tensor<T> result = new Tensor<T>(new[] { batchSize, _loraB.Columns }, outputData);\n+\n+        // Apply activation if specified\n+        if (ScalarActivation != null)\n+        {\n+            result = ApplyActivation(result);\n+        }\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Performs the backward pass through the LoRA layer.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient flowing back from the next layer.</param>\n+    /// <returns>Gradient to pass to the previous layer.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The backward pass computes gradients for both LoRA matrices and propagates gradients back to the input.\n+    /// Gradients are computed as:\n+    /// - dL/dB = A^T * input^T * outputGradient * scaling\n+    /// - dL/dA = input^T * outputGradient * B^T * scaling\n+    /// - dL/dinput = outputGradient * B^T * A^T * scaling\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is where learning happens! The backward pass:\n+    /// 1. Figures out how to adjust matrix A and B to reduce error\n+    /// 2. Passes gradients back to earlier layers so they can learn too\n+    ///\n+    /// It uses calculus (specifically, the chain rule) to figure out how each parameter\n+    /// contributed to the error.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (_lastInput == null)\n+        {\n+            throw new InvalidOperationException(\"Forward pass must be called before backward pass\");\n+        }\n+\n+        // Get dimensions\n+        int batchSize = _lastInput.Shape[0];\n+        int inputSize = _lastInput.Shape.Length > 1 ? _lastInput.Shape[1] : _lastInput.Length;\n+\n+        // Apply activation gradient if needed\n+        // Ensure activation is identity or not set (non-identity activations require pre-activation storage)\n+        if (ScalarActivation != null && !(ScalarActivation is IdentityActivation<T>))\n+        {\n+            throw new NotSupportedException(\"Non-identity activation functions are not yet fully supported in LoRALayer. \" +\n+                \"Full support requires storing pre-activation values during the forward pass.\");\n+        }","path":"src/NeuralNetworks/Layers/LoRALayer.cs","commit_id":"8d5a1b6564c1a2bc1b334f1863e6003a6fd142b2","original_commit_id":"5018b6bdca277af9a122f41988c3e03baf40cc83","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"This check throws an exception during backpropagation rather than at construction time. Consider validating activation function compatibility in the constructor to fail fast and provide clearer feedback to users when they attempt to create a LoRALayer with unsupported activation functions.","created_at":"2025-11-01T18:06:37Z","updated_at":"2025-11-01T18:06:38Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2483820490","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2483820490"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2483820490"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2483820490/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":306,"start_side":"RIGHT","line":null,"original_line":310,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":310,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2483820495","pull_request_review_id":3407620231,"id":2483820495,"node_id":"PRRC_kwDOKSXUF86UDBfP","diff_hunk":"@@ -0,0 +1,575 @@\n+namespace AiDotNet.NeuralNetworks.Layers;\n+\n+/// <summary>\n+/// Implements Low-Rank Adaptation (LoRA) layer for parameter-efficient fine-tuning of neural networks.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// LoRA works by decomposing weight updates into two low-rank matrices A and B, where the actual update\n+/// is computed as B * A. This dramatically reduces the number of trainable parameters compared to\n+/// fine-tuning all weights directly.\n+/// </para>\n+/// <para><b>For Beginners:</b> LoRA is a technique that makes it much cheaper to adapt large neural networks\n+/// to new tasks. Instead of updating all the weights in a layer (which can be millions of parameters),\n+/// LoRA adds two small matrices that work together to approximate the needed changes.\n+///\n+/// Think of it like this:\n+/// - Traditional fine-tuning: Adjusting every single knob on a massive control panel\n+/// - LoRA: Using just a few master controls that influence many knobs at once\n+///\n+/// The key insight is that the changes needed for fine-tuning often lie in a \"low-rank\" space,\n+/// meaning we don't need full freedom to adjust every parameter independently.\n+///\n+/// Key parameters:\n+/// - Rank (r): Controls how many \"master controls\" you have. Higher rank = more flexibility but more parameters\n+/// - Alpha: A scaling factor that controls how much influence the LoRA adaptation has\n+///\n+/// For example, adapting a layer with 1000x1000 weights (1M parameters) using LoRA with rank=8 only\n+/// requires 8x1000 + 8x1000 = 16,000 parameters (98.4% reduction!).\n+/// </para>\n+/// </remarks>\n+public class LoRALayer<T> : LayerBase<T>\n+{\n+    /// <summary>\n+    /// Low-rank matrix A with dimensions (inputSize √ó rank).\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Matrix A is the first part of the low-rank decomposition. It projects the input from\n+    /// inputSize dimensions down to rank dimensions. This matrix is initialized with random values\n+    /// and trained during fine-tuning.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is the first of two small matrices that work together.\n+    /// Think of it as compressing the input data into a smaller representation before expanding it again.\n+    /// </para>\n+    /// </remarks>\n+    private Matrix<T> _loraA;\n+\n+    /// <summary>\n+    /// Low-rank matrix B with dimensions (rank √ó outputSize).\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Matrix B is the second part of the low-rank decomposition. It projects from the rank dimensions\n+    /// back up to outputSize dimensions. This matrix is initialized to zero so that at the start of\n+    /// training, the LoRA layer has no effect on the base model's behavior.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is the second matrix that expands the compressed data back\n+    /// to full size. It starts at zero so the adapted model initially behaves exactly like the original.\n+    /// </para>\n+    /// </remarks>\n+    private Matrix<T> _loraB;\n+\n+    /// <summary>\n+    /// The rank of the low-rank decomposition.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// The rank determines the dimensionality of the intermediate representation. Lower ranks mean\n+    /// fewer parameters but less expressiveness. Typical values range from 1 to 64, with 8 being\n+    /// a common choice.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> The rank is like the number of \"compression channels\" you use.\n+    /// Higher rank = more flexibility but more parameters to train. It's a trade-off between\n+    /// efficiency and capability.\n+    /// </para>\n+    /// </remarks>\n+    private readonly int _rank;\n+\n+    /// <summary>\n+    /// Scaling factor for the LoRA contribution.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Alpha controls how much the LoRA adaptation influences the final output. The actual scaling\n+    /// applied is alpha/rank, which helps normalize the contribution across different rank values.\n+    /// Typical values for alpha are in the range of the rank (e.g., alpha = 16 with rank = 8).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This controls how strongly the LoRA adaptation affects the output.\n+    /// It's like a volume knob for the adaptations. The formula alpha/rank automatically adjusts\n+    /// so that different rank values produce similar strength adaptations.\n+    /// </para>\n+    /// </remarks>\n+    private readonly T _alpha;\n+\n+    /// <summary>\n+    /// Computed scaling factor (alpha / rank) used during forward pass.\n+    /// </summary>\n+    private readonly T _scaling;\n+\n+    /// <summary>\n+    /// Gradients for matrix A computed during backpropagation.\n+    /// </summary>\n+    private Matrix<T>? _loraAGradient;\n+\n+    /// <summary>\n+    /// Gradients for matrix B computed during backpropagation.\n+    /// </summary>\n+    private Matrix<T>? _loraBGradient;\n+\n+    /// <summary>\n+    /// Stored input from the forward pass, needed for gradient computation.\n+    /// </summary>\n+    private Tensor<T>? _lastInput;\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters (elements in A and B matrices).\n+    /// </summary>\n+    public override int ParameterCount => (_loraA.Rows * _loraA.Columns) + (_loraB.Rows * _loraB.Columns);\n+\n+    /// <summary>\n+    /// Gets whether this layer supports training (always true for LoRA).\n+    /// </summary>\n+    public override bool SupportsTraining => true;\n+\n+    /// <summary>\n+    /// Initializes a new LoRA layer with the specified dimensions and hyperparameters.\n+    /// </summary>\n+    /// <param name=\"inputSize\">The number of input features.</param>\n+    /// <param name=\"outputSize\">The number of output features.</param>\n+    /// <param name=\"rank\">The rank of the low-rank decomposition (must be positive and less than min(inputSize, outputSize)).</param>\n+    /// <param name=\"alpha\">The scaling factor for LoRA contributions (typically similar to rank value).</param>\n+    /// <param name=\"activationFunction\">Optional activation function to apply after the LoRA transformation.</param>\n+    /// <exception cref=\"ArgumentException\">Thrown when rank is invalid.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// The LoRA matrices are initialized as follows:\n+    /// - Matrix A: Random values from a Gaussian distribution (similar to Kaiming initialization)\n+    /// - Matrix B: Zero initialization (so LoRA starts with no effect)\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This creates a new LoRA layer. You specify the input and output sizes\n+    /// (which should match the layer you're adapting), the rank (how much compression), and alpha\n+    /// (how strong the adaptation is).\n+    ///\n+    /// The initialization is carefully chosen:\n+    /// - Matrix A gets random values (so training can start moving in useful directions)\n+    /// - Matrix B starts at zero (so initially, LoRA doesn't change anything)\n+    /// </para>\n+    /// </remarks>\n+    public LoRALayer(int inputSize, int outputSize, int rank, double alpha = -1, IActivationFunction<T>? activationFunction = null)\n+        : base(new[] { inputSize }, new[] { outputSize }, activationFunction ?? new IdentityActivation<T>())\n+    {\n+        if (rank <= 0)\n+        {\n+            throw new ArgumentException(\"Rank must be positive\", nameof(rank));\n+        }\n+\n+        if (rank > Math.Min(inputSize, outputSize))\n+        {\n+            throw new ArgumentException($\"Rank ({rank}) cannot exceed min(inputSize, outputSize) = {Math.Min(inputSize, outputSize)}\", nameof(rank));\n+        }\n+\n+        _rank = rank;\n+\n+        // Default alpha to rank if not specified\n+        _alpha = alpha > 0 ? NumOps.FromDouble(alpha) : NumOps.FromDouble(rank);\n+        _scaling = NumOps.Divide(_alpha, NumOps.FromDouble(rank));\n+\n+        // Initialize LoRA matrices\n+        // Matrix A: Random initialization (Gaussian with std = 1/sqrt(rank))\n+        _loraA = new Matrix<T>(inputSize, rank);\n+        T stddev = NumOps.Sqrt(NumOps.Divide(NumOps.One, NumOps.FromDouble(rank)));\n+        for (int i = 0; i < _loraA.Rows; i++)\n+        {\n+            for (int j = 0; j < _loraA.Columns; j++)\n+            {\n+                // Box-Muller transform for Gaussian random numbers\n+                double u1 = Random.NextDouble();\n+                double u2 = Random.NextDouble();\n+                double randStdNormal = Math.Sqrt(-2.0 * Math.Log(u1)) * Math.Sin(2.0 * Math.PI * u2);\n+                _loraA[i, j] = NumOps.Multiply(NumOps.FromDouble(randStdNormal), stddev);\n+            }\n+        }\n+\n+        // Matrix B: Zero initialization (so LoRA has no effect initially)\n+        _loraB = new Matrix<T>(rank, outputSize);\n+        for (int i = 0; i < _loraB.Rows; i++)\n+        {\n+            for (int j = 0; j < _loraB.Columns; j++)\n+            {\n+                _loraB[i, j] = NumOps.Zero;\n+            }\n+        }\n+\n+        // Initialize parameter vector\n+        Parameters = new Vector<T>(ParameterCount);\n+        UpdateParametersFromMatrices();\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass through the LoRA layer.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor of shape [batchSize, inputSize].</param>\n+    /// <returns>Output tensor of shape [batchSize, outputSize].</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The forward pass computes: output = input * A * B * scaling\n+    /// where scaling = alpha / rank.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This processes data through the LoRA layer. The input is:\n+    /// 1. Multiplied by matrix A (compressing to rank dimensions)\n+    /// 2. Multiplied by matrix B (expanding back to output dimensions)\n+    /// 3. Scaled by alpha/rank (controlling the strength)\n+    ///\n+    /// The result represents the adaptation that gets added to the base layer's output.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        _lastInput = input.Clone();\n+\n+        // Get batch size and validate input shape\n+        int batchSize = input.Shape[0];\n+        int inputSize = input.Shape.Length > 1 ? input.Shape[1] : input.Length;\n+\n+        if (inputSize != _loraA.Rows)\n+        {\n+            throw new ArgumentException($\"Input size {inputSize} does not match expected input size {_loraA.Rows}\");\n+        }\n+\n+        // Convert input to matrix [batchSize, inputSize]\n+        Matrix<T> inputMatrix = new Matrix<T>(batchSize, inputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                inputMatrix[i, j] = input[i * inputSize + j];\n+            }\n+        }\n+\n+        // Compute: input * A (result: [batchSize, rank])\n+        Matrix<T> intermediate = inputMatrix.Multiply(_loraA);\n+\n+        // Compute: intermediate * B (result: [batchSize, outputSize])\n+        Matrix<T> output = intermediate.Multiply(_loraB);\n+\n+        // Apply scaling\n+        output = output.Multiply(_scaling);\n+\n+        // Convert back to tensor\n+        Vector<T> outputData = new Vector<T>(batchSize * _loraB.Columns);\n+        int idx = 0;\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < _loraB.Columns; j++)\n+            {\n+                outputData[idx++] = output[i, j];\n+            }\n+        }\n+\n+        Tensor<T> result = new Tensor<T>(new[] { batchSize, _loraB.Columns }, outputData);\n+\n+        // Apply activation if specified\n+        if (ScalarActivation != null)\n+        {\n+            result = ApplyActivation(result);\n+        }\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Performs the backward pass through the LoRA layer.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient flowing back from the next layer.</param>\n+    /// <returns>Gradient to pass to the previous layer.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The backward pass computes gradients for both LoRA matrices and propagates gradients back to the input.\n+    /// Gradients are computed as:\n+    /// - dL/dB = A^T * input^T * outputGradient * scaling\n+    /// - dL/dA = input^T * outputGradient * B^T * scaling\n+    /// - dL/dinput = outputGradient * B^T * A^T * scaling\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is where learning happens! The backward pass:\n+    /// 1. Figures out how to adjust matrix A and B to reduce error\n+    /// 2. Passes gradients back to earlier layers so they can learn too\n+    ///\n+    /// It uses calculus (specifically, the chain rule) to figure out how each parameter\n+    /// contributed to the error.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (_lastInput == null)\n+        {\n+            throw new InvalidOperationException(\"Forward pass must be called before backward pass\");\n+        }\n+\n+        // Get dimensions\n+        int batchSize = _lastInput.Shape[0];\n+        int inputSize = _lastInput.Shape.Length > 1 ? _lastInput.Shape[1] : _lastInput.Length;\n+\n+        // Apply activation gradient if needed\n+        // Ensure activation is identity or not set (non-identity activations require pre-activation storage)\n+        if (ScalarActivation != null && !(ScalarActivation is IdentityActivation<T>))\n+        {\n+            throw new NotSupportedException(\"Non-identity activation functions are not yet fully supported in LoRALayer. \" +\n+                \"Full support requires storing pre-activation values during the forward pass.\");\n+        }\n+\n+        if (ScalarActivation != null)\n+        {\n+            outputGradient = ApplyActivationDerivative(_lastInput, outputGradient);","path":"src/NeuralNetworks/Layers/LoRALayer.cs","commit_id":"8d5a1b6564c1a2bc1b334f1863e6003a6fd142b2","original_commit_id":"5018b6bdca277af9a122f41988c3e03baf40cc83","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"The activation derivative is being applied to `_lastInput` which contains the pre-LoRA input, but `ApplyActivationDerivative` typically expects the post-activation output. This will produce incorrect gradients when an activation function is used. The derivative should be applied using the stored forward pass output (post-activation), not the input.","created_at":"2025-11-01T18:06:37Z","updated_at":"2025-11-01T18:06:38Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2483820495","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2483820495"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2483820495"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2483820495/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":null,"original_line":314,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":314,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2483820502","pull_request_review_id":3407620231,"id":2483820502,"node_id":"PRRC_kwDOKSXUF86UDBfW","diff_hunk":"@@ -0,0 +1,400 @@\n+namespace AiDotNet.NeuralNetworks.Layers;\n+\n+/// <summary>\n+/// Wraps an existing layer with LoRA functionality, allowing parameter-efficient fine-tuning.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// The LoRAAdapter wraps an existing layer (called the base layer) and adds a LoRA layer in parallel.\n+/// During forward pass, both the base layer and LoRA layer process the input, and their outputs are\n+/// summed. The base layer's parameters can be frozen while only the LoRA parameters are trained.\n+/// </para>\n+/// <para><b>For Beginners:</b> This adapter lets you add LoRA to an existing layer without modifying it.\n+/// Think of it like adding a \"correction layer\" that learns what adjustments are needed:\n+///\n+/// - The base layer keeps its original weights (optionally frozen)\n+/// - The LoRA layer learns a small correction\n+/// - The final output is: original_output + lora_correction\n+///\n+/// This is incredibly useful for fine-tuning pre-trained models:\n+/// 1. Load a pre-trained model\n+/// 2. Wrap its layers with LoRAAdapter\n+/// 3. Freeze the base layers\n+/// 4. Train only the small LoRA corrections\n+/// 5. Achieve similar results with 100x fewer trainable parameters!\n+/// </para>\n+/// </remarks>\n+public class LoRAAdapter<T> : LayerBase<T>\n+{\n+    /// <summary>\n+    /// The base layer being adapted.\n+    /// </summary>\n+    private readonly ILayer<T> _baseLayer;\n+\n+    /// <summary>\n+    /// The LoRA layer that provides the adaptation.\n+    /// </summary>\n+    private readonly LoRALayer<T> _loraLayer;\n+\n+    /// <summary>\n+    /// Whether the base layer's parameters are frozen (not trainable).\n+    /// </summary>\n+    private readonly bool _freezeBaseLayer;\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters.\n+    /// </summary>\n+    /// <remarks>\n+    /// If the base layer is frozen, this returns only the LoRA parameter count.\n+    /// Otherwise, it returns the sum of base and LoRA parameters.\n+    /// </remarks>\n+    public override int ParameterCount => _freezeBaseLayer ? _loraLayer.ParameterCount : (_baseLayer.ParameterCount + _loraLayer.ParameterCount);\n+\n+    /// <summary>\n+    /// Gets whether this adapter supports training.\n+    /// </summary>\n+    public override bool SupportsTraining => true;\n+\n+    /// <summary>\n+    /// Initializes a new LoRA adapter wrapping an existing layer.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with LoRA.</param>\n+    /// <param name=\"rank\">The rank of the LoRA decomposition.</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when the base layer doesn't have compatible dimensions.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This creates an adapter that adds LoRA to an existing layer.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to make more efficient to fine-tune\n+    /// - rank: How much compression (lower = fewer parameters, less flexibility)\n+    /// - alpha: How strong the LoRA adaptation is\n+    /// - freezeBaseLayer: Whether to lock the original layer's weights (usually true for efficiency)\n+    ///\n+    /// Example: If you have a dense layer with 1000x1000 weights, wrapping it with rank=8 LoRA\n+    /// (frozen) reduces trainable parameters from 1,000,000 to just 16,000!\n+    /// </para>\n+    /// </remarks>\n+    public LoRAAdapter(ILayer<T> baseLayer, int rank, double alpha = -1, bool freezeBaseLayer = true)\n+        : base(\n+            (baseLayer ?? throw new ArgumentNullException(nameof(baseLayer))).GetInputShape(),\n+            (baseLayer ?? throw new ArgumentNullException(nameof(baseLayer))).GetOutputShape())\n+    {","path":"src/LoRA/Adapters/LoRAAdapterBase.cs","commit_id":"fa815036ee570162056ba6aca773f34c80f36948","original_commit_id":"5018b6bdca277af9a122f41988c3e03baf40cc83","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"The null check for `baseLayer` is duplicated and performed inline within the base constructor call. This check occurs four times in lines 83-86. Consider validating the parameter once before the base constructor call for better readability and to avoid redundant null checks.\n```suggestion\n            baseLayer.GetInputShape(),\n            baseLayer.GetOutputShape())\n    {\n        if (baseLayer == null)\n            throw new ArgumentNullException(nameof(baseLayer));\n```","created_at":"2025-11-01T18:06:37Z","updated_at":"2025-11-01T18:06:38Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2483820502","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2483820502"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2483820502"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2483820502/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":142,"original_start_line":83,"start_side":"RIGHT","line":144,"original_line":85,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":85,"position":144,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106168","pull_request_review_id":3407993158,"id":2484106168,"node_id":"PRRC_kwDOKSXUF86UEHO4","diff_hunk":"@@ -0,0 +1,101 @@\n+namespace AiDotNet.Interfaces;\n+\n+/// <summary>\n+/// Interface for LoRA (Low-Rank Adaptation) adapters that wrap existing layers with parameter-efficient adaptations.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// LoRA adapters enable efficient fine-tuning of neural networks by learning low-rank decompositions\n+/// of weight updates instead of modifying all weights directly. This interface defines the contract\n+/// for all LoRA adapter implementations across different layer types.\n+/// </para>\n+/// <para><b>For Beginners:</b> A LoRA adapter wraps an existing layer (like a dense or convolutional layer)\n+/// and adds a small \"correction layer\" that learns what adjustments are needed. This is much more\n+/// memory-efficient than retraining all the weights in a large model.\n+///\n+/// Think of it like:\n+/// - The base layer has the original knowledge (frozen or trainable)\n+/// - The LoRA layer learns a small correction\n+/// - The final output combines both: original + correction\n+///\n+/// This allows you to adapt large pre-trained models with 100x fewer trainable parameters!\n+/// </para>\n+/// </remarks>\n+public interface ILoRAAdapter<T> : ILayer<T>\n+{\n+    /// <summary>\n+    /// Gets the base layer being adapted with LoRA.\n+    /// </summary>\n+    /// <remarks>\n+    /// This is the original layer that's being enhanced with LoRA adaptations.\n+    /// It may be frozen (non-trainable) during fine-tuning for maximum efficiency.\n+    /// </remarks>\n+    ILayer<T> BaseLayer { get; }\n+\n+    /// <summary>\n+    /// Gets the LoRA layer providing the low-rank adaptation.\n+    /// </summary>\n+    /// <remarks>\n+    /// This layer implements the low-rank decomposition (A and B matrices)\n+    /// that provides the adaptation to the base layer's behavior.\n+    /// </remarks>\n+    LoRALayer<T> LoRALayer { get; }\n+","path":"src/Interfaces/ILoRAAdapter.cs","commit_id":"fa815036ee570162056ba6aca773f34c80f36948","original_commit_id":"c11235933bf7c341c8b4a223cf6ac11b757d1480","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Missing namespace import for LoRALayer.**\n\nLines 1-44: `LoRALayer<T>` lives in `AiDotNet.NeuralNetworks.Layers`, but this file has no `using` for that namespace, so the interface won‚Äôt compile (`CS0246`). Please add `using AiDotNet.NeuralNetworks.Layers;` (or fully qualify the type) at the top.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/Interfaces/ILoRAAdapter.cs around lines 1 to 44, the type LoRALayer<T> is\nundefined because the file is missing the namespace import; add using\nAiDotNet.NeuralNetworks.Layers; at the top of the file (below the existing\nnamespace/usings) or alternatively fully qualify the LoRALayer<T> reference as\nAiDotNet.NeuralNetworks.Layers.LoRALayer<T> so the interface compiles without\nCS0246.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits 36ecbde to bf7f155","created_at":"2025-11-02T02:32:34Z","updated_at":"2025-11-02T03:36:10Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106168","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106168"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106168"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106168/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":3,"original_start_line":1,"start_side":"RIGHT","line":46,"original_line":44,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":44,"position":46,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106173","pull_request_review_id":3407993158,"id":2484106173,"node_id":"PRRC_kwDOKSXUF86UEHO9","diff_hunk":"@@ -322,4 +322,33 @@ public interface IPredictionModelBuilder<T, TInput, TOutput>\n     /// <param name=\"evaluator\">The fairness evaluator implementation to use.</param>\n     /// <returns>The builder instance for method chaining.</returns>\n     IPredictionModelBuilder<T, TInput, TOutput> ConfigureFairnessEvaluator(IFairnessEvaluator<T> evaluator);\n+\n+    /// <summary>\n+    /// Configures LoRA (Low-Rank Adaptation) for parameter-efficient fine-tuning.\n+    /// </summary>\n+    /// <remarks>\n+    /// LoRA enables efficient fine-tuning of neural networks by learning low-rank decompositions\n+    /// of weight updates instead of modifying all weights directly. This dramatically reduces\n+    /// the number of trainable parameters while maintaining model performance.\n+    ///\n+    /// <b>For Beginners:</b> LoRA is a technique that lets you adapt large pre-trained models\n+    /// with 100x fewer parameters than traditional fine-tuning. Instead of updating all weights,\n+    /// LoRA adds small \"correction layers\" that learn what adjustments are needed.\n+    ///\n+    /// Think of it like:\n+    /// - The original model has the base knowledge (optionally frozen)\n+    /// - LoRA layers learn small corrections for your specific task\n+    /// - The final output combines both: original + correction\n+    ///\n+    /// This is especially useful when:\n+    /// - You want to fine-tune a large model with limited memory\n+    /// - You need to create multiple task-specific versions of the same model\n+    /// - You want to adapt pre-trained models without retraining everything\n+    ///\n+    /// The configuration determines which layers get LoRA adaptations, what rank to use,\n+    /// and whether to freeze the base layers during training.\n+    /// </remarks>\n+    /// <param name=\"loraConfiguration\">The LoRA configuration implementation to use.</param>\n+    /// <returns>The builder instance for method chaining.</returns>\n+    IPredictionModelBuilder<T, TInput, TOutput> ConfigureLoRA(ILoRAConfiguration<T> loraConfiguration);","path":"src/Interfaces/IPredictionModelBuilder.cs","commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","original_commit_id":"c11235933bf7c341c8b4a223cf6ac11b757d1480","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**New interface method is a breaking change.**\n\nLines 326-353: `IPredictionModelBuilder<T, TInput, TOutput>` is part of the public surface. Adding `ConfigureLoRA(...)` forces every existing implementation to recompile and implement the new member, which is a breaking change. Unless you are planning a major version bump, please provide a default interface implementation, convert this into an extension method, or introduce a new optional interface so existing consumers remain source/binary compatible.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nsrc/Interfaces/IPredictionModelBuilder.cs lines 326-353: Adding\nConfigureLoRA(...) to the public interface is a breaking change; fix by making\nit non-breaking‚Äîeither (A) provide a default interface implementation (C# 8+)\nthat returns the builder or throws NotSupportedException (e.g., default\nimplementation: return this or throw new NotSupportedException(\"ConfigureLoRA\nnot supported\")), (B) instead move the method into a static extension method in\na new Extensions class so existing implementations remain unchanged, or (C)\nintroduce a new optional interface like\nILoRACapablePredictionModelBuilder<T,TInput,TOutput> and leave\nIPredictionModelBuilder unchanged; pick one approach, implement it consistently\n(add the default method body OR add the extension method OR add the new\ninterface and update consuming code to use it).\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:32:34Z","updated_at":"2025-11-02T02:32:40Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106173","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106173"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106173"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106173/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":326,"original_start_line":326,"start_side":"RIGHT","line":353,"original_line":353,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":32,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106177","pull_request_review_id":3407993158,"id":2484106177,"node_id":"PRRC_kwDOKSXUF86UEHPB","diff_hunk":"@@ -0,0 +1,629 @@\n+using AiDotNet.Interfaces;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.NeuralNetworks.Layers;\n+\n+/// <summary>\n+/// Chain-of-LoRA adapter that implements sequential composition of multiple LoRA adapters.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// Chain-of-LoRA (COLA) is an advanced LoRA technique that enables sequential composition\n+/// of multiple LoRA adaptations through an iterative optimization framework. Unlike standard\n+/// LoRA which applies a single low-rank adaptation, COLA builds a chain of adaptations where\n+/// each adapter is trained, merged into the model, and then a new adapter is initialized for\n+/// further refinement.\n+/// </para>\n+/// <para>\n+/// This approach bridges the performance gap between standard LoRA and full fine-tuning by\n+/// employing residual learning principles. Each iteration in the chain adds incremental\n+/// improvements to the model's task-specific performance without incurring additional\n+/// computational costs or memory overhead during inference.\n+/// </para>\n+/// <para><b>Key Concepts:</b>\n+///\n+/// <b>Sequential Adaptation:</b>\n+/// Chain-of-LoRA applies adaptations in sequence (Task A ‚Üí Task B ‚Üí Task C), where each\n+/// stage builds upon the previous one. This is inspired by the Frank-Wolfe optimization\n+/// algorithm, which makes greedy updates along the direction of maximum improvement.\n+///\n+/// <b>Merge and Re-initialize:</b>\n+/// After training each LoRA adapter, the learned weights are merged back into the base layer,\n+/// and a new LoRA adapter is initialized. This \"tying a knot\" process allows the model to\n+/// consolidate learned knowledge before adding new adaptations.\n+///\n+/// <b>Knowledge Preservation:</b>\n+/// By freezing the base layer and only training the LoRA components, the chain preserves\n+/// previously learned knowledge while allowing new task-specific adaptations. Each adapter\n+/// in the chain captures a specific aspect of the task or a refinement step.\n+///\n+/// <b>Incremental Fine-tuning Pipeline:</b>\n+/// COLA enables continual learning scenarios where tasks are presented sequentially, and\n+/// the model must adapt to new tasks while maintaining performance on previous ones.\n+/// </para>\n+/// <para><b>Benefits of Chain-of-LoRA:</b>\n+///\n+/// - <b>Better Performance:</b> Achieves up to 6.47% relative accuracy gain over standard LoRA\n+/// - <b>No Extra Overhead:</b> After merging, inference cost is identical to the base model\n+/// - <b>Modular Adaptation:</b> Each adapter can be trained, tested, and validated independently\n+/// - <b>Catastrophic Forgetting Mitigation:</b> Sequential merging helps preserve prior knowledge\n+/// - <b>Task Chaining:</b> Naturally supports multi-task learning and transfer learning scenarios\n+/// - <b>Flexible Deployment:</b> Can deploy the full chain or selected adapters as needed\n+/// </para>\n+/// <para><b>For Beginners:</b>\n+///\n+/// Imagine you're learning a complex skill in stages:\n+/// 1. First, you learn the basics (Adapter 1)\n+/// 2. Then you practice and the basics become automatic (Merge)\n+/// 3. Next, you learn intermediate techniques on top of the basics (Adapter 2)\n+/// 4. Again, you practice until they're automatic (Merge)\n+/// 5. Finally, you learn advanced skills building on everything before (Adapter 3)\n+///\n+/// Chain-of-LoRA works the same way: each adapter learns something new, then it's consolidated\n+/// into the model, and the next adapter can focus on the next refinement. This stepwise approach\n+/// often achieves better results than trying to learn everything at once.\n+/// </para>\n+/// <para><b>Research Reference:</b>\n+///\n+/// Based on \"Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning\"\n+/// (arXiv:2401.04151, January 2024). The paper demonstrates that sequential low-rank adaptations\n+/// can significantly improve task performance compared to single-stage LoRA, especially on\n+/// complex reasoning and multi-step tasks.\n+/// </para>\n+/// <para><b>Usage Example:</b>\n+/// <code>\n+/// // Create a chain with 3 sequential adaptations\n+/// var chain = new ChainLoRAAdapter&lt;double&gt;(baseLayer, rank: 8, chainLength: 3);\n+///\n+/// // Train first adapter on Task A\n+/// chain.SetActiveAdapterIndex(0);\n+/// TrainModel(chain, taskAData);\n+/// chain.MergeActiveAdapter(); // Consolidate Task A knowledge\n+///\n+/// // Train second adapter on Task B\n+/// chain.SetActiveAdapterIndex(1);\n+/// TrainModel(chain, taskBData);\n+/// chain.MergeActiveAdapter(); // Consolidate Task B knowledge\n+///\n+/// // Train third adapter on Task C\n+/// chain.SetActiveAdapterIndex(2);\n+/// TrainModel(chain, taskCData);\n+///\n+/// // Deploy: all adaptations are now part of the model\n+/// ILayer&lt;double&gt; finalLayer = chain.MergeToOriginalLayer();\n+/// </code>\n+/// </para>\n+/// </remarks>\n+public class ChainLoRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// The chain of LoRA adapters applied sequentially.\n+    /// </summary>\n+    private readonly List<LoRALayer<T>> _adapterChain;\n+\n+    /// <summary>\n+    /// The index of the currently active adapter being trained.\n+    /// </summary>\n+    private int _activeAdapterIndex;\n+\n+    /// <summary>\n+    /// Whether each adapter in the chain has been merged.\n+    /// </summary>\n+    private readonly List<bool> _mergedStatus;\n+\n+    /// <summary>\n+    /// The total length of the adapter chain.\n+    /// </summary>\n+    private readonly int _chainLength;\n+\n+    /// <summary>\n+    /// Gets the total number of adapters in the chain.\n+    /// </summary>\n+    /// <remarks>\n+    /// This represents the maximum number of sequential adaptation stages that can be applied.\n+    /// Each adapter can be trained independently and then merged before proceeding to the next.\n+    /// </remarks>\n+    public int ChainLength => _chainLength;\n+\n+    /// <summary>\n+    /// Gets the index of the currently active adapter (0-based).\n+    /// </summary>\n+    /// <remarks>\n+    /// The active adapter is the one currently being trained. Other adapters in the chain\n+    /// are either waiting to be trained (higher indices) or have been merged (lower indices).\n+    /// </remarks>\n+    public int ActiveAdapterIndex => _activeAdapterIndex;\n+\n+    /// <summary>\n+    /// Gets the list of LoRA adapters in the chain.\n+    /// </summary>\n+    /// <remarks>\n+    /// Each adapter in the chain represents one stage of sequential adaptation.\n+    /// Adapters are applied in order during forward passes.\n+    /// </remarks>\n+    public IReadOnlyList<LoRALayer<T>> AdapterChain => _adapterChain.AsReadOnly();\n+\n+    /// <summary>\n+    /// Gets the merged status of each adapter in the chain.\n+    /// </summary>\n+    /// <remarks>\n+    /// True indicates that an adapter has been merged into the base layer and should\n+    /// no longer contribute trainable parameters. Merged adapters still contribute\n+    /// to the forward pass until the entire chain is collapsed.\n+    /// </remarks>\n+    public IReadOnlyList<bool> MergedStatus => _mergedStatus.AsReadOnly();\n+\n+    /// <summary>\n+    /// Initializes a new Chain-of-LoRA adapter with the specified configuration.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with the LoRA chain.</param>\n+    /// <param name=\"rank\">The rank of each LoRA decomposition in the chain.</param>\n+    /// <param name=\"chainLength\">The number of sequential adapters in the chain (default: 3).</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor for each adapter (defaults to rank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training (default: true).</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when chainLength is less than 1.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// Creates a chain of LoRA adapters for sequential fine-tuning. Each adapter in the chain\n+    /// can be trained independently, merged into the model, and then the next adapter can be\n+    /// activated for further refinement.\n+    /// </para>\n+    /// <para><b>For Beginners:</b>\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to adapt (e.g., a dense or convolutional layer)\n+    /// - rank: How compressed each adapter is (lower = fewer parameters per stage)\n+    /// - chainLength: How many sequential adaptation stages you want (typical: 2-5)\n+    /// - alpha: Controls adaptation strength (usually equals rank)\n+    /// - freezeBaseLayer: Lock base weights to preserve pre-trained knowledge (recommended: true)\n+    ///\n+    /// Example: chainLength=3 means you can do three rounds of training and merging,\n+    /// allowing the model to incrementally improve on complex tasks.\n+    /// </para>\n+    /// </remarks>\n+    public ChainLoRAAdapter(\n+        ILayer<T> baseLayer,\n+        int rank,\n+        int chainLength = 3,\n+        double alpha = -1,\n+        bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        if (chainLength < 1)\n+        {\n+            throw new ArgumentException(\"Chain length must be at least 1\", nameof(chainLength));\n+        }\n+\n+        _chainLength = chainLength;\n+        _activeAdapterIndex = 0;\n+        _adapterChain = new List<LoRALayer<T>>(chainLength);\n+        _mergedStatus = new List<bool>(chainLength);\n+\n+        // Create the chain of LoRA adapters\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+\n+        for (int i = 0; i < chainLength; i++)\n+        {\n+            var adapter = new LoRALayer<T>(inputSize, outputSize, rank, alpha);\n+            _adapterChain.Add(adapter);\n+            _mergedStatus.Add(false);\n+        }\n+\n+        // Update parameter count to reflect all unmerged adapters\n+        UpdateParameterCount();\n+    }\n+","path":"src/NeuralNetworks/Layers/ChainLoRAAdapter.cs","commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","original_commit_id":"c11235933bf7c341c8b4a223cf6ac11b757d1480","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Keep `ParameterCount` in sync with the chain.**\n\nBecause the class still inherits `ParameterCount` from `LoRAAdapterBase`, it always reports only the base layer plus the single `_loraLayer`. After adding extra adapters the internal `Parameters` vector is much larger, so `SetParameters`, `UpdateParameterGradientsFromChain`, etc. either throw or allocate undersized buffers. Track the chain‚Äôs size explicitly (e.g. store `_currentParameterCount` that `UpdateParameterCount` refreshes) and fall back to `base.ParameterCount` until the chain has been initialised.\n\n```diff\n-    /// <summary>\n-    /// The total length of the adapter chain.\n-    /// </summary>\n-    private readonly int _chainLength;\n+    /// <summary>\n+    /// The total length of the adapter chain.\n+    /// </summary>\n+    private readonly int _chainLength;\n+\n+    private int _currentParameterCount;\n...\n-    /// <summary>\n-    /// Gets the current parameters as a vector.\n-    /// </summary>\n-    /// <returns>Vector containing parameters from base layer (if not frozen) and all unmerged adapters.</returns>\n-    public override Vector<T> GetParameters()\n-    {\n-        return Parameters.Clone();\n-    }\n+    public override int ParameterCount => _currentParameterCount > 0 ? _currentParameterCount : base.ParameterCount;\n+\n+    /// <summary>\n+    /// Gets the current parameters as a vector.\n+    /// </summary>\n+    /// <returns>Vector containing parameters from base layer (if not frozen) and all unmerged adapters.</returns>\n+    public override Vector<T> GetParameters()\n+    {\n+        return Parameters.Clone();\n+    }\n...\n-        Parameters = new Vector<T>(count);\n-        ParameterGradients = new Vector<T>(count);\n+        _currentParameterCount = count;\n+        Parameters = new Vector<T>(_currentParameterCount);\n+        ParameterGradients = new Vector<T>(_currentParameterCount);\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nsrc/NeuralNetworks/Layers/ChainLoRAAdapter.cs lines 124-220: The class currently\nreports ParameterCount from LoRAAdapterBase (base.ParameterCount) and doesn't\nreflect additional adapters in _adapterChain, causing undersized buffers and\nexceptions; introduce a private field like _currentParameterCount which\nUpdateParameterCount will compute as base.ParameterCount plus the sum of\nparameter counts for every adapter in _adapterChain (or simply compute from\nchain length, rank and layer dims), ensure all public ParameterCount and any\ncode that allocates or validates parameter buffers read this\n_currentParameterCount, and initialize _currentParameterCount to\nbase.ParameterCount (or use base.ParameterCount until _adapterChain is\npopulated) so methods like SetParameters and UpdateParameterGradientsFromChain\nuse the correct size once the chain is created.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:32:34Z","updated_at":"2025-11-02T02:32:40Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106177","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106177"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106177"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106177/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":134,"original_start_line":124,"start_side":"RIGHT","line":230,"original_line":220,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":220,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106189","pull_request_review_id":3407993158,"id":2484106189,"node_id":"PRRC_kwDOKSXUF86UEHPN","diff_hunk":"@@ -0,0 +1,629 @@\n+using AiDotNet.Interfaces;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.NeuralNetworks.Layers;\n+\n+/// <summary>\n+/// Chain-of-LoRA adapter that implements sequential composition of multiple LoRA adapters.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// Chain-of-LoRA (COLA) is an advanced LoRA technique that enables sequential composition\n+/// of multiple LoRA adaptations through an iterative optimization framework. Unlike standard\n+/// LoRA which applies a single low-rank adaptation, COLA builds a chain of adaptations where\n+/// each adapter is trained, merged into the model, and then a new adapter is initialized for\n+/// further refinement.\n+/// </para>\n+/// <para>\n+/// This approach bridges the performance gap between standard LoRA and full fine-tuning by\n+/// employing residual learning principles. Each iteration in the chain adds incremental\n+/// improvements to the model's task-specific performance without incurring additional\n+/// computational costs or memory overhead during inference.\n+/// </para>\n+/// <para><b>Key Concepts:</b>\n+///\n+/// <b>Sequential Adaptation:</b>\n+/// Chain-of-LoRA applies adaptations in sequence (Task A ‚Üí Task B ‚Üí Task C), where each\n+/// stage builds upon the previous one. This is inspired by the Frank-Wolfe optimization\n+/// algorithm, which makes greedy updates along the direction of maximum improvement.\n+///\n+/// <b>Merge and Re-initialize:</b>\n+/// After training each LoRA adapter, the learned weights are merged back into the base layer,\n+/// and a new LoRA adapter is initialized. This \"tying a knot\" process allows the model to\n+/// consolidate learned knowledge before adding new adaptations.\n+///\n+/// <b>Knowledge Preservation:</b>\n+/// By freezing the base layer and only training the LoRA components, the chain preserves\n+/// previously learned knowledge while allowing new task-specific adaptations. Each adapter\n+/// in the chain captures a specific aspect of the task or a refinement step.\n+///\n+/// <b>Incremental Fine-tuning Pipeline:</b>\n+/// COLA enables continual learning scenarios where tasks are presented sequentially, and\n+/// the model must adapt to new tasks while maintaining performance on previous ones.\n+/// </para>\n+/// <para><b>Benefits of Chain-of-LoRA:</b>\n+///\n+/// - <b>Better Performance:</b> Achieves up to 6.47% relative accuracy gain over standard LoRA\n+/// - <b>No Extra Overhead:</b> After merging, inference cost is identical to the base model\n+/// - <b>Modular Adaptation:</b> Each adapter can be trained, tested, and validated independently\n+/// - <b>Catastrophic Forgetting Mitigation:</b> Sequential merging helps preserve prior knowledge\n+/// - <b>Task Chaining:</b> Naturally supports multi-task learning and transfer learning scenarios\n+/// - <b>Flexible Deployment:</b> Can deploy the full chain or selected adapters as needed\n+/// </para>\n+/// <para><b>For Beginners:</b>\n+///\n+/// Imagine you're learning a complex skill in stages:\n+/// 1. First, you learn the basics (Adapter 1)\n+/// 2. Then you practice and the basics become automatic (Merge)\n+/// 3. Next, you learn intermediate techniques on top of the basics (Adapter 2)\n+/// 4. Again, you practice until they're automatic (Merge)\n+/// 5. Finally, you learn advanced skills building on everything before (Adapter 3)\n+///\n+/// Chain-of-LoRA works the same way: each adapter learns something new, then it's consolidated\n+/// into the model, and the next adapter can focus on the next refinement. This stepwise approach\n+/// often achieves better results than trying to learn everything at once.\n+/// </para>\n+/// <para><b>Research Reference:</b>\n+///\n+/// Based on \"Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning\"\n+/// (arXiv:2401.04151, January 2024). The paper demonstrates that sequential low-rank adaptations\n+/// can significantly improve task performance compared to single-stage LoRA, especially on\n+/// complex reasoning and multi-step tasks.\n+/// </para>\n+/// <para><b>Usage Example:</b>\n+/// <code>\n+/// // Create a chain with 3 sequential adaptations\n+/// var chain = new ChainLoRAAdapter&lt;double&gt;(baseLayer, rank: 8, chainLength: 3);\n+///\n+/// // Train first adapter on Task A\n+/// chain.SetActiveAdapterIndex(0);\n+/// TrainModel(chain, taskAData);\n+/// chain.MergeActiveAdapter(); // Consolidate Task A knowledge\n+///\n+/// // Train second adapter on Task B\n+/// chain.SetActiveAdapterIndex(1);\n+/// TrainModel(chain, taskBData);\n+/// chain.MergeActiveAdapter(); // Consolidate Task B knowledge\n+///\n+/// // Train third adapter on Task C\n+/// chain.SetActiveAdapterIndex(2);\n+/// TrainModel(chain, taskCData);\n+///\n+/// // Deploy: all adaptations are now part of the model\n+/// ILayer&lt;double&gt; finalLayer = chain.MergeToOriginalLayer();\n+/// </code>\n+/// </para>\n+/// </remarks>\n+public class ChainLoRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// The chain of LoRA adapters applied sequentially.\n+    /// </summary>\n+    private readonly List<LoRALayer<T>> _adapterChain;\n+\n+    /// <summary>\n+    /// The index of the currently active adapter being trained.\n+    /// </summary>\n+    private int _activeAdapterIndex;\n+\n+    /// <summary>\n+    /// Whether each adapter in the chain has been merged.\n+    /// </summary>\n+    private readonly List<bool> _mergedStatus;\n+\n+    /// <summary>\n+    /// The total length of the adapter chain.\n+    /// </summary>\n+    private readonly int _chainLength;\n+\n+    /// <summary>\n+    /// Gets the total number of adapters in the chain.\n+    /// </summary>\n+    /// <remarks>\n+    /// This represents the maximum number of sequential adaptation stages that can be applied.\n+    /// Each adapter can be trained independently and then merged before proceeding to the next.\n+    /// </remarks>\n+    public int ChainLength => _chainLength;\n+\n+    /// <summary>\n+    /// Gets the index of the currently active adapter (0-based).\n+    /// </summary>\n+    /// <remarks>\n+    /// The active adapter is the one currently being trained. Other adapters in the chain\n+    /// are either waiting to be trained (higher indices) or have been merged (lower indices).\n+    /// </remarks>\n+    public int ActiveAdapterIndex => _activeAdapterIndex;\n+\n+    /// <summary>\n+    /// Gets the list of LoRA adapters in the chain.\n+    /// </summary>\n+    /// <remarks>\n+    /// Each adapter in the chain represents one stage of sequential adaptation.\n+    /// Adapters are applied in order during forward passes.\n+    /// </remarks>\n+    public IReadOnlyList<LoRALayer<T>> AdapterChain => _adapterChain.AsReadOnly();\n+\n+    /// <summary>\n+    /// Gets the merged status of each adapter in the chain.\n+    /// </summary>\n+    /// <remarks>\n+    /// True indicates that an adapter has been merged into the base layer and should\n+    /// no longer contribute trainable parameters. Merged adapters still contribute\n+    /// to the forward pass until the entire chain is collapsed.\n+    /// </remarks>\n+    public IReadOnlyList<bool> MergedStatus => _mergedStatus.AsReadOnly();\n+\n+    /// <summary>\n+    /// Initializes a new Chain-of-LoRA adapter with the specified configuration.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with the LoRA chain.</param>\n+    /// <param name=\"rank\">The rank of each LoRA decomposition in the chain.</param>\n+    /// <param name=\"chainLength\">The number of sequential adapters in the chain (default: 3).</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor for each adapter (defaults to rank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training (default: true).</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when chainLength is less than 1.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// Creates a chain of LoRA adapters for sequential fine-tuning. Each adapter in the chain\n+    /// can be trained independently, merged into the model, and then the next adapter can be\n+    /// activated for further refinement.\n+    /// </para>\n+    /// <para><b>For Beginners:</b>\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to adapt (e.g., a dense or convolutional layer)\n+    /// - rank: How compressed each adapter is (lower = fewer parameters per stage)\n+    /// - chainLength: How many sequential adaptation stages you want (typical: 2-5)\n+    /// - alpha: Controls adaptation strength (usually equals rank)\n+    /// - freezeBaseLayer: Lock base weights to preserve pre-trained knowledge (recommended: true)\n+    ///\n+    /// Example: chainLength=3 means you can do three rounds of training and merging,\n+    /// allowing the model to incrementally improve on complex tasks.\n+    /// </para>\n+    /// </remarks>\n+    public ChainLoRAAdapter(\n+        ILayer<T> baseLayer,\n+        int rank,\n+        int chainLength = 3,\n+        double alpha = -1,\n+        bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        if (chainLength < 1)\n+        {\n+            throw new ArgumentException(\"Chain length must be at least 1\", nameof(chainLength));\n+        }\n+\n+        _chainLength = chainLength;\n+        _activeAdapterIndex = 0;\n+        _adapterChain = new List<LoRALayer<T>>(chainLength);\n+        _mergedStatus = new List<bool>(chainLength);\n+\n+        // Create the chain of LoRA adapters\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+\n+        for (int i = 0; i < chainLength; i++)\n+        {\n+            var adapter = new LoRALayer<T>(inputSize, outputSize, rank, alpha);\n+            _adapterChain.Add(adapter);\n+            _mergedStatus.Add(false);\n+        }\n+\n+        // Update parameter count to reflect all unmerged adapters\n+        UpdateParameterCount();\n+    }\n+\n+    /// <summary>\n+    /// Sets which adapter in the chain is currently active for training.\n+    /// </summary>\n+    /// <param name=\"index\">The 0-based index of the adapter to activate.</param>\n+    /// <exception cref=\"ArgumentOutOfRangeException\">Thrown when index is out of range.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// Only the active adapter receives gradient updates during training. Other adapters\n+    /// are either frozen (already merged) or inactive (waiting to be trained).\n+    /// </para>\n+    /// <para><b>For Beginners:</b>\n+    /// This is like choosing which stage of learning you're currently working on.\n+    /// Set to 0 for the first stage, 1 for the second, etc. Only that stage's adapter\n+    /// will be trained while the others remain frozen.\n+    /// </para>\n+    /// </remarks>\n+    public void SetActiveAdapterIndex(int index)\n+    {\n+        if (index < 0 || index >= _chainLength)\n+        {\n+            throw new ArgumentOutOfRangeException(nameof(index), $\"Index must be between 0 and {_chainLength - 1}\");\n+        }\n+\n+        _activeAdapterIndex = index;\n+    }\n+\n+    /// <summary>\n+    /// Merges the currently active adapter into the base layer representation.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This \"ties a knot\" in the chain by marking the active adapter as merged and frozen.\n+    /// The adapter's weights are conceptually incorporated into the model, allowing the\n+    /// next adapter in the chain to build upon this consolidated knowledge.\n+    /// </para>\n+    /// <para>\n+    /// Note: The actual weight merging into a single layer happens when MergeToOriginalLayer()\n+    /// is called. This method only marks the adapter as merged for training purposes.\n+    /// </para>\n+    /// <para><b>For Beginners:</b>\n+    /// After training an adapter stage, call this to \"lock it in\" before moving to the\n+    /// next stage. It's like saving your progress before starting the next level.\n+    /// </para>\n+    /// </remarks>\n+    public void MergeActiveAdapter()\n+    {\n+        if (_activeAdapterIndex < 0 || _activeAdapterIndex >= _chainLength)\n+        {\n+            throw new InvalidOperationException($\"Invalid active adapter index: {_activeAdapterIndex}\");\n+        }\n+\n+        _mergedStatus[_activeAdapterIndex] = true;\n+        UpdateParameterCount();\n+    }\n+\n+    /// <summary>\n+    /// Unmerges a previously merged adapter, making it trainable again.\n+    /// </summary>\n+    /// <param name=\"index\">The index of the adapter to unmerge.</param>\n+    /// <exception cref=\"ArgumentOutOfRangeException\">Thrown when index is out of range.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// This allows re-training a previously merged adapter if needed for iterative refinement.\n+    /// Useful for scenarios where you want to go back and adjust an earlier stage.\n+    /// </para>\n+    /// </remarks>\n+    public void UnmergeAdapter(int index)\n+    {\n+        if (index < 0 || index >= _chainLength)\n+        {\n+            throw new ArgumentOutOfRangeException(nameof(index), $\"Index must be between 0 and {_chainLength - 1}\");\n+        }\n+\n+        _mergedStatus[index] = false;\n+        UpdateParameterCount();\n+    }\n+\n+    /// <summary>\n+    /// Gets the number of adapters that have been merged.\n+    /// </summary>\n+    /// <returns>Count of merged adapters.</returns>\n+    public int GetMergedCount()\n+    {\n+        return _mergedStatus.Count(merged => merged);\n+    }\n+\n+    /// <summary>\n+    /// Gets the number of adapters that are still trainable (not merged).\n+    /// </summary>\n+    /// <returns>Count of unmerged adapters.</returns>\n+    public int GetTrainableAdapterCount()\n+    {\n+        return _mergedStatus.Count(merged => !merged);\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass through the base layer and all adapters in the chain.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Output with all adapter contributions summed.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The forward pass computes:\n+    /// output = base_layer(input) + adapter_0(input) + adapter_1(input) + ... + adapter_n(input)\n+    /// </para>\n+    /// <para>\n+    /// All adapters contribute to the output, regardless of merge status. Merged adapters\n+    /// are conceptually part of the model but still computed separately until final merging.\n+    /// </para>\n+    /// <para><b>For Beginners:</b>\n+    /// During inference or training, the input goes through the base layer and ALL adapters\n+    /// in the chain. Their outputs are added together to get the final result. This is how\n+    /// all the sequential adaptations combine to produce the improved output.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        // Forward through base layer\n+        Tensor<T> result = _baseLayer.Forward(input);\n+\n+        // Forward through each adapter in the chain and sum contributions\n+        foreach (var adapter in _adapterChain)\n+        {\n+            Tensor<T> adapterOutput = adapter.Forward(input);\n+\n+            // Add adapter contribution to result\n+            for (int i = 0; i < result.Length; i++)\n+            {\n+                result[i] = NumOps.Add(result[i], adapterOutput[i]);\n+            }\n+        }\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Performs the backward pass through all layers in the chain.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient flowing back from the next layer.</param>\n+    /// <returns>Gradient to pass to the previous layer.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Gradients flow through all adapters and the base layer. Only unmerged adapters\n+    /// and the base layer (if not frozen) receive parameter updates.\n+    /// </para>\n+    /// <para><b>For Beginners:</b>\n+    /// During learning, this figures out how to improve each adapter. Only the active,\n+    /// unmerged adapter gets updated - the others are frozen to preserve their knowledge.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        // Initialize input gradient accumulator\n+        Tensor<T> inputGrad = new Tensor<T>(GetInputShape());\n+\n+        // Backward through each adapter in the chain\n+        for (int i = 0; i < _adapterChain.Count; i++)\n+        {\n+            Tensor<T> adapterInputGrad = _adapterChain[i].Backward(outputGradient);\n+\n+            // Accumulate input gradients\n+            for (int j = 0; j < inputGrad.Length; j++)\n+            {\n+                inputGrad[j] = NumOps.Add(inputGrad[j], adapterInputGrad[j]);\n+            }\n+        }\n+\n+        // Backward through base layer if not frozen\n+        if (!_freezeBaseLayer)\n+        {\n+            Tensor<T> baseInputGrad = _baseLayer.Backward(outputGradient);\n+\n+            // Accumulate base layer gradients\n+            for (int j = 0; j < inputGrad.Length; j++)\n+            {\n+                inputGrad[j] = NumOps.Add(inputGrad[j], baseInputGrad[j]);\n+            }\n+        }","path":"src/NeuralNetworks/Layers/ChainLoRAAdapter.cs","commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","original_commit_id":"c11235933bf7c341c8b4a223cf6ac11b757d1480","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Do not drop the base-layer gradient when it is frozen.**\n\nFreezing should skip parameter *updates* only; we still need the base layer‚Äôs Jacobian to propagate gradients to earlier layers. Returning early here zeroes out that contribution, so upstream layers learn as if the base weights were disconnected. Always call `_baseLayer.Backward(outputGradient)` and simply guard the later parameter-update path.\n\n```diff\n-        if (!_freezeBaseLayer)\n-        {\n-            Tensor<T> baseInputGrad = _baseLayer.Backward(outputGradient);\n-\n-            // Accumulate base layer gradients\n-            for (int j = 0; j < inputGrad.Length; j++)\n-            {\n-                inputGrad[j] = NumOps.Add(inputGrad[j], baseInputGrad[j]);\n-            }\n-        }\n+        Tensor<T> baseInputGrad = _baseLayer.Backward(outputGradient);\n+        for (int j = 0; j < inputGrad.Length; j++)\n+        {\n+            inputGrad[j] = NumOps.Add(inputGrad[j], baseInputGrad[j]);\n+        }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        Tensor<T> baseInputGrad = _baseLayer.Backward(outputGradient);\n        for (int j = 0; j < inputGrad.Length; j++)\n        {\n            inputGrad[j] = NumOps.Add(inputGrad[j], baseInputGrad[j]);\n        }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/Layers/ChainLoRAAdapter.cs around lines 389 to 398, the\ncode currently skips calling _baseLayer.Backward when _freezeBaseLayer is true\nwhich drops the base-layer Jacobian and prevents gradient propagation; always\ncall Tensor<T> baseInputGrad = _baseLayer.Backward(outputGradient) and\naccumulate baseInputGrad into inputGrad regardless of _freezeBaseLayer, and only\nguard the parameter-update logic (i.e., skip applying updates to base layer\nweights when frozen) while keeping the backward accumulation intact.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:32:34Z","updated_at":"2025-11-02T02:32:40Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106189","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106189"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106189"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106189/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":389,"start_side":"RIGHT","line":null,"original_line":398,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":398,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106200","pull_request_review_id":3407993158,"id":2484106200,"node_id":"PRRC_kwDOKSXUF86UEHPY","diff_hunk":"@@ -0,0 +1,509 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.NeuralNetworks.Layers;\n+\n+/// <summary>\n+/// Delta-LoRA adapter that focuses on parameter-efficient delta updates with momentum.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// Delta-LoRA is a variant of LoRA that explicitly models the change (delta) in parameters\n+/// rather than the absolute values. This approach can achieve better convergence in certain\n+/// scenarios by focusing on the parameter update dynamics with momentum-based accumulation.\n+/// </para>\n+/// <para><b>For Beginners:</b> Think of Delta-LoRA as \"change-focused\" LoRA.\n+///\n+/// Regular LoRA learns: \"What should the weights be?\"\n+/// Delta-LoRA learns: \"How should the weights change?\"\n+///\n+/// This difference matters because:\n+/// 1. Changes (deltas) often have simpler patterns than absolute values\n+/// 2. Momentum helps smooth out noisy updates\n+/// 3. Can converge faster when the optimal adaptation is a smooth transformation\n+///\n+/// Key concepts:\n+/// - <b>Delta weights</b>: Accumulated changes to parameters (not the parameters themselves)\n+/// - <b>Delta scaling</b>: Controls how strongly deltas affect the output\n+/// - <b>Momentum</b>: Smooths updates by remembering previous changes\n+///\n+/// When Delta-LoRA works better than standard LoRA:\n+/// - Tasks requiring smooth, gradual adaptations\n+/// - Fine-tuning where the base model is already close to optimal\n+/// - Scenarios with noisy gradients that benefit from momentum\n+/// - Transfer learning where you want to preserve more of the original model's behavior\n+///\n+/// Example: If you're adapting a language model to a new domain, Delta-LoRA can\n+/// make smaller, more conservative changes that preserve the model's general knowledge\n+/// while adapting to domain-specific patterns.\n+/// </para>\n+/// </remarks>\n+public class DeltaLoRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Matrix storing the cumulative weight deltas (changes over time).\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This matrix accumulates the changes to the weights rather than storing absolute weight values.\n+    /// It has the same dimensions as the output of the LoRA layer (outputSize √ó inputSize).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is like a running total of all the adjustments made during training.\n+    /// Instead of \"what are the weights\", it tracks \"how much have they changed\".\n+    /// </para>\n+    /// </remarks>\n+    private Matrix<T> _deltaWeights;\n+\n+    /// <summary>\n+    /// Scaling factor applied to delta updates before adding to the output.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Controls the magnitude of the delta contribution. Lower values make smaller adjustments,\n+    /// higher values make larger adjustments. Typical range: 0.01 to 1.0.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is like a \"sensitivity\" knob. Higher values mean the\n+    /// accumulated changes have a stronger effect on the output.\n+    /// </para>\n+    /// </remarks>\n+    private readonly double _deltaScaling;\n+\n+    /// <summary>\n+    /// Momentum factor for delta accumulation (0 to 1).\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Controls how much previous delta updates influence new updates.\n+    /// - 0.0 = No momentum (each update is independent)\n+    /// - 0.9 = High momentum (updates are heavily influenced by history)\n+    /// Typical value: 0.9\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Momentum is like inertia in physics. It makes updates smoother\n+    /// by remembering the direction you were moving before. This helps avoid erratic changes and\n+    /// can speed up convergence.\n+    /// </para>\n+    /// </remarks>\n+    private readonly double _momentumFactor;\n+\n+    /// <summary>\n+    /// Velocity matrix for momentum-based updates.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Stores the moving average of gradients, used for momentum-based optimization.\n+    /// Has the same dimensions as _deltaWeights.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This tracks the \"speed and direction\" of parameter changes.\n+    /// When gradients point in consistent directions, velocity builds up, making updates faster.\n+    /// When gradients change direction, velocity slows down, preventing oscillation.\n+    /// </para>\n+    /// </remarks>\n+    private Matrix<T> _velocity;\n+\n+    /// <summary>\n+    /// Gradients for the delta weights computed during backpropagation.\n+    /// </summary>\n+    private Matrix<T>? _deltaGradients;\n+\n+    /// <summary>\n+    /// Stored input from the forward pass, needed for gradient computation.\n+    /// </summary>\n+    private Tensor<T>? _lastInput;\n+\n+    /// <summary>\n+    /// Gets the scaling factor for delta updates.\n+    /// </summary>\n+    public double DeltaScaling => _deltaScaling;\n+\n+    /// <summary>\n+    /// Gets the momentum factor for delta accumulation.\n+    /// </summary>\n+    public double MomentumFactor => _momentumFactor;\n+\n+    /// <summary>\n+    /// Initializes a new Delta-LoRA adapter wrapping an existing layer.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with Delta-LoRA.</param>\n+    /// <param name=\"rank\">The rank of the LoRA decomposition.</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"deltaScaling\">Scaling factor for delta updates (default: 0.1).</param>\n+    /// <param name=\"momentumFactor\">Momentum factor for delta accumulation (default: 0.9).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when deltaScaling or momentumFactor are out of valid range.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This creates a Delta-LoRA adapter with momentum-based updates.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to adapt\n+    /// - rank: Compression level (lower = fewer parameters)\n+    /// - alpha: LoRA strength\n+    /// - deltaScaling: How strongly deltas affect output (0.01 to 1.0, default 0.1)\n+    /// - momentumFactor: How much to smooth updates (0.0 to 1.0, default 0.9)\n+    /// - freezeBaseLayer: Whether to lock the original layer (usually true)\n+    ///\n+    /// Recommended settings:\n+    /// - For stable tasks: deltaScaling=0.1, momentumFactor=0.9\n+    /// - For aggressive adaptation: deltaScaling=0.5, momentumFactor=0.5\n+    /// - For conservative adaptation: deltaScaling=0.01, momentumFactor=0.95\n+    /// </para>\n+    /// </remarks>\n+    public DeltaLoRAAdapter(\n+        ILayer<T> baseLayer,\n+        int rank,\n+        double alpha = -1,\n+        double deltaScaling = 0.1,\n+        double momentumFactor = 0.9,\n+        bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        if (deltaScaling <= 0.0)\n+        {\n+            throw new ArgumentException(\"Delta scaling must be positive\", nameof(deltaScaling));\n+        }\n+\n+        if (momentumFactor < 0.0 || momentumFactor >= 1.0)\n+        {\n+            throw new ArgumentException(\"Momentum factor must be in range [0.0, 1.0)\", nameof(momentumFactor));\n+        }\n+\n+        _deltaScaling = deltaScaling;\n+        _momentumFactor = momentumFactor;\n+\n+        // Initialize delta weights and velocity matrices\n+        int outputSize = GetOutputShape()[0];\n+        int inputSize = GetInputShape()[0];\n+        _deltaWeights = new Matrix<T>(outputSize, inputSize);\n+        _velocity = new Matrix<T>(outputSize, inputSize);\n+\n+        // Initialize to zero\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                _deltaWeights[i, j] = NumOps.Zero;\n+                _velocity[i, j] = NumOps.Zero;\n+            }\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass: output = base_layer(input) + LoRA(input) + delta_weights @ input * delta_scaling.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Combined output from base layer, LoRA layer, and delta weights.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The forward pass computes three components:\n+    /// 1. Base layer output (original layer behavior)\n+    /// 2. LoRA output (low-rank adaptation)\n+    /// 3. Delta output (accumulated parameter changes scaled by deltaScaling)\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This combines three sources of information:\n+    /// - The original layer's predictions (base)\n+    /// - The LoRA adaptation (learned low-rank changes)\n+    /// - The accumulated deltas (momentum-smoothed changes)\n+    ///\n+    /// The delta component is what makes this different from standard LoRA - it explicitly\n+    /// applies the accumulated changes with scaling, allowing for more controlled adaptation.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        // Store input for backward pass\n+        _lastInput = input.Clone();\n+\n+        // Get base layer output\n+        Tensor<T> baseOutput = _baseLayer.Forward(input);\n+\n+        // Get LoRA layer output\n+        Tensor<T> loraOutput = _loraLayer.Forward(input);\n+\n+        // Compute delta contribution: delta_weights @ input * delta_scaling\n+        Tensor<T> deltaOutput = new Tensor<T>(baseOutput.Shape);\n+\n+        // For each output dimension\n+        for (int i = 0; i < _deltaWeights.Rows; i++)\n+        {\n+            T sum = NumOps.Zero;\n+            // Dot product with input\n+            for (int j = 0; j < _deltaWeights.Columns; j++)\n+            {\n+                sum = NumOps.Add(sum, NumOps.Multiply(_deltaWeights[i, j], input[j]));\n+            }\n+            // Apply delta scaling\n+            deltaOutput[i] = NumOps.Multiply(sum, NumOps.FromDouble(_deltaScaling));\n+        }\n+\n+        // Combine all three outputs\n+        Tensor<T> result = new Tensor<T>(baseOutput.Shape);\n+        for (int i = 0; i < baseOutput.Length; i++)\n+        {\n+            result[i] = NumOps.Add(NumOps.Add(baseOutput[i], loraOutput[i]), deltaOutput[i]);\n+        }\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Performs the backward pass, computing gradients for delta weights with momentum.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient flowing back from the next layer.</param>\n+    /// <returns>Gradient to pass to the previous layer.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The backward pass:\n+    /// 1. Propagates gradients through base and LoRA layers (from base class)\n+    /// 2. Computes gradients for delta weights\n+    /// 3. Updates velocity using momentum\n+    /// 4. Accumulates all input gradients\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This figures out how to improve all components:\n+    /// - The LoRA matrices (via the base class)\n+    /// - The delta weights (computed here)\n+    /// - Applies momentum to smooth out the delta updates\n+    ///\n+    /// Momentum helps by:\n+    /// - Accelerating convergence when gradients are consistent\n+    /// - Dampening oscillations when gradients are noisy\n+    /// - Creating smoother, more stable training dynamics\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (_lastInput == null)\n+        {\n+            throw new InvalidOperationException(\"Forward pass must be called before backward pass\");\n+        }\n+\n+        // Compute delta gradients: outputGradient ‚äó input (outer product)\n+        _deltaGradients = new Matrix<T>(_deltaWeights.Rows, _deltaWeights.Columns);\n+\n+        for (int i = 0; i < _deltaWeights.Rows; i++)\n+        {\n+            for (int j = 0; j < _deltaWeights.Columns; j++)\n+            {\n+                // Gradient for delta[i,j] = outputGradient[i] * input[j] * delta_scaling\n+                T grad = NumOps.Multiply(\n+                    NumOps.Multiply(outputGradient[i], _lastInput[j]),\n+                    NumOps.FromDouble(_deltaScaling)\n+                );\n+                _deltaGradients[i, j] = grad;\n+            }\n+        }\n+\n+        // Compute input gradient contribution from delta weights\n+        Tensor<T> deltaInputGrad = new Tensor<T>(_lastInput.Shape);\n+        for (int j = 0; j < _deltaWeights.Columns; j++)\n+        {\n+            T sum = NumOps.Zero;\n+            for (int i = 0; i < _deltaWeights.Rows; i++)\n+            {\n+                sum = NumOps.Add(sum, NumOps.Multiply(\n+                    _deltaWeights[i, j],\n+                    NumOps.Multiply(outputGradient[i], NumOps.FromDouble(_deltaScaling))\n+                ));\n+            }\n+            deltaInputGrad[j] = sum;\n+        }\n+\n+        // Backward through LoRA layer\n+        Tensor<T> loraInputGrad = _loraLayer.Backward(outputGradient);\n+\n+        // Backward through base layer\n+        Tensor<T> baseInputGrad = _baseLayer.Backward(outputGradient);\n+\n+        // Combine all input gradients\n+        Tensor<T> inputGrad = new Tensor<T>(loraInputGrad.Shape);\n+        for (int i = 0; i < loraInputGrad.Length; i++)\n+        {\n+            inputGrad[i] = NumOps.Add(\n+                NumOps.Add(loraInputGrad[i], baseInputGrad[i]),\n+                deltaInputGrad[i]\n+            );\n+        }\n+\n+        return inputGrad;\n+    }\n+\n+    /// <summary>\n+    /// Updates parameters using momentum-based delta updates.\n+    /// </summary>\n+    /// <param name=\"learningRate\">The learning rate for parameter updates.</param>\n+    /// <remarks>\n+    /// <para>\n+    /// The update process:\n+    /// 1. Update base and LoRA parameters (via base class)\n+    /// 2. Update velocity with momentum: velocity = momentum * velocity + (1 - momentum) * gradient\n+    /// 3. Update delta weights: delta_weights -= learning_rate * velocity\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is where the momentum magic happens!\n+    ///\n+    /// Without momentum:\n+    /// - Updates can be jerky and unstable\n+    /// - Training might oscillate around the optimum\n+    ///\n+    /// With momentum:\n+    /// - Velocity builds up in consistent gradient directions (speeds up convergence)\n+    /// - Velocity dampens in inconsistent directions (reduces oscillation)\n+    /// - Results in smoother, faster convergence\n+    ///\n+    /// Think of it like pushing a shopping cart: if you keep pushing in the same direction,\n+    /// it picks up speed (momentum). If you change direction, it slows down first.\n+    /// </para>\n+    /// </remarks>\n+    public override void UpdateParameters(T learningRate)\n+    {\n+        // Update base and LoRA parameters via base class\n+        base.UpdateParameters(learningRate);\n+\n+        // Update delta weights with momentum\n+        if (_deltaGradients != null)\n+        {\n+            T momentumT = NumOps.FromDouble(_momentumFactor);\n+            T oneMinusMomentumT = NumOps.FromDouble(1.0 - _momentumFactor);\n+\n+            for (int i = 0; i < _deltaWeights.Rows; i++)\n+            {\n+                for (int j = 0; j < _deltaWeights.Columns; j++)\n+                {\n+                    // Update velocity: v = momentum * v + (1 - momentum) * gradient\n+                    _velocity[i, j] = NumOps.Add(\n+                        NumOps.Multiply(momentumT, _velocity[i, j]),\n+                        NumOps.Multiply(oneMinusMomentumT, _deltaGradients[i, j])\n+                    );\n+\n+                    // Update delta weights: delta -= learning_rate * velocity\n+                    _deltaWeights[i, j] = NumOps.Subtract(\n+                        _deltaWeights[i, j],\n+                        NumOps.Multiply(learningRate, _velocity[i, j])\n+                    );\n+                }\n+            }\n+        }","path":"src/NeuralNetworks/Layers/DeltaLoRAAdapter.cs","commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","original_commit_id":"c11235933bf7c341c8b4a223cf6ac11b757d1480","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**Expose delta weights through the LayerBase parameter API**  \n`UpdateParameters` (Lines [355-383]) updates `_deltaWeights`, but the adapter never overrides `ParameterCount`, `GetParameters`, `SetParameters`, or gradient packing to include those deltas. As a result, callers still see only the LoRA/base parameters: serialization, checkpointing, optimizers that call `GetParameters()`/`SetParameters()`, and any tooling relying on `ParameterCount` will silently drop the delta state. Please expose the delta tensor via the standard parameter vector‚Äîoverride `ParameterCount`, pack/unpack delta weights (and their gradients) alongside the existing data, and keep `ParameterGradients` in sync‚Äîso the adapter behaves like the rest of the layer hierarchy.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/Layers/DeltaLoRAAdapter.cs around lines 355 to 383,\nUpdateParameters mutates _deltaWeights but the adapter never exposes those\ndeltas via the LayerBase parameter API; you must override ParameterCount,\nGetParameters, SetParameters (and corresponding gradient packing/unpacking) to\ninclude the delta tensor so serialization, checkpointing and optimizers see and\nrestore delta state. Implement overrides that call base.ParameterCount/Get/Set\nto obtain the base/LoRA parameter vector, then append/prepend the flattened\n_deltaWeights (and when present _deltaGradients) with clear offset arithmetic\nand null checks; ensure ParameterGradients reflects the same layout, maintain\nordering compatibility with existing layers, and keep UpdateParameters using the\nsame offsets so Save/Load and optimizer steps operate on the combined parameter\nvector.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:32:35Z","updated_at":"2025-11-02T02:32:40Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106200","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106200"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106200"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106200/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":371,"original_start_line":355,"start_side":"RIGHT","line":399,"original_line":383,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":383,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106205","pull_request_review_id":3407993158,"id":2484106205,"node_id":"PRRC_kwDOKSXUF86UEHPd","diff_hunk":"@@ -0,0 +1,767 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.NeuralNetworks.Layers;\n+\n+/// <summary>\n+/// DoRA (Weight-Decomposed Low-Rank Adaptation) adapter for parameter-efficient fine-tuning with improved stability.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// DoRA (Weight-Decomposed LoRA) extends standard LoRA by decomposing pre-trained weights into\n+/// magnitude and direction components, then applying LoRA only to the direction component.\n+/// This decomposition leads to more stable training and better convergence compared to standard LoRA.\n+/// </para>\n+/// <para>\n+/// <b>Mathematical Formulation:</b>\n+/// Given pre-trained weights W, DoRA decomposes them as:\n+/// - W = m * d, where m is magnitude (scalar per neuron) and d is direction (unit vector)\n+/// - W' = m * normalize(d + LoRA_delta)\n+/// - LoRA_delta = (alpha/rank) * B * A\n+///\n+/// This ensures that LoRA adaptations primarily affect the direction of weights, not their magnitude,\n+/// which improves training stability and convergence.\n+/// </para>\n+/// <para>\n+/// <b>Research Context:</b>\n+/// DoRA was published in February 2024 and presented as an ICML 2024 Oral paper.\n+/// In experiments on LLaMA-7B, DoRA achieved +3.7% improvement over standard LoRA.\n+/// The key insight is that separating magnitude and direction allows more stable gradient flow\n+/// and better control over the adaptation process.\n+/// </para>\n+/// <para>\n+/// <b>For Beginners:</b> DoRA is an improved version of LoRA that works better in practice.\n+///\n+/// Think of neural network weights as arrows:\n+/// - Each arrow has a length (magnitude) and a direction\n+/// - Standard LoRA adjusts both length and direction at the same time\n+/// - DoRA separates them: it keeps the length fixed and only adjusts the direction\n+/// - This makes training more stable and gives better results\n+///\n+/// Why this matters:\n+/// - More stable training (fewer divergences and NaN errors)\n+/// - Better final performance (+3.7% on LLaMA-7B)\n+/// - Same parameter efficiency as standard LoRA\n+/// - Slightly more computation (due to normalization), but worth it for the stability\n+///\n+/// When to use DoRA over standard LoRA:\n+/// - When training stability is important (large models, complex tasks)\n+/// - When you want the best possible fine-tuning results\n+/// - When you have the computational budget for normalization overhead\n+/// - When adapting very large pre-trained models (LLMs, large vision models)\n+/// </para>\n+/// <para>\n+/// <b>Reference:</b>\n+/// \"DoRA: Weight-Decomposed Low-Rank Adaptation\"\n+/// ICML 2024 Oral\n+/// https://arxiv.org/abs/2402.09353\n+/// </para>\n+/// </remarks>\n+public class DoRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Magnitude component of the decomposed weights (scalar per output neuron).\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// The magnitude vector stores the L2 norm of each weight vector (one per output neuron).\n+    /// During forward pass, this magnitude is applied after normalizing the direction vectors.\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This stores the \"strength\" of each output neuron.\n+    /// When we decompose weights into magnitude and direction, this is the magnitude part.\n+    /// Each output neuron gets one magnitude value.\n+    /// </para>\n+    /// </remarks>\n+    private Vector<T> _magnitude;\n+\n+    /// <summary>\n+    /// Gradients for the magnitude component, computed during backpropagation.\n+    /// </summary>\n+    private Vector<T>? _magnitudeGradient;\n+\n+    /// <summary>\n+    /// Cached normalized direction from the last forward pass, used in backpropagation.\n+    /// </summary>\n+    private Matrix<T>? _lastNormalizedDirection;\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// DoRA adds the magnitude parameters (one per output neuron) to the standard LoRA parameters.\n+    /// Total = (base layer parameters if not frozen) + LoRA parameters + magnitude parameters.\n+    /// </para>\n+    /// </remarks>\n+    public override int ParameterCount\n+    {\n+        get\n+        {\n+            int baseCount = _freezeBaseLayer ? 0 : _baseLayer.ParameterCount;\n+            int loraCount = _loraLayer.ParameterCount;\n+            int magnitudeCount = _magnitude.Length;\n+            return baseCount + loraCount + magnitudeCount;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Initializes a new DoRA adapter wrapping an existing layer.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with DoRA.</param>\n+    /// <param name=\"rank\">The rank of the LoRA decomposition.</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// The constructor initializes the DoRA adapter by:\n+    /// 1. Setting up the standard LoRA components (via base constructor)\n+    /// 2. Decomposing the base layer's initial weights into magnitude and direction\n+    /// 3. Initializing magnitude gradients\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This creates a DoRA adapter around your existing layer.\n+    ///\n+    /// What happens during initialization:\n+    /// - The base class sets up standard LoRA (matrices A and B)\n+    /// - We then decompose the layer's weights into magnitude and direction\n+    /// - The magnitude starts as the actual magnitudes from the original weights\n+    /// - During training, both the LoRA matrices and the magnitudes will be updated\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to fine-tune efficiently\n+    /// - rank: How much compression for LoRA (lower = fewer parameters)\n+    /// - alpha: Scaling factor for LoRA contribution\n+    /// - freezeBaseLayer: Usually true - we only train LoRA + magnitude, not base weights\n+    /// </para>\n+    /// </remarks>\n+    public DoRAAdapter(ILayer<T> baseLayer, int rank, double alpha = -1, bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        // Initialize magnitude from base layer weights\n+        int outputSize = GetOutputShape()[0];\n+        _magnitude = new Vector<T>(outputSize);\n+\n+        // Decompose initial weights to get magnitude\n+        DecomposeWeights();\n+\n+        // Update parameters to include magnitude\n+        Parameters = new Vector<T>(ParameterCount);\n+        UpdateParametersFromComponents();\n+    }\n+\n+    /// <summary>\n+    /// Decomposes the base layer's weights into magnitude and direction components.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// For each output neuron, this method:\n+    /// 1. Extracts the weight vector (all connections to that neuron)\n+    /// 2. Computes the L2 norm (magnitude)\n+    /// 3. Stores the magnitude\n+    ///\n+    /// The direction is implicitly W/||W|| and doesn't need to be stored separately.\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This splits weights into magnitude (length) and direction.\n+    ///\n+    /// Imagine each weight vector as an arrow:\n+    /// - Magnitude = how long the arrow is\n+    /// - Direction = which way the arrow points\n+    ///\n+    /// We store the magnitude separately so we can apply LoRA only to the direction.\n+    /// This is the key innovation of DoRA over standard LoRA.\n+    /// </para>\n+    /// </remarks>\n+    private void DecomposeWeights()\n+    {\n+        // Get base layer parameters\n+        Vector<T> baseParams = _baseLayer.GetParameters();\n+\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        int weightCount = inputSize * outputSize;\n+\n+        // For each output neuron, compute the magnitude of its weight vector\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            T sumSquares = NumOps.Zero;\n+\n+            // Sum squares of all weights for this output neuron\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                int idx = i * inputSize + j;\n+                if (idx < weightCount && idx < baseParams.Length)\n+                {\n+                    T weight = baseParams[idx];\n+                    sumSquares = NumOps.Add(sumSquares, NumOps.Multiply(weight, weight));\n+                }\n+            }\n+\n+            // Magnitude is the L2 norm\n+            _magnitude[i] = NumOps.Sqrt(sumSquares);\n+\n+            // Ensure magnitude is never zero (for numerical stability)\n+            if (NumOps.Equals(_magnitude[i], NumOps.Zero))\n+            {\n+                _magnitude[i] = NumOps.FromDouble(1e-8);\n+            }\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Recomposes weights from magnitude and direction components.\n+    /// </summary>\n+    /// <param name=\"direction\">The normalized direction matrix.</param>\n+    /// <returns>The full weight matrix (magnitude * direction).</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method reconstructs the full weight matrix by scaling each direction vector\n+    /// by its corresponding magnitude value.\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This puts magnitude and direction back together.\n+    ///\n+    /// After we've adjusted the direction with LoRA and have the magnitude stored separately,\n+    /// this combines them back into normal weights. Think of it as:\n+    /// - Take each direction vector (unit vector)\n+    /// - Scale it by its magnitude (scalar)\n+    /// - Result: the full weight vector\n+    ///\n+    /// This is used during forward pass to get the effective weights.\n+    /// </para>\n+    /// </remarks>\n+    private Matrix<T> RecomposeWeights(Matrix<T> direction)\n+    {\n+        int outputSize = direction.Rows;\n+        int inputSize = direction.Columns;\n+\n+        Matrix<T> weights = new Matrix<T>(outputSize, inputSize);\n+\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                weights[i, j] = NumOps.Multiply(_magnitude[i], direction[i, j]);\n+            }\n+        }\n+\n+        return weights;\n+    }\n+\n+    /// <summary>\n+    /// Normalizes a matrix row-wise (each row becomes a unit vector).\n+    /// </summary>\n+    /// <param name=\"matrix\">The matrix to normalize.</param>\n+    /// <returns>Row-normalized matrix where each row has unit L2 norm.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// For each row (weight vector), this computes the L2 norm and divides all elements by it.\n+    /// This ensures each direction vector has unit length.\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This makes each weight vector have length 1.\n+    ///\n+    /// When we separate magnitude and direction, the direction must be a unit vector\n+    /// (length = 1). This method ensures that by dividing each weight vector by its length.\n+    ///\n+    /// Example: vector [3, 4] has length 5, so normalized it becomes [0.6, 0.8]\n+    /// </para>\n+    /// </remarks>\n+    private Matrix<T> NormalizeRows(Matrix<T> matrix)\n+    {\n+        int rows = matrix.Rows;\n+        int cols = matrix.Columns;\n+\n+        Matrix<T> normalized = new Matrix<T>(rows, cols);\n+\n+        for (int i = 0; i < rows; i++)\n+        {\n+            // Compute L2 norm of row\n+            T sumSquares = NumOps.Zero;\n+            for (int j = 0; j < cols; j++)\n+            {\n+                T val = matrix[i, j];\n+                sumSquares = NumOps.Add(sumSquares, NumOps.Multiply(val, val));\n+            }\n+\n+            T norm = NumOps.Sqrt(sumSquares);\n+\n+            // Avoid division by zero\n+            if (NumOps.Equals(norm, NumOps.Zero))\n+            {\n+                norm = NumOps.FromDouble(1e-8);\n+            }\n+\n+            // Normalize row\n+            for (int j = 0; j < cols; j++)\n+            {\n+                normalized[i, j] = NumOps.Divide(matrix[i, j], norm);\n+            }\n+        }\n+\n+        return normalized;\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass through DoRA adapter.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Output combining base layer with DoRA-adapted weights.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The DoRA forward pass:\n+    /// 1. Gets base layer weights W\n+    /// 2. Computes direction: d = W / ||W||\n+    /// 3. Applies LoRA to direction: d' = d + LoRA(input)\n+    /// 4. Normalizes adapted direction: d_norm = d' / ||d'||\n+    /// 5. Recomposes weights: W' = m * d_norm\n+    /// 6. Computes output: y = input @ W'^T\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This is where DoRA's magic happens during prediction.\n+    ///\n+    /// Step by step:\n+    /// 1. Get the original weights from the base layer\n+    /// 2. Split into magnitude (stored) and direction (computed)\n+    /// 3. Apply LoRA's correction to the direction (not the magnitude!)\n+    /// 4. Normalize the new direction to keep it as a unit vector\n+    /// 5. Multiply magnitude back in to get final weights\n+    /// 6. Use these adjusted weights to compute the output\n+    ///\n+    /// The key difference from standard LoRA:\n+    /// - Standard LoRA: output = base_output + lora_output\n+    /// - DoRA: output = input @ (m * normalize(d + lora_output))\n+    ///\n+    /// DoRA's approach gives more stable training because we control magnitude separately.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        // Get base layer parameters and extract weights\n+        Vector<T> baseParams = _baseLayer.GetParameters();\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        int weightCount = inputSize * outputSize;\n+\n+        // Extract weight matrix from base layer (assuming weights come first)\n+        Matrix<T> baseWeights = new Matrix<T>(outputSize, inputSize);\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                int weightIdx = i * inputSize + j;\n+                if (weightIdx < weightCount && weightIdx < baseParams.Length)\n+                {\n+                    baseWeights[i, j] = baseParams[weightIdx];\n+                }\n+                else\n+                {\n+                    baseWeights[i, j] = NumOps.Zero;\n+                }\n+            }\n+        }\n+\n+        // Compute base direction (W / ||W||)\n+        Matrix<T> baseDirection = NormalizeRows(baseWeights);\n+\n+        // Get LoRA contribution (this is already scaled by alpha/rank)\n+        Tensor<T> loraOutput = _loraLayer.Forward(input);\n+\n+        // Convert LoRA output to matrix form (batch_size x output_size)\n+        int batchSize = input.Shape[0];\n+        Matrix<T> loraMatrix = new Matrix<T>(batchSize, outputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                loraMatrix[i, j] = loraOutput[i * outputSize + j];\n+            }\n+        }\n+\n+        // For DoRA, we need to add LoRA to the direction component, not the output\n+        // This requires reconstructing how LoRA affects the weight matrix\n+        // LoRA computes: input @ A @ B, which is equivalent to input @ (A @ B)^T\n+        // We need (A @ B)^T to add to the direction\n+        Matrix<T> loraWeightDelta = _loraLayer.MergeWeights(); // This gives us [outputSize, inputSize]\n+\n+        // Add LoRA delta to base direction: d' = d + delta\n+        Matrix<T> adaptedDirection = new Matrix<T>(outputSize, inputSize);\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                adaptedDirection[i, j] = NumOps.Add(baseDirection[i, j], loraWeightDelta[i, j]);\n+            }\n+        }\n+\n+        // Normalize the adapted direction: d_norm = d' / ||d'||\n+        _lastNormalizedDirection = NormalizeRows(adaptedDirection);\n+\n+        // Recompose weights: W' = m * d_norm\n+        Matrix<T> finalWeights = RecomposeWeights(_lastNormalizedDirection);\n+\n+        // Compute output: y = input @ W'^T\n+        // Convert input to matrix\n+        Matrix<T> inputMatrix = new Matrix<T>(batchSize, inputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                inputMatrix[i, j] = input[i * inputSize + j];\n+            }\n+        }\n+\n+        // Matrix multiply: [batchSize, inputSize] @ [inputSize, outputSize]\n+        Matrix<T> outputMatrix = inputMatrix.Multiply(finalWeights.Transpose());\n+\n+        // Convert back to tensor\n+        Vector<T> outputData = new Vector<T>(batchSize * outputSize);\n+        int idx = 0;\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                outputData[idx++] = outputMatrix[i, j];\n+            }\n+        }\n+\n+        return new Tensor<T>(new[] { batchSize, outputSize }, outputData);\n+    }\n+\n+    /// <summary>\n+    /// Performs the backward pass through DoRA adapter.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient flowing back from the next layer.</param>\n+    /// <returns>Gradient to pass to the previous layer.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The backward pass computes gradients for:\n+    /// 1. Magnitude parameters (one per output neuron)\n+    /// 2. LoRA matrices A and B (via LoRA layer's backward)\n+    /// 3. Base layer weights (if not frozen)\n+    ///\n+    /// The key challenge is computing how changes to magnitude and direction affect the loss,\n+    /// given that the direction is normalized during forward pass.\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This is where DoRA learns during training.\n+    ///\n+    /// Backward pass figures out how to improve three things:\n+    /// 1. The magnitude of each output neuron's weights\n+    /// 2. The LoRA matrices that adjust the direction\n+    /// 3. The base layer weights (if we're training them too)\n+    ///\n+    /// The math is complex because we need to account for the normalization step.\n+    /// When we normalize the direction, it creates a dependency between all elements\n+    /// of a weight vector, so the gradients need to account for that.\n+    ///\n+    /// For simplicity, this implementation computes approximate gradients that work well\n+    /// in practice. The exact gradients would require storing more intermediate values\n+    /// from the forward pass.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (_lastNormalizedDirection == null)\n+        {\n+            throw new InvalidOperationException(\"Forward pass must be called before backward pass\");\n+        }\n+\n+        int batchSize = outputGradient.Shape[0];\n+        int outputSize = GetOutputShape()[0];\n+        int inputSize = GetInputShape()[0];\n+\n+        // Convert output gradient to matrix\n+        Matrix<T> gradMatrix = new Matrix<T>(batchSize, outputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                gradMatrix[i, j] = outputGradient[i * outputSize + j];\n+            }\n+        }\n+\n+        // Compute magnitude gradients\n+        // dL/dm_i = sum over batch of (outputGrad_i * normalizedDirection_i)\n+        _magnitudeGradient = new Vector<T>(_magnitude.Length);\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            T gradSum = NumOps.Zero;\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                T grad = gradMatrix[b, i];\n+                // Gradient contribution from this output\n+                // Each output is computed as: output_i = m_i * (normalized_direction_i ¬∑ input)\n+                // We need the input, but we can approximate the magnitude gradient\n+                gradSum = NumOps.Add(gradSum, grad);\n+            }\n+            _magnitudeGradient[i] = gradSum;\n+        }\n+\n+        // Propagate gradient through LoRA layer\n+        // The LoRA layer's backward will compute gradients for A and B matrices\n+        Tensor<T> loraInputGrad = _loraLayer.Backward(outputGradient);\n+\n+        // If base layer is not frozen, propagate through it too\n+        Tensor<T> baseInputGrad;\n+        if (!_freezeBaseLayer)\n+        {\n+            baseInputGrad = _baseLayer.Backward(outputGradient);\n+\n+            // Sum input gradients from both paths\n+            Vector<T> inputGradData = new Vector<T>(loraInputGrad.Length);\n+            for (int i = 0; i < loraInputGrad.Length; i++)\n+            {\n+                inputGradData[i] = NumOps.Add(loraInputGrad[i], baseInputGrad[i]);\n+            }\n+\n+            return new Tensor<T>(loraInputGrad.Shape, inputGradData);\n+        }\n+        else\n+        {\n+            // Only LoRA input gradient\n+            return loraInputGrad;\n+        }","path":"src/NeuralNetworks/Layers/DoRAAdapter.cs","commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","original_commit_id":"c11235933bf7c341c8b4a223cf6ac11b757d1480","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**LoRA backward path ignores DoRA‚Äôs weight reconstruction.**\n\nThe forward pass recomposes weights via `W' = RecomposeWeights(normalizedDirection)` and never adds the LoRA output directly. Yet `Backward` calls `_loraLayer.Backward(outputGradient)`, which assumes the adapter‚Äôs forward contribution was `loraOutput` added to the base result. That gradient does not match DoRA‚Äôs definition, where the low-rank update affects the normalized direction and must be differentiated through the normalization step.([emergentmind.com](https://www.emergentmind.com/papers/2402.09353?utm_source=openai))\n\nPlease differentiate the merged weight:\n1. Recompute `‚àáW'` from `outputGradient`.\n2. Project it through the normalization and magnitude decomposition to obtain gradients for the LoRA factors (`A`, `B`) and `m`, as described in the DoRA paper.\n3. Drop the `_loraLayer.Backward` shortcut.\n\nUntil this is fixed, DoRA fine-tuning will update the wrong parameters.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/Layers/DoRAAdapter.cs around lines 503 to 526, the\nbackward currently calls _loraLayer.Backward(outputGradient) which is incorrect\nfor DoRA because the forward recomposes weights via W' =\nRecomposeWeights(normalizedDirection) and the gradient must be propagated\nthrough the normalization and magnitude decomposition; replace the shortcut by\ncomputing the gradient w.r.t. the merged weight W' from outputGradient, then\ndifferentiate W' through the normalization and magnitude decomposition steps\nused in RecomposeWeights to obtain gradients for the low-rank factors A and B\nand the scalar magnitude m (i.e., compute ‚àÇL/‚àÇW' ‚Üí project to\n‚àÇL/‚àÇnormalizedDirection and ‚àÇL/‚àÇm, then backpropagate through the low-rank\nreconstruction to get ‚àÇL/‚àÇA and ‚àÇL/‚àÇB), apply those parameter gradients to the\nLoRA factors and magnitude, and remove the call to _loraLayer.Backward; ensure\nshapes and broadcasting match and preserve frozen-base behavior (only skip base\nlayer backprop when _freezeBaseLayer is true).\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:32:35Z","updated_at":"2025-11-02T02:32:40Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106205","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106205"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106205"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106205/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":503,"original_start_line":503,"start_side":"RIGHT","line":526,"original_line":526,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":526,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106215","pull_request_review_id":3407993158,"id":2484106215,"node_id":"PRRC_kwDOKSXUF86UEHPn","diff_hunk":"@@ -0,0 +1,1116 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.NeuralNetworks.Layers;\n+\n+/// <summary>\n+/// DVoRA (DoRA + VeRA) adapter - combines DoRA's magnitude-direction decomposition with VeRA's extreme parameter efficiency.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// DVoRA achieves the best of both worlds by:\n+/// - Applying DoRA's magnitude-direction decomposition for training stability\n+/// - Using VeRA's shared frozen matrices and scaling vectors for extreme parameter efficiency\n+/// - Applying the VeRA adaptation only to the direction component (not the magnitude)\n+/// </para>\n+/// <para>\n+/// <b>Mathematical Formulation:</b>\n+/// Given pre-trained weights W, DVoRA:\n+/// 1. Decomposes: W = m * d (magnitude and direction)\n+/// 2. Applies VeRA to direction: d' = d + d_scale * (B * A * input) * b_scale\n+/// 3. Normalizes direction: d_norm = d' / ||d'||\n+/// 4. Recomposes: W' = m * d_norm\n+///\n+/// Where:\n+/// - m: magnitude vector (trainable)\n+/// - d: direction matrix (normalized weight vectors)\n+/// - A, B: shared frozen random matrices (VeRA style)\n+/// - d_scale, b_scale: per-layer trainable scaling vectors (VeRA style)\n+/// </para>\n+/// <para>\n+/// <b>Research Context:</b>\n+/// DVoRA scores 5.0 vs VeRA's 4.3 (improvement of 16%) while maintaining ultra-low parameter counts.\n+/// It combines DoRA's superior training stability with VeRA's extreme parameter efficiency.\n+/// </para>\n+/// <para>\n+/// <b>For Beginners:</b> DVoRA is the ultimate parameter-efficient adapter.\n+///\n+/// Think of it as a hybrid technique:\n+/// - From DoRA: Separate magnitude (strength) from direction for stability\n+/// - From VeRA: Use shared random matrices and tiny scaling vectors for efficiency\n+/// - The magic: Apply VeRA's adaptation only to the direction, not the magnitude\n+///\n+/// Parameter comparison for 1000x1000 layer with rank=8:\n+/// - Full fine-tuning: 1,000,000 parameters\n+/// - Standard LoRA: 16,000 parameters (98.4% reduction)\n+/// - DoRA: 17,000 parameters (LoRA + magnitude vector)\n+/// - VeRA: 1,600 parameters (99.84% reduction)\n+/// - DVoRA: ~1,600 parameters (same as VeRA!) but with better performance (5.0 vs 4.3)\n+///\n+/// Benefits:\n+/// - ‚úÖ Extremely parameter-efficient (10x fewer than standard LoRA, same as VeRA)\n+/// - ‚úÖ Better performance than VeRA alone (5.0 vs 4.3 score)\n+/// - ‚úÖ Training stability from DoRA's magnitude-direction decomposition\n+/// - ‚úÖ Shared matrices reduce storage when adapting many layers\n+/// - ‚úÖ Best choice for extreme memory constraints with quality requirements\n+///\n+/// Trade-offs:\n+/// - ‚ö†Ô∏è Requires shared matrix initialization before use\n+/// - ‚ö†Ô∏è Slightly more computation than VeRA (due to normalization)\n+/// - ‚ö†Ô∏è More complex than standard adapters (combines two techniques)\n+///\n+/// When to use DVoRA:\n+/// - Extreme memory constraints but need better quality than VeRA\n+/// - Mobile/edge deployment with limited resources\n+/// - Fine-tuning many layers efficiently\n+/// - When you want the absolute best parameter efficiency + quality balance\n+/// </para>\n+/// <para>\n+/// <b>References:</b>\n+/// - DoRA: \"Weight-Decomposed Low-Rank Adaptation\" (ICML 2024 Oral)\n+/// - VeRA: \"Vector-based Random Matrix Adaptation\"\n+/// - DVoRA: Combines both techniques for optimal efficiency and performance\n+/// </para>\n+/// </remarks>\n+public class DVoRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Shared frozen random matrix A (inputSize √ó rank) used by all DVoRA adapters.\n+    /// </summary>\n+    /// <remarks>\n+    /// This matrix is initialized once globally and shared across all DVoRA layers.\n+    /// It is NEVER trained - it remains frozen at its random initialization values.\n+    /// This is the VeRA component of DVoRA.\n+    /// </remarks>\n+    private static Matrix<T>? _sharedMatrixA;\n+\n+    /// <summary>\n+    /// Shared frozen random matrix B (rank √ó outputSize) used by all DVoRA adapters.\n+    /// </summary>\n+    /// <remarks>\n+    /// This matrix is initialized once globally and shared across all DVoRA layers.\n+    /// It is NEVER trained - it remains frozen at its random initialization values.\n+    /// This is the VeRA component of DVoRA.\n+    /// </remarks>\n+    private static Matrix<T>? _sharedMatrixB;\n+\n+    /// <summary>\n+    /// Lock object for thread-safe shared matrix initialization.\n+    /// </summary>\n+    private static readonly object _initLock = new object();\n+\n+    /// <summary>\n+    /// Magnitude component of the decomposed weights (scalar per output neuron).\n+    /// Trainable per-layer parameter.\n+    /// </summary>\n+    /// <remarks>\n+    /// The magnitude vector stores the L2 norm of each weight vector (one per output neuron).\n+    /// This is the DoRA component of DVoRA.\n+    /// </remarks>\n+    private Vector<T> _magnitude;\n+\n+    /// <summary>\n+    /// Scaling vector d (outputSize) - trainable per-layer parameter.\n+    /// </summary>\n+    /// <remarks>\n+    /// This vector scales the VeRA output on a per-dimension basis.\n+    /// This is the VeRA component of DVoRA.\n+    /// </remarks>\n+    private Vector<T> _scalingVectorD;\n+\n+    /// <summary>\n+    /// Scaling vector b (rank) - trainable per-layer parameter.\n+    /// </summary>\n+    /// <remarks>\n+    /// This vector scales the intermediate rank-dimensional representation.\n+    /// This is the VeRA component of DVoRA.\n+    /// </remarks>\n+    private Vector<T> _scalingVectorB;\n+\n+    /// <summary>\n+    /// Gradient for magnitude vector computed during backpropagation.\n+    /// </summary>\n+    private Vector<T>? _magnitudeGradient;\n+\n+    /// <summary>\n+    /// Gradient for scaling vector d computed during backpropagation.\n+    /// </summary>\n+    private Vector<T>? _scalingVectorDGradient;\n+\n+    /// <summary>\n+    /// Gradient for scaling vector b computed during backpropagation.\n+    /// </summary>\n+    private Vector<T>? _scalingVectorBGradient;\n+\n+    /// <summary>\n+    /// Cached normalized direction from the last forward pass, used in backpropagation.\n+    /// </summary>\n+    private Matrix<T>? _lastNormalizedDirection;\n+\n+    /// <summary>\n+    /// Stored input from the forward pass, needed for gradient computation.\n+    /// </summary>\n+    private Tensor<T>? _lastInput;\n+\n+    /// <summary>\n+    /// Stored intermediate value from forward pass, needed for backward pass.\n+    /// </summary>\n+    private Matrix<T>? _lastIntermediate;\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters.\n+    /// </summary>\n+    /// <remarks>\n+    /// DVoRA parameters = magnitude (outputSize) + d_scale (outputSize) + b_scale (rank).\n+    /// This is only slightly more than VeRA (adds magnitude vector) but much fewer than DoRA (no full LoRA matrices).\n+    /// </remarks>\n+    public override int ParameterCount\n+    {\n+        get\n+        {\n+            int dvoraParams = _magnitude.Length + _scalingVectorD.Length + _scalingVectorB.Length;\n+            return _freezeBaseLayer ? dvoraParams : (_baseLayer.ParameterCount + dvoraParams);\n+        }\n+    }","path":"src/NeuralNetworks/Layers/DVoRAAdapter.cs","commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","original_commit_id":"c11235933bf7c341c8b4a223cf6ac11b757d1480","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Guard `ParameterCount` against uninitialised fields.**\n\n`LoRAAdapterBase` reads `ParameterCount` while its ctor is still running. At that point `_magnitude`, `_scalingVectorD`, and `_scalingVectorB` are null, so the getter throws. Fall back to `base.ParameterCount` until those vectors have been created.\n\n```diff\n-            int dvoraParams = _magnitude.Length + _scalingVectorD.Length + _scalingVectorB.Length;\n+            if (_magnitude == null || _scalingVectorD == null || _scalingVectorB == null)\n+            {\n+                return base.ParameterCount;\n+            }\n+\n+            int dvoraParams = _magnitude.Length + _scalingVectorD.Length + _scalingVectorB.Length;\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    public override int ParameterCount\n    {\n        get\n        {\n            if (_magnitude == null || _scalingVectorD == null || _scalingVectorB == null)\n            {\n                return base.ParameterCount;\n            }\n\n            int dvoraParams = _magnitude.Length + _scalingVectorD.Length + _scalingVectorB.Length;\n            return _freezeBaseLayer ? dvoraParams : (_baseLayer.ParameterCount + dvoraParams);\n        }\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/Layers/DVoRAAdapter.cs around lines 168-175, guard the\nParameterCount getter against uninitialized fields by first checking if\n_magnitude, _scalingVectorD, or _scalingVectorB are null and, if any are null,\nreturn base.ParameterCount immediately; otherwise compute dvoraParams and return\n_freezeBaseLayer ? dvoraParams : (_baseLayer.ParameterCount + dvoraParams). This\nensures callers during construction get a safe fallback to base.ParameterCount\nuntil the adapter vectors are created.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:32:35Z","updated_at":"2025-11-02T02:32:40Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106215","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106215"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106215"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106215/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":168,"original_start_line":168,"start_side":"RIGHT","line":175,"original_line":175,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":175,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106230","pull_request_review_id":3407993158,"id":2484106230,"node_id":"PRRC_kwDOKSXUF86UEHP2","diff_hunk":"@@ -0,0 +1,1116 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.NeuralNetworks.Layers;\n+\n+/// <summary>\n+/// DVoRA (DoRA + VeRA) adapter - combines DoRA's magnitude-direction decomposition with VeRA's extreme parameter efficiency.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// DVoRA achieves the best of both worlds by:\n+/// - Applying DoRA's magnitude-direction decomposition for training stability\n+/// - Using VeRA's shared frozen matrices and scaling vectors for extreme parameter efficiency\n+/// - Applying the VeRA adaptation only to the direction component (not the magnitude)\n+/// </para>\n+/// <para>\n+/// <b>Mathematical Formulation:</b>\n+/// Given pre-trained weights W, DVoRA:\n+/// 1. Decomposes: W = m * d (magnitude and direction)\n+/// 2. Applies VeRA to direction: d' = d + d_scale * (B * A * input) * b_scale\n+/// 3. Normalizes direction: d_norm = d' / ||d'||\n+/// 4. Recomposes: W' = m * d_norm\n+///\n+/// Where:\n+/// - m: magnitude vector (trainable)\n+/// - d: direction matrix (normalized weight vectors)\n+/// - A, B: shared frozen random matrices (VeRA style)\n+/// - d_scale, b_scale: per-layer trainable scaling vectors (VeRA style)\n+/// </para>\n+/// <para>\n+/// <b>Research Context:</b>\n+/// DVoRA scores 5.0 vs VeRA's 4.3 (improvement of 16%) while maintaining ultra-low parameter counts.\n+/// It combines DoRA's superior training stability with VeRA's extreme parameter efficiency.\n+/// </para>\n+/// <para>\n+/// <b>For Beginners:</b> DVoRA is the ultimate parameter-efficient adapter.\n+///\n+/// Think of it as a hybrid technique:\n+/// - From DoRA: Separate magnitude (strength) from direction for stability\n+/// - From VeRA: Use shared random matrices and tiny scaling vectors for efficiency\n+/// - The magic: Apply VeRA's adaptation only to the direction, not the magnitude\n+///\n+/// Parameter comparison for 1000x1000 layer with rank=8:\n+/// - Full fine-tuning: 1,000,000 parameters\n+/// - Standard LoRA: 16,000 parameters (98.4% reduction)\n+/// - DoRA: 17,000 parameters (LoRA + magnitude vector)\n+/// - VeRA: 1,600 parameters (99.84% reduction)\n+/// - DVoRA: ~1,600 parameters (same as VeRA!) but with better performance (5.0 vs 4.3)\n+///\n+/// Benefits:\n+/// - ‚úÖ Extremely parameter-efficient (10x fewer than standard LoRA, same as VeRA)\n+/// - ‚úÖ Better performance than VeRA alone (5.0 vs 4.3 score)\n+/// - ‚úÖ Training stability from DoRA's magnitude-direction decomposition\n+/// - ‚úÖ Shared matrices reduce storage when adapting many layers\n+/// - ‚úÖ Best choice for extreme memory constraints with quality requirements\n+///\n+/// Trade-offs:\n+/// - ‚ö†Ô∏è Requires shared matrix initialization before use\n+/// - ‚ö†Ô∏è Slightly more computation than VeRA (due to normalization)\n+/// - ‚ö†Ô∏è More complex than standard adapters (combines two techniques)\n+///\n+/// When to use DVoRA:\n+/// - Extreme memory constraints but need better quality than VeRA\n+/// - Mobile/edge deployment with limited resources\n+/// - Fine-tuning many layers efficiently\n+/// - When you want the absolute best parameter efficiency + quality balance\n+/// </para>\n+/// <para>\n+/// <b>References:</b>\n+/// - DoRA: \"Weight-Decomposed Low-Rank Adaptation\" (ICML 2024 Oral)\n+/// - VeRA: \"Vector-based Random Matrix Adaptation\"\n+/// - DVoRA: Combines both techniques for optimal efficiency and performance\n+/// </para>\n+/// </remarks>\n+public class DVoRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Shared frozen random matrix A (inputSize √ó rank) used by all DVoRA adapters.\n+    /// </summary>\n+    /// <remarks>\n+    /// This matrix is initialized once globally and shared across all DVoRA layers.\n+    /// It is NEVER trained - it remains frozen at its random initialization values.\n+    /// This is the VeRA component of DVoRA.\n+    /// </remarks>\n+    private static Matrix<T>? _sharedMatrixA;\n+\n+    /// <summary>\n+    /// Shared frozen random matrix B (rank √ó outputSize) used by all DVoRA adapters.\n+    /// </summary>\n+    /// <remarks>\n+    /// This matrix is initialized once globally and shared across all DVoRA layers.\n+    /// It is NEVER trained - it remains frozen at its random initialization values.\n+    /// This is the VeRA component of DVoRA.\n+    /// </remarks>\n+    private static Matrix<T>? _sharedMatrixB;\n+\n+    /// <summary>\n+    /// Lock object for thread-safe shared matrix initialization.\n+    /// </summary>\n+    private static readonly object _initLock = new object();\n+\n+    /// <summary>\n+    /// Magnitude component of the decomposed weights (scalar per output neuron).\n+    /// Trainable per-layer parameter.\n+    /// </summary>\n+    /// <remarks>\n+    /// The magnitude vector stores the L2 norm of each weight vector (one per output neuron).\n+    /// This is the DoRA component of DVoRA.\n+    /// </remarks>\n+    private Vector<T> _magnitude;\n+\n+    /// <summary>\n+    /// Scaling vector d (outputSize) - trainable per-layer parameter.\n+    /// </summary>\n+    /// <remarks>\n+    /// This vector scales the VeRA output on a per-dimension basis.\n+    /// This is the VeRA component of DVoRA.\n+    /// </remarks>\n+    private Vector<T> _scalingVectorD;\n+\n+    /// <summary>\n+    /// Scaling vector b (rank) - trainable per-layer parameter.\n+    /// </summary>\n+    /// <remarks>\n+    /// This vector scales the intermediate rank-dimensional representation.\n+    /// This is the VeRA component of DVoRA.\n+    /// </remarks>\n+    private Vector<T> _scalingVectorB;\n+\n+    /// <summary>\n+    /// Gradient for magnitude vector computed during backpropagation.\n+    /// </summary>\n+    private Vector<T>? _magnitudeGradient;\n+\n+    /// <summary>\n+    /// Gradient for scaling vector d computed during backpropagation.\n+    /// </summary>\n+    private Vector<T>? _scalingVectorDGradient;\n+\n+    /// <summary>\n+    /// Gradient for scaling vector b computed during backpropagation.\n+    /// </summary>\n+    private Vector<T>? _scalingVectorBGradient;\n+\n+    /// <summary>\n+    /// Cached normalized direction from the last forward pass, used in backpropagation.\n+    /// </summary>\n+    private Matrix<T>? _lastNormalizedDirection;\n+\n+    /// <summary>\n+    /// Stored input from the forward pass, needed for gradient computation.\n+    /// </summary>\n+    private Tensor<T>? _lastInput;\n+\n+    /// <summary>\n+    /// Stored intermediate value from forward pass, needed for backward pass.\n+    /// </summary>\n+    private Matrix<T>? _lastIntermediate;\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters.\n+    /// </summary>\n+    /// <remarks>\n+    /// DVoRA parameters = magnitude (outputSize) + d_scale (outputSize) + b_scale (rank).\n+    /// This is only slightly more than VeRA (adds magnitude vector) but much fewer than DoRA (no full LoRA matrices).\n+    /// </remarks>\n+    public override int ParameterCount\n+    {\n+        get\n+        {\n+            int dvoraParams = _magnitude.Length + _scalingVectorD.Length + _scalingVectorB.Length;\n+            return _freezeBaseLayer ? dvoraParams : (_baseLayer.ParameterCount + dvoraParams);\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Initializes a new DVoRA adapter wrapping an existing layer.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with DVoRA.</param>\n+    /// <param name=\"rank\">The rank of the low-rank decomposition (shared across all DVoRA layers).</param>\n+    /// <param name=\"alpha\">The scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"InvalidOperationException\">Thrown when shared matrices are not initialized.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// Before creating any DVoRA adapters, you must call InitializeSharedMatrices() once to set up\n+    /// the shared random matrices that all DVoRA layers will use.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This creates a DVoRA adapter for a layer. Unlike standard LoRA,\n+    /// you must initialize the shared random matrices first by calling:\n+    ///\n+    /// DVoRAAdapter&lt;T&gt;.InitializeSharedMatrices(inputSize, outputSize, rank);\n+    ///\n+    /// This needs to be done once before creating any DVoRA adapters.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to adapt\n+    /// - rank: How much compression (lower = fewer parameters)\n+    /// - alpha: How strong the adaptation is\n+    /// - freezeBaseLayer: Whether to lock the original layer's weights (usually true)\n+    /// </para>\n+    /// </remarks>\n+    public DVoRAAdapter(ILayer<T> baseLayer, int rank, double alpha = -1, bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        if (baseLayer == null)\n+        {\n+            throw new ArgumentNullException(nameof(baseLayer));\n+        }\n+\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+\n+        // Ensure shared matrices are initialized\n+        if (_sharedMatrixA == null || _sharedMatrixB == null)\n+        {\n+            throw new InvalidOperationException(\n+                \"Shared matrices must be initialized before creating DVoRA adapters. \" +\n+                \"Call DVoRAAdapter<T>.InitializeSharedMatrices(inputSize, outputSize, rank) first.\");\n+        }\n+\n+        // Validate shared matrix dimensions match this layer\n+        if (_sharedMatrixA.Rows != inputSize || _sharedMatrixA.Columns != rank)\n+        {\n+            throw new ArgumentException(\n+                $\"Shared matrix A dimensions ({_sharedMatrixA.Rows}√ó{_sharedMatrixA.Columns}) \" +\n+                $\"do not match required dimensions ({inputSize}√ó{rank})\", nameof(baseLayer));\n+        }\n+\n+        if (_sharedMatrixB.Rows != rank || _sharedMatrixB.Columns != outputSize)\n+        {\n+            throw new ArgumentException(\n+                $\"Shared matrix B dimensions ({_sharedMatrixB.Rows}√ó{_sharedMatrixB.Columns}) \" +\n+                $\"do not match required dimensions ({rank}√ó{outputSize})\", nameof(baseLayer));\n+        }\n+\n+        // Initialize magnitude from base layer weights (DoRA component)\n+        _magnitude = new Vector<T>(outputSize);\n+        DecomposeWeights();\n+\n+        // Initialize scaling vectors to ones (VeRA component - no initial effect)\n+        _scalingVectorD = new Vector<T>(outputSize);\n+        _scalingVectorB = new Vector<T>(rank);\n+\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            _scalingVectorD[i] = NumOps.One;\n+        }\n+\n+        for (int i = 0; i < rank; i++)\n+        {\n+            _scalingVectorB[i] = NumOps.One;\n+        }\n+\n+        // Update parameter vector\n+        Parameters = new Vector<T>(ParameterCount);\n+        UpdateParametersFromComponents();\n+    }\n+\n+    /// <summary>\n+    /// Initializes the shared random matrices used by all DVoRA adapters.\n+    /// </summary>\n+    /// <param name=\"inputSize\">The input dimension for the layers.</param>\n+    /// <param name=\"outputSize\">The output dimension for the layers.</param>\n+    /// <param name=\"rank\">The rank of the low-rank decomposition.</param>\n+    /// <param name=\"seed\">Optional random seed for reproducibility.</param>\n+    /// <remarks>\n+    /// <para>\n+    /// This method must be called once before creating any DVoRA adapters. It initializes the\n+    /// shared matrices A and B with random values that are frozen (never trained).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Call this once at the start before creating any DVoRA layers:\n+    ///\n+    /// // Initialize shared random matrices (do this once)\n+    /// DVoRAAdapter&lt;double&gt;.InitializeSharedMatrices(inputSize: 784, outputSize: 128, rank: 8);\n+    ///\n+    /// // Now create DVoRA adapters (they will use the shared matrices)\n+    /// var adapter1 = new DVoRAAdapter&lt;double&gt;(layer1, rank: 8);\n+    /// var adapter2 = new DVoRAAdapter&lt;double&gt;(layer2, rank: 8);\n+    ///\n+    /// All adapters share the same random A and B matrices, saving memory!\n+    /// </para>\n+    /// </remarks>\n+    public static void InitializeSharedMatrices(int inputSize, int outputSize, int rank, int? seed = null)\n+    {\n+        lock (_initLock)\n+        {\n+            Random rng = seed.HasValue ? new Random(seed.Value) : new Random();\n+            var ops = MathHelper.GetNumericOperations<T>();\n+\n+            // Initialize matrix A (inputSize √ó rank) with Gaussian random values\n+            _sharedMatrixA = new Matrix<T>(inputSize, rank);\n+            T stddevA = ops.Sqrt(ops.Divide(ops.One, ops.FromDouble(rank)));\n+            for (int i = 0; i < inputSize; i++)\n+            {\n+                for (int j = 0; j < rank; j++)\n+                {\n+                    // Box-Muller transform for Gaussian random numbers\n+                    double u1 = rng.NextDouble();\n+                    double u2 = rng.NextDouble();\n+                    double randStdNormal = Math.Sqrt(-2.0 * Math.Log(u1)) * Math.Sin(2.0 * Math.PI * u2);\n+                    _sharedMatrixA[i, j] = ops.Multiply(ops.FromDouble(randStdNormal), stddevA);\n+                }\n+            }\n+\n+            // Initialize matrix B (rank √ó outputSize) with Gaussian random values\n+            _sharedMatrixB = new Matrix<T>(rank, outputSize);\n+            T stddevB = ops.Sqrt(ops.Divide(ops.One, ops.FromDouble(rank)));\n+            for (int i = 0; i < rank; i++)\n+            {\n+                for (int j = 0; j < outputSize; j++)\n+                {\n+                    // Box-Muller transform for Gaussian random numbers\n+                    double u1 = rng.NextDouble();\n+                    double u2 = rng.NextDouble();\n+                    double randStdNormal = Math.Sqrt(-2.0 * Math.Log(u1)) * Math.Sin(2.0 * Math.PI * u2);\n+                    _sharedMatrixB[i, j] = ops.Multiply(ops.FromDouble(randStdNormal), stddevB);\n+                }\n+            }\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Resets the shared matrices (useful for testing or reinitializing).\n+    /// </summary>\n+    public static void ResetSharedMatrices()\n+    {\n+        lock (_initLock)\n+        {\n+            _sharedMatrixA = null;\n+            _sharedMatrixB = null;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Gets whether the shared matrices have been initialized.\n+    /// </summary>\n+    public static bool AreSharedMatricesInitialized => _sharedMatrixA != null && _sharedMatrixB != null;\n+\n+    /// <summary>\n+    /// Decomposes the base layer's weights into magnitude and direction components.\n+    /// </summary>\n+    /// <remarks>\n+    /// This is the DoRA component of DVoRA. For each output neuron:\n+    /// 1. Extract the weight vector\n+    /// 2. Compute the L2 norm (magnitude)\n+    /// 3. Store the magnitude\n+    ///\n+    /// The direction is implicitly W/||W|| and doesn't need to be stored separately.\n+    /// </remarks>\n+    private void DecomposeWeights()\n+    {\n+        Vector<T> baseParams = _baseLayer.GetParameters();\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        int weightCount = inputSize * outputSize;\n+\n+        // For each output neuron, compute the magnitude of its weight vector\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            T sumSquares = NumOps.Zero;\n+\n+            // Sum squares of all weights for this output neuron\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                int idx = i * inputSize + j;\n+                if (idx < weightCount && idx < baseParams.Length)\n+                {\n+                    T weight = baseParams[idx];\n+                    sumSquares = NumOps.Add(sumSquares, NumOps.Multiply(weight, weight));\n+                }\n+            }\n+\n+            // Magnitude is the L2 norm\n+            _magnitude[i] = NumOps.Sqrt(sumSquares);\n+\n+            // Ensure magnitude is never zero (for numerical stability)\n+            if (NumOps.Equals(_magnitude[i], NumOps.Zero))\n+            {\n+                _magnitude[i] = NumOps.FromDouble(1e-8);\n+            }\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Normalizes a matrix row-wise (each row becomes a unit vector).\n+    /// </summary>\n+    /// <param name=\"matrix\">The matrix to normalize.</param>\n+    /// <returns>Row-normalized matrix where each row has unit L2 norm.</returns>\n+    private Matrix<T> NormalizeRows(Matrix<T> matrix)\n+    {\n+        int rows = matrix.Rows;\n+        int cols = matrix.Columns;\n+        Matrix<T> normalized = new Matrix<T>(rows, cols);\n+\n+        for (int i = 0; i < rows; i++)\n+        {\n+            // Compute L2 norm of row\n+            T sumSquares = NumOps.Zero;\n+            for (int j = 0; j < cols; j++)\n+            {\n+                T val = matrix[i, j];\n+                sumSquares = NumOps.Add(sumSquares, NumOps.Multiply(val, val));\n+            }\n+\n+            T norm = NumOps.Sqrt(sumSquares);\n+\n+            // Avoid division by zero\n+            if (NumOps.Equals(norm, NumOps.Zero))\n+            {\n+                norm = NumOps.FromDouble(1e-8);\n+            }\n+\n+            // Normalize row\n+            for (int j = 0; j < cols; j++)\n+            {\n+                normalized[i, j] = NumOps.Divide(matrix[i, j], norm);\n+            }\n+        }\n+\n+        return normalized;\n+    }\n+\n+    /// <summary>\n+    /// Recomposes weights from magnitude and direction components.\n+    /// </summary>\n+    /// <param name=\"direction\">The normalized direction matrix.</param>\n+    /// <returns>The full weight matrix (magnitude * direction).</returns>\n+    private Matrix<T> RecomposeWeights(Matrix<T> direction)\n+    {\n+        int outputSize = direction.Rows;\n+        int inputSize = direction.Columns;\n+        Matrix<T> weights = new Matrix<T>(outputSize, inputSize);\n+\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                weights[i, j] = NumOps.Multiply(_magnitude[i], direction[i, j]);\n+            }\n+        }\n+\n+        return weights;\n+    }\n+\n+    /// <summary>\n+    /// Creates a dummy LoRA layer (not used since DVoRA uses custom logic).\n+    /// </summary>\n+    protected override LoRALayer<T> CreateLoRALayer(int rank, double alpha)\n+    {\n+        // DVoRA doesn't use a standard LoRA layer, but we need to satisfy the base class\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        return new LoRALayer<T>(inputSize, outputSize, rank, alpha);\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass through the DVoRA adapter.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Output combining base layer with DVoRA-adapted weights.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The DVoRA forward pass combines DoRA and VeRA:\n+    /// 1. Gets base layer weights W\n+    /// 2. Computes direction: d = W / ||W|| (DoRA)\n+    /// 3. Applies VeRA to direction: d' = d + d_scale * (B * A * input) * b_scale (VeRA)\n+    /// 4. Normalizes adapted direction: d_norm = d' / ||d'|| (DoRA)\n+    /// 5. Recomposes weights: W' = m * d_norm (DoRA)\n+    /// 6. Computes output: y = input @ W'^T\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is where DVoRA combines both techniques:\n+    ///\n+    /// DoRA part:\n+    /// - Split weights into magnitude (strength) and direction\n+    /// - Keep magnitude separate, work only with direction\n+    ///\n+    /// VeRA part:\n+    /// - Apply shared random matrices + tiny scaling vectors to the direction\n+    ///\n+    /// Final step:\n+    /// - Normalize the adjusted direction\n+    /// - Multiply magnitude back in\n+    /// - Use these hybrid-adapted weights for prediction\n+    ///\n+    /// Result: Stability of DoRA + efficiency of VeRA = best of both worlds!\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        _lastInput = input.Clone();\n+\n+        // Get base layer parameters and extract weights\n+        Vector<T> baseParams = _baseLayer.GetParameters();\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        int weightCount = inputSize * outputSize;\n+\n+        // Extract weight matrix from base layer\n+        Matrix<T> baseWeights = new Matrix<T>(outputSize, inputSize);\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                int weightIdx = i * inputSize + j;\n+                if (weightIdx < weightCount && weightIdx < baseParams.Length)\n+                {\n+                    baseWeights[i, j] = baseParams[weightIdx];\n+                }\n+                else\n+                {\n+                    baseWeights[i, j] = NumOps.Zero;\n+                }\n+            }\n+        }\n+\n+        // Compute base direction (W / ||W||) - DoRA component\n+        Matrix<T> baseDirection = NormalizeRows(baseWeights);","path":"src/NeuralNetworks/Layers/DVoRAAdapter.cs","commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","original_commit_id":"c11235933bf7c341c8b4a223cf6ac11b757d1480","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Prime the base layer before invoking `Backward`.**\n\nWhen `freezeBaseLayer` is false we call `_baseLayer.Backward`, but this adapter never ran `_baseLayer.Forward` on the current batch, so its cached activations are stale. Any stateful dense layer (e.g. with momentum buffers, dropout masks, etc.) will emit incorrect gradients. Run the base forward pass when the base layer is trainable (or compute its gradients manually) before relying on `_baseLayer.Backward`.\n\n```diff\n         _lastInput = input.Clone();\n \n+        if (!_freezeBaseLayer)\n+        {\n+            _baseLayer.Forward(input);\n+        }\n+\n         // Get base layer parameters and extract weights\n         Vector<T> baseParams = _baseLayer.GetParameters();\n```\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:32:35Z","updated_at":"2025-11-02T02:32:40Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106230","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106230"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106230"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106230/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":491,"original_start_line":491,"start_side":"RIGHT","line":520,"original_line":520,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":520,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106236","pull_request_review_id":3407993158,"id":2484106236,"node_id":"PRRC_kwDOKSXUF86UEHP8","diff_hunk":"@@ -0,0 +1,1116 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.NeuralNetworks.Layers;\n+\n+/// <summary>\n+/// DVoRA (DoRA + VeRA) adapter - combines DoRA's magnitude-direction decomposition with VeRA's extreme parameter efficiency.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// DVoRA achieves the best of both worlds by:\n+/// - Applying DoRA's magnitude-direction decomposition for training stability\n+/// - Using VeRA's shared frozen matrices and scaling vectors for extreme parameter efficiency\n+/// - Applying the VeRA adaptation only to the direction component (not the magnitude)\n+/// </para>\n+/// <para>\n+/// <b>Mathematical Formulation:</b>\n+/// Given pre-trained weights W, DVoRA:\n+/// 1. Decomposes: W = m * d (magnitude and direction)\n+/// 2. Applies VeRA to direction: d' = d + d_scale * (B * A * input) * b_scale\n+/// 3. Normalizes direction: d_norm = d' / ||d'||\n+/// 4. Recomposes: W' = m * d_norm\n+///\n+/// Where:\n+/// - m: magnitude vector (trainable)\n+/// - d: direction matrix (normalized weight vectors)\n+/// - A, B: shared frozen random matrices (VeRA style)\n+/// - d_scale, b_scale: per-layer trainable scaling vectors (VeRA style)\n+/// </para>\n+/// <para>\n+/// <b>Research Context:</b>\n+/// DVoRA scores 5.0 vs VeRA's 4.3 (improvement of 16%) while maintaining ultra-low parameter counts.\n+/// It combines DoRA's superior training stability with VeRA's extreme parameter efficiency.\n+/// </para>\n+/// <para>\n+/// <b>For Beginners:</b> DVoRA is the ultimate parameter-efficient adapter.\n+///\n+/// Think of it as a hybrid technique:\n+/// - From DoRA: Separate magnitude (strength) from direction for stability\n+/// - From VeRA: Use shared random matrices and tiny scaling vectors for efficiency\n+/// - The magic: Apply VeRA's adaptation only to the direction, not the magnitude\n+///\n+/// Parameter comparison for 1000x1000 layer with rank=8:\n+/// - Full fine-tuning: 1,000,000 parameters\n+/// - Standard LoRA: 16,000 parameters (98.4% reduction)\n+/// - DoRA: 17,000 parameters (LoRA + magnitude vector)\n+/// - VeRA: 1,600 parameters (99.84% reduction)\n+/// - DVoRA: ~1,600 parameters (same as VeRA!) but with better performance (5.0 vs 4.3)\n+///\n+/// Benefits:\n+/// - ‚úÖ Extremely parameter-efficient (10x fewer than standard LoRA, same as VeRA)\n+/// - ‚úÖ Better performance than VeRA alone (5.0 vs 4.3 score)\n+/// - ‚úÖ Training stability from DoRA's magnitude-direction decomposition\n+/// - ‚úÖ Shared matrices reduce storage when adapting many layers\n+/// - ‚úÖ Best choice for extreme memory constraints with quality requirements\n+///\n+/// Trade-offs:\n+/// - ‚ö†Ô∏è Requires shared matrix initialization before use\n+/// - ‚ö†Ô∏è Slightly more computation than VeRA (due to normalization)\n+/// - ‚ö†Ô∏è More complex than standard adapters (combines two techniques)\n+///\n+/// When to use DVoRA:\n+/// - Extreme memory constraints but need better quality than VeRA\n+/// - Mobile/edge deployment with limited resources\n+/// - Fine-tuning many layers efficiently\n+/// - When you want the absolute best parameter efficiency + quality balance\n+/// </para>\n+/// <para>\n+/// <b>References:</b>\n+/// - DoRA: \"Weight-Decomposed Low-Rank Adaptation\" (ICML 2024 Oral)\n+/// - VeRA: \"Vector-based Random Matrix Adaptation\"\n+/// - DVoRA: Combines both techniques for optimal efficiency and performance\n+/// </para>\n+/// </remarks>\n+public class DVoRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Shared frozen random matrix A (inputSize √ó rank) used by all DVoRA adapters.\n+    /// </summary>\n+    /// <remarks>\n+    /// This matrix is initialized once globally and shared across all DVoRA layers.\n+    /// It is NEVER trained - it remains frozen at its random initialization values.\n+    /// This is the VeRA component of DVoRA.\n+    /// </remarks>\n+    private static Matrix<T>? _sharedMatrixA;\n+\n+    /// <summary>\n+    /// Shared frozen random matrix B (rank √ó outputSize) used by all DVoRA adapters.\n+    /// </summary>\n+    /// <remarks>\n+    /// This matrix is initialized once globally and shared across all DVoRA layers.\n+    /// It is NEVER trained - it remains frozen at its random initialization values.\n+    /// This is the VeRA component of DVoRA.\n+    /// </remarks>\n+    private static Matrix<T>? _sharedMatrixB;\n+\n+    /// <summary>\n+    /// Lock object for thread-safe shared matrix initialization.\n+    /// </summary>\n+    private static readonly object _initLock = new object();\n+\n+    /// <summary>\n+    /// Magnitude component of the decomposed weights (scalar per output neuron).\n+    /// Trainable per-layer parameter.\n+    /// </summary>\n+    /// <remarks>\n+    /// The magnitude vector stores the L2 norm of each weight vector (one per output neuron).\n+    /// This is the DoRA component of DVoRA.\n+    /// </remarks>\n+    private Vector<T> _magnitude;\n+\n+    /// <summary>\n+    /// Scaling vector d (outputSize) - trainable per-layer parameter.\n+    /// </summary>\n+    /// <remarks>\n+    /// This vector scales the VeRA output on a per-dimension basis.\n+    /// This is the VeRA component of DVoRA.\n+    /// </remarks>\n+    private Vector<T> _scalingVectorD;\n+\n+    /// <summary>\n+    /// Scaling vector b (rank) - trainable per-layer parameter.\n+    /// </summary>\n+    /// <remarks>\n+    /// This vector scales the intermediate rank-dimensional representation.\n+    /// This is the VeRA component of DVoRA.\n+    /// </remarks>\n+    private Vector<T> _scalingVectorB;\n+\n+    /// <summary>\n+    /// Gradient for magnitude vector computed during backpropagation.\n+    /// </summary>\n+    private Vector<T>? _magnitudeGradient;\n+\n+    /// <summary>\n+    /// Gradient for scaling vector d computed during backpropagation.\n+    /// </summary>\n+    private Vector<T>? _scalingVectorDGradient;\n+\n+    /// <summary>\n+    /// Gradient for scaling vector b computed during backpropagation.\n+    /// </summary>\n+    private Vector<T>? _scalingVectorBGradient;\n+\n+    /// <summary>\n+    /// Cached normalized direction from the last forward pass, used in backpropagation.\n+    /// </summary>\n+    private Matrix<T>? _lastNormalizedDirection;\n+\n+    /// <summary>\n+    /// Stored input from the forward pass, needed for gradient computation.\n+    /// </summary>\n+    private Tensor<T>? _lastInput;\n+\n+    /// <summary>\n+    /// Stored intermediate value from forward pass, needed for backward pass.\n+    /// </summary>\n+    private Matrix<T>? _lastIntermediate;\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters.\n+    /// </summary>\n+    /// <remarks>\n+    /// DVoRA parameters = magnitude (outputSize) + d_scale (outputSize) + b_scale (rank).\n+    /// This is only slightly more than VeRA (adds magnitude vector) but much fewer than DoRA (no full LoRA matrices).\n+    /// </remarks>\n+    public override int ParameterCount\n+    {\n+        get\n+        {\n+            int dvoraParams = _magnitude.Length + _scalingVectorD.Length + _scalingVectorB.Length;\n+            return _freezeBaseLayer ? dvoraParams : (_baseLayer.ParameterCount + dvoraParams);\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Initializes a new DVoRA adapter wrapping an existing layer.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with DVoRA.</param>\n+    /// <param name=\"rank\">The rank of the low-rank decomposition (shared across all DVoRA layers).</param>\n+    /// <param name=\"alpha\">The scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"InvalidOperationException\">Thrown when shared matrices are not initialized.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// Before creating any DVoRA adapters, you must call InitializeSharedMatrices() once to set up\n+    /// the shared random matrices that all DVoRA layers will use.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This creates a DVoRA adapter for a layer. Unlike standard LoRA,\n+    /// you must initialize the shared random matrices first by calling:\n+    ///\n+    /// DVoRAAdapter&lt;T&gt;.InitializeSharedMatrices(inputSize, outputSize, rank);\n+    ///\n+    /// This needs to be done once before creating any DVoRA adapters.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to adapt\n+    /// - rank: How much compression (lower = fewer parameters)\n+    /// - alpha: How strong the adaptation is\n+    /// - freezeBaseLayer: Whether to lock the original layer's weights (usually true)\n+    /// </para>\n+    /// </remarks>\n+    public DVoRAAdapter(ILayer<T> baseLayer, int rank, double alpha = -1, bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        if (baseLayer == null)\n+        {\n+            throw new ArgumentNullException(nameof(baseLayer));\n+        }\n+\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+\n+        // Ensure shared matrices are initialized\n+        if (_sharedMatrixA == null || _sharedMatrixB == null)\n+        {\n+            throw new InvalidOperationException(\n+                \"Shared matrices must be initialized before creating DVoRA adapters. \" +\n+                \"Call DVoRAAdapter<T>.InitializeSharedMatrices(inputSize, outputSize, rank) first.\");\n+        }\n+\n+        // Validate shared matrix dimensions match this layer\n+        if (_sharedMatrixA.Rows != inputSize || _sharedMatrixA.Columns != rank)\n+        {\n+            throw new ArgumentException(\n+                $\"Shared matrix A dimensions ({_sharedMatrixA.Rows}√ó{_sharedMatrixA.Columns}) \" +\n+                $\"do not match required dimensions ({inputSize}√ó{rank})\", nameof(baseLayer));\n+        }\n+\n+        if (_sharedMatrixB.Rows != rank || _sharedMatrixB.Columns != outputSize)\n+        {\n+            throw new ArgumentException(\n+                $\"Shared matrix B dimensions ({_sharedMatrixB.Rows}√ó{_sharedMatrixB.Columns}) \" +\n+                $\"do not match required dimensions ({rank}√ó{outputSize})\", nameof(baseLayer));\n+        }\n+\n+        // Initialize magnitude from base layer weights (DoRA component)\n+        _magnitude = new Vector<T>(outputSize);\n+        DecomposeWeights();\n+\n+        // Initialize scaling vectors to ones (VeRA component - no initial effect)\n+        _scalingVectorD = new Vector<T>(outputSize);\n+        _scalingVectorB = new Vector<T>(rank);\n+\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            _scalingVectorD[i] = NumOps.One;\n+        }\n+\n+        for (int i = 0; i < rank; i++)\n+        {\n+            _scalingVectorB[i] = NumOps.One;\n+        }\n+\n+        // Update parameter vector\n+        Parameters = new Vector<T>(ParameterCount);\n+        UpdateParametersFromComponents();\n+    }\n+\n+    /// <summary>\n+    /// Initializes the shared random matrices used by all DVoRA adapters.\n+    /// </summary>\n+    /// <param name=\"inputSize\">The input dimension for the layers.</param>\n+    /// <param name=\"outputSize\">The output dimension for the layers.</param>\n+    /// <param name=\"rank\">The rank of the low-rank decomposition.</param>\n+    /// <param name=\"seed\">Optional random seed for reproducibility.</param>\n+    /// <remarks>\n+    /// <para>\n+    /// This method must be called once before creating any DVoRA adapters. It initializes the\n+    /// shared matrices A and B with random values that are frozen (never trained).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Call this once at the start before creating any DVoRA layers:\n+    ///\n+    /// // Initialize shared random matrices (do this once)\n+    /// DVoRAAdapter&lt;double&gt;.InitializeSharedMatrices(inputSize: 784, outputSize: 128, rank: 8);\n+    ///\n+    /// // Now create DVoRA adapters (they will use the shared matrices)\n+    /// var adapter1 = new DVoRAAdapter&lt;double&gt;(layer1, rank: 8);\n+    /// var adapter2 = new DVoRAAdapter&lt;double&gt;(layer2, rank: 8);\n+    ///\n+    /// All adapters share the same random A and B matrices, saving memory!\n+    /// </para>\n+    /// </remarks>\n+    public static void InitializeSharedMatrices(int inputSize, int outputSize, int rank, int? seed = null)\n+    {\n+        lock (_initLock)\n+        {\n+            Random rng = seed.HasValue ? new Random(seed.Value) : new Random();\n+            var ops = MathHelper.GetNumericOperations<T>();\n+\n+            // Initialize matrix A (inputSize √ó rank) with Gaussian random values\n+            _sharedMatrixA = new Matrix<T>(inputSize, rank);\n+            T stddevA = ops.Sqrt(ops.Divide(ops.One, ops.FromDouble(rank)));\n+            for (int i = 0; i < inputSize; i++)\n+            {\n+                for (int j = 0; j < rank; j++)\n+                {\n+                    // Box-Muller transform for Gaussian random numbers\n+                    double u1 = rng.NextDouble();\n+                    double u2 = rng.NextDouble();\n+                    double randStdNormal = Math.Sqrt(-2.0 * Math.Log(u1)) * Math.Sin(2.0 * Math.PI * u2);\n+                    _sharedMatrixA[i, j] = ops.Multiply(ops.FromDouble(randStdNormal), stddevA);\n+                }\n+            }\n+\n+            // Initialize matrix B (rank √ó outputSize) with Gaussian random values\n+            _sharedMatrixB = new Matrix<T>(rank, outputSize);\n+            T stddevB = ops.Sqrt(ops.Divide(ops.One, ops.FromDouble(rank)));\n+            for (int i = 0; i < rank; i++)\n+            {\n+                for (int j = 0; j < outputSize; j++)\n+                {\n+                    // Box-Muller transform for Gaussian random numbers\n+                    double u1 = rng.NextDouble();\n+                    double u2 = rng.NextDouble();\n+                    double randStdNormal = Math.Sqrt(-2.0 * Math.Log(u1)) * Math.Sin(2.0 * Math.PI * u2);\n+                    _sharedMatrixB[i, j] = ops.Multiply(ops.FromDouble(randStdNormal), stddevB);\n+                }\n+            }\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Resets the shared matrices (useful for testing or reinitializing).\n+    /// </summary>\n+    public static void ResetSharedMatrices()\n+    {\n+        lock (_initLock)\n+        {\n+            _sharedMatrixA = null;\n+            _sharedMatrixB = null;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Gets whether the shared matrices have been initialized.\n+    /// </summary>\n+    public static bool AreSharedMatricesInitialized => _sharedMatrixA != null && _sharedMatrixB != null;\n+\n+    /// <summary>\n+    /// Decomposes the base layer's weights into magnitude and direction components.\n+    /// </summary>\n+    /// <remarks>\n+    /// This is the DoRA component of DVoRA. For each output neuron:\n+    /// 1. Extract the weight vector\n+    /// 2. Compute the L2 norm (magnitude)\n+    /// 3. Store the magnitude\n+    ///\n+    /// The direction is implicitly W/||W|| and doesn't need to be stored separately.\n+    /// </remarks>\n+    private void DecomposeWeights()\n+    {\n+        Vector<T> baseParams = _baseLayer.GetParameters();\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        int weightCount = inputSize * outputSize;\n+\n+        // For each output neuron, compute the magnitude of its weight vector\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            T sumSquares = NumOps.Zero;\n+\n+            // Sum squares of all weights for this output neuron\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                int idx = i * inputSize + j;\n+                if (idx < weightCount && idx < baseParams.Length)\n+                {\n+                    T weight = baseParams[idx];\n+                    sumSquares = NumOps.Add(sumSquares, NumOps.Multiply(weight, weight));\n+                }\n+            }\n+\n+            // Magnitude is the L2 norm\n+            _magnitude[i] = NumOps.Sqrt(sumSquares);\n+\n+            // Ensure magnitude is never zero (for numerical stability)\n+            if (NumOps.Equals(_magnitude[i], NumOps.Zero))\n+            {\n+                _magnitude[i] = NumOps.FromDouble(1e-8);\n+            }\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Normalizes a matrix row-wise (each row becomes a unit vector).\n+    /// </summary>\n+    /// <param name=\"matrix\">The matrix to normalize.</param>\n+    /// <returns>Row-normalized matrix where each row has unit L2 norm.</returns>\n+    private Matrix<T> NormalizeRows(Matrix<T> matrix)\n+    {\n+        int rows = matrix.Rows;\n+        int cols = matrix.Columns;\n+        Matrix<T> normalized = new Matrix<T>(rows, cols);\n+\n+        for (int i = 0; i < rows; i++)\n+        {\n+            // Compute L2 norm of row\n+            T sumSquares = NumOps.Zero;\n+            for (int j = 0; j < cols; j++)\n+            {\n+                T val = matrix[i, j];\n+                sumSquares = NumOps.Add(sumSquares, NumOps.Multiply(val, val));\n+            }\n+\n+            T norm = NumOps.Sqrt(sumSquares);\n+\n+            // Avoid division by zero\n+            if (NumOps.Equals(norm, NumOps.Zero))\n+            {\n+                norm = NumOps.FromDouble(1e-8);\n+            }\n+\n+            // Normalize row\n+            for (int j = 0; j < cols; j++)\n+            {\n+                normalized[i, j] = NumOps.Divide(matrix[i, j], norm);\n+            }\n+        }\n+\n+        return normalized;\n+    }\n+\n+    /// <summary>\n+    /// Recomposes weights from magnitude and direction components.\n+    /// </summary>\n+    /// <param name=\"direction\">The normalized direction matrix.</param>\n+    /// <returns>The full weight matrix (magnitude * direction).</returns>\n+    private Matrix<T> RecomposeWeights(Matrix<T> direction)\n+    {\n+        int outputSize = direction.Rows;\n+        int inputSize = direction.Columns;\n+        Matrix<T> weights = new Matrix<T>(outputSize, inputSize);\n+\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                weights[i, j] = NumOps.Multiply(_magnitude[i], direction[i, j]);\n+            }\n+        }\n+\n+        return weights;\n+    }\n+\n+    /// <summary>\n+    /// Creates a dummy LoRA layer (not used since DVoRA uses custom logic).\n+    /// </summary>\n+    protected override LoRALayer<T> CreateLoRALayer(int rank, double alpha)\n+    {\n+        // DVoRA doesn't use a standard LoRA layer, but we need to satisfy the base class\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        return new LoRALayer<T>(inputSize, outputSize, rank, alpha);\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass through the DVoRA adapter.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Output combining base layer with DVoRA-adapted weights.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The DVoRA forward pass combines DoRA and VeRA:\n+    /// 1. Gets base layer weights W\n+    /// 2. Computes direction: d = W / ||W|| (DoRA)\n+    /// 3. Applies VeRA to direction: d' = d + d_scale * (B * A * input) * b_scale (VeRA)\n+    /// 4. Normalizes adapted direction: d_norm = d' / ||d'|| (DoRA)\n+    /// 5. Recomposes weights: W' = m * d_norm (DoRA)\n+    /// 6. Computes output: y = input @ W'^T\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is where DVoRA combines both techniques:\n+    ///\n+    /// DoRA part:\n+    /// - Split weights into magnitude (strength) and direction\n+    /// - Keep magnitude separate, work only with direction\n+    ///\n+    /// VeRA part:\n+    /// - Apply shared random matrices + tiny scaling vectors to the direction\n+    ///\n+    /// Final step:\n+    /// - Normalize the adjusted direction\n+    /// - Multiply magnitude back in\n+    /// - Use these hybrid-adapted weights for prediction\n+    ///\n+    /// Result: Stability of DoRA + efficiency of VeRA = best of both worlds!\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        _lastInput = input.Clone();\n+\n+        // Get base layer parameters and extract weights\n+        Vector<T> baseParams = _baseLayer.GetParameters();\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        int weightCount = inputSize * outputSize;\n+\n+        // Extract weight matrix from base layer\n+        Matrix<T> baseWeights = new Matrix<T>(outputSize, inputSize);\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                int weightIdx = i * inputSize + j;\n+                if (weightIdx < weightCount && weightIdx < baseParams.Length)\n+                {\n+                    baseWeights[i, j] = baseParams[weightIdx];\n+                }\n+                else\n+                {\n+                    baseWeights[i, j] = NumOps.Zero;\n+                }\n+            }\n+        }\n+\n+        // Compute base direction (W / ||W||) - DoRA component\n+        Matrix<T> baseDirection = NormalizeRows(baseWeights);\n+\n+        // Apply VeRA to get direction delta\n+        int batchSize = input.Shape[0];\n+        int rank = _scalingVectorB.Length;\n+\n+        // Convert input to matrix [batchSize, inputSize]\n+        Matrix<T> inputMatrix = new Matrix<T>(batchSize, inputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                inputMatrix[i, j] = input[i * inputSize + j];\n+            }\n+        }\n+\n+        // VeRA forward: (B * A * input) with scaling vectors\n+        // Compute: input * A (shared, frozen) ‚Üí [batchSize, rank]\n+        Matrix<T> afterA = inputMatrix.Multiply(_sharedMatrixA!);\n+\n+        // Apply scaling vector b element-wise: afterA * diag(b) ‚Üí [batchSize, rank]\n+        Matrix<T> afterB = new Matrix<T>(batchSize, rank);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < rank; j++)\n+            {\n+                afterB[i, j] = NumOps.Multiply(afterA[i, j], _scalingVectorB[j]);\n+            }\n+        }\n+\n+        // Compute: afterB * B (shared, frozen) ‚Üí [batchSize, outputSize]\n+        Matrix<T> afterSharedB = afterB.Multiply(_sharedMatrixB!);\n+        _lastIntermediate = afterSharedB.Clone();\n+\n+        // Apply scaling vector d element-wise: afterSharedB * diag(d) ‚Üí [batchSize, outputSize]\n+        Matrix<T> veraContribution = new Matrix<T>(batchSize, outputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                veraContribution[i, j] = NumOps.Multiply(afterSharedB[i, j], _scalingVectorD[j]);\n+            }\n+        }\n+\n+        // Apply alpha/rank scaling\n+        T scaling = NumOps.Divide(NumOps.FromDouble(Alpha), NumOps.FromDouble(Rank));\n+\n+        // For direction update, we need the VeRA contribution as a weight delta, not an output\n+        // Average over batch to get per-weight contribution\n+        Matrix<T> veraWeightDelta = new Matrix<T>(outputSize, inputSize);\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                T sum = NumOps.Zero;\n+                for (int b = 0; b < batchSize; b++)\n+                {\n+                    // Approximate weight gradient contribution\n+                    T contrib = NumOps.Multiply(veraContribution[b, i], inputMatrix[b, j]);\n+                    sum = NumOps.Add(sum, contrib);\n+                }\n+                veraWeightDelta[i, j] = NumOps.Multiply(\n+                    NumOps.Divide(sum, NumOps.FromDouble(batchSize)),\n+                    scaling);\n+            }\n+        }\n+\n+        // Add VeRA delta to base direction: d' = d + delta\n+        Matrix<T> adaptedDirection = new Matrix<T>(outputSize, inputSize);\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                adaptedDirection[i, j] = NumOps.Add(baseDirection[i, j], veraWeightDelta[i, j]);\n+            }\n+        }\n+\n+        // Normalize the adapted direction: d_norm = d' / ||d'|| - DoRA component\n+        _lastNormalizedDirection = NormalizeRows(adaptedDirection);\n+\n+        // Recompose weights: W' = m * d_norm - DoRA component\n+        Matrix<T> finalWeights = RecomposeWeights(_lastNormalizedDirection);\n+\n+        // Compute output: y = input @ W'^T\n+        Matrix<T> outputMatrix = inputMatrix.Multiply(finalWeights.Transpose());\n+\n+        // Convert back to tensor\n+        Vector<T> outputData = new Vector<T>(batchSize * outputSize);\n+        int idx = 0;\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                outputData[idx++] = outputMatrix[i, j];\n+            }\n+        }\n+\n+        return new Tensor<T>(new[] { batchSize, outputSize }, outputData);\n+    }","path":"src/NeuralNetworks/Layers/DVoRAAdapter.cs","commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","original_commit_id":"c11235933bf7c341c8b4a223cf6ac11b757d1480","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Include the bias term in the forward pass.**\n\n`outputMatrix = inputMatrix √ó W'·µÄ` only accounts for weights. The base layer‚Äôs biases (still stored in `baseParams` after `weightCount`) are never added, so every prediction is off by the learned bias vector. Extract the biases and add them to each row before returning.\n\n```diff\n-        // Compute output: y = input @ W'^T\n-        Matrix<T> outputMatrix = inputMatrix.Multiply(finalWeights.Transpose());\n+        // Compute output: y = input @ W'^T + bias\n+        Matrix<T> outputMatrix = inputMatrix.Multiply(finalWeights.Transpose());\n+\n+        Vector<T> biases = new Vector<T>(outputSize);\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            int biasIdx = weightCount + i;\n+            biases[i] = biasIdx < baseParams.Length ? baseParams[biasIdx] : NumOps.Zero;\n+        }\n+\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                outputMatrix[i, j] = NumOps.Add(outputMatrix[i, j], biases[j]);\n+            }\n+        }\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/Layers/DVoRAAdapter.cs around lines 604 to 618, the\nforward pass multiplies inputMatrix by finalWeights.Transpose() but omits the\nbias term stored in baseParams after weightCount; extract the bias vector\n(length = outputSize) from baseParams starting at weightCount, then add that\nbias to each row of outputMatrix (or add to each element when filling\noutputData) before constructing and returning the Tensor so each output row\nincludes the learned bias.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:32:35Z","updated_at":"2025-11-02T02:32:40Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106236","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106236"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106236"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106236/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":604,"original_start_line":604,"start_side":"RIGHT","line":618,"original_line":618,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":618,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106245","pull_request_review_id":3407993158,"id":2484106245,"node_id":"PRRC_kwDOKSXUF86UEHQF","diff_hunk":"@@ -0,0 +1,602 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.NeuralNetworks.Layers;\n+\n+/// <summary>\n+/// DyLoRA (Dynamic LoRA) adapter that trains with multiple ranks simultaneously.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// DyLoRA extends the standard LoRA approach by training multiple rank configurations simultaneously\n+/// using a nested dropout technique. This allows a single trained adapter to be deployed at different\n+/// rank levels without retraining, providing flexibility for different hardware constraints or\n+/// performance requirements.\n+/// </para>\n+/// <para>\n+/// The key innovation is nested dropout: during training, for each forward pass, a random rank r\n+/// is selected from the active ranks, and only the first r components of matrices A and B are used.\n+/// This ensures that smaller ranks can function independently and don't rely on higher-rank components.\n+/// </para>\n+/// <para><b>For Beginners:</b> DyLoRA is like LoRA with a superpower - flexibility!\n+///\n+/// Standard LoRA problem:\n+/// - You choose rank=8 and train\n+/// - Later realize rank=4 would work fine (save memory/speed)\n+/// - Or need rank=16 for better quality\n+/// - Must retrain from scratch with the new rank\n+///\n+/// DyLoRA solution:\n+/// - Train once with multiple ranks (e.g., [2, 4, 8, 16])\n+/// - Deploy with ANY of those ranks without retraining\n+/// - Switch between ranks at runtime based on device capabilities\n+///\n+/// How it works:\n+/// 1. Train with MaxRank (e.g., 16) but randomly use smaller ranks during training\n+/// 2. Nested dropout ensures each rank works independently\n+/// 3. After training, pick deployment rank based on needs (2=fastest, 16=best quality)\n+///\n+/// Use cases:\n+/// - Deploy same model to mobile (rank=2) and server (rank=16)\n+/// - Dynamic quality scaling based on battery level\n+/// - A/B testing different rank/quality trade-offs\n+/// - Training once, deploying everywhere\n+///\n+/// Example: Train with ActiveRanks=[2,4,8], deploy with:\n+/// - Rank=2 for mobile devices (98% parameter reduction, good quality)\n+/// - Rank=4 for tablets (95% parameter reduction, better quality)\n+/// - Rank=8 for desktops (90% parameter reduction, best quality)\n+/// </para>\n+/// </remarks>\n+public class DyLoRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Maximum rank for the LoRA decomposition.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This is the highest rank that can be used during inference. The actual matrices A and B\n+    /// are sized for this maximum rank, but smaller ranks can be used by only accessing the\n+    /// first r columns/rows.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is the \"full size\" of your LoRA adapter. You can always\n+    /// use a smaller rank, but you can't exceed this maximum without retraining.\n+    /// </para>\n+    /// </remarks>\n+    private readonly int _maxRank;\n+\n+    /// <summary>\n+    /// Array of ranks to train simultaneously during nested dropout.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// During training, each forward pass randomly selects one of these ranks and only uses\n+    /// that many components. This ensures all these ranks are viable for deployment.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> These are the rank options you can choose from after training.\n+    /// For example, [2, 4, 8, 16] means you can deploy with any of these four ranks.\n+    /// </para>\n+    /// </remarks>\n+    private readonly int[] _activeRanks;\n+\n+    /// <summary>\n+    /// Current rank to use during inference (forward pass in eval mode).\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This determines how many components of the LoRA matrices are used during inference.\n+    /// Can be changed at runtime to trade off between speed and quality.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is the \"deployment rank\" - the actual rank you're using\n+    /// right now for predictions. You can change this at any time without retraining!\n+    /// </para>\n+    /// </remarks>\n+    private int _currentDeploymentRank;\n+\n+    /// <summary>\n+    /// Random number generator for nested dropout during training.\n+    /// </summary>\n+    private readonly Random _random;\n+\n+    /// <summary>\n+    /// Whether the adapter is in training mode (uses nested dropout).\n+    /// </summary>\n+    private bool _isTraining;\n+\n+    /// <summary>\n+    /// Gets the maximum rank of the DyLoRA adapter.\n+    /// </summary>\n+    public int MaxRank => _maxRank;\n+\n+    /// <summary>\n+    /// Gets the array of active ranks used during training.\n+    /// </summary>\n+    public int[] ActiveRanks => _activeRanks.ToArray();\n+\n+    /// <summary>\n+    /// Gets or sets the current deployment rank used during inference.\n+    /// </summary>\n+    /// <exception cref=\"ArgumentException\">Thrown when attempting to set a rank not in ActiveRanks.</exception>\n+    public int CurrentDeploymentRank\n+    {\n+        get => _currentDeploymentRank;\n+        set => SetDeploymentRank(value);\n+    }\n+\n+    /// <summary>\n+    /// Gets or sets whether the adapter is in training mode.\n+    /// </summary>\n+    /// <remarks>\n+    /// When in training mode, nested dropout is applied. In eval mode, the deployment rank is used.\n+    /// </remarks>\n+    public bool IsTraining\n+    {\n+        get => _isTraining;\n+        set => _isTraining = value;\n+    }\n+\n+    /// <summary>\n+    /// Initializes a new DyLoRA adapter with the specified parameters.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with DyLoRA.</param>\n+    /// <param name=\"maxRank\">The maximum rank of the LoRA decomposition.</param>\n+    /// <param name=\"activeRanks\">Array of ranks to train simultaneously (must be sorted ascending and all &lt;= maxRank).</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to maxRank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer or activeRanks is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when activeRanks is invalid.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This creates a DyLoRA adapter that can train and deploy with multiple ranks.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to make flexible and efficient\n+    /// - maxRank: The maximum rank you might need (e.g., 16)\n+    /// - activeRanks: Which ranks to make available (e.g., [2, 4, 8, 16])\n+    /// - alpha: How strong the LoRA adaptation is (usually equals maxRank)\n+    /// - freezeBaseLayer: Whether to lock the original layer (usually true)\n+    ///\n+    /// Example:\n+    /// new DyLoRAAdapter(denseLayer, maxRank: 16, activeRanks: [2, 4, 8, 16])\n+    /// This trains a single adapter that can deploy with ranks 2, 4, 8, or 16.\n+    /// </para>\n+    /// </remarks>\n+    public DyLoRAAdapter(\n+        ILayer<T> baseLayer,\n+        int maxRank,\n+        int[] activeRanks,\n+        double alpha = -1,\n+        bool freezeBaseLayer = true)\n+        : base(baseLayer, maxRank, alpha, freezeBaseLayer)\n+    {\n+        if (activeRanks == null)\n+        {\n+            throw new ArgumentNullException(nameof(activeRanks));\n+        }\n+\n+        if (activeRanks.Length == 0)\n+        {\n+            throw new ArgumentException(\"ActiveRanks must contain at least one rank\", nameof(activeRanks));\n+        }\n+\n+        // Validate activeRanks are sorted and within bounds\n+        for (int i = 0; i < activeRanks.Length; i++)\n+        {\n+            if (activeRanks[i] <= 0)\n+            {\n+                throw new ArgumentException($\"All ranks must be positive, but activeRanks[{i}] = {activeRanks[i]}\", nameof(activeRanks));\n+            }\n+\n+            if (activeRanks[i] > maxRank)\n+            {\n+                throw new ArgumentException($\"All ranks must be <= maxRank ({maxRank}), but activeRanks[{i}] = {activeRanks[i]}\", nameof(activeRanks));\n+            }\n+\n+            if (i > 0 && activeRanks[i] <= activeRanks[i - 1])\n+            {\n+                throw new ArgumentException(\"ActiveRanks must be sorted in ascending order with no duplicates\", nameof(activeRanks));\n+            }\n+        }\n+\n+        _maxRank = maxRank;\n+        _activeRanks = activeRanks.ToArray();\n+        _currentDeploymentRank = activeRanks[activeRanks.Length - 1]; // Default to highest rank\n+        _random = new Random();\n+        _isTraining = true; // Start in training mode\n+    }\n+\n+    /// <summary>\n+    /// Sets the deployment rank for inference.\n+    /// </summary>\n+    /// <param name=\"rank\">The rank to use (must be in ActiveRanks).</param>\n+    /// <exception cref=\"ArgumentException\">Thrown when rank is not in ActiveRanks.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// This allows switching between different ranks at runtime without retraining.\n+    /// The rank must be one of the ActiveRanks that were trained.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This changes the quality/speed trade-off of your model.\n+    /// Higher rank = better quality but slower. Lower rank = faster but slightly lower quality.\n+    ///\n+    /// Example usage:\n+    /// - Battery low? adapter.SetDeploymentRank(2) for speed\n+    /// - Plugged in? adapter.SetDeploymentRank(16) for quality\n+    /// - On mobile? adapter.SetDeploymentRank(4) for balance\n+    /// </para>\n+    /// </remarks>\n+    public void SetDeploymentRank(int rank)\n+    {\n+        if (!_activeRanks.Contains(rank))\n+        {\n+            throw new ArgumentException(\n+                $\"Deployment rank {rank} is not in ActiveRanks [{string.Join(\", \", _activeRanks)}]. \" +\n+                $\"Only trained ranks can be used for deployment.\",\n+                nameof(rank));\n+        }\n+\n+        _currentDeploymentRank = rank;\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass with dynamic rank selection.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Sum of base layer output and DyLoRA output.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// During training, a random rank is selected from ActiveRanks for nested dropout.\n+    /// During inference, the CurrentDeploymentRank is used consistently.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This processes input through both the base layer and DyLoRA:\n+    ///\n+    /// Training mode:\n+    /// - Randomly picks a rank from ActiveRanks each forward pass\n+    /// - Uses only that many components of A and B matrices\n+    /// - This trains all ranks to work independently\n+    ///\n+    /// Inference mode:\n+    /// - Always uses CurrentDeploymentRank\n+    /// - Consistent behavior for production\n+    /// - Can change rank without retraining\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        // Select rank for this forward pass\n+        int activeRank = _isTraining\n+            ? _activeRanks[_random.Next(_activeRanks.Length)]  // Random rank during training\n+            : _currentDeploymentRank;                          // Fixed rank during inference\n+\n+        // Forward through base layer\n+        Tensor<T> baseOutput = _baseLayer.Forward(input);\n+\n+        // Forward through LoRA layer with restricted rank\n+        Tensor<T> loraOutput = ForwardWithRank(input, activeRank);\n+\n+        // Sum the outputs\n+        Tensor<T> result = new Tensor<T>(baseOutput.Shape);\n+        for (int i = 0; i < baseOutput.Length; i++)\n+        {\n+            result[i] = NumOps.Add(baseOutput[i], loraOutput[i]);\n+        }\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Performs forward pass through LoRA layer using only the first 'rank' components.\n+    /// </summary>","path":"src/NeuralNetworks/Layers/DyLoRAAdapter.cs","commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","original_commit_id":"c11235933bf7c341c8b4a223cf6ac11b757d1480","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Route the forward pass through `_loraLayer` before calling `base.Backward`**\n\n`base.Backward` ultimately delegates to `_loraLayer.Backward`, which expects the latest input from `_loraLayer.Forward`. Because this override computes the LoRA contribution manually and never calls `_loraLayer.Forward`, `_loraLayer.Backward` works with stale caches‚Äîgradients become incorrect and the sampled rank has no effect on the update. Please keep `_loraLayer`‚Äôs internal state in sync with the tensors you return (e.g., by forwarding through `_loraLayer` and masking the slices you drop, or by updating the cached input yourself before calling `base.Backward`).\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/Layers/DyLoRAAdapter.cs around lines 262 to 287, the\noverride computes the LoRA contribution manually and never calls\n_loraLayer.Forward, leaving _loraLayer‚Äôs internal caches stale so base.Backward\n(which delegates to _loraLayer.Backward) receives incorrect inputs; fix by\nrouting the forward pass through _loraLayer so its caches match what will be\nbackpropagated: call _loraLayer.Forward(input) to obtain a full loraOutput and\nthen either zero-out / mask the components beyond the selected activeRank (or\notherwise constrain that output to match ForwardWithRank) before summing with\nbaseOutput, or update _loraLayer‚Äôs cached input/state to reflect the manually\nproduced loraOutput; ensure the same activeRank selection logic is used for both\nforward and the state you leave in _loraLayer so gradients and updates are\ncomputed against the exact tensors returned.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:32:35Z","updated_at":"2025-11-02T02:32:40Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106245","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106245"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106245"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106245/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":262,"original_start_line":262,"start_side":"RIGHT","line":287,"original_line":287,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":287,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106253","pull_request_review_id":3407993158,"id":2484106253,"node_id":"PRRC_kwDOKSXUF86UEHQN","diff_hunk":"@@ -0,0 +1,290 @@\n+using AiDotNet.Interfaces;\n+using System;\n+\n+namespace AiDotNet.NeuralNetworks.Layers;\n+\n+/// <summary>\n+/// Implements Flora (Low-Rank Adapters Are Secretly Gradient Compressors) adapter for memory-efficient fine-tuning.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// Flora reinterprets LoRA as a gradient compression mechanism and achieves high-rank updates through\n+/// periodic resampling of projection matrices while maintaining sublinear space complexity for optimizer states.\n+/// </para>\n+/// <para><b>Research Paper:</b> \"Flora: Low-Rank Adapters Are Secretly Gradient Compressors\"\n+/// by Yongchang Hao et al., ICML 2024. arXiv:2402.03293\n+/// </para>\n+/// <para><b>Key Innovation:</b> Unlike standard LoRA which restricts weight updates to a fixed low-rank subspace,\n+/// Flora periodically resamples the projection matrices (A and B), allowing the effective rank of cumulative\n+/// updates to grow over time. This achieves performance comparable to full-rank fine-tuning while maintaining\n+/// the memory efficiency of LoRA.\n+/// </para>\n+/// </remarks>\n+public class FloraAdapter<T> : LoRAAdapterBase<T>\n+{\n+    private readonly int _resamplingInterval;\n+    private readonly int _rank;\n+    private int _currentStep;\n+    private Matrix<T>? _compressedMomentum;\n+    private Matrix<T>? _compressedSecondMoment;\n+    private readonly Random _random;\n+    private readonly double _momentumDecay;\n+    private readonly double _secondMomentDecay;\n+    private readonly bool _useAdaptiveLearningRate;\n+\n+    public FloraAdapter(\n+        ILayer<T> baseLayer,\n+        int rank,\n+        double alpha = -1,\n+        int resamplingInterval = 1000,\n+        double momentumDecay = 0.9,\n+        double secondMomentDecay = 0.999,\n+        bool useAdaptiveLearningRate = true,\n+        bool freezeBaseLayer = true,\n+        int seed = 42)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        if (resamplingInterval < 1)\n+        {\n+            throw new ArgumentException(\"Resampling interval must be at least 1\", nameof(resamplingInterval));\n+        }\n+\n+        _resamplingInterval = resamplingInterval;\n+        _rank = rank;\n+        _currentStep = 0;\n+        _momentumDecay = momentumDecay;\n+        _secondMomentDecay = secondMomentDecay;\n+        _useAdaptiveLearningRate = useAdaptiveLearningRate;\n+        _random = new Random(seed);\n+\n+        int outputSize = GetOutputShape()[0];\n+        _compressedMomentum = new Matrix<T>(rank, outputSize);\n+\n+        if (_useAdaptiveLearningRate)\n+        {\n+            _compressedSecondMoment = new Matrix<T>(rank, outputSize);\n+        }\n+    }\n+\n+    public int ResamplingInterval => _resamplingInterval;\n+    public int CurrentStep => _currentStep;\n+\n+    public override void UpdateParameters(T learningRate)\n+    {\n+        _currentStep++;\n+\n+        if (_currentStep % _resamplingInterval == 0)\n+        {\n+            ResampleProjectionMatrices();\n+        }\n+\n+        Vector<T> loraGradients = _loraLayer.GetParameterGradients();\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+\n+        Matrix<T> gradB = new Matrix<T>(_rank, outputSize);\n+        int bOffset = inputSize * _rank;\n+\n+        for (int i = 0; i < _rank; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                gradB[i, j] = loraGradients[bOffset + i * outputSize + j];\n+            }\n+        }\n+\n+        T beta1 = NumOps.FromDouble(_momentumDecay);\n+        T oneMinusBeta1 = NumOps.FromDouble(1.0 - _momentumDecay);\n+\n+        for (int i = 0; i < _rank; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                T oldMomentum = _compressedMomentum![i, j];\n+                T newMomentum = NumOps.Add(\n+                    NumOps.Multiply(beta1, oldMomentum),\n+                    NumOps.Multiply(oneMinusBeta1, gradB[i, j])\n+                );\n+                _compressedMomentum[i, j] = newMomentum;\n+            }\n+        }\n+\n+        if (_useAdaptiveLearningRate)\n+        {\n+            T beta2 = NumOps.FromDouble(_secondMomentDecay);\n+            T oneMinusBeta2 = NumOps.FromDouble(1.0 - _secondMomentDecay);\n+\n+            for (int i = 0; i < _rank; i++)\n+            {\n+                for (int j = 0; j < outputSize; j++)\n+                {\n+                    T grad = gradB[i, j];\n+                    T gradSquared = NumOps.Multiply(grad, grad);\n+                    T oldSecondMoment = _compressedSecondMoment![i, j];\n+                    T newSecondMoment = NumOps.Add(\n+                        NumOps.Multiply(beta2, oldSecondMoment),\n+                        NumOps.Multiply(oneMinusBeta2, gradSquared)\n+                    );\n+                    _compressedSecondMoment[i, j] = newSecondMoment;\n+                }\n+            }\n+        }\n+\n+        _loraLayer.UpdateParameters(learningRate);\n+\n+        if (!_freezeBaseLayer)\n+        {\n+            _baseLayer.UpdateParameters(learningRate);\n+        }\n+\n+        SyncParametersFromLayers();\n+    }\n+\n+    private void ResampleProjectionMatrices()\n+    {\n+        Vector<T> currentParams = _loraLayer.GetParameters();\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+\n+        Matrix<T> oldA = new Matrix<T>(inputSize, _rank);\n+        for (int i = 0; i < inputSize; i++)\n+        {\n+            for (int j = 0; j < _rank; j++)\n+            {\n+                oldA[i, j] = currentParams[i * _rank + j];\n+            }\n+        }\n+\n+        Matrix<T> newA = new Matrix<T>(inputSize, _rank);\n+        double stddev = 1.0 / Math.Sqrt(_rank);\n+        for (int i = 0; i < inputSize; i++)\n+        {\n+            for (int j = 0; j < _rank; j++)\n+            {\n+                double u1 = 1.0 - _random.NextDouble();\n+                double u2 = 1.0 - _random.NextDouble();\n+                double gaussianValue = Math.Sqrt(-2.0 * Math.Log(u1)) * Math.Cos(2.0 * Math.PI * u2);\n+                newA[i, j] = NumOps.FromDouble(gaussianValue * stddev);\n+            }\n+        }\n+\n+        Matrix<T> transferMatrix = ComputeTransferMatrix(oldA, newA);\n+        Matrix<T> newMomentum = MultiplyMatrices(_compressedMomentum!, transferMatrix);\n+        _compressedMomentum = newMomentum;\n+\n+        if (_useAdaptiveLearningRate && _compressedSecondMoment != null)\n+        {\n+            Matrix<T> newSecondMoment = MultiplyMatrices(_compressedSecondMoment, transferMatrix);\n+            _compressedSecondMoment = newSecondMoment;","path":"src/NeuralNetworks/Layers/FloraAdapter.cs","commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","original_commit_id":"c11235933bf7c341c8b4a223cf6ac11b757d1480","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Fix the transfer transform multiplication.**  \n`_compressedMomentum` is shaped `(rank √ó outputSize)`, but you right-multiply it by `transferMatrix` `(rank √ó rank)`. The multiplication routine expects `_compressedMomentum.Columns == transferMatrix.Rows`, i.e. `outputSize == rank`, which is false for almost every dense layer. As soon as resampling fires this throws `ArgumentException`. Multiply on the left (or transpose the transfer matrix) so the inner dimensions line up:\n\n```diff\n-        Matrix<T> newMomentum = MultiplyMatrices(_compressedMomentum!, transferMatrix);\n+        Matrix<T> newMomentum = MultiplyMatrices(transferMatrix.Transpose(), _compressedMomentum!);\n```\n\nApply the same fix to the second-moment path.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/Layers/FloraAdapter.cs around lines 172 to 179, the code\nright-multiplies _compressedMomentum (rank √ó outputSize) by transferMatrix (rank\n√ó rank) causing mismatched inner dimensions and an ArgumentException; change the\nmultiplication so transferMatrix is applied on the left (transferMatrix *\n_compressedMomentum) or transpose transferMatrix before right-multiplying so the\ninner dimensions align, and make the identical change for the\n_compressedSecondMoment path when _useAdaptiveLearningRate is true.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:32:35Z","updated_at":"2025-11-02T02:32:40Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106253","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106253"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106253"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106253/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":172,"original_start_line":172,"start_side":"RIGHT","line":179,"original_line":179,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":179,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106259","pull_request_review_id":3407993158,"id":2484106259,"node_id":"PRRC_kwDOKSXUF86UEHQT","diff_hunk":"@@ -0,0 +1,478 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.NeuralNetworks.Layers;\n+\n+/// <summary>\n+/// Generalized LoRA (GLoRA) implementation that adapts both weights AND activations.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// GLoRA extends standard LoRA by adding adaptation to both the layer's weights and its activations.\n+/// This provides more flexibility for multi-task learning scenarios where different tasks may need\n+/// different feature representations at each layer.\n+/// </para>\n+/// <para>\n+/// The forward pass computes:\n+/// - adapted_weights = base_weights + B_w * A_w (weight adaptation)\n+/// - base_output = input * adapted_weights\n+/// - adapted_output = base_output + B_a * A_a * input (activation adaptation)\n+/// </para>\n+/// <para><b>For Beginners:</b> While standard LoRA only adapts what the layer learns (its weights),\n+/// GLoRA also adapts what the layer produces (its activations). Think of it like this:\n+///\n+/// - Standard LoRA: Adjusts the \"recipe\" (weights) but produces the same type of output\n+/// - GLoRA: Adjusts both the \"recipe\" (weights) AND transforms the output for different uses\n+///\n+/// This is especially useful when:\n+/// 1. Different tasks need different feature representations\n+/// 2. You're doing multi-task learning (e.g., the same base features used differently)\n+/// 3. You need more flexibility than weight-only adaptation provides\n+///\n+/// Key differences from StandardLoRA:\n+/// - WeightAdaptation: Standard LoRA component that modifies layer weights\n+/// - ActivationAdaptation: Additional LoRA component that modifies layer outputs\n+/// - ActivationRank: Can be different from weight rank for fine-tuned control\n+///\n+/// Trade-offs:\n+/// + More flexible: Can adapt representations for different tasks\n+/// + Better for multi-task: Each task can use features differently\n+/// - More parameters: Two LoRA components instead of one\n+/// - Slightly slower: Two adaptation computations per forward pass\n+///\n+/// Example: For a 1000x1000 layer with weight_rank=8 and activation_rank=4:\n+/// - Weight adaptation: 16,000 parameters (same as standard LoRA)\n+/// - Activation adaptation: 8,000 additional parameters\n+/// - Total: 24,000 parameters (still 97.6% reduction from 1M!)\n+/// </para>\n+/// </remarks>\n+public class GLoRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// The LoRA layer that adapts activations (layer outputs).\n+    /// </summary>\n+    private readonly LoRALayer<T> _activationAdaptation;\n+\n+    /// <summary>\n+    /// Gets the weight adaptation LoRA layer.\n+    /// </summary>\n+    /// <remarks>\n+    /// This adapts the layer's weights using standard LoRA (B_w * A_w).\n+    /// </remarks>\n+    public LoRALayer<T> WeightAdaptation => _loraLayer;\n+\n+    /// <summary>\n+    /// Gets the activation adaptation LoRA layer.\n+    /// </summary>\n+    /// <remarks>\n+    /// This adapts the layer's outputs/activations using a second LoRA component (B_a * A_a).\n+    /// </remarks>\n+    public LoRALayer<T> ActivationAdaptation => _activationAdaptation;\n+\n+    /// <summary>\n+    /// Gets the rank of the activation adaptation.\n+    /// </summary>\n+    /// <remarks>\n+    /// This can be different from the weight adaptation rank, allowing for independent\n+    /// control over the complexity of weight vs. activation adaptations.\n+    /// </remarks>\n+    public int ActivationRank => _activationAdaptation.Rank;\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters (both weight and activation adaptations).\n+    /// </summary>\n+    /// <remarks>\n+    /// If the base layer is frozen, this returns the sum of weight and activation LoRA parameters.\n+    /// Otherwise, it includes base layer parameters as well.\n+    /// </remarks>\n+    public override int ParameterCount => _freezeBaseLayer\n+        ? (_loraLayer.ParameterCount + _activationAdaptation.ParameterCount)\n+        : (_baseLayer.ParameterCount + _loraLayer.ParameterCount + _activationAdaptation.ParameterCount);","path":"src/NeuralNetworks/Layers/GLoRAAdapter.cs","commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","original_commit_id":"c11235933bf7c341c8b4a223cf6ac11b757d1480","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**`ParameterCount` dereferences a null field during construction.**\n\n`LoRAAdapterBase` queries `ParameterCount` inside its ctor *before* the derived ctor runs. At that moment `_activationAdaptation` is still null, so `_activationAdaptation.ParameterCount` triggers a `NullReferenceException`. Add a null guard (e.g., fall back to `_loraLayer.ParameterCount` until `_activationAdaptation` is created) or restructure initialization so the secondary LoRA layer exists before the base ctor needs it. Currently the adapter can‚Äôt be instantiated.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/Layers/GLoRAAdapter.cs around lines 88-90, ParameterCount\ndereferences _activationAdaptation which can be null during base class\nconstruction; change the getter to guard against null (e.g., use a\nnull-check/fallback to only _loraLayer.ParameterCount when _activationAdaptation\nis null) or ensure _activationAdaptation is created before the base ctor queries\nParameterCount (reorder initialization or lazy-initialize _activationAdaptation)\nso no NullReferenceException occurs.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:32:36Z","updated_at":"2025-11-02T02:32:40Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106259","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106259"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106259"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106259/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":88,"original_start_line":88,"start_side":"RIGHT","line":90,"original_line":90,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":90,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106268","pull_request_review_id":3407993158,"id":2484106268,"node_id":"PRRC_kwDOKSXUF86UEHQc","diff_hunk":"@@ -0,0 +1,819 @@\n+using AiDotNet.Interfaces;\n+using System.Collections.Generic;\n+\n+namespace AiDotNet.NeuralNetworks.Layers;\n+\n+/// <summary>\n+/// HRA (Hybrid Rank Adaptation) adapter that combines low-rank and full-rank updates for optimal parameter efficiency.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// HRA addresses a key limitation of standard LoRA: while low-rank updates are efficient, some parameters\n+/// benefit from full-rank updates. HRA uses a hybrid approach:\n+/// - Dense low-rank updates for most parameters (efficient, like LoRA)\n+/// - Sparse full-rank updates for critical parameters (precise, targeted)\n+/// - Importance-based allocation between the two components\n+/// </para>\n+/// <para>\n+/// The forward computation is: output = base_layer(input) + low_rank(input) + sparse_full_rank(input)\n+/// where the hybrid allocation provides the best of both worlds.\n+/// </para>\n+/// <para><b>For Beginners:</b> HRA is like having two tools instead of one:\n+///\n+/// Standard LoRA problem:\n+/// - Uses only low-rank updates (compressed, efficient)\n+/// - Some parameters need precise full-rank updates\n+/// - Full fine-tuning is too expensive\n+/// - Need something in between\n+///\n+/// HRA solution:\n+/// - Most parameters use low-rank updates (efficient, covers 95% of needs)\n+/// - Critical parameters get full-rank updates (precise, covers remaining 5%)\n+/// - Automatically learns which parameters are critical\n+/// - Best quality with minimal parameter overhead\n+///\n+/// Analogy: Think of home renovation:\n+/// - Low-rank updates: Paint the walls (cheap, covers large area, good enough)\n+/// - Full-rank updates: Replace key structural beams (expensive, small area, critical)\n+/// - HRA: Do both where appropriate for best results\n+///\n+/// How it works:\n+/// 1. Start with LoRA-style low-rank matrices (B * A)\n+/// 2. Add sparse full-rank updates for most important parameters\n+/// 3. Track importance scores during training\n+/// 4. Allocate parameter budget optimally between low-rank and sparse full-rank\n+///\n+/// Benefits:\n+/// - Better quality than pure LoRA (full-rank updates where needed)\n+/// - More efficient than full fine-tuning (most updates are low-rank)\n+/// - Adaptive: learns which parameters need full-rank updates\n+/// - Flexible: adjustable sparsity budget for full-rank component\n+///\n+/// Use cases:\n+/// - Tasks where LoRA quality is not quite sufficient\n+/// - Fine-tuning with specific architectural bottlenecks\n+/// - When you have slightly more parameter budget than LoRA but much less than full fine-tuning\n+/// - Domains where certain parameters are known to be critical\n+///\n+/// Example parameter comparison for a 1000x1000 layer:\n+/// - Full fine-tuning: 1,000,000 parameters\n+/// - Standard LoRA (rank=8): 16,000 parameters (98.4% reduction)\n+/// - HRA (rank=8, 1% sparsity): 26,000 parameters (97.4% reduction, better quality)\n+///\n+/// Reference: Based on \"Hybrid Rank Adaptation\" research combining low-rank and sparse full-rank approaches\n+/// </para>\n+/// </remarks>\n+public class HRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Sparse full-rank update matrix storing only non-zero entries.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This dictionary maps (row, col) positions to their update values.\n+    /// Only the most important parameters have non-zero entries here.\n+    /// This provides targeted full-rank updates while maintaining parameter efficiency.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is like a selective paint touch-up kit.\n+    /// Instead of repainting the whole wall (full-rank), we only fix the important spots\n+    /// that need precise attention. The dictionary only stores the spots we're fixing,\n+    /// saving memory.\n+    /// </para>\n+    /// </remarks>\n+    private Dictionary<(int row, int col), T> _sparseFullRankUpdates;\n+\n+    /// <summary>\n+    /// Importance scores for each parameter in the weight matrix.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Each score represents how important that parameter is for the adaptation.\n+    /// Higher scores indicate parameters that should receive full-rank updates.\n+    /// Lower scores indicate parameters that are fine with low-rank updates.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> These scores tell us which parameters are VIPs.\n+    /// High score = this parameter is critical, give it a full-rank update.\n+    /// Low score = this parameter is fine with a low-rank approximation.\n+    /// </para>\n+    /// </remarks>\n+    private Matrix<T> _parameterImportance;\n+\n+    /// <summary>\n+    /// Gradient accumulator for the sparse full-rank component.\n+    /// </summary>\n+    private Dictionary<(int row, int col), T>? _sparseGradients;\n+\n+    /// <summary>\n+    /// Maximum number of sparse full-rank parameters to allocate.\n+    /// </summary>\n+    /// <remarks>\n+    /// Controls the parameter budget for the sparse full-rank component.\n+    /// Typical values: 1-5% of total weight parameters.\n+    /// </remarks>\n+    private readonly int _maxSparseParams;\n+\n+    /// <summary>\n+    /// Sparsity ratio for full-rank updates (0.0 to 1.0).\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Determines what fraction of parameters can receive full-rank updates.\n+    /// For example, 0.01 means 1% of parameters can have full-rank updates.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is your \"special attention budget\".\n+    /// If you have 1000 parameters and sparsity=0.01, you can give 10 parameters\n+    /// the VIP treatment (full-rank updates). Choose wisely!\n+    /// </para>\n+    /// </remarks>\n+    private readonly double _sparsityRatio;\n+\n+    /// <summary>\n+    /// Number of training steps between importance updates.\n+    /// </summary>\n+    private readonly int _importanceUpdateInterval;\n+\n+    /// <summary>\n+    /// Current training step counter.\n+    /// </summary>\n+    private int _stepCount;\n+\n+    /// <summary>\n+    /// Exponential moving average factor for importance score updates.\n+    /// </summary>\n+    /// <remarks>\n+    /// Controls how quickly importance scores adapt to new gradient information.\n+    /// Typical values: 0.9 to 0.99 (higher = more smoothing, lower = faster adaptation).\n+    /// </remarks>\n+    private readonly double _importanceEMA;\n+\n+    /// <summary>\n+    /// Scaling factor for the sparse full-rank component.\n+    /// </summary>\n+    private readonly T _sparseScaling;\n+\n+    /// <summary>\n+    /// Whether to use dynamic importance-based allocation.\n+    /// </summary>\n+    private readonly bool _useDynamicAllocation;\n+\n+    /// <summary>\n+    /// Gets the number of active sparse full-rank parameters.\n+    /// </summary>\n+    public int ActiveSparseParams => _sparseFullRankUpdates.Count;\n+\n+    /// <summary>\n+    /// Gets the maximum allowed sparse parameters.\n+    /// </summary>\n+    public int MaxSparseParams => _maxSparseParams;\n+\n+    /// <summary>\n+    /// Gets the current sparsity ratio.\n+    /// </summary>\n+    public double SparsityRatio => _sparsityRatio;\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters (low-rank + sparse full-rank).\n+    /// </summary>\n+    public override int ParameterCount\n+    {\n+        get\n+        {\n+            int loraParams = _loraLayer.ParameterCount;\n+            int sparseParams = _sparseFullRankUpdates.Count;\n+            int baseParams = _freezeBaseLayer ? 0 : _baseLayer.ParameterCount;\n+            return baseParams + loraParams + sparseParams;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Initializes a new HRA adapter with hybrid low-rank and sparse full-rank updates.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with HRA.</param>\n+    /// <param name=\"rank\">The rank of the low-rank decomposition.</param>\n+    /// <param name=\"sparsityRatio\">Fraction of parameters for sparse full-rank updates (0.0 to 1.0, default: 0.01).</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <param name=\"importanceUpdateInterval\">Steps between importance recalculation (default: 100).</param>\n+    /// <param name=\"importanceEMA\">EMA factor for importance smoothing (default: 0.95).</param>\n+    /// <param name=\"useDynamicAllocation\">Whether to dynamically reallocate sparse parameters (default: true).</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when parameters are invalid.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This creates an HRA adapter that combines two update strategies.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to adapt\n+    /// - rank: Size of the low-rank component (typical: 8-16)\n+    /// - sparsityRatio: Budget for full-rank updates (0.01 = 1% of parameters get special treatment)\n+    /// - alpha: Strength of the low-rank adaptation\n+    /// - freezeBaseLayer: Lock original weights (usually true)\n+    /// - importanceUpdateInterval: How often to reassess which parameters are important\n+    /// - importanceEMA: How stable importance scores are (higher = more stable)\n+    /// - useDynamicAllocation: Automatically move sparse budget to most important parameters\n+    ///\n+    /// Example:\n+    /// new HRAAdapter(layer, rank: 8, sparsityRatio: 0.01)\n+    /// This gives you LoRA-style updates for most parameters, plus precise updates for the top 1%.\n+    /// </para>\n+    /// </remarks>\n+    public HRAAdapter(\n+        ILayer<T> baseLayer,\n+        int rank,\n+        double sparsityRatio = 0.01,\n+        double alpha = -1,\n+        bool freezeBaseLayer = true,\n+        int importanceUpdateInterval = 100,\n+        double importanceEMA = 0.95,\n+        bool useDynamicAllocation = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        if (sparsityRatio < 0.0 || sparsityRatio > 1.0)\n+        {\n+            throw new ArgumentException(\"Sparsity ratio must be between 0 and 1\", nameof(sparsityRatio));\n+        }\n+\n+        if (importanceEMA <= 0 || importanceEMA >= 1)\n+        {\n+            throw new ArgumentException(\"Importance EMA factor must be between 0 and 1\", nameof(importanceEMA));\n+        }\n+\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        int totalWeightParams = inputSize * outputSize;\n+\n+        _sparsityRatio = sparsityRatio;\n+        _maxSparseParams = (int)(totalWeightParams * sparsityRatio);\n+        _importanceUpdateInterval = importanceUpdateInterval;\n+        _importanceEMA = importanceEMA;\n+        _useDynamicAllocation = useDynamicAllocation;\n+        _stepCount = 0;\n+\n+        // Initialize sparse full-rank updates (empty initially)\n+        _sparseFullRankUpdates = new Dictionary<(int row, int col), T>();\n+\n+        // Initialize importance scores (uniform initially)\n+        _parameterImportance = new Matrix<T>(outputSize, inputSize);\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                _parameterImportance[i, j] = NumOps.Zero;\n+            }\n+        }\n+\n+        // Sparse scaling factor (typically smaller than LoRA scaling)\n+        _sparseScaling = NumOps.FromDouble(0.1);\n+\n+        // Initialize parameters\n+        Parameters = new Vector<T>(ParameterCount);\n+        UpdateParametersFromComponents();\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass through the HRA adapter.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Sum of base layer output, low-rank LoRA output, and sparse full-rank output.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The HRA forward pass computes three components:\n+    /// 1. Base layer output (original behavior)\n+    /// 2. Low-rank LoRA output: scaling * B * A * input\n+    /// 3. Sparse full-rank output: sparse_scaling * S * input (where S is sparse)\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This processes input through three paths and adds them:\n+    /// 1. Original layer (base behavior)\n+    /// 2. LoRA low-rank path (efficient updates for most parameters)\n+    /// 3. Sparse full-rank path (precise updates for VIP parameters)\n+    ///\n+    /// Think of it as a team effort:\n+    /// - Base layer: The foundation\n+    /// - Low-rank: The general workforce (handles most of the load efficiently)\n+    /// - Sparse full-rank: The specialists (handle critical details precisely)\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        // 1. Forward through base layer\n+        Tensor<T> baseOutput = _baseLayer.Forward(input);\n+\n+        // 2. Forward through LoRA layer (low-rank component)\n+        Tensor<T> loraOutput = _loraLayer.Forward(input);\n+\n+        // 3. Forward through sparse full-rank component\n+        Tensor<T> sparseOutput = ForwardSparseFullRank(input);\n+\n+        // Sum all three components\n+        Tensor<T> result = new Tensor<T>(baseOutput.Shape);\n+        for (int i = 0; i < baseOutput.Length; i++)\n+        {\n+            T sum = NumOps.Add(baseOutput[i], loraOutput[i]);\n+            result[i] = NumOps.Add(sum, sparseOutput[i]);\n+        }\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Performs forward pass through the sparse full-rank component.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Sparse full-rank output tensor.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Computes output using only the sparse full-rank parameters.\n+    /// This is a standard matrix multiplication but using a sparse weight matrix.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This applies the \"specialist\" updates.\n+    /// Only the VIP parameters (stored in _sparseFullRankUpdates) are used here.\n+    /// Everything else is treated as zero, maintaining efficiency.\n+    /// </para>\n+    /// </remarks>\n+    private Tensor<T> ForwardSparseFullRank(Tensor<T> input)\n+    {\n+        int batchSize = input.Shape[0];\n+        int inputSize = input.Shape.Length > 1 ? input.Shape[1] : input.Length;\n+        int outputSize = GetOutputShape()[0];\n+\n+        // If no sparse parameters, return zeros\n+        if (_sparseFullRankUpdates.Count == 0)\n+        {\n+            Vector<T> zeroData = new Vector<T>(batchSize * outputSize);\n+            return new Tensor<T>(new[] { batchSize, outputSize }, zeroData);\n+        }\n+\n+        // Convert input to matrix [batchSize, inputSize]\n+        Matrix<T> inputMatrix = new Matrix<T>(batchSize, inputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                inputMatrix[i, j] = input[i * inputSize + j];\n+            }\n+        }\n+\n+        // Compute sparse matrix multiplication\n+        Matrix<T> output = new Matrix<T>(batchSize, outputSize);\n+        foreach (var kvp in _sparseFullRankUpdates)\n+        {\n+            int row = kvp.Key.row;\n+            int col = kvp.Key.col;\n+            T weight = NumOps.Multiply(kvp.Value, _sparseScaling);\n+\n+            // output[b, row] += weight * input[b, col]\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                T contribution = NumOps.Multiply(weight, inputMatrix[b, col]);\n+                output[b, row] = NumOps.Add(output[b, row], contribution);\n+            }\n+        }\n+\n+        // Convert back to tensor\n+        Vector<T> outputData = new Vector<T>(batchSize * outputSize);\n+        int idx = 0;\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                outputData[idx++] = output[i, j];\n+            }\n+        }\n+\n+        return new Tensor<T>(new[] { batchSize, outputSize }, outputData);\n+    }\n+\n+    /// <summary>\n+    /// Performs the backward pass through the HRA adapter.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient flowing back from the next layer.</param>\n+    /// <returns>Gradient to pass to the previous layer.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The backward pass computes gradients for:\n+    /// 1. Low-rank LoRA matrices (A and B)\n+    /// 2. Sparse full-rank parameters\n+    /// 3. Updates importance scores based on gradient magnitudes\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is where HRA learns which parameters are important!\n+    /// During backpropagation:\n+    /// 1. Compute gradients for low-rank component (standard LoRA)\n+    /// 2. Compute gradients for sparse full-rank parameters\n+    /// 3. Track which parameters have large gradients (they're important!)\n+    /// 4. Periodically reassign sparse budget to most important parameters\n+    ///\n+    /// This adaptive approach ensures the sparse full-rank budget is always\n+    /// allocated to the parameters that need it most.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        // Backward through LoRA layer\n+        Tensor<T> loraInputGrad = _loraLayer.Backward(outputGradient);\n+\n+        // Backward through sparse full-rank component\n+        Tensor<T> sparseInputGrad = BackwardSparseFullRank(outputGradient);\n+\n+        // Backward through base layer\n+        Tensor<T> baseInputGrad = _baseLayer.Backward(outputGradient);\n+\n+        // Update importance scores based on gradients\n+        UpdateImportanceScores(outputGradient);\n+\n+        // Increment step and check if we should reallocate sparse parameters\n+        _stepCount++;\n+        if (_useDynamicAllocation && _stepCount % _importanceUpdateInterval == 0)\n+        {\n+            ReallocateSparseParameters();\n+        }\n+\n+        // Sum input gradients\n+        Tensor<T> inputGrad = new Tensor<T>(loraInputGrad.Shape);\n+        for (int i = 0; i < loraInputGrad.Length; i++)\n+        {\n+            T sum = NumOps.Add(loraInputGrad[i], sparseInputGrad[i]);\n+            inputGrad[i] = NumOps.Add(sum, baseInputGrad[i]);\n+        }\n+\n+        return inputGrad;\n+    }\n+\n+    /// <summary>\n+    /// Performs backward pass through the sparse full-rank component.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Output gradient tensor.</param>\n+    /// <returns>Input gradient tensor.</returns>\n+    private Tensor<T> BackwardSparseFullRank(Tensor<T> outputGradient)\n+    {\n+        int batchSize = outputGradient.Shape[0];\n+        int outputSize = outputGradient.Shape.Length > 1 ? outputGradient.Shape[1] : outputGradient.Length;\n+        int inputSize = GetInputShape()[0];\n+\n+        // Initialize sparse gradients\n+        _sparseGradients = new Dictionary<(int row, int col), T>();\n+\n+        // If no sparse parameters, return zeros\n+        if (_sparseFullRankUpdates.Count == 0)\n+        {\n+            Vector<T> zeroData = new Vector<T>(batchSize * inputSize);\n+            return new Tensor<T>(new[] { batchSize, inputSize }, zeroData);\n+        }\n+\n+        // Convert gradient to matrix\n+        Matrix<T> gradMatrix = new Matrix<T>(batchSize, outputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                gradMatrix[i, j] = outputGradient[i * outputSize + j];\n+            }\n+        }\n+\n+        // Compute input gradients and parameter gradients\n+        Matrix<T> inputGradMatrix = new Matrix<T>(batchSize, inputSize);\n+\n+        foreach (var kvp in _sparseFullRankUpdates)\n+        {\n+            int row = kvp.Key.row;\n+            int col = kvp.Key.col;\n+            T weight = NumOps.Multiply(kvp.Value, _sparseScaling);\n+\n+            T paramGrad = NumOps.Zero;\n+\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                // Input gradient: dL/dInput[b, col] += weight * dL/dOutput[b, row]\n+                T grad = NumOps.Multiply(weight, gradMatrix[b, row]);\n+                inputGradMatrix[b, col] = NumOps.Add(inputGradMatrix[b, col], grad);\n+\n+                // Parameter gradient: dL/dWeight[row, col] += input[b, col] * dL/dOutput[b, row]\n+                // Note: We need input from forward pass, stored in base layer\n+                // For simplicity, accumulate gradient magnitude for importance\n+                paramGrad = NumOps.Add(paramGrad, NumOps.Abs(gradMatrix[b, row]));\n+            }\n+\n+            _sparseGradients[kvp.Key] = NumOps.Multiply(paramGrad, _sparseScaling);\n+        }\n+\n+        // Convert input gradients back to tensor\n+        Vector<T> inputGradData = new Vector<T>(batchSize * inputSize);\n+        int idx = 0;\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                inputGradData[idx++] = inputGradMatrix[i, j];\n+            }\n+        }","path":"src/NeuralNetworks/Layers/HRAAdapter.cs","commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","original_commit_id":"c11235933bf7c341c8b4a223cf6ac11b757d1480","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Sparse gradient drops the input term and is always non‚Äënegative.**\n\n`BackwardSparseFullRank` accumulates `_sparseGradients` from `|‚àÇL/‚àÇy|` only. It never multiplies by the stored inputs, so the update ignores the sign from the activations and becomes non-negative, pushing every sparse weight monotonically downward. For a linear layer the correct gradient is `Œ£_b (‚àÇL/‚àÇy_{b,row}) * x_{b,col}`, i.e., an outer product of output error and input.([little-book-of.github.io](https://little-book-of.github.io/matrix-calculus/?utm_source=openai))\n\nPlease cache the last input in `Forward`, and compute:\n\n```csharp\nparamGrad = NumOps.Add(\n    paramGrad,\n    NumOps.Multiply(\n        gradMatrix[b, row],\n        _lastInput[b * inputSize + col]));\n```\n\nthen store that in `_sparseGradients`. Without this fix the sparse updates never move toward the optimum.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/Layers/HRAAdapter.cs around lines 483 to 507, the backward\npass accumulates _sparseGradients using only the absolute value of output\ngradients (|‚àÇL/‚àÇy|) so updates ignore the input sign and are always\nnon-negative; fix by caching the flattened input tensor in Forward (e.g.\n_lastInput Vector<T> sized batchSize*inputSize) and in BackwardSparseFullRank\nmultiply each gradMatrix[b,row] by the corresponding cached input entry for col\nwhen accumulating paramGrad (paramGrad += gradMatrix[b,row] * _lastInput[b *\ninputSize + col]), then store NumOps.Multiply(paramGrad, _sparseScaling) into\n_sparseGradients so sparse updates use the correct signed outer-product\ngradient.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:32:36Z","updated_at":"2025-11-02T02:32:40Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106268","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106268"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106268"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106268/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":483,"original_start_line":483,"start_side":"RIGHT","line":507,"original_line":507,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":507,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106277","pull_request_review_id":3407993158,"id":2484106277,"node_id":"PRRC_kwDOKSXUF86UEHQl","diff_hunk":"@@ -0,0 +1,819 @@\n+using AiDotNet.Interfaces;\n+using System.Collections.Generic;\n+\n+namespace AiDotNet.NeuralNetworks.Layers;\n+\n+/// <summary>\n+/// HRA (Hybrid Rank Adaptation) adapter that combines low-rank and full-rank updates for optimal parameter efficiency.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// HRA addresses a key limitation of standard LoRA: while low-rank updates are efficient, some parameters\n+/// benefit from full-rank updates. HRA uses a hybrid approach:\n+/// - Dense low-rank updates for most parameters (efficient, like LoRA)\n+/// - Sparse full-rank updates for critical parameters (precise, targeted)\n+/// - Importance-based allocation between the two components\n+/// </para>\n+/// <para>\n+/// The forward computation is: output = base_layer(input) + low_rank(input) + sparse_full_rank(input)\n+/// where the hybrid allocation provides the best of both worlds.\n+/// </para>\n+/// <para><b>For Beginners:</b> HRA is like having two tools instead of one:\n+///\n+/// Standard LoRA problem:\n+/// - Uses only low-rank updates (compressed, efficient)\n+/// - Some parameters need precise full-rank updates\n+/// - Full fine-tuning is too expensive\n+/// - Need something in between\n+///\n+/// HRA solution:\n+/// - Most parameters use low-rank updates (efficient, covers 95% of needs)\n+/// - Critical parameters get full-rank updates (precise, covers remaining 5%)\n+/// - Automatically learns which parameters are critical\n+/// - Best quality with minimal parameter overhead\n+///\n+/// Analogy: Think of home renovation:\n+/// - Low-rank updates: Paint the walls (cheap, covers large area, good enough)\n+/// - Full-rank updates: Replace key structural beams (expensive, small area, critical)\n+/// - HRA: Do both where appropriate for best results\n+///\n+/// How it works:\n+/// 1. Start with LoRA-style low-rank matrices (B * A)\n+/// 2. Add sparse full-rank updates for most important parameters\n+/// 3. Track importance scores during training\n+/// 4. Allocate parameter budget optimally between low-rank and sparse full-rank\n+///\n+/// Benefits:\n+/// - Better quality than pure LoRA (full-rank updates where needed)\n+/// - More efficient than full fine-tuning (most updates are low-rank)\n+/// - Adaptive: learns which parameters need full-rank updates\n+/// - Flexible: adjustable sparsity budget for full-rank component\n+///\n+/// Use cases:\n+/// - Tasks where LoRA quality is not quite sufficient\n+/// - Fine-tuning with specific architectural bottlenecks\n+/// - When you have slightly more parameter budget than LoRA but much less than full fine-tuning\n+/// - Domains where certain parameters are known to be critical\n+///\n+/// Example parameter comparison for a 1000x1000 layer:\n+/// - Full fine-tuning: 1,000,000 parameters\n+/// - Standard LoRA (rank=8): 16,000 parameters (98.4% reduction)\n+/// - HRA (rank=8, 1% sparsity): 26,000 parameters (97.4% reduction, better quality)\n+///\n+/// Reference: Based on \"Hybrid Rank Adaptation\" research combining low-rank and sparse full-rank approaches\n+/// </para>\n+/// </remarks>\n+public class HRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Sparse full-rank update matrix storing only non-zero entries.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This dictionary maps (row, col) positions to their update values.\n+    /// Only the most important parameters have non-zero entries here.\n+    /// This provides targeted full-rank updates while maintaining parameter efficiency.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is like a selective paint touch-up kit.\n+    /// Instead of repainting the whole wall (full-rank), we only fix the important spots\n+    /// that need precise attention. The dictionary only stores the spots we're fixing,\n+    /// saving memory.\n+    /// </para>\n+    /// </remarks>\n+    private Dictionary<(int row, int col), T> _sparseFullRankUpdates;\n+\n+    /// <summary>\n+    /// Importance scores for each parameter in the weight matrix.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Each score represents how important that parameter is for the adaptation.\n+    /// Higher scores indicate parameters that should receive full-rank updates.\n+    /// Lower scores indicate parameters that are fine with low-rank updates.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> These scores tell us which parameters are VIPs.\n+    /// High score = this parameter is critical, give it a full-rank update.\n+    /// Low score = this parameter is fine with a low-rank approximation.\n+    /// </para>\n+    /// </remarks>\n+    private Matrix<T> _parameterImportance;\n+\n+    /// <summary>\n+    /// Gradient accumulator for the sparse full-rank component.\n+    /// </summary>\n+    private Dictionary<(int row, int col), T>? _sparseGradients;\n+\n+    /// <summary>\n+    /// Maximum number of sparse full-rank parameters to allocate.\n+    /// </summary>\n+    /// <remarks>\n+    /// Controls the parameter budget for the sparse full-rank component.\n+    /// Typical values: 1-5% of total weight parameters.\n+    /// </remarks>\n+    private readonly int _maxSparseParams;\n+\n+    /// <summary>\n+    /// Sparsity ratio for full-rank updates (0.0 to 1.0).\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Determines what fraction of parameters can receive full-rank updates.\n+    /// For example, 0.01 means 1% of parameters can have full-rank updates.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is your \"special attention budget\".\n+    /// If you have 1000 parameters and sparsity=0.01, you can give 10 parameters\n+    /// the VIP treatment (full-rank updates). Choose wisely!\n+    /// </para>\n+    /// </remarks>\n+    private readonly double _sparsityRatio;\n+\n+    /// <summary>\n+    /// Number of training steps between importance updates.\n+    /// </summary>\n+    private readonly int _importanceUpdateInterval;\n+\n+    /// <summary>\n+    /// Current training step counter.\n+    /// </summary>\n+    private int _stepCount;\n+\n+    /// <summary>\n+    /// Exponential moving average factor for importance score updates.\n+    /// </summary>\n+    /// <remarks>\n+    /// Controls how quickly importance scores adapt to new gradient information.\n+    /// Typical values: 0.9 to 0.99 (higher = more smoothing, lower = faster adaptation).\n+    /// </remarks>\n+    private readonly double _importanceEMA;\n+\n+    /// <summary>\n+    /// Scaling factor for the sparse full-rank component.\n+    /// </summary>\n+    private readonly T _sparseScaling;\n+\n+    /// <summary>\n+    /// Whether to use dynamic importance-based allocation.\n+    /// </summary>\n+    private readonly bool _useDynamicAllocation;\n+\n+    /// <summary>\n+    /// Gets the number of active sparse full-rank parameters.\n+    /// </summary>\n+    public int ActiveSparseParams => _sparseFullRankUpdates.Count;\n+\n+    /// <summary>\n+    /// Gets the maximum allowed sparse parameters.\n+    /// </summary>\n+    public int MaxSparseParams => _maxSparseParams;\n+\n+    /// <summary>\n+    /// Gets the current sparsity ratio.\n+    /// </summary>\n+    public double SparsityRatio => _sparsityRatio;\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters (low-rank + sparse full-rank).\n+    /// </summary>\n+    public override int ParameterCount\n+    {\n+        get\n+        {\n+            int loraParams = _loraLayer.ParameterCount;\n+            int sparseParams = _sparseFullRankUpdates.Count;\n+            int baseParams = _freezeBaseLayer ? 0 : _baseLayer.ParameterCount;\n+            return baseParams + loraParams + sparseParams;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Initializes a new HRA adapter with hybrid low-rank and sparse full-rank updates.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with HRA.</param>\n+    /// <param name=\"rank\">The rank of the low-rank decomposition.</param>\n+    /// <param name=\"sparsityRatio\">Fraction of parameters for sparse full-rank updates (0.0 to 1.0, default: 0.01).</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <param name=\"importanceUpdateInterval\">Steps between importance recalculation (default: 100).</param>\n+    /// <param name=\"importanceEMA\">EMA factor for importance smoothing (default: 0.95).</param>\n+    /// <param name=\"useDynamicAllocation\">Whether to dynamically reallocate sparse parameters (default: true).</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when parameters are invalid.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This creates an HRA adapter that combines two update strategies.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to adapt\n+    /// - rank: Size of the low-rank component (typical: 8-16)\n+    /// - sparsityRatio: Budget for full-rank updates (0.01 = 1% of parameters get special treatment)\n+    /// - alpha: Strength of the low-rank adaptation\n+    /// - freezeBaseLayer: Lock original weights (usually true)\n+    /// - importanceUpdateInterval: How often to reassess which parameters are important\n+    /// - importanceEMA: How stable importance scores are (higher = more stable)\n+    /// - useDynamicAllocation: Automatically move sparse budget to most important parameters\n+    ///\n+    /// Example:\n+    /// new HRAAdapter(layer, rank: 8, sparsityRatio: 0.01)\n+    /// This gives you LoRA-style updates for most parameters, plus precise updates for the top 1%.\n+    /// </para>\n+    /// </remarks>\n+    public HRAAdapter(\n+        ILayer<T> baseLayer,\n+        int rank,\n+        double sparsityRatio = 0.01,\n+        double alpha = -1,\n+        bool freezeBaseLayer = true,\n+        int importanceUpdateInterval = 100,\n+        double importanceEMA = 0.95,\n+        bool useDynamicAllocation = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        if (sparsityRatio < 0.0 || sparsityRatio > 1.0)\n+        {\n+            throw new ArgumentException(\"Sparsity ratio must be between 0 and 1\", nameof(sparsityRatio));\n+        }\n+\n+        if (importanceEMA <= 0 || importanceEMA >= 1)\n+        {\n+            throw new ArgumentException(\"Importance EMA factor must be between 0 and 1\", nameof(importanceEMA));\n+        }\n+\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        int totalWeightParams = inputSize * outputSize;\n+\n+        _sparsityRatio = sparsityRatio;\n+        _maxSparseParams = (int)(totalWeightParams * sparsityRatio);\n+        _importanceUpdateInterval = importanceUpdateInterval;\n+        _importanceEMA = importanceEMA;\n+        _useDynamicAllocation = useDynamicAllocation;\n+        _stepCount = 0;\n+\n+        // Initialize sparse full-rank updates (empty initially)\n+        _sparseFullRankUpdates = new Dictionary<(int row, int col), T>();\n+\n+        // Initialize importance scores (uniform initially)\n+        _parameterImportance = new Matrix<T>(outputSize, inputSize);\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                _parameterImportance[i, j] = NumOps.Zero;\n+            }\n+        }\n+\n+        // Sparse scaling factor (typically smaller than LoRA scaling)\n+        _sparseScaling = NumOps.FromDouble(0.1);\n+\n+        // Initialize parameters\n+        Parameters = new Vector<T>(ParameterCount);\n+        UpdateParametersFromComponents();\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass through the HRA adapter.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Sum of base layer output, low-rank LoRA output, and sparse full-rank output.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The HRA forward pass computes three components:\n+    /// 1. Base layer output (original behavior)\n+    /// 2. Low-rank LoRA output: scaling * B * A * input\n+    /// 3. Sparse full-rank output: sparse_scaling * S * input (where S is sparse)\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This processes input through three paths and adds them:\n+    /// 1. Original layer (base behavior)\n+    /// 2. LoRA low-rank path (efficient updates for most parameters)\n+    /// 3. Sparse full-rank path (precise updates for VIP parameters)\n+    ///\n+    /// Think of it as a team effort:\n+    /// - Base layer: The foundation\n+    /// - Low-rank: The general workforce (handles most of the load efficiently)\n+    /// - Sparse full-rank: The specialists (handle critical details precisely)\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        // 1. Forward through base layer\n+        Tensor<T> baseOutput = _baseLayer.Forward(input);\n+\n+        // 2. Forward through LoRA layer (low-rank component)\n+        Tensor<T> loraOutput = _loraLayer.Forward(input);\n+\n+        // 3. Forward through sparse full-rank component\n+        Tensor<T> sparseOutput = ForwardSparseFullRank(input);\n+\n+        // Sum all three components\n+        Tensor<T> result = new Tensor<T>(baseOutput.Shape);\n+        for (int i = 0; i < baseOutput.Length; i++)\n+        {\n+            T sum = NumOps.Add(baseOutput[i], loraOutput[i]);\n+            result[i] = NumOps.Add(sum, sparseOutput[i]);\n+        }\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Performs forward pass through the sparse full-rank component.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Sparse full-rank output tensor.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Computes output using only the sparse full-rank parameters.\n+    /// This is a standard matrix multiplication but using a sparse weight matrix.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This applies the \"specialist\" updates.\n+    /// Only the VIP parameters (stored in _sparseFullRankUpdates) are used here.\n+    /// Everything else is treated as zero, maintaining efficiency.\n+    /// </para>\n+    /// </remarks>\n+    private Tensor<T> ForwardSparseFullRank(Tensor<T> input)\n+    {\n+        int batchSize = input.Shape[0];\n+        int inputSize = input.Shape.Length > 1 ? input.Shape[1] : input.Length;\n+        int outputSize = GetOutputShape()[0];\n+\n+        // If no sparse parameters, return zeros\n+        if (_sparseFullRankUpdates.Count == 0)\n+        {\n+            Vector<T> zeroData = new Vector<T>(batchSize * outputSize);\n+            return new Tensor<T>(new[] { batchSize, outputSize }, zeroData);\n+        }\n+\n+        // Convert input to matrix [batchSize, inputSize]\n+        Matrix<T> inputMatrix = new Matrix<T>(batchSize, inputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                inputMatrix[i, j] = input[i * inputSize + j];\n+            }\n+        }\n+\n+        // Compute sparse matrix multiplication\n+        Matrix<T> output = new Matrix<T>(batchSize, outputSize);\n+        foreach (var kvp in _sparseFullRankUpdates)\n+        {\n+            int row = kvp.Key.row;\n+            int col = kvp.Key.col;\n+            T weight = NumOps.Multiply(kvp.Value, _sparseScaling);\n+\n+            // output[b, row] += weight * input[b, col]\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                T contribution = NumOps.Multiply(weight, inputMatrix[b, col]);\n+                output[b, row] = NumOps.Add(output[b, row], contribution);\n+            }\n+        }\n+\n+        // Convert back to tensor\n+        Vector<T> outputData = new Vector<T>(batchSize * outputSize);\n+        int idx = 0;\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                outputData[idx++] = output[i, j];\n+            }\n+        }\n+\n+        return new Tensor<T>(new[] { batchSize, outputSize }, outputData);\n+    }\n+\n+    /// <summary>\n+    /// Performs the backward pass through the HRA adapter.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient flowing back from the next layer.</param>\n+    /// <returns>Gradient to pass to the previous layer.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The backward pass computes gradients for:\n+    /// 1. Low-rank LoRA matrices (A and B)\n+    /// 2. Sparse full-rank parameters\n+    /// 3. Updates importance scores based on gradient magnitudes\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is where HRA learns which parameters are important!\n+    /// During backpropagation:\n+    /// 1. Compute gradients for low-rank component (standard LoRA)\n+    /// 2. Compute gradients for sparse full-rank parameters\n+    /// 3. Track which parameters have large gradients (they're important!)\n+    /// 4. Periodically reassign sparse budget to most important parameters\n+    ///\n+    /// This adaptive approach ensures the sparse full-rank budget is always\n+    /// allocated to the parameters that need it most.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        // Backward through LoRA layer\n+        Tensor<T> loraInputGrad = _loraLayer.Backward(outputGradient);\n+\n+        // Backward through sparse full-rank component\n+        Tensor<T> sparseInputGrad = BackwardSparseFullRank(outputGradient);\n+\n+        // Backward through base layer\n+        Tensor<T> baseInputGrad = _baseLayer.Backward(outputGradient);\n+\n+        // Update importance scores based on gradients\n+        UpdateImportanceScores(outputGradient);\n+\n+        // Increment step and check if we should reallocate sparse parameters\n+        _stepCount++;\n+        if (_useDynamicAllocation && _stepCount % _importanceUpdateInterval == 0)\n+        {\n+            ReallocateSparseParameters();\n+        }\n+\n+        // Sum input gradients\n+        Tensor<T> inputGrad = new Tensor<T>(loraInputGrad.Shape);\n+        for (int i = 0; i < loraInputGrad.Length; i++)\n+        {\n+            T sum = NumOps.Add(loraInputGrad[i], sparseInputGrad[i]);\n+            inputGrad[i] = NumOps.Add(sum, baseInputGrad[i]);\n+        }\n+\n+        return inputGrad;\n+    }\n+\n+    /// <summary>\n+    /// Performs backward pass through the sparse full-rank component.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Output gradient tensor.</param>\n+    /// <returns>Input gradient tensor.</returns>\n+    private Tensor<T> BackwardSparseFullRank(Tensor<T> outputGradient)\n+    {\n+        int batchSize = outputGradient.Shape[0];\n+        int outputSize = outputGradient.Shape.Length > 1 ? outputGradient.Shape[1] : outputGradient.Length;\n+        int inputSize = GetInputShape()[0];\n+\n+        // Initialize sparse gradients\n+        _sparseGradients = new Dictionary<(int row, int col), T>();\n+\n+        // If no sparse parameters, return zeros\n+        if (_sparseFullRankUpdates.Count == 0)\n+        {\n+            Vector<T> zeroData = new Vector<T>(batchSize * inputSize);\n+            return new Tensor<T>(new[] { batchSize, inputSize }, zeroData);\n+        }\n+\n+        // Convert gradient to matrix\n+        Matrix<T> gradMatrix = new Matrix<T>(batchSize, outputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                gradMatrix[i, j] = outputGradient[i * outputSize + j];\n+            }\n+        }\n+\n+        // Compute input gradients and parameter gradients\n+        Matrix<T> inputGradMatrix = new Matrix<T>(batchSize, inputSize);\n+\n+        foreach (var kvp in _sparseFullRankUpdates)\n+        {\n+            int row = kvp.Key.row;\n+            int col = kvp.Key.col;\n+            T weight = NumOps.Multiply(kvp.Value, _sparseScaling);\n+\n+            T paramGrad = NumOps.Zero;\n+\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                // Input gradient: dL/dInput[b, col] += weight * dL/dOutput[b, row]\n+                T grad = NumOps.Multiply(weight, gradMatrix[b, row]);\n+                inputGradMatrix[b, col] = NumOps.Add(inputGradMatrix[b, col], grad);\n+\n+                // Parameter gradient: dL/dWeight[row, col] += input[b, col] * dL/dOutput[b, row]\n+                // Note: We need input from forward pass, stored in base layer\n+                // For simplicity, accumulate gradient magnitude for importance\n+                paramGrad = NumOps.Add(paramGrad, NumOps.Abs(gradMatrix[b, row]));\n+            }\n+\n+            _sparseGradients[kvp.Key] = NumOps.Multiply(paramGrad, _sparseScaling);\n+        }\n+\n+        // Convert input gradients back to tensor\n+        Vector<T> inputGradData = new Vector<T>(batchSize * inputSize);\n+        int idx = 0;\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                inputGradData[idx++] = inputGradMatrix[i, j];\n+            }\n+        }\n+\n+        return new Tensor<T>(new[] { batchSize, inputSize }, inputGradData);\n+    }\n+\n+    /// <summary>\n+    /// Updates importance scores based on current gradient magnitudes.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Output gradient from backward pass.</param>\n+    /// <remarks>\n+    /// <para>\n+    /// Importance is computed using exponential moving average of gradient magnitudes.\n+    /// Parameters with consistently high gradients are considered important candidates\n+    /// for sparse full-rank updates.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This identifies which parameters are VIPs.\n+    ///\n+    /// We track gradient magnitudes over time using exponential moving average:\n+    /// - new_importance = 0.95 * old_importance + 0.05 * current_gradient_magnitude\n+    ///\n+    /// Parameters with consistently high gradients get high importance scores.\n+    /// These are the ones that will receive sparse full-rank updates.\n+    /// </para>\n+    /// </remarks>\n+    private void UpdateImportanceScores(Tensor<T> outputGradient)\n+    {\n+        int outputSize = GetOutputShape()[0];\n+        int inputSize = GetInputShape()[0];\n+\n+        // Get LoRA parameter gradients to estimate per-parameter importance\n+        Vector<T> loraGradients = _loraLayer.GetParameterGradients();\n+\n+        // Update importance based on gradient flow through LoRA component\n+        // This is a proxy for which parameters would benefit from full-rank updates\n+        Matrix<T> matrixA = _loraLayer.GetMatrixA();\n+        Matrix<T> matrixB = _loraLayer.GetMatrixB();\n+\n+        T emaFactor = NumOps.FromDouble(_importanceEMA);\n+        T oneMinusEma = NumOps.FromDouble(1.0 - _importanceEMA);\n+\n+        // Estimate per-parameter importance from LoRA gradients\n+        // Higher LoRA gradients suggest that parameter needs more capacity\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                // Compute approximate gradient magnitude for this weight\n+                // by looking at contributions through LoRA paths\n+                T gradMagnitude = NumOps.Zero;\n+\n+                // Sum contributions from all rank components\n+                int rank = Rank;\n+                for (int r = 0; r < rank; r++)\n+                {\n+                    // Gradient flows through A[j,r] and B[r,i]\n+                    int aIndex = j * rank + r;\n+                    int bIndex = r * outputSize + i;\n+\n+                    if (aIndex < loraGradients.Length && bIndex < loraGradients.Length)\n+                    {\n+                        T contribution = NumOps.Multiply(\n+                            NumOps.Abs(loraGradients[aIndex]),\n+                            NumOps.Abs(loraGradients[bIndex]));\n+                        gradMagnitude = NumOps.Add(gradMagnitude, contribution);\n+                    }\n+                }\n+\n+                // Update importance with EMA\n+                T oldImportance = _parameterImportance[i, j];\n+                T newImportance = NumOps.Add(\n+                    NumOps.Multiply(emaFactor, oldImportance),\n+                    NumOps.Multiply(oneMinusEma, gradMagnitude));\n+\n+                _parameterImportance[i, j] = newImportance;\n+            }\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Reallocates sparse full-rank parameters to the most important locations.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This method identifies the top-k most important parameters and assigns\n+    /// sparse full-rank updates to them. Previously allocated parameters that\n+    /// are no longer in the top-k are removed.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is like reassigning specialists to where they're needed most.\n+    ///\n+    /// Every few hundred training steps:\n+    /// 1. Look at all importance scores\n+    /// 2. Find the top 1% most important parameters\n+    /// 3. Assign sparse full-rank budget to those parameters\n+    /// 4. Remove it from parameters that are no longer important\n+    ///\n+    /// This ensures the sparse budget is always optimally allocated.\n+    /// </para>\n+    /// </remarks>\n+    private void ReallocateSparseParameters()\n+    {\n+        int outputSize = GetOutputShape()[0];\n+        int inputSize = GetInputShape()[0];\n+\n+        // Create list of (importance, position) pairs\n+        var importanceList = new List<(T importance, int row, int col)>();\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                importanceList.Add((_parameterImportance[i, j], i, j));\n+            }\n+        }\n+\n+        // Sort by importance (descending)\n+        importanceList.Sort((a, b) =>\n+            Convert.ToDouble(b.importance).CompareTo(Convert.ToDouble(a.importance)));\n+\n+        // Select top-k positions for sparse full-rank updates\n+        var newSparseUpdates = new Dictionary<(int row, int col), T>();\n+        for (int i = 0; i < Math.Min(_maxSparseParams, importanceList.Count); i++)\n+        {\n+            var entry = importanceList[i];\n+            var key = (entry.row, entry.col);\n+\n+            // Preserve existing values if already allocated, otherwise initialize small random\n+            if (_sparseFullRankUpdates.ContainsKey(key))\n+            {\n+                newSparseUpdates[key] = _sparseFullRankUpdates[key];\n+            }\n+            else\n+            {\n+                // Initialize new sparse parameter with small random value\n+                Random rng = new Random();\n+                double randVal = (rng.NextDouble() - 0.5) * 0.02; // Small initialization\n+                newSparseUpdates[key] = NumOps.FromDouble(randVal);\n+            }\n+        }\n+\n+        _sparseFullRankUpdates = newSparseUpdates;\n+    }\n+\n+    /// <summary>\n+    /// Updates parameters using the specified learning rate.\n+    /// </summary>\n+    /// <param name=\"learningRate\">The learning rate for parameter updates.</param>\n+    public override void UpdateParameters(T learningRate)\n+    {\n+        // Update LoRA layer\n+        _loraLayer.UpdateParameters(learningRate);\n+\n+        // Update sparse full-rank parameters\n+        if (_sparseGradients != null)\n+        {\n+            var updatedSparse = new Dictionary<(int row, int col), T>();\n+            foreach (var kvp in _sparseFullRankUpdates)\n+            {\n+                T currentValue = kvp.Value;\n+                T gradient = _sparseGradients.ContainsKey(kvp.Key) ? _sparseGradients[kvp.Key] : NumOps.Zero;\n+                T update = NumOps.Multiply(gradient, learningRate);\n+                T newValue = NumOps.Subtract(currentValue, update);\n+                updatedSparse[kvp.Key] = newValue;\n+            }\n+            _sparseFullRankUpdates = updatedSparse;\n+        }\n+\n+        // Update base layer if not frozen\n+        if (!_freezeBaseLayer)\n+        {\n+            _baseLayer.UpdateParameters(learningRate);\n+        }\n+\n+        // Update parameter vector\n+        UpdateParametersFromComponents();\n+    }\n+\n+    /// <summary>\n+    /// Updates the parameter vector from the current component states.\n+    /// </summary>\n+    private void UpdateParametersFromComponents()\n+    {\n+        Parameters = new Vector<T>(ParameterCount);\n+        int idx = 0;\n+\n+        // Pack base layer parameters if not frozen\n+        if (!_freezeBaseLayer)\n+        {\n+            Vector<T> baseParams = _baseLayer.GetParameters();\n+            for (int i = 0; i < baseParams.Length; i++)\n+            {\n+                Parameters[idx++] = baseParams[i];\n+            }\n+        }\n+\n+        // Pack LoRA parameters\n+        Vector<T> loraParams = _loraLayer.GetParameters();\n+        for (int i = 0; i < loraParams.Length; i++)\n+        {\n+            Parameters[idx++] = loraParams[i];\n+        }\n+\n+        // Pack sparse parameters (just the values, positions are implicit)\n+        foreach (var kvp in _sparseFullRankUpdates)\n+        {\n+            Parameters[idx++] = kvp.Value;\n+        }\n+    }","path":"src/NeuralNetworks/Layers/HRAAdapter.cs","commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","original_commit_id":"c11235933bf7c341c8b4a223cf6ac11b757d1480","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**SetParameters drops every sparse update on load.**\n\n`ParameterCount` includes the sparse values, but `SetParameters` delegates to `LoRAAdapterBase.SetParameters`, which only unpacks the base and LoRA segments. The trailing sparse entries are ignored, so any deserialization/reset wipes out the sparse state while keeping the old dictionary contents. This breaks checkpoint reloads and optimizer restores.\n\nPlease override `SetParameters` (and the inverse helper) to unpack/pack `_sparseFullRankUpdates` explicitly‚Äîfor example, iterate over the dictionary keys after the base/LoRA segments and assign their values from the parameter vector.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/Layers/HRAAdapter.cs around lines 685 to 712, the current\nSetParameters path (inherited from LoRAAdapterBase) only unpacks base and LoRA\nsegments and thus drops the trailing sparse entries included in ParameterCount;\noverride SetParameters (and the corresponding pack helper) in HRAAdapter to\nexplicitly unpack/pack _sparseFullRankUpdates: compute the starting index after\nbase (respecting _freezeBaseLayer) and LoRA segments, then iterate the sparse\ndictionary in the same iteration order used by UpdateParametersFromComponents\n(e.g., foreach over _sparseFullRankUpdates.Keys or kvp) and assign\nParameters[idx++] to each dictionary entry‚Äôs Value when loading, and conversely\nwrite each entry‚Äôs Value into the Parameters vector when creating the flattened\nparameter vector; ensure indices advance consistently so checkpoint reloads and\noptimizer restores preserve sparse state.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:32:36Z","updated_at":"2025-11-02T02:32:40Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106277","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106277"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106277"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106277/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":685,"original_start_line":685,"start_side":"RIGHT","line":712,"original_line":712,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":712,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106281","pull_request_review_id":3407993158,"id":2484106281,"node_id":"PRRC_kwDOKSXUF86UEHQp","diff_hunk":"@@ -0,0 +1,936 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.NeuralNetworks.Layers;\n+\n+/// <summary>\n+/// LoftQ (LoRA-Fine-Tuning-Quantized) adapter that combines quantization and LoRA with improved initialization.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// LoftQ improves upon QLoRA by using an alternating optimization strategy during initialization\n+/// to find better LoRA adapter parameters for quantized models. Instead of simply quantizing\n+/// a pre-trained model and adding LoRA on top, LoftQ alternates between:\n+/// 1. Optimizing the quantization of the base weights\n+/// 2. Optimizing the LoRA adapter matrices to compensate for quantization error\n+/// </para>\n+/// <para>\n+/// <b>Key Features:</b>\n+/// - Alternating optimization between quantization and LoRA initialization\n+/// - Better initialization than naive quantization + LoRA\n+/// - Supports both 4-bit INT4 and NF4 quantization\n+/// - Reduces the gap between quantized and full-precision fine-tuning\n+/// - Compatible with all QLoRA features (double quantization, block-wise quantization)\n+/// </para>\n+/// <para>\n+/// <b>How LoftQ Differs from QLoRA:</b>\n+/// QLoRA:\n+/// 1. Quantize pre-trained weights\n+/// 2. Initialize LoRA randomly\n+/// 3. Fine-tune LoRA only\n+///\n+/// LoftQ:\n+/// 1. Start with pre-trained weights\n+/// 2. Alternate K times:\n+///    a. Fix LoRA, optimize quantization\n+///    b. Fix quantization, optimize LoRA (via SVD to minimize error)\n+/// 3. Fine-tune LoRA only\n+///\n+/// This alternating initialization creates better starting LoRA parameters that compensate\n+/// for quantization error from the beginning, leading to better final performance.\n+/// </para>\n+/// <para>\n+/// <b>Alternating Optimization Process:</b>\n+/// For K iterations (typically 3-5):\n+/// - Quantization step: Quantize W to get Q, keeping A and B fixed\n+/// - LoRA step: Update A and B to minimize ||W - (Q + AB)||, keeping Q fixed\n+///\n+/// This ensures the LoRA adapter specifically compensates for quantization error,\n+/// rather than learning generic adaptations.\n+/// </para>\n+/// <para>\n+/// <b>Memory Efficiency:</b>\n+/// Same as QLoRA - base weights in 4-bit, LoRA in full precision:\n+/// - 75% memory reduction on base weights\n+/// - Only LoRA parameters trainable (typically 0.1-1% of model size)\n+/// - Additional one-time cost during initialization for alternating optimization\n+/// </para>\n+/// <para>\n+/// <b>For Beginners:</b> LoftQ is an improved version of QLoRA that starts with better settings.\n+///\n+/// Think of it like this:\n+/// - QLoRA: Compress your model, then add random corrections, then train\n+/// - LoftQ: Compress your model, figure out what corrections are needed upfront, then train\n+///\n+/// The key insight: If we're going to compress the weights anyway, let's make sure our\n+/// correction layer (LoRA) is specifically designed to fix compression errors!\n+///\n+/// The process:\n+/// 1. Start with your pre-trained model\n+/// 2. Repeatedly:\n+///    - Try different compressions\n+///    - Adjust LoRA to compensate for compression error\n+///    - Pick the best combination\n+/// 3. Now train LoRA (which already knows how to fix compression issues)\n+///\n+/// Benefits:\n+/// - Better starting point for training\n+/// - Converges faster during fine-tuning\n+/// - Better final accuracy than QLoRA with same memory usage\n+/// - Still only trains LoRA (same efficiency as QLoRA)\n+///\n+/// Trade-offs:\n+/// - Longer initialization time (worth it for better results)\n+/// - Same runtime memory and speed as QLoRA\n+/// - More complex implementation\n+/// </para>\n+/// <para>\n+/// <b>Research Background:</b>\n+/// LoftQ was introduced in \"LoftQ: LoRA-Fine-Tuning-Aware Quantization\" (Li et al., 2023).\n+/// It addresses a key limitation of QLoRA: random LoRA initialization doesn't account for\n+/// the specific quantization errors introduced. By using alternating optimization, LoftQ\n+/// creates LoRA parameters that are \"aware\" of the quantization, leading to better downstream\n+/// fine-tuning performance with no additional runtime cost.\n+/// </para>\n+/// <para>\n+/// <b>When to Use LoftQ vs QLoRA:</b>\n+/// - Use LoftQ when: Training accuracy is critical, willing to spend extra time on initialization\n+/// - Use QLoRA when: Fast experimentation needed, initialization time is critical\n+/// - Both have identical runtime memory and speed characteristics\n+/// </para>\n+/// </remarks>\n+public class LoftQAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Specifies the type of 4-bit quantization to use for base layer weights.\n+    /// </summary>\n+    /// <remarks>\n+    /// Same quantization types as QLoRA. The alternating optimization works with both.\n+    /// </remarks>\n+    public enum QuantizationType\n+    {\n+        /// <summary>\n+        /// 4-bit integer quantization with uniform spacing (-8 to 7).\n+        /// </summary>\n+        INT4,\n+\n+        /// <summary>\n+        /// 4-bit Normal Float quantization optimized for normally distributed weights.\n+        /// </summary>\n+        /// <remarks>\n+        /// Recommended for most neural network weights. NF4 with LoftQ initialization\n+        /// provides the best accuracy-memory trade-off.\n+        /// </remarks>\n+        NF4\n+    }\n+\n+    /// <summary>\n+    /// The type of quantization used for base layer weights.\n+    /// </summary>\n+    private readonly QuantizationType _quantizationType;\n+\n+    /// <summary>\n+    /// Whether to use double quantization for quantization constants.\n+    /// </summary>\n+    private readonly bool _useDoubleQuantization;\n+\n+    /// <summary>\n+    /// The block size for quantization.\n+    /// </summary>\n+    private readonly int _quantizationBlockSize;\n+\n+    /// <summary>\n+    /// Number of alternating optimization iterations during initialization.\n+    /// </summary>\n+    /// <remarks>\n+    /// Typical values: 3-5 iterations. More iterations improve initialization quality\n+    /// but increase initialization time. Empirically, 3-5 iterations provide good\n+    /// balance between quality and speed.\n+    /// </remarks>\n+    private readonly int _numAlternatingIterations;\n+\n+    /// <summary>\n+    /// Quantized base layer weights stored as 4-bit values.\n+    /// </summary>\n+    private byte[]? _quantizedWeights;\n+\n+    /// <summary>\n+    /// Scale factors for dequantization (one per quantization block).\n+    /// </summary>\n+    private T[]? _quantizationScales;\n+\n+    /// <summary>\n+    /// Zero points for asymmetric quantization (one per quantization block).\n+    /// </summary>\n+    private T[]? _quantizationZeroPoints;\n+\n+    /// <summary>\n+    /// Cached dequantized weights for forward pass.\n+    /// </summary>\n+    private Matrix<T>? _dequantizedWeights;\n+\n+    /// <summary>\n+    /// NF4 quantization lookup table (16 values optimized for normal distribution).\n+    /// </summary>\n+    private static readonly double[] _nf4Table = new double[]\n+    {\n+        -1.0,\n+        -0.6961928009986877,\n+        -0.5250730514526367,\n+        -0.39491748809814453,\n+        -0.28444138169288635,\n+        -0.18477343022823334,\n+        -0.09105003625154495,\n+        0.0,\n+        0.07958029955625534,\n+        0.16093020141124725,\n+        0.24611230194568634,\n+        0.33791524171829224,\n+        0.44070982933044434,\n+        0.5626170039176941,\n+        0.7229568362236023,\n+        1.0\n+    };\n+\n+    /// <summary>\n+    /// Gets the quantization type used for base layer weights.\n+    /// </summary>\n+    public QuantizationType Quantization => _quantizationType;\n+\n+    /// <summary>\n+    /// Gets whether double quantization is enabled.\n+    /// </summary>\n+    public bool UsesDoubleQuantization => _useDoubleQuantization;\n+\n+    /// <summary>\n+    /// Gets the quantization block size.\n+    /// </summary>\n+    public int BlockSize => _quantizationBlockSize;\n+\n+    /// <summary>\n+    /// Gets the number of alternating optimization iterations used during initialization.\n+    /// </summary>\n+    public int AlternatingIterations => _numAlternatingIterations;\n+\n+    /// <summary>\n+    /// Initializes a new LoftQ adapter with alternating optimization for improved initialization.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The Dense or FullyConnected layer to adapt with LoftQ.</param>\n+    /// <param name=\"rank\">The rank of the LoRA decomposition.</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"numAlternatingIterations\">Number of alternating optimization iterations for initialization (default: 5).</param>\n+    /// <param name=\"quantizationType\">The type of 4-bit quantization to use (default: NF4).</param>\n+    /// <param name=\"useDoubleQuantization\">Whether to use double quantization for constants (default: true).</param>\n+    /// <param name=\"quantizationBlockSize\">The block size for quantization (default: 64).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training (default: true).</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when the base layer doesn't have 1D input/output shapes or when parameters are invalid.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// This constructor performs LoftQ initialization using alternating optimization:\n+    /// 1. Extracts base layer weights\n+    /// 2. For K iterations:\n+    ///    a. Quantize current weights\n+    ///    b. Compute quantization error\n+    ///    c. Update LoRA to minimize error (via SVD)\n+    ///    d. Update weights = quantized + LoRA\n+    /// 3. Store final quantized weights and LoRA parameters\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> Creating a LoftQ adapter takes longer than QLoRA because\n+    /// we're doing smart initialization. Here's what happens:\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: Your existing layer to compress and adapt\n+    /// - rank: LoRA adapter size (lower = more efficient)\n+    /// - alpha: LoRA strength\n+    /// - numAlternatingIterations: How many times to optimize initialization (3-5 is good)\n+    /// - quantizationType: NF4 recommended for best results\n+    /// - Other parameters: Same as QLoRA\n+    ///\n+    /// Initialization process (this happens once):\n+    /// 1. Look at your original weights\n+    /// 2. Try compressing them\n+    /// 3. See what errors compression creates\n+    /// 4. Adjust LoRA to fix those errors\n+    /// 5. Repeat steps 2-4 several times to find the best combination\n+    /// 6. Save the optimized compression and LoRA\n+    ///\n+    /// This extra work during initialization pays off with better training results!\n+    /// </para>\n+    /// </remarks>\n+    public LoftQAdapter(\n+        ILayer<T> baseLayer,\n+        int rank,\n+        double alpha = -1,\n+        int numAlternatingIterations = 5,\n+        QuantizationType quantizationType = QuantizationType.NF4,\n+        bool useDoubleQuantization = true,\n+        int quantizationBlockSize = 64,\n+        bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        // Validate base layer\n+        if (baseLayer.GetInputShape().Length != 1 || baseLayer.GetOutputShape().Length != 1)\n+        {\n+            throw new ArgumentException(\"LoftQAdapter only supports layers with 1D input/output shapes (Dense/FullyConnected layers)\", nameof(baseLayer));\n+        }\n+\n+        if (quantizationBlockSize <= 0)\n+        {\n+            throw new ArgumentException(\"Quantization block size must be positive\", nameof(quantizationBlockSize));\n+        }\n+\n+        if (numAlternatingIterations < 1)\n+        {\n+            throw new ArgumentException(\"Number of alternating iterations must be at least 1\", nameof(numAlternatingIterations));\n+        }\n+\n+        _quantizationType = quantizationType;\n+        _useDoubleQuantization = useDoubleQuantization;\n+        _quantizationBlockSize = quantizationBlockSize;\n+        _numAlternatingIterations = numAlternatingIterations;\n+\n+        // Perform LoftQ initialization with alternating optimization\n+        PerformLoftQInitialization();\n+    }\n+\n+    /// <summary>\n+    /// Performs LoftQ initialization using alternating optimization between quantization and LoRA.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This is the core LoftQ algorithm:\n+    /// 1. Extract base layer weights W\n+    /// 2. For K iterations:\n+    ///    a. Quantize current weights: Q = Quantize(W_current)\n+    ///    b. Compute residual: R = W - Q\n+    ///    c. Decompose residual via SVD: R ‚âà U * S * V^T\n+    ///    d. Set LoRA matrices: A = V^T[:rank, :], B = U[:, :rank] * S[:rank, :rank]\n+    ///    e. Update: W_current = Q + A * B (scaled by alpha/rank)\n+    /// 3. Store final Q as quantized weights, final A and B as LoRA parameters\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This is where the \"smart initialization\" happens.\n+    ///\n+    /// The algorithm:\n+    /// - Start with your original weights W\n+    /// - Repeat several times:\n+    ///   1. Compress W to get Q (quantized version)\n+    ///   2. Calculate error: R = W - Q (what we lost in compression)\n+    ///   3. Use math (SVD) to find the best LoRA matrices that approximate R\n+    ///   4. Update W = Q + LoRA (compressed + correction)\n+    ///   5. Go back to step 1 with the new W\n+    ///\n+    /// Why alternate?\n+    /// - Each iteration, LoRA learns to fix compression errors better\n+    /// - Each iteration, compression is done knowing LoRA will help\n+    /// - They work together to find the best combination\n+    ///\n+    /// Result: LoRA starts already knowing how to compensate for compression!\n+    /// </para>\n+    /// </remarks>\n+    private void PerformLoftQInitialization()\n+    {\n+        // Get base layer parameters\n+        Vector<T> baseParams = _baseLayer.GetParameters();\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        int weightCount = inputSize * outputSize;\n+\n+        // Extract weights (shape: [outputSize, inputSize])\n+        Matrix<T> weights = new Matrix<T>(outputSize, inputSize);\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                weights[i, j] = baseParams[i * inputSize + j];\n+            }\n+        }\n+\n+        // Store original weights for alternating optimization\n+        Matrix<T> currentWeights = weights.Clone();\n+\n+        // Alternating optimization loop\n+        for (int iter = 0; iter < _numAlternatingIterations; iter++)\n+        {\n+            // Step 1: Quantize current weights\n+            QuantizeWeights(currentWeights);\n+\n+            // Step 2: Dequantize to get Q\n+            Matrix<T> quantizedWeights = DequantizeWeights();\n+\n+            // Step 3: Compute residual R = W - Q\n+            Matrix<T> residual = new Matrix<T>(outputSize, inputSize);\n+            for (int i = 0; i < outputSize; i++)\n+            {\n+                for (int j = 0; j < inputSize; j++)\n+                {\n+                    residual[i, j] = NumOps.Subtract(weights[i, j], quantizedWeights[i, j]);\n+                }\n+            }\n+\n+            // Step 4: Decompose residual via SVD and update LoRA matrices\n+            UpdateLoRAFromResidual(residual);\n+\n+            // Step 5: Update current weights = Q + LoRA (for next iteration)\n+            Matrix<T> loraWeights = _loraLayer.MergeWeights();\n+            for (int i = 0; i < outputSize; i++)\n+            {\n+                for (int j = 0; j < inputSize; j++)\n+                {\n+                    currentWeights[i, j] = NumOps.Add(quantizedWeights[i, j], loraWeights[i, j]);\n+                }\n+            }\n+        }\n+\n+        // Final quantization (already done in last iteration)\n+        // LoRA parameters are also set from last iteration\n+\n+        // Apply double quantization if enabled\n+        if (_useDoubleQuantization)\n+        {\n+            DoubleQuantizeScales();\n+        }\n+\n+        // Update parameter vector\n+        UpdateParametersFromLayers();\n+    }\n+\n+    /// <summary>\n+    /// Updates LoRA matrices A and B to minimize the residual via SVD decomposition.\n+    /// </summary>\n+    /// <param name=\"residual\">The residual matrix to decompose (W - Q).</param>\n+    /// <remarks>\n+    /// <para>\n+    /// Uses SVD to decompose the residual and extract low-rank approximation:\n+    /// - Compute SVD: R = U * S * V^T\n+    /// - Take rank-r approximation: R_approx = U[:, :r] * S[:r, :r] * V^T[:r, :]\n+    /// - Set LoRA matrices: B = U[:, :r] * sqrt(S[:r, :r]), A = sqrt(S[:r, :r]) * V^T[:r, :]\n+    /// - This ensures BA ‚âà R with minimal error in Frobenius norm\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This uses a mathematical technique called SVD to find the best\n+    /// LoRA matrices that approximate the compression error.\n+    ///\n+    /// Think of it like:\n+    /// - You have a big error matrix (difference between original and compressed)\n+    /// - SVD finds the \"most important patterns\" in that error\n+    /// - We keep only the top 'rank' patterns (low-rank approximation)\n+    /// - Split these patterns into two smaller matrices A and B\n+    /// - When multiplied, A * B ‚âà error, but using much fewer parameters!\n+    ///\n+    /// This is mathematically optimal - no other rank-r approximation can do better.\n+    /// </para>\n+    /// </remarks>\n+    private void UpdateLoRAFromResidual(Matrix<T> residual)\n+    {\n+        int outputSize = residual.Rows;\n+        int inputSize = residual.Columns;\n+        int rank = _loraLayer.Rank;\n+\n+        // Compute SVD of residual matrix\n+        // For efficiency, we'll use a simplified approach:\n+        // 1. Compute R * R^T (smaller if outputSize < inputSize)\n+        // 2. Get eigenvalues/eigenvectors\n+        // 3. Construct low-rank approximation\n+\n+        // Compute R * R^T\n+        Matrix<T> rrt = residual.Multiply(residual.Transpose());\n+\n+        // Get eigenvalues and eigenvectors (we'll use power iteration for top-k)\n+        // For a production implementation, use a proper SVD library\n+        // Here we'll use a simplified approach with the full matrices\n+\n+        // Simplified: Just use the residual directly with truncation\n+        // Extract top-rank components\n+\n+        Vector<T> loraParams = _loraLayer.GetParameters();\n+        int aRows = rank;\n+        int aCols = inputSize;\n+        int bRows = outputSize;\n+        int bCols = rank;\n+\n+        // Initialize A and B from truncated residual\n+        // A: [rank, inputSize] - initialized from top rank rows of residual\n+        // B: [outputSize, rank] - initialized to produce low-rank approximation\n+\n+        // Simple initialization: Use first 'rank' singular vectors\n+        // For proper SVD, we'd compute U, S, V and use:\n+        // B = U[:, :rank] * sqrt(S[:rank, :rank])\n+        // A = sqrt(S[:rank, :rank]) * V^T[:rank, :]\n+\n+        // Simplified approach: Initialize A from residual rows, B to scale appropriately\n+        int idx = 0;\n+\n+        // Set A matrix in LoRA parameters (first part)\n+        double scaleFactor = 1.0 / Math.Sqrt(rank); // Simple scaling\n+        for (int i = 0; i < aRows; i++)\n+        {\n+            for (int j = 0; j < aCols; j++)\n+            {\n+                // Take patterns from residual with scaling\n+                int resRow = i % outputSize;\n+                loraParams[idx++] = NumOps.Multiply(residual[resRow, j], NumOps.FromDouble(scaleFactor));\n+            }\n+        }\n+\n+        // Set B matrix in LoRA parameters (second part)\n+        for (int i = 0; i < bRows; i++)\n+        {\n+            for (int j = 0; j < bCols; j++)\n+            {\n+                // Initialize B to create rank-r approximation\n+                T value = NumOps.Zero;\n+                for (int k = 0; k < inputSize; k++)\n+                {\n+                    int aRow = j;\n+                    T aVal = loraParams[aRow * aCols + k];\n+                    value = NumOps.Add(value, NumOps.Multiply(residual[i, k], aVal));\n+                }\n+                loraParams[idx++] = NumOps.Multiply(value, NumOps.FromDouble(scaleFactor));\n+            }\n+        }\n+\n+        // Update LoRA layer with new parameters\n+        _loraLayer.SetParameters(loraParams);\n+    }\n+\n+    /// <summary>\n+    /// Quantizes a weight matrix to 4-bit precision.\n+    /// </summary>\n+    /// <param name=\"weights\">The weight matrix to quantize.</param>\n+    private void QuantizeWeights(Matrix<T> weights)\n+    {\n+        int outputSize = weights.Rows;\n+        int inputSize = weights.Columns;\n+        int weightCount = outputSize * inputSize;\n+\n+        // Flatten weights for quantization\n+        T[] flatWeights = new T[weightCount];\n+        int idx = 0;\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                flatWeights[idx++] = weights[i, j];\n+            }\n+        }\n+\n+        // Quantize in blocks\n+        int numBlocks = (weightCount + _quantizationBlockSize - 1) / _quantizationBlockSize;\n+        _quantizedWeights = new byte[(weightCount + 1) / 2]; // 2 values per byte\n+        _quantizationScales = new T[numBlocks];\n+        _quantizationZeroPoints = new T[numBlocks];\n+\n+        for (int blockIdx = 0; blockIdx < numBlocks; blockIdx++)\n+        {\n+            int blockStart = blockIdx * _quantizationBlockSize;\n+            int blockEnd = Math.Min(blockStart + _quantizationBlockSize, weightCount);\n+\n+            // Find min/max for this block\n+            T minVal = flatWeights[blockStart];\n+            T maxVal = flatWeights[blockStart];\n+            for (int i = blockStart + 1; i < blockEnd; i++)\n+            {\n+                if (NumOps.LessThan(flatWeights[i], minVal))\n+                    minVal = flatWeights[i];\n+                if (NumOps.GreaterThan(flatWeights[i], maxVal))\n+                    maxVal = flatWeights[i];\n+            }\n+\n+            // Compute scale and zero point\n+            T range = NumOps.Subtract(maxVal, minVal);\n+            T scale = NumOps.Divide(range, NumOps.FromDouble(15.0));\n+            T zeroPoint = minVal;\n+\n+            _quantizationScales[blockIdx] = scale;\n+            _quantizationZeroPoints[blockIdx] = zeroPoint;\n+\n+            // Quantize values in this block\n+            for (int i = blockStart; i < blockEnd; i++)\n+            {\n+                byte quantizedValue = QuantizeValue(flatWeights[i], scale, zeroPoint);\n+\n+                // Pack two 4-bit values per byte\n+                int byteIdx = i / 2;\n+                if (i % 2 == 0)\n+                {\n+                    _quantizedWeights[byteIdx] = (byte)(quantizedValue & 0x0F);\n+                }\n+                else\n+                {\n+                    _quantizedWeights[byteIdx] |= (byte)((quantizedValue & 0x0F) << 4);\n+                }\n+            }\n+        }","path":"src/NeuralNetworks/Layers/LoftQAdapter.cs","commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","original_commit_id":"c11235933bf7c341c8b4a223cf6ac11b757d1480","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Guard against zero quantization range before dividing**\n\nWhen `minVal == maxVal`, `range` becomes zero and `scale = range / 15` is also zero. The normalization that follows divides by `scale`, so blocks containing constant weights will throw at runtime (or propagate NaNs) as soon as you hit a uniform block. Please special‚Äëcase the zero-range case before you divide. One option is to pin `scale` to `1` (or any non-zero sentinel) so the block still quantizes deterministically.\n\n```diff\n-            // Compute scale and zero point\n-            T range = NumOps.Subtract(maxVal, minVal);\n-            T scale = NumOps.Divide(range, NumOps.FromDouble(15.0));\n-            T zeroPoint = minVal;\n+            // Compute scale and zero point\n+            T range = NumOps.Subtract(maxVal, minVal);\n+            T scale;\n+            if (Convert.ToDouble(range) == 0.0)\n+            {\n+                scale = NumOps.One;\n+            }\n+            else\n+            {\n+                scale = NumOps.Divide(range, NumOps.FromDouble(15.0));\n+            }\n+            T zeroPoint = minVal;\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n            T range = NumOps.Subtract(maxVal, minVal);\n            T scale;\n            if (Convert.ToDouble(range) == 0.0)\n            {\n                scale = NumOps.One;\n            }\n            else\n            {\n                scale = NumOps.Divide(range, NumOps.FromDouble(15.0));\n            }\n            T zeroPoint = minVal;\n\n            _quantizationScales[blockIdx] = scale;\n            _quantizationZeroPoints[blockIdx] = zeroPoint;\n\n            // Quantize values in this block\n            for (int i = blockStart; i < blockEnd; i++)\n            {\n                byte quantizedValue = QuantizeValue(flatWeights[i], scale, zeroPoint);\n\n                // Pack two 4-bit values per byte\n                int byteIdx = i / 2;\n                if (i % 2 == 0)\n                {\n                    _quantizedWeights[byteIdx] = (byte)(quantizedValue & 0x0F);\n                }\n                else\n                {\n                    _quantizedWeights[byteIdx] |= (byte)((quantizedValue & 0x0F) << 4);\n                }\n            }\n        }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/Layers/LoftQAdapter.cs around lines 543 to 566, handle the\ncase where minVal == maxVal before computing scale; detect a zero range and set\nscale to a non-zero sentinel (e.g. NumOps.FromDouble(1.0) or a tiny non-zero\nvalue) and keep zeroPoint = minVal so quantization is deterministic, then store\nthose values into _quantizationScales/_quantizationZeroPoints and continue\npacking; this avoids dividing by zero or producing NaNs during later\nnormalization/dequantization.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:32:36Z","updated_at":"2025-11-02T02:32:40Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106281","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106281"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106281"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106281/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":543,"original_start_line":543,"start_side":"RIGHT","line":566,"original_line":566,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":566,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106283","pull_request_review_id":3407993158,"id":2484106283,"node_id":"PRRC_kwDOKSXUF86UEHQr","diff_hunk":"@@ -0,0 +1,903 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.NeuralNetworks.Layers;\n+\n+/// <summary>\n+/// LoHa (Low-Rank Hadamard Product Adaptation) adapter for parameter-efficient fine-tuning.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// LoHa uses element-wise Hadamard products (‚äô) instead of matrix multiplication for adaptation.\n+/// Instead of computing ŒîW = B * A like standard LoRA, LoHa computes:\n+/// ŒîW = sum over rank of (A[i] ‚äô B[i])\n+///\n+/// This formulation can capture element-wise patterns that matrix multiplication may miss,\n+/// making it particularly effective for:\n+/// - Convolutional layers (local spatial patterns)\n+/// - Element-wise transformations\n+/// - Fine-grained weight adjustments\n+/// </para>\n+/// <para><b>Mathematical Formulation:</b>\n+///\n+/// Standard LoRA: ŒîW = B * A where B is rank√óoutput, A is input√órank\n+/// LoHa: ŒîW = Œ£(A[i] ‚äô B[i]) where A[i] and B[i] are both input√óoutput\n+///\n+/// The Hadamard product (‚äô) performs element-wise multiplication, allowing each element\n+/// of the weight matrix to be adjusted independently across the rank dimensions.\n+/// </para>\n+/// <para><b>For Beginners:</b> LoHa is a variant of LoRA that uses element-wise multiplication\n+/// instead of matrix multiplication. Think of it this way:\n+///\n+/// - Standard LoRA: Learns \"row and column patterns\" that combine via matrix multiply\n+/// - LoHa: Learns \"pixel-by-pixel patterns\" that combine via element-wise multiply\n+///\n+/// LoHa is especially good when:\n+/// 1. You need to capture local, element-wise patterns (like in images)\n+/// 2. The weight matrix has spatial structure (like convolutional filters)\n+/// 3. You want each weight to be adjusted somewhat independently\n+///\n+/// Trade-offs compared to LoRA:\n+/// - More parameters: Both A and B must be full-sized (input√óoutput) per rank dimension\n+/// - Different expressiveness: Better for element-wise patterns, different from matrix patterns\n+/// - Better for CNNs: The element-wise nature matches convolutional structure better\n+///\n+/// Example: A 100√ó100 weight matrix with rank=8\n+/// - Standard LoRA: 8√ó100 + 100√ó8 = 1,600 parameters\n+/// - LoHa: 8√ó(100√ó100) + 8√ó(100√ó100) = 160,000 parameters\n+///\n+/// Despite more parameters, LoHa is still far more efficient than full fine-tuning (10,000 params).\n+/// </para>\n+/// </remarks>\n+public class LoHaAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Low-rank matrices A with dimensions (rank, inputSize, outputSize).\n+    /// Each A[i] is a full-sized matrix for the i-th rank dimension.\n+    /// </summary>\n+    private readonly Matrix<T>[] _matricesA;\n+\n+    /// <summary>\n+    /// Low-rank matrices B with dimensions (rank, inputSize, outputSize).\n+    /// Each B[i] is a full-sized matrix for the i-th rank dimension.\n+    /// </summary>\n+    private readonly Matrix<T>[] _matricesB;\n+\n+    /// <summary>\n+    /// Gradients for matrices A computed during backpropagation.\n+    /// </summary>\n+    private Matrix<T>[]? _matricesAGradient;\n+\n+    /// <summary>\n+    /// Gradients for matrices B computed during backpropagation.\n+    /// </summary>\n+    private Matrix<T>[]? _matricesBGradient;\n+\n+    /// <summary>\n+    /// Stored input from the forward pass, needed for gradient computation.\n+    /// </summary>\n+    private Tensor<T>? _lastInput;\n+\n+    /// <summary>\n+    /// Stored base layer output from the forward pass.\n+    /// </summary>\n+    private Tensor<T>? _lastBaseOutput;\n+\n+    /// <summary>\n+    /// Computed scaling factor (alpha / rank) used during forward pass.\n+    /// </summary>\n+    private readonly T _scaling;\n+\n+    /// <summary>\n+    /// Initializes a new LoHa adapter wrapping an existing layer.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with LoHa.</param>\n+    /// <param name=\"rank\">The rank of the low-rank decomposition.</param>\n+    /// <param name=\"alpha\">The LoHa scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when the base layer doesn't have 1D input/output shapes.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This creates a LoHa adapter for any layer with 1D input/output.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to make more efficient to fine-tune\n+    /// - rank: How many element-wise patterns to learn (more = more flexibility, more parameters)\n+    /// - alpha: How strong the LoHa adaptation is (typically same as rank)\n+    /// - freezeBaseLayer: Whether to lock the original layer's weights (usually true for efficiency)\n+    ///\n+    /// The adapter creates 2√órank full-sized matrices (A and B for each rank dimension),\n+    /// which are combined using element-wise Hadamard products during forward/backward passes.\n+    /// </para>\n+    /// </remarks>\n+    public LoHaAdapter(ILayer<T> baseLayer, int rank, double alpha = -1, bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        // Validate base layer has single-dimensional input/output\n+        if (baseLayer.GetInputShape().Length != 1 || baseLayer.GetOutputShape().Length != 1)\n+        {\n+            throw new ArgumentException(\"LoHaAdapter only supports layers with 1D input/output shapes\", nameof(baseLayer));\n+        }\n+\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+\n+        // Calculate scaling\n+        _scaling = NumOps.Divide(_loraLayer.Alpha, NumOps.FromDouble(rank));\n+\n+        // Initialize LoHa matrices (rank sets of full-sized matrices)\n+        _matricesA = new Matrix<T>[rank];\n+        _matricesB = new Matrix<T>[rank];\n+\n+        for (int r = 0; r < rank; r++)\n+        {\n+            // Initialize A[r] with random values (Gaussian with std = 1/sqrt(rank))\n+            _matricesA[r] = new Matrix<T>(inputSize, outputSize);\n+            T stddev = NumOps.Sqrt(NumOps.Divide(NumOps.One, NumOps.FromDouble(rank)));\n+            for (int i = 0; i < inputSize; i++)\n+            {\n+                for (int j = 0; j < outputSize; j++)\n+                {\n+                    // Box-Muller transform for Gaussian random numbers\n+                    double u1 = Random.NextDouble();\n+                    double u2 = Random.NextDouble();\n+                    double randStdNormal = Math.Sqrt(-2.0 * Math.Log(u1)) * Math.Sin(2.0 * Math.PI * u2);\n+                    _matricesA[r][i, j] = NumOps.Multiply(NumOps.FromDouble(randStdNormal), stddev);\n+                }\n+            }\n+\n+            // Initialize B[r] to zero (so LoHa has no effect initially)\n+            _matricesB[r] = new Matrix<T>(inputSize, outputSize);\n+            for (int i = 0; i < inputSize; i++)\n+            {\n+                for (int j = 0; j < outputSize; j++)\n+                {\n+                    _matricesB[r][i, j] = NumOps.Zero;\n+                }\n+            }\n+        }\n+\n+        // Initialize parameter vector\n+        Parameters = new Vector<T>(ParameterCount);\n+        UpdateParametersFromMatrices();\n+    }\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters.\n+    /// </summary>\n+    /// <remarks>\n+    /// LoHa has 2 * rank * inputSize * outputSize parameters (A and B matrices for each rank).\n+    /// This is more than standard LoRA but still far less than full fine-tuning.\n+    /// </remarks>\n+    public override int ParameterCount\n+    {\n+        get\n+        {\n+            int inputSize = GetInputShape()[0];\n+            int outputSize = GetOutputShape()[0];\n+            int lohaParams = 2 * Rank * inputSize * outputSize;\n+            return _freezeBaseLayer ? lohaParams : (_baseLayer.ParameterCount + lohaParams);\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass through both base layer and LoHa adaptation.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Sum of base layer output and LoHa delta (computed via Hadamard products).</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The forward pass computes:\n+    /// 1. base_output = base_layer(input)\n+    /// 2. loha_delta = sum over rank of (input * A[i] ‚äô B[i]) * scaling\n+    /// 3. output = base_output + loha_delta\n+    ///\n+    /// The Hadamard product (‚äô) multiplies corresponding elements, allowing element-wise adaptations.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This runs the input through the original layer and adds a correction.\n+    ///\n+    /// The correction is computed by:\n+    /// 1. Transforming input through each A[i] matrix (one per rank dimension)\n+    /// 2. Multiplying element-wise with corresponding B[i] matrix (Hadamard product)\n+    /// 3. Summing all rank contributions together\n+    /// 4. Scaling by alpha/rank\n+    ///\n+    /// This element-wise approach lets LoHa learn fine-grained adjustments to each weight independently.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        _lastInput = input.Clone();\n+\n+        // Forward through base layer\n+        Tensor<T> baseOutput = _baseLayer.Forward(input);\n+        _lastBaseOutput = baseOutput.Clone();\n+\n+        // Compute LoHa delta using Hadamard products\n+        Tensor<T> lohaDelta = ComputeLoHaDelta(input);\n+\n+        // Sum the outputs: base + loha_delta\n+        Tensor<T> result = new Tensor<T>(baseOutput.Shape);\n+        for (int i = 0; i < baseOutput.Length; i++)\n+        {\n+            result[i] = NumOps.Add(baseOutput[i], lohaDelta[i]);\n+        }\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Computes the LoHa delta using Hadamard products across all rank dimensions.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor of shape [batchSize, inputSize].</param>\n+    /// <returns>LoHa delta tensor of shape [batchSize, outputSize].</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Computes: delta = scaling * sum over rank of (input * A[i]) ‚äô B[i]\n+    ///\n+    /// For each rank dimension i:\n+    /// 1. Multiply input by A[i] matrix: intermediate[i] = input * A[i]\n+    /// 2. Apply Hadamard product with B[i]: result[i] = intermediate[i] ‚äô B[i]\n+    /// 3. Sum all results and scale: delta = scaling * sum(result[i])\n+    /// </para>\n+    /// </remarks>\n+    private Tensor<T> ComputeLoHaDelta(Tensor<T> input)\n+    {\n+        int batchSize = input.Shape[0];\n+        int inputSize = input.Shape.Length > 1 ? input.Shape[1] : input.Length;\n+        int outputSize = GetOutputShape()[0];\n+\n+        // Convert input to matrix [batchSize, inputSize]\n+        Matrix<T> inputMatrix = new Matrix<T>(batchSize, inputSize);\n+        for (int b = 0; b < batchSize; b++)\n+        {\n+            for (int i = 0; i < inputSize; i++)\n+            {\n+                inputMatrix[b, i] = input[b * inputSize + i];\n+            }\n+        }\n+\n+        // Accumulate Hadamard product results across all ranks\n+        Matrix<T> deltaMatrix = new Matrix<T>(batchSize, outputSize);\n+        for (int b = 0; b < batchSize; b++)\n+        {\n+            for (int o = 0; o < outputSize; o++)\n+            {\n+                deltaMatrix[b, o] = NumOps.Zero;\n+            }\n+        }\n+\n+        // Sum over rank: delta += (input * A[r]) ‚äô B[r] for each r\n+        for (int r = 0; r < Rank; r++)\n+        {\n+            // Compute input * A[r] for each batch and output dimension\n+            Matrix<T> intermediate = new Matrix<T>(batchSize, outputSize);\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                for (int o = 0; o < outputSize; o++)\n+                {\n+                    T sum = NumOps.Zero;\n+                    for (int i = 0; i < inputSize; i++)\n+                    {\n+                        // (input * A[r])[b, o] = sum over i of input[b, i] * A[r][i, o]\n+                        sum = NumOps.Add(sum, NumOps.Multiply(inputMatrix[b, i], _matricesA[r][i, o]));\n+                    }\n+                    intermediate[b, o] = sum;\n+                }\n+            }\n+\n+            // Apply Hadamard product with B[r]: result ‚äô= B[r]\n+            Matrix<T> hadamardResult = HadamardProduct(intermediate, _matricesB[r]);\n+\n+            // Accumulate into delta\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                for (int o = 0; o < outputSize; o++)\n+                {\n+                    deltaMatrix[b, o] = NumOps.Add(deltaMatrix[b, o], hadamardResult[b, o]);\n+                }\n+            }\n+        }\n+\n+        // Apply scaling\n+        for (int b = 0; b < batchSize; b++)\n+        {\n+            for (int o = 0; o < outputSize; o++)\n+            {\n+                deltaMatrix[b, o] = NumOps.Multiply(deltaMatrix[b, o], _scaling);\n+            }\n+        }\n+\n+        // Convert back to tensor\n+        Vector<T> deltaData = new Vector<T>(batchSize * outputSize);\n+        int idx = 0;\n+        for (int b = 0; b < batchSize; b++)\n+        {\n+            for (int o = 0; o < outputSize; o++)\n+            {\n+                deltaData[idx++] = deltaMatrix[b, o];\n+            }\n+        }\n+\n+        return new Tensor<T>(new[] { batchSize, outputSize }, deltaData);\n+    }\n+\n+    /// <summary>\n+    /// Computes element-wise Hadamard product between a batch matrix and a weight matrix.\n+    /// </summary>\n+    /// <param name=\"batchMatrix\">Matrix of shape [batchSize, size].</param>\n+    /// <param name=\"weightMatrix\">Matrix of shape [inputSize, outputSize] (broadcasted across batch).</param>\n+    /// <returns>Hadamard product result of same shape as batchMatrix.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// For LoHa, the Hadamard product is applied between the intermediate activations\n+    /// (batchSize √ó outputSize) and the B matrix (inputSize √ó outputSize).\n+    ///\n+    /// Since the intermediate is [batch, output] and B is [input, output], we take the\n+    /// element-wise product along the output dimension.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> The Hadamard product is just element-wise multiplication.\n+    /// For each position (i, j), multiply the corresponding elements: result[i,j] = a[i,j] * b[i,j]\n+    ///\n+    /// This is different from matrix multiplication, which sums over a dimension.\n+    /// Hadamard product keeps dimensions the same and multiplies element-by-element.\n+    /// </para>\n+    /// </remarks>\n+    private Matrix<T> HadamardProduct(Matrix<T> batchMatrix, Matrix<T> weightMatrix)\n+    {\n+        int batchSize = batchMatrix.Rows;\n+        int outputSize = batchMatrix.Columns;\n+\n+        // For LoHa: batchMatrix is [batch, output], weightMatrix is [input, output]\n+        // We broadcast weightMatrix across batch dimension and multiply element-wise along output\n+        Matrix<T> result = new Matrix<T>(batchSize, outputSize);\n+\n+        for (int b = 0; b < batchSize; b++)\n+        {\n+            for (int o = 0; o < outputSize; o++)\n+            {\n+                // Since intermediate is already projected to output space,\n+                // we multiply element-wise with the first row of B\n+                // (This is a simplification; full LoHa may have different broadcasting)\n+                T sum = NumOps.Zero;\n+                for (int i = 0; i < weightMatrix.Rows; i++)\n+                {\n+                    sum = NumOps.Add(sum, weightMatrix[i, o]);\n+                }\n+                // Average across input dimension\n+                T avg = NumOps.Divide(sum, NumOps.FromDouble(weightMatrix.Rows));\n+                result[b, o] = NumOps.Multiply(batchMatrix[b, o], avg);\n+            }\n+        }","path":"src/NeuralNetworks/Layers/LoHaAdapter.cs","commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","original_commit_id":"c11235933bf7c341c8b4a223cf6ac11b757d1480","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Hadamard product is reduced to a column average.**\n\n`HadamardProduct` collapses `_matricesB[r]` by averaging each column and multiplies the entire column of `intermediate` by that average. This no longer implements LoHa‚Äôs element-wise Hadamard composition of the low-rank factors, so the adapter degenerates to a diagonal rescaling and loses the enhanced rank LoHa is designed to provide.([medium.com](https://medium.com/predict/beyond-lora-a-comprehensive-guide-to-efficient-model-fine-tuning-5983bb4899fb?utm_source=openai))\n\nPlease compute the true element-wise product (and then apply the result in `ComputeLoHaDelta`) instead of averaging, for example:\n\n```diff\n-        for (int b = 0; b < batchSize; b++)\n-        {\n-            for (int o = 0; o < outputSize; o++)\n-            {\n-                T sum = NumOps.Zero;\n-                for (int i = 0; i < weightMatrix.Rows; i++)\n-                {\n-                    sum = NumOps.Add(sum, weightMatrix[i, o]);\n-                }\n-                T avg = NumOps.Divide(sum, NumOps.FromDouble(weightMatrix.Rows));\n-                result[b, o] = NumOps.Multiply(batchMatrix[b, o], avg);\n-            }\n-        }\n+        for (int b = 0; b < batchSize; b++)\n+        {\n+            for (int o = 0; o < outputSize; o++)\n+            {\n+                T value = NumOps.Zero;\n+                for (int i = 0; i < weightMatrix.Rows; i++)\n+                {\n+                    // (intermediate ‚äô B)[b, o] accumulates input contribution i\n+                    T contribution = NumOps.Multiply(\n+                        batchMatrix[b, o],\n+                        weightMatrix[i, o]);\n+                    value = NumOps.Add(value, contribution);\n+                }\n+                result[b, o] = value;\n+            }\n+        }\n```\n\nand adjust `ComputeLoHaDelta` accordingly so you form the Hadamard weight update before multiplying by the input. Otherwise LoHa will not match its published behavior.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/Layers/LoHaAdapter.cs around lines 325 to 371, the current\nHadamardProduct implementation incorrectly collapses weightMatrix columns to a\nsingle average per column (reducing LoHa to a diagonal rescale); replace that\nlogic with a true element-wise Hadamard computation: for each batch b and output\no, compute the element-wise product between the intermediate activation\n(batchMatrix[b,o]) and the corresponding B vector across the input dimension to\nproduce a per-input per-output Hadamard factor (do not sum/average across input\nrows). Then update ComputeLoHaDelta to use that per-input√óoutput Hadamard weight\nupdate before applying the input projection (i.e., form the Hadamard weight\nupdate matrix/tensor first, then multiply by the input to produce the final\ndelta), ensuring broadcasting and dimensions match LoHa's published element-wise\ncomposition instead of averaging.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:32:36Z","updated_at":"2025-11-02T02:32:40Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106283","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106283"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106283"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106283/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":325,"original_start_line":325,"start_side":"RIGHT","line":371,"original_line":371,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":371,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106284","pull_request_review_id":3407993158,"id":2484106284,"node_id":"PRRC_kwDOKSXUF86UEHQs","diff_hunk":"@@ -0,0 +1,759 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.NeuralNetworks.Layers;\n+\n+/// <summary>\n+/// LoKr (Low-Rank Kronecker Product Adaptation) adapter for parameter-efficient fine-tuning.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// LoKr uses Kronecker products instead of standard matrix multiplication for low-rank adaptation.\n+/// Instead of computing ŒîW = A √ó B (standard LoRA), LoKr computes ŒîW = A ‚äó B where ‚äó is the\n+/// Kronecker product. This is particularly efficient for very large weight matrices.\n+/// </para>\n+/// <para><b>Kronecker Product Definition:</b>\n+/// For matrices A (m√ón) and B (p√óq), the Kronecker product A ‚äó B is an (m√óp) √ó (n√óq) matrix:\n+///\n+/// A ‚äó B = [a‚ÇÅ‚ÇÅB  a‚ÇÅ‚ÇÇB  ...  a‚ÇÅ‚ÇôB]\n+///         [a‚ÇÇ‚ÇÅB  a‚ÇÇ‚ÇÇB  ...  a‚ÇÇ‚ÇôB]\n+///         [  ‚ãÆ     ‚ãÆ    ‚ã±    ‚ãÆ  ]\n+///         [a‚Çò‚ÇÅB  a‚Çò‚ÇÇB  ...  a‚Çò‚ÇôB]\n+///\n+/// Each element a·µ¢‚±º of A is multiplied by the entire matrix B, creating a block structure.\n+/// </para>\n+/// <para><b>For Beginners:</b> LoKr is a variant of LoRA that uses a different mathematical operation\n+/// called the Kronecker product. Think of it this way:\n+///\n+/// - Standard LoRA: Multiplies two small matrices (like 1000√ó8 and 8√ó1000) to approximate changes\n+/// - LoKr: Uses Kronecker product of two even smaller matrices (like 50√ó4 and 20√ó4) to create the same size output\n+///\n+/// The Kronecker product creates a larger matrix by taking every element of the first matrix and\n+/// multiplying it by the entire second matrix. This creates a block pattern that's very efficient\n+/// for representing certain types of structured transformations.\n+///\n+/// <b>When to use LoKr vs standard LoRA:</b>\n+/// - LoKr is better for very wide or very deep layers (e.g., 10000√ó10000 weight matrices)\n+/// - LoKr can achieve similar expressiveness with fewer parameters than LoRA\n+/// - Standard LoRA is simpler and works well for typical layer sizes\n+///\n+/// <b>Parameter Efficiency Example:</b>\n+/// For a 1000√ó1000 weight matrix with rank r=8:\n+/// - Standard LoRA: 1000√ó8 + 8√ó1000 = 16,000 parameters\n+/// - LoKr: 50√ó4 + 20√ó4 = 200 + 80 = 280 parameters (57x fewer!)\n+///   (where 50√ó20 = 1000 for both dimensions)\n+/// </para>\n+/// </remarks>\n+public class LoKrAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// First Kronecker factor matrix A with dimensions (m √ó n).\n+    /// </summary>\n+    /// <remarks>\n+    /// This is one of the two matrices used in the Kronecker product decomposition.\n+    /// </remarks>\n+    private Matrix<T> _matrixA;\n+\n+    /// <summary>\n+    /// Second Kronecker factor matrix B with dimensions (p √ó q).\n+    /// </summary>\n+    /// <remarks>\n+    /// This is the second matrix used in the Kronecker product decomposition.\n+    /// The Kronecker product A ‚äó B produces a (m√óp) √ó (n√óq) matrix.\n+    /// </remarks>\n+    private Matrix<T> _matrixB;\n+\n+    /// <summary>\n+    /// Scaling factor for the LoKr contribution.\n+    /// </summary>\n+    private readonly T _alpha;\n+\n+    /// <summary>\n+    /// Computed scaling factor (alpha / effective_rank) used during forward pass.\n+    /// </summary>\n+    private readonly T _scaling;\n+\n+    /// <summary>\n+    /// Gradients for matrix A computed during backpropagation.\n+    /// </summary>\n+    private Matrix<T>? _gradientA;\n+\n+    /// <summary>\n+    /// Gradients for matrix B computed during backpropagation.\n+    /// </summary>\n+    private Matrix<T>? _gradientB;\n+\n+    /// <summary>\n+    /// Stored input from the forward pass, needed for gradient computation.\n+    /// </summary>\n+    private Tensor<T>? _lastInput;\n+\n+    /// <summary>\n+    /// Dimensions for matrix A (m, n).\n+    /// </summary>\n+    private readonly (int m, int n) _dimsA;\n+\n+    /// <summary>\n+    /// Dimensions for matrix B (p, q).\n+    /// </summary>\n+    private readonly (int p, int q) _dimsB;\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters (elements in A and B matrices).\n+    /// </summary>\n+    public override int ParameterCount => (_matrixA.Rows * _matrixA.Columns) + (_matrixB.Rows * _matrixB.Columns);\n+","path":"src/NeuralNetworks/Layers/LoKrAdapter.cs","commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","original_commit_id":"c11235933bf7c341c8b4a223cf6ac11b757d1480","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Initialize matrices before exposing them via `ParameterCount`**\n\n`ParameterCount` is queried inside the base `LoRAAdapterBase` constructor. At that moment `_matrixA` and `_matrixB` are still null, so this override will throw during construction (before your own constructor body runs). Allow the property to defer to the base implementation until the matrices are ready.\n\n```diff\n-    public override int ParameterCount => (_matrixA.Rows * _matrixA.Columns) + (_matrixB.Rows * _matrixB.Columns);\n+    public override int ParameterCount =>\n+        _matrixA != null && _matrixB != null\n+            ? (_matrixA.Rows * _matrixA.Columns) + (_matrixB.Rows * _matrixB.Columns)\n+            : base.ParameterCount;\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    /// </summary>\n    public override int ParameterCount =>\n        _matrixA != null && _matrixB != null\n            ? (_matrixA.Rows * _matrixA.Columns) + (_matrixB.Rows * _matrixB.Columns)\n            : base.ParameterCount;\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/Layers/LoKrAdapter.cs around lines 103 to 105, the\noverride of ParameterCount accesses _matrixA/_matrixB which are null when the\nbase LoRAAdapterBase constructor queries this property; change the\nimplementation so it safely defers to the base implementation until the matrices\nare initialized ‚Äî i.e., if either _matrixA or _matrixB is null return\nbase.ParameterCount, otherwise compute and return (_matrixA.Rows *\n_matrixA.Columns) + (_matrixB.Rows * _matrixB.Columns).\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:32:36Z","updated_at":"2025-11-02T02:32:40Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106284","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106284"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106284"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106284/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":103,"original_start_line":103,"start_side":"RIGHT","line":105,"original_line":105,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":105,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106286","pull_request_review_id":3407993158,"id":2484106286,"node_id":"PRRC_kwDOKSXUF86UEHQu","diff_hunk":"@@ -0,0 +1,587 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.NeuralNetworks.Layers;\n+\n+/// <summary>\n+/// LongLoRA adapter that efficiently extends LoRA to handle longer context lengths using shifted sparse attention.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// LongLoRA (2023) addresses the challenge of adapting large language models to longer context windows\n+/// in a parameter-efficient manner. While standard LoRA works well for same-length fine-tuning,\n+/// extending context windows naively would require substantial computational resources.\n+/// </para>\n+/// <para>\n+/// LongLoRA introduces two key innovations:\n+/// 1. Shifted Sparse Attention (S¬≤-Attn): During training only, uses shifted group attention patterns\n+///    that are more efficient while maintaining effectiveness for long contexts\n+/// 2. Dense Attention at Inference: At inference time, switches back to standard dense attention\n+///    for full context utilization without the training overhead\n+/// </para>\n+/// <para><b>For Beginners:</b> LongLoRA makes it affordable to train models on longer sequences.\n+///\n+/// The Problem:\n+/// - Standard LoRA works great for adapting models, but extending context length is expensive\n+/// - Full dense attention on long sequences requires O(n¬≤) computation\n+/// - Training on 32k tokens instead of 2k tokens would be 256x slower!\n+///\n+/// LongLoRA's Solution:\n+/// - Uses a clever \"shifted sparse attention\" trick during training\n+/// - Divides the sequence into groups and shifts them to maintain information flow\n+/// - Much cheaper to train: O(n * k) where k is group size (typically 2048)\n+/// - At inference, uses full dense attention to maintain quality\n+///\n+/// Key Parameters:\n+/// - OriginalContextLength: The base model's context window (e.g., 2048)\n+/// - ExtendedContextLength: The target longer context (e.g., 8192 or 32768)\n+/// - UseShiftedAttention: Enable shifted sparse attention (training only)\n+/// - AttentionShiftSize: How many positions to shift attention groups (usually half the group size)\n+///\n+/// Example Use Case:\n+/// You have a model trained on 2k token contexts but need to process 16k token documents.\n+/// LongLoRA lets you extend the context efficiently:\n+/// - Training: Use shifted sparse attention (much faster)\n+/// - Inference: Use full dense attention (full quality)\n+///\n+/// Comparison to Standard LoRA:\n+/// - Standard LoRA: Efficient parameter adaptation, same context length\n+/// - LongLoRA: Efficient parameter adaptation + context length extension\n+/// - Adds minimal overhead (just the attention shift mechanism)\n+///\n+/// Research Background:\n+/// LongLoRA has been successfully used to extend:\n+/// - LLaMA 2 7B from 4k to 32k context (8x extension)\n+/// - LLaMA 2 13B from 4k to 64k context (16x extension)\n+/// - With only ~10% of the training cost compared to full fine-tuning\n+///\n+/// Reference: LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models (2023)\n+/// https://arxiv.org/abs/2309.12307\n+/// </para>\n+/// </remarks>\n+public class LongLoRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// The original context length that the base model was trained on.\n+    /// </summary>\n+    private readonly int _originalContextLength;\n+\n+    /// <summary>\n+    /// The extended context length that this adapter targets.\n+    /// </summary>\n+    private readonly int _extendedContextLength;\n+\n+    /// <summary>\n+    /// Whether to use shifted sparse attention during training (disabled at inference).\n+    /// </summary>\n+    private bool _useShiftedAttention;\n+\n+    /// <summary>\n+    /// The shift size for shifted sparse attention (typically half the group size).\n+    /// </summary>\n+    private readonly int _attentionShiftSize;\n+\n+    /// <summary>\n+    /// Whether the model is currently in training mode.\n+    /// </summary>\n+    private bool _isTraining;\n+\n+    /// <summary>\n+    /// Gets the original context length of the base model.\n+    /// </summary>\n+    /// <remarks>\n+    /// This is the maximum sequence length the base model was originally trained to handle.\n+    /// Typical values: 512, 1024, 2048, 4096.\n+    /// </remarks>\n+    public int OriginalContextLength => _originalContextLength;\n+\n+    /// <summary>\n+    /// Gets the extended context length this adapter targets.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This is the new, longer context window you want to support after adaptation.\n+    /// Should be larger than OriginalContextLength.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is how long of a sequence your adapted model can handle.\n+    /// For example, extending from 2k to 16k tokens means you can process 8x longer documents!\n+    /// </para>\n+    /// </remarks>\n+    public int ExtendedContextLength => _extendedContextLength;\n+\n+    /// <summary>\n+    /// Gets or sets whether to use shifted sparse attention during forward/backward passes.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// When enabled (training mode):\n+    /// - Uses shifted group attention pattern for efficiency\n+    /// - Divides sequence into groups and shifts them\n+    /// - Significantly reduces computational cost\n+    /// </para>\n+    /// <para>\n+    /// When disabled (inference mode):\n+    /// - Uses standard dense attention\n+    /// - Full context utilization\n+    /// - Better quality but slower\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Enable this during training to save compute, disable it\n+    /// during inference to get the best quality. The training trick doesn't hurt the final\n+    /// model's ability to use full attention at inference time!\n+    /// </para>\n+    /// </remarks>\n+    public bool UseShiftedAttention\n+    {\n+        get => _useShiftedAttention;\n+        set => _useShiftedAttention = value;\n+    }\n+\n+    /// <summary>\n+    /// Gets the attention shift size used in shifted sparse attention.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This determines how much groups are shifted to maintain information flow.\n+    /// Typically set to half the group size (e.g., 1024 for 2048 group size).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is the \"sliding window\" amount that ensures\n+    /// different parts of the sequence can communicate across groups. Too small and\n+    /// information doesn't flow well; too large and you lose the efficiency benefit.\n+    /// </para>\n+    /// </remarks>\n+    public int AttentionShiftSize => _attentionShiftSize;\n+\n+    /// <summary>\n+    /// Gets or sets whether the adapter is in training mode.\n+    /// </summary>\n+    /// <remarks>\n+    /// Training mode affects whether shifted attention is applied.\n+    /// Set to false during inference to use standard dense attention.\n+    /// </remarks>\n+    public bool IsTraining\n+    {\n+        get => _isTraining;\n+        set => _isTraining = value;\n+    }\n+\n+    /// <summary>\n+    /// Initializes a new LongLoRA adapter for efficient context length extension.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with LongLoRA.</param>\n+    /// <param name=\"rank\">The rank of the LoRA decomposition.</param>\n+    /// <param name=\"originalContextLength\">The original context length of the base model.</param>\n+    /// <param name=\"extendedContextLength\">The target extended context length.</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"attentionShiftSize\">The shift size for shifted sparse attention (defaults to originalContextLength/2).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when context lengths or shift size are invalid.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This creates a LongLoRA adapter to extend your model's context window.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to adapt (typically attention layers)\n+    /// - rank: How much LoRA compression to use (8-16 is typical)\n+    /// - originalContextLength: How long sequences your base model handles (e.g., 2048)\n+    /// - extendedContextLength: How long you want to extend it to (e.g., 8192 or 16384)\n+    /// - alpha: LoRA strength (usually equals rank)\n+    /// - attentionShiftSize: How much to shift attention groups (auto-calculated if not specified)\n+    /// - freezeBaseLayer: Whether to freeze original weights (usually true for efficiency)\n+    ///\n+    /// The adapter will use shifted sparse attention during training for efficiency,\n+    /// and you can switch to dense attention during inference for quality.\n+    /// </para>\n+    /// </remarks>\n+    public LongLoRAAdapter(\n+        ILayer<T> baseLayer,\n+        int rank,\n+        int originalContextLength,\n+        int extendedContextLength,\n+        double alpha = -1,\n+        int attentionShiftSize = -1,\n+        bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        if (originalContextLength <= 0)\n+        {\n+            throw new ArgumentException(\"Original context length must be positive\", nameof(originalContextLength));\n+        }\n+\n+        if (extendedContextLength <= originalContextLength)\n+        {\n+            throw new ArgumentException(\"Extended context length must be greater than original context length\", nameof(extendedContextLength));\n+        }\n+\n+        _originalContextLength = originalContextLength;\n+        _extendedContextLength = extendedContextLength;\n+        _useShiftedAttention = true;  // Default to shifted attention for training\n+        _isTraining = true;\n+\n+        // Default shift size is half the original context length (typical for shifted sparse attention)\n+        _attentionShiftSize = attentionShiftSize > 0\n+            ? attentionShiftSize\n+            : originalContextLength / 2;\n+\n+        if (_attentionShiftSize >= originalContextLength)\n+        {\n+            throw new ArgumentException(\"Attention shift size must be less than original context length\", nameof(attentionShiftSize));\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass with optional shifted sparse attention.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor of shape [batchSize, sequenceLength, featureDim].</param>\n+    /// <returns>Output tensor with LoRA adaptation applied.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The forward pass behavior depends on the UseShiftedAttention flag:\n+    /// - When true (training): Applies shifted group attention for efficiency\n+    /// - When false (inference): Uses standard dense attention\n+    /// </para>\n+    /// <para>\n+    /// Shifted Sparse Attention Process:\n+    /// 1. Divide the sequence into groups of size OriginalContextLength\n+    /// 2. Shift alternate groups by AttentionShiftSize positions\n+    /// 3. Apply attention within each group\n+    /// 4. Shift back to restore original positions\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This processes your input through the adapted layer.\n+    ///\n+    /// During training (shifted attention enabled):\n+    /// - Breaks long sequence into manageable chunks\n+    /// - Shifts them to allow cross-chunk communication\n+    /// - Much faster than processing the full sequence at once\n+    ///\n+    /// During inference (shifted attention disabled):\n+    /// - Processes the full sequence with complete attention\n+    /// - Slower but gives best quality\n+    ///\n+    /// The magic is that training with the shifted trick still produces a model\n+    /// that works great with full attention at inference!\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        // If not using shifted attention or not in training mode, use standard LoRA forward\n+        if (!_useShiftedAttention || !_isTraining)\n+        {\n+            return base.Forward(input);\n+        }\n+\n+        // Apply shifted sparse attention during training\n+        Tensor<T> shiftedInput = ApplyShiftedAttention(input);\n+\n+        // Forward through base layer with shifted input\n+        Tensor<T> baseOutput = _baseLayer.Forward(shiftedInput);\n+\n+        // Forward through LoRA layer with shifted input\n+        Tensor<T> loraOutput = _loraLayer.Forward(shiftedInput);\n+\n+        // Sum the outputs\n+        Tensor<T> result = new Tensor<T>(baseOutput.Shape);\n+        for (int i = 0; i < baseOutput.Length; i++)\n+        {\n+            result[i] = NumOps.Add(baseOutput[i], loraOutput[i]);\n+        }\n+\n+        // Reverse the shift to restore original sequence positions\n+        result = ReverseShiftedAttention(result);\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Performs the backward pass with optional shifted sparse attention.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient flowing back from the next layer.</param>\n+    /// <returns>Gradient to pass to the previous layer.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The backward pass mirrors the forward pass behavior:\n+    /// - Applies the same shifting pattern to gradients during training\n+    /// - Ensures gradient flow is consistent with the forward pass attention pattern\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This propagates learning signals backward through the network.\n+    /// It uses the same shifted pattern as the forward pass to ensure the gradients match\n+    /// the attention pattern used during the forward pass.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        // If not using shifted attention or not in training mode, use standard LoRA backward\n+        if (!_useShiftedAttention || !_isTraining)\n+        {\n+            return base.Backward(outputGradient);\n+        }\n+\n+        // Apply shift to output gradient to match forward pass shifting\n+        Tensor<T> shiftedGradient = ApplyShiftedAttention(outputGradient);\n+\n+        // Backward through LoRA layer\n+        Tensor<T> loraInputGrad = _loraLayer.Backward(shiftedGradient);\n+\n+        // Backward through base layer\n+        Tensor<T> baseInputGrad = _baseLayer.Backward(shiftedGradient);\n+\n+        // Sum input gradients\n+        Tensor<T> inputGrad = new Tensor<T>(loraInputGrad.Shape);\n+        for (int i = 0; i < loraInputGrad.Length; i++)\n+        {\n+            inputGrad[i] = NumOps.Add(loraInputGrad[i], baseInputGrad[i]);\n+        }\n+\n+        // Reverse the shift to restore original sequence positions\n+        inputGrad = ReverseShiftedAttention(inputGrad);\n+\n+        // Update parameter gradients vector\n+        UpdateParameterGradientsFromLayers();\n+\n+        return inputGrad;\n+    }\n+\n+    /// <summary>\n+    /// Applies shifted sparse attention pattern to the input tensor.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor to shift.</param>\n+    /// <returns>Tensor with shifted attention pattern applied.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The shifting pattern works as follows:\n+    /// 1. Divide sequence into groups of size OriginalContextLength\n+    /// 2. For alternate groups, shift by AttentionShiftSize positions\n+    /// 3. This creates overlapping attention windows that allow information flow\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Imagine sliding windows along a long document.\n+    /// Instead of having fixed non-overlapping windows, we shift every other window\n+    /// by half its size. This ensures that each part of the document can \"see\"\n+    /// parts from neighboring windows, maintaining information flow while keeping\n+    /// computation efficient.\n+    /// </para>\n+    /// </remarks>\n+    private Tensor<T> ApplyShiftedAttention(Tensor<T> input)\n+    {\n+        // For simplicity, this is a conceptual implementation\n+        // In practice, this would integrate with the attention mechanism\n+        // Here we just apply a circular shift to alternate groups\n+\n+        int sequenceLength = input.Shape.Length > 1 ? input.Shape[1] : input.Length;\n+\n+        // If sequence is shorter than group size, no shifting needed\n+        if (sequenceLength <= _originalContextLength)\n+        {\n+            return input.Clone();\n+        }\n+\n+        Tensor<T> shifted = input.Clone();\n+        int groupSize = _originalContextLength;\n+        int numGroups = (sequenceLength + groupSize - 1) / groupSize;\n+\n+        // Apply shift to alternate groups\n+        for (int g = 1; g < numGroups; g += 2)\n+        {\n+            int groupStart = g * groupSize;\n+            int groupEnd = Math.Min(groupStart + groupSize, sequenceLength);\n+\n+            // Circular shift within this group\n+            ShiftGroup(shifted, groupStart, groupEnd, _attentionShiftSize);\n+        }\n+\n+        return shifted;\n+    }\n+\n+    /// <summary>\n+    /// Reverses the shifted sparse attention pattern to restore original positions.\n+    /// </summary>\n+    /// <param name=\"input\">Tensor with shifted attention pattern.</param>\n+    /// <returns>Tensor with original sequence positions restored.</returns>\n+    /// <remarks>\n+    /// This reverses the shifting applied by ApplyShiftedAttention to restore\n+    /// the output to the original sequence order.\n+    /// </remarks>\n+    private Tensor<T> ReverseShiftedAttention(Tensor<T> input)\n+    {\n+        int sequenceLength = input.Shape.Length > 1 ? input.Shape[1] : input.Length;\n+\n+        // If sequence is shorter than group size, no shifting was applied\n+        if (sequenceLength <= _originalContextLength)\n+        {\n+            return input.Clone();\n+        }\n+\n+        Tensor<T> unshifted = input.Clone();\n+        int groupSize = _originalContextLength;\n+        int numGroups = (sequenceLength + groupSize - 1) / groupSize;\n+\n+        // Reverse shift for alternate groups (shift in opposite direction)\n+        for (int g = 1; g < numGroups; g += 2)\n+        {\n+            int groupStart = g * groupSize;\n+            int groupEnd = Math.Min(groupStart + groupSize, sequenceLength);\n+\n+            // Reverse circular shift within this group\n+            ShiftGroup(unshifted, groupStart, groupEnd, -_attentionShiftSize);\n+        }\n+\n+        return unshifted;\n+    }\n+\n+    /// <summary>\n+    /// Shifts elements within a group by the specified amount.\n+    /// </summary>\n+    /// <param name=\"tensor\">Tensor to modify.</param>\n+    /// <param name=\"groupStart\">Start index of the group.</param>\n+    /// <param name=\"groupEnd\">End index of the group (exclusive).</param>\n+    /// <param name=\"shiftAmount\">Amount to shift (positive for right, negative for left).</param>\n+    /// <remarks>\n+    /// <para>\n+    /// Performs a circular shift within the specified range. Elements that shift\n+    /// past the end wrap around to the beginning.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is like rotating a portion of an array.\n+    /// If you shift [1,2,3,4,5] by 2 positions, you get [4,5,1,2,3].\n+    /// The \"circular\" part means elements wrap around instead of falling off the end.\n+    /// </para>\n+    /// </remarks>\n+    private void ShiftGroup(Tensor<T> tensor, int groupStart, int groupEnd, int shiftAmount)\n+    {\n+        int groupSize = groupEnd - groupStart;\n+        if (groupSize <= 0)\n+        {\n+            return;\n+        }\n+\n+        // Normalize shift amount to be within group size\n+        shiftAmount = shiftAmount % groupSize;\n+        if (shiftAmount < 0)\n+        {\n+            shiftAmount += groupSize;\n+        }\n+\n+        if (shiftAmount == 0)\n+        {\n+            return;\n+        }\n+\n+        // Create temporary buffer for the group\n+        T[] buffer = new T[groupSize];\n+\n+        // Copy group to buffer\n+        for (int i = 0; i < groupSize; i++)\n+        {\n+            buffer[i] = tensor[groupStart + i];\n+        }\n+\n+        // Write back with shift\n+        for (int i = 0; i < groupSize; i++)\n+        {\n+            int newPos = (i + shiftAmount) % groupSize;\n+            tensor[groupStart + newPos] = buffer[i];\n+        }\n+    }","path":"src/NeuralNetworks/Layers/LongLoRAAdapter.cs","commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","original_commit_id":"c11235933bf7c341c8b4a223cf6ac11b757d1480","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Shifted-attention implementation corrupts multi-dimensional tensors.**\n\nLines 362-481: `ShiftGroup` treats each scalar in the flattened buffer as an independent position, but `Tensor<T>` here represents `[batch, sequence, feature]`. Rotating by `groupStart + i` moves individual feature scalars instead of whole token slices, so the operation scrambles feature vectors and even mixes batches when `batch > 1`. Example: with shape `[1,4,2]`, `groupSize = 2` and `shift = 1`, the code rotates only two scalars when each token spans `featureDim` values, destroying the token structure. Please shift per token (and per batch) by copying blocks of `featureDim` values‚Äîor use tensor slicing helpers‚Äîso entire tokens move together.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/Layers/LongLoRAAdapter.cs around lines 362-481, the\nShiftGroup implementation rotates scalar elements from the flattened tensor\nwhich corrupts multi-dimensional tensors ([batch, sequence, feature]) by moving\nindividual feature scalars and mixing batches; change the logic to rotate whole\ntoken blocks not scalars: compute featureDim from input.Shape (handle rank 2 as\nfeatureDim=1 and rank 3 as Shape[2]), treat groupSize as number of tokens,\nallocate a buffer of length groupSize * featureDim, copy token-sized blocks\n(featureDim elements) into the buffer per batch and sequence range, then write\nthem back shifted by block positions (adjust indices by batch offset), ensuring\nyou perform the operation per batch to avoid cross-batch mixing and normalize\nshiftAmount by groupSize (tokens) not by feature count.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:32:36Z","updated_at":"2025-11-02T02:32:40Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106286","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106286"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106286"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106286/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":362,"original_start_line":362,"start_side":"RIGHT","line":481,"original_line":481,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":481,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106287","pull_request_review_id":3407993158,"id":2484106287,"node_id":"PRRC_kwDOKSXUF86UEHQv","diff_hunk":"@@ -0,0 +1,393 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.NeuralNetworks.Layers;\n+\n+/// <summary>\n+/// LoRA-FA (LoRA with Frozen A matrix) adapter for parameter-efficient fine-tuning.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// LoRA-FA is a variant of standard LoRA that freezes matrix A after random initialization and only\n+/// trains matrix B. This provides approximately 50% parameter reduction compared to standard LoRA\n+/// with minimal performance loss in most scenarios.\n+/// </para>\n+/// <para><b>For Beginners:</b> LoRA-FA makes LoRA even more efficient!\n+///\n+/// Standard LoRA uses two small matrices (A and B) that both get trained:\n+/// - Matrix A: Compresses input (trained)\n+/// - Matrix B: Expands to output (trained)\n+///\n+/// LoRA-FA optimizes this further:\n+/// - Matrix A: Compresses input (frozen - never changes after initialization)\n+/// - Matrix B: Expands to output (trained - the only thing that learns)\n+///\n+/// Why freeze matrix A?\n+/// - Research shows matrix A can be randomly initialized and frozen without much performance loss\n+/// - This cuts trainable parameters in half (only matrix B is trained)\n+/// - Training is faster and uses less memory\n+/// - Perfect when you need maximum efficiency\n+///\n+/// Example parameter counts for a 1000√ó1000 layer with rank=8:\n+/// - Standard LoRA: 8,000 (A) + 8,000 (B) = 16,000 trainable parameters\n+/// - LoRA-FA: 0 (A frozen) + 8,000 (B) = 8,000 trainable parameters (50% reduction!)\n+///\n+/// When to use LoRA-FA:\n+/// - Memory is very limited\n+/// - Training speed is critical\n+/// - You can tolerate a small performance trade-off\n+/// - You're working with very large models\n+/// </para>\n+/// </remarks>\n+public class LoRAFAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Whether matrix A is frozen (always true for LoRA-FA).\n+    /// </summary>\n+    private readonly bool _freezeMatrixA = true;\n+\n+    /// <summary>\n+    /// Gets whether matrix A is frozen during training (always true for LoRA-FA).\n+    /// </summary>\n+    /// <remarks>\n+    /// This is a key characteristic of LoRA-FA - matrix A is randomly initialized\n+    /// and then frozen, never updated during training.\n+    /// </remarks>\n+    public bool IsMatrixAFrozen => _freezeMatrixA;\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters (only matrix B).\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// For LoRA-FA, only matrix B is trainable. Matrix A is frozen, so it doesn't count\n+    /// toward trainable parameters. This results in approximately 50% parameter reduction\n+    /// compared to standard LoRA.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This returns how many parameters will actually be trained.\n+    /// Since matrix A is frozen, we only count matrix B's parameters. If the base layer is\n+    /// also frozen (typical case), this is just matrix B. Otherwise, it's base layer + matrix B.\n+    ///\n+    /// For a layer with input size 1000, output size 1000, and rank 8:\n+    /// - Matrix B size: rank √ó outputSize = 8 √ó 1000 = 8,000 parameters\n+    /// - Matrix A size: inputSize √ó rank = 1000 √ó 8 = 8,000 parameters (but frozen, so not counted)\n+    /// - Total trainable: 8,000 (50% less than standard LoRA's 16,000)\n+    /// </para>\n+    /// </remarks>\n+    public override int ParameterCount\n+    {\n+        get\n+        {\n+            // Only count matrix B parameters (matrix A is frozen)\n+            int matrixBParams = _loraLayer.Rank * GetOutputShape()[0];\n+\n+            // Add base layer parameters if not frozen\n+            if (!_freezeBaseLayer)\n+            {\n+                return _baseLayer.ParameterCount + matrixBParams;\n+            }\n+\n+            return matrixBParams;\n+        }\n+    }","path":"src/NeuralNetworks/Layers/LoRAFAAdapter.cs","commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","original_commit_id":"c11235933bf7c341c8b4a223cf6ac11b757d1480","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**ParameterCount override breaks base-class invariants.**\n\nLine 77: `LoRAAdapterBase`‚Äôs constructor allocates `Parameters = new Vector<T>(ParameterCount)` and its private helpers immediately pack the entire LoRA parameter vector (matrix‚ÄØA + matrix‚ÄØB). By overriding `ParameterCount` to return only matrix‚ÄØB, the base constructor now allocates a shorter buffer, and the subsequent copy overruns it (and later `UpdateParameterGradientsFromLayers()` also writes past the end), so the adapter throws an `IndexOutOfRangeException` as soon as it‚Äôs instantiated. Because those helpers are private, the new `UpdateParametersFromLayers()` here never runs in those code paths. Please keep `ParameterCount` aligned with `_loraLayer.ParameterCount` (or refactor the base class to make the layout overridable) and restrict the freeze logic to `UpdateParameters` so the base-class invariants remain intact.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/Layers/LoRAFAAdapter.cs around lines 77 to 92, the\noverride of ParameterCount currently returns only matrix B sizes which breaks\nbase-class invariants (the base constructor expects the full LoRA parameter size\nand packs A+B), causing buffer overruns; restore ParameterCount to match\n_loraLayer.ParameterCount (i.e., the full A+B count) so the base constructor\nallocates the correct buffer, and move any freeze behavior into\nUpdateParameters/UpdateParametersFromLayers so only update/copy operations skip\nmatrix A when _freezeBaseLayer is true rather than changing the parameter vector\nlength.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:32:37Z","updated_at":"2025-11-02T02:32:40Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106287","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106287"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106287"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106287/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":77,"original_start_line":77,"start_side":"RIGHT","line":92,"original_line":92,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":92,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106289","pull_request_review_id":3407993158,"id":2484106289,"node_id":"PRRC_kwDOKSXUF86UEHQx","diff_hunk":"@@ -0,0 +1,789 @@\n+using AiDotNet.DecompositionMethods.MatrixDecomposition;\n+using AiDotNet.Enums.AlgorithmTypes;\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.NeuralNetworks.Layers;\n+\n+/// <summary>\n+/// LoRA-XS (Extremely Small) adapter for ultra-parameter-efficient fine-tuning using SVD with trainable scaling matrix.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// LoRA-XS achieves extreme parameter efficiency by leveraging SVD of pretrained weights to create frozen\n+/// orthonormal bases (U and V matrices), with only a small r√ór trainable matrix R positioned between them.\n+/// This architecture reduces parameter count to r¬≤ instead of 2nr (standard LoRA), achieving 100x+ reduction\n+/// while matching or exceeding full fine-tuning performance.\n+/// </para>\n+/// <para><b>Architecture Comparison:</b>\n+/// - Standard LoRA: W' = W + BA, where A ‚àà ‚Ñù^(d√ór), B ‚àà ‚Ñù^(r√ód) (2dr parameters)\n+/// - LoRA-XS: W' = W + U_r Œ£_r R V_r^T, where only R ‚àà ‚Ñù^(r√ór) is trainable (r¬≤ parameters)\n+/// - U_r and V_r are frozen orthonormal bases from SVD of pretrained W\n+/// - Œ£_r is the frozen diagonal matrix of top-r singular values\n+/// </para>\n+/// <para><b>Key Innovation:</b>\n+/// Instead of training both A and B matrices (standard LoRA), LoRA-XS:\n+/// 1. Computes SVD of pretrained weights: W = U Œ£ V^T\n+/// 2. Freezes U_r (top-r left singular vectors) and V_r^T (top-r right singular vectors)\n+/// 3. Freezes Œ£_r (top-r singular values as diagonal matrix)\n+/// 4. Trains only R (r√ór mixing matrix) that interpolates between frozen bases\n+/// 5. Parameter count is independent of hidden dimensions: only r¬≤ trainable parameters\n+/// </para>\n+/// <para><b>Performance Metrics (from paper):</b>\n+///\n+/// RoBERTa-large on GLUE (6 tasks):\n+/// - LoRA-XS (rank 16): 88.03% avg accuracy, 24.6K parameters\n+/// - Standard LoRA (rank 16): Similar accuracy, 100x more parameters\n+/// - Full fine-tuning: 88.0% avg accuracy, ~125M parameters per task\n+///\n+/// LLaMA2-7B on Commonsense Reasoning:\n+/// - LoRA-XS: 80.5% avg accuracy, 3.67M parameters\n+/// - Standard LoRA: 77.6% avg accuracy, 56M parameters (15x more)\n+///\n+/// Mistral-7B on GSM8K (Math Reasoning):\n+/// - LoRA-XS: 70.35% accuracy, 3.67M parameters\n+/// - Standard LoRA: 67.70% accuracy, 168M parameters (46x more)\n+///\n+/// GPT-3 Personalization (1M models):\n+/// - LoRA-XS: 96GB total storage\n+/// - Standard LoRA: 144TB total storage (1500x reduction)\n+/// </para>\n+/// <para><b>Mathematical Formulation:</b>\n+/// Forward pass computes:\n+///   output = (W + U_r Œ£_r R V_r^T) * input\n+///          = W * input + (U_r Œ£_r) * (R * (V_r^T * input))\n+///\n+/// Where:\n+/// - W is frozen pretrained weights\n+/// - U_r ‚àà ‚Ñù^(d_out √ó r): frozen left singular vectors (orthonormal columns)\n+/// - Œ£_r ‚àà ‚Ñù^(r √ó r): frozen diagonal matrix of singular values\n+/// - R ‚àà ‚Ñù^(r √ó r): trainable mixing matrix (only trainable component!)\n+/// - V_r^T ‚àà ‚Ñù^(r √ó d_in): frozen right singular vectors (orthonormal rows)\n+/// </para>\n+/// <para><b>Why This Works:</b>\n+/// The SVD provides an optimal orthonormal basis for representing weight updates. By freezing\n+/// these bases and training only the mixing matrix R, LoRA-XS achieves:\n+/// - Drastically fewer parameters (r¬≤ vs 2dr)\n+/// - Better generalization (constrained to pretrained subspace)\n+/// - Faster convergence (optimal basis from initialization)\n+/// - No inference overhead (can be merged back into W)\n+/// - Scalable personalization (parameter count independent of model size)\n+/// </para>\n+/// <para><b>For Beginners:</b> Think of LoRA-XS as \"ultra-compressed LoRA\".\n+///\n+/// Imagine you have a large language model with huge weight matrices (e.g., 4096√ó4096):\n+///\n+/// Standard LoRA (rank 8):\n+/// - Creates two matrices: A (4096√ó8) and B (8√ó4096)\n+/// - Total parameters: 4096*8 + 8*4096 = 65,536 parameters\n+/// - Both matrices are trainable\n+///\n+/// LoRA-XS (rank 8):\n+/// - Decomposes pretrained weights with SVD into U, Œ£, V\n+/// - Keeps top 8 singular vectors (U_8, Œ£_8, V_8) FROZEN\n+/// - Trains only R matrix: 8√ó8 = 64 parameters\n+/// - Achieves similar or better performance with 1000x fewer parameters!\n+///\n+/// It's like having two fixed \"coordinate systems\" from the pretrained model,\n+/// and you only train a small \"rotation matrix\" between them. The fixed coordinate\n+/// systems capture the pretrained knowledge, while the rotation matrix adapts to your task.\n+///\n+/// Example workflow:\n+/// 1. Load pretrained model weights W\n+/// 2. Compute SVD: W = U Œ£ V^T\n+/// 3. Extract top-r components: U_r, Œ£_r, V_r\n+/// 4. Create LoRA-XS adapter with these frozen bases\n+/// 5. Train only the tiny R matrix (64 params for rank 8)\n+/// 6. Deploy with merged weights: W' = W + U_r Œ£_r R V_r^T\n+/// </para>\n+/// <para><b>References:</b>\n+/// - Paper: \"LoRA-XS: Low-Rank Adaptation with Extremely Small Number of Parameters\"\n+/// - arXiv: 2405.17604 (May 2024)\n+/// - GitHub: MohammadrezaBanaei/LoRA-XS\n+/// - Key Innovation: Parameter count O(r¬≤) instead of O(dr), enabling extreme efficiency\n+/// </para>\n+/// </remarks>\n+public class LoRAXSAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Frozen left singular vectors (U_r) from SVD of pretrained weights.\n+    /// Shape: [outputSize, rank]\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// These are the top-r left singular vectors from the SVD decomposition of pretrained weights.\n+    /// They form an orthonormal basis for the output space and remain frozen during training.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This matrix contains the most important \"output patterns\" from\n+    /// the pretrained model. It's like having a fixed set of \"building blocks\" that the model\n+    /// learned during pretraining. We keep these fixed and only learn how to combine them.\n+    /// </para>\n+    /// </remarks>\n+    private Matrix<T>? _frozenU;\n+\n+    /// <summary>\n+    /// Frozen singular values (diagonal of Œ£_r) from SVD of pretrained weights.\n+    /// Length: rank\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// These are the top-r singular values from the SVD decomposition. They represent the\n+    /// importance/strength of each corresponding singular vector pair. Stored as a vector\n+    /// representing the diagonal of Œ£_r matrix.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> These numbers tell you how important each \"pattern\" is.\n+    /// Larger values mean more important patterns. We keep the top-r most important ones\n+    /// and use them to scale the contributions during forward pass.\n+    /// </para>\n+    /// </remarks>\n+    private Vector<T>? _frozenSigma;\n+\n+    /// <summary>\n+    /// Frozen right singular vectors transposed (V_r^T) from SVD of pretrained weights.\n+    /// Shape: [rank, inputSize]\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// These are the top-r right singular vectors (transposed) from the SVD decomposition.\n+    /// They form an orthonormal basis for the input space and remain frozen during training.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This matrix contains the most important \"input patterns\" from\n+    /// the pretrained model. Like U, these are fixed building blocks. Together, U and V define\n+    /// the coordinate system in which we'll make small adjustments via the R matrix.\n+    /// </para>\n+    /// </remarks>\n+    private Matrix<T>? _frozenVt;\n+\n+    /// <summary>\n+    /// Trainable r√ór mixing matrix R - the ONLY trainable parameters in LoRA-XS.\n+    /// Shape: [rank, rank]\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This is the core trainable component of LoRA-XS. It's a small r√ór matrix that learns\n+    /// how to mix/interpolate between the frozen singular vector bases. The forward pass computes:\n+    /// adaptation = U_r * Œ£_r * R * V_r^T, where only R is updated during training.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This tiny matrix (e.g., 8√ó8 = 64 parameters for rank 8) is\n+    /// what actually gets trained! It learns how to \"rotate\" or \"mix\" between the frozen patterns\n+    /// in U and V to adapt to your specific task. This is where all the magic happens with\n+    /// minimal parameters.\n+    /// </para>\n+    /// </remarks>\n+    private Matrix<T> _trainableR;\n+\n+    /// <summary>\n+    /// Gradient of the trainable R matrix computed during backpropagation.\n+    /// </summary>\n+    private Matrix<T>? _trainableRGradient;\n+\n+    /// <summary>\n+    /// Intermediate result from forward pass: V_r^T * input\n+    /// Cached for use in backward pass.\n+    /// </summary>\n+    private Tensor<T>? _cachedVtInput;\n+\n+    /// <summary>\n+    /// Intermediate result from forward pass: R * (V_r^T * input)\n+    /// Cached for use in backward pass.\n+    /// </summary>\n+    private Tensor<T>? _cachedRVtInput;\n+\n+    /// <summary>\n+    /// Intermediate result from forward pass: Œ£_r * R * (V_r^T * input)\n+    /// Cached for use in backward pass.\n+    /// </summary>\n+    private Tensor<T>? _cachedSigmaRVtInput;\n+\n+    /// <summary>\n+    /// Indicates whether the adapter was initialized from SVD of pretrained weights.\n+    /// </summary>\n+    private bool _initializedFromSVD;\n+\n+    /// <summary>\n+    /// Gets whether this adapter was initialized from SVD.\n+    /// </summary>\n+    /// <remarks>\n+    /// Returns true if InitializeFromSVD was called successfully. Without SVD initialization,\n+    /// LoRA-XS loses its key advantages and effectively becomes a very limited random adapter.\n+    /// </remarks>\n+    public bool InitializedFromSVD => _initializedFromSVD;\n+\n+    /// <summary>\n+    /// Gets the frozen U matrix (left singular vectors).\n+    /// </summary>\n+    public Matrix<T>? FrozenU => _frozenU?.Clone();\n+\n+    /// <summary>\n+    /// Gets the frozen singular values.\n+    /// </summary>\n+    public Vector<T>? FrozenSigma => _frozenSigma?.Clone();\n+\n+    /// <summary>\n+    /// Gets the frozen V^T matrix (right singular vectors transposed).\n+    /// </summary>\n+    public Matrix<T>? FrozenVt => _frozenVt?.Clone();\n+\n+    /// <summary>\n+    /// Gets the trainable R matrix.\n+    /// </summary>\n+    public Matrix<T> TrainableR => _trainableR.Clone();\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters (only r¬≤ for the R matrix).\n+    /// </summary>\n+    /// <remarks>\n+    /// LoRA-XS parameter count is rank¬≤ (r¬≤), independent of the layer dimensions.\n+    /// This is dramatically smaller than standard LoRA's 2 * rank * dimension.\n+    /// </remarks>\n+    public override int ParameterCount => Rank * Rank;\n+","path":"src/NeuralNetworks/Layers/LoRAXSAdapter.cs","commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","original_commit_id":"c11235933bf7c341c8b4a223cf6ac11b757d1480","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Critical mismatch in `ParameterCount` crashes the adapter constructor.**\n\n`LoRAAdapterBase`‚Äôs ctor immediately calls `UpdateParametersFromLayers()`, which copies all `_loraLayer` parameters (size = inputSize¬∑rank + rank¬∑outputSize) into `Parameters`. After overriding `ParameterCount` to `rank * rank`, `Parameters` is far too short, so the first instantiation throws `IndexOutOfRangeException`. Either keep `ParameterCount` aligned with the underlying LoRA layer or override the base packing logic so the vector length still matches what the base ctor writes. Right now the class can‚Äôt even be constructed.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/Layers/LoRAXSAdapter.cs around lines 232-240,\nParameterCount currently returns Rank*Rank which is too short for the base\nconstructor's packing (it expects the full underlying LoRA layer size), causing\nIndexOutOfRangeException; change ParameterCount to return the actual number of\nparameters the base ctor writes (inputSize * Rank + Rank * outputSize / or\nwhatever the underlying _loraLayer uses ‚Äî compute it from the layer dimensions\navailable to the adapter) so the Parameters vector is allocated the correct\nlength, or alternatively override the packing/update method used by the base\nctor so it doesn't write the full layer parameters into a smaller vector; ensure\nthe adapter still exposes an rSquared property if you need rank*rank separately\nbut do not let ParameterCount be smaller than the base packing expects.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:32:37Z","updated_at":"2025-11-02T02:32:40Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106289","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106289"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106289"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106289/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":232,"original_start_line":232,"start_side":"RIGHT","line":240,"original_line":240,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":240,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106294","pull_request_review_id":3407993158,"id":2484106294,"node_id":"PRRC_kwDOKSXUF86UEHQ2","diff_hunk":"@@ -0,0 +1,928 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.NeuralNetworks.Layers;\n+\n+/// <summary>\n+/// LoRETTA (Low-Rank Economic Tensor-Train Adaptation) adapter for parameter-efficient fine-tuning.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// LoRETTA extends LoRA by using tensor-train decomposition instead of simple matrix factorization.\n+/// Instead of representing weight updates as W = A √ó B, LoRETTA uses a tensor-train decomposition\n+/// that captures higher-order correlations with even fewer parameters.\n+/// </para>\n+/// <para>\n+/// Tensor-train decomposition represents a high-dimensional tensor as a sequence of lower-dimensional\n+/// \"cores\" that are contracted together. For a weight matrix W of size (m √ó n), the tensor-train\n+/// representation is:\n+///\n+/// W[i,j] = G1[i] √ó G2 √ó G3 √ó ... √ó Gd[j]\n+///\n+/// where each core Gk has dimensions (r_{k-1} √ó n_k √ó r_k), and r_k are the TT-ranks.\n+/// The boundary ranks are r_0 = r_d = 1.\n+/// </para>\n+/// <para><b>For Beginners:</b> LoRETTA is an advanced version of LoRA that uses \"tensor-train decomposition\"!\n+///\n+/// Standard LoRA uses two matrices (A and B) to approximate weight changes:\n+/// - Matrix A: Compresses input to rank dimensions\n+/// - Matrix B: Expands back to output dimensions\n+/// - Parameters: inputSize √ó rank + rank √ó outputSize\n+///\n+/// LoRETTA uses multiple small \"cores\" chained together:\n+/// - Instead of 2 large matrices, use many small tensors\n+/// - Each core captures local correlations\n+/// - The cores are \"contracted\" (multiplied in sequence)\n+/// - Can express more complex patterns with fewer parameters\n+///\n+/// Why tensor-train decomposition?\n+/// 1. More expressive: Can capture higher-order correlations\n+/// 2. More efficient: Fewer parameters than matrix factorization\n+/// 3. Better compression: Exploits structure in weight updates\n+/// 4. Scalable: Grows logarithmically with dimensions\n+///\n+/// Example parameter counts for 1000√ó1000 layer:\n+/// - Full update: 1,000,000 parameters\n+/// - Standard LoRA (rank=8): 16,000 parameters (98.4% reduction)\n+/// - LoRETTA (rank=4, 3 cores): ~6,000 parameters (99.4% reduction, even better!)\n+///\n+/// Key parameters:\n+/// - ttRank: Controls compression (like LoRA's rank but more powerful)\n+/// - numCores: How many tensor cores in the chain (typically 3-5)\n+/// - alpha: Scaling factor for the adaptation strength\n+///\n+/// When to use LoRETTA:\n+/// - Maximum parameter efficiency needed\n+/// - Weight updates have higher-order structure\n+/// - You have very large layers to adapt\n+/// - Standard LoRA isn't expressive enough at low ranks\n+///\n+/// Reference:\n+/// Tensor-train decomposition: I. V. Oseledets, \"Tensor-train decomposition,\"\n+/// SIAM J. Scientific Computing, 2011.\n+/// </para>\n+/// </remarks>\n+public class LoRETTAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Tensor-train cores representing the weight decomposition.\n+    /// Core k has shape (ttRanks[k-1], coreShape[k], ttRanks[k]).\n+    /// </summary>\n+    private readonly List<Tensor<T>> _ttCores;\n+\n+    /// <summary>\n+    /// The ranks of the tensor-train decomposition.\n+    /// Length is numCores + 1, with ttRanks[0] = ttRanks[numCores] = 1.\n+    /// </summary>\n+    private readonly int[] _ttRanks;\n+\n+    /// <summary>\n+    /// The shape of each core in the tensor-train.\n+    /// </summary>\n+    private readonly int[] _coreShapes;\n+\n+    /// <summary>\n+    /// Number of cores in the tensor-train.\n+    /// </summary>\n+    private readonly int _numCores;\n+\n+    /// <summary>\n+    /// Gradients for each TT core computed during backpropagation.\n+    /// </summary>\n+    private List<Tensor<T>>? _ttCoreGradients;\n+\n+    /// <summary>\n+    /// Cached intermediate tensors from forward pass, needed for gradient computation.\n+    /// </summary>\n+    private List<Tensor<T>>? _forwardIntermediates;\n+\n+    /// <summary>\n+    /// Gets the tensor-train rank.\n+    /// </summary>\n+    /// <remarks>\n+    /// This is the maximum rank in the tensor-train decomposition. Lower rank means\n+    /// more compression but less expressiveness.\n+    /// </remarks>\n+    public int TTRank => _ttRanks.Max();\n+\n+    /// <summary>\n+    /// Gets the number of cores in the tensor-train.\n+    /// </summary>\n+    public int NumCores => _numCores;\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters in the tensor-train cores.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// The total parameters is the sum of all core sizes:\n+    /// sum_k (ttRanks[k-1] √ó coreShapes[k] √ó ttRanks[k])\n+    /// </para>\n+    /// <para>\n+    /// This is typically much smaller than standard LoRA for the same expressiveness.\n+    /// </para>\n+    /// </remarks>\n+    public override int ParameterCount\n+    {\n+        get\n+        {\n+            int ttParams = 0;\n+            for (int k = 0; k < _numCores; k++)\n+            {\n+                ttParams += _ttRanks[k] * _coreShapes[k] * _ttRanks[k + 1];\n+            }\n+\n+            // Add base layer parameters if not frozen\n+            if (!_freezeBaseLayer)\n+            {\n+                return _baseLayer.ParameterCount + ttParams;\n+            }\n+\n+            return ttParams;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Initializes a new LoRETTA adapter wrapping an existing layer.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with LoRETTA.</param>\n+    /// <param name=\"ttRank\">The rank of the tensor-train decomposition.</param>\n+    /// <param name=\"numCores\">Number of cores in the tensor-train (default: 3).</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to ttRank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when ttRank or numCores are invalid.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This creates a LoRETTA adapter that wraps any layer.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to adapt efficiently\n+    /// - ttRank: Controls compression (lower = fewer parameters, less flexibility)\n+    /// - numCores: How many tensor cores to use (more cores = more expressive but more params)\n+    /// - alpha: How strong the adaptation is\n+    /// - freezeBaseLayer: Whether to lock the original layer's weights (usually true)\n+    ///\n+    /// The cores are initialized carefully:\n+    /// - First and last cores connect to input/output dimensions\n+    /// - Middle cores have uniform shapes\n+    /// - All cores start with small random values (Gaussian initialization)\n+    /// - Designed so initial LoRETTA has minimal effect\n+    ///\n+    /// Recommended settings:\n+    /// - ttRank=4 to 8: Good balance of efficiency and expressiveness\n+    /// - numCores=3: Standard choice (input core, middle core, output core)\n+    /// - numCores=4-5: For very large layers or complex adaptations\n+    /// </para>\n+    /// </remarks>\n+    public LoRETTAAdapter(\n+        ILayer<T> baseLayer,\n+        int ttRank,\n+        int numCores = 3,\n+        double alpha = -1,\n+        bool freezeBaseLayer = true)\n+        : base(baseLayer, ttRank, alpha, freezeBaseLayer)\n+    {\n+        if (ttRank <= 0)\n+        {\n+            throw new ArgumentException(\"TT-rank must be positive\", nameof(ttRank));\n+        }\n+\n+        if (numCores < 2)\n+        {\n+            throw new ArgumentException(\"Number of cores must be at least 2\", nameof(numCores));\n+        }\n+\n+        _numCores = numCores;\n+\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+\n+        // Initialize TT-ranks: [1, ttRank, ttRank, ..., ttRank, 1]\n+        _ttRanks = new int[numCores + 1];\n+        _ttRanks[0] = 1;\n+        _ttRanks[numCores] = 1;\n+        for (int k = 1; k < numCores; k++)\n+        {\n+            _ttRanks[k] = ttRank;\n+        }\n+\n+        // Compute core shapes by factorizing input and output dimensions\n+        _coreShapes = ComputeCoreShapes(inputSize, outputSize, numCores);\n+\n+        // Initialize TT cores\n+        _ttCores = new List<Tensor<T>>(numCores);\n+        InitializeTTCores();\n+\n+        // Update parameter vector\n+        Parameters = new Vector<T>(ParameterCount);\n+        UpdateParametersFromCores();\n+    }\n+\n+    /// <summary>\n+    /// Computes the shape of each core by factorizing the total dimension.\n+    /// </summary>\n+    /// <param name=\"inputSize\">Input dimension.</param>\n+    /// <param name=\"outputSize\">Output dimension.</param>\n+    /// <param name=\"numCores\">Number of cores.</param>\n+    /// <returns>Array of core shapes.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// We need to factorize the total dimensionality (inputSize √ó outputSize) across the cores.\n+    /// The product of all core shapes should approximately equal inputSize √ó outputSize.\n+    ///\n+    /// Strategy: Use geometric decomposition\n+    /// - First core: ~inputSize^(1/2) √ó outputSize^(1/(numCores-1))\n+    /// - Last core: ~inputSize^(1/2) √ó outputSize^(1/(numCores-1))\n+    /// - Middle cores: uniform sizes based on geometric mean\n+    /// </para>\n+    /// </remarks>\n+    private int[] ComputeCoreShapes(int inputSize, int outputSize, int numCores)\n+    {\n+        int[] shapes = new int[numCores];\n+\n+        // Total \"logical\" dimension to decompose\n+        double totalDim = Math.Sqrt((double)inputSize * outputSize);\n+\n+        // Use geometric factorization\n+        double dimPerCore = Math.Pow(totalDim, 2.0 / numCores);\n+\n+        // Ensure each core has at least dimension 2\n+        int baseDim = Math.Max(2, (int)Math.Ceiling(dimPerCore));\n+\n+        // Distribute dimensions\n+        for (int k = 0; k < numCores; k++)\n+        {\n+            shapes[k] = baseDim;\n+        }\n+\n+        // Adjust first and last cores to better match input/output sizes\n+        shapes[0] = Math.Max(2, (int)Math.Ceiling(Math.Sqrt(inputSize)));\n+        shapes[numCores - 1] = Math.Max(2, (int)Math.Ceiling(Math.Sqrt(outputSize)));\n+\n+        return shapes;\n+    }\n+\n+    /// <summary>\n+    /// Initializes all TT cores with small random values.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Each core is initialized with Gaussian noise scaled by 1/sqrt(product of dimensions).\n+    /// This ensures the overall adaptation starts small.\n+    /// </para>\n+    /// </remarks>\n+    private void InitializeTTCores()\n+    {\n+        Random random = new Random(42);\n+\n+        for (int k = 0; k < _numCores; k++)\n+        {\n+            int leftRank = _ttRanks[k];\n+            int coreShape = _coreShapes[k];\n+            int rightRank = _ttRanks[k + 1];\n+\n+            // Core has shape [leftRank, coreShape, rightRank]\n+            int[] shape = new int[] { leftRank, coreShape, rightRank };\n+            Tensor<T> core = new Tensor<T>(shape);\n+\n+            // Initialize with small Gaussian noise\n+            double scale = 1.0 / Math.Sqrt(leftRank * coreShape * rightRank);\n+\n+            for (int i = 0; i < core.Length; i++)\n+            {\n+                // Box-Muller transform for Gaussian random numbers\n+                double u1 = random.NextDouble();\n+                double u2 = random.NextDouble();\n+                double randStdNormal = Math.Sqrt(-2.0 * Math.Log(u1)) * Math.Sin(2.0 * Math.PI * u2);\n+                core[i] = NumOps.Multiply(NumOps.FromDouble(randStdNormal), NumOps.FromDouble(scale));\n+            }\n+\n+            _ttCores.Add(core);\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass through the LoRETTA adapter.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Sum of base layer output and LoRETTA output.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The forward pass computes the tensor-train contraction to produce the adaptation,\n+    /// then adds it to the base layer output.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This processes input through both the original layer and\n+    /// the LoRETTA adaptation, then combines them.\n+    ///\n+    /// The LoRETTA forward pass:\n+    /// 1. Forward through base layer (original behavior)\n+    /// 2. Contract tensor-train cores with input (compute adaptation)\n+    /// 3. Add base output + adaptation output\n+    ///\n+    /// The tensor contraction is done sequentially through the cores, which is efficient\n+    /// even though it looks complex mathematically.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        // Store intermediates for backward pass\n+        _forwardIntermediates = new List<Tensor<T>>();\n+\n+        // Forward through base layer\n+        Tensor<T> baseOutput = _baseLayer.Forward(input);\n+\n+        // Compute LoRETTA adaptation via tensor-train contraction\n+        Tensor<T> ttOutput = ComputeTensorTrainForward(input);\n+\n+        // Sum the outputs\n+        Tensor<T> result = new Tensor<T>(baseOutput.Shape);\n+        for (int i = 0; i < baseOutput.Length; i++)\n+        {\n+            result[i] = NumOps.Add(baseOutput[i], ttOutput[i]);\n+        }\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Computes the forward pass through the tensor-train decomposition.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor of shape [batchSize, inputSize].</param>\n+    /// <returns>Output tensor of shape [batchSize, outputSize].</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This performs the tensor-train contraction:\n+    /// 1. Reshape input to match first core dimensions\n+    /// 2. Contract through each core sequentially\n+    /// 3. Reshape output to match expected output dimensions\n+    /// </para>\n+    /// </remarks>\n+    private Tensor<T> ComputeTensorTrainForward(Tensor<T> input)\n+    {\n+        int batchSize = input.Shape[0];\n+        int inputSize = input.Shape.Length > 1 ? input.Shape[1] : input.Length;\n+\n+        // Start with input reshaped to work with first core\n+        // For simplicity, we'll use a matrix-based contraction approach\n+\n+        // Flatten input to [batchSize √ó inputSize]\n+        Matrix<T> currentMatrix = new Matrix<T>(batchSize, inputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                currentMatrix[i, j] = input[i * inputSize + j];\n+            }\n+        }\n+\n+        // Contract through each core\n+        for (int k = 0; k < _numCores; k++)\n+        {\n+            currentMatrix = ContractWithCore(currentMatrix, _ttCores[k], k);\n+\n+            // Store intermediate for backward pass\n+            if (_forwardIntermediates != null)\n+            {\n+                _forwardIntermediates.Add(TensorFromMatrix(currentMatrix));\n+            }\n+        }\n+\n+        // Extract output\n+        int outputSize = GetOutputShape()[0];\n+        Vector<T> outputData = new Vector<T>(batchSize * outputSize);\n+\n+        int idx = 0;\n+        int currentCols = currentMatrix.Columns;\n+        int outputCols = Math.Min(outputSize, currentCols);\n+\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                if (j < outputCols && i < currentMatrix.Rows)\n+                {\n+                    outputData[idx] = currentMatrix[i, j % currentMatrix.Columns];\n+                }\n+                else\n+                {\n+                    outputData[idx] = NumOps.Zero;\n+                }\n+                idx++;\n+            }\n+        }\n+\n+        // Apply scaling (alpha / rank)\n+        T scaling = NumOps.Divide(\n+            NumOps.FromDouble(Alpha),\n+            NumOps.FromDouble(TTRank)\n+        );\n+\n+        for (int i = 0; i < outputData.Length; i++)\n+        {\n+            outputData[i] = NumOps.Multiply(outputData[i], scaling);\n+        }\n+\n+        return new Tensor<T>(new[] { batchSize, outputSize }, outputData);\n+    }\n+\n+    /// <summary>\n+    /// Contracts a matrix with a tensor-train core.\n+    /// </summary>\n+    /// <param name=\"input\">Input matrix [batchSize, currentDim].</param>\n+    /// <param name=\"core\">TT core tensor [leftRank, coreShape, rightRank].</param>\n+    /// <param name=\"coreIndex\">Index of the core being processed.</param>\n+    /// <returns>Output matrix [batchSize, nextDim].</returns>\n+    private Matrix<T> ContractWithCore(Matrix<T> input, Tensor<T> core, int coreIndex)\n+    {\n+        int batchSize = input.Rows;\n+        int leftRank = _ttRanks[coreIndex];\n+        int coreShape = _coreShapes[coreIndex];\n+        int rightRank = _ttRanks[coreIndex + 1];\n+\n+        // Simplified contraction: treat core as a sequence of matrices\n+        // Core shape: [leftRank, coreShape, rightRank]\n+        // We'll contract by reshaping and matrix multiplication\n+\n+        int inputDim = input.Columns;\n+        int outputDim = coreShape * rightRank;\n+\n+        Matrix<T> output = new Matrix<T>(batchSize, outputDim);\n+\n+        // For each batch element\n+        for (int b = 0; b < batchSize; b++)\n+        {\n+            // Contract input with core\n+            // Simplified: use first 'leftRank' dimensions of input\n+            for (int r = 0; r < rightRank; r++)\n+            {\n+                for (int c = 0; c < coreShape; c++)\n+                {\n+                    T sum = NumOps.Zero;\n+\n+                    for (int l = 0; l < leftRank && l < inputDim; l++)\n+                    {\n+                        int coreIdx = (l * coreShape * rightRank) + (c * rightRank) + r;\n+                        if (coreIdx < core.Length)\n+                        {\n+                            T inputVal = input[b, l];\n+                            T coreVal = core[coreIdx];\n+                            sum = NumOps.Add(sum, NumOps.Multiply(inputVal, coreVal));\n+                        }\n+                    }\n+\n+                    int outIdx = c * rightRank + r;\n+                    if (outIdx < outputDim)\n+                    {\n+                        output[b, outIdx] = sum;\n+                    }\n+                }\n+            }\n+        }\n+\n+        return output;\n+    }\n+\n+    /// <summary>\n+    /// Converts a matrix to a tensor.\n+    /// </summary>\n+    private Tensor<T> TensorFromMatrix(Matrix<T> matrix)\n+    {\n+        Vector<T> data = new Vector<T>(matrix.Rows * matrix.Columns);\n+        int idx = 0;\n+        for (int i = 0; i < matrix.Rows; i++)\n+        {\n+            for (int j = 0; j < matrix.Columns; j++)\n+            {\n+                data[idx++] = matrix[i, j];\n+            }\n+        }\n+        return new Tensor<T>(new[] { matrix.Rows, matrix.Columns }, data);\n+    }\n+\n+    /// <summary>\n+    /// Performs the backward pass through the LoRETTA adapter.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient flowing back from the next layer.</param>\n+    /// <returns>Gradient to pass to the previous layer.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The backward pass computes gradients for all TT cores and propagates gradients\n+    /// back through the tensor-train contraction.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is where learning happens for LoRETTA!\n+    ///\n+    /// The backward pass:\n+    /// 1. Backpropagate through base layer\n+    /// 2. Backpropagate through tensor-train cores\n+    /// 3. Compute gradients for each core\n+    /// 4. Combine input gradients from both paths\n+    ///\n+    /// This is more complex than standard LoRA because we need to backpropagate through\n+    /// multiple cores, but the principle is the same: figure out how each parameter\n+    /// contributed to the error.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        // Backward through base layer\n+        Tensor<T> baseInputGrad = _baseLayer.Backward(outputGradient);\n+\n+        // Backward through tensor-train\n+        Tensor<T> ttInputGrad = ComputeTensorTrainBackward(outputGradient);\n+\n+        // Sum input gradients\n+        Tensor<T> inputGrad = new Tensor<T>(baseInputGrad.Shape);\n+        for (int i = 0; i < baseInputGrad.Length; i++)\n+        {\n+            inputGrad[i] = NumOps.Add(baseInputGrad[i], ttInputGrad[i]);\n+        }\n+\n+        // Update parameter gradients vector\n+        UpdateParameterGradientsFromCores();\n+\n+        return inputGrad;\n+    }\n+\n+    /// <summary>\n+    /// Computes the backward pass through the tensor-train decomposition.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient from the output.</param>\n+    /// <returns>Gradient with respect to input.</returns>\n+    private Tensor<T> ComputeTensorTrainBackward(Tensor<T> outputGradient)\n+    {\n+        // Initialize core gradients\n+        _ttCoreGradients = new List<Tensor<T>>();\n+        for (int k = 0; k < _numCores; k++)\n+        {\n+            _ttCoreGradients.Add(new Tensor<T>(_ttCores[k].Shape));\n+        }\n+\n+        // Simplified backward: compute gradients using finite differences approximation\n+        // For production, would implement proper backpropagation through tensor contractions\n+\n+        int batchSize = outputGradient.Shape[0];\n+        int inputSize = GetInputShape()[0];\n+\n+        // Create zero gradient for input\n+        Tensor<T> inputGradient = new Tensor<T>(new[] { batchSize, inputSize });\n+\n+        // For each core, compute gradient (simplified using the chain rule)\n+        for (int k = 0; k < _numCores; k++)\n+        {\n+            // Gradient computation would use stored intermediates\n+            // For now, initialize with small values\n+            for (int i = 0; i < _ttCoreGradients[k].Length; i++)\n+            {\n+                _ttCoreGradients[k][i] = NumOps.Multiply(\n+                    outputGradient[i % outputGradient.Length],\n+                    NumOps.FromDouble(0.01)\n+                );\n+            }\n+        }\n+\n+        return inputGradient;\n+    }","path":"src/NeuralNetworks/Layers/LoRETTAAdapter.cs","commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","original_commit_id":"c11235933bf7c341c8b4a223cf6ac11b757d1480","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Propagate tensor-train gradients back to the input.**\n\n`ComputeTensorTrainBackward` fills `_ttCoreGradients`, but `inputGradient` is never updated‚Äîit remains all zeros. Upstream layers therefore never see the LoRETTA contribution, and training proceeds as if the tensor-train branch were absent. Use the cached forward intermediates to contract the gradient back through each core so that `inputGradient` reflects the LoRETTA path before returning.\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:32:37Z","updated_at":"2025-11-02T02:32:40Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106294","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106294"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106294"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106294/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":552,"original_start_line":552,"start_side":"RIGHT","line":584,"original_line":584,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":584,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106301","pull_request_review_id":3407993158,"id":2484106301,"node_id":"PRRC_kwDOKSXUF86UEHQ9","diff_hunk":"@@ -0,0 +1,444 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.NeuralNetworks.Layers;\n+\n+/// <summary>\n+/// Implements MoRA (High-Rank Updating for Parameter-Efficient Fine-Tuning) adapter.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// <b>Paper Reference:</b> \"MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning\"\n+/// by Ting Jiang, Shaohan Huang, et al. (arXiv:2405.12130, May 2024)\n+/// </para>\n+/// <para>\n+/// MoRA addresses a fundamental limitation of LoRA: the low-rank constraint restricts the model's\n+/// ability to learn and memorize new knowledge. While LoRA uses two rectangular matrices (A and B)\n+/// to create low-rank updates, MoRA uses a single square matrix M combined with non-parameter-sharing\n+/// operators to achieve high-rank updates while maintaining the same parameter count.\n+/// </para>\n+/// <para><b>Key Innovations:</b>\n+///\n+/// 1. <b>High-Rank Updates</b>: Unlike LoRA's rank-r updates (r &lt;&lt; d), MoRA achieves rank-rÃÇ\n+///    updates where rÃÇ can equal the full dimension d, enabling the model to learn richer representations.\n+///\n+/// 2. <b>Square Matrix M</b>: Instead of LoRA's A (d√ór) and B (r√ód) matrices, MoRA uses a single\n+///    square matrix M (r√ór) where r = sqrt(d√ód / 2). For the same parameter count as LoRA,\n+///    MoRA achieves much higher effective rank.\n+///\n+/// 3. <b>Non-Parameter-Sharing Operators</b>: MoRA uses rotation, permutation, or other linear\n+///    transformations that don't add trainable parameters but enable dimension compression\n+///    and decompression around the square matrix M.\n+///\n+/// 4. <b>Input Compression / Output Decompression</b>: The architecture is:\n+///    - Compress: Input (d) to Compressed (r) via rotation/permutation\n+///    - Transform: Compressed (r) to Transformed (r) via trainable matrix M\n+///    - Decompress: Transformed (r) to Output (d) via inverse rotation/permutation\n+/// </para>\n+/// <para><b>Architecture Comparison:</b>\n+///\n+/// LoRA: W = W‚ÇÄ + BA where A ‚àà ‚Ñù^(d√ór), B ‚àà ‚Ñù^(r√ód)\n+/// - Parameters: 2dr\n+/// - Rank: r (low-rank constraint)\n+/// - Typical r: 8-64\n+///\n+/// MoRA: W = W‚ÇÄ + R_d^(-1) M R_c where M ‚àà ‚Ñù^(r√ór)\n+/// - Parameters: r¬≤\n+/// - Rank: min(r, d) (can be full-rank)\n+/// - For same param count as LoRA: r = sqrt(2dr), so rank ‚âà sqrt(2dr)\n+/// - Example: LoRA with r=8, d=1024 has 16,384 params and rank 8\n+///            MoRA with same params: r=128, rank 128 (16√ó higher!)\n+/// </para>\n+/// <para><b>Performance (from paper):</b>\n+///\n+/// Compared to LoRA on various tasks:\n+/// - <b>Memory-Intensive Tasks</b>: MoRA significantly outperforms LoRA\n+///   * Continual Pretraining: ~15% better perplexity\n+///   * Instruction Tuning: ~8% better accuracy on knowledge-intensive QA\n+/// - <b>Reasoning Tasks</b>: MoRA performs comparably to LoRA\n+///   * Mathematical Reasoning: Similar performance (within 1-2%)\n+/// - <b>Parameter Efficiency</b>: Same parameter count as LoRA\n+/// - <b>Training Speed</b>: Slightly slower than LoRA due to rotation operations (‚âà5-10% overhead)\n+/// </para>\n+/// <para><b>When to Use MoRA vs LoRA:</b>\n+///\n+/// Use MoRA when:\n+/// - Task requires memorizing new facts or knowledge\n+/// - Domain adaptation with significant vocabulary changes\n+/// - Continual learning scenarios\n+/// - You need the model to \"remember\" rather than just \"adapt\"\n+///\n+/// Use LoRA when:\n+/// - Task is primarily reasoning or pattern recognition\n+/// - Minimal new knowledge acquisition needed\n+/// - Training speed is critical\n+/// - Standard parameter-efficient fine-tuning is sufficient\n+/// </para>\n+/// <para><b>Implementation Details:</b>\n+///\n+/// This implementation uses rotation matrices as the non-parameter-sharing operators:\n+/// - Compression R_c: Projects input from dimension d to dimension r\n+/// - Decompression R_d: Projects from dimension r back to dimension d\n+/// - These are generated using random orthogonal matrices (Gram-Schmidt orthogonalization)\n+/// - They remain fixed during training (non-trainable)\n+///\n+/// Alternative operators mentioned in the paper (not implemented here):\n+/// - RoPE-based rotations (Rotary Position Embeddings)\n+/// - Random permutations\n+/// - Structured rotations (e.g., Hadamard transforms)\n+/// </para>\n+/// <para><b>For Beginners:</b> MoRA is like an upgraded version of LoRA that can learn\n+/// more complex changes to a model while using the same amount of memory.\n+///\n+/// Think of it like this:\n+/// - LoRA is like having 2 small notebooks to write changes (matrices A and B)\n+/// - MoRA is like having 1 square notebook plus a compression/decompression scheme\n+///\n+/// The key insight: By compressing the input, applying changes in compressed space,\n+/// and then decompressing, MoRA can make higher-rank updates that capture more\n+/// complex patterns. This is especially useful when you're teaching the model\n+/// entirely new facts or concepts, not just adapting its existing knowledge.\n+///\n+/// Example: If you're fine-tuning a model to learn medical terminology, MoRA\n+/// will be better at memorizing the new terms, while LoRA might be better at\n+/// learning to reason about medical cases using existing knowledge.\n+/// </para>\n+/// </remarks>\n+public class MoRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Square matrix M for high-rank adaptation (r√ór dimensions).\n+    /// </summary>\n+    /// <remarks>\n+    /// This is the core trainable component of MoRA. Unlike LoRA's rectangular matrices,\n+    /// M is square with dimensions (r√ór), enabling higher-rank updates.\n+    /// </remarks>\n+    private Matrix<T> _matrixM;\n+\n+    /// <summary>\n+    /// Compression matrix that reduces input dimension from d to r (non-trainable).\n+    /// </summary>\n+    /// <remarks>\n+    /// This is a non-trainable orthogonal matrix that compresses the input.\n+    /// It's generated once during initialization using Gram-Schmidt orthogonalization and remains fixed.\n+    /// </remarks>\n+    private readonly Matrix<T> _compressionMatrix;\n+\n+    /// <summary>\n+    /// Decompression matrix that expands dimension from r back to d (non-trainable).\n+    /// </summary>\n+    /// <remarks>\n+    /// This is a non-trainable orthogonal matrix that decompresses the output.\n+    /// In this implementation, it's the transpose of the compression matrix.\n+    /// </remarks>\n+    private readonly Matrix<T> _decompressionMatrix;\n+\n+    /// <summary>\n+    /// The dimension of the square matrix M.\n+    /// </summary>\n+    /// <remarks>\n+    /// For MoRA, this is calculated to match the parameter count of LoRA.\n+    /// If LoRA uses 2dr parameters, MoRA uses r¬≤ = 2dr, so rÃÇ = sqrt(2dr).\n+    /// This gives MoRA a much higher effective rank than LoRA.\n+    /// </remarks>\n+    private readonly int _squareRank;\n+\n+    /// <summary>\n+    /// Gradients for matrix M computed during backpropagation.\n+    /// </summary>\n+    private Matrix<T>? _matrixMGradient;\n+\n+    /// <summary>\n+    /// Stored input from the forward pass, needed for gradient computation.\n+    /// </summary>\n+    private Tensor<T>? _lastInput;\n+\n+    /// <summary>\n+    /// Cached compressed input from forward pass.\n+    /// </summary>\n+    private Matrix<T>? _lastCompressed;\n+\n+    /// <summary>\n+    /// Gets the effective rank of the MoRA adaptation.\n+    /// </summary>\n+    /// <remarks>\n+    /// This is the dimension of the square matrix M, which determines the\n+    /// maximum rank of the updates MoRA can make. Unlike LoRA where this\n+    /// is typically 8-64, MoRA can achieve ranks of 128+ with the same\n+    /// parameter count.\n+    /// </remarks>\n+    public int SquareRank => _squareRank;\n+\n+    public MoRAAdapter(ILayer<T> baseLayer, int rank, double alpha = 1.0, bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+\n+        if (inputSize != outputSize)\n+        {\n+            throw new ArgumentException(\n+                $\"MoRA requires square layers (input size = output size). Got input={inputSize}, output={outputSize}. \" +\n+                \"For non-square layers, use LoRA instead.\", nameof(baseLayer));\n+        }\n+\n+        int dimension = inputSize;\n+        _squareRank = (int)Math.Sqrt(2.0 * dimension * rank);\n+\n+        if (_squareRank < 1)\n+        {\n+            _squareRank = 1;\n+        }\n+\n+        if (_squareRank > dimension)\n+        {\n+            _squareRank = dimension;\n+        }\n+\n+        _matrixM = new Matrix<T>(_squareRank, _squareRank);\n+        InitializeMatrixM();\n+\n+        _compressionMatrix = GenerateOrthogonalMatrix(dimension, _squareRank);\n+        _decompressionMatrix = _compressionMatrix.Transpose();\n+    }","path":"src/NeuralNetworks/Layers/MoRAAdapter.cs","commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","original_commit_id":"c11235933bf7c341c8b4a223cf6ac11b757d1480","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Fix ParameterCount/parameter packing before the base ctor runs.**  \n`LoRAAdapterBase` calls `Parameters = new Vector<T>(ParameterCount);` inside its constructor. At that moment `_squareRank` is still the default `0`, so your override returns `0`, the base class allocates a zero-length parameter vector, and it never gets rebuilt. Afterwards `ParameterCount` reports `_squareRank * _squareRank` (plus base params), but `Parameters` still has length 0 and only contains the default LoRA state. Any caller that does `SetParameters(GetParameters())` (or serialization) will immediately hit the length mismatch and throw. It also means `_matrixM` is never exposed through `GetParameters`/`SetParameters`, so the advertised ParameterCount doesn‚Äôt match the data you persist. Please compute `_squareRank` before the base ctor (or immediately reallocate `Parameters` after `_squareRank` is assigned) and add explicit packing/unpacking so `_matrixM` is included. For example:\n\n```diff\n-        _squareRank = (int)Math.Sqrt(2.0 * dimension * rank);\n+        _squareRank = ComputeSquareRank(dimension, rank);\n+        RebuildParameterSnapshot();\n```\n\nwith `RebuildParameterSnapshot` repopulating `Parameters`, `ParameterGradients`, and `SetParameters`/`GetParameters` updated to include `_matrixM`.  \n\n\n\nAlso applies to: 408-415\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/Layers/MoRAAdapter.cs around lines 172 to 203 (and\nsimilarly fix lines 408-415), the constructor currently lets the base ctor\nallocate Parameters when _squareRank is still zero which yields a zero-length\nParameters vector and omits _matrixM from the packed parameters; compute and\nassign _squareRank (and any dependent sizes) before the base constructor runs or\nimmediately reallocate and repopulate Parameters/ParameterGradients after\n_squareRank is set; add a RebuildParameterSnapshot method that allocates\nParameters and ParameterGradients to the full ParameterCount, packs base\nparameters plus the flattened _matrixM into Parameters, and similarly unpacks in\nSetParameters/GetParameters so _matrixM is included and lengths match; ensure\nParameterCount is consistent with the packed layout and call\nRebuildParameterSnapshot from the constructor after initializing _squareRank and\n_matrixM (or compute _squareRank prior to base ctor).\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:32:37Z","updated_at":"2025-11-02T02:32:40Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106301","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106301"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106301"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106301/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":172,"original_start_line":172,"start_side":"RIGHT","line":203,"original_line":203,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":203,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106306","pull_request_review_id":3407993158,"id":2484106306,"node_id":"PRRC_kwDOKSXUF86UEHRC","diff_hunk":"@@ -0,0 +1,444 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.NeuralNetworks.Layers;\n+\n+/// <summary>\n+/// Implements MoRA (High-Rank Updating for Parameter-Efficient Fine-Tuning) adapter.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// <b>Paper Reference:</b> \"MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning\"\n+/// by Ting Jiang, Shaohan Huang, et al. (arXiv:2405.12130, May 2024)\n+/// </para>\n+/// <para>\n+/// MoRA addresses a fundamental limitation of LoRA: the low-rank constraint restricts the model's\n+/// ability to learn and memorize new knowledge. While LoRA uses two rectangular matrices (A and B)\n+/// to create low-rank updates, MoRA uses a single square matrix M combined with non-parameter-sharing\n+/// operators to achieve high-rank updates while maintaining the same parameter count.\n+/// </para>\n+/// <para><b>Key Innovations:</b>\n+///\n+/// 1. <b>High-Rank Updates</b>: Unlike LoRA's rank-r updates (r &lt;&lt; d), MoRA achieves rank-rÃÇ\n+///    updates where rÃÇ can equal the full dimension d, enabling the model to learn richer representations.\n+///\n+/// 2. <b>Square Matrix M</b>: Instead of LoRA's A (d√ór) and B (r√ód) matrices, MoRA uses a single\n+///    square matrix M (r√ór) where r = sqrt(d√ód / 2). For the same parameter count as LoRA,\n+///    MoRA achieves much higher effective rank.\n+///\n+/// 3. <b>Non-Parameter-Sharing Operators</b>: MoRA uses rotation, permutation, or other linear\n+///    transformations that don't add trainable parameters but enable dimension compression\n+///    and decompression around the square matrix M.\n+///\n+/// 4. <b>Input Compression / Output Decompression</b>: The architecture is:\n+///    - Compress: Input (d) to Compressed (r) via rotation/permutation\n+///    - Transform: Compressed (r) to Transformed (r) via trainable matrix M\n+///    - Decompress: Transformed (r) to Output (d) via inverse rotation/permutation\n+/// </para>\n+/// <para><b>Architecture Comparison:</b>\n+///\n+/// LoRA: W = W‚ÇÄ + BA where A ‚àà ‚Ñù^(d√ór), B ‚àà ‚Ñù^(r√ód)\n+/// - Parameters: 2dr\n+/// - Rank: r (low-rank constraint)\n+/// - Typical r: 8-64\n+///\n+/// MoRA: W = W‚ÇÄ + R_d^(-1) M R_c where M ‚àà ‚Ñù^(r√ór)\n+/// - Parameters: r¬≤\n+/// - Rank: min(r, d) (can be full-rank)\n+/// - For same param count as LoRA: r = sqrt(2dr), so rank ‚âà sqrt(2dr)\n+/// - Example: LoRA with r=8, d=1024 has 16,384 params and rank 8\n+///            MoRA with same params: r=128, rank 128 (16√ó higher!)\n+/// </para>\n+/// <para><b>Performance (from paper):</b>\n+///\n+/// Compared to LoRA on various tasks:\n+/// - <b>Memory-Intensive Tasks</b>: MoRA significantly outperforms LoRA\n+///   * Continual Pretraining: ~15% better perplexity\n+///   * Instruction Tuning: ~8% better accuracy on knowledge-intensive QA\n+/// - <b>Reasoning Tasks</b>: MoRA performs comparably to LoRA\n+///   * Mathematical Reasoning: Similar performance (within 1-2%)\n+/// - <b>Parameter Efficiency</b>: Same parameter count as LoRA\n+/// - <b>Training Speed</b>: Slightly slower than LoRA due to rotation operations (‚âà5-10% overhead)\n+/// </para>\n+/// <para><b>When to Use MoRA vs LoRA:</b>\n+///\n+/// Use MoRA when:\n+/// - Task requires memorizing new facts or knowledge\n+/// - Domain adaptation with significant vocabulary changes\n+/// - Continual learning scenarios\n+/// - You need the model to \"remember\" rather than just \"adapt\"\n+///\n+/// Use LoRA when:\n+/// - Task is primarily reasoning or pattern recognition\n+/// - Minimal new knowledge acquisition needed\n+/// - Training speed is critical\n+/// - Standard parameter-efficient fine-tuning is sufficient\n+/// </para>\n+/// <para><b>Implementation Details:</b>\n+///\n+/// This implementation uses rotation matrices as the non-parameter-sharing operators:\n+/// - Compression R_c: Projects input from dimension d to dimension r\n+/// - Decompression R_d: Projects from dimension r back to dimension d\n+/// - These are generated using random orthogonal matrices (Gram-Schmidt orthogonalization)\n+/// - They remain fixed during training (non-trainable)\n+///\n+/// Alternative operators mentioned in the paper (not implemented here):\n+/// - RoPE-based rotations (Rotary Position Embeddings)\n+/// - Random permutations\n+/// - Structured rotations (e.g., Hadamard transforms)\n+/// </para>\n+/// <para><b>For Beginners:</b> MoRA is like an upgraded version of LoRA that can learn\n+/// more complex changes to a model while using the same amount of memory.\n+///\n+/// Think of it like this:\n+/// - LoRA is like having 2 small notebooks to write changes (matrices A and B)\n+/// - MoRA is like having 1 square notebook plus a compression/decompression scheme\n+///\n+/// The key insight: By compressing the input, applying changes in compressed space,\n+/// and then decompressing, MoRA can make higher-rank updates that capture more\n+/// complex patterns. This is especially useful when you're teaching the model\n+/// entirely new facts or concepts, not just adapting its existing knowledge.\n+///\n+/// Example: If you're fine-tuning a model to learn medical terminology, MoRA\n+/// will be better at memorizing the new terms, while LoRA might be better at\n+/// learning to reason about medical cases using existing knowledge.\n+/// </para>\n+/// </remarks>\n+public class MoRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Square matrix M for high-rank adaptation (r√ór dimensions).\n+    /// </summary>\n+    /// <remarks>\n+    /// This is the core trainable component of MoRA. Unlike LoRA's rectangular matrices,\n+    /// M is square with dimensions (r√ór), enabling higher-rank updates.\n+    /// </remarks>\n+    private Matrix<T> _matrixM;\n+\n+    /// <summary>\n+    /// Compression matrix that reduces input dimension from d to r (non-trainable).\n+    /// </summary>\n+    /// <remarks>\n+    /// This is a non-trainable orthogonal matrix that compresses the input.\n+    /// It's generated once during initialization using Gram-Schmidt orthogonalization and remains fixed.\n+    /// </remarks>\n+    private readonly Matrix<T> _compressionMatrix;\n+\n+    /// <summary>\n+    /// Decompression matrix that expands dimension from r back to d (non-trainable).\n+    /// </summary>\n+    /// <remarks>\n+    /// This is a non-trainable orthogonal matrix that decompresses the output.\n+    /// In this implementation, it's the transpose of the compression matrix.\n+    /// </remarks>\n+    private readonly Matrix<T> _decompressionMatrix;\n+\n+    /// <summary>\n+    /// The dimension of the square matrix M.\n+    /// </summary>\n+    /// <remarks>\n+    /// For MoRA, this is calculated to match the parameter count of LoRA.\n+    /// If LoRA uses 2dr parameters, MoRA uses r¬≤ = 2dr, so rÃÇ = sqrt(2dr).\n+    /// This gives MoRA a much higher effective rank than LoRA.\n+    /// </remarks>\n+    private readonly int _squareRank;\n+\n+    /// <summary>\n+    /// Gradients for matrix M computed during backpropagation.\n+    /// </summary>\n+    private Matrix<T>? _matrixMGradient;\n+\n+    /// <summary>\n+    /// Stored input from the forward pass, needed for gradient computation.\n+    /// </summary>\n+    private Tensor<T>? _lastInput;\n+\n+    /// <summary>\n+    /// Cached compressed input from forward pass.\n+    /// </summary>\n+    private Matrix<T>? _lastCompressed;\n+\n+    /// <summary>\n+    /// Gets the effective rank of the MoRA adaptation.\n+    /// </summary>\n+    /// <remarks>\n+    /// This is the dimension of the square matrix M, which determines the\n+    /// maximum rank of the updates MoRA can make. Unlike LoRA where this\n+    /// is typically 8-64, MoRA can achieve ranks of 128+ with the same\n+    /// parameter count.\n+    /// </remarks>\n+    public int SquareRank => _squareRank;\n+\n+    public MoRAAdapter(ILayer<T> baseLayer, int rank, double alpha = 1.0, bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+\n+        if (inputSize != outputSize)\n+        {\n+            throw new ArgumentException(\n+                $\"MoRA requires square layers (input size = output size). Got input={inputSize}, output={outputSize}. \" +\n+                \"For non-square layers, use LoRA instead.\", nameof(baseLayer));\n+        }\n+\n+        int dimension = inputSize;\n+        _squareRank = (int)Math.Sqrt(2.0 * dimension * rank);\n+\n+        if (_squareRank < 1)\n+        {\n+            _squareRank = 1;\n+        }\n+\n+        if (_squareRank > dimension)\n+        {\n+            _squareRank = dimension;\n+        }\n+\n+        _matrixM = new Matrix<T>(_squareRank, _squareRank);\n+        InitializeMatrixM();\n+\n+        _compressionMatrix = GenerateOrthogonalMatrix(dimension, _squareRank);\n+        _decompressionMatrix = _compressionMatrix.Transpose();\n+    }\n+\n+    private void InitializeMatrixM()\n+    {\n+        T stddev = NumOps.Sqrt(NumOps.Divide(NumOps.One, NumOps.FromDouble(_squareRank)));\n+\n+        for (int i = 0; i < _matrixM.Rows; i++)\n+        {\n+            for (int j = 0; j < _matrixM.Columns; j++)\n+            {\n+                double u1 = Random.NextDouble();\n+                double u2 = Random.NextDouble();\n+                double randStdNormal = Math.Sqrt(-2.0 * Math.Log(u1)) * Math.Sin(2.0 * Math.PI * u2);\n+                _matrixM[i, j] = NumOps.Multiply(NumOps.FromDouble(randStdNormal), stddev);\n+            }\n+        }\n+    }\n+\n+    private Matrix<T> GenerateOrthogonalMatrix(int rows, int cols)\n+    {\n+        Matrix<T> randomMatrix = new Matrix<T>(rows, cols);\n+        for (int i = 0; i < rows; i++)\n+        {\n+            for (int j = 0; j < cols; j++)\n+            {\n+                double u1 = Random.NextDouble();\n+                double u2 = Random.NextDouble();\n+                double randStdNormal = Math.Sqrt(-2.0 * Math.Log(u1)) * Math.Sin(2.0 * Math.PI * u2);\n+                randomMatrix[i, j] = NumOps.FromDouble(randStdNormal);\n+            }\n+        }\n+\n+        Matrix<T> orthogonal = new Matrix<T>(rows, cols);\n+\n+        for (int j = 0; j < cols; j++)\n+        {\n+            Vector<T> column = new Vector<T>(rows);\n+            for (int i = 0; i < rows; i++)\n+            {\n+                column[i] = randomMatrix[i, j];\n+            }\n+\n+            for (int k = 0; k < j; k++)\n+            {\n+                Vector<T> prevColumn = new Vector<T>(rows);\n+                for (int i = 0; i < rows; i++)\n+                {\n+                    prevColumn[i] = orthogonal[i, k];\n+                }\n+\n+                T dotProduct = NumOps.Zero;\n+                for (int i = 0; i < rows; i++)\n+                {\n+                    dotProduct = NumOps.Add(dotProduct, NumOps.Multiply(column[i], prevColumn[i]));\n+                }\n+\n+                for (int i = 0; i < rows; i++)\n+                {\n+                    column[i] = NumOps.Subtract(column[i], NumOps.Multiply(dotProduct, prevColumn[i]));\n+                }\n+            }\n+\n+            T norm = NumOps.Zero;\n+            for (int i = 0; i < rows; i++)\n+            {\n+                norm = NumOps.Add(norm, NumOps.Multiply(column[i], column[i]));\n+            }\n+            norm = NumOps.Sqrt(norm);\n+\n+            if (NumOps.GreaterThan(norm, NumOps.FromDouble(1e-10)))\n+            {\n+                for (int i = 0; i < rows; i++)\n+                {\n+                    orthogonal[i, j] = NumOps.Divide(column[i], norm);\n+                }\n+            }\n+            else\n+            {\n+                for (int i = 0; i < rows; i++)\n+                {\n+                    orthogonal[i, j] = i == j ? NumOps.One : NumOps.Zero;\n+                }\n+            }\n+        }\n+\n+        return orthogonal;\n+    }\n+\n+    protected override LoRALayer<T> CreateLoRALayer(int rank, double alpha)\n+    {\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        return new LoRALayer<T>(inputSize, outputSize, 1, alpha);\n+    }\n+\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        _lastInput = input.Clone();\n+        Tensor<T> baseOutput = _baseLayer.Forward(input);\n+\n+        int batchSize = input.Shape[0];\n+        int dimension = input.Shape.Length > 1 ? input.Shape[1] : input.Length;\n+\n+        Matrix<T> inputMatrix = new Matrix<T>(batchSize, dimension);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < dimension; j++)\n+            {\n+                inputMatrix[i, j] = input[i * dimension + j];\n+            }\n+        }\n+\n+        Matrix<T> compressed = inputMatrix.Multiply(_compressionMatrix);\n+        _lastCompressed = compressed;\n+\n+        Matrix<T> transformed = compressed.Multiply(_matrixM);\n+        Matrix<T> decompressed = transformed.Multiply(_decompressionMatrix);\n+\n+        T scalingFactor = NumOps.FromDouble(Alpha);\n+        decompressed = decompressed.Multiply(scalingFactor);\n+\n+        Tensor<T> moraOutput = new Tensor<T>(baseOutput.Shape);\n+        int idx = 0;\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < dimension; j++)\n+            {\n+                moraOutput[idx] = decompressed[i, j];\n+                idx++;\n+            }\n+        }\n+\n+        Tensor<T> result = new Tensor<T>(baseOutput.Shape);\n+        for (int i = 0; i < baseOutput.Length; i++)\n+        {\n+            result[i] = NumOps.Add(baseOutput[i], moraOutput[i]);\n+        }\n+\n+        return result;\n+    }\n+\n+    public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (_lastInput == null || _lastCompressed == null)\n+        {\n+            throw new InvalidOperationException(\"Forward pass must be called before backward pass\");\n+        }\n+\n+        int batchSize = _lastInput.Shape[0];\n+        int dimension = _lastInput.Shape.Length > 1 ? _lastInput.Shape[1] : _lastInput.Length;\n+\n+        Matrix<T> gradMatrix = new Matrix<T>(batchSize, dimension);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < dimension; j++)\n+            {\n+                gradMatrix[i, j] = outputGradient[i * dimension + j];\n+            }\n+        }\n+\n+        T scalingFactor = NumOps.FromDouble(Alpha);\n+        Matrix<T> gradTransformed = gradMatrix.Multiply(_decompressionMatrix.Transpose()).Multiply(scalingFactor);\n+        _matrixMGradient = _lastCompressed.Transpose().Multiply(gradTransformed);\n+        Matrix<T> gradCompressed = gradTransformed.Multiply(_matrixM.Transpose());\n+        Matrix<T> moraInputGradient = gradCompressed.Multiply(_compressionMatrix.Transpose());\n+\n+        Tensor<T> baseInputGrad = _baseLayer.Backward(outputGradient);\n+\n+        Tensor<T> inputGrad = new Tensor<T>(_lastInput.Shape);\n+        int idx = 0;\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < dimension; j++)\n+            {\n+                T moraGrad = moraInputGradient[i, j];\n+                inputGrad[idx] = NumOps.Add(baseInputGrad[idx], moraGrad);\n+                idx++;\n+            }\n+        }\n+\n+        return inputGrad;\n+    }\n+\n+    public override void UpdateParameters(T learningRate)\n+    {\n+        if (_matrixMGradient == null)\n+        {\n+            return;\n+        }\n+\n+        for (int i = 0; i < _matrixM.Rows; i++)\n+        {\n+            for (int j = 0; j < _matrixM.Columns; j++)\n+            {\n+                T update = NumOps.Multiply(_matrixMGradient[i, j], learningRate);\n+                _matrixM[i, j] = NumOps.Subtract(_matrixM[i, j], update);\n+            }\n+        }\n+\n+        if (!_freezeBaseLayer)\n+        {\n+            _baseLayer.UpdateParameters(learningRate);\n+        }\n+    }\n+\n+    public override int ParameterCount\n+    {\n+        get\n+        {\n+            int moraParams = _squareRank * _squareRank;\n+            return _freezeBaseLayer ? moraParams : (_baseLayer.ParameterCount + moraParams);\n+        }\n+    }\n+\n+    public override ILayer<T> MergeToOriginalLayer()\n+    {\n+        Matrix<T> temp = _matrixM.Multiply(_compressionMatrix.Transpose());\n+        Matrix<T> fullAdaptation = _decompressionMatrix.Multiply(temp);\n+\n+        T scalingFactor = NumOps.FromDouble(Alpha);\n+        fullAdaptation = fullAdaptation.Multiply(scalingFactor);\n+\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+\n+        IActivationFunction<T> identityActivation = new IdentityActivation<T>();\n+        DenseLayer<T> merged = new DenseLayer<T>(inputSize, outputSize, identityActivation);\n+\n+        Matrix<T> adaptationWeights = fullAdaptation.Transpose();\n+        merged.SetWeights(adaptationWeights);\n+\n+        return merged;\n+    }","path":"src/NeuralNetworks/Layers/MoRAAdapter.cs","commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","original_commit_id":"c11235933bf7c341c8b4a223cf6ac11b757d1480","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Merged layer drops the base weights.**  \n`MergeToOriginalLayer` currently returns a brand-new `DenseLayer` that contains only the MoRA adaptation (`fullAdaptation`), so the original base-layer weights/biases are discarded. The merged layer therefore produces just the delta instead of the adapted model. Grab the base layer‚Äôs weights/biases, add the MoRA contribution, and then write the combined weights back (biases should be copied untouched). Until that‚Äôs done the merge path yields incorrect inference.  \n\n```diff\n-        Matrix<T> adaptationWeights = fullAdaptation.Transpose();\n-        merged.SetWeights(adaptationWeights);\n+        Vector<T> baseParams = _baseLayer.GetParameters();\n+        Matrix<T> baseWeights = BuildWeightMatrixFromParameters(baseParams, inputSize, outputSize);\n+        Matrix<T> mergedWeights = baseWeights.Add(fullAdaptation.Transpose());\n+        merged.SetWeights(mergedWeights);\n+        CopyBiases(baseParams, merged, inputSize, outputSize);\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:32:37Z","updated_at":"2025-11-02T02:32:40Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106306","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106306"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106306"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106306/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":417,"original_start_line":417,"start_side":"RIGHT","line":435,"original_line":435,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":435,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106315","pull_request_review_id":3407993158,"id":2484106315,"node_id":"PRRC_kwDOKSXUF86UEHRL","diff_hunk":"@@ -0,0 +1,638 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.NeuralNetworks.Layers;\n+\n+/// <summary>\n+/// Multi-task LoRA adapter that manages multiple task-specific LoRA layers for complex multi-task learning scenarios.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// MultiLoRA extends the basic LoRA concept to handle multiple tasks simultaneously within a single layer.\n+/// Instead of having one LoRA adaptation, it maintains a dictionary of task-specific LoRA layers,\n+/// with a routing mechanism to select the appropriate adapter for each task.\n+/// </para>\n+/// <para>\n+/// Key features:\n+/// - Multiple task-specific LoRA adapters sharing the same base layer\n+/// - Dynamic task switching during inference and training\n+/// - Per-task rank configuration for optimal parameter efficiency\n+/// - Shared base layer weights across all tasks\n+/// - Task-specific merging for deployment\n+/// </para>\n+/// <para><b>For Beginners:</b> Think of MultiLoRA as having one teacher (the base layer) and multiple\n+/// students (task-specific LoRA adapters), each specializing in different subjects.\n+///\n+/// In regular LoRA:\n+/// - You have one base layer (the teacher)\n+/// - One LoRA adapter (one student learning one subject)\n+/// - Output = base + lora_adaptation\n+///\n+/// In MultiLoRA:\n+/// - You have one base layer (the teacher)\n+/// - Multiple LoRA adapters (multiple students, each specializing in different tasks)\n+/// - Output = base + task_specific_lora_adaptation\n+///\n+/// This is powerful for:\n+/// 1. Multi-domain learning: Train on medical, legal, and technical documents simultaneously\n+/// 2. Multi-lingual models: One adapter per language\n+/// 3. Multi-task learning: Sentiment analysis, named entity recognition, question answering, etc.\n+/// 4. Continual learning: Add new tasks without forgetting old ones\n+///\n+/// Example use case:\n+/// - Base: Pre-trained language model\n+/// - Task 1: Sentiment analysis (rank=4)\n+/// - Task 2: Named entity recognition (rank=8)\n+/// - Task 3: Question answering (rank=16)\n+///\n+/// You can switch between tasks at runtime, and each task only trains its specific LoRA weights!\n+/// </para>\n+/// </remarks>\n+public class MultiLoRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Dictionary mapping task names to their specific LoRA layers.\n+    /// </summary>\n+    private readonly Dictionary<string, LoRALayer<T>> _taskAdapters;\n+\n+    /// <summary>\n+    /// The name of the currently active task.\n+    /// </summary>\n+    private string _currentTask;\n+\n+    /// <summary>\n+    /// Gets the dictionary of task-specific LoRA adapters.\n+    /// </summary>\n+    /// <remarks>\n+    /// Each task has its own dedicated LoRA layer with potentially different ranks.\n+    /// This allows for task-specific parameter efficiency optimization.\n+    /// </remarks>\n+    public IReadOnlyDictionary<string, LoRALayer<T>> TaskAdapters => _taskAdapters;\n+\n+    /// <summary>\n+    /// Gets or sets the name of the currently active task.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Changing this property switches which task-specific adapter is used during forward/backward passes.\n+    /// This allows dynamic task switching during inference or training.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is like switching between different \"modes\" of your model.\n+    /// Set it to \"sentiment\" for sentiment analysis, \"ner\" for named entity recognition, etc.\n+    /// The base layer stays the same, but the adaptation changes based on the task.\n+    /// </para>\n+    /// </remarks>\n+    /// <exception cref=\"ArgumentException\">Thrown when trying to set a task that hasn't been added.</exception>\n+    public string CurrentTask\n+    {\n+        get => _currentTask;\n+        set\n+        {\n+            if (!_taskAdapters.ContainsKey(value))\n+            {\n+                throw new ArgumentException($\"Task '{value}' has not been added. Available tasks: {string.Join(\", \", _taskAdapters.Keys)}\", nameof(value));\n+            }\n+            _currentTask = value;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Gets the number of tasks configured in this adapter.\n+    /// </summary>\n+    public int NumberOfTasks => _taskAdapters.Count;\n+\n+    /// <summary>\n+    /// Gets the total parameter count across all task adapters.\n+    /// </summary>\n+    /// <remarks>\n+    /// This includes parameters from the base layer (if not frozen) plus all task-specific LoRA layers.\n+    /// </remarks>\n+    public override int ParameterCount\n+    {\n+        get\n+        {\n+            int totalParams = _freezeBaseLayer ? 0 : _baseLayer.ParameterCount;\n+            foreach (var adapter in _taskAdapters.Values)\n+            {\n+                totalParams += adapter.ParameterCount;\n+            }\n+            return totalParams;\n+        }\n+    }","path":"src/NeuralNetworks/Layers/MultiLoRAAdapter.cs","commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","original_commit_id":"c11235933bf7c341c8b4a223cf6ac11b757d1480","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Prevent `ParameterCount` from crashing during construction.**\n\n`LoRAAdapterBase` queries `ParameterCount` inside its ctor, before `_taskAdapters` has been populated. The current override dereferences the null dictionary (and even if you initialise it, the getter returns 0, so the base class allocates a zero-length parameter vector). Please fall back to the base implementation until the task map is ready, then sum the per-task counts.\n\n```diff\n-    private readonly Dictionary<string, LoRALayer<T>> _taskAdapters;\n+    private readonly Dictionary<string, LoRALayer<T>> _taskAdapters = new Dictionary<string, LoRALayer<T>>();\n...\n-            int totalParams = _freezeBaseLayer ? 0 : _baseLayer.ParameterCount;\n+            if (_taskAdapters == null || _taskAdapters.Count == 0)\n+            {\n+                return base.ParameterCount;\n+            }\n+\n+            int totalParams = _freezeBaseLayer ? 0 : _baseLayer.ParameterCount;\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    public override int ParameterCount\n    {\n        get\n        {\n            if (_taskAdapters == null || _taskAdapters.Count == 0)\n            {\n                return base.ParameterCount;\n            }\n\n            int totalParams = _freezeBaseLayer ? 0 : _baseLayer.ParameterCount;\n            foreach (var adapter in _taskAdapters.Values)\n            {\n                totalParams += adapter.ParameterCount;\n            }\n            return totalParams;\n        }\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/Layers/MultiLoRAAdapter.cs around lines 110 to 121, the\nParameterCount getter dereferences _taskAdapters during construction causing a\ncrash; change the getter to first check if _taskAdapters is null and if so\nreturn base.ParameterCount (so the base ctor can safely query it), otherwise\ncompute totalParams starting from (_freezeBaseLayer ? 0 : base.ParameterCount)\nand then add each adapter.ParameterCount from _taskAdapters.Values and return\nthe sum.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:32:37Z","updated_at":"2025-11-02T02:32:40Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106315","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106315"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106315"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106315/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":110,"original_start_line":110,"start_side":"RIGHT","line":121,"original_line":121,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":121,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106326","pull_request_review_id":3407993158,"id":2484106326,"node_id":"PRRC_kwDOKSXUF86UEHRW","diff_hunk":"@@ -0,0 +1,638 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.NeuralNetworks.Layers;\n+\n+/// <summary>\n+/// Multi-task LoRA adapter that manages multiple task-specific LoRA layers for complex multi-task learning scenarios.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// MultiLoRA extends the basic LoRA concept to handle multiple tasks simultaneously within a single layer.\n+/// Instead of having one LoRA adaptation, it maintains a dictionary of task-specific LoRA layers,\n+/// with a routing mechanism to select the appropriate adapter for each task.\n+/// </para>\n+/// <para>\n+/// Key features:\n+/// - Multiple task-specific LoRA adapters sharing the same base layer\n+/// - Dynamic task switching during inference and training\n+/// - Per-task rank configuration for optimal parameter efficiency\n+/// - Shared base layer weights across all tasks\n+/// - Task-specific merging for deployment\n+/// </para>\n+/// <para><b>For Beginners:</b> Think of MultiLoRA as having one teacher (the base layer) and multiple\n+/// students (task-specific LoRA adapters), each specializing in different subjects.\n+///\n+/// In regular LoRA:\n+/// - You have one base layer (the teacher)\n+/// - One LoRA adapter (one student learning one subject)\n+/// - Output = base + lora_adaptation\n+///\n+/// In MultiLoRA:\n+/// - You have one base layer (the teacher)\n+/// - Multiple LoRA adapters (multiple students, each specializing in different tasks)\n+/// - Output = base + task_specific_lora_adaptation\n+///\n+/// This is powerful for:\n+/// 1. Multi-domain learning: Train on medical, legal, and technical documents simultaneously\n+/// 2. Multi-lingual models: One adapter per language\n+/// 3. Multi-task learning: Sentiment analysis, named entity recognition, question answering, etc.\n+/// 4. Continual learning: Add new tasks without forgetting old ones\n+///\n+/// Example use case:\n+/// - Base: Pre-trained language model\n+/// - Task 1: Sentiment analysis (rank=4)\n+/// - Task 2: Named entity recognition (rank=8)\n+/// - Task 3: Question answering (rank=16)\n+///\n+/// You can switch between tasks at runtime, and each task only trains its specific LoRA weights!\n+/// </para>\n+/// </remarks>\n+public class MultiLoRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Dictionary mapping task names to their specific LoRA layers.\n+    /// </summary>\n+    private readonly Dictionary<string, LoRALayer<T>> _taskAdapters;\n+\n+    /// <summary>\n+    /// The name of the currently active task.\n+    /// </summary>\n+    private string _currentTask;\n+\n+    /// <summary>\n+    /// Gets the dictionary of task-specific LoRA adapters.\n+    /// </summary>\n+    /// <remarks>\n+    /// Each task has its own dedicated LoRA layer with potentially different ranks.\n+    /// This allows for task-specific parameter efficiency optimization.\n+    /// </remarks>\n+    public IReadOnlyDictionary<string, LoRALayer<T>> TaskAdapters => _taskAdapters;\n+\n+    /// <summary>\n+    /// Gets or sets the name of the currently active task.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Changing this property switches which task-specific adapter is used during forward/backward passes.\n+    /// This allows dynamic task switching during inference or training.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is like switching between different \"modes\" of your model.\n+    /// Set it to \"sentiment\" for sentiment analysis, \"ner\" for named entity recognition, etc.\n+    /// The base layer stays the same, but the adaptation changes based on the task.\n+    /// </para>\n+    /// </remarks>\n+    /// <exception cref=\"ArgumentException\">Thrown when trying to set a task that hasn't been added.</exception>\n+    public string CurrentTask\n+    {\n+        get => _currentTask;\n+        set\n+        {\n+            if (!_taskAdapters.ContainsKey(value))\n+            {\n+                throw new ArgumentException($\"Task '{value}' has not been added. Available tasks: {string.Join(\", \", _taskAdapters.Keys)}\", nameof(value));\n+            }\n+            _currentTask = value;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Gets the number of tasks configured in this adapter.\n+    /// </summary>\n+    public int NumberOfTasks => _taskAdapters.Count;\n+\n+    /// <summary>\n+    /// Gets the total parameter count across all task adapters.\n+    /// </summary>\n+    /// <remarks>\n+    /// This includes parameters from the base layer (if not frozen) plus all task-specific LoRA layers.\n+    /// </remarks>\n+    public override int ParameterCount\n+    {\n+        get\n+        {\n+            int totalParams = _freezeBaseLayer ? 0 : _baseLayer.ParameterCount;\n+            foreach (var adapter in _taskAdapters.Values)\n+            {\n+                totalParams += adapter.ParameterCount;\n+            }\n+            return totalParams;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Initializes a new Multi-LoRA adapter with an initial default task.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with multiple LoRA adapters.</param>\n+    /// <param name=\"defaultTaskName\">The name of the default task.</param>\n+    /// <param name=\"defaultRank\">The rank for the default task's LoRA layer.</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer or defaultTaskName is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when defaultTaskName is empty or whitespace.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// The adapter is initialized with one default task. Additional tasks can be added using AddTask().\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This creates a MultiLoRA adapter starting with one task.\n+    /// Think of it like creating a multi-tool that starts with one blade, and you can add more tools later.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The shared foundation layer (like the handle of a multi-tool)\n+    /// - defaultTaskName: A name for your first task (e.g., \"sentiment\", \"translation\")\n+    /// - defaultRank: How complex this task's adaptation is (higher = more parameters)\n+    /// - alpha: Strength of the adaptation\n+    /// - freezeBaseLayer: Whether to lock the base layer (usually true to save memory)\n+    ///\n+    /// After creation, you can add more tasks with different ranks optimized for each task's complexity.\n+    /// </para>\n+    /// </remarks>\n+    public MultiLoRAAdapter(\n+        ILayer<T> baseLayer,\n+        string defaultTaskName,\n+        int defaultRank,\n+        double alpha = -1,\n+        bool freezeBaseLayer = true)\n+        : base(baseLayer, defaultRank, alpha, freezeBaseLayer)\n+    {\n+        if (string.IsNullOrWhiteSpace(defaultTaskName))\n+        {\n+            throw new ArgumentException(\"Default task name cannot be null or whitespace\", nameof(defaultTaskName));\n+        }\n+\n+        _taskAdapters = new Dictionary<string, LoRALayer<T>>();\n+        _currentTask = defaultTaskName;\n+\n+        // Add the default task using the base class's LoRA layer\n+        _taskAdapters[defaultTaskName] = _loraLayer;\n+    }\n+\n+    /// <summary>\n+    /// Adds a new task with its own LoRA adapter.\n+    /// </summary>\n+    /// <param name=\"taskName\">The name of the task (must be unique).</param>\n+    /// <param name=\"rank\">The rank for this task's LoRA layer.</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor for this task (defaults to rank if negative).</param>\n+    /// <exception cref=\"ArgumentException\">Thrown when taskName is null, empty, whitespace, or already exists.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// Each task can have a different rank, allowing you to optimize parameter usage based on task complexity.\n+    /// More complex tasks can use higher ranks, while simpler tasks can use lower ranks.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This adds a new \"mode\" to your model.\n+    ///\n+    /// Example:\n+    /// - Task \"sentiment\" with rank=4: Simple classification (positive/negative/neutral)\n+    /// - Task \"ner\" with rank=8: More complex named entity recognition\n+    /// - Task \"qa\" with rank=16: Even more complex question answering\n+    ///\n+    /// Each task gets its own small set of parameters (determined by rank) that learn task-specific\n+    /// adaptations, while all tasks share the same base layer knowledge.\n+    ///\n+    /// Benefits:\n+    /// - Different ranks for different task complexities\n+    /// - No interference between tasks (each has separate parameters)\n+    /// - Can train tasks independently or simultaneously\n+    /// - Add new tasks without retraining existing ones\n+    /// </para>\n+    /// </remarks>\n+    public void AddTask(string taskName, int rank, double alpha = -1)\n+    {\n+        if (string.IsNullOrWhiteSpace(taskName))\n+        {\n+            throw new ArgumentException(\"Task name cannot be null or whitespace\", nameof(taskName));\n+        }\n+\n+        if (_taskAdapters.ContainsKey(taskName))\n+        {\n+            throw new ArgumentException($\"Task '{taskName}' already exists\", nameof(taskName));\n+        }\n+\n+        // Create a new LoRA layer for this task\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        LoRALayer<T> taskAdapter = new LoRALayer<T>(inputSize, outputSize, rank, alpha);\n+\n+        _taskAdapters[taskName] = taskAdapter;\n+    }\n+\n+    /// <summary>\n+    /// Removes a task and its associated LoRA adapter.\n+    /// </summary>\n+    /// <param name=\"taskName\">The name of the task to remove.</param>\n+    /// <returns>True if the task was removed, false if it didn't exist.</returns>\n+    /// <exception cref=\"InvalidOperationException\">Thrown when trying to remove the last remaining task.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// You cannot remove the last task. At least one task must always be present.\n+    /// If removing the current task, the CurrentTask property will be set to the first remaining task.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This removes a task you no longer need.\n+    /// Like removing a tool from your multi-tool, but you must always keep at least one.\n+    /// If you remove the currently active task, the adapter automatically switches to another available task.\n+    /// </para>\n+    /// </remarks>\n+    public bool RemoveTask(string taskName)\n+    {\n+        if (_taskAdapters.Count <= 1)\n+        {\n+            throw new InvalidOperationException(\"Cannot remove the last task. At least one task must remain.\");\n+        }\n+\n+        bool removed = _taskAdapters.Remove(taskName);\n+\n+        // If we removed the current task, switch to the first available task\n+        if (removed && _currentTask == taskName)\n+        {\n+            _currentTask = _taskAdapters.Keys.First();\n+        }\n+\n+        return removed;\n+    }\n+\n+    /// <summary>\n+    /// Sets the current task for subsequent forward/backward operations.\n+    /// </summary>\n+    /// <param name=\"taskName\">The name of the task to activate.</param>\n+    /// <exception cref=\"ArgumentException\">Thrown when the task doesn't exist.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This switches which task the model is currently working on.\n+    /// Call this before forward() to tell the model what kind of task it should perform.\n+    ///\n+    /// Example usage:\n+    /// ```csharp\n+    /// adapter.SetCurrentTask(\"sentiment\");\n+    /// var sentimentOutput = adapter.Forward(input);\n+    ///\n+    /// adapter.SetCurrentTask(\"ner\");\n+    /// var nerOutput = adapter.Forward(sameInput);\n+    /// ```\n+    ///\n+    /// Same input, different outputs based on which task is active!\n+    /// </para>\n+    /// </remarks>\n+    public void SetCurrentTask(string taskName)\n+    {\n+        CurrentTask = taskName; // Uses property setter for validation\n+    }\n+\n+    /// <summary>\n+    /// Gets the LoRA layer for a specific task.\n+    /// </summary>\n+    /// <param name=\"taskName\">The name of the task.</param>\n+    /// <returns>The LoRA layer for the specified task.</returns>\n+    /// <exception cref=\"ArgumentException\">Thrown when the task doesn't exist.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This lets you access a specific task's LoRA layer directly.\n+    /// Useful for inspecting parameters, getting statistics, or manual manipulation.\n+    /// </para>\n+    /// </remarks>\n+    public LoRALayer<T> GetTaskAdapter(string taskName)\n+    {\n+        if (!_taskAdapters.TryGetValue(taskName, out var adapter))\n+        {\n+            throw new ArgumentException($\"Task '{taskName}' not found. Available tasks: {string.Join(\", \", _taskAdapters.Keys)}\", nameof(taskName));\n+        }\n+        return adapter;\n+    }\n+\n+    /// <summary>\n+    /// Gets the rank of a specific task's LoRA adapter.\n+    /// </summary>\n+    /// <param name=\"taskName\">The name of the task.</param>\n+    /// <returns>The rank of the task's LoRA layer.</returns>\n+    /// <exception cref=\"ArgumentException\">Thrown when the task doesn't exist.</exception>\n+    public int GetTaskRank(string taskName)\n+    {\n+        return GetTaskAdapter(taskName).Rank;\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass using the currently active task's adapter.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Sum of base layer output and current task's LoRA output.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The forward pass computes: output = base_layer(input) + current_task_lora(input)\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This processes data through the model using the current task.\n+    /// 1. Input goes through the base layer (shared knowledge)\n+    /// 2. Input goes through the current task's LoRA layer (task-specific adaptation)\n+    /// 3. Results are added together\n+    ///\n+    /// The magic: Different tasks produce different outputs even though they share the same base layer!\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        // Forward through base layer\n+        Tensor<T> baseOutput = _baseLayer.Forward(input);\n+\n+        // Forward through current task's LoRA layer\n+        LoRALayer<T> currentAdapter = _taskAdapters[_currentTask];\n+        Tensor<T> loraOutput = currentAdapter.Forward(input);\n+\n+        // Sum the outputs\n+        Tensor<T> result = new Tensor<T>(baseOutput.Shape);\n+        for (int i = 0; i < baseOutput.Length; i++)\n+        {\n+            result[i] = NumOps.Add(baseOutput[i], loraOutput[i]);\n+        }\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Performs the backward pass through the current task's adapter.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient flowing back from the next layer.</param>\n+    /// <returns>Gradient to pass to the previous layer.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The backward pass only updates the current task's LoRA parameters. Other tasks are unaffected.\n+    /// This allows task-specific learning without interference.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> During training, this updates only the current task's parameters.\n+    ///\n+    /// Benefits:\n+    /// - Training task A doesn't mess up task B's learning\n+    /// - Can train tasks one at a time or in batches\n+    /// - No \"catastrophic forgetting\" between tasks\n+    ///\n+    /// The gradients flow through:\n+    /// 1. Current task's LoRA layer (gets updated)\n+    /// 2. Base layer (only updated if not frozen)\n+    /// 3. Combined gradients flow back to previous layers\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        // Backward through current task's LoRA layer\n+        LoRALayer<T> currentAdapter = _taskAdapters[_currentTask];\n+        Tensor<T> loraInputGrad = currentAdapter.Backward(outputGradient);\n+\n+        // Backward through base layer\n+        Tensor<T> baseInputGrad = _baseLayer.Backward(outputGradient);\n+\n+        // Sum input gradients\n+        Tensor<T> inputGrad = new Tensor<T>(loraInputGrad.Shape);\n+        for (int i = 0; i < loraInputGrad.Length; i++)\n+        {\n+            inputGrad[i] = NumOps.Add(loraInputGrad[i], baseInputGrad[i]);\n+        }\n+\n+        // Update parameter gradients vector\n+        UpdateParameterGradientsFromLayers();\n+\n+        return inputGrad;\n+    }\n+\n+    /// <summary>\n+    /// Updates parameters for the current task only.\n+    /// </summary>\n+    /// <param name=\"learningRate\">The learning rate for parameter updates.</param>\n+    /// <remarks>\n+    /// <para>\n+    /// Only the current task's LoRA parameters are updated. Other tasks remain unchanged.\n+    /// The base layer is updated only if not frozen.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is where learning happens for the current task.\n+    /// Only the active task's parameters get updated, leaving other tasks untouched.\n+    /// This is key to multi-task learning without interference!\n+    /// </para>\n+    /// </remarks>\n+    public override void UpdateParameters(T learningRate)\n+    {\n+        // Update current task's LoRA layer\n+        LoRALayer<T> currentAdapter = _taskAdapters[_currentTask];\n+        currentAdapter.UpdateParameters(learningRate);\n+\n+        // Update base layer if not frozen\n+        if (!_freezeBaseLayer)\n+        {\n+            _baseLayer.UpdateParameters(learningRate);\n+        }\n+\n+        // Update parameter vector\n+        UpdateParametersFromLayers();\n+    }\n+\n+    /// <summary>\n+    /// Gets the current parameters as a vector.\n+    /// </summary>\n+    /// <returns>Vector containing base parameters (if not frozen) and all task adapters' parameters.</returns>\n+    public override Vector<T> GetParameters()\n+    {\n+        Vector<T> parameters = new Vector<T>(ParameterCount);\n+        int idx = 0;\n+\n+        // Base layer parameters (if not frozen)\n+        if (!_freezeBaseLayer)\n+        {\n+            Vector<T> baseParams = _baseLayer.GetParameters();\n+            for (int i = 0; i < baseParams.Length; i++)\n+            {\n+                parameters[idx++] = baseParams[i];\n+            }\n+        }\n+\n+        // All task adapters' parameters\n+        foreach (var adapter in _taskAdapters.Values)\n+        {\n+            Vector<T> taskParams = adapter.GetParameters();\n+            for (int i = 0; i < taskParams.Length; i++)\n+            {\n+                parameters[idx++] = taskParams[i];\n+            }\n+        }\n+\n+        return parameters;\n+    }\n+\n+    /// <summary>\n+    /// Sets the layer parameters from a vector.\n+    /// </summary>\n+    /// <param name=\"parameters\">Vector containing all parameters.</param>\n+    public override void SetParameters(Vector<T> parameters)\n+    {\n+        if (parameters.Length != ParameterCount)\n+        {\n+            throw new ArgumentException($\"Expected {ParameterCount} parameters, got {parameters.Length}\", nameof(parameters));\n+        }\n+\n+        int idx = 0;\n+\n+        // Base layer parameters (if not frozen)\n+        if (!_freezeBaseLayer)\n+        {\n+            int baseParamCount = _baseLayer.ParameterCount;\n+            Vector<T> baseParams = new Vector<T>(baseParamCount);\n+            for (int i = 0; i < baseParamCount; i++)\n+            {\n+                baseParams[i] = parameters[idx++];\n+            }\n+            _baseLayer.SetParameters(baseParams);\n+        }\n+\n+        // All task adapters' parameters\n+        foreach (var adapter in _taskAdapters.Values)\n+        {\n+            int taskParamCount = adapter.ParameterCount;\n+            Vector<T> taskParams = new Vector<T>(taskParamCount);\n+            for (int i = 0; i < taskParamCount; i++)\n+            {\n+                taskParams[i] = parameters[idx++];\n+            }\n+            adapter.SetParameters(taskParams);\n+        }\n+\n+        Parameters = parameters.Clone();\n+    }\n+\n+    /// <summary>\n+    /// Merges a specific task's LoRA weights into the base layer.\n+    /// </summary>\n+    /// <param name=\"taskName\">The name of the task to merge.</param>\n+    /// <returns>A new layer with the specified task's LoRA weights merged into the base layer.</returns>\n+    /// <exception cref=\"ArgumentException\">Thrown when the task doesn't exist.</exception>\n+    /// <exception cref=\"NotSupportedException\">Thrown when the base layer type doesn't support merging.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// This creates a deployment-ready layer for a specific task by merging its LoRA weights\n+    /// into the base layer. This is useful when you want to deploy a single-task model.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This \"bakes in\" one task's adaptations for deployment.\n+    ///\n+    /// Use case:\n+    /// - You trained a MultiLoRA model with 5 tasks\n+    /// - For production, you only need the \"sentiment\" task\n+    /// - Call MergeTaskToLayer(\"sentiment\") to create a standalone layer\n+    /// - Deploy just that layer (smaller, faster, simpler)\n+    ///\n+    /// The merged layer has the base weights + that task's LoRA weights combined into one.\n+    /// </para>\n+    /// </remarks>\n+    public ILayer<T> MergeTaskToLayer(string taskName)\n+    {\n+        if (!_taskAdapters.TryGetValue(taskName, out var taskAdapter))\n+        {\n+            throw new ArgumentException($\"Task '{taskName}' not found. Available tasks: {string.Join(\", \", _taskAdapters.Keys)}\", nameof(taskName));\n+        }\n+\n+        // This implementation assumes the base layer is a DenseLayer or FullyConnectedLayer\n+        // More sophisticated implementations could support other layer types\n+        DenseLayer<T>? denseBase = _baseLayer as DenseLayer<T>;\n+        FullyConnectedLayer<T>? fcBase = _baseLayer as FullyConnectedLayer<T>;\n+\n+        if (denseBase == null && fcBase == null)\n+        {\n+            throw new NotSupportedException($\"Merging is currently only supported for DenseLayer and FullyConnectedLayer base layers. Base layer type: {_baseLayer.GetType().Name}\");\n+        }\n+\n+        // Get the LoRA weight contribution for this task\n+        Matrix<T> loraWeights = taskAdapter.MergeWeights();\n+\n+        // Get base layer parameters\n+        Vector<T> baseParams = _baseLayer.GetParameters();\n+\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        int weightCount = inputSize * outputSize;\n+\n+        // Create new parameters with merged weights\n+        Vector<T> mergedParams = new Vector<T>(baseParams.Length);\n+\n+        // Merge weights\n+        for (int i = 0; i < weightCount; i++)\n+        {\n+            int row = i / inputSize;\n+            int col = i % inputSize;\n+            mergedParams[i] = NumOps.Add(baseParams[i], loraWeights[row, col]);\n+        }\n+\n+        // Copy biases unchanged\n+        for (int i = weightCount; i < baseParams.Length; i++)\n+        {\n+            mergedParams[i] = baseParams[i];\n+        }\n+\n+        // Create a new dense layer with merged parameters\n+        DenseLayer<T> mergedLayer = new DenseLayer<T>(inputSize, outputSize, (IActivationFunction<T>?)null);\n+        mergedLayer.SetParameters(mergedParams);\n+\n+        return mergedLayer;\n+    }\n+\n+    /// <summary>\n+    /// Merges the currently active task's LoRA weights into the base layer.\n+    /// </summary>\n+    /// <returns>A new layer with current task's LoRA weights merged into the base layer.</returns>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This is a shortcut to merge the current task without specifying its name.\n+    /// Equivalent to calling MergeTaskToLayer(CurrentTask).\n+    /// </para>\n+    /// </remarks>\n+    public override ILayer<T> MergeToOriginalLayer()\n+    {\n+        return MergeTaskToLayer(_currentTask);\n+    }\n+\n+    /// <summary>\n+    /// Updates the parameter vector from the current layer states.\n+    /// </summary>\n+    private void UpdateParametersFromLayers()\n+    {\n+        Parameters = GetParameters();\n+    }\n+\n+    /// <summary>\n+    /// Updates the parameter gradients vector from the layer gradients.\n+    /// </summary>\n+    private void UpdateParameterGradientsFromLayers()\n+    {\n+        ParameterGradients = new Vector<T>(ParameterCount);\n+        int idx = 0;\n+\n+        // Base layer gradients (if not frozen)\n+        if (!_freezeBaseLayer)\n+        {\n+            Vector<T> baseGrads = _baseLayer.GetParameterGradients();\n+            for (int i = 0; i < baseGrads.Length; i++)\n+            {\n+                ParameterGradients[idx++] = baseGrads[i];\n+            }\n+        }\n+\n+        // Current task's gradients\n+        LoRALayer<T> currentAdapter = _taskAdapters[_currentTask];\n+        Vector<T> loraGrads = currentAdapter.GetParameterGradients();\n+        for (int i = 0; i < loraGrads.Length; i++)\n+        {\n+            ParameterGradients[idx++] = loraGrads[i];\n+        }\n+\n+        // Other tasks have zero gradients (they weren't updated)\n+        while (idx < ParameterCount)\n+        {\n+            ParameterGradients[idx++] = NumOps.Zero;\n+        }\n+    }","path":"src/NeuralNetworks/Layers/MultiLoRAAdapter.cs","commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","original_commit_id":"c11235933bf7c341c8b4a223cf6ac11b757d1480","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Pack gradients in the same order as the parameters.**\n\n`GetParameters`/`SetParameters` iterate `_taskAdapters` to lay out each task‚Äôs slice. Here we only emit the active task‚Äôs gradients and then append a tail of zeros, so when the active task is not the first entry the gradient block ends up in the wrong parameter segment. Iterate the adapters in dictionary order and write zeros for inactive tasks so the layout stays aligned.\n\n```diff\n-        LoRALayer<T> currentAdapter = _taskAdapters[_currentTask];\n-        Vector<T> loraGrads = currentAdapter.GetParameterGradients();\n-        for (int i = 0; i < loraGrads.Length; i++)\n-        {\n-            ParameterGradients[idx++] = loraGrads[i];\n-        }\n-\n-        // Other tasks have zero gradients (they weren't updated)\n-        while (idx < ParameterCount)\n-        {\n-            ParameterGradients[idx++] = NumOps.Zero;\n-        }\n+        LoRALayer<T> currentAdapter = _taskAdapters[_currentTask];\n+        foreach (var adapter in _taskAdapters.Values)\n+        {\n+            Vector<T>? grads = adapter == currentAdapter ? adapter.GetParameterGradients() : null;\n+\n+            for (int i = 0; i < adapter.ParameterCount; i++)\n+            {\n+                ParameterGradients[idx++] = grads != null ? grads[i] : NumOps.Zero;\n+            }\n+        }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    private void UpdateParameterGradientsFromLayers()\n    {\n        ParameterGradients = new Vector<T>(ParameterCount);\n        int idx = 0;\n\n        // Base layer gradients (if not frozen)\n        if (!_freezeBaseLayer)\n        {\n            Vector<T> baseGrads = _baseLayer.GetParameterGradients();\n            for (int i = 0; i < baseGrads.Length; i++)\n            {\n                ParameterGradients[idx++] = baseGrads[i];\n            }\n        }\n\n        // Current task's gradients\n        LoRALayer<T> currentAdapter = _taskAdapters[_currentTask];\n        foreach (var adapter in _taskAdapters.Values)\n        {\n            Vector<T>? grads = adapter == currentAdapter ? adapter.GetParameterGradients() : null;\n\n            for (int i = 0; i < adapter.ParameterCount; i++)\n            {\n                ParameterGradients[idx++] = grads != null ? grads[i] : NumOps.Zero;\n            }\n        }\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/Layers/MultiLoRAAdapter.cs around lines 592 to 620, the\nmethod currently appends only the active task's gradients then a tail of zeros\nwhich misaligns gradient blocks when the active task is not the first; change it\nto pack gradients in the exact same order as GetParameters/SetParameters by\niterating the task adapter collection in the same deterministic (dictionary)\norder used elsewhere: first emit base layer gradients (if not frozen), then\niterate each task adapter in that ordered sequence and for each adapter write\nits actual gradient block if it is the active task or write zeros of that\nadapter's parameter length if it is inactive, ensuring the idx increments by\neach adapter's parameter length so the final ParameterGradients layout matches\nparameters exactly.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:32:37Z","updated_at":"2025-11-02T02:32:40Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106326","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106326"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106326"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106326/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":592,"original_start_line":592,"start_side":"RIGHT","line":620,"original_line":620,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":620,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106334","pull_request_review_id":3407993158,"id":2484106334,"node_id":"PRRC_kwDOKSXUF86UEHRe","diff_hunk":"@@ -0,0 +1,756 @@\n+using AiDotNet.Interfaces;\n+using System;\n+\n+namespace AiDotNet.NeuralNetworks.Layers;\n+\n+/// <summary>\n+/// Implements NOLA (Compressing LoRA using Linear Combination of Random Basis) adapter for extreme parameter efficiency.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// NOLA overcomes the rank-one lower bound in traditional LoRA by re-parameterizing the low-rank matrices\n+/// using linear combinations of randomly generated basis matrices. Instead of optimizing the full low-rank\n+/// matrices A and B, NOLA:\n+/// 1. Generates fixed random basis matrices using a deterministic seed\n+/// 2. Optimizes only scalar coefficients that linearly combine these basis matrices\n+/// 3. Regenerates basis matrices during forward/backward passes to minimize memory usage\n+/// </para>\n+/// <para>\n+/// This decouples the number of trainable parameters from both the choice of rank and the network architecture,\n+/// achieving compression ratios of 20x over standard LoRA without accuracy degradation.\n+/// </para>\n+/// <para><b>For Beginners:</b> NOLA is an extreme compression technique for LoRA that makes fine-tuning\n+/// even more efficient. Instead of storing and training two low-rank matrices (A and B), NOLA:\n+///\n+/// - Generates random \"template\" matrices on-the-fly (same random numbers every time due to fixed seed)\n+/// - Only trains small coefficients that control how much of each template to use\n+/// - Achieves 2-3x fewer parameters than LoRA while maintaining performance\n+///\n+/// Think of it like this:\n+/// - Traditional LoRA: You have 100 adjustable knobs (parameters)\n+/// - NOLA: You have 5 master controls that blend pre-defined settings\n+///\n+/// Key innovations:\n+/// 1. <b>Memory efficiency:</b> Random basis matrices are discarded after use and regenerated when needed\n+/// 2. <b>Parameter efficiency:</b> Only coefficients are trained, not full matrices\n+/// 3. <b>Performance:</b> Achieves similar or better results than LoRA with far fewer parameters\n+///\n+/// Example compression (1000x1000 layer, rank=8):\n+/// - LoRA: 16,000 parameters (1000√ó8 + 8√ó1000)\n+/// - NOLA with 100 basis: 200 parameters (100 coefficients for A + 100 for B) - 80x reduction!\n+///\n+/// On LLaMA-2 70B, NOLA achieves 20x compression over LoRA with no accuracy loss.\n+/// </para>\n+/// <para><b>Reference:</b> NOLA: Compressing LoRA using Linear Combination of Random Basis\n+/// (Koohpayegani et al., ICLR 2024) - https://arxiv.org/abs/2310.02556\n+/// </para>\n+/// </remarks>\n+public class NOLAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Random number generator with fixed seed for reproducible basis generation.\n+    /// </summary>\n+    private readonly Random _basisGenerator;\n+\n+    /// <summary>\n+    /// Number of random basis matrices to use for each low-rank matrix.\n+    /// </summary>\n+    private readonly int _numBasis;\n+\n+    /// <summary>\n+    /// Trainable coefficients for matrix A basis combination (size: numBasis).\n+    /// </summary>\n+    private Vector<T> _coefficientsA;\n+\n+    /// <summary>\n+    /// Trainable coefficients for matrix B basis combination (size: numBasis).\n+    /// </summary>\n+    private Vector<T> _coefficientsB;\n+\n+    /// <summary>\n+    /// Gradients for coefficients A computed during backpropagation.\n+    /// </summary>\n+    private Vector<T>? _coefficientsAGradient;\n+\n+    /// <summary>\n+    /// Gradients for coefficients B computed during backpropagation.\n+    /// </summary>\n+    private Vector<T>? _coefficientsBGradient;\n+\n+    /// <summary>\n+    /// Cached matrix A from last forward pass (used in backward pass).\n+    /// </summary>\n+    private Matrix<T>? _cachedMatrixA;\n+\n+    /// <summary>\n+    /// Cached matrix B from last forward pass (used in backward pass).\n+    /// </summary>\n+    private Matrix<T>? _cachedMatrixB;\n+\n+    /// <summary>\n+    /// Cached input from last forward pass (needed for gradient computation).\n+    /// </summary>\n+    private Tensor<T>? _lastInput;\n+\n+    /// <summary>\n+    /// Seed for reproducible random basis generation.\n+    /// </summary>\n+    private readonly int _seed;\n+\n+    /// <summary>\n+    /// Gets the number of basis matrices used for compression.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This determines the compression ratio. Fewer basis matrices = more compression but less flexibility.\n+    /// Typical values range from 10 to 100 depending on the task.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is the number of \"template\" matrices we use. More templates\n+    /// give more flexibility but require more coefficients to train. It's the main knob for controlling\n+    /// the compression-accuracy trade-off in NOLA.\n+    /// </para>\n+    /// </remarks>\n+    public int NumBasis => _numBasis;\n+\n+    /// <summary>\n+    /// Gets the compression ratio compared to standard LoRA.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Compression ratio = (LoRA parameters) / (NOLA parameters)\n+    /// Higher values indicate more extreme compression.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This tells you how much more efficient NOLA is compared to regular LoRA.\n+    /// For example, a compression ratio of 20 means NOLA uses 20 times fewer parameters!\n+    /// </para>\n+    /// </remarks>\n+    public double CompressionRatio\n+    {\n+        get\n+        {\n+            int inputSize = GetInputShape()[0];\n+            int outputSize = GetOutputShape()[0];\n+            int loraParams = (inputSize * Rank) + (Rank * outputSize);\n+            int nolaParams = 2 * _numBasis;  // coefficients for A and B\n+            return (double)loraParams / nolaParams;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Initializes a new NOLA adapter with the specified parameters.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with NOLA.</param>\n+    /// <param name=\"rank\">The rank of the low-rank decomposition (determines basis matrix dimensions).</param>\n+    /// <param name=\"numBasis\">Number of random basis matrices to use (controls compression ratio).</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"seed\">Random seed for reproducible basis generation (default: 42).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when rank or numBasis are invalid.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// NOLA initialization:\n+    /// - Coefficients are initialized to zero (so NOLA starts with no effect, like LoRA)\n+    /// - Random basis matrices are generated on-demand during forward/backward passes\n+    /// - A fixed seed ensures reproducible basis generation across training\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This creates a new NOLA adapter. Important parameters:\n+    ///\n+    /// - baseLayer: The layer you want to make ultra-efficient to fine-tune\n+    /// - rank: Controls the \"bottleneck\" dimension (same as in LoRA)\n+    /// - numBasis: Controls compression (fewer = more compression, less flexibility)\n+    /// - seed: Ensures you get the same random \"templates\" every time\n+    ///\n+    /// Recommended values:\n+    /// - For extreme compression (20x): numBasis = rank / 2\n+    /// - For balanced compression (10x): numBasis = rank\n+    /// - For moderate compression (5x): numBasis = rank * 2\n+    ///\n+    /// Example: rank=8, numBasis=4 gives ~40x compression over full fine-tuning!\n+    /// </para>\n+    /// </remarks>\n+    public NOLAAdapter(\n+        ILayer<T> baseLayer,\n+        int rank,\n+        int numBasis,\n+        double alpha = -1,\n+        int seed = 42,\n+        bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        if (numBasis <= 0)\n+        {\n+            throw new ArgumentException(\"Number of basis matrices must be positive\", nameof(numBasis));\n+        }\n+\n+        _numBasis = numBasis;\n+        _seed = seed;\n+        _basisGenerator = new Random(_seed);\n+\n+        // Initialize coefficients to zero (NOLA starts with no effect)\n+        _coefficientsA = new Vector<T>(_numBasis);\n+        _coefficientsB = new Vector<T>(_numBasis);\n+        for (int i = 0; i < _numBasis; i++)\n+        {\n+            _coefficientsA[i] = NumOps.Zero;\n+            _coefficientsB[i] = NumOps.Zero;\n+        }\n+\n+        // Update parameter count to reflect NOLA compression\n+        // Parameters: coefficientsA + coefficientsB (+ base layer if not frozen)\n+        int nolaParams = 2 * _numBasis;\n+        Parameters = new Vector<T>(_freezeBaseLayer ? nolaParams : (_baseLayer.ParameterCount + nolaParams));\n+        UpdateParametersFromCoefficients();\n+    }\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters.\n+    /// </summary>\n+    /// <remarks>\n+    /// For NOLA, this is just 2 * numBasis (coefficients for A and B), plus base layer parameters if not frozen.\n+    /// This is dramatically smaller than standard LoRA's (inputSize * rank) + (rank * outputSize).\n+    /// </remarks>\n+    public override int ParameterCount => _freezeBaseLayer\n+        ? (2 * _numBasis)\n+        : (_baseLayer.ParameterCount + 2 * _numBasis);\n+","path":"src/NeuralNetworks/Layers/NOLAAdapter.cs","commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","original_commit_id":"c11235933bf7c341c8b4a223cf6ac11b757d1480","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Constructor fails because ParameterCount override returns zero**  \nLines [214-217] override `ParameterCount` using `_numBasis`, but `_numBasis` remains 0 while `LoRAAdapterBase`‚Äôs constructor runs. That base constructor immediately queries `ParameterCount`, allocates `Parameters` with length 0, and then calls `UpdateParametersFromLayers()`, which tries to copy the LoRA weights into that zero-length vector. Result: constructing `NOLAAdapter` throws (typically `IndexOutOfRangeException`) before the derived constructor body executes. Please guard the override so the base ctor sees at least the base count (e.g., fall back to `base.ParameterCount` while `_numBasis == 0`), and then resize/repurpose the parameter buffer after `_numBasis` is initialized. Also make sure the pack/unpack helpers include the extra coefficient slots once the override starts returning the larger size.\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:32:38Z","updated_at":"2025-11-02T02:32:41Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106334","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106334"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106334"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106334/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":214,"original_start_line":214,"start_side":"RIGHT","line":217,"original_line":217,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":217,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106339","pull_request_review_id":3407993158,"id":2484106339,"node_id":"PRRC_kwDOKSXUF86UEHRj","diff_hunk":"@@ -0,0 +1,628 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.NeuralNetworks.Layers;\n+\n+/// <summary>\n+/// Quantization-Aware LoRA (QA-LoRA) adapter that combines parameter-efficient fine-tuning with group-wise quantization awareness.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// QA-LoRA extends standard LoRA by being aware of quantization during training. This allows the adapter\n+/// to learn compensations for quantization errors, resulting in better final accuracy compared to\n+/// post-training quantization approaches. The key innovation is simulating quantization during the\n+/// forward pass so that gradients account for quantization effects.\n+/// </para>\n+/// <para><b>For Beginners:</b> QA-LoRA solves a critical problem when deploying models to resource-constrained devices.\n+///\n+/// The Problem:\n+/// - Modern neural networks use high-precision numbers (32-bit floats)\n+/// - Mobile/edge devices need lower precision (4-bit or 8-bit integers) for speed and memory\n+/// - Converting after training (post-training quantization) often loses accuracy\n+///\n+/// QA-LoRA's Solution:\n+/// - Simulates low-precision during training (quantization-aware training)\n+/// - Learns to compensate for quantization errors\n+/// - Uses LoRA for parameter efficiency (only trains the adaptation, not full model)\n+/// - Applies group-wise quantization (groups of weights share scaling factors)\n+///\n+/// Key Concepts:\n+///\n+/// 1. Quantization: Converting high-precision numbers to low-precision\n+///    Example: 32-bit float 0.7234 ‚Üí 4-bit integer 11 (range 0-15)\n+///\n+/// 2. Group-wise Quantization: Instead of one scale for all weights, weights are divided into groups,\n+///    each with its own scale. This preserves more information.\n+///    Example: 64 weights ‚Üí 4 groups of 16 weights each, each group has its own scale\n+///\n+/// 3. Quantization-Aware Training: During training, simulate quantization in forward pass:\n+///    - Convert weights to low-precision (quantize)\n+///    - Immediately convert back to high-precision (dequantize)\n+///    - Use these \"quantized\" values for computation\n+///    - Gradients learn to compensate for the quantization noise\n+///\n+/// 4. Straight-Through Estimator (STE): During backward pass, treat quantization as identity\n+///    - Forward: y = quantize(x)\n+///    - Backward: ‚àÇy/‚àÇx ‚âà 1 (gradient flows through unchanged)\n+///    - This allows gradients to update the full-precision weights\n+///\n+/// Parameters:\n+/// - QuantizationBits: How many bits to use (4-bit, 8-bit, etc.)\n+/// - GroupSize: How many weights per quantization group (e.g., 64, 128)\n+/// - Smaller GroupSize = more scales = better accuracy but more overhead\n+/// - Larger GroupSize = fewer scales = more efficient but less accurate\n+///\n+/// Example Workflow:\n+/// 1. Training: Forward pass uses simulated 4-bit quantization\n+/// 2. Gradients: Backward pass learns to work around quantization errors\n+/// 3. Deployment: Actually quantize the merged weights to 4-bit for inference\n+/// 4. Result: Much better accuracy than quantizing after training\n+///\n+/// Research Context:\n+/// - QLoRA (May 2023): Introduced efficient 4-bit quantization for LoRA\n+/// - QA-LoRA: Extends this with quantization-aware training for better results\n+/// - Typical improvement: 1-3% accuracy gain over post-training quantization\n+///\n+/// Use Cases:\n+/// - Deploying large language models on mobile devices\n+/// - Edge AI applications with strict memory constraints\n+/// - Reducing model size while maintaining accuracy\n+/// - Fine-tuning for deployment on specific hardware (TPUs, specialized accelerators)\n+/// </para>\n+/// </remarks>\n+public class QALoRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Number of bits to use for quantization (e.g., 4, 8).\n+    /// </summary>\n+    private int _quantizationBits;\n+\n+    /// <summary>\n+    /// Number of weights per quantization group.\n+    /// </summary>\n+    /// <remarks>\n+    /// Smaller groups preserve more information but require more scaling factors.\n+    /// Typical values: 64, 128, 256.\n+    /// </remarks>\n+    private int _groupSize;\n+\n+    /// <summary>\n+    /// Whether quantization simulation is currently enabled.\n+    /// </summary>\n+    /// <remarks>\n+    /// Can be disabled during initial warmup or final evaluation.\n+    /// </remarks>\n+    private bool _quantizationEnabled;\n+\n+    /// <summary>\n+    /// Gets or sets the number of bits used for quantization.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Common values:\n+    /// - 4 bits: Extremely memory-efficient, requires careful tuning\n+    /// - 8 bits: Good balance of efficiency and accuracy\n+    /// - 16 bits: Close to full precision, minimal savings\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This controls how much compression you apply.\n+    /// - 4-bit: 8x compression (32-bit ‚Üí 4-bit), more aggressive\n+    /// - 8-bit: 4x compression (32-bit ‚Üí 8-bit), safer choice\n+    /// Lower bits = smaller model but harder to maintain accuracy.\n+    /// </para>\n+    /// </remarks>\n+    public int QuantizationBits\n+    {\n+        get => _quantizationBits;\n+        set\n+        {\n+            if (value < 1 || value > 16)\n+            {\n+                throw new ArgumentException(\"Quantization bits must be between 1 and 16\", nameof(value));\n+            }\n+            _quantizationBits = value;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Gets or sets the group size for group-wise quantization.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Group-wise quantization divides weights into groups, each with independent scaling factors.\n+    /// This preserves more dynamic range than using a single scale for all weights.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Imagine you have 1024 weights to quantize:\n+    /// - GroupSize = 1024: One scale for all weights (simple but loses information)\n+    /// - GroupSize = 128: Eight scales (1024/128 = 8 groups, better accuracy)\n+    /// - GroupSize = 64: Sixteen scales (1024/64 = 16 groups, even better but more overhead)\n+    ///\n+    /// Smaller groups mean each group's weights are more similar, so a single scale per group\n+    /// is more accurate. But you need to store more scales.\n+    /// </para>\n+    /// </remarks>\n+    public int GroupSize\n+    {\n+        get => _groupSize;\n+        set\n+        {\n+            if (value < 1)\n+            {\n+                throw new ArgumentException(\"Group size must be positive\", nameof(value));\n+            }\n+            _groupSize = value;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Gets or sets whether quantization simulation is enabled during forward/backward passes.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Disabling quantization can be useful for:\n+    /// - Initial warmup phases\n+    /// - Evaluating full-precision performance\n+    /// - Debugging training issues\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is like a toggle switch:\n+    /// - Enabled: Simulate low-precision during training (quantization-aware)\n+    /// - Disabled: Use full-precision (standard LoRA training)\n+    /// You might start with it disabled for stability, then enable it partway through training.\n+    /// </para>\n+    /// </remarks>\n+    public bool QuantizationEnabled\n+    {\n+        get => _quantizationEnabled;\n+        set => _quantizationEnabled = value;\n+    }\n+\n+    /// <summary>\n+    /// Initializes a new QA-LoRA adapter with quantization awareness.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with QA-LoRA.</param>\n+    /// <param name=\"rank\">The rank of the LoRA decomposition.</param>\n+    /// <param name=\"quantizationBits\">Number of bits for quantization (e.g., 4, 8).</param>\n+    /// <param name=\"groupSize\">Number of weights per quantization group.</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when quantizationBits or groupSize are invalid.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This creates a QA-LoRA adapter that will train with quantization awareness.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to efficiently fine-tune\n+    /// - rank: How much compression for LoRA (lower = fewer parameters)\n+    /// - quantizationBits: Target precision for deployment (4 or 8 typically)\n+    /// - groupSize: Granularity of quantization (64-128 recommended)\n+    /// - alpha: How strong the LoRA effect is\n+    /// - freezeBaseLayer: Whether to lock the original weights (usually true)\n+    ///\n+    /// Example: QALoRAAdapter(myLayer, rank=8, quantizationBits=4, groupSize=64)\n+    /// - Uses 8-rank LoRA for parameter efficiency\n+    /// - Simulates 4-bit quantization during training\n+    /// - Groups of 64 weights share scaling factors\n+    /// </para>\n+    /// </remarks>\n+    public QALoRAAdapter(\n+        ILayer<T> baseLayer,\n+        int rank,\n+        int quantizationBits,\n+        int groupSize,\n+        double alpha = -1,\n+        bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        if (quantizationBits < 1 || quantizationBits > 16)\n+        {\n+            throw new ArgumentException(\"Quantization bits must be between 1 and 16\", nameof(quantizationBits));\n+        }\n+\n+        if (groupSize < 1)\n+        {\n+            throw new ArgumentException(\"Group size must be positive\", nameof(groupSize));\n+        }\n+\n+        _quantizationBits = quantizationBits;\n+        _groupSize = groupSize;\n+        _quantizationEnabled = true; // Enabled by default\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass through both base and LoRA layers with quantization simulation.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Sum of base layer output and quantized LoRA output.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The forward pass with quantization awareness:\n+    /// 1. Compute base layer output (no quantization)\n+    /// 2. Get LoRA layer parameters\n+    /// 3. Simulate quantization: quantize ‚Üí dequantize (if enabled)\n+    /// 4. Compute LoRA output with quantized parameters\n+    /// 5. Sum base + quantized LoRA outputs\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is where quantization-aware training happens!\n+    ///\n+    /// Normal LoRA forward pass:\n+    /// - base_output = base_layer(input)\n+    /// - lora_output = lora_layer(input)  // Uses full-precision weights\n+    /// - return base_output + lora_output\n+    ///\n+    /// QA-LoRA forward pass:\n+    /// - base_output = base_layer(input)\n+    /// - lora_weights_full = get_lora_weights()  // Full precision\n+    /// - lora_weights_quant = dequantize(quantize(lora_weights_full))  // Simulate quantization\n+    /// - lora_output = compute_with_quantized_weights(input, lora_weights_quant)\n+    /// - return base_output + lora_output\n+    ///\n+    /// The key difference: We temporarily quantize and dequantize the LoRA weights,\n+    /// which adds noise. The gradients will learn to work despite this noise!\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        // Forward through base layer (unchanged)\n+        Tensor<T> baseOutput = _baseLayer.Forward(input);\n+\n+        // Forward through LoRA layer with optional quantization simulation\n+        Tensor<T> loraOutput;\n+\n+        if (_quantizationEnabled)\n+        {\n+            // Simulate quantization on LoRA parameters\n+            Vector<T> originalParams = _loraLayer.GetParameters();\n+            Vector<T> quantizedParams = QuantizeAndDequantize(originalParams);\n+\n+            // Temporarily set quantized parameters\n+            _loraLayer.SetParameters(quantizedParams);\n+\n+            // Forward with quantized parameters\n+            loraOutput = _loraLayer.Forward(input);\n+\n+            // Restore original parameters (important for gradient computation)\n+            _loraLayer.SetParameters(originalParams);\n+        }\n+        else\n+        {\n+            // Standard LoRA forward (no quantization simulation)\n+            loraOutput = _loraLayer.Forward(input);\n+        }\n+\n+        // Sum the outputs\n+        Tensor<T> result = new Tensor<T>(baseOutput.Shape);\n+        for (int i = 0; i < baseOutput.Length; i++)\n+        {\n+            result[i] = NumOps.Add(baseOutput[i], loraOutput[i]);\n+        }\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Performs the backward pass through both layers, accounting for quantization in gradients.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient flowing back from the next layer.</param>\n+    /// <returns>Gradient to pass to the previous layer.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The backward pass uses the Straight-Through Estimator (STE) for quantization:\n+    /// - Forward: y = quantize(x)\n+    /// - Backward: ‚àÇL/‚àÇx = ‚àÇL/‚àÇy (gradient passes through unchanged)\n+    ///\n+    /// This allows gradients to flow to the full-precision weights despite quantization.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is the tricky part of quantization-aware training!\n+    ///\n+    /// The Problem:\n+    /// - Quantization is a discontinuous operation (rounding)\n+    /// - Discontinuous operations have zero or undefined gradients\n+    /// - If gradients can't flow, we can't update weights, so training fails\n+    ///\n+    /// The Solution (Straight-Through Estimator):\n+    /// - Pretend quantization is the identity function during backprop\n+    /// - Forward: actually quantize (add noise)\n+    /// - Backward: pretend we didn't quantize (gradient flows through)\n+    /// - This is mathematically \"wrong\" but works well in practice!\n+    ///\n+    /// Why it works:\n+    /// - The forward pass sees quantized values (learns to compensate)\n+    /// - The backward pass updates full-precision weights (maintains precision)\n+    /// - The network learns weights that work well when quantized\n+    ///\n+    /// Example:\n+    /// Forward: weight = 0.7234 ‚Üí quantize ‚Üí 0.7333 (closest 4-bit value)\n+    /// Backward: gradient flows as if 0.7234 ‚Üí 0.7234 (identity)\n+    /// Update: 0.7234 - learning_rate * gradient (updates full-precision weight)\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        // The Straight-Through Estimator (STE) means we compute gradients\n+        // as if quantization was the identity function.\n+        // The base implementation handles this correctly because:\n+        // 1. We restored original (full-precision) parameters after Forward\n+        // 2. Backward computes gradients w.r.t. those full-precision parameters\n+        // 3. Gradient flow is not blocked by quantization\n+\n+        // Standard LoRA backward pass\n+        Tensor<T> loraInputGrad = _loraLayer.Backward(outputGradient);\n+        Tensor<T> baseInputGrad = _baseLayer.Backward(outputGradient);\n+\n+        // Sum input gradients\n+        Tensor<T> inputGrad = new Tensor<T>(loraInputGrad.Shape);\n+        for (int i = 0; i < loraInputGrad.Length; i++)\n+        {\n+            inputGrad[i] = NumOps.Add(loraInputGrad[i], baseInputGrad[i]);\n+        }\n+\n+        // Update parameter gradients vector\n+        UpdateParameterGradientsFromLayers();\n+\n+        return inputGrad;\n+    }\n+\n+    /// <summary>\n+    /// Simulates quantization and dequantization using group-wise scaling.\n+    /// </summary>\n+    /// <param name=\"parameters\">Full-precision parameters to quantize.</param>\n+    /// <returns>Parameters after quantize‚Üídequantize cycle (simulating quantization noise).</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Group-wise quantization process:\n+    /// 1. Divide parameters into groups of size GroupSize\n+    /// 2. For each group:\n+    ///    a. Find the maximum absolute value in the group\n+    ///    b. Compute scale = max_abs / (2^bits - 1)\n+    ///    c. Quantize: int_value = round(parameter / scale)\n+    ///    d. Clamp to range [0, 2^bits - 1]\n+    ///    e. Dequantize: parameter = int_value * scale\n+    /// 3. Concatenate all groups back together\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is the core of quantization simulation!\n+    ///\n+    /// Step-by-step example with 4-bit quantization, group size 4:\n+    ///\n+    /// Input: [0.8, 0.6, -0.4, 0.2, 0.9, -0.7, 0.3, -0.5]\n+    ///\n+    /// Group 1: [0.8, 0.6, -0.4, 0.2]\n+    ///   - Max absolute value: 0.8\n+    ///   - Range for 4-bit: 0 to 15 (2^4 - 1 = 15)\n+    ///   - Scale: 0.8 / 15 = 0.0533\n+    ///   - Quantize: [15, 11, -8, 4] (divided by scale, rounded)\n+    ///   - Clamp to [0, 15]: [15, 11, 0, 4] (negative values clamped)\n+    ///   - Dequantize: [0.8, 0.5867, 0.0, 0.2133] (multiply by scale)\n+    ///   - Information lost: -0.4 became 0.0, 0.6 became 0.5867\n+    ///\n+    /// Group 2: [0.9, -0.7, 0.3, -0.5]\n+    ///   - Max absolute value: 0.9\n+    ///   - Scale: 0.9 / 15 = 0.06\n+    ///   - Similar process...\n+    ///\n+    /// The network learns to work with these quantized values during training,\n+    /// so when we actually deploy with 4-bit weights, accuracy is maintained!\n+    /// </para>\n+    /// </remarks>\n+    private Vector<T> QuantizeAndDequantize(Vector<T> parameters)\n+    {\n+        int numParams = parameters.Length;\n+        Vector<T> quantized = new Vector<T>(numParams);\n+\n+        // Calculate number of groups\n+        int numGroups = (numParams + _groupSize - 1) / _groupSize; // Ceiling division\n+\n+        // Maximum value for quantization (e.g., 15 for 4-bit, 255 for 8-bit)\n+        double maxQuantizedValue = Math.Pow(2.0, _quantizationBits) - 1.0;\n+\n+        // Process each group\n+        for (int g = 0; g < numGroups; g++)\n+        {\n+            int groupStart = g * _groupSize;\n+            int groupEnd = Math.Min(groupStart + _groupSize, numParams);\n+            int groupActualSize = groupEnd - groupStart;\n+\n+            // Find maximum absolute value in this group\n+            T maxAbs = NumOps.Zero;\n+            for (int i = groupStart; i < groupEnd; i++)\n+            {\n+                T absValue = NumOps.Abs(parameters[i]);\n+                if (NumOps.GreaterThan(absValue, maxAbs))\n+                {\n+                    maxAbs = absValue;\n+                }\n+            }\n+\n+            // Compute scale factor for this group\n+            // scale = max_abs / (2^bits - 1)\n+            // If max_abs is zero, use a small epsilon to avoid division by zero\n+            if (NumOps.Equals(maxAbs, NumOps.Zero))\n+            {\n+                maxAbs = NumOps.FromDouble(1e-8);\n+            }\n+\n+            T scale = NumOps.Divide(maxAbs, NumOps.FromDouble(maxQuantizedValue));\n+\n+            // Quantize and dequantize each parameter in the group\n+            for (int i = groupStart; i < groupEnd; i++)\n+            {\n+                // Quantize: int_value = round(param / scale)\n+                T normalized = NumOps.Divide(parameters[i], scale);\n+                double normalizedDouble = Convert.ToDouble(normalized);\n+                double quantizedDouble = Math.Round(normalizedDouble);\n+\n+                // Clamp to valid range [0, maxQuantizedValue] for unsigned\n+                // Or [-maxQuantizedValue/2, maxQuantizedValue/2] for signed\n+                // Using unsigned for simplicity (common in QLoRA)\n+                quantizedDouble = Math.Max(0.0, Math.Min(maxQuantizedValue, quantizedDouble));\n+\n+                // Dequantize: param = int_value * scale\n+                T dequantized = NumOps.Multiply(NumOps.FromDouble(quantizedDouble), scale);\n+                quantized[i] = dequantized;","path":"src/NeuralNetworks/Layers/QALoRAAdapter.cs","commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","original_commit_id":"c11235933bf7c341c8b4a223cf6ac11b757d1480","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Restore signed quantization range.**\n\n`QuantizeAndDequantize` clamps every coefficient to `[0, max]`, so any negative LoRA weight is forced to zero after the simulated quantize‚Üídequantize cycle. That destroys the symmetry assumed by INT4/NF4 schemes and produces a biased forward pass.([docs.nvidia.com](https://docs.nvidia.com/deeplearning/tensorrt/10.13.0/inference-library/work-quantized-types.html?utm_source=openai))\n\nPlease clamp in a signed range instead:\n\n```diff\n-                // Clamp to valid range [0, maxQuantizedValue] for unsigned\n-                // Or [-maxQuantizedValue/2, maxQuantizedValue/2] for signed\n-                // Using unsigned for simplicity (common in QLoRA)\n-                quantizedDouble = Math.Max(0.0, Math.Min(maxQuantizedValue, quantizedDouble));\n+                double signedMax = Math.Pow(2.0, _quantizationBits - 1) - 1.0;\n+                quantizedDouble = Math.Max(-signedMax - 1.0, Math.Min(signedMax, quantizedDouble));\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n                // Quantize: int_value = round(param / scale)\n                T normalized = NumOps.Divide(parameters[i], scale);\n                double normalizedDouble = Convert.ToDouble(normalized);\n                double quantizedDouble = Math.Round(normalizedDouble);\n\n                double signedMax = Math.Pow(2.0, _quantizationBits - 1) - 1.0;\n                quantizedDouble = Math.Max(-signedMax - 1.0, Math.Min(signedMax, quantizedDouble));\n\n                // Dequantize: param = int_value * scale\n                T dequantized = NumOps.Multiply(NumOps.FromDouble(quantizedDouble), scale);\n                quantized[i] = dequantized;\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/Layers/QALoRAAdapter.cs around lines 447 to 459, the code\ncurrently clamps quantizedDouble to an unsigned range [0, maxQuantizedValue]\nwhich zeros negative LoRA weights; change the clamp to a signed range so\nnegative values are preserved by clamping to [-maxQuantizedValue/2,\nmaxQuantizedValue/2]. Update the Math.Max/Math.Min clamp to use the signed\nbounds (ensure you use double arithmetic, e.g. -maxQuantizedValue/2.0 and\nmaxQuantizedValue/2.0) so the subsequent dequantization preserves sign and\nsymmetry for INT4/NF4-style quantization.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:32:38Z","updated_at":"2025-11-02T02:32:41Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106339","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106339"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106339"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106339/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":447,"original_start_line":447,"start_side":"RIGHT","line":459,"original_line":459,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":459,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106353","pull_request_review_id":3407993158,"id":2484106353,"node_id":"PRRC_kwDOKSXUF86UEHRx","diff_hunk":"@@ -0,0 +1,821 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.NeuralNetworks.Layers;\n+\n+/// <summary>\n+/// QLoRA (Quantized LoRA) adapter for parameter-efficient fine-tuning with 4-bit quantized base weights.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// QLoRA extends the LoRA (Low-Rank Adaptation) technique by quantizing the base layer's weights\n+/// to 4-bit precision while keeping the LoRA adapter matrices (A and B) in full precision.\n+/// This achieves dramatic memory savings (typically 4x reduction) while maintaining training quality\n+/// comparable to full 16-bit fine-tuning.\n+/// </para>\n+/// <para>\n+/// <b>Key Features:</b>\n+/// - Base layer weights stored in 4-bit precision (INT4 or NF4)\n+/// - LoRA matrices (A and B) remain in full precision for accurate gradient updates\n+/// - Double quantization for constant quantization parameters (further memory savings)\n+/// - Paged optimizers support for handling memory spikes during training\n+/// - Dequantization happens on-the-fly during forward pass\n+/// </para>\n+/// <para>\n+/// <b>Memory Savings:</b>\n+/// For a typical transformer layer with 1000x1000 weights:\n+/// - Standard 16-bit: 2MB for weights\n+/// - QLoRA 4-bit base: 0.5MB for base weights + full precision LoRA (e.g., 32KB for rank 8)\n+/// - Total savings: ~75% memory reduction on base weights\n+/// </para>\n+/// <para>\n+/// <b>Quantization Types:</b>\n+/// - INT4: Uniform 4-bit integer quantization (-8 to 7)\n+/// - NF4 (4-bit Normal Float): Information-theoretically optimal for normally distributed weights\n+/// </para>\n+/// <para>\n+/// <b>For Beginners:</b> QLoRA is an advanced technique that makes fine-tuning large models\n+/// even more memory-efficient than standard LoRA. Here's how it works:\n+///\n+/// Imagine you have a huge model with millions of parameters:\n+/// - Standard LoRA: Freezes the base model, trains small adapters (huge memory savings)\n+/// - QLoRA: Does the same BUT also compresses the base model to 4-bit (even more savings!)\n+///\n+/// Think of it like storing a high-resolution image:\n+/// - Original model: Full 16-bit floating point (2 bytes per number)\n+/// - QLoRA base: Compressed to 4-bit (0.5 bytes per number)\n+/// - LoRA adapters: Still full precision (for accurate learning)\n+///\n+/// The result: You can fine-tune models 4x larger on the same hardware, or use 4x less GPU memory!\n+///\n+/// <b>When to use QLoRA vs Standard LoRA:</b>\n+/// - Use QLoRA when: GPU memory is very limited, model is huge, inference speed is critical\n+/// - Use Standard LoRA when: Memory is not a constraint, maximum accuracy is needed\n+/// - Both achieve similar quality in practice, QLoRA just uses less memory\n+///\n+/// <b>Trade-offs:</b>\n+/// - Pros: 75% less memory, same performance as 16-bit LoRA, faster inference after merging\n+/// - Cons: Slightly slower forward pass (dequantization overhead), more complex implementation\n+/// </para>\n+/// <para>\n+/// <b>Research Background:</b>\n+/// QLoRA was introduced in \"QLoRA: Efficient Finetuning of Quantized LLMs\" (Dettmers et al., 2023).\n+/// It enables fine-tuning of 65B parameter models on a single 48GB GPU by combining:\n+/// 1. 4-bit NormalFloat (NF4) quantization optimized for normally distributed weights\n+/// 2. Double quantization to reduce memory footprint of quantization constants\n+/// 3. Paged optimizers to handle memory spikes during gradient checkpointing\n+/// </para>\n+/// </remarks>\n+public class QLoRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Specifies the type of 4-bit quantization to use for base layer weights.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This determines how we compress numbers from full precision to 4-bit.\n+    /// Think of it like choosing between different image compression algorithms - each has trade-offs.\n+    /// </para>\n+    /// </remarks>\n+    public enum QuantizationType\n+    {\n+        /// <summary>\n+        /// 4-bit integer quantization with uniform spacing (-8 to 7).\n+        /// </summary>\n+        /// <remarks>\n+        /// Simple linear quantization mapping 16 values uniformly across the range.\n+        /// Fast and straightforward, but not optimal for normally distributed weights.\n+        /// </remarks>\n+        INT4,\n+\n+        /// <summary>\n+        /// 4-bit Normal Float quantization optimized for normally distributed weights.\n+        /// </summary>\n+        /// <remarks>\n+        /// Uses information-theoretically optimal quantization levels for normal distributions.\n+        /// Provides better accuracy for typical neural network weights at the same bit width.\n+        /// This is the recommended and default quantization type for QLoRA.\n+        /// </remarks>\n+        NF4\n+    }\n+\n+    /// <summary>\n+    /// The type of quantization used for base layer weights.\n+    /// </summary>\n+    private readonly QuantizationType _quantizationType;\n+\n+    /// <summary>\n+    /// Whether to use double quantization for quantization constants.\n+    /// </summary>\n+    /// <remarks>\n+    /// Double quantization quantizes the quantization constants themselves (e.g., scale factors)\n+    /// to save additional memory. This provides ~3-5% extra memory savings with negligible quality impact.\n+    /// </remarks>\n+    private readonly bool _useDoubleQuantization;\n+\n+    /// <summary>\n+    /// The block size for quantization (number of values sharing the same quantization parameters).\n+    /// </summary>\n+    /// <remarks>\n+    /// Smaller blocks provide finer-grained quantization (better accuracy, more memory for constants).\n+    /// Larger blocks use less memory for constants but may lose precision.\n+    /// Default: 64 (good balance between accuracy and memory).\n+    /// </remarks>\n+    private readonly int _quantizationBlockSize;\n+\n+    /// <summary>\n+    /// Quantized base layer weights stored as 4-bit values.\n+    /// </summary>\n+    /// <remarks>\n+    /// Stored as packed bytes where each byte contains two 4-bit values.\n+    /// Shape matches the base layer's weight matrix.\n+    /// </remarks>\n+    private byte[]? _quantizedWeights;\n+\n+    /// <summary>\n+    /// Scale factors for dequantization (one per quantization block).\n+    /// </summary>\n+    /// <remarks>\n+    /// These scaling factors are used to map 4-bit quantized values back to full precision.\n+    /// For double quantization, these are themselves quantized to save memory.\n+    /// </remarks>\n+    private T[]? _quantizationScales;\n+\n+    /// <summary>\n+    /// Zero points for asymmetric quantization (one per quantization block).\n+    /// </summary>\n+    /// <remarks>\n+    /// Used for asymmetric quantization where the quantization range doesn't center on zero.\n+    /// Optional - set to null for symmetric quantization.\n+    /// </remarks>\n+    private T[]? _quantizationZeroPoints;\n+\n+    /// <summary>\n+    /// Cached dequantized weights for forward pass.\n+    /// </summary>\n+    /// <remarks>\n+    /// Weights are dequantized at the start of forward pass and cached to avoid repeated dequantization.\n+    /// Cleared after backward pass to save memory.\n+    /// </remarks>\n+    private Matrix<T>? _dequantizedWeights;\n+\n+    /// <summary>\n+    /// NF4 quantization lookup table (16 values optimized for normal distribution).\n+    /// </summary>\n+    /// <remarks>\n+    /// These values are derived from optimal quantization for a standard normal distribution.\n+    /// They are NOT evenly spaced - more values near zero where probability mass is concentrated.\n+    /// </remarks>\n+    private static readonly double[] _nf4Table = new double[]\n+    {\n+        -1.0,\n+        -0.6961928009986877,\n+        -0.5250730514526367,\n+        -0.39491748809814453,\n+        -0.28444138169288635,\n+        -0.18477343022823334,\n+        -0.09105003625154495,\n+        0.0,\n+        0.07958029955625534,\n+        0.16093020141124725,\n+        0.24611230194568634,\n+        0.33791524171829224,\n+        0.44070982933044434,\n+        0.5626170039176941,\n+        0.7229568362236023,\n+        1.0\n+    };\n+\n+    /// <summary>\n+    /// Gets the quantization type used for base layer weights.\n+    /// </summary>\n+    public QuantizationType Quantization => _quantizationType;\n+\n+    /// <summary>\n+    /// Gets whether double quantization is enabled.\n+    /// </summary>\n+    public bool UsesDoubleQuantization => _useDoubleQuantization;\n+\n+    /// <summary>\n+    /// Gets the quantization block size.\n+    /// </summary>\n+    public int BlockSize => _quantizationBlockSize;\n+\n+    /// <summary>\n+    /// Initializes a new QLoRA adapter wrapping an existing Dense or FullyConnected layer.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The Dense or FullyConnected layer to adapt with QLoRA.</param>\n+    /// <param name=\"rank\">The rank of the LoRA decomposition.</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"quantizationType\">The type of 4-bit quantization to use (default: NF4).</param>\n+    /// <param name=\"useDoubleQuantization\">Whether to use double quantization for constants (default: true).</param>\n+    /// <param name=\"quantizationBlockSize\">The block size for quantization (default: 64).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training (default: true, recommended for QLoRA).</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when the base layer doesn't have 1D input/output shapes or when block size is invalid.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// The constructor quantizes the base layer's weights immediately to save memory.\n+    /// LoRA matrices are initialized normally and remain in full precision.\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This creates a QLoRA adapter that wraps your existing layer.\n+    ///\n+    /// Parameters explained:\n+    /// - baseLayer: The layer you want to compress and adapt (e.g., a Dense layer)\n+    /// - rank: How many parameters for the LoRA adapter (lower = more efficient)\n+    /// - alpha: How strong the LoRA corrections are\n+    /// - quantizationType: NF4 (recommended) or INT4 (simpler but less accurate)\n+    /// - useDoubleQuantization: true (recommended) saves extra 3-5% memory\n+    /// - quantizationBlockSize: 64 (recommended) balances accuracy and memory\n+    /// - freezeBaseLayer: true (recommended) - only train the LoRA adapter, not the base weights\n+    ///\n+    /// After construction, the base layer's weights are immediately compressed to 4-bit,\n+    /// freeing up 75% of the memory they were using!\n+    /// </para>\n+    /// </remarks>\n+    public QLoRAAdapter(\n+        ILayer<T> baseLayer,\n+        int rank,\n+        double alpha = -1,\n+        QuantizationType quantizationType = QuantizationType.NF4,\n+        bool useDoubleQuantization = true,\n+        int quantizationBlockSize = 64,\n+        bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        // Validate base layer has single-dimensional input/output (specific to Dense layers)\n+        if (baseLayer.GetInputShape().Length != 1 || baseLayer.GetOutputShape().Length != 1)\n+        {\n+            throw new ArgumentException(\"QLoRAAdapter only supports layers with 1D input/output shapes (Dense/FullyConnected layers)\", nameof(baseLayer));\n+        }\n+\n+        if (quantizationBlockSize <= 0)\n+        {\n+            throw new ArgumentException(\"Quantization block size must be positive\", nameof(quantizationBlockSize));\n+        }\n+\n+        _quantizationType = quantizationType;\n+        _useDoubleQuantization = useDoubleQuantization;\n+        _quantizationBlockSize = quantizationBlockSize;\n+\n+        // Quantize base layer weights immediately to save memory\n+        QuantizeBaseLayerWeights();\n+    }\n+\n+    /// <summary>\n+    /// Quantizes the base layer's weights to 4-bit precision.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This method extracts the weight matrix from the base layer and quantizes it\n+    /// using the specified quantization type. The quantized weights and quantization\n+    /// parameters (scales, zero points) are stored for later dequantization.\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This is where the magic happens - we compress the weights\n+    /// from full precision (2 bytes per value) to 4-bit (0.5 bytes per value).\n+    ///\n+    /// The process:\n+    /// 1. Get the full-precision weights from the base layer\n+    /// 2. Split them into blocks (e.g., 64 values per block)\n+    /// 3. For each block, find the best way to map values to 4-bit\n+    /// 4. Store the compressed values and the mapping parameters\n+    /// </para>\n+    /// </remarks>\n+    private void QuantizeBaseLayerWeights()\n+    {\n+        // Get base layer parameters\n+        Vector<T> baseParams = _baseLayer.GetParameters();\n+\n+        // For Dense layers, parameters are stored as [weights..., biases...]\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        int weightCount = inputSize * outputSize;\n+\n+        // Extract weights (skip biases)\n+        T[] weights = new T[weightCount];\n+        for (int i = 0; i < weightCount; i++)\n+        {\n+            weights[i] = baseParams[i];\n+        }\n+\n+        // Quantize weights in blocks\n+        int numBlocks = (weightCount + _quantizationBlockSize - 1) / _quantizationBlockSize;\n+        _quantizedWeights = new byte[(weightCount + 1) / 2]; // 2 values per byte\n+        _quantizationScales = new T[numBlocks];\n+        _quantizationZeroPoints = new T[numBlocks];\n+\n+        for (int blockIdx = 0; blockIdx < numBlocks; blockIdx++)\n+        {\n+            int blockStart = blockIdx * _quantizationBlockSize;\n+            int blockEnd = Math.Min(blockStart + _quantizationBlockSize, weightCount);\n+            int blockLength = blockEnd - blockStart;\n+\n+            // Find min/max for this block\n+            T minVal = weights[blockStart];\n+            T maxVal = weights[blockStart];\n+            for (int i = blockStart + 1; i < blockEnd; i++)\n+            {\n+                if (NumOps.LessThan(weights[i], minVal))\n+                    minVal = weights[i];\n+                if (NumOps.GreaterThan(weights[i], maxVal))\n+                    maxVal = weights[i];\n+            }\n+\n+            // Compute scale and zero point\n+            T range = NumOps.Subtract(maxVal, minVal);\n+            T scale = NumOps.Divide(range, NumOps.FromDouble(15.0)); // 4-bit has 16 levels (0-15)\n+            T zeroPoint = minVal;\n+\n+            _quantizationScales[blockIdx] = scale;\n+            _quantizationZeroPoints[blockIdx] = zeroPoint;\n+\n+            // Quantize values in this block\n+            for (int i = blockStart; i < blockEnd; i++)\n+            {\n+                byte quantizedValue = QuantizeValue(weights[i], scale, zeroPoint);\n+\n+                // Pack two 4-bit values per byte\n+                int byteIdx = i / 2;\n+                if (i % 2 == 0)\n+                {\n+                    // Lower 4 bits\n+                    _quantizedWeights[byteIdx] = (byte)(quantizedValue & 0x0F);\n+                }\n+                else\n+                {\n+                    // Upper 4 bits\n+                    _quantizedWeights[byteIdx] |= (byte)((quantizedValue & 0x0F) << 4);\n+                }\n+            }\n+        }\n+\n+        // If using double quantization, quantize the scales themselves\n+        if (_useDoubleQuantization)\n+        {\n+            DoubleQuantizeScales();\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Quantizes a single value to 4-bit using the specified scale and zero point.\n+    /// </summary>\n+    /// <param name=\"value\">The value to quantize.</param>\n+    /// <param name=\"scale\">The quantization scale factor.</param>\n+    /// <param name=\"zeroPoint\">The quantization zero point.</param>\n+    /// <returns>A 4-bit quantized value (0-15).</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// <b>For Beginners:</b> This converts one full-precision number into a 4-bit value.\n+    /// It's like mapping a continuous color spectrum to just 16 colors - you lose some\n+    /// precision but save a lot of space.\n+    /// </para>\n+    /// </remarks>\n+    private byte QuantizeValue(T value, T scale, T zeroPoint)\n+    {\n+        if (_quantizationType == QuantizationType.NF4)\n+        {\n+            return QuantizeNF4(value, scale, zeroPoint);\n+        }\n+        else // INT4\n+        {\n+            return QuantizeINT4(value, scale, zeroPoint);\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Quantizes a value using 4-bit integer quantization.\n+    /// </summary>\n+    /// <param name=\"value\">The value to quantize.</param>\n+    /// <param name=\"scale\">The quantization scale factor.</param>\n+    /// <param name=\"zeroPoint\">The quantization zero point.</param>\n+    /// <returns>A 4-bit quantized value (0-15).</returns>\n+    private byte QuantizeINT4(T value, T scale, T zeroPoint)\n+    {\n+        // Normalize to range [0, 1]\n+        T normalized = NumOps.Divide(NumOps.Subtract(value, zeroPoint), scale);\n+\n+        // Scale to [0, 15] and round\n+        double scaledValue = Convert.ToDouble(normalized);\n+        int quantized = (int)Math.Round(scaledValue);\n+\n+        // Clamp to [0, 15]\n+        quantized = Math.Max(0, Math.Min(15, quantized));\n+\n+        return (byte)quantized;\n+    }","path":"src/NeuralNetworks/Layers/QLoRAAdapter.cs","commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","original_commit_id":"c11235933bf7c341c8b4a223cf6ac11b757d1480","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Guard against zero-range quantization blocks.**  \n`scale = (maxVal - minVal) / 15` becomes zero whenever a block contains a constant weight. The very next call to `QuantizeValue` divides by `scale`, so you emit NaNs (or throw in `NumOps.Divide`). Constant blocks happen frequently (e.g., bias-only columns or pruned weights). Protect against this before quantizing:\n\n```diff\n-            T range = NumOps.Subtract(maxVal, minVal);\n-            T scale = NumOps.Divide(range, NumOps.FromDouble(15.0));\n+            T range = NumOps.Subtract(maxVal, minVal);\n+            if (!NumOps.GreaterThan(range, NumOps.FromDouble(1e-12)))\n+            {\n+                range = NumOps.FromDouble(1e-12);\n+            }\n+            T scale = NumOps.Divide(range, NumOps.FromDouble(15.0));\n```\n\nand ensure `QuantizeValue` / `DequantizeValue` handles the fallback consistently (e.g., return zero offset). Without this, any flat weight block blows up the quantization pipeline.\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n            // Compute scale and zero point\n            T range = NumOps.Subtract(maxVal, minVal);\n            if (!NumOps.GreaterThan(range, NumOps.FromDouble(1e-12)))\n            {\n                range = NumOps.FromDouble(1e-12);\n            }\n            T scale = NumOps.Divide(range, NumOps.FromDouble(15.0)); // 4-bit has 16 levels (0-15)\n            T zeroPoint = minVal;\n\n            _quantizationScales[blockIdx] = scale;\n            _quantizationZeroPoints[blockIdx] = zeroPoint;\n\n            // Quantize values in this block\n            for (int i = blockStart; i < blockEnd; i++)\n            {\n                byte quantizedValue = QuantizeValue(weights[i], scale, zeroPoint);\n\n                // Pack two 4-bit values per byte\n                int byteIdx = i / 2;\n                if (i % 2 == 0)\n                {\n                    // Lower 4 bits\n                    _quantizedWeights[byteIdx] = (byte)(quantizedValue & 0x0F);\n                }\n                else\n                {\n                    // Upper 4 bits\n                    _quantizedWeights[byteIdx] |= (byte)((quantizedValue & 0x0F) << 4);\n                }\n            }\n        }\n\n        // If using double quantization, quantize the scales themselves\n        if (_useDoubleQuantization)\n        {\n            DoubleQuantizeScales();\n        }\n    }\n\n    /// <summary>\n    /// Quantizes a single value to 4-bit using the specified scale and zero point.\n    /// </summary>\n    /// <param name=\"value\">The value to quantize.</param>\n    /// <param name=\"scale\">The quantization scale factor.</param>\n    /// <param name=\"zeroPoint\">The quantization zero point.</param>\n    /// <returns>A 4-bit quantized value (0-15).</returns>\n    /// <remarks>\n    /// <para>\n    /// <b>For Beginners:</b> This converts one full-precision number into a 4-bit value.\n    /// It's like mapping a continuous color spectrum to just 16 colors - you lose some\n    /// precision but save a lot of space.\n    /// </para>\n    /// </remarks>\n    private byte QuantizeValue(T value, T scale, T zeroPoint)\n    {\n        if (_quantizationType == QuantizationType.NF4)\n        {\n            return QuantizeNF4(value, scale, zeroPoint);\n        }\n        else // INT4\n        {\n            return QuantizeINT4(value, scale, zeroPoint);\n        }\n    }\n\n    /// <summary>\n    /// Quantizes a value using 4-bit integer quantization.\n    /// </summary>\n    /// <param name=\"value\">The value to quantize.</param>\n    /// <param name=\"scale\">The quantization scale factor.</param>\n    /// <param name=\"zeroPoint\">The quantization zero point.</param>\n    /// <returns>A 4-bit quantized value (0-15).</returns>\n    private byte QuantizeINT4(T value, T scale, T zeroPoint)\n    {\n        // Normalize to range [0, 1]\n        T normalized = NumOps.Divide(NumOps.Subtract(value, zeroPoint), scale);\n\n        // Scale to [0, 15] and round\n        double scaledValue = Convert.ToDouble(normalized);\n        int quantized = (int)Math.Round(scaledValue);\n\n        // Clamp to [0, 15]\n        quantized = Math.Max(0, Math.Min(15, quantized));\n\n        return (byte)quantized;\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/Layers/QLoRAAdapter.cs around lines 325-406, the computed\nscale = (maxVal - minVal)/15 can be zero for constant blocks which leads to\ndivide-by-zero/NaN in QuantizeValue; detect zero (or near-zero) range and\nreplace the scale with a safe fallback (e.g., a non-zero epsilon or 1.0 in\nnumeric type) and keep zeroPoint = minVal so quantization maps the constant\nblock to a single 4-bit value; also update QuantizeValue and the corresponding\nDequantizeValue to check for a zero/placeholder scale and return the consistent\nfallback quantized value (e.g., the offset/zero mapping) instead of performing a\ndivide, ensuring constant blocks are handled deterministically.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:32:38Z","updated_at":"2025-11-02T02:32:41Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106353","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106353"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106353"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106353/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":325,"original_start_line":325,"start_side":"RIGHT","line":406,"original_line":406,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":406,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106362","pull_request_review_id":3407993158,"id":2484106362,"node_id":"PRRC_kwDOKSXUF86UEHR6","diff_hunk":"@@ -0,0 +1,627 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.NeuralNetworks.Layers;\n+\n+/// <summary>\n+/// Restart LoRA (ReLoRA) adapter that periodically merges and restarts LoRA training for continual learning.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// ReLoRA addresses the challenge of continual learning and long-running training by periodically:\n+/// 1. Merging the LoRA weights into the base layer (accumulating the adaptation)\n+/// 2. Resetting the LoRA matrices to restart training fresh\n+/// 3. Continuing training with a clean slate while preserving previous learning\n+/// </para>\n+/// <para>\n+/// This approach:\n+/// - Prevents catastrophic forgetting by accumulating adaptations into the base layer\n+/// - Allows continuous adaptation to new data without losing old knowledge\n+/// - Maintains parameter efficiency by resetting LoRA to small matrices\n+/// - Enables training on continuously evolving data streams\n+/// </para>\n+/// <para><b>For Beginners:</b> ReLoRA is like having multiple rounds of LoRA training.\n+///\n+/// Imagine you're fine-tuning a model on data that keeps changing:\n+/// - Round 1: Train LoRA on dataset A for 1000 steps\n+/// - Merge: Add the learned changes into the base model\n+/// - Restart: Reset LoRA matrices and train on dataset B for 1000 steps\n+/// - Merge: Add these new changes to the (already updated) base model\n+/// - Repeat...\n+///\n+/// Benefits:\n+/// - Continual learning: Can keep learning from new data indefinitely\n+/// - No catastrophic forgetting: Old knowledge is preserved in the base layer\n+/// - Parameter efficient: LoRA matrices stay small even after many restarts\n+/// - Flexible: Can adapt to distribution shifts and new tasks\n+///\n+/// How it works:\n+/// 1. Train normally with LoRA for N steps (restart interval)\n+/// 2. At step N: Merge LoRA weights ‚Üí AccumulatedWeight += LoRA\n+/// 3. Reset LoRA matrices to zero (fresh start)\n+/// 4. Continue training for another N steps\n+/// 5. Repeat indefinitely\n+///\n+/// Use cases:\n+/// - Training on streaming data (news articles, user behavior, etc.)\n+/// - Adapting to distribution shifts over time\n+/// - Long-running training sessions that need checkpoints\n+/// - Multi-task learning with periodic task switches\n+///\n+/// Reference: \"ReLoRA: High-Rank Training Through Low-Rank Updates\" (2023)\n+/// https://arxiv.org/abs/2307.05695\n+/// </para>\n+/// </remarks>\n+public class ReLoRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Number of training steps between restart operations.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// The restart interval determines how frequently the LoRA weights are merged and reset.\n+    /// Typical values:\n+    /// - Short interval (100-500): Frequent restarts, better for rapidly changing data\n+    /// - Medium interval (1000-2000): Balance between stability and adaptation\n+    /// - Long interval (5000+): Fewer restarts, more thorough learning per cycle\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is how many training steps to run before merging and restarting.\n+    /// Think of it as the length of each training \"session\" before taking a checkpoint.\n+    /// </para>\n+    /// </remarks>\n+    private readonly int _restartInterval;\n+\n+    /// <summary>\n+    /// Current training step counter.\n+    /// </summary>\n+    /// <remarks>\n+    /// This counts up from 0 to restartInterval, then resets to 0 after each restart.\n+    /// </remarks>\n+    private int _currentStep;\n+\n+    /// <summary>\n+    /// Accumulated weight changes from all previous restart cycles.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This matrix accumulates all LoRA adaptations across restart cycles:\n+    /// AccumulatedWeight = sum of all (A * B * scaling) across all cycles.\n+    /// It represents the total learned adaptation that gets added to the base layer.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is like a running total of all the changes made across\n+    /// all restart cycles. Each time we restart, we add the current LoRA changes to this total.\n+    /// This is how we prevent forgetting - all previous learning is saved here.\n+    /// </para>\n+    /// </remarks>\n+    private Matrix<T> _accumulatedWeight;\n+\n+    /// <summary>\n+    /// Total number of restarts that have occurred.\n+    /// </summary>\n+    private int _restartCount;\n+\n+    /// <summary>\n+    /// Whether to use warmup after each restart.\n+    /// </summary>\n+    /// <remarks>\n+    /// When true, the first few steps after restart use a reduced learning rate to stabilize training.\n+    /// </remarks>\n+    private readonly bool _useWarmup;\n+\n+    /// <summary>\n+    /// Number of warmup steps to use after each restart.\n+    /// </summary>\n+    private readonly int _warmupSteps;\n+\n+    /// <summary>\n+    /// Whether to freeze the base layer during training (typical for LoRA).\n+    /// </summary>\n+    private readonly bool _freezeBase;\n+\n+    /// <summary>\n+    /// Gets the number of training steps between restarts.\n+    /// </summary>\n+    public int RestartInterval => _restartInterval;\n+\n+    /// <summary>\n+    /// Gets the current step within the current restart cycle.\n+    /// </summary>\n+    public int CurrentStep => _currentStep;\n+\n+    /// <summary>\n+    /// Gets the total number of restarts that have occurred.\n+    /// </summary>\n+    public int RestartCount => _restartCount;\n+\n+    /// <summary>\n+    /// Gets a copy of the accumulated weight matrix.\n+    /// </summary>\n+    public Matrix<T> GetAccumulatedWeight() => _accumulatedWeight.Clone();\n+\n+    /// <summary>\n+    /// Initializes a new ReLoRA adapter with restart-based continual learning.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with ReLoRA.</param>\n+    /// <param name=\"rank\">The rank of the LoRA decomposition.</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"restartInterval\">Number of steps between restart operations (default: 1000).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training (default: true).</param>\n+    /// <param name=\"useWarmup\">Whether to use warmup after restarts (default: true).</param>\n+    /// <param name=\"warmupSteps\">Number of warmup steps after restart (default: 10).</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when restartInterval is invalid.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This creates a ReLoRA adapter for continual learning.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to adapt continuously\n+    /// - rank: Size of the LoRA matrices (lower = more efficient)\n+    /// - alpha: Strength of the LoRA adaptation\n+    /// - restartInterval: How often to merge and restart (in training steps)\n+    /// - freezeBaseLayer: Lock the base layer weights (typical for LoRA)\n+    /// - useWarmup: Use reduced learning rate after restarts (helps stability)\n+    /// - warmupSteps: How many steps to warm up for\n+    ///\n+    /// The adapter will automatically handle merging and restarting at the specified interval.\n+    /// You just train normally, and it takes care of the restart logic.\n+    /// </para>\n+    /// </remarks>\n+    public ReLoRAAdapter(\n+        ILayer<T> baseLayer,\n+        int rank,\n+        double alpha = -1,\n+        int restartInterval = 1000,\n+        bool freezeBaseLayer = true,\n+        bool useWarmup = true,\n+        int warmupSteps = 10)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        if (restartInterval <= 0)\n+        {\n+            throw new ArgumentException(\"Restart interval must be positive\", nameof(restartInterval));\n+        }\n+\n+        if (warmupSteps < 0)\n+        {\n+            throw new ArgumentException(\"Warmup steps cannot be negative\", nameof(warmupSteps));\n+        }\n+\n+        _restartInterval = restartInterval;\n+        _currentStep = 0;\n+        _restartCount = 0;\n+        _freezeBase = freezeBaseLayer;\n+        _useWarmup = useWarmup;\n+        _warmupSteps = warmupSteps;\n+\n+        // Initialize accumulated weight matrix to zero\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        _accumulatedWeight = new Matrix<T>(outputSize, inputSize);\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                _accumulatedWeight[i, j] = NumOps.Zero;\n+            }\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Checks if a restart should be performed based on the current step count.\n+    /// </summary>\n+    /// <returns>True if current step has reached the restart interval.</returns>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This checks if it's time for a restart.\n+    /// Returns true when we've completed a full training cycle (reached the interval).\n+    /// </para>\n+    /// </remarks>\n+    public bool ShouldRestart()\n+    {\n+        return _currentStep >= _restartInterval;\n+    }\n+\n+    /// <summary>\n+    /// Performs the restart operation: merges current LoRA weights and reinitializes.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// The restart process:\n+    /// 1. Merge current LoRA weights: W_accumulated += W_A * W_B * scaling\n+    /// 2. Reinitialize LoRA matrices: A gets new random values, B reset to zero\n+    /// 3. Reset step counter to 0\n+    /// 4. Increment restart count\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This performs the \"checkpoint and restart\" operation.\n+    ///\n+    /// Steps:\n+    /// 1. Save progress: Add current LoRA changes to the accumulated total\n+    /// 2. Fresh start: Reset LoRA matrices (A gets new random values, B starts at zero)\n+    /// 3. Reset counter: Start counting steps from 0 again\n+    ///\n+    /// After this, training continues normally for another cycle.\n+    /// The accumulated changes are preserved and will be included in the final output.\n+    /// </para>\n+    /// </remarks>\n+    public void RestartLoRA()\n+    {\n+        // Get the current LoRA weight contribution\n+        Matrix<T> loraWeights = _loraLayer.MergeWeights();\n+\n+        // Accumulate the LoRA weights\n+        for (int i = 0; i < _accumulatedWeight.Rows; i++)\n+        {\n+            for (int j = 0; j < _accumulatedWeight.Columns; j++)\n+            {\n+                _accumulatedWeight[i, j] = NumOps.Add(_accumulatedWeight[i, j], loraWeights[i, j]);\n+            }\n+        }\n+\n+        // Reinitialize LoRA matrices\n+        Matrix<T> matrixA = _loraLayer.GetMatrixA();\n+        Matrix<T> matrixB = _loraLayer.GetMatrixB();\n+\n+        // Reinitialize A with random values (same as initial LoRA initialization)\n+        T stddev = NumOps.Sqrt(NumOps.Divide(NumOps.One, NumOps.FromDouble(_loraLayer.Rank)));\n+        for (int i = 0; i < matrixA.Rows; i++)\n+        {\n+            for (int j = 0; j < matrixA.Columns; j++)\n+            {\n+                // Box-Muller transform for Gaussian random numbers\n+                double u1 = Random.NextDouble();\n+                double u2 = Random.NextDouble();\n+                double randStdNormal = Math.Sqrt(-2.0 * Math.Log(u1)) * Math.Sin(2.0 * Math.PI * u2);\n+                matrixA[i, j] = NumOps.Multiply(NumOps.FromDouble(randStdNormal), stddev);\n+            }\n+        }","path":"src/NeuralNetworks/Layers/ReLoRAAdapter.cs","commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","original_commit_id":"c11235933bf7c341c8b4a223cf6ac11b757d1480","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**`Random.NextDouble()` usage won‚Äôt compile.**\n\n`Random.NextDouble()` is an instance method; there is no static `NextDouble` on `System.Random`. This block won‚Äôt build unless you create or reuse a `Random` instance (e.g., `Random.Shared.NextDouble()` on .NET 6+, or keep a private `Random` field). Please fix the random sampling logic so the project compiles.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/Layers/ReLoRAAdapter.cs around lines 270 to 275, the code\ncalls Random.NextDouble() which is not a static method; replace it with a proper\nRandom instance (for example use Random.Shared.NextDouble() on .NET 6+ or add a\nprivate readonly Random _rng = new Random() for older runtimes) and use that\ninstance when sampling u1 and u2 so the Gaussian sampling compiles; ensure the\nRNG is reused (or thread-safe) and update any references in this loop to use the\nchosen instance.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:32:38Z","updated_at":"2025-11-02T02:32:41Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106362","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106362"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106362"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106362/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":270,"original_start_line":270,"start_side":"RIGHT","line":275,"original_line":275,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":275,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106367","pull_request_review_id":3407993158,"id":2484106367,"node_id":"PRRC_kwDOKSXUF86UEHR_","diff_hunk":"@@ -0,0 +1,827 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.NeuralNetworks.Layers;\n+\n+/// <summary>\n+/// RoSA (Robust Adaptation) adapter for parameter-efficient fine-tuning with improved robustness to distribution shifts.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// RoSA (Robust Adaptation) extends standard LoRA by combining two complementary components:\n+/// 1. Low-rank component (standard LoRA): Captures common, structured patterns in adaptations\n+/// 2. Sparse component: Captures specific, rare, or outlier patterns that low-rank cannot represent\n+/// </para>\n+/// <para>\n+/// <b>Mathematical Formulation:</b>\n+/// Given input x and pre-trained weights W, RoSA computes:\n+/// - Low-rank component: L = (alpha/rank) * B * A * x\n+/// - Sparse component: S = W_sparse * x (where W_sparse is highly sparse)\n+/// - Final output: y = W*x + L + S\n+///\n+/// The sparse component is maintained through magnitude-based pruning, keeping only the\n+/// most significant weights and zeroing out the rest. This creates a sparse matrix that\n+/// captures specific patterns while remaining parameter-efficient.\n+/// </para>\n+/// <para>\n+/// <b>Research Context:</b>\n+/// RoSA was introduced in January 2024 as a robust alternative to standard LoRA.\n+/// The key insight is that low-rank approximations work well for common patterns but\n+/// struggle with distribution shifts and rare patterns. By adding a sparse component,\n+/// RoSA can capture outliers and domain-specific patterns without significantly\n+/// increasing parameter count.\n+///\n+/// In experiments on domain adaptation tasks, RoSA showed:\n+/// - Better generalization to new domains (+5-10% over standard LoRA)\n+/// - More robust to distribution shifts\n+/// - Ability to capture both global patterns (low-rank) and local exceptions (sparse)\n+/// - Only modest increase in parameters (typically 5-15% more than pure LoRA)\n+/// </para>\n+/// <para>\n+/// <b>For Beginners:</b> RoSA is like LoRA with a safety net for unusual cases.\n+///\n+/// Think of it this way:\n+/// - Low-rank LoRA is like learning general rules (\"most images of cats have pointed ears\")\n+/// - Sparse component is like remembering specific exceptions (\"this one cat breed has round ears\")\n+/// - Together they make a robust model that handles both common and rare cases\n+///\n+/// Why RoSA is more robust:\n+/// - Low-rank component: Efficient for common patterns across domains\n+/// - Sparse component: Handles outliers and domain-specific quirks\n+/// - Result: Better performance when test data differs from training data\n+///\n+/// When to use RoSA over standard LoRA:\n+/// - When you expect distribution shifts (train on news, test on social media)\n+/// - When your data has outliers or rare patterns that matter\n+/// - When you need robustness more than absolute parameter efficiency\n+/// - When adapting to multiple related but distinct domains\n+///\n+/// Trade-offs vs standard LoRA:\n+/// + More robust to distribution shifts\n+/// + Better handles rare patterns\n+/// + More flexible adaptation\n+/// - Slightly more parameters (sparse component adds ~5-15%)\n+/// - Slightly more computation (extra sparse matrix multiply)\n+/// - Requires tuning sparsity ratio\n+/// </para>\n+/// <para>\n+/// <b>Reference:</b>\n+/// \"RoSA: Robust Adaptation through Sparse Regularization\"\n+/// January 2024\n+/// </para>\n+/// </remarks>\n+public class RoSAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Sparse weight matrix that captures specific/rare patterns.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This matrix has the same dimensions as the base layer's weights but is highly sparse\n+    /// (typically 90-99% zeros). It's maintained through magnitude-based pruning during training.\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This is the \"exception handler\" of RoSA.\n+    /// Most of its values are zero, but the few non-zero values capture specific patterns\n+    /// that the low-rank component can't represent efficiently.\n+    /// </para>\n+    /// </remarks>\n+    private Matrix<T> _sparseWeights;\n+\n+    /// <summary>\n+    /// Gradients for the sparse weight component, computed during backpropagation.\n+    /// </summary>\n+    private Matrix<T>? _sparseGradients;\n+\n+    /// <summary>\n+    /// Threshold for magnitude-based pruning of sparse weights.\n+    /// Weights with magnitude below this threshold are set to zero.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This threshold controls the sparsity of the sparse component. Lower values\n+    /// result in more non-zero weights (less sparse), higher values result in\n+    /// fewer non-zero weights (more sparse).\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This is like a \"minimum importance\" cutoff.\n+    /// If a weight's importance is below this value, we zero it out to maintain\n+    /// sparsity. Typical values: 0.001 to 0.1\n+    /// </para>\n+    /// </remarks>\n+    public double SparseThreshold { get; set; }\n+\n+    /// <summary>\n+    /// Target sparsity ratio (fraction of zeros in sparse component).\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This value controls how sparse the sparse component should be.\n+    /// - 0.0 = no sparsity (all weights can be non-zero)\n+    /// - 0.5 = 50% of weights are zero\n+    /// - 0.95 = 95% of weights are zero (very sparse)\n+    /// - 0.99 = 99% of weights are zero (extremely sparse)\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This is the target percentage of zeros we want.\n+    /// Higher values (like 0.95) mean fewer non-zero weights, which keeps the\n+    /// model efficient. Lower values mean more flexibility but more parameters.\n+    ///\n+    /// Typical values:\n+    /// - 0.90 (90% zeros): More flexible, for complex domains\n+    /// - 0.95 (95% zeros): Good balance (recommended starting point)\n+    /// - 0.99 (99% zeros): Very efficient, for simple adaptations\n+    /// </para>\n+    /// </remarks>\n+    public double SparsityRatio { get; set; }\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// RoSA parameters include:\n+    /// - Base layer parameters (if not frozen)\n+    /// - LoRA parameters (rank * (inputSize + outputSize))\n+    /// - Non-zero sparse parameters (varies based on sparsity)\n+    ///\n+    /// For parameter counting, we report the full sparse matrix size, but in practice\n+    /// only the non-zero elements need to be stored and updated.\n+    /// </para>\n+    /// </remarks>\n+    public override int ParameterCount\n+    {\n+        get\n+        {\n+            int baseCount = _freezeBaseLayer ? 0 : _baseLayer.ParameterCount;\n+            int loraCount = _loraLayer.ParameterCount;\n+            int sparseCount = _sparseWeights.Rows * _sparseWeights.Columns;\n+            return baseCount + loraCount + sparseCount;\n+        }","path":"src/NeuralNetworks/Layers/RoSAAdapter.cs","commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","original_commit_id":"c11235933bf7c341c8b4a223cf6ac11b757d1480","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Prevent constructor-time NullReference in `ParameterCount`.**\n\n`LoRAAdapterBase`‚Äôs constructor allocates `Parameters = new Vector<T>(ParameterCount);` *before* the `RoSAAdapter` constructor body runs (see `LoRAAdapterBase.cs`, lines 92-112). On that invocation `_sparseWeights` is still null, so Line 157 dereferences it and throws. Guard the override so it can be evaluated safely during base construction.  \n\n\n```diff\n         get\n         {\n             int baseCount = _freezeBaseLayer ? 0 : _baseLayer.ParameterCount;\n             int loraCount = _loraLayer.ParameterCount;\n-            int sparseCount = _sparseWeights.Rows * _sparseWeights.Columns;\n+            int sparseCount = _sparseWeights != null\n+                ? _sparseWeights.Rows * _sparseWeights.Columns\n+                : GetOutputShape()[0] * GetInputShape()[0];\n             return baseCount + loraCount + sparseCount;\n         }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    public override int ParameterCount\n    {\n        get\n        {\n            int baseCount = _freezeBaseLayer ? 0 : _baseLayer.ParameterCount;\n            int loraCount = _loraLayer.ParameterCount;\n            int sparseCount = _sparseWeights != null\n                ? _sparseWeights.Rows * _sparseWeights.Columns\n                : GetOutputShape()[0] * GetInputShape()[0];\n            return baseCount + loraCount + sparseCount;\n        }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/Layers/RoSAAdapter.cs around lines 152 to 160, the\noverride of ParameterCount dereferences _sparseWeights during base-class\nconstruction causing a NullReference; change the calculation to guard against\nnull (and other potentially null members) by treating null _sparseWeights as\nzero-sized and null _baseLayer/_loraLayer as zero-parameter, e.g. use\nconditional checks or null-conditional/property accesses so ParameterCount\nreturns baseCount + loraCount + sparseCount with sparseCount = (_sparseWeights\n== null ? 0 : _sparseWeights.Rows * _sparseWeights.Columns) (and similarly guard\n_baseLayer/_loraLayer) so the property is safe to call from the base\nconstructor.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:32:38Z","updated_at":"2025-11-02T02:32:41Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106367","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106367"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106367"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106367/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":161,"original_start_line":152,"start_side":"RIGHT","line":169,"original_line":160,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":160,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106372","pull_request_review_id":3407993158,"id":2484106372,"node_id":"PRRC_kwDOKSXUF86UEHSE","diff_hunk":"@@ -0,0 +1,827 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.NeuralNetworks.Layers;\n+\n+/// <summary>\n+/// RoSA (Robust Adaptation) adapter for parameter-efficient fine-tuning with improved robustness to distribution shifts.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// RoSA (Robust Adaptation) extends standard LoRA by combining two complementary components:\n+/// 1. Low-rank component (standard LoRA): Captures common, structured patterns in adaptations\n+/// 2. Sparse component: Captures specific, rare, or outlier patterns that low-rank cannot represent\n+/// </para>\n+/// <para>\n+/// <b>Mathematical Formulation:</b>\n+/// Given input x and pre-trained weights W, RoSA computes:\n+/// - Low-rank component: L = (alpha/rank) * B * A * x\n+/// - Sparse component: S = W_sparse * x (where W_sparse is highly sparse)\n+/// - Final output: y = W*x + L + S\n+///\n+/// The sparse component is maintained through magnitude-based pruning, keeping only the\n+/// most significant weights and zeroing out the rest. This creates a sparse matrix that\n+/// captures specific patterns while remaining parameter-efficient.\n+/// </para>\n+/// <para>\n+/// <b>Research Context:</b>\n+/// RoSA was introduced in January 2024 as a robust alternative to standard LoRA.\n+/// The key insight is that low-rank approximations work well for common patterns but\n+/// struggle with distribution shifts and rare patterns. By adding a sparse component,\n+/// RoSA can capture outliers and domain-specific patterns without significantly\n+/// increasing parameter count.\n+///\n+/// In experiments on domain adaptation tasks, RoSA showed:\n+/// - Better generalization to new domains (+5-10% over standard LoRA)\n+/// - More robust to distribution shifts\n+/// - Ability to capture both global patterns (low-rank) and local exceptions (sparse)\n+/// - Only modest increase in parameters (typically 5-15% more than pure LoRA)\n+/// </para>\n+/// <para>\n+/// <b>For Beginners:</b> RoSA is like LoRA with a safety net for unusual cases.\n+///\n+/// Think of it this way:\n+/// - Low-rank LoRA is like learning general rules (\"most images of cats have pointed ears\")\n+/// - Sparse component is like remembering specific exceptions (\"this one cat breed has round ears\")\n+/// - Together they make a robust model that handles both common and rare cases\n+///\n+/// Why RoSA is more robust:\n+/// - Low-rank component: Efficient for common patterns across domains\n+/// - Sparse component: Handles outliers and domain-specific quirks\n+/// - Result: Better performance when test data differs from training data\n+///\n+/// When to use RoSA over standard LoRA:\n+/// - When you expect distribution shifts (train on news, test on social media)\n+/// - When your data has outliers or rare patterns that matter\n+/// - When you need robustness more than absolute parameter efficiency\n+/// - When adapting to multiple related but distinct domains\n+///\n+/// Trade-offs vs standard LoRA:\n+/// + More robust to distribution shifts\n+/// + Better handles rare patterns\n+/// + More flexible adaptation\n+/// - Slightly more parameters (sparse component adds ~5-15%)\n+/// - Slightly more computation (extra sparse matrix multiply)\n+/// - Requires tuning sparsity ratio\n+/// </para>\n+/// <para>\n+/// <b>Reference:</b>\n+/// \"RoSA: Robust Adaptation through Sparse Regularization\"\n+/// January 2024\n+/// </para>\n+/// </remarks>\n+public class RoSAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Sparse weight matrix that captures specific/rare patterns.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This matrix has the same dimensions as the base layer's weights but is highly sparse\n+    /// (typically 90-99% zeros). It's maintained through magnitude-based pruning during training.\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This is the \"exception handler\" of RoSA.\n+    /// Most of its values are zero, but the few non-zero values capture specific patterns\n+    /// that the low-rank component can't represent efficiently.\n+    /// </para>\n+    /// </remarks>\n+    private Matrix<T> _sparseWeights;\n+\n+    /// <summary>\n+    /// Gradients for the sparse weight component, computed during backpropagation.\n+    /// </summary>\n+    private Matrix<T>? _sparseGradients;\n+\n+    /// <summary>\n+    /// Threshold for magnitude-based pruning of sparse weights.\n+    /// Weights with magnitude below this threshold are set to zero.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This threshold controls the sparsity of the sparse component. Lower values\n+    /// result in more non-zero weights (less sparse), higher values result in\n+    /// fewer non-zero weights (more sparse).\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This is like a \"minimum importance\" cutoff.\n+    /// If a weight's importance is below this value, we zero it out to maintain\n+    /// sparsity. Typical values: 0.001 to 0.1\n+    /// </para>\n+    /// </remarks>\n+    public double SparseThreshold { get; set; }\n+\n+    /// <summary>\n+    /// Target sparsity ratio (fraction of zeros in sparse component).\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This value controls how sparse the sparse component should be.\n+    /// - 0.0 = no sparsity (all weights can be non-zero)\n+    /// - 0.5 = 50% of weights are zero\n+    /// - 0.95 = 95% of weights are zero (very sparse)\n+    /// - 0.99 = 99% of weights are zero (extremely sparse)\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This is the target percentage of zeros we want.\n+    /// Higher values (like 0.95) mean fewer non-zero weights, which keeps the\n+    /// model efficient. Lower values mean more flexibility but more parameters.\n+    ///\n+    /// Typical values:\n+    /// - 0.90 (90% zeros): More flexible, for complex domains\n+    /// - 0.95 (95% zeros): Good balance (recommended starting point)\n+    /// - 0.99 (99% zeros): Very efficient, for simple adaptations\n+    /// </para>\n+    /// </remarks>\n+    public double SparsityRatio { get; set; }\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// RoSA parameters include:\n+    /// - Base layer parameters (if not frozen)\n+    /// - LoRA parameters (rank * (inputSize + outputSize))\n+    /// - Non-zero sparse parameters (varies based on sparsity)\n+    ///\n+    /// For parameter counting, we report the full sparse matrix size, but in practice\n+    /// only the non-zero elements need to be stored and updated.\n+    /// </para>\n+    /// </remarks>\n+    public override int ParameterCount\n+    {\n+        get\n+        {\n+            int baseCount = _freezeBaseLayer ? 0 : _baseLayer.ParameterCount;\n+            int loraCount = _loraLayer.ParameterCount;\n+            int sparseCount = _sparseWeights.Rows * _sparseWeights.Columns;\n+            return baseCount + loraCount + sparseCount;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Initializes a new RoSA adapter wrapping an existing layer.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with RoSA.</param>\n+    /// <param name=\"rank\">The rank of the low-rank LoRA decomposition.</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"sparsityRatio\">Target sparsity ratio (0.0 to 1.0, typically 0.9-0.99).</param>\n+    /// <param name=\"sparseThreshold\">Magnitude threshold for pruning sparse weights (typically 0.001-0.1).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when sparsityRatio is not between 0 and 1.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// The constructor initializes the RoSA adapter by:\n+    /// 1. Setting up the standard LoRA components (via base constructor)\n+    /// 2. Initializing the sparse weight matrix (starts with small random values)\n+    /// 3. Applying initial pruning to enforce sparsity\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This creates a RoSA adapter around your existing layer.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to fine-tune efficiently and robustly\n+    /// - rank: How much compression for the low-rank component (lower = fewer parameters)\n+    /// - alpha: Scaling factor for LoRA contribution (usually equals rank)\n+    /// - sparsityRatio: How sparse the sparse component should be (0.95 = 95% zeros)\n+    /// - sparseThreshold: Minimum importance for keeping a sparse weight (0.01 is typical)\n+    /// - freezeBaseLayer: Usually true - we only train LoRA + sparse, not base weights\n+    ///\n+    /// Example: For a 1000x1000 layer with rank=8 and sparsityRatio=0.95:\n+    /// - Base layer: 1,000,000 parameters (frozen)\n+    /// - LoRA: 16,000 parameters (8 * (1000 + 1000))\n+    /// - Sparse: ~50,000 parameters (5% of 1,000,000)\n+    /// - Total trainable: ~66,000 parameters (vs 1M for full fine-tuning!)\n+    /// </para>\n+    /// </remarks>\n+    public RoSAAdapter(\n+        ILayer<T> baseLayer,\n+        int rank,\n+        double alpha = -1,\n+        double sparsityRatio = 0.95,\n+        double sparseThreshold = 0.01,\n+        bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        if (sparsityRatio < 0.0 || sparsityRatio >= 1.0)\n+        {\n+            throw new ArgumentException(\"Sparsity ratio must be between 0.0 and 1.0 (exclusive of 1.0)\", nameof(sparsityRatio));\n+        }\n+\n+        SparsityRatio = sparsityRatio;\n+        SparseThreshold = sparseThreshold;\n+\n+        // Initialize sparse weights\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        _sparseWeights = new Matrix<T>(outputSize, inputSize);\n+\n+        // Initialize with small random values (will be pruned)\n+        InitializeSparseWeights();\n+\n+        // Apply initial pruning to enforce sparsity\n+        PruneSparseWeights();\n+\n+        // Update parameters to include sparse component\n+        Parameters = new Vector<T>(ParameterCount);\n+        UpdateParametersFromComponents();\n+    }\n+\n+    /// <summary>\n+    /// Initializes sparse weights with small random values.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// The sparse weights are initialized with small random values drawn from a\n+    /// normal distribution with standard deviation 0.01. These values will be\n+    /// pruned based on magnitude to enforce sparsity.\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This gives the sparse component a random starting point.\n+    /// Most of these values will be pruned (set to zero) immediately, but this\n+    /// initialization ensures we start with a diverse set of potential patterns.\n+    /// </para>\n+    /// </remarks>\n+    private void InitializeSparseWeights()\n+    {\n+        Random random = new Random();\n+        for (int i = 0; i < _sparseWeights.Rows; i++)\n+        {\n+            for (int j = 0; j < _sparseWeights.Columns; j++)\n+            {\n+                // Small random initialization\n+                double value = random.NextGaussian(0.0, 0.01);\n+                _sparseWeights[i, j] = NumOps.FromDouble(value);\n+            }\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Prunes sparse weights based on magnitude to maintain target sparsity.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This method implements magnitude-based pruning:\n+    /// 1. Computes magnitude of all sparse weights\n+    /// 2. Determines threshold based on target sparsity ratio\n+    /// 3. Sets weights below threshold to zero\n+    ///\n+    /// This ensures the sparse component maintains its sparsity during training.\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This is like cleaning up the sparse component.\n+    ///\n+    /// We keep only the most important weights:\n+    /// 1. Look at all the weights and their magnitudes\n+    /// 2. Sort them by importance (magnitude)\n+    /// 3. Keep the top X% (based on sparsity ratio)\n+    /// 4. Zero out the rest\n+    ///\n+    /// Example with sparsity ratio 0.95:\n+    /// - We have 1000 weights\n+    /// - We want 95% zeros (950 zeros, 50 non-zeros)\n+    /// - Keep the 50 largest magnitudes\n+    /// - Set the other 950 to zero\n+    ///\n+    /// This is called periodically during training to maintain sparsity.\n+    /// </para>\n+    /// </remarks>\n+    public void PruneSparseWeights()\n+    {\n+        int rows = _sparseWeights.Rows;\n+        int cols = _sparseWeights.Columns;\n+        int totalWeights = rows * cols;\n+\n+        // Collect magnitudes\n+        List<(int row, int col, double magnitude)> magnitudes = new List<(int, int, double)>();\n+        for (int i = 0; i < rows; i++)\n+        {\n+            for (int j = 0; j < cols; j++)\n+            {\n+                double mag = Math.Abs(Convert.ToDouble(_sparseWeights[i, j]));\n+                magnitudes.Add((i, j, mag));\n+            }\n+        }\n+\n+        // Sort by magnitude (descending)\n+        magnitudes.Sort((a, b) => b.magnitude.CompareTo(a.magnitude));\n+\n+        // Determine number of non-zero weights to keep\n+        int keepCount = (int)((1.0 - SparsityRatio) * totalWeights);\n+        keepCount = Math.Max(1, keepCount); // Keep at least one weight\n+\n+        // Also consider threshold-based pruning\n+        double adaptiveThreshold = SparseThreshold;\n+        if (keepCount < magnitudes.Count)\n+        {\n+            // Use the larger of: fixed threshold or magnitude of keepCount-th element\n+            adaptiveThreshold = Math.Max(SparseThreshold, magnitudes[keepCount].magnitude);\n+        }\n+\n+        // Apply pruning: zero out weights below threshold\n+        for (int i = 0; i < rows; i++)\n+        {\n+            for (int j = 0; j < cols; j++)\n+            {\n+                double mag = Math.Abs(Convert.ToDouble(_sparseWeights[i, j]));\n+                if (mag < adaptiveThreshold)\n+                {\n+                    _sparseWeights[i, j] = NumOps.Zero;\n+                }\n+            }\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Gets the current sparsity of the sparse component.\n+    /// </summary>\n+    /// <returns>The fraction of zeros in the sparse weight matrix (0.0 to 1.0).</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method computes the actual sparsity by counting zero and near-zero elements.\n+    /// The result can be compared to SparsityRatio to see how well pruning is working.\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This tells you what percentage of the sparse component is actually zero.\n+    ///\n+    /// If you set SparsityRatio to 0.95, this should return close to 0.95 after pruning.\n+    /// If it's much lower, you might need to adjust the threshold or pruning frequency.\n+    ///\n+    /// Example return values:\n+    /// - 0.95 = 95% zeros (good for target of 0.95)\n+    /// - 0.80 = 80% zeros (less sparse than target)\n+    /// - 0.99 = 99% zeros (more sparse than target)\n+    /// </para>\n+    /// </remarks>\n+    public double GetSparsity()\n+    {\n+        int totalWeights = _sparseWeights.Rows * _sparseWeights.Columns;\n+        int zeroCount = 0;\n+        double epsilon = 1e-10;\n+\n+        for (int i = 0; i < _sparseWeights.Rows; i++)\n+        {\n+            for (int j = 0; j < _sparseWeights.Columns; j++)\n+            {\n+                double val = Math.Abs(Convert.ToDouble(_sparseWeights[i, j]));\n+                if (val < epsilon)\n+                {\n+                    zeroCount++;\n+                }\n+            }\n+        }\n+\n+        return (double)zeroCount / totalWeights;\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass through RoSA adapter.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Output combining base layer, low-rank LoRA, and sparse components.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The RoSA forward pass computes:\n+    /// 1. Base output: y_base = base_layer(input)\n+    /// 2. LoRA output: y_lora = lora_layer(input)\n+    /// 3. Sparse output: y_sparse = input @ sparse_weights^T\n+    /// 4. Final output: y = y_base + y_lora + y_sparse\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This is where all three components work together.\n+    ///\n+    /// Think of it as three parallel processing paths:\n+    /// - Base layer: Original pre-trained knowledge (usually frozen)\n+    /// - LoRA component: Low-rank corrections for common patterns\n+    /// - Sparse component: Specific corrections for rare patterns\n+    ///\n+    /// All three outputs are added together to get the final result.\n+    /// This combination gives RoSA its robustness: the low-rank handles\n+    /// common patterns efficiently, while sparse handles outliers.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        // 1. Forward through base layer\n+        Tensor<T> baseOutput = _baseLayer.Forward(input);\n+\n+        // 2. Forward through LoRA layer (low-rank component)\n+        Tensor<T> loraOutput = _loraLayer.Forward(input);\n+\n+        // 3. Forward through sparse component\n+        // Compute: sparse_output = input @ sparse_weights^T\n+        int batchSize = input.Shape[0];\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+\n+        // Convert input to matrix\n+        Matrix<T> inputMatrix = new Matrix<T>(batchSize, inputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                inputMatrix[i, j] = input[i * inputSize + j];\n+            }\n+        }\n+\n+        // Multiply by sparse weights: [batchSize, inputSize] @ [inputSize, outputSize]\n+        Matrix<T> sparseOutputMatrix = inputMatrix.Multiply(_sparseWeights.Transpose());\n+\n+        // Convert to tensor\n+        Vector<T> sparseOutputData = new Vector<T>(batchSize * outputSize);\n+        int idx = 0;\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                sparseOutputData[idx++] = sparseOutputMatrix[i, j];\n+            }\n+        }\n+        Tensor<T> sparseOutput = new Tensor<T>(new[] { batchSize, outputSize }, sparseOutputData);\n+\n+        // 4. Sum all three outputs\n+        Tensor<T> result = new Tensor<T>(baseOutput.Shape);\n+        for (int i = 0; i < baseOutput.Length; i++)\n+        {\n+            T sum = NumOps.Add(baseOutput[i], loraOutput[i]);\n+            sum = NumOps.Add(sum, sparseOutput[i]);\n+            result[i] = sum;\n+        }\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Performs the backward pass through RoSA adapter.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient flowing back from the next layer.</param>\n+    /// <returns>Gradient to pass to the previous layer.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The backward pass computes gradients for all three components:\n+    /// 1. LoRA component (via LoRA layer's backward)\n+    /// 2. Sparse component (direct gradient computation)\n+    /// 3. Base layer (if not frozen)\n+    ///\n+    /// Gradients are accumulated and input gradients are summed.\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This is where RoSA learns from errors.\n+    ///\n+    /// The backward pass tells each component how to improve:\n+    /// - LoRA component: Update low-rank matrices A and B\n+    /// - Sparse component: Update the sparse weight matrix\n+    /// - Base layer: Update if not frozen (usually frozen)\n+    ///\n+    /// After this, UpdateParameters() will apply the learning using these gradients.\n+    /// The sparse gradients will be pruned to maintain sparsity.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        int batchSize = outputGradient.Shape[0];\n+        int outputSize = GetOutputShape()[0];\n+        int inputSize = GetInputShape()[0];\n+\n+        // 1. Backward through LoRA layer\n+        Tensor<T> loraInputGrad = _loraLayer.Backward(outputGradient);\n+\n+        // 2. Backward through base layer\n+        Tensor<T> baseInputGrad = _baseLayer.Backward(outputGradient);\n+\n+        // 3. Compute gradients for sparse component\n+        // Sparse gradient: dL/dW_sparse = output_gradient^T @ input\n+        // Convert output gradient to matrix\n+        Matrix<T> gradMatrix = new Matrix<T>(batchSize, outputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                gradMatrix[i, j] = outputGradient[i * outputSize + j];\n+            }\n+        }\n+\n+        // Get input from base layer (we'll need to store this in a more complete implementation)\n+        // For now, we'll compute sparse weight gradients from the output gradient\n+        // In practice, you'd cache the input from forward pass\n+        _sparseGradients = new Matrix<T>(outputSize, inputSize);\n+\n+        // Simplified gradient computation (assumes gradients are averaged across batch)\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                T gradSum = NumOps.Zero;\n+                for (int b = 0; b < batchSize; b++)\n+                {\n+                    gradSum = NumOps.Add(gradSum, gradMatrix[b, i]);\n+                }\n+                // Average over batch\n+                _sparseGradients[i, j] = NumOps.Divide(gradSum, NumOps.FromDouble(batchSize));\n+            }\n+        }\n+","path":"src/NeuralNetworks/Layers/RoSAAdapter.cs","commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","original_commit_id":"c11235933bf7c341c8b4a223cf6ac11b757d1480","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Sparse gradient must include cached inputs.**\n\nLines 504-523 average the upstream gradient over the batch but never multiply by the corresponding input activations, so `dL/dW_sparse` is flat-out wrong. Because `Forward` also fails to cache the input, you currently have no way to compute the correct outer-product. This makes sparse-weight learning impossible. Please cache the forward input and use it when forming the sparse gradients.  \n\n\n```diff\n-        _sparseGradients = new Matrix<T>(outputSize, inputSize);\n-\n-        // Simplified gradient computation (assumes gradients are averaged across batch)\n-        for (int i = 0; i < outputSize; i++)\n-        {\n-            for (int j = 0; j < inputSize; j++)\n-            {\n-                T gradSum = NumOps.Zero;\n-                for (int b = 0; b < batchSize; b++)\n-                {\n-                    gradSum = NumOps.Add(gradSum, gradMatrix[b, i]);\n-                }\n-                // Average over batch\n-                _sparseGradients[i, j] = NumOps.Divide(gradSum, NumOps.FromDouble(batchSize));\n-            }\n-        }\n+        if (_cachedInputMatrix == null)\n+        {\n+            throw new InvalidOperationException(\"Forward input must be cached to compute sparse gradients.\");\n+        }\n+\n+        _sparseGradients = new Matrix<T>(outputSize, inputSize);\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                T gradSum = NumOps.Zero;\n+                for (int b = 0; b < batchSize; b++)\n+                {\n+                    T contribution = NumOps.Multiply(gradMatrix[b, i], _cachedInputMatrix[b, j]);\n+                    gradSum = NumOps.Add(gradSum, contribution);\n+                }\n+                _sparseGradients[i, j] = NumOps.Divide(gradSum, NumOps.FromDouble(batchSize));\n+            }\n+        }\n```\n\n_Add the cached input support (outside this hunk)_: declare `private Matrix<T>? _cachedInputMatrix;` near the sparse fields, store it in `Forward` right after constructing `inputMatrix` (e.g., `_cachedInputMatrix = inputMatrix.Clone();`), and reset it to null in `ResetState()` to avoid stale data.\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/Layers/RoSAAdapter.cs around lines 494 to 525, the sparse\ngradient computation averages the upstream gradient but never multiplies by the\ninput activations and Forward does not cache inputs, so dL/dW_sparse is\nincorrect; fix by adding a private Matrix<T>? _cachedInputMatrix field near the\nsparse fields, store a clone of inputMatrix in Forward immediately after\nconstructing it (e.g., _cachedInputMatrix = inputMatrix.Clone()), and reset it\nto null in ResetState(); then change this hunk to use the cached input\nper-batch: for each output i and input j accumulate gradMatrix[b,i] *\n_cachedInputMatrix[b,j] across batch and divide by batchSize (or equivalent\ntyped division) when writing _sparseGradients[i,j], ensuring you validate\n_cachedInputMatrix is non-null and handle sizes accordingly.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:32:39Z","updated_at":"2025-11-02T02:32:41Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106372","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106372"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106372"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106372/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":506,"original_start_line":494,"start_side":"RIGHT","line":543,"original_line":525,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":525,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106377","pull_request_review_id":3407993158,"id":2484106377,"node_id":"PRRC_kwDOKSXUF86UEHSJ","diff_hunk":"@@ -0,0 +1,827 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.NeuralNetworks.Layers;\n+\n+/// <summary>\n+/// RoSA (Robust Adaptation) adapter for parameter-efficient fine-tuning with improved robustness to distribution shifts.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// RoSA (Robust Adaptation) extends standard LoRA by combining two complementary components:\n+/// 1. Low-rank component (standard LoRA): Captures common, structured patterns in adaptations\n+/// 2. Sparse component: Captures specific, rare, or outlier patterns that low-rank cannot represent\n+/// </para>\n+/// <para>\n+/// <b>Mathematical Formulation:</b>\n+/// Given input x and pre-trained weights W, RoSA computes:\n+/// - Low-rank component: L = (alpha/rank) * B * A * x\n+/// - Sparse component: S = W_sparse * x (where W_sparse is highly sparse)\n+/// - Final output: y = W*x + L + S\n+///\n+/// The sparse component is maintained through magnitude-based pruning, keeping only the\n+/// most significant weights and zeroing out the rest. This creates a sparse matrix that\n+/// captures specific patterns while remaining parameter-efficient.\n+/// </para>\n+/// <para>\n+/// <b>Research Context:</b>\n+/// RoSA was introduced in January 2024 as a robust alternative to standard LoRA.\n+/// The key insight is that low-rank approximations work well for common patterns but\n+/// struggle with distribution shifts and rare patterns. By adding a sparse component,\n+/// RoSA can capture outliers and domain-specific patterns without significantly\n+/// increasing parameter count.\n+///\n+/// In experiments on domain adaptation tasks, RoSA showed:\n+/// - Better generalization to new domains (+5-10% over standard LoRA)\n+/// - More robust to distribution shifts\n+/// - Ability to capture both global patterns (low-rank) and local exceptions (sparse)\n+/// - Only modest increase in parameters (typically 5-15% more than pure LoRA)\n+/// </para>\n+/// <para>\n+/// <b>For Beginners:</b> RoSA is like LoRA with a safety net for unusual cases.\n+///\n+/// Think of it this way:\n+/// - Low-rank LoRA is like learning general rules (\"most images of cats have pointed ears\")\n+/// - Sparse component is like remembering specific exceptions (\"this one cat breed has round ears\")\n+/// - Together they make a robust model that handles both common and rare cases\n+///\n+/// Why RoSA is more robust:\n+/// - Low-rank component: Efficient for common patterns across domains\n+/// - Sparse component: Handles outliers and domain-specific quirks\n+/// - Result: Better performance when test data differs from training data\n+///\n+/// When to use RoSA over standard LoRA:\n+/// - When you expect distribution shifts (train on news, test on social media)\n+/// - When your data has outliers or rare patterns that matter\n+/// - When you need robustness more than absolute parameter efficiency\n+/// - When adapting to multiple related but distinct domains\n+///\n+/// Trade-offs vs standard LoRA:\n+/// + More robust to distribution shifts\n+/// + Better handles rare patterns\n+/// + More flexible adaptation\n+/// - Slightly more parameters (sparse component adds ~5-15%)\n+/// - Slightly more computation (extra sparse matrix multiply)\n+/// - Requires tuning sparsity ratio\n+/// </para>\n+/// <para>\n+/// <b>Reference:</b>\n+/// \"RoSA: Robust Adaptation through Sparse Regularization\"\n+/// January 2024\n+/// </para>\n+/// </remarks>\n+public class RoSAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Sparse weight matrix that captures specific/rare patterns.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This matrix has the same dimensions as the base layer's weights but is highly sparse\n+    /// (typically 90-99% zeros). It's maintained through magnitude-based pruning during training.\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This is the \"exception handler\" of RoSA.\n+    /// Most of its values are zero, but the few non-zero values capture specific patterns\n+    /// that the low-rank component can't represent efficiently.\n+    /// </para>\n+    /// </remarks>\n+    private Matrix<T> _sparseWeights;\n+\n+    /// <summary>\n+    /// Gradients for the sparse weight component, computed during backpropagation.\n+    /// </summary>\n+    private Matrix<T>? _sparseGradients;\n+\n+    /// <summary>\n+    /// Threshold for magnitude-based pruning of sparse weights.\n+    /// Weights with magnitude below this threshold are set to zero.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This threshold controls the sparsity of the sparse component. Lower values\n+    /// result in more non-zero weights (less sparse), higher values result in\n+    /// fewer non-zero weights (more sparse).\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This is like a \"minimum importance\" cutoff.\n+    /// If a weight's importance is below this value, we zero it out to maintain\n+    /// sparsity. Typical values: 0.001 to 0.1\n+    /// </para>\n+    /// </remarks>\n+    public double SparseThreshold { get; set; }\n+\n+    /// <summary>\n+    /// Target sparsity ratio (fraction of zeros in sparse component).\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This value controls how sparse the sparse component should be.\n+    /// - 0.0 = no sparsity (all weights can be non-zero)\n+    /// - 0.5 = 50% of weights are zero\n+    /// - 0.95 = 95% of weights are zero (very sparse)\n+    /// - 0.99 = 99% of weights are zero (extremely sparse)\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This is the target percentage of zeros we want.\n+    /// Higher values (like 0.95) mean fewer non-zero weights, which keeps the\n+    /// model efficient. Lower values mean more flexibility but more parameters.\n+    ///\n+    /// Typical values:\n+    /// - 0.90 (90% zeros): More flexible, for complex domains\n+    /// - 0.95 (95% zeros): Good balance (recommended starting point)\n+    /// - 0.99 (99% zeros): Very efficient, for simple adaptations\n+    /// </para>\n+    /// </remarks>\n+    public double SparsityRatio { get; set; }\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// RoSA parameters include:\n+    /// - Base layer parameters (if not frozen)\n+    /// - LoRA parameters (rank * (inputSize + outputSize))\n+    /// - Non-zero sparse parameters (varies based on sparsity)\n+    ///\n+    /// For parameter counting, we report the full sparse matrix size, but in practice\n+    /// only the non-zero elements need to be stored and updated.\n+    /// </para>\n+    /// </remarks>\n+    public override int ParameterCount\n+    {\n+        get\n+        {\n+            int baseCount = _freezeBaseLayer ? 0 : _baseLayer.ParameterCount;\n+            int loraCount = _loraLayer.ParameterCount;\n+            int sparseCount = _sparseWeights.Rows * _sparseWeights.Columns;\n+            return baseCount + loraCount + sparseCount;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Initializes a new RoSA adapter wrapping an existing layer.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with RoSA.</param>\n+    /// <param name=\"rank\">The rank of the low-rank LoRA decomposition.</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"sparsityRatio\">Target sparsity ratio (0.0 to 1.0, typically 0.9-0.99).</param>\n+    /// <param name=\"sparseThreshold\">Magnitude threshold for pruning sparse weights (typically 0.001-0.1).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when sparsityRatio is not between 0 and 1.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// The constructor initializes the RoSA adapter by:\n+    /// 1. Setting up the standard LoRA components (via base constructor)\n+    /// 2. Initializing the sparse weight matrix (starts with small random values)\n+    /// 3. Applying initial pruning to enforce sparsity\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This creates a RoSA adapter around your existing layer.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to fine-tune efficiently and robustly\n+    /// - rank: How much compression for the low-rank component (lower = fewer parameters)\n+    /// - alpha: Scaling factor for LoRA contribution (usually equals rank)\n+    /// - sparsityRatio: How sparse the sparse component should be (0.95 = 95% zeros)\n+    /// - sparseThreshold: Minimum importance for keeping a sparse weight (0.01 is typical)\n+    /// - freezeBaseLayer: Usually true - we only train LoRA + sparse, not base weights\n+    ///\n+    /// Example: For a 1000x1000 layer with rank=8 and sparsityRatio=0.95:\n+    /// - Base layer: 1,000,000 parameters (frozen)\n+    /// - LoRA: 16,000 parameters (8 * (1000 + 1000))\n+    /// - Sparse: ~50,000 parameters (5% of 1,000,000)\n+    /// - Total trainable: ~66,000 parameters (vs 1M for full fine-tuning!)\n+    /// </para>\n+    /// </remarks>\n+    public RoSAAdapter(\n+        ILayer<T> baseLayer,\n+        int rank,\n+        double alpha = -1,\n+        double sparsityRatio = 0.95,\n+        double sparseThreshold = 0.01,\n+        bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        if (sparsityRatio < 0.0 || sparsityRatio >= 1.0)\n+        {\n+            throw new ArgumentException(\"Sparsity ratio must be between 0.0 and 1.0 (exclusive of 1.0)\", nameof(sparsityRatio));\n+        }\n+\n+        SparsityRatio = sparsityRatio;\n+        SparseThreshold = sparseThreshold;\n+\n+        // Initialize sparse weights\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        _sparseWeights = new Matrix<T>(outputSize, inputSize);\n+\n+        // Initialize with small random values (will be pruned)\n+        InitializeSparseWeights();\n+\n+        // Apply initial pruning to enforce sparsity\n+        PruneSparseWeights();\n+\n+        // Update parameters to include sparse component\n+        Parameters = new Vector<T>(ParameterCount);\n+        UpdateParametersFromComponents();\n+    }\n+\n+    /// <summary>\n+    /// Initializes sparse weights with small random values.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// The sparse weights are initialized with small random values drawn from a\n+    /// normal distribution with standard deviation 0.01. These values will be\n+    /// pruned based on magnitude to enforce sparsity.\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This gives the sparse component a random starting point.\n+    /// Most of these values will be pruned (set to zero) immediately, but this\n+    /// initialization ensures we start with a diverse set of potential patterns.\n+    /// </para>\n+    /// </remarks>\n+    private void InitializeSparseWeights()\n+    {\n+        Random random = new Random();\n+        for (int i = 0; i < _sparseWeights.Rows; i++)\n+        {\n+            for (int j = 0; j < _sparseWeights.Columns; j++)\n+            {\n+                // Small random initialization\n+                double value = random.NextGaussian(0.0, 0.01);\n+                _sparseWeights[i, j] = NumOps.FromDouble(value);\n+            }\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Prunes sparse weights based on magnitude to maintain target sparsity.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This method implements magnitude-based pruning:\n+    /// 1. Computes magnitude of all sparse weights\n+    /// 2. Determines threshold based on target sparsity ratio\n+    /// 3. Sets weights below threshold to zero\n+    ///\n+    /// This ensures the sparse component maintains its sparsity during training.\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This is like cleaning up the sparse component.\n+    ///\n+    /// We keep only the most important weights:\n+    /// 1. Look at all the weights and their magnitudes\n+    /// 2. Sort them by importance (magnitude)\n+    /// 3. Keep the top X% (based on sparsity ratio)\n+    /// 4. Zero out the rest\n+    ///\n+    /// Example with sparsity ratio 0.95:\n+    /// - We have 1000 weights\n+    /// - We want 95% zeros (950 zeros, 50 non-zeros)\n+    /// - Keep the 50 largest magnitudes\n+    /// - Set the other 950 to zero\n+    ///\n+    /// This is called periodically during training to maintain sparsity.\n+    /// </para>\n+    /// </remarks>\n+    public void PruneSparseWeights()\n+    {\n+        int rows = _sparseWeights.Rows;\n+        int cols = _sparseWeights.Columns;\n+        int totalWeights = rows * cols;\n+\n+        // Collect magnitudes\n+        List<(int row, int col, double magnitude)> magnitudes = new List<(int, int, double)>();\n+        for (int i = 0; i < rows; i++)\n+        {\n+            for (int j = 0; j < cols; j++)\n+            {\n+                double mag = Math.Abs(Convert.ToDouble(_sparseWeights[i, j]));\n+                magnitudes.Add((i, j, mag));\n+            }\n+        }\n+\n+        // Sort by magnitude (descending)\n+        magnitudes.Sort((a, b) => b.magnitude.CompareTo(a.magnitude));\n+\n+        // Determine number of non-zero weights to keep\n+        int keepCount = (int)((1.0 - SparsityRatio) * totalWeights);\n+        keepCount = Math.Max(1, keepCount); // Keep at least one weight\n+\n+        // Also consider threshold-based pruning\n+        double adaptiveThreshold = SparseThreshold;\n+        if (keepCount < magnitudes.Count)\n+        {\n+            // Use the larger of: fixed threshold or magnitude of keepCount-th element\n+            adaptiveThreshold = Math.Max(SparseThreshold, magnitudes[keepCount].magnitude);\n+        }\n+\n+        // Apply pruning: zero out weights below threshold\n+        for (int i = 0; i < rows; i++)\n+        {\n+            for (int j = 0; j < cols; j++)\n+            {\n+                double mag = Math.Abs(Convert.ToDouble(_sparseWeights[i, j]));\n+                if (mag < adaptiveThreshold)\n+                {\n+                    _sparseWeights[i, j] = NumOps.Zero;\n+                }\n+            }\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Gets the current sparsity of the sparse component.\n+    /// </summary>\n+    /// <returns>The fraction of zeros in the sparse weight matrix (0.0 to 1.0).</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method computes the actual sparsity by counting zero and near-zero elements.\n+    /// The result can be compared to SparsityRatio to see how well pruning is working.\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This tells you what percentage of the sparse component is actually zero.\n+    ///\n+    /// If you set SparsityRatio to 0.95, this should return close to 0.95 after pruning.\n+    /// If it's much lower, you might need to adjust the threshold or pruning frequency.\n+    ///\n+    /// Example return values:\n+    /// - 0.95 = 95% zeros (good for target of 0.95)\n+    /// - 0.80 = 80% zeros (less sparse than target)\n+    /// - 0.99 = 99% zeros (more sparse than target)\n+    /// </para>\n+    /// </remarks>\n+    public double GetSparsity()\n+    {\n+        int totalWeights = _sparseWeights.Rows * _sparseWeights.Columns;\n+        int zeroCount = 0;\n+        double epsilon = 1e-10;\n+\n+        for (int i = 0; i < _sparseWeights.Rows; i++)\n+        {\n+            for (int j = 0; j < _sparseWeights.Columns; j++)\n+            {\n+                double val = Math.Abs(Convert.ToDouble(_sparseWeights[i, j]));\n+                if (val < epsilon)\n+                {\n+                    zeroCount++;\n+                }\n+            }\n+        }\n+\n+        return (double)zeroCount / totalWeights;\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass through RoSA adapter.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Output combining base layer, low-rank LoRA, and sparse components.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The RoSA forward pass computes:\n+    /// 1. Base output: y_base = base_layer(input)\n+    /// 2. LoRA output: y_lora = lora_layer(input)\n+    /// 3. Sparse output: y_sparse = input @ sparse_weights^T\n+    /// 4. Final output: y = y_base + y_lora + y_sparse\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This is where all three components work together.\n+    ///\n+    /// Think of it as three parallel processing paths:\n+    /// - Base layer: Original pre-trained knowledge (usually frozen)\n+    /// - LoRA component: Low-rank corrections for common patterns\n+    /// - Sparse component: Specific corrections for rare patterns\n+    ///\n+    /// All three outputs are added together to get the final result.\n+    /// This combination gives RoSA its robustness: the low-rank handles\n+    /// common patterns efficiently, while sparse handles outliers.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        // 1. Forward through base layer\n+        Tensor<T> baseOutput = _baseLayer.Forward(input);\n+\n+        // 2. Forward through LoRA layer (low-rank component)\n+        Tensor<T> loraOutput = _loraLayer.Forward(input);\n+\n+        // 3. Forward through sparse component\n+        // Compute: sparse_output = input @ sparse_weights^T\n+        int batchSize = input.Shape[0];\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+\n+        // Convert input to matrix\n+        Matrix<T> inputMatrix = new Matrix<T>(batchSize, inputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                inputMatrix[i, j] = input[i * inputSize + j];\n+            }\n+        }\n+\n+        // Multiply by sparse weights: [batchSize, inputSize] @ [inputSize, outputSize]\n+        Matrix<T> sparseOutputMatrix = inputMatrix.Multiply(_sparseWeights.Transpose());\n+\n+        // Convert to tensor\n+        Vector<T> sparseOutputData = new Vector<T>(batchSize * outputSize);\n+        int idx = 0;\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                sparseOutputData[idx++] = sparseOutputMatrix[i, j];\n+            }\n+        }\n+        Tensor<T> sparseOutput = new Tensor<T>(new[] { batchSize, outputSize }, sparseOutputData);\n+\n+        // 4. Sum all three outputs\n+        Tensor<T> result = new Tensor<T>(baseOutput.Shape);\n+        for (int i = 0; i < baseOutput.Length; i++)\n+        {\n+            T sum = NumOps.Add(baseOutput[i], loraOutput[i]);\n+            sum = NumOps.Add(sum, sparseOutput[i]);\n+            result[i] = sum;\n+        }\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Performs the backward pass through RoSA adapter.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient flowing back from the next layer.</param>\n+    /// <returns>Gradient to pass to the previous layer.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The backward pass computes gradients for all three components:\n+    /// 1. LoRA component (via LoRA layer's backward)\n+    /// 2. Sparse component (direct gradient computation)\n+    /// 3. Base layer (if not frozen)\n+    ///\n+    /// Gradients are accumulated and input gradients are summed.\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This is where RoSA learns from errors.\n+    ///\n+    /// The backward pass tells each component how to improve:\n+    /// - LoRA component: Update low-rank matrices A and B\n+    /// - Sparse component: Update the sparse weight matrix\n+    /// - Base layer: Update if not frozen (usually frozen)\n+    ///\n+    /// After this, UpdateParameters() will apply the learning using these gradients.\n+    /// The sparse gradients will be pruned to maintain sparsity.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        int batchSize = outputGradient.Shape[0];\n+        int outputSize = GetOutputShape()[0];\n+        int inputSize = GetInputShape()[0];\n+\n+        // 1. Backward through LoRA layer\n+        Tensor<T> loraInputGrad = _loraLayer.Backward(outputGradient);\n+\n+        // 2. Backward through base layer\n+        Tensor<T> baseInputGrad = _baseLayer.Backward(outputGradient);\n+\n+        // 3. Compute gradients for sparse component\n+        // Sparse gradient: dL/dW_sparse = output_gradient^T @ input\n+        // Convert output gradient to matrix\n+        Matrix<T> gradMatrix = new Matrix<T>(batchSize, outputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                gradMatrix[i, j] = outputGradient[i * outputSize + j];\n+            }\n+        }\n+\n+        // Get input from base layer (we'll need to store this in a more complete implementation)\n+        // For now, we'll compute sparse weight gradients from the output gradient\n+        // In practice, you'd cache the input from forward pass\n+        _sparseGradients = new Matrix<T>(outputSize, inputSize);\n+\n+        // Simplified gradient computation (assumes gradients are averaged across batch)\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                T gradSum = NumOps.Zero;\n+                for (int b = 0; b < batchSize; b++)\n+                {\n+                    gradSum = NumOps.Add(gradSum, gradMatrix[b, i]);\n+                }\n+                // Average over batch\n+                _sparseGradients[i, j] = NumOps.Divide(gradSum, NumOps.FromDouble(batchSize));\n+            }\n+        }\n+\n+        // 4. Compute input gradient for sparse component\n+        // input_grad_sparse = output_gradient @ sparse_weights\n+        Matrix<T> sparseInputGradMatrix = gradMatrix.Multiply(_sparseWeights);\n+\n+        // Convert to tensor\n+        Vector<T> sparseInputGradData = new Vector<T>(batchSize * inputSize);\n+        int idx = 0;\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                sparseInputGradData[idx++] = sparseInputGradMatrix[i, j];\n+            }\n+        }\n+        Tensor<T> sparseInputGrad = new Tensor<T>(new[] { batchSize, inputSize }, sparseInputGradData);\n+\n+        // 5. Sum input gradients from all three paths\n+        Tensor<T> inputGrad = new Tensor<T>(loraInputGrad.Shape);\n+        for (int i = 0; i < loraInputGrad.Length; i++)\n+        {\n+            T sum = NumOps.Add(loraInputGrad[i], baseInputGrad[i]);\n+            sum = NumOps.Add(sum, sparseInputGrad[i]);\n+            inputGrad[i] = sum;\n+        }\n+\n+        return inputGrad;","path":"src/NeuralNetworks/Layers/RoSAAdapter.cs","commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","original_commit_id":"c11235933bf7c341c8b4a223cf6ac11b757d1480","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Pack parameter gradients after backprop.**\n\nAfter Lines 543-549 you return the input gradient without ever rebuilding `ParameterGradients`. `LoRAAdapterBase` normally does this (see Lines 204-233 in `LoRAAdapterBase.cs`), but because you override `Backward` the vector stays stale. Optimizers pulling `GetParameterGradients()` will see zeros, so sparse weights never update. Please pack base, LoRA, and sparse gradients into `ParameterGradients` before returning.  \n\n\n```diff\n-        // 5. Sum input gradients from all three paths\n-        Tensor<T> inputGrad = new Tensor<T>(loraInputGrad.Shape);\n-        for (int i = 0; i < loraInputGrad.Length; i++)\n-        {\n-            T sum = NumOps.Add(loraInputGrad[i], baseInputGrad[i]);\n-            sum = NumOps.Add(sum, sparseInputGrad[i]);\n-            inputGrad[i] = sum;\n-        }\n-\n-        return inputGrad;\n+        // 5. Sum input gradients from all three paths\n+        Tensor<T> inputGrad = new Tensor<T>(loraInputGrad.Shape);\n+        for (int i = 0; i < loraInputGrad.Length; i++)\n+        {\n+            T sum = NumOps.Add(loraInputGrad[i], baseInputGrad[i]);\n+            sum = NumOps.Add(sum, sparseInputGrad[i]);\n+            inputGrad[i] = sum;\n+        }\n+\n+        // 6. Pack parameter gradients for optimizers\n+        ParameterGradients = new Vector<T>(ParameterCount);\n+        int gradIdx = 0;\n+\n+        if (!_freezeBaseLayer)\n+        {\n+            Vector<T> baseGrads = _baseLayer.GetParameterGradients();\n+            for (int i = 0; i < baseGrads.Length; i++)\n+            {\n+                ParameterGradients[gradIdx++] = baseGrads[i];\n+            }\n+        }\n+\n+        Vector<T> loraGrads = _loraLayer.GetParameterGradients();\n+        for (int i = 0; i < loraGrads.Length; i++)\n+        {\n+            ParameterGradients[gradIdx++] = loraGrads[i];\n+        }\n+\n+        if (_sparseGradients != null)\n+        {\n+            for (int i = 0; i < _sparseGradients.Rows; i++)\n+            {\n+                for (int j = 0; j < _sparseGradients.Columns; j++)\n+                {\n+                    ParameterGradients[gradIdx++] = _sparseGradients[i, j];\n+                }\n+            }\n+        }\n+\n+        return inputGrad;\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        // 5. Sum input gradients from all three paths\n        Tensor<T> inputGrad = new Tensor<T>(loraInputGrad.Shape);\n        for (int i = 0; i < loraInputGrad.Length; i++)\n        {\n            T sum = NumOps.Add(loraInputGrad[i], baseInputGrad[i]);\n            sum = NumOps.Add(sum, sparseInputGrad[i]);\n            inputGrad[i] = sum;\n        }\n\n        // 6. Pack parameter gradients for optimizers\n        ParameterGradients = new Vector<T>(ParameterCount);\n        int gradIdx = 0;\n\n        if (!_freezeBaseLayer)\n        {\n            Vector<T> baseGrads = _baseLayer.GetParameterGradients();\n            for (int i = 0; i < baseGrads.Length; i++)\n            {\n                ParameterGradients[gradIdx++] = baseGrads[i];\n            }\n        }\n\n        Vector<T> loraGrads = _loraLayer.GetParameterGradients();\n        for (int i = 0; i < loraGrads.Length; i++)\n        {\n            ParameterGradients[gradIdx++] = loraGrads[i];\n        }\n\n        if (_sparseGradients != null)\n        {\n            for (int i = 0; i < _sparseGradients.Rows; i++)\n            {\n                for (int j = 0; j < _sparseGradients.Columns; j++)\n                {\n                    ParameterGradients[gradIdx++] = _sparseGradients[i, j];\n                }\n            }\n        }\n\n        return inputGrad;\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/Layers/RoSAAdapter.cs around lines 542 to 551, you compute\nand return the input gradient but never rebuild the layer's ParameterGradients,\nso optimizers read stale zero gradients; after summing input gradients and\nbefore returning, pack the base, LoRA and sparse parameter gradients into the\nParameterGradients field exactly as LoRAAdapterBase does (see LoRAAdapterBase.cs\nlines 204-233): compute or copy each parameter's gradient vector (base grads,\nlora A/B grads, sparse grads) into the correct slices/order of\nParameterGradients with matching shapes/types, and update the property so\nGetParameterGradients() returns the combined gradient vector prior to returning\ninputGrad.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:32:39Z","updated_at":"2025-11-02T02:32:41Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106377","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106377"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106377"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106377/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":560,"original_start_line":542,"start_side":"RIGHT","line":603,"original_line":551,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":551,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106380","pull_request_review_id":3407993158,"id":2484106380,"node_id":"PRRC_kwDOKSXUF86UEHSM","diff_hunk":"@@ -0,0 +1,910 @@\n+using AiDotNet.Interfaces;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.NeuralNetworks.Layers;\n+\n+/// <summary>\n+/// S-LoRA adapter for scalable serving of thousands of concurrent LoRA adapters.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// S-LoRA (Scalable LoRA) is a system designed for efficient serving of many LoRA adapters simultaneously.\n+/// Published in November 2023, it addresses the challenge of deploying thousands of task-specific LoRA adapters\n+/// in production environments with limited GPU memory.\n+/// </para>\n+/// <para><b>For Beginners:</b> S-LoRA solves a real-world problem in production AI systems.\n+///\n+/// The problem:\n+/// - You have a large base model (like GPT or LLaMA)\n+/// - You want to serve thousands of different LoRA adapters (one per customer, task, or use case)\n+/// - Each adapter is small (few MB), but thousands of them won't fit in GPU memory\n+/// - Naive approaches either: load one adapter at a time (slow) or reserve memory for all (wasteful)\n+///\n+/// S-LoRA's solution:\n+/// - Unified memory pool: Dynamically manage adapter weights and cache together\n+/// - Batched computation: Process multiple adapters in parallel efficiently\n+/// - Adapter clustering: Group adapters by rank for optimized computation\n+/// - On-demand loading: Fetch adapters from CPU to GPU memory only when needed\n+///\n+/// Key features implemented:\n+/// 1. **Unified Memory Pool**: Single pool for adapter weights (no pre-allocation waste)\n+/// 2. **Adapter Clustering**: Group adapters by rank for batched computation\n+/// 3. **Dynamic Loading**: Load adapters on-demand, evict when not needed\n+/// 4. **Batched Forward Pass**: Process multiple requests with different adapters simultaneously\n+/// 5. **Memory Efficiency**: Serve 100x more adapters than naive approaches\n+///\n+/// Research Paper Reference:\n+/// \"S-LoRA: Serving Thousands of Concurrent LoRA Adapters\"\n+/// Ying Sheng, Shiyi Cao, et al. (November 2023)\n+/// arXiv:2311.03285\n+///\n+/// Performance (from paper):\n+/// - Throughput: 4x improvement over vLLM, 30x over HuggingFace PEFT\n+/// - Adapter capacity: 2,000+ concurrent adapters on single server\n+/// - Memory efficiency: 75-90% GPU memory utilization\n+/// - Scalability: Superlinear throughput scaling with more GPUs\n+///\n+/// Example usage:\n+/// ```csharp\n+/// // Create S-LoRA serving system for base layer\n+/// var sloraAdapter = new SLoRAAdapter&lt;double&gt;(baseLayer, rank: 8);\n+///\n+/// // Register multiple adapters for different tasks\n+/// sloraAdapter.RegisterAdapter(\"customer_1\", adapter1);\n+/// sloraAdapter.RegisterAdapter(\"customer_2\", adapter2);\n+/// sloraAdapter.RegisterAdapter(\"task_classification\", adapter3);\n+///\n+/// // Process batched requests efficiently\n+/// var outputs = sloraAdapter.BatchForward(inputs, adapterIds);\n+/// ```\n+///\n+/// When to use S-LoRA:\n+/// - Serving multiple LoRA adapters in production\n+/// - Multi-tenant AI systems (one adapter per tenant)\n+/// - Task-specific fine-tuning at scale\n+/// - Limited GPU memory but many adapters\n+/// - Need high throughput with many concurrent users\n+///\n+/// Differences from standard LoRA:\n+/// - Standard LoRA: Single adapter, simple forward/backward pass\n+/// - S-LoRA: Multiple adapters, optimized for concurrent serving, memory pooling\n+/// </para>\n+/// </remarks>\n+public class SLoRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Represents an adapter entry in the memory pool.\n+    /// </summary>\n+    private class AdapterEntry\n+    {\n+        /// <summary>\n+        /// The adapter's unique identifier.\n+        /// </summary>\n+        public string Id { get; set; }\n+\n+        /// <summary>\n+        /// The LoRA layer for this adapter.\n+        /// </summary>\n+        public LoRALayer<T> Layer { get; set; }\n+\n+        /// <summary>\n+        /// The rank of this adapter.\n+        /// </summary>\n+        public int Rank { get; set; }\n+\n+        /// <summary>\n+        /// Whether this adapter is currently loaded in \"GPU memory\" (in-memory cache).\n+        /// </summary>\n+        public bool IsLoaded { get; set; }\n+\n+        /// <summary>\n+        /// Last access timestamp for LRU eviction.\n+        /// </summary>\n+        public long LastAccess { get; set; }\n+\n+        /// <summary>\n+        /// Reference count for active requests using this adapter.\n+        /// </summary>\n+        public int ReferenceCount { get; set; }\n+\n+        /// <summary>\n+        /// Initializes a new adapter entry.\n+        /// </summary>\n+        public AdapterEntry(string id, LoRALayer<T> layer, int rank)\n+        {\n+            Id = id ?? string.Empty;\n+            Layer = layer;\n+            Rank = rank;\n+            IsLoaded = false;\n+            LastAccess = 0;\n+            ReferenceCount = 0;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Unified memory pool storing all registered adapters.\n+    /// </summary>\n+    /// <remarks>\n+    /// This simulates S-LoRA's unified memory pool where all adapters reside in CPU memory\n+    /// and are dynamically loaded to GPU memory based on demand.\n+    /// </remarks>\n+    private readonly Dictionary<string, AdapterEntry> _adapterPool;\n+\n+    /// <summary>\n+    /// Adapters currently loaded in \"GPU memory\" (in-memory cache).\n+    /// </summary>\n+    private readonly Dictionary<string, AdapterEntry> _loadedAdapters;\n+\n+    /// <summary>\n+    /// Adapters clustered by rank for efficient batched computation.\n+    /// </summary>\n+    private readonly Dictionary<int, List<string>> _rankClusters;\n+\n+    /// <summary>\n+    /// Maximum number of adapters that can be loaded simultaneously (simulates GPU memory limit).\n+    /// </summary>\n+    private readonly int _maxLoadedAdapters;\n+\n+    /// <summary>\n+    /// Current timestamp for LRU eviction policy.\n+    /// </summary>\n+    private long _timestamp;\n+\n+    /// <summary>\n+    /// Gets the total number of registered adapters in the pool.\n+    /// </summary>\n+    /// <remarks>\n+    /// This represents all adapters in the system, including those not currently loaded.\n+    /// S-LoRA can serve thousands of adapters from a unified pool.\n+    /// </remarks>\n+    public int TotalAdapterCount => _adapterPool.Count;\n+\n+    /// <summary>\n+    /// Gets the number of adapters currently loaded in memory.\n+    /// </summary>\n+    /// <remarks>\n+    /// This represents the \"hot\" adapters actively being used or cached.\n+    /// S-LoRA dynamically loads/evicts adapters based on request patterns.\n+    /// </remarks>\n+    public int LoadedAdapterCount => _loadedAdapters.Count;\n+\n+    /// <summary>\n+    /// Gets the maximum number of adapters that can be loaded simultaneously.\n+    /// </summary>\n+    /// <remarks>\n+    /// This simulates GPU memory constraints. S-LoRA's unified paging mechanism\n+    /// efficiently manages this limited resource.\n+    /// </remarks>\n+    public int MaxLoadedAdapters => _maxLoadedAdapters;\n+\n+    /// <summary>\n+    /// Gets the number of rank clusters for batched computation optimization.\n+    /// </summary>\n+    /// <remarks>\n+    /// Adapters with the same rank are clustered together for efficient batched computation.\n+    /// This is a key optimization in S-LoRA for heterogeneous adapter serving.\n+    /// </remarks>\n+    public int RankClusterCount => _rankClusters.Count;\n+\n+    /// <summary>\n+    /// Initializes a new S-LoRA adapter for scalable multi-adapter serving.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The base layer to adapt with S-LoRA.</param>\n+    /// <param name=\"rank\">The default rank for the primary LoRA decomposition.</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"maxLoadedAdapters\">Maximum number of adapters to keep loaded simultaneously (default: 100).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when maxLoadedAdapters is less than 1.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This creates an S-LoRA serving system for efficient multi-adapter deployment.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The shared base model that all adapters modify\n+    /// - rank: Default rank for new adapters (typical: 8-32)\n+    /// - alpha: Scaling factor for LoRA contributions\n+    /// - maxLoadedAdapters: How many adapters to cache in \"GPU memory\" (100 = good balance)\n+    /// - freezeBaseLayer: Lock base weights (true for serving, false for continued training)\n+    ///\n+    /// How S-LoRA works:\n+    /// 1. One base model shared across all adapters (memory efficient)\n+    /// 2. Thousands of small adapters registered in unified pool\n+    /// 3. Only popular adapters kept loaded in fast memory\n+    /// 4. Unpopular adapters evicted and loaded on-demand\n+    /// 5. Batched computation for multiple adapters simultaneously\n+    ///\n+    /// Example: Serving 10,000 customer-specific adapters:\n+    /// - Base model: 7B parameters (14 GB)\n+    /// - Each adapter: rank 16 (few MB)\n+    /// - Total pool: 10,000 adapters (few GB in CPU memory)\n+    /// - Loaded cache: 100 most-used adapters (hundreds of MB in GPU memory)\n+    /// - Result: Serve 10,000 adapters with GPU memory for 1 base model + 100 adapters!\n+    ///\n+    /// This is 100x more efficient than loading full fine-tuned models.\n+    /// </para>\n+    /// </remarks>\n+    public SLoRAAdapter(\n+        ILayer<T> baseLayer,\n+        int rank,\n+        double alpha = -1,\n+        int maxLoadedAdapters = 100,\n+        bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        if (maxLoadedAdapters < 1)\n+        {\n+            throw new ArgumentException(\"Max loaded adapters must be at least 1\", nameof(maxLoadedAdapters));\n+        }\n+\n+        _adapterPool = new Dictionary<string, AdapterEntry>();\n+        _loadedAdapters = new Dictionary<string, AdapterEntry>();\n+        _rankClusters = new Dictionary<int, List<string>>();\n+        _maxLoadedAdapters = maxLoadedAdapters;\n+        _timestamp = 0;\n+\n+        // Register the primary adapter (from base class)\n+        RegisterAdapter(\"primary\", _loraLayer, rank);\n+        LoadAdapter(\"primary\");\n+    }\n+\n+    /// <summary>\n+    /// Registers a new adapter in the unified memory pool.\n+    /// </summary>\n+    /// <param name=\"adapterId\">Unique identifier for this adapter.</param>\n+    /// <param name=\"loraLayer\">The LoRA layer to register.</param>\n+    /// <param name=\"rank\">The rank of this adapter.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when adapterId or loraLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when an adapter with this ID already exists.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// This method adds a new adapter to S-LoRA's unified memory pool. The adapter is not immediately\n+    /// loaded into GPU memory but is available for on-demand loading when needed.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is like adding a new customer or task-specific adapter to your system.\n+    ///\n+    /// What happens when you register an adapter:\n+    /// 1. Adapter stored in CPU memory pool (cheap storage)\n+    /// 2. Added to rank cluster for batched computation optimization\n+    /// 3. Not loaded to GPU yet (only loaded when first used)\n+    /// 4. Can register thousands of adapters this way\n+    ///\n+    /// Example: Multi-tenant SaaS application\n+    /// ```csharp\n+    /// var slora = new SLoRAAdapter&lt;double&gt;(baseModel, rank: 8, maxLoadedAdapters: 100);\n+    ///\n+    /// // Register 1000 customer adapters\n+    /// for (int i = 0; i &lt; 1000; i++)\n+    /// {\n+    ///     var adapter = LoadCustomerAdapter(i);\n+    ///     slora.RegisterAdapter($\"customer_{i}\", adapter, rank: 8);\n+    /// }\n+    ///\n+    /// // All 1000 adapters registered, but only 100 will be loaded at once\n+    /// // Popular customers get fast GPU-cached access\n+    /// // Inactive customers loaded on-demand from CPU pool\n+    /// ```\n+    ///\n+    /// This enables serving far more adapters than GPU memory allows!\n+    /// </para>\n+    /// </remarks>\n+    public void RegisterAdapter(string adapterId, LoRALayer<T> loraLayer, int rank)\n+    {\n+        if (adapterId == null)\n+        {\n+            throw new ArgumentNullException(nameof(adapterId));\n+        }\n+\n+        if (loraLayer == null)\n+        {\n+            throw new ArgumentNullException(nameof(loraLayer));\n+        }\n+\n+        if (_adapterPool.ContainsKey(adapterId))\n+        {\n+            throw new ArgumentException($\"Adapter with ID '{adapterId}' already exists\", nameof(adapterId));\n+        }\n+\n+        // Create adapter entry\n+        var entry = new AdapterEntry(adapterId, loraLayer, rank);\n+        _adapterPool[adapterId] = entry;\n+\n+        // Add to rank cluster for batched computation\n+        if (!_rankClusters.ContainsKey(rank))\n+        {\n+            _rankClusters[rank] = new List<string>();\n+        }\n+        _rankClusters[rank].Add(adapterId);\n+    }\n+\n+    /// <summary>\n+    /// Loads an adapter from the pool into active memory (simulates GPU loading).\n+    /// </summary>\n+    /// <param name=\"adapterId\">The ID of the adapter to load.</param>\n+    /// <exception cref=\"ArgumentException\">Thrown when adapter ID is not found in pool.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// This method simulates S-LoRA's dynamic adapter loading from CPU to GPU memory.\n+    /// If the loaded adapter cache is full, it evicts the least recently used adapter.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This moves an adapter from slow storage to fast cache.\n+    ///\n+    /// In S-LoRA's architecture:\n+    /// - CPU memory: All adapters stored here (slow but large capacity)\n+    /// - GPU memory: Hot adapters cached here (fast but limited capacity)\n+    ///\n+    /// Loading process:\n+    /// 1. Check if adapter already loaded (if yes, update access time and return)\n+    /// 2. Check if cache is full (if yes, evict least recently used adapter)\n+    /// 3. Load adapter into cache\n+    /// 4. Mark as loaded and update access timestamp\n+    ///\n+    /// LRU eviction policy:\n+    /// - Adapters with oldest last access time evicted first\n+    /// - Adapters with active references (in-flight requests) never evicted\n+    /// - This keeps popular adapters hot in cache\n+    ///\n+    /// Example: Customer request patterns\n+    /// ```\n+    /// Time 0: Customer A requests (load adapter A)\n+    /// Time 1: Customer B requests (load adapter B)\n+    /// ...\n+    /// Time 99: Customer Z requests (load adapter Z, cache now full at 100)\n+    /// Time 100: Customer AA requests (evict least-used, load adapter AA)\n+    /// Time 101: Customer A requests again (adapter A was evicted, reload)\n+    /// ```\n+    ///\n+    /// Popular customers stay cached, inactive ones evicted automatically!\n+    /// </para>\n+    /// </remarks>\n+    public void LoadAdapter(string adapterId)\n+    {\n+        if (!_adapterPool.ContainsKey(adapterId))\n+        {\n+            throw new ArgumentException($\"Adapter '{adapterId}' not found in pool\", nameof(adapterId));\n+        }\n+\n+        var entry = _adapterPool[adapterId];\n+\n+        // If already loaded, just update access time\n+        if (entry.IsLoaded)\n+        {\n+            entry.LastAccess = ++_timestamp;\n+            return;\n+        }\n+\n+        // Evict if cache is full\n+        while (_loadedAdapters.Count >= _maxLoadedAdapters)\n+        {\n+            EvictLRUAdapter();\n+        }","path":"src/NeuralNetworks/Layers/SLoRAAdapter.cs","commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","original_commit_id":"c11235933bf7c341c8b4a223cf6ac11b757d1480","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Prevent the eviction loop from hanging.**  \nWhen the cache is full and every loaded adapter has `ReferenceCount > 0`, `EvictLRUAdapter()` walks the cache, finds nothing to evict, and returns without touching `_loadedAdapters`. The `while (_loadedAdapters.Count >= _maxLoadedAdapters)` loop never progresses, so `LoadAdapter` spins forever and the adapter is never loaded. Have `EvictLRUAdapter` report whether it actually evicted something and break or throw when eviction is impossible, e.g.\n\n```diff\n-        while (_loadedAdapters.Count >= _maxLoadedAdapters)\n-        {\n-            EvictLRUAdapter();\n-        }\n+        while (_loadedAdapters.Count >= _maxLoadedAdapters)\n+        {\n+            if (!EvictLRUAdapter())\n+            {\n+                throw new InvalidOperationException(\"All loaded adapters are in-use; cannot load another adapter.\");\n+            }\n+        }\n```\n\nand return a `bool` from `EvictLRUAdapter`. Otherwise any sustained burst of requests that pins the cache will deadlock the loader.  \n\n\n\nAlso applies to: 428-452\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/Layers/SLoRAAdapter.cs around lines 377-381 and 428-452,\nthe eviction loop can spin forever when all loaded adapters have ReferenceCount\n> 0 because EvictLRUAdapter() never indicates failure; change EvictLRUAdapter to\nreturn a bool (true if it actually removed an adapter, false if nothing could be\nevicted), update its internal logic to return false when no candidate is\nremovable, and update callers (the while loop in LoadAdapter and any other\ninvocations in the 428-452 region) to break and throw a clear exception (or\notherwise bail out) when EvictLRUAdapter returns false instead of looping\nindefinitely; ensure the new signature is used consistently and add a concise\nerror message indicating the cache is pinned so callers can handle the failure.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:32:39Z","updated_at":"2025-11-02T02:32:41Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106380","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106380"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484106380"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484106380/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":377,"original_start_line":377,"start_side":"RIGHT","line":388,"original_line":381,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":381,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118382","pull_request_review_id":3408015014,"id":2484118382,"node_id":"PRRC_kwDOKSXUF86UEKNu","diff_hunk":"@@ -0,0 +1,529 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// Adaptive Low-Rank Adaptation (AdaLoRA) adapter that dynamically allocates parameter budgets among weight matrices.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// AdaLoRA improves upon standard LoRA by dynamically adjusting the rank allocation based on importance scores.\n+/// Instead of using a fixed rank for all weight matrices, AdaLoRA:\n+/// - Starts with a maximum rank and adaptively reduces it during training\n+/// - Computes importance scores for each singular value component\n+/// - Prunes less important components to focus parameter budget on critical adaptations\n+/// - Allows different layers to have different effective ranks\n+/// </para>\n+/// <para>\n+/// This leads to more efficient parameter usage compared to fixed-rank LoRA, especially for large models\n+/// where some layers need more adaptation capacity than others.\n+/// </para>\n+/// <para><b>For Beginners:</b> AdaLoRA is like smart LoRA that learns which parts of the adaptation matter most.\n+///\n+/// Think of standard LoRA as giving every layer the same budget (rank=8 everywhere).\n+/// AdaLoRA is smarter:\n+/// - Some layers get more budget (rank=16) because they're important for the task\n+/// - Other layers get less budget (rank=2) because small changes are enough\n+/// - The model learns this automatically during training\n+///\n+/// How it works:\n+/// 1. Start with a large rank (e.g., maxRank=32)\n+/// 2. During training, track how important each component is\n+/// 3. Prune components with low importance scores\n+/// 4. Focus parameters on what actually helps\n+///\n+/// Benefits:\n+/// - More parameter-efficient than fixed-rank LoRA\n+/// - Better performance with same parameter budget\n+/// - Automatically finds optimal rank per layer\n+///\n+/// Reference: \"Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning\" (ICLR 2023)\n+/// https://arxiv.org/abs/2303.10512\n+/// </para>\n+/// </remarks>\n+public class AdaLoRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Maximum possible rank for this adapter.\n+    /// </summary>\n+    /// <remarks>\n+    /// The adapter starts with this rank and may reduce it during training through pruning.\n+    /// This is the upper bound on the number of singular value components.\n+    /// </remarks>\n+    private readonly int _maxRank;\n+\n+    /// <summary>\n+    /// Current active rank after pruning.\n+    /// </summary>\n+    /// <remarks>\n+    /// This represents the number of singular value components currently being used.\n+    /// It starts at maxRank and decreases as low-importance components are pruned.\n+    /// </remarks>\n+    private int _currentRank;\n+\n+    /// <summary>\n+    /// Importance scores for each singular value component.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Each score represents how important that singular value is for the adaptation.\n+    /// Higher scores indicate more important components that should be retained.\n+    /// These scores are updated during training based on gradient magnitudes.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Think of these as \"usefulness ratings\" for each component.\n+    /// Components with high scores are helping a lot, low scores mean they're not doing much.\n+    /// We keep the high-scoring components and prune the low-scoring ones.\n+    /// </para>\n+    /// </remarks>\n+    private Vector<T> _importanceScores;\n+\n+    /// <summary>\n+    /// Threshold for pruning singular values based on importance.\n+    /// </summary>\n+    /// <remarks>\n+    /// Components with importance scores below this threshold are candidates for pruning.\n+    /// This value is typically set as a small fraction (e.g., 0.01 to 0.1).\n+    /// </remarks>\n+    private readonly double _rankPruningThreshold;\n+\n+    /// <summary>\n+    /// Exponential moving average factor for importance score updates.\n+    /// </summary>\n+    /// <remarks>\n+    /// Controls how quickly importance scores adapt to new gradient information.\n+    /// Typical values: 0.9 to 0.99 (higher = more smoothing, lower = faster adaptation)\n+    /// </remarks>\n+    private readonly double _importanceScoreEMA;\n+\n+    /// <summary>\n+    /// Minimum rank to maintain (prevents pruning below this threshold).\n+    /// </summary>\n+    private readonly int _minRank;\n+\n+    /// <summary>\n+    /// Number of training steps between rank pruning operations.\n+    /// </summary>\n+    private readonly int _pruningInterval;\n+\n+    /// <summary>\n+    /// Current training step counter.\n+    /// </summary>\n+    private int _stepCount;\n+\n+    /// <summary>\n+    /// Gets the maximum rank this adapter can use.\n+    /// </summary>\n+    public int MaxRank => _maxRank;\n+\n+    /// <summary>\n+    /// Gets the current active rank after pruning.\n+    /// </summary>\n+    public int CurrentRank => _currentRank;\n+\n+    /// <summary>\n+    /// Gets a copy of the current importance scores.\n+    /// </summary>\n+    public Vector<T> GetImportanceScores() => _importanceScores.Clone();\n+\n+    /// <summary>\n+    /// Initializes a new AdaLoRA adapter with adaptive rank allocation.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with AdaLoRA.</param>\n+    /// <param name=\"maxRank\">The maximum rank for the LoRA decomposition.</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to maxRank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <param name=\"rankPruningThreshold\">Threshold for pruning based on importance scores (default: 0.05).</param>\n+    /// <param name=\"minRank\">Minimum rank to maintain after pruning (default: 1).</param>\n+    /// <param name=\"pruningInterval\">Number of steps between pruning operations (default: 100).</param>\n+    /// <param name=\"importanceScoreEMA\">EMA factor for importance score updates (default: 0.95).</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when rank parameters are invalid.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This creates an AdaLoRA adapter with smart rank allocation.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to adapt (typically Dense or FullyConnected)\n+    /// - maxRank: Start with this many components (will prune down during training)\n+    /// - alpha: How strong the adaptation is\n+    /// - freezeBaseLayer: Lock the original weights (usually true for efficiency)\n+    /// - rankPruningThreshold: How unimportant a component must be to get pruned (0.05 = bottom 5%)\n+    /// - minRank: Never prune below this rank (safety net)\n+    /// - pruningInterval: How often to check for pruning (in training steps)\n+    /// - importanceScoreEMA: How smooth importance tracking is (higher = more stable)\n+    ///\n+    /// The adapter will automatically adjust its rank during training to focus parameters\n+    /// on the most important components.\n+    /// </para>\n+    /// </remarks>\n+    public AdaLoRAAdapter(\n+        ILayer<T> baseLayer,\n+        int maxRank,\n+        double alpha = -1,\n+        bool freezeBaseLayer = true,\n+        double rankPruningThreshold = 0.05,\n+        int minRank = 1,\n+        int pruningInterval = 100,\n+        double importanceScoreEMA = 0.95)\n+        : base(baseLayer, maxRank, alpha, freezeBaseLayer)\n+    {\n+        if (minRank < 1)\n+        {\n+            throw new ArgumentException(\"Minimum rank must be at least 1\", nameof(minRank));\n+        }\n+\n+        if (minRank > maxRank)\n+        {\n+            throw new ArgumentException($\"Minimum rank ({minRank}) cannot exceed maximum rank ({maxRank})\", nameof(minRank));\n+        }\n+\n+        if (rankPruningThreshold <= 0 || rankPruningThreshold >= 1)\n+        {\n+            throw new ArgumentException(\"Rank pruning threshold must be between 0 and 1\", nameof(rankPruningThreshold));\n+        }\n+\n+        if (importanceScoreEMA <= 0 || importanceScoreEMA >= 1)\n+        {\n+            throw new ArgumentException(\"Importance score EMA factor must be between 0 and 1\", nameof(importanceScoreEMA));\n+        }\n+\n+        _maxRank = maxRank;\n+        _currentRank = maxRank;\n+        _rankPruningThreshold = rankPruningThreshold;\n+        _minRank = minRank;\n+        _pruningInterval = pruningInterval;\n+        _importanceScoreEMA = importanceScoreEMA;\n+        _stepCount = 0;\n+\n+        // Initialize importance scores (start with uniform importance)\n+        _importanceScores = new Vector<T>(maxRank);\n+        T initialScore = NumOps.One;\n+        for (int i = 0; i < maxRank; i++)\n+        {\n+            _importanceScores[i] = initialScore;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass using only the top-k most important singular values.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Sum of base layer output and AdaLoRA output (using current rank).</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Unlike standard LoRA which uses all rank components, AdaLoRA only uses the currentRank\n+    /// most important components based on importance scores. This is more efficient and focuses\n+    /// computation on the most impactful adaptations.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This computes the output using only the important components.\n+    /// If we started with rank=32 but pruned to rank=8, we only use the top 8 most important\n+    /// singular values. This makes computation faster and more focused.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        // Forward through base layer\n+        Tensor<T> baseOutput = _baseLayer.Forward(input);\n+\n+        // Forward through LoRA layer (it will use all components, but we'll mask based on importance)\n+        Tensor<T> loraOutput = _loraLayer.Forward(input);\n+\n+        // If current rank < max rank, we need to mask the output\n+        // This is implicitly handled by the pruned matrices in the LoRA layer\n+        // For simplicity, we use the LoRA output as-is (pruning happens in UpdateParameters)\n+\n+        // Sum the outputs\n+        Tensor<T> result = new Tensor<T>(baseOutput.Shape);\n+        for (int i = 0; i < baseOutput.Length; i++)\n+        {\n+            result[i] = NumOps.Add(baseOutput[i], loraOutput[i]);\n+        }\n+\n+        return result;\n+    }\n+","path":"src/LoRA/Adapters/AdaLoRAAdapter.cs","commit_id":"fa815036ee570162056ba6aca773f34c80f36948","original_commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Pruning never takes effect‚Äîadapter stays full rank**\n\n`Forward` still uses the raw `_loraLayer` output, and `PruneRank` only trims bookkeeping (`_currentRank`, `_importanceScores`). None of the LoRA matrices are zeroed/masked, so even after pruning we continue to train and apply all `maxRank` components. The adapter therefore behaves exactly like vanilla LoRA despite advertising adaptive rank, which is a functional bug. Please either zero the columns/rows beyond `_currentRank` (both parameters and gradients) or gate the forward/backward contributions so that pruned components truly disappear from computation and parameter updates.  \n\n\n\nAlso applies to: 384-423\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/LoRA/Adapters/AdaLoRAAdapter.cs around lines 223 to 244 (and similarly\n384‚Äì423), Forward currently sums the raw _loraLayer output so pruning never\ntakes effect; update Forward to apply a runtime mask using _currentRank (or an\nimportance-based boolean mask) so that components beyond _currentRank contribute\nzero: either zero the corresponding columns/rows of the LoRA parameter matrices\nand their gradients when PruneRank is called, or multiply the loraOutput by a\nmask (and ensure the same mask is applied during Backward/UpdateParameters to\nprevent gradient updates to pruned components). Ensure the mask is computed from\n_currentRank/_importanceScores and applied to both forward outputs and\nparameter/gradient updates so pruned components are fully excluded from\ncomputation and training.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commit fa81503","created_at":"2025-11-02T02:41:59Z","updated_at":"2025-11-02T15:10:34Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484118382","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118382"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484118382"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118382/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":223,"original_start_line":223,"start_side":"RIGHT","line":244,"original_line":244,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":244,"position":244,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118383","pull_request_review_id":3408015014,"id":2484118383,"node_id":"PRRC_kwDOKSXUF86UEKNv","diff_hunk":"@@ -0,0 +1,629 @@\n+using AiDotNet.Interfaces;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// Chain-of-LoRA adapter that implements sequential composition of multiple LoRA adapters.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// Chain-of-LoRA (COLA) is an advanced LoRA technique that enables sequential composition\n+/// of multiple LoRA adaptations through an iterative optimization framework. Unlike standard\n+/// LoRA which applies a single low-rank adaptation, COLA builds a chain of adaptations where\n+/// each adapter is trained, merged into the model, and then a new adapter is initialized for\n+/// further refinement.\n+/// </para>\n+/// <para>\n+/// This approach bridges the performance gap between standard LoRA and full fine-tuning by\n+/// employing residual learning principles. Each iteration in the chain adds incremental\n+/// improvements to the model's task-specific performance without incurring additional\n+/// computational costs or memory overhead during inference.\n+/// </para>\n+/// <para><b>Key Concepts:</b>\n+///\n+/// <b>Sequential Adaptation:</b>\n+/// Chain-of-LoRA applies adaptations in sequence (Task A ‚Üí Task B ‚Üí Task C), where each\n+/// stage builds upon the previous one. This is inspired by the Frank-Wolfe optimization\n+/// algorithm, which makes greedy updates along the direction of maximum improvement.\n+///\n+/// <b>Merge and Re-initialize:</b>\n+/// After training each LoRA adapter, the learned weights are merged back into the base layer,\n+/// and a new LoRA adapter is initialized. This \"tying a knot\" process allows the model to\n+/// consolidate learned knowledge before adding new adaptations.\n+///\n+/// <b>Knowledge Preservation:</b>\n+/// By freezing the base layer and only training the LoRA components, the chain preserves\n+/// previously learned knowledge while allowing new task-specific adaptations. Each adapter\n+/// in the chain captures a specific aspect of the task or a refinement step.\n+///\n+/// <b>Incremental Fine-tuning Pipeline:</b>\n+/// COLA enables continual learning scenarios where tasks are presented sequentially, and\n+/// the model must adapt to new tasks while maintaining performance on previous ones.\n+/// </para>\n+/// <para><b>Benefits of Chain-of-LoRA:</b>\n+///\n+/// - <b>Better Performance:</b> Achieves up to 6.47% relative accuracy gain over standard LoRA\n+/// - <b>No Extra Overhead:</b> After merging, inference cost is identical to the base model\n+/// - <b>Modular Adaptation:</b> Each adapter can be trained, tested, and validated independently\n+/// - <b>Catastrophic Forgetting Mitigation:</b> Sequential merging helps preserve prior knowledge\n+/// - <b>Task Chaining:</b> Naturally supports multi-task learning and transfer learning scenarios\n+/// - <b>Flexible Deployment:</b> Can deploy the full chain or selected adapters as needed\n+/// </para>\n+/// <para><b>For Beginners:</b>\n+///\n+/// Imagine you're learning a complex skill in stages:\n+/// 1. First, you learn the basics (Adapter 1)\n+/// 2. Then you practice and the basics become automatic (Merge)\n+/// 3. Next, you learn intermediate techniques on top of the basics (Adapter 2)\n+/// 4. Again, you practice until they're automatic (Merge)\n+/// 5. Finally, you learn advanced skills building on everything before (Adapter 3)\n+///\n+/// Chain-of-LoRA works the same way: each adapter learns something new, then it's consolidated\n+/// into the model, and the next adapter can focus on the next refinement. This stepwise approach\n+/// often achieves better results than trying to learn everything at once.\n+/// </para>\n+/// <para><b>Research Reference:</b>\n+///\n+/// Based on \"Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning\"\n+/// (arXiv:2401.04151, January 2024). The paper demonstrates that sequential low-rank adaptations\n+/// can significantly improve task performance compared to single-stage LoRA, especially on\n+/// complex reasoning and multi-step tasks.\n+/// </para>\n+/// <para><b>Usage Example:</b>\n+/// <code>\n+/// // Create a chain with 3 sequential adaptations\n+/// var chain = new ChainLoRAAdapter&lt;double&gt;(baseLayer, rank: 8, chainLength: 3);\n+///\n+/// // Train first adapter on Task A\n+/// chain.SetActiveAdapterIndex(0);\n+/// TrainModel(chain, taskAData);\n+/// chain.MergeActiveAdapter(); // Consolidate Task A knowledge\n+///\n+/// // Train second adapter on Task B\n+/// chain.SetActiveAdapterIndex(1);\n+/// TrainModel(chain, taskBData);\n+/// chain.MergeActiveAdapter(); // Consolidate Task B knowledge\n+///\n+/// // Train third adapter on Task C\n+/// chain.SetActiveAdapterIndex(2);\n+/// TrainModel(chain, taskCData);\n+///\n+/// // Deploy: all adaptations are now part of the model\n+/// ILayer&lt;double&gt; finalLayer = chain.MergeToOriginalLayer();\n+/// </code>\n+/// </para>\n+/// </remarks>\n+public class ChainLoRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// The chain of LoRA adapters applied sequentially.\n+    /// </summary>\n+    private readonly List<LoRALayer<T>> _adapterChain;\n+\n+    /// <summary>\n+    /// The index of the currently active adapter being trained.\n+    /// </summary>\n+    private int _activeAdapterIndex;\n+\n+    /// <summary>\n+    /// Whether each adapter in the chain has been merged.\n+    /// </summary>\n+    private readonly List<bool> _mergedStatus;\n+\n+    /// <summary>\n+    /// The total length of the adapter chain.\n+    /// </summary>\n+    private readonly int _chainLength;\n+\n+    /// <summary>\n+    /// Gets the total number of adapters in the chain.\n+    /// </summary>\n+    /// <remarks>\n+    /// This represents the maximum number of sequential adaptation stages that can be applied.\n+    /// Each adapter can be trained independently and then merged before proceeding to the next.\n+    /// </remarks>\n+    public int ChainLength => _chainLength;\n+\n+    /// <summary>\n+    /// Gets the index of the currently active adapter (0-based).\n+    /// </summary>\n+    /// <remarks>\n+    /// The active adapter is the one currently being trained. Other adapters in the chain\n+    /// are either waiting to be trained (higher indices) or have been merged (lower indices).\n+    /// </remarks>\n+    public int ActiveAdapterIndex => _activeAdapterIndex;\n+\n+    /// <summary>\n+    /// Gets the list of LoRA adapters in the chain.\n+    /// </summary>\n+    /// <remarks>\n+    /// Each adapter in the chain represents one stage of sequential adaptation.\n+    /// Adapters are applied in order during forward passes.\n+    /// </remarks>\n+    public IReadOnlyList<LoRALayer<T>> AdapterChain => _adapterChain.AsReadOnly();\n+\n+    /// <summary>\n+    /// Gets the merged status of each adapter in the chain.\n+    /// </summary>\n+    /// <remarks>\n+    /// True indicates that an adapter has been merged into the base layer and should\n+    /// no longer contribute trainable parameters. Merged adapters still contribute\n+    /// to the forward pass until the entire chain is collapsed.\n+    /// </remarks>\n+    public IReadOnlyList<bool> MergedStatus => _mergedStatus.AsReadOnly();\n+\n+    /// <summary>\n+    /// Initializes a new Chain-of-LoRA adapter with the specified configuration.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with the LoRA chain.</param>\n+    /// <param name=\"rank\">The rank of each LoRA decomposition in the chain.</param>\n+    /// <param name=\"chainLength\">The number of sequential adapters in the chain (default: 3).</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor for each adapter (defaults to rank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training (default: true).</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when chainLength is less than 1.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// Creates a chain of LoRA adapters for sequential fine-tuning. Each adapter in the chain\n+    /// can be trained independently, merged into the model, and then the next adapter can be\n+    /// activated for further refinement.\n+    /// </para>\n+    /// <para><b>For Beginners:</b>\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to adapt (e.g., a dense or convolutional layer)\n+    /// - rank: How compressed each adapter is (lower = fewer parameters per stage)\n+    /// - chainLength: How many sequential adaptation stages you want (typical: 2-5)\n+    /// - alpha: Controls adaptation strength (usually equals rank)\n+    /// - freezeBaseLayer: Lock base weights to preserve pre-trained knowledge (recommended: true)\n+    ///\n+    /// Example: chainLength=3 means you can do three rounds of training and merging,\n+    /// allowing the model to incrementally improve on complex tasks.\n+    /// </para>\n+    /// </remarks>\n+    public ChainLoRAAdapter(\n+        ILayer<T> baseLayer,\n+        int rank,\n+        int chainLength = 3,\n+        double alpha = -1,\n+        bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        if (chainLength < 1)\n+        {\n+            throw new ArgumentException(\"Chain length must be at least 1\", nameof(chainLength));\n+        }\n+\n+        _chainLength = chainLength;\n+        _activeAdapterIndex = 0;\n+        _adapterChain = new List<LoRALayer<T>>(chainLength);\n+        _mergedStatus = new List<bool>(chainLength);\n+\n+        // Create the chain of LoRA adapters\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+\n+        for (int i = 0; i < chainLength; i++)\n+        {\n+            var adapter = new LoRALayer<T>(inputSize, outputSize, rank, alpha);\n+            _adapterChain.Add(adapter);\n+            _mergedStatus.Add(false);\n+        }\n+\n+        // Update parameter count to reflect all unmerged adapters\n+        UpdateParameterCount();\n+    }\n+\n+    /// <summary>\n+    /// Sets which adapter in the chain is currently active for training.\n+    /// </summary>\n+    /// <param name=\"index\">The 0-based index of the adapter to activate.</param>\n+    /// <exception cref=\"ArgumentOutOfRangeException\">Thrown when index is out of range.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// Only the active adapter receives gradient updates during training. Other adapters\n+    /// are either frozen (already merged) or inactive (waiting to be trained).\n+    /// </para>\n+    /// <para><b>For Beginners:</b>\n+    /// This is like choosing which stage of learning you're currently working on.\n+    /// Set to 0 for the first stage, 1 for the second, etc. Only that stage's adapter\n+    /// will be trained while the others remain frozen.\n+    /// </para>\n+    /// </remarks>\n+    public void SetActiveAdapterIndex(int index)\n+    {\n+        if (index < 0 || index >= _chainLength)\n+        {\n+            throw new ArgumentOutOfRangeException(nameof(index), $\"Index must be between 0 and {_chainLength - 1}\");\n+        }\n+\n+        _activeAdapterIndex = index;\n+    }\n+\n+    /// <summary>\n+    /// Merges the currently active adapter into the base layer representation.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This \"ties a knot\" in the chain by marking the active adapter as merged and frozen.\n+    /// The adapter's weights are conceptually incorporated into the model, allowing the\n+    /// next adapter in the chain to build upon this consolidated knowledge.\n+    /// </para>\n+    /// <para>\n+    /// Note: The actual weight merging into a single layer happens when MergeToOriginalLayer()\n+    /// is called. This method only marks the adapter as merged for training purposes.\n+    /// </para>\n+    /// <para><b>For Beginners:</b>\n+    /// After training an adapter stage, call this to \"lock it in\" before moving to the\n+    /// next stage. It's like saving your progress before starting the next level.\n+    /// </para>\n+    /// </remarks>\n+    public void MergeActiveAdapter()\n+    {\n+        if (_activeAdapterIndex < 0 || _activeAdapterIndex >= _chainLength)\n+        {\n+            throw new InvalidOperationException($\"Invalid active adapter index: {_activeAdapterIndex}\");\n+        }\n+\n+        _mergedStatus[_activeAdapterIndex] = true;\n+        UpdateParameterCount();\n+    }\n+\n+    /// <summary>\n+    /// Unmerges a previously merged adapter, making it trainable again.\n+    /// </summary>\n+    /// <param name=\"index\">The index of the adapter to unmerge.</param>\n+    /// <exception cref=\"ArgumentOutOfRangeException\">Thrown when index is out of range.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// This allows re-training a previously merged adapter if needed for iterative refinement.\n+    /// Useful for scenarios where you want to go back and adjust an earlier stage.\n+    /// </para>\n+    /// </remarks>\n+    public void UnmergeAdapter(int index)\n+    {\n+        if (index < 0 || index >= _chainLength)\n+        {\n+            throw new ArgumentOutOfRangeException(nameof(index), $\"Index must be between 0 and {_chainLength - 1}\");\n+        }\n+\n+        _mergedStatus[index] = false;\n+        UpdateParameterCount();\n+    }\n+\n+    /// <summary>\n+    /// Gets the number of adapters that have been merged.\n+    /// </summary>\n+    /// <returns>Count of merged adapters.</returns>\n+    public int GetMergedCount()\n+    {\n+        return _mergedStatus.Count(merged => merged);\n+    }\n+\n+    /// <summary>\n+    /// Gets the number of adapters that are still trainable (not merged).\n+    /// </summary>\n+    /// <returns>Count of unmerged adapters.</returns>\n+    public int GetTrainableAdapterCount()\n+    {\n+        return _mergedStatus.Count(merged => !merged);\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass through the base layer and all adapters in the chain.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Output with all adapter contributions summed.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The forward pass computes:\n+    /// output = base_layer(input) + adapter_0(input) + adapter_1(input) + ... + adapter_n(input)\n+    /// </para>\n+    /// <para>\n+    /// All adapters contribute to the output, regardless of merge status. Merged adapters\n+    /// are conceptually part of the model but still computed separately until final merging.\n+    /// </para>\n+    /// <para><b>For Beginners:</b>\n+    /// During inference or training, the input goes through the base layer and ALL adapters\n+    /// in the chain. Their outputs are added together to get the final result. This is how\n+    /// all the sequential adaptations combine to produce the improved output.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        // Forward through base layer\n+        Tensor<T> result = _baseLayer.Forward(input);\n+\n+        // Forward through each adapter in the chain and sum contributions\n+        foreach (var adapter in _adapterChain)\n+        {\n+            Tensor<T> adapterOutput = adapter.Forward(input);\n+\n+            // Add adapter contribution to result\n+            for (int i = 0; i < result.Length; i++)\n+            {\n+                result[i] = NumOps.Add(result[i], adapterOutput[i]);\n+            }\n+        }\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Performs the backward pass through all layers in the chain.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient flowing back from the next layer.</param>\n+    /// <returns>Gradient to pass to the previous layer.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Gradients flow through all adapters and the base layer. Only unmerged adapters\n+    /// and the base layer (if not frozen) receive parameter updates.\n+    /// </para>\n+    /// <para><b>For Beginners:</b>\n+    /// During learning, this figures out how to improve each adapter. Only the active,\n+    /// unmerged adapter gets updated - the others are frozen to preserve their knowledge.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        // Initialize input gradient accumulator\n+        Tensor<T> inputGrad = new Tensor<T>(GetInputShape());\n+\n+        // Backward through each adapter in the chain\n+        for (int i = 0; i < _adapterChain.Count; i++)\n+        {\n+            Tensor<T> adapterInputGrad = _adapterChain[i].Backward(outputGradient);\n+\n+            // Accumulate input gradients\n+            for (int j = 0; j < inputGrad.Length; j++)\n+            {\n+                inputGrad[j] = NumOps.Add(inputGrad[j], adapterInputGrad[j]);\n+            }\n+        }\n+\n+        // Backward through base layer if not frozen\n+        if (!_freezeBaseLayer)\n+        {\n+            Tensor<T> baseInputGrad = _baseLayer.Backward(outputGradient);\n+\n+            // Accumulate base layer gradients\n+            for (int j = 0; j < inputGrad.Length; j++)\n+            {\n+                inputGrad[j] = NumOps.Add(inputGrad[j], baseInputGrad[j]);\n+            }\n+        }\n+\n+        // Update parameter gradients\n+        UpdateParameterGradientsFromChain();\n+\n+        return inputGrad;\n+    }\n+\n+    /// <summary>\n+    /// Updates parameters using the specified learning rate.\n+    /// </summary>\n+    /// <param name=\"learningRate\">The learning rate for parameter updates.</param>\n+    /// <remarks>\n+    /// Only the active unmerged adapter receives updates. Merged adapters and the base layer\n+    /// (if frozen) do not receive parameter updates.\n+    /// </remarks>\n+    public override void UpdateParameters(T learningRate)\n+    {\n+        // Update base layer only if not frozen\n+        if (!_freezeBaseLayer)\n+        {\n+            _baseLayer.UpdateParameters(learningRate);\n+        }\n+\n+        // Update only the active unmerged adapter\n+        if (_activeAdapterIndex >= 0 && _activeAdapterIndex < _chainLength && !_mergedStatus[_activeAdapterIndex])\n+        {\n+            _adapterChain[_activeAdapterIndex].UpdateParameters(learningRate);\n+        }\n+\n+        // Update parameter vector\n+        UpdateParametersFromChain();\n+    }\n+\n+    /// <summary>\n+    /// Gets the current parameters as a vector.\n+    /// </summary>\n+    /// <returns>Vector containing parameters from base layer (if not frozen) and all unmerged adapters.</returns>\n+    public override Vector<T> GetParameters()\n+    {\n+        return Parameters.Clone();\n+    }\n+\n+    /// <summary>\n+    /// Sets the layer parameters from a vector.\n+    /// </summary>\n+    /// <param name=\"parameters\">Vector containing parameters.</param>\n+    /// <exception cref=\"ArgumentException\">Thrown when parameter count doesn't match.</exception>\n+    public override void SetParameters(Vector<T> parameters)\n+    {\n+        if (parameters.Length != ParameterCount)\n+        {\n+            throw new ArgumentException($\"Expected {ParameterCount} parameters, got {parameters.Length}\", nameof(parameters));\n+        }\n+\n+        Parameters = parameters.Clone();\n+        UpdateChainFromParameters();\n+    }\n+\n+    /// <summary>\n+    /// Merges all adapters in the chain into the original base layer.\n+    /// </summary>\n+    /// <returns>A new layer with all LoRA adaptations merged into the base weights.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This creates a single layer that includes all the sequential adaptations from the chain.\n+    /// The resulting layer has the same computational cost as the original base layer but\n+    /// includes all the learned improvements from each stage of the chain.\n+    /// </para>\n+    /// <para><b>For Beginners:</b>\n+    /// After training all stages of the chain, call this to create a final optimized layer.\n+    /// The result is a regular layer (no LoRA overhead) that performs as well as the full chain.\n+    /// Perfect for deployment when you want maximum speed with all the learned adaptations.\n+    /// </para>\n+    /// <para><b>Implementation Note:</b>\n+    /// This is a simplified implementation that returns the base layer. In a full implementation,\n+    /// you would merge all adapter weights into a cloned base layer. The merging strategy depends\n+    /// on the specific layer type (Dense, Convolutional, etc.).\n+    /// </para>\n+    /// </remarks>\n+    public override ILayer<T> MergeToOriginalLayer()\n+    {\n+        // Note: This is a simplified implementation that returns the base layer.\n+        // In a production implementation, you would:\n+        // 1. Clone the base layer\n+        // 2. For each adapter in the chain, compute the low-rank update (B √ó A)\n+        // 3. Scale by (alpha / rank)\n+        // 4. Add to the cloned layer's weights\n+        // 5. Return the merged layer\n+        //\n+        // The exact merging process depends on the base layer type and is typically\n+        // implemented by derived classes that specialize for specific layer types.\n+\n+        return _baseLayer;\n+    }\n+\n+    /// <summary>\n+    /// Resets the internal state of the base layer and all adapters in the chain.\n+    /// </summary>\n+    public override void ResetState()\n+    {\n+        _baseLayer.ResetState();\n+        foreach (var adapter in _adapterChain)\n+        {\n+            adapter.ResetState();\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Updates the parameter count based on current merge status.\n+    /// </summary>\n+    private void UpdateParameterCount()\n+    {\n+        int count = 0;\n+\n+        // Add base layer parameters if not frozen\n+        if (!_freezeBaseLayer)\n+        {\n+            count += _baseLayer.ParameterCount;\n+        }\n+\n+        // Add unmerged adapter parameters\n+        for (int i = 0; i < _chainLength; i++)\n+        {\n+            if (!_mergedStatus[i])\n+            {\n+                count += _adapterChain[i].ParameterCount;\n+            }\n+        }\n+\n+        Parameters = new Vector<T>(count);\n+        ParameterGradients = new Vector<T>(count);\n+    }\n+\n+    /// <summary>\n+    /// Updates the parameter vector from the current state of the chain.\n+    /// </summary>\n+    private void UpdateParametersFromChain()\n+    {\n+        int idx = 0;\n+\n+        // Pack base layer parameters if not frozen\n+        if (!_freezeBaseLayer)\n+        {\n+            Vector<T> baseParams = _baseLayer.GetParameters();\n+            for (int i = 0; i < baseParams.Length; i++)\n+            {\n+                Parameters[idx++] = baseParams[i];\n+            }\n+        }\n+\n+        // Pack unmerged adapter parameters\n+        for (int i = 0; i < _chainLength; i++)\n+        {\n+            if (!_mergedStatus[i])\n+            {\n+                Vector<T> adapterParams = _adapterChain[i].GetParameters();\n+                for (int j = 0; j < adapterParams.Length; j++)\n+                {\n+                    Parameters[idx++] = adapterParams[j];\n+                }\n+            }\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Updates the chain from the parameter vector.\n+    /// </summary>\n+    private void UpdateChainFromParameters()\n+    {\n+        int idx = 0;\n+\n+        // Unpack base layer parameters if not frozen\n+        if (!_freezeBaseLayer)\n+        {\n+            int baseParamCount = _baseLayer.ParameterCount;\n+            Vector<T> baseParams = new Vector<T>(baseParamCount);\n+            for (int i = 0; i < baseParamCount; i++)\n+            {\n+                baseParams[i] = Parameters[idx++];\n+            }\n+            _baseLayer.SetParameters(baseParams);","path":"src/LoRA/Adapters/ChainLoRAAdapter.cs","commit_id":"489180133c7a9db9460c84a6174f60284717b952","original_commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**ParameterCount is still the base single-adapter size**\n\n`UpdateParameterCount` resizes `Parameters`, but `ParameterCount` remains the inherited value (`_loraLayer.ParameterCount`). As soon as the chain holds more than one active adapter, `SetParameters` rejects valid vectors and `UpdateParameterGradientsFromChain` allocates a buffer that's too small. Track the aggregate count and override the property, e.g.:\n\n```diff\n-    private void UpdateParameterCount()\n+    private int _currentParameterCount;\n+    public override int ParameterCount => _currentParameterCount;\n+\n+    private void UpdateParameterCount()\n     {\n-        int count = 0;\n+        int count = 0;\n         ...\n-        Parameters = new Vector<T>(count);\n-        ParameterGradients = new Vector<T>(count);\n+        _currentParameterCount = count;\n+        Parameters = new Vector<T>(_currentParameterCount);\n+        ParameterGradients = new Vector<T>(_currentParameterCount);\n     }\n```\n\nAlso use `_currentParameterCount` anywhere you size gradient buffers so serialization stays consistent with the new layout.\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    /// </summary>\n    private int _currentParameterCount;\n    public override int ParameterCount => _currentParameterCount;\n\n    private void UpdateParameterCount()\n    {\n        int count = 0;\n\n        // Add base layer parameters if not frozen\n        if (!_freezeBaseLayer)\n        {\n            count += _baseLayer.ParameterCount;\n        }\n\n        // Add unmerged adapter parameters\n        for (int i = 0; i < _chainLength; i++)\n        {\n            if (!_mergedStatus[i])\n            {\n                count += _adapterChain[i].ParameterCount;\n            }\n        }\n\n        _currentParameterCount = count;\n        Parameters = new Vector<T>(_currentParameterCount);\n        ParameterGradients = new Vector<T>(_currentParameterCount);\n    }\n\n    /// <summary>\n    /// Updates the parameter vector from the current state of the chain.\n    /// </summary>\n    private void UpdateParametersFromChain()\n    {\n        int idx = 0;\n\n        // Pack base layer parameters if not frozen\n        if (!_freezeBaseLayer)\n        {\n            Vector<T> baseParams = _baseLayer.GetParameters();\n            for (int i = 0; i < baseParams.Length; i++)\n            {\n                Parameters[idx++] = baseParams[i];\n            }\n        }\n\n        // Pack unmerged adapter parameters\n        for (int i = 0; i < _chainLength; i++)\n        {\n            if (!_mergedStatus[i])\n            {\n                Vector<T> adapterParams = _adapterChain[i].GetParameters();\n                for (int j = 0; j < adapterParams.Length; j++)\n                {\n                    Parameters[idx++] = adapterParams[j];\n                }\n            }\n        }\n    }\n\n    /// <summary>\n    /// Updates the chain from the parameter vector.\n    /// </summary>\n    private void UpdateChainFromParameters()\n    {\n        int idx = 0;\n\n        // Unpack base layer parameters if not frozen\n        if (!_freezeBaseLayer)\n        {\n            int baseParamCount = _baseLayer.ParameterCount;\n            Vector<T> baseParams = new Vector<T>(baseParamCount);\n            for (int i = 0; i < baseParamCount; i++)\n            {\n                baseParams[i] = Parameters[idx++];\n            }\n            _baseLayer.SetParameters(baseParams);\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits 36ecbde to bf7f155","created_at":"2025-11-02T02:41:59Z","updated_at":"2025-11-02T03:36:14Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484118383","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118383"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484118383"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118383/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":555,"original_start_line":508,"start_side":"RIGHT","line":630,"original_line":579,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":579,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118384","pull_request_review_id":3408015014,"id":2484118384,"node_id":"PRRC_kwDOKSXUF86UEKNw","diff_hunk":"@@ -0,0 +1,767 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// DoRA (Weight-Decomposed Low-Rank Adaptation) adapter for parameter-efficient fine-tuning with improved stability.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// DoRA (Weight-Decomposed LoRA) extends standard LoRA by decomposing pre-trained weights into\n+/// magnitude and direction components, then applying LoRA only to the direction component.\n+/// This decomposition leads to more stable training and better convergence compared to standard LoRA.\n+/// </para>\n+/// <para>\n+/// <b>Mathematical Formulation:</b>\n+/// Given pre-trained weights W, DoRA decomposes them as:\n+/// - W = m * d, where m is magnitude (scalar per neuron) and d is direction (unit vector)\n+/// - W' = m * normalize(d + LoRA_delta)\n+/// - LoRA_delta = (alpha/rank) * B * A\n+///\n+/// This ensures that LoRA adaptations primarily affect the direction of weights, not their magnitude,\n+/// which improves training stability and convergence.\n+/// </para>\n+/// <para>\n+/// <b>Research Context:</b>\n+/// DoRA was published in February 2024 and presented as an ICML 2024 Oral paper.\n+/// In experiments on LLaMA-7B, DoRA achieved +3.7% improvement over standard LoRA.\n+/// The key insight is that separating magnitude and direction allows more stable gradient flow\n+/// and better control over the adaptation process.\n+/// </para>\n+/// <para>\n+/// <b>For Beginners:</b> DoRA is an improved version of LoRA that works better in practice.\n+///\n+/// Think of neural network weights as arrows:\n+/// - Each arrow has a length (magnitude) and a direction\n+/// - Standard LoRA adjusts both length and direction at the same time\n+/// - DoRA separates them: it keeps the length fixed and only adjusts the direction\n+/// - This makes training more stable and gives better results\n+///\n+/// Why this matters:\n+/// - More stable training (fewer divergences and NaN errors)\n+/// - Better final performance (+3.7% on LLaMA-7B)\n+/// - Same parameter efficiency as standard LoRA\n+/// - Slightly more computation (due to normalization), but worth it for the stability\n+///\n+/// When to use DoRA over standard LoRA:\n+/// - When training stability is important (large models, complex tasks)\n+/// - When you want the best possible fine-tuning results\n+/// - When you have the computational budget for normalization overhead\n+/// - When adapting very large pre-trained models (LLMs, large vision models)\n+/// </para>\n+/// <para>\n+/// <b>Reference:</b>\n+/// \"DoRA: Weight-Decomposed Low-Rank Adaptation\"\n+/// ICML 2024 Oral\n+/// https://arxiv.org/abs/2402.09353\n+/// </para>\n+/// </remarks>\n+public class DoRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Magnitude component of the decomposed weights (scalar per output neuron).\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// The magnitude vector stores the L2 norm of each weight vector (one per output neuron).\n+    /// During forward pass, this magnitude is applied after normalizing the direction vectors.\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This stores the \"strength\" of each output neuron.\n+    /// When we decompose weights into magnitude and direction, this is the magnitude part.\n+    /// Each output neuron gets one magnitude value.\n+    /// </para>\n+    /// </remarks>\n+    private Vector<T> _magnitude;\n+\n+    /// <summary>\n+    /// Gradients for the magnitude component, computed during backpropagation.\n+    /// </summary>\n+    private Vector<T>? _magnitudeGradient;\n+\n+    /// <summary>\n+    /// Cached normalized direction from the last forward pass, used in backpropagation.\n+    /// </summary>\n+    private Matrix<T>? _lastNormalizedDirection;\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// DoRA adds the magnitude parameters (one per output neuron) to the standard LoRA parameters.\n+    /// Total = (base layer parameters if not frozen) + LoRA parameters + magnitude parameters.\n+    /// </para>\n+    /// </remarks>\n+    public override int ParameterCount\n+    {\n+        get\n+        {\n+            int baseCount = _freezeBaseLayer ? 0 : _baseLayer.ParameterCount;\n+            int loraCount = _loraLayer.ParameterCount;\n+            int magnitudeCount = _magnitude.Length;\n+            return baseCount + loraCount + magnitudeCount;\n+        }","path":"src/LoRA/Adapters/DoRAAdapter.cs","commit_id":"489180133c7a9db9460c84a6174f60284717b952","original_commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Guard `ParameterCount` against uninitialised state.**\n\nDuring the base constructor call, `LoRAAdapterBase` queries `ParameterCount` before `_magnitude` has been initialised, so `_magnitude.Length` throws a `NullReferenceException`. As written, the adapter cannot be constructed. Please make the getter resilient to the pre-initialisation phase.\n\n```diff\n-    public override int ParameterCount\n-    {\n-        get\n-        {\n-            int baseCount = _freezeBaseLayer ? 0 : _baseLayer.ParameterCount;\n-            int loraCount = _loraLayer.ParameterCount;\n-            int magnitudeCount = _magnitude.Length;\n-            return baseCount + loraCount + magnitudeCount;\n-        }\n-    }\n+    public override int ParameterCount\n+    {\n+        get\n+        {\n+            int baseCount = _freezeBaseLayer ? 0 : _baseLayer.ParameterCount;\n+            int loraCount = _loraLayer?.ParameterCount ?? 0;\n+            int magnitudeCount = _magnitude?.Length ?? GetOutputShape()[0];\n+            return baseCount + loraCount + magnitudeCount;\n+        }\n+    }\n```\n\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/LoRA/Adapters/DoRAAdapter.cs around lines 97-105, the ParameterCount\ngetter dereferences fields that may be null during base-construction (e.g.\n_magnitude.Length) causing a NullReferenceException; change the getter to be\ndefensive by treating uninitialized fields as zero ‚Äî e.g. use null checks or\nnull-coalescing (/_?.Length/_ ?? 0) for _magnitude, and guard _baseLayer and\n_loraLayer similarly so the getter returns baseCount + loraCount +\nmagnitudeCount with any missing piece counted as 0 until fully initialized.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commit fa81503","created_at":"2025-11-02T02:42:00Z","updated_at":"2025-11-02T15:10:35Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484118384","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118384"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484118384"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118384/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":97,"original_start_line":97,"start_side":"RIGHT","line":105,"original_line":105,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":105,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118388","pull_request_review_id":3408015014,"id":2484118388,"node_id":"PRRC_kwDOKSXUF86UEKN0","diff_hunk":"@@ -0,0 +1,1116 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// DVoRA (DoRA + VeRA) adapter - combines DoRA's magnitude-direction decomposition with VeRA's extreme parameter efficiency.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// DVoRA achieves the best of both worlds by:\n+/// - Applying DoRA's magnitude-direction decomposition for training stability\n+/// - Using VeRA's shared frozen matrices and scaling vectors for extreme parameter efficiency\n+/// - Applying the VeRA adaptation only to the direction component (not the magnitude)\n+/// </para>\n+/// <para>\n+/// <b>Mathematical Formulation:</b>\n+/// Given pre-trained weights W, DVoRA:\n+/// 1. Decomposes: W = m * d (magnitude and direction)\n+/// 2. Applies VeRA to direction: d' = d + d_scale * (B * A * input) * b_scale\n+/// 3. Normalizes direction: d_norm = d' / ||d'||\n+/// 4. Recomposes: W' = m * d_norm\n+///\n+/// Where:\n+/// - m: magnitude vector (trainable)\n+/// - d: direction matrix (normalized weight vectors)\n+/// - A, B: shared frozen random matrices (VeRA style)\n+/// - d_scale, b_scale: per-layer trainable scaling vectors (VeRA style)\n+/// </para>\n+/// <para>\n+/// <b>Research Context:</b>\n+/// DVoRA scores 5.0 vs VeRA's 4.3 (improvement of 16%) while maintaining ultra-low parameter counts.\n+/// It combines DoRA's superior training stability with VeRA's extreme parameter efficiency.\n+/// </para>\n+/// <para>\n+/// <b>For Beginners:</b> DVoRA is the ultimate parameter-efficient adapter.\n+///\n+/// Think of it as a hybrid technique:\n+/// - From DoRA: Separate magnitude (strength) from direction for stability\n+/// - From VeRA: Use shared random matrices and tiny scaling vectors for efficiency\n+/// - The magic: Apply VeRA's adaptation only to the direction, not the magnitude\n+///\n+/// Parameter comparison for 1000x1000 layer with rank=8:\n+/// - Full fine-tuning: 1,000,000 parameters\n+/// - Standard LoRA: 16,000 parameters (98.4% reduction)\n+/// - DoRA: 17,000 parameters (LoRA + magnitude vector)\n+/// - VeRA: 1,600 parameters (99.84% reduction)\n+/// - DVoRA: ~1,600 parameters (same as VeRA!) but with better performance (5.0 vs 4.3)\n+///\n+/// Benefits:\n+/// - ‚úÖ Extremely parameter-efficient (10x fewer than standard LoRA, same as VeRA)\n+/// - ‚úÖ Better performance than VeRA alone (5.0 vs 4.3 score)\n+/// - ‚úÖ Training stability from DoRA's magnitude-direction decomposition\n+/// - ‚úÖ Shared matrices reduce storage when adapting many layers\n+/// - ‚úÖ Best choice for extreme memory constraints with quality requirements\n+///\n+/// Trade-offs:\n+/// - ‚ö†Ô∏è Requires shared matrix initialization before use\n+/// - ‚ö†Ô∏è Slightly more computation than VeRA (due to normalization)\n+/// - ‚ö†Ô∏è More complex than standard adapters (combines two techniques)\n+///\n+/// When to use DVoRA:\n+/// - Extreme memory constraints but need better quality than VeRA\n+/// - Mobile/edge deployment with limited resources\n+/// - Fine-tuning many layers efficiently\n+/// - When you want the absolute best parameter efficiency + quality balance\n+/// </para>\n+/// <para>\n+/// <b>References:</b>\n+/// - DoRA: \"Weight-Decomposed Low-Rank Adaptation\" (ICML 2024 Oral)\n+/// - VeRA: \"Vector-based Random Matrix Adaptation\"\n+/// - DVoRA: Combines both techniques for optimal efficiency and performance\n+/// </para>\n+/// </remarks>\n+public class DVoRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Shared frozen random matrix A (inputSize √ó rank) used by all DVoRA adapters.\n+    /// </summary>\n+    /// <remarks>\n+    /// This matrix is initialized once globally and shared across all DVoRA layers.\n+    /// It is NEVER trained - it remains frozen at its random initialization values.\n+    /// This is the VeRA component of DVoRA.\n+    /// </remarks>\n+    private static Matrix<T>? _sharedMatrixA;\n+\n+    /// <summary>\n+    /// Shared frozen random matrix B (rank √ó outputSize) used by all DVoRA adapters.\n+    /// </summary>\n+    /// <remarks>\n+    /// This matrix is initialized once globally and shared across all DVoRA layers.\n+    /// It is NEVER trained - it remains frozen at its random initialization values.\n+    /// This is the VeRA component of DVoRA.\n+    /// </remarks>\n+    private static Matrix<T>? _sharedMatrixB;\n+\n+    /// <summary>\n+    /// Lock object for thread-safe shared matrix initialization.\n+    /// </summary>\n+    private static readonly object _initLock = new object();\n+\n+    /// <summary>\n+    /// Magnitude component of the decomposed weights (scalar per output neuron).\n+    /// Trainable per-layer parameter.\n+    /// </summary>\n+    /// <remarks>\n+    /// The magnitude vector stores the L2 norm of each weight vector (one per output neuron).\n+    /// This is the DoRA component of DVoRA.\n+    /// </remarks>\n+    private Vector<T> _magnitude;\n+\n+    /// <summary>\n+    /// Scaling vector d (outputSize) - trainable per-layer parameter.\n+    /// </summary>\n+    /// <remarks>\n+    /// This vector scales the VeRA output on a per-dimension basis.\n+    /// This is the VeRA component of DVoRA.\n+    /// </remarks>\n+    private Vector<T> _scalingVectorD;\n+\n+    /// <summary>\n+    /// Scaling vector b (rank) - trainable per-layer parameter.\n+    /// </summary>\n+    /// <remarks>\n+    /// This vector scales the intermediate rank-dimensional representation.\n+    /// This is the VeRA component of DVoRA.\n+    /// </remarks>\n+    private Vector<T> _scalingVectorB;\n+\n+    /// <summary>\n+    /// Gradient for magnitude vector computed during backpropagation.\n+    /// </summary>\n+    private Vector<T>? _magnitudeGradient;\n+\n+    /// <summary>\n+    /// Gradient for scaling vector d computed during backpropagation.\n+    /// </summary>\n+    private Vector<T>? _scalingVectorDGradient;\n+\n+    /// <summary>\n+    /// Gradient for scaling vector b computed during backpropagation.\n+    /// </summary>\n+    private Vector<T>? _scalingVectorBGradient;\n+\n+    /// <summary>\n+    /// Cached normalized direction from the last forward pass, used in backpropagation.\n+    /// </summary>\n+    private Matrix<T>? _lastNormalizedDirection;\n+\n+    /// <summary>\n+    /// Stored input from the forward pass, needed for gradient computation.\n+    /// </summary>\n+    private Tensor<T>? _lastInput;\n+\n+    /// <summary>\n+    /// Stored intermediate value from forward pass, needed for backward pass.\n+    /// </summary>\n+    private Matrix<T>? _lastIntermediate;\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters.\n+    /// </summary>\n+    /// <remarks>\n+    /// DVoRA parameters = magnitude (outputSize) + d_scale (outputSize) + b_scale (rank).\n+    /// This is only slightly more than VeRA (adds magnitude vector) but much fewer than DoRA (no full LoRA matrices).\n+    /// </remarks>\n+    public override int ParameterCount\n+    {\n+        get\n+        {\n+            int dvoraParams = _magnitude.Length + _scalingVectorD.Length + _scalingVectorB.Length;\n+            return _freezeBaseLayer ? dvoraParams : (_baseLayer.ParameterCount + dvoraParams);\n+        }\n+    }","path":"src/LoRA/Adapters/DVoRAAdapter.cs","commit_id":"489180133c7a9db9460c84a6174f60284717b952","original_commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Fix `ParameterCount` initialisation and include LoRA slots.**\n\n`ParameterCount` is queried while `_magnitude`, `_scalingVectorD`, and `_scalingVectorB` are still null, which currently triggers a `NullReferenceException`. Moreover, the value you return omits `_loraLayer.ParameterCount`, yet the base constructor immediately copies LoRA parameters into the shared vector‚Äîthis mismatch causes index overruns and corrupts parameter bookkeeping. Please guard against the pre-initialised state and include every segment the base class expects.\n\n```diff\n-    public override int ParameterCount\n-    {\n-        get\n-        {\n-            int dvoraParams = _magnitude.Length + _scalingVectorD.Length + _scalingVectorB.Length;\n-            return _freezeBaseLayer ? dvoraParams : (_baseLayer.ParameterCount + dvoraParams);\n-        }\n-    }\n+    public override int ParameterCount\n+    {\n+        get\n+        {\n+            int baseCount = _freezeBaseLayer ? 0 : _baseLayer.ParameterCount;\n+            int loraCount = _loraLayer?.ParameterCount ?? 0;\n+            int outputSize = GetOutputShape()[0];\n+            int magnitudeCount = _magnitude?.Length ?? outputSize;\n+            int scalingDCount = _scalingVectorD?.Length ?? outputSize;\n+            int scalingBCount = _scalingVectorB?.Length ?? Rank;\n+            return baseCount + loraCount + magnitudeCount + scalingDCount + scalingBCount;\n+        }\n+    }\n```\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/LoRA/Adapters/DVoRAAdapter.cs around lines 168 to 175, ParameterCount\naccesses _magnitude, _scalingVectorD and _scalingVectorB while they may be null\nand also omits the LoRA layer slot count, causing NullReferenceException and\nincorrect total that breaks the base constructor's parameter indexing; fix by\nnull-guarding those arrays (treat null as length 0) and include\n_loraLayer.ParameterCount in the returned total in the same order/segments the\nbase class expects, and when _freezeBaseLayer is false include\n_baseLayer.ParameterCount as well so the total matches the shared parameter\nvector layout.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:42:00Z","updated_at":"2025-11-02T02:42:05Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484118388","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118388"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484118388"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118388/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":168,"original_start_line":168,"start_side":"RIGHT","line":175,"original_line":175,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":175,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118395","pull_request_review_id":3408015014,"id":2484118395,"node_id":"PRRC_kwDOKSXUF86UEKN7","diff_hunk":"@@ -0,0 +1,1116 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// DVoRA (DoRA + VeRA) adapter - combines DoRA's magnitude-direction decomposition with VeRA's extreme parameter efficiency.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// DVoRA achieves the best of both worlds by:\n+/// - Applying DoRA's magnitude-direction decomposition for training stability\n+/// - Using VeRA's shared frozen matrices and scaling vectors for extreme parameter efficiency\n+/// - Applying the VeRA adaptation only to the direction component (not the magnitude)\n+/// </para>\n+/// <para>\n+/// <b>Mathematical Formulation:</b>\n+/// Given pre-trained weights W, DVoRA:\n+/// 1. Decomposes: W = m * d (magnitude and direction)\n+/// 2. Applies VeRA to direction: d' = d + d_scale * (B * A * input) * b_scale\n+/// 3. Normalizes direction: d_norm = d' / ||d'||\n+/// 4. Recomposes: W' = m * d_norm\n+///\n+/// Where:\n+/// - m: magnitude vector (trainable)\n+/// - d: direction matrix (normalized weight vectors)\n+/// - A, B: shared frozen random matrices (VeRA style)\n+/// - d_scale, b_scale: per-layer trainable scaling vectors (VeRA style)\n+/// </para>\n+/// <para>\n+/// <b>Research Context:</b>\n+/// DVoRA scores 5.0 vs VeRA's 4.3 (improvement of 16%) while maintaining ultra-low parameter counts.\n+/// It combines DoRA's superior training stability with VeRA's extreme parameter efficiency.\n+/// </para>\n+/// <para>\n+/// <b>For Beginners:</b> DVoRA is the ultimate parameter-efficient adapter.\n+///\n+/// Think of it as a hybrid technique:\n+/// - From DoRA: Separate magnitude (strength) from direction for stability\n+/// - From VeRA: Use shared random matrices and tiny scaling vectors for efficiency\n+/// - The magic: Apply VeRA's adaptation only to the direction, not the magnitude\n+///\n+/// Parameter comparison for 1000x1000 layer with rank=8:\n+/// - Full fine-tuning: 1,000,000 parameters\n+/// - Standard LoRA: 16,000 parameters (98.4% reduction)\n+/// - DoRA: 17,000 parameters (LoRA + magnitude vector)\n+/// - VeRA: 1,600 parameters (99.84% reduction)\n+/// - DVoRA: ~1,600 parameters (same as VeRA!) but with better performance (5.0 vs 4.3)\n+///\n+/// Benefits:\n+/// - ‚úÖ Extremely parameter-efficient (10x fewer than standard LoRA, same as VeRA)\n+/// - ‚úÖ Better performance than VeRA alone (5.0 vs 4.3 score)\n+/// - ‚úÖ Training stability from DoRA's magnitude-direction decomposition\n+/// - ‚úÖ Shared matrices reduce storage when adapting many layers\n+/// - ‚úÖ Best choice for extreme memory constraints with quality requirements\n+///\n+/// Trade-offs:\n+/// - ‚ö†Ô∏è Requires shared matrix initialization before use\n+/// - ‚ö†Ô∏è Slightly more computation than VeRA (due to normalization)\n+/// - ‚ö†Ô∏è More complex than standard adapters (combines two techniques)\n+///\n+/// When to use DVoRA:\n+/// - Extreme memory constraints but need better quality than VeRA\n+/// - Mobile/edge deployment with limited resources\n+/// - Fine-tuning many layers efficiently\n+/// - When you want the absolute best parameter efficiency + quality balance\n+/// </para>\n+/// <para>\n+/// <b>References:</b>\n+/// - DoRA: \"Weight-Decomposed Low-Rank Adaptation\" (ICML 2024 Oral)\n+/// - VeRA: \"Vector-based Random Matrix Adaptation\"\n+/// - DVoRA: Combines both techniques for optimal efficiency and performance\n+/// </para>\n+/// </remarks>\n+public class DVoRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Shared frozen random matrix A (inputSize √ó rank) used by all DVoRA adapters.\n+    /// </summary>\n+    /// <remarks>\n+    /// This matrix is initialized once globally and shared across all DVoRA layers.\n+    /// It is NEVER trained - it remains frozen at its random initialization values.\n+    /// This is the VeRA component of DVoRA.\n+    /// </remarks>\n+    private static Matrix<T>? _sharedMatrixA;\n+\n+    /// <summary>\n+    /// Shared frozen random matrix B (rank √ó outputSize) used by all DVoRA adapters.\n+    /// </summary>\n+    /// <remarks>\n+    /// This matrix is initialized once globally and shared across all DVoRA layers.\n+    /// It is NEVER trained - it remains frozen at its random initialization values.\n+    /// This is the VeRA component of DVoRA.\n+    /// </remarks>\n+    private static Matrix<T>? _sharedMatrixB;\n+\n+    /// <summary>\n+    /// Lock object for thread-safe shared matrix initialization.\n+    /// </summary>\n+    private static readonly object _initLock = new object();\n+\n+    /// <summary>\n+    /// Magnitude component of the decomposed weights (scalar per output neuron).\n+    /// Trainable per-layer parameter.\n+    /// </summary>\n+    /// <remarks>\n+    /// The magnitude vector stores the L2 norm of each weight vector (one per output neuron).\n+    /// This is the DoRA component of DVoRA.\n+    /// </remarks>\n+    private Vector<T> _magnitude;\n+\n+    /// <summary>\n+    /// Scaling vector d (outputSize) - trainable per-layer parameter.\n+    /// </summary>\n+    /// <remarks>\n+    /// This vector scales the VeRA output on a per-dimension basis.\n+    /// This is the VeRA component of DVoRA.\n+    /// </remarks>\n+    private Vector<T> _scalingVectorD;\n+\n+    /// <summary>\n+    /// Scaling vector b (rank) - trainable per-layer parameter.\n+    /// </summary>\n+    /// <remarks>\n+    /// This vector scales the intermediate rank-dimensional representation.\n+    /// This is the VeRA component of DVoRA.\n+    /// </remarks>\n+    private Vector<T> _scalingVectorB;\n+\n+    /// <summary>\n+    /// Gradient for magnitude vector computed during backpropagation.\n+    /// </summary>\n+    private Vector<T>? _magnitudeGradient;\n+\n+    /// <summary>\n+    /// Gradient for scaling vector d computed during backpropagation.\n+    /// </summary>\n+    private Vector<T>? _scalingVectorDGradient;\n+\n+    /// <summary>\n+    /// Gradient for scaling vector b computed during backpropagation.\n+    /// </summary>\n+    private Vector<T>? _scalingVectorBGradient;\n+\n+    /// <summary>\n+    /// Cached normalized direction from the last forward pass, used in backpropagation.\n+    /// </summary>\n+    private Matrix<T>? _lastNormalizedDirection;\n+\n+    /// <summary>\n+    /// Stored input from the forward pass, needed for gradient computation.\n+    /// </summary>\n+    private Tensor<T>? _lastInput;\n+\n+    /// <summary>\n+    /// Stored intermediate value from forward pass, needed for backward pass.\n+    /// </summary>\n+    private Matrix<T>? _lastIntermediate;\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters.\n+    /// </summary>\n+    /// <remarks>\n+    /// DVoRA parameters = magnitude (outputSize) + d_scale (outputSize) + b_scale (rank).\n+    /// This is only slightly more than VeRA (adds magnitude vector) but much fewer than DoRA (no full LoRA matrices).\n+    /// </remarks>\n+    public override int ParameterCount\n+    {\n+        get\n+        {\n+            int dvoraParams = _magnitude.Length + _scalingVectorD.Length + _scalingVectorB.Length;\n+            return _freezeBaseLayer ? dvoraParams : (_baseLayer.ParameterCount + dvoraParams);\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Initializes a new DVoRA adapter wrapping an existing layer.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with DVoRA.</param>\n+    /// <param name=\"rank\">The rank of the low-rank decomposition (shared across all DVoRA layers).</param>\n+    /// <param name=\"alpha\">The scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"InvalidOperationException\">Thrown when shared matrices are not initialized.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// Before creating any DVoRA adapters, you must call InitializeSharedMatrices() once to set up\n+    /// the shared random matrices that all DVoRA layers will use.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This creates a DVoRA adapter for a layer. Unlike standard LoRA,\n+    /// you must initialize the shared random matrices first by calling:\n+    ///\n+    /// DVoRAAdapter&lt;T&gt;.InitializeSharedMatrices(inputSize, outputSize, rank);\n+    ///\n+    /// This needs to be done once before creating any DVoRA adapters.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to adapt\n+    /// - rank: How much compression (lower = fewer parameters)\n+    /// - alpha: How strong the adaptation is\n+    /// - freezeBaseLayer: Whether to lock the original layer's weights (usually true)\n+    /// </para>\n+    /// </remarks>\n+    public DVoRAAdapter(ILayer<T> baseLayer, int rank, double alpha = -1, bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        if (baseLayer == null)\n+        {\n+            throw new ArgumentNullException(nameof(baseLayer));\n+        }\n+\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+\n+        // Ensure shared matrices are initialized\n+        if (_sharedMatrixA == null || _sharedMatrixB == null)\n+        {\n+            throw new InvalidOperationException(\n+                \"Shared matrices must be initialized before creating DVoRA adapters. \" +\n+                \"Call DVoRAAdapter<T>.InitializeSharedMatrices(inputSize, outputSize, rank) first.\");\n+        }\n+\n+        // Validate shared matrix dimensions match this layer\n+        if (_sharedMatrixA.Rows != inputSize || _sharedMatrixA.Columns != rank)\n+        {\n+            throw new ArgumentException(\n+                $\"Shared matrix A dimensions ({_sharedMatrixA.Rows}√ó{_sharedMatrixA.Columns}) \" +\n+                $\"do not match required dimensions ({inputSize}√ó{rank})\", nameof(baseLayer));\n+        }\n+\n+        if (_sharedMatrixB.Rows != rank || _sharedMatrixB.Columns != outputSize)\n+        {\n+            throw new ArgumentException(\n+                $\"Shared matrix B dimensions ({_sharedMatrixB.Rows}√ó{_sharedMatrixB.Columns}) \" +\n+                $\"do not match required dimensions ({rank}√ó{outputSize})\", nameof(baseLayer));\n+        }\n+\n+        // Initialize magnitude from base layer weights (DoRA component)\n+        _magnitude = new Vector<T>(outputSize);\n+        DecomposeWeights();\n+\n+        // Initialize scaling vectors to ones (VeRA component - no initial effect)\n+        _scalingVectorD = new Vector<T>(outputSize);\n+        _scalingVectorB = new Vector<T>(rank);\n+\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            _scalingVectorD[i] = NumOps.One;\n+        }\n+\n+        for (int i = 0; i < rank; i++)\n+        {\n+            _scalingVectorB[i] = NumOps.One;\n+        }\n+\n+        // Update parameter vector\n+        Parameters = new Vector<T>(ParameterCount);\n+        UpdateParametersFromComponents();\n+    }\n+\n+    /// <summary>\n+    /// Initializes the shared random matrices used by all DVoRA adapters.\n+    /// </summary>\n+    /// <param name=\"inputSize\">The input dimension for the layers.</param>\n+    /// <param name=\"outputSize\">The output dimension for the layers.</param>\n+    /// <param name=\"rank\">The rank of the low-rank decomposition.</param>\n+    /// <param name=\"seed\">Optional random seed for reproducibility.</param>\n+    /// <remarks>\n+    /// <para>\n+    /// This method must be called once before creating any DVoRA adapters. It initializes the\n+    /// shared matrices A and B with random values that are frozen (never trained).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Call this once at the start before creating any DVoRA layers:\n+    ///\n+    /// // Initialize shared random matrices (do this once)\n+    /// DVoRAAdapter&lt;double&gt;.InitializeSharedMatrices(inputSize: 784, outputSize: 128, rank: 8);\n+    ///\n+    /// // Now create DVoRA adapters (they will use the shared matrices)\n+    /// var adapter1 = new DVoRAAdapter&lt;double&gt;(layer1, rank: 8);\n+    /// var adapter2 = new DVoRAAdapter&lt;double&gt;(layer2, rank: 8);\n+    ///\n+    /// All adapters share the same random A and B matrices, saving memory!\n+    /// </para>\n+    /// </remarks>\n+    public static void InitializeSharedMatrices(int inputSize, int outputSize, int rank, int? seed = null)\n+    {\n+        lock (_initLock)\n+        {\n+            Random rng = seed.HasValue ? new Random(seed.Value) : new Random();\n+            var ops = MathHelper.GetNumericOperations<T>();\n+\n+            // Initialize matrix A (inputSize √ó rank) with Gaussian random values\n+            _sharedMatrixA = new Matrix<T>(inputSize, rank);\n+            T stddevA = ops.Sqrt(ops.Divide(ops.One, ops.FromDouble(rank)));\n+            for (int i = 0; i < inputSize; i++)\n+            {\n+                for (int j = 0; j < rank; j++)\n+                {\n+                    // Box-Muller transform for Gaussian random numbers\n+                    double u1 = rng.NextDouble();\n+                    double u2 = rng.NextDouble();\n+                    double randStdNormal = Math.Sqrt(-2.0 * Math.Log(u1)) * Math.Sin(2.0 * Math.PI * u2);\n+                    _sharedMatrixA[i, j] = ops.Multiply(ops.FromDouble(randStdNormal), stddevA);\n+                }\n+            }\n+\n+            // Initialize matrix B (rank √ó outputSize) with Gaussian random values\n+            _sharedMatrixB = new Matrix<T>(rank, outputSize);\n+            T stddevB = ops.Sqrt(ops.Divide(ops.One, ops.FromDouble(rank)));\n+            for (int i = 0; i < rank; i++)\n+            {\n+                for (int j = 0; j < outputSize; j++)\n+                {\n+                    // Box-Muller transform for Gaussian random numbers\n+                    double u1 = rng.NextDouble();\n+                    double u2 = rng.NextDouble();\n+                    double randStdNormal = Math.Sqrt(-2.0 * Math.Log(u1)) * Math.Sin(2.0 * Math.PI * u2);\n+                    _sharedMatrixB[i, j] = ops.Multiply(ops.FromDouble(randStdNormal), stddevB);\n+                }\n+            }\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Resets the shared matrices (useful for testing or reinitializing).\n+    /// </summary>\n+    public static void ResetSharedMatrices()\n+    {\n+        lock (_initLock)\n+        {\n+            _sharedMatrixA = null;\n+            _sharedMatrixB = null;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Gets whether the shared matrices have been initialized.\n+    /// </summary>\n+    public static bool AreSharedMatricesInitialized => _sharedMatrixA != null && _sharedMatrixB != null;\n+\n+    /// <summary>\n+    /// Decomposes the base layer's weights into magnitude and direction components.\n+    /// </summary>\n+    /// <remarks>\n+    /// This is the DoRA component of DVoRA. For each output neuron:\n+    /// 1. Extract the weight vector\n+    /// 2. Compute the L2 norm (magnitude)\n+    /// 3. Store the magnitude\n+    ///\n+    /// The direction is implicitly W/||W|| and doesn't need to be stored separately.\n+    /// </remarks>\n+    private void DecomposeWeights()\n+    {\n+        Vector<T> baseParams = _baseLayer.GetParameters();\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        int weightCount = inputSize * outputSize;\n+\n+        // For each output neuron, compute the magnitude of its weight vector\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            T sumSquares = NumOps.Zero;\n+\n+            // Sum squares of all weights for this output neuron\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                int idx = i * inputSize + j;\n+                if (idx < weightCount && idx < baseParams.Length)\n+                {\n+                    T weight = baseParams[idx];\n+                    sumSquares = NumOps.Add(sumSquares, NumOps.Multiply(weight, weight));\n+                }\n+            }\n+\n+            // Magnitude is the L2 norm\n+            _magnitude[i] = NumOps.Sqrt(sumSquares);\n+\n+            // Ensure magnitude is never zero (for numerical stability)\n+            if (NumOps.Equals(_magnitude[i], NumOps.Zero))\n+            {\n+                _magnitude[i] = NumOps.FromDouble(1e-8);\n+            }\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Normalizes a matrix row-wise (each row becomes a unit vector).\n+    /// </summary>\n+    /// <param name=\"matrix\">The matrix to normalize.</param>\n+    /// <returns>Row-normalized matrix where each row has unit L2 norm.</returns>\n+    private Matrix<T> NormalizeRows(Matrix<T> matrix)\n+    {\n+        int rows = matrix.Rows;\n+        int cols = matrix.Columns;\n+        Matrix<T> normalized = new Matrix<T>(rows, cols);\n+\n+        for (int i = 0; i < rows; i++)\n+        {\n+            // Compute L2 norm of row\n+            T sumSquares = NumOps.Zero;\n+            for (int j = 0; j < cols; j++)\n+            {\n+                T val = matrix[i, j];\n+                sumSquares = NumOps.Add(sumSquares, NumOps.Multiply(val, val));\n+            }\n+\n+            T norm = NumOps.Sqrt(sumSquares);\n+\n+            // Avoid division by zero\n+            if (NumOps.Equals(norm, NumOps.Zero))\n+            {\n+                norm = NumOps.FromDouble(1e-8);\n+            }\n+\n+            // Normalize row\n+            for (int j = 0; j < cols; j++)\n+            {\n+                normalized[i, j] = NumOps.Divide(matrix[i, j], norm);\n+            }\n+        }\n+\n+        return normalized;\n+    }\n+\n+    /// <summary>\n+    /// Recomposes weights from magnitude and direction components.\n+    /// </summary>\n+    /// <param name=\"direction\">The normalized direction matrix.</param>\n+    /// <returns>The full weight matrix (magnitude * direction).</returns>\n+    private Matrix<T> RecomposeWeights(Matrix<T> direction)\n+    {\n+        int outputSize = direction.Rows;\n+        int inputSize = direction.Columns;\n+        Matrix<T> weights = new Matrix<T>(outputSize, inputSize);\n+\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                weights[i, j] = NumOps.Multiply(_magnitude[i], direction[i, j]);\n+            }\n+        }\n+\n+        return weights;\n+    }\n+\n+    /// <summary>\n+    /// Creates a dummy LoRA layer (not used since DVoRA uses custom logic).\n+    /// </summary>\n+    protected override LoRALayer<T> CreateLoRALayer(int rank, double alpha)\n+    {\n+        // DVoRA doesn't use a standard LoRA layer, but we need to satisfy the base class\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        return new LoRALayer<T>(inputSize, outputSize, rank, alpha);\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass through the DVoRA adapter.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Output combining base layer with DVoRA-adapted weights.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The DVoRA forward pass combines DoRA and VeRA:\n+    /// 1. Gets base layer weights W\n+    /// 2. Computes direction: d = W / ||W|| (DoRA)\n+    /// 3. Applies VeRA to direction: d' = d + d_scale * (B * A * input) * b_scale (VeRA)\n+    /// 4. Normalizes adapted direction: d_norm = d' / ||d'|| (DoRA)\n+    /// 5. Recomposes weights: W' = m * d_norm (DoRA)\n+    /// 6. Computes output: y = input @ W'^T\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is where DVoRA combines both techniques:\n+    ///\n+    /// DoRA part:\n+    /// - Split weights into magnitude (strength) and direction\n+    /// - Keep magnitude separate, work only with direction\n+    ///\n+    /// VeRA part:\n+    /// - Apply shared random matrices + tiny scaling vectors to the direction\n+    ///\n+    /// Final step:\n+    /// - Normalize the adjusted direction\n+    /// - Multiply magnitude back in\n+    /// - Use these hybrid-adapted weights for prediction\n+    ///\n+    /// Result: Stability of DoRA + efficiency of VeRA = best of both worlds!\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        _lastInput = input.Clone();\n+\n+        // Get base layer parameters and extract weights\n+        Vector<T> baseParams = _baseLayer.GetParameters();\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        int weightCount = inputSize * outputSize;\n+\n+        // Extract weight matrix from base layer\n+        Matrix<T> baseWeights = new Matrix<T>(outputSize, inputSize);\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                int weightIdx = i * inputSize + j;\n+                if (weightIdx < weightCount && weightIdx < baseParams.Length)\n+                {\n+                    baseWeights[i, j] = baseParams[weightIdx];\n+                }\n+                else\n+                {\n+                    baseWeights[i, j] = NumOps.Zero;\n+                }\n+            }\n+        }\n+\n+        // Compute base direction (W / ||W||) - DoRA component\n+        Matrix<T> baseDirection = NormalizeRows(baseWeights);\n+\n+        // Apply VeRA to get direction delta\n+        int batchSize = input.Shape[0];\n+        int rank = _scalingVectorB.Length;\n+\n+        // Convert input to matrix [batchSize, inputSize]\n+        Matrix<T> inputMatrix = new Matrix<T>(batchSize, inputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                inputMatrix[i, j] = input[i * inputSize + j];\n+            }\n+        }\n+\n+        // VeRA forward: (B * A * input) with scaling vectors\n+        // Compute: input * A (shared, frozen) ‚Üí [batchSize, rank]\n+        Matrix<T> afterA = inputMatrix.Multiply(_sharedMatrixA!);\n+\n+        // Apply scaling vector b element-wise: afterA * diag(b) ‚Üí [batchSize, rank]\n+        Matrix<T> afterB = new Matrix<T>(batchSize, rank);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < rank; j++)\n+            {\n+                afterB[i, j] = NumOps.Multiply(afterA[i, j], _scalingVectorB[j]);\n+            }\n+        }\n+\n+        // Compute: afterB * B (shared, frozen) ‚Üí [batchSize, outputSize]\n+        Matrix<T> afterSharedB = afterB.Multiply(_sharedMatrixB!);\n+        _lastIntermediate = afterSharedB.Clone();\n+\n+        // Apply scaling vector d element-wise: afterSharedB * diag(d) ‚Üí [batchSize, outputSize]\n+        Matrix<T> veraContribution = new Matrix<T>(batchSize, outputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                veraContribution[i, j] = NumOps.Multiply(afterSharedB[i, j], _scalingVectorD[j]);\n+            }\n+        }\n+\n+        // Apply alpha/rank scaling\n+        T scaling = NumOps.Divide(NumOps.FromDouble(Alpha), NumOps.FromDouble(Rank));\n+\n+        // For direction update, we need the VeRA contribution as a weight delta, not an output\n+        // Average over batch to get per-weight contribution\n+        Matrix<T> veraWeightDelta = new Matrix<T>(outputSize, inputSize);\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                T sum = NumOps.Zero;\n+                for (int b = 0; b < batchSize; b++)\n+                {\n+                    // Approximate weight gradient contribution\n+                    T contrib = NumOps.Multiply(veraContribution[b, i], inputMatrix[b, j]);\n+                    sum = NumOps.Add(sum, contrib);\n+                }\n+                veraWeightDelta[i, j] = NumOps.Multiply(\n+                    NumOps.Divide(sum, NumOps.FromDouble(batchSize)),\n+                    scaling);\n+            }\n+        }\n+\n+        // Add VeRA delta to base direction: d' = d + delta\n+        Matrix<T> adaptedDirection = new Matrix<T>(outputSize, inputSize);\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                adaptedDirection[i, j] = NumOps.Add(baseDirection[i, j], veraWeightDelta[i, j]);\n+            }\n+        }\n+\n+        // Normalize the adapted direction: d_norm = d' / ||d'|| - DoRA component\n+        _lastNormalizedDirection = NormalizeRows(adaptedDirection);\n+\n+        // Recompose weights: W' = m * d_norm - DoRA component\n+        Matrix<T> finalWeights = RecomposeWeights(_lastNormalizedDirection);\n+\n+        // Compute output: y = input @ W'^T\n+        Matrix<T> outputMatrix = inputMatrix.Multiply(finalWeights.Transpose());\n+\n+        // Convert back to tensor\n+        Vector<T> outputData = new Vector<T>(batchSize * outputSize);\n+        int idx = 0;\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                outputData[idx++] = outputMatrix[i, j];\n+            }\n+        }\n+\n+        return new Tensor<T>(new[] { batchSize, outputSize }, outputData);\n+    }\n+\n+    /// <summary>\n+    /// Performs the backward pass through the DVoRA adapter.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient flowing back from the next layer.</param>\n+    /// <returns>Gradient to pass to the previous layer.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The backward pass computes gradients for:\n+    /// 1. Magnitude parameters (DoRA component, one per output neuron)\n+    /// 2. Scaling vectors d and b (VeRA component, per-layer)\n+    /// 3. Base layer weights (if not frozen)\n+    ///\n+    /// The shared matrices A and B remain frozen and are never updated.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is where DVoRA learns! During backpropagation:\n+    /// 1. Compute gradients for magnitude (DoRA learning)\n+    /// 2. Compute gradients for scaling vectors d and b (VeRA learning)\n+    /// 3. Shared matrices A and B stay frozen (VeRA efficiency)\n+    /// 4. Pass gradients back to earlier layers\n+    ///\n+    /// We only train: magnitude + d + b = very few parameters!\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (_lastInput == null || _lastNormalizedDirection == null || _lastIntermediate == null)\n+        {\n+            throw new InvalidOperationException(\"Forward pass must be called before backward pass\");\n+        }\n+\n+        int batchSize = outputGradient.Shape[0];\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        int rank = _scalingVectorB.Length;\n+\n+        // Convert gradient to matrix\n+        Matrix<T> gradMatrix = new Matrix<T>(batchSize, outputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                gradMatrix[i, j] = outputGradient[i * outputSize + j];\n+            }\n+        }\n+\n+        T scaling = NumOps.Divide(NumOps.FromDouble(Alpha), NumOps.FromDouble(Rank));\n+\n+        // Compute magnitude gradients (DoRA component)\n+        _magnitudeGradient = new Vector<T>(outputSize);\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            T gradSum = NumOps.Zero;\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                gradSum = NumOps.Add(gradSum, gradMatrix[b, i]);\n+            }\n+            _magnitudeGradient[i] = gradSum;\n+        }\n+\n+        // Compute gradient for scaling vector d (VeRA component)\n+        _scalingVectorDGradient = new Vector<T>(outputSize);\n+        for (int j = 0; j < outputSize; j++)\n+        {\n+            T sum = NumOps.Zero;\n+            for (int i = 0; i < batchSize; i++)\n+            {\n+                T grad = NumOps.Multiply(gradMatrix[i, j], _lastIntermediate[i, j]);\n+                grad = NumOps.Multiply(grad, scaling);\n+                sum = NumOps.Add(sum, grad);\n+            }\n+            _scalingVectorDGradient[j] = sum;\n+        }\n+\n+        // Propagate gradient back through d scaling\n+        Matrix<T> gradAfterSharedB = new Matrix<T>(batchSize, outputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                gradAfterSharedB[i, j] = NumOps.Multiply(\n+                    NumOps.Multiply(gradMatrix[i, j], _scalingVectorD[j]),\n+                    scaling);\n+            }\n+        }\n+\n+        // Propagate through shared B\n+        Matrix<T> gradAfterB = gradAfterSharedB.Multiply(_sharedMatrixB!.Transpose());\n+\n+        // Convert input to matrix for gradient computation\n+        Matrix<T> inputMatrix = new Matrix<T>(batchSize, inputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                inputMatrix[i, j] = _lastInput[i * inputSize + j];\n+            }\n+        }\n+\n+        // Compute intermediate: input * A\n+        Matrix<T> afterA = inputMatrix.Multiply(_sharedMatrixA!);\n+\n+        // Compute gradient for scaling vector b (VeRA component)\n+        _scalingVectorBGradient = new Vector<T>(rank);\n+        for (int j = 0; j < rank; j++)\n+        {\n+            T sum = NumOps.Zero;\n+            for (int i = 0; i < batchSize; i++)\n+            {\n+                T grad = NumOps.Multiply(gradAfterB[i, j], afterA[i, j]);\n+                sum = NumOps.Add(sum, grad);\n+            }\n+            _scalingVectorBGradient[j] = sum;\n+        }\n+\n+        // Propagate gradient back through b scaling\n+        Matrix<T> gradAfterA = new Matrix<T>(batchSize, rank);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < rank; j++)\n+            {\n+                gradAfterA[i, j] = NumOps.Multiply(gradAfterB[i, j], _scalingVectorB[j]);\n+            }\n+        }\n+\n+        // Propagate through shared A\n+        Matrix<T> veraInputGrad = gradAfterA.Multiply(_sharedMatrixA!.Transpose());\n+\n+        // Backward through base layer (if not frozen)\n+        Tensor<T> baseInputGrad;\n+        if (!_freezeBaseLayer)\n+        {\n+            baseInputGrad = _baseLayer.Backward(outputGradient);\n+        }\n+        else\n+        {\n+            // Create zero gradient for base layer\n+            baseInputGrad = new Tensor<T>(_lastInput.Shape);\n+        }\n+\n+        // Sum input gradients from DVoRA and base layer\n+        Vector<T> inputGradData = new Vector<T>(batchSize * inputSize);\n+        int idx = 0;\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                T dvoraGrad = veraInputGrad[i, j];\n+                T baseGrad = baseInputGrad[i * inputSize + j];\n+                inputGradData[idx++] = NumOps.Add(dvoraGrad, baseGrad);\n+            }\n+        }\n+\n+        // Update parameter gradients\n+        UpdateParameterGradientsFromComponents();\n+\n+        return new Tensor<T>(new[] { batchSize, inputSize }, inputGradData);\n+    }\n+\n+    /// <summary>\n+    /// Updates parameters using the specified learning rate.\n+    /// </summary>\n+    /// <param name=\"learningRate\">The learning rate for parameter updates.</param>\n+    public override void UpdateParameters(T learningRate)\n+    {\n+        if (_magnitudeGradient == null || _scalingVectorDGradient == null || _scalingVectorBGradient == null)\n+        {\n+            return;\n+        }\n+\n+        // Update magnitude parameters (DoRA component)\n+        for (int i = 0; i < _magnitude.Length; i++)\n+        {\n+            T update = NumOps.Multiply(_magnitudeGradient[i], learningRate);\n+            _magnitude[i] = NumOps.Subtract(_magnitude[i], update);\n+\n+            // Ensure magnitude stays positive\n+            if (NumOps.LessThan(_magnitude[i], NumOps.FromDouble(1e-8)))\n+            {\n+                _magnitude[i] = NumOps.FromDouble(1e-8);\n+            }\n+        }\n+\n+        // Update scaling vector d (VeRA component)\n+        for (int i = 0; i < _scalingVectorD.Length; i++)\n+        {\n+            T update = NumOps.Multiply(_scalingVectorDGradient[i], learningRate);\n+            _scalingVectorD[i] = NumOps.Subtract(_scalingVectorD[i], update);\n+        }\n+\n+        // Update scaling vector b (VeRA component)\n+        for (int i = 0; i < _scalingVectorB.Length; i++)\n+        {\n+            T update = NumOps.Multiply(_scalingVectorBGradient[i], learningRate);\n+            _scalingVectorB[i] = NumOps.Subtract(_scalingVectorB[i], update);\n+        }\n+\n+        // Update base layer if not frozen\n+        if (!_freezeBaseLayer)\n+        {\n+            _baseLayer.UpdateParameters(learningRate);\n+        }\n+\n+        // Update parameter vector\n+        UpdateParametersFromComponents();\n+    }\n+\n+    /// <summary>\n+    /// Gets the current parameters as a vector.\n+    /// </summary>\n+    /// <returns>Vector containing all DVoRA parameters (magnitude, d, b).</returns>\n+    public override Vector<T> GetParameters()\n+    {\n+        return Parameters.Clone();\n+    }\n+\n+    /// <summary>\n+    /// Sets the layer parameters from a vector.\n+    /// </summary>\n+    /// <param name=\"parameters\">Vector containing all parameters.</param>\n+    public override void SetParameters(Vector<T> parameters)\n+    {\n+        if (parameters.Length != ParameterCount)\n+        {\n+            throw new ArgumentException($\"Expected {ParameterCount} parameters, got {parameters.Length}\", nameof(parameters));\n+        }\n+\n+        Parameters = parameters.Clone();\n+        UpdateComponentsFromParameters();\n+    }\n+\n+    /// <summary>\n+    /// Updates the parameter vector from the current component states.\n+    /// </summary>\n+    private void UpdateParametersFromComponents()\n+    {\n+        int idx = 0;\n+\n+        // Pack base layer parameters (if not frozen)\n+        if (!_freezeBaseLayer)\n+        {\n+            Vector<T> baseParams = _baseLayer.GetParameters();\n+            for (int i = 0; i < baseParams.Length; i++)\n+            {\n+                Parameters[idx++] = baseParams[i];\n+            }\n+        }\n+\n+        // Pack magnitude parameters\n+        for (int i = 0; i < _magnitude.Length; i++)\n+        {\n+            Parameters[idx++] = _magnitude[i];\n+        }\n+\n+        // Pack scaling vector d\n+        for (int i = 0; i < _scalingVectorD.Length; i++)\n+        {\n+            Parameters[idx++] = _scalingVectorD[i];\n+        }\n+\n+        // Pack scaling vector b\n+        for (int i = 0; i < _scalingVectorB.Length; i++)\n+        {\n+            Parameters[idx++] = _scalingVectorB[i];\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Updates the components from the parameter vector.\n+    /// </summary>\n+    private void UpdateComponentsFromParameters()\n+    {\n+        int idx = 0;\n+\n+        // Unpack base layer parameters (if not frozen)\n+        if (!_freezeBaseLayer)\n+        {\n+            int baseParamCount = _baseLayer.ParameterCount;\n+            Vector<T> baseParams = new Vector<T>(baseParamCount);\n+            for (int i = 0; i < baseParamCount; i++)\n+            {\n+                baseParams[i] = Parameters[idx++];\n+            }\n+            _baseLayer.SetParameters(baseParams);\n+        }\n+\n+        // Unpack magnitude parameters\n+        for (int i = 0; i < _magnitude.Length; i++)\n+        {\n+            _magnitude[i] = Parameters[idx++];\n+        }\n+\n+        // Unpack scaling vector d\n+        for (int i = 0; i < _scalingVectorD.Length; i++)\n+        {\n+            _scalingVectorD[i] = Parameters[idx++];\n+        }\n+\n+        // Unpack scaling vector b\n+        for (int i = 0; i < _scalingVectorB.Length; i++)\n+        {\n+            _scalingVectorB[i] = Parameters[idx++];\n+        }\n+    }","path":"src/LoRA/Adapters/DVoRAAdapter.cs","commit_id":"489180133c7a9db9460c84a6174f60284717b952","original_commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Align parameter packing with the declared layout.**\n\nEven after fixing `ParameterCount`, these packing methods ignore the LoRA segment altogether. `LoRAAdapterBase` expects the vector to contain base params (optional) followed by the LoRA params; skipping them here leaves that slice stale and makes `SetParameters`/`GetParameters` inconsistent. You need to insert the LoRA parameters in both pack and unpack routines to stay compatible with the base class contract.\n\n```diff\n         if (!_freezeBaseLayer)\n         {\n             Vector<T> baseParams = _baseLayer.GetParameters();\n             for (int i = 0; i < baseParams.Length; i++)\n             {\n                 Parameters[idx++] = baseParams[i];\n             }\n         }\n \n-        // Pack magnitude parameters\n+        // Pack LoRA parameters\n+        Vector<T> loraParams = _loraLayer.GetParameters();\n+        for (int i = 0; i < loraParams.Length; i++)\n+        {\n+            Parameters[idx++] = loraParams[i];\n+        }\n+\n+        // Pack magnitude parameters\n         for (int i = 0; i < _magnitude.Length; i++)\n         {\n             Parameters[idx++] = _magnitude[i];\n         }\n```\n\nMirror the same change in `UpdateComponentsFromParameters`.\n\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/LoRA/Adapters/DVoRAAdapter.cs around lines 853 to 922, the pack/unpack\nroutines currently omit the LoRA segment expected by LoRAAdapterBase; insert\ncode to serialize the LoRA parameters into Parameters immediately after the\noptional base-layer block and before the magnitude block, and mirror that in\nUpdateComponentsFromParameters by reading that same LoRA slice back into the\nadapter's LoRA storage (flattened low-rank A/B matrices or the adapter's LoRA\nparameter vector), advancing idx appropriately so the following magnitude, d and\nb segments align with the declared layout.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:42:00Z","updated_at":"2025-11-02T02:42:05Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484118395","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118395"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484118395"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118395/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":853,"original_start_line":853,"start_side":"RIGHT","line":922,"original_line":922,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":922,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118398","pull_request_review_id":3408015014,"id":2484118398,"node_id":"PRRC_kwDOKSXUF86UEKN-","diff_hunk":"@@ -0,0 +1,1116 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// DVoRA (DoRA + VeRA) adapter - combines DoRA's magnitude-direction decomposition with VeRA's extreme parameter efficiency.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// DVoRA achieves the best of both worlds by:\n+/// - Applying DoRA's magnitude-direction decomposition for training stability\n+/// - Using VeRA's shared frozen matrices and scaling vectors for extreme parameter efficiency\n+/// - Applying the VeRA adaptation only to the direction component (not the magnitude)\n+/// </para>\n+/// <para>\n+/// <b>Mathematical Formulation:</b>\n+/// Given pre-trained weights W, DVoRA:\n+/// 1. Decomposes: W = m * d (magnitude and direction)\n+/// 2. Applies VeRA to direction: d' = d + d_scale * (B * A * input) * b_scale\n+/// 3. Normalizes direction: d_norm = d' / ||d'||\n+/// 4. Recomposes: W' = m * d_norm\n+///\n+/// Where:\n+/// - m: magnitude vector (trainable)\n+/// - d: direction matrix (normalized weight vectors)\n+/// - A, B: shared frozen random matrices (VeRA style)\n+/// - d_scale, b_scale: per-layer trainable scaling vectors (VeRA style)\n+/// </para>\n+/// <para>\n+/// <b>Research Context:</b>\n+/// DVoRA scores 5.0 vs VeRA's 4.3 (improvement of 16%) while maintaining ultra-low parameter counts.\n+/// It combines DoRA's superior training stability with VeRA's extreme parameter efficiency.\n+/// </para>\n+/// <para>\n+/// <b>For Beginners:</b> DVoRA is the ultimate parameter-efficient adapter.\n+///\n+/// Think of it as a hybrid technique:\n+/// - From DoRA: Separate magnitude (strength) from direction for stability\n+/// - From VeRA: Use shared random matrices and tiny scaling vectors for efficiency\n+/// - The magic: Apply VeRA's adaptation only to the direction, not the magnitude\n+///\n+/// Parameter comparison for 1000x1000 layer with rank=8:\n+/// - Full fine-tuning: 1,000,000 parameters\n+/// - Standard LoRA: 16,000 parameters (98.4% reduction)\n+/// - DoRA: 17,000 parameters (LoRA + magnitude vector)\n+/// - VeRA: 1,600 parameters (99.84% reduction)\n+/// - DVoRA: ~1,600 parameters (same as VeRA!) but with better performance (5.0 vs 4.3)\n+///\n+/// Benefits:\n+/// - ‚úÖ Extremely parameter-efficient (10x fewer than standard LoRA, same as VeRA)\n+/// - ‚úÖ Better performance than VeRA alone (5.0 vs 4.3 score)\n+/// - ‚úÖ Training stability from DoRA's magnitude-direction decomposition\n+/// - ‚úÖ Shared matrices reduce storage when adapting many layers\n+/// - ‚úÖ Best choice for extreme memory constraints with quality requirements\n+///\n+/// Trade-offs:\n+/// - ‚ö†Ô∏è Requires shared matrix initialization before use\n+/// - ‚ö†Ô∏è Slightly more computation than VeRA (due to normalization)\n+/// - ‚ö†Ô∏è More complex than standard adapters (combines two techniques)\n+///\n+/// When to use DVoRA:\n+/// - Extreme memory constraints but need better quality than VeRA\n+/// - Mobile/edge deployment with limited resources\n+/// - Fine-tuning many layers efficiently\n+/// - When you want the absolute best parameter efficiency + quality balance\n+/// </para>\n+/// <para>\n+/// <b>References:</b>\n+/// - DoRA: \"Weight-Decomposed Low-Rank Adaptation\" (ICML 2024 Oral)\n+/// - VeRA: \"Vector-based Random Matrix Adaptation\"\n+/// - DVoRA: Combines both techniques for optimal efficiency and performance\n+/// </para>\n+/// </remarks>\n+public class DVoRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Shared frozen random matrix A (inputSize √ó rank) used by all DVoRA adapters.\n+    /// </summary>\n+    /// <remarks>\n+    /// This matrix is initialized once globally and shared across all DVoRA layers.\n+    /// It is NEVER trained - it remains frozen at its random initialization values.\n+    /// This is the VeRA component of DVoRA.\n+    /// </remarks>\n+    private static Matrix<T>? _sharedMatrixA;\n+\n+    /// <summary>\n+    /// Shared frozen random matrix B (rank √ó outputSize) used by all DVoRA adapters.\n+    /// </summary>\n+    /// <remarks>\n+    /// This matrix is initialized once globally and shared across all DVoRA layers.\n+    /// It is NEVER trained - it remains frozen at its random initialization values.\n+    /// This is the VeRA component of DVoRA.\n+    /// </remarks>\n+    private static Matrix<T>? _sharedMatrixB;\n+\n+    /// <summary>\n+    /// Lock object for thread-safe shared matrix initialization.\n+    /// </summary>\n+    private static readonly object _initLock = new object();\n+\n+    /// <summary>\n+    /// Magnitude component of the decomposed weights (scalar per output neuron).\n+    /// Trainable per-layer parameter.\n+    /// </summary>\n+    /// <remarks>\n+    /// The magnitude vector stores the L2 norm of each weight vector (one per output neuron).\n+    /// This is the DoRA component of DVoRA.\n+    /// </remarks>\n+    private Vector<T> _magnitude;\n+\n+    /// <summary>\n+    /// Scaling vector d (outputSize) - trainable per-layer parameter.\n+    /// </summary>\n+    /// <remarks>\n+    /// This vector scales the VeRA output on a per-dimension basis.\n+    /// This is the VeRA component of DVoRA.\n+    /// </remarks>\n+    private Vector<T> _scalingVectorD;\n+\n+    /// <summary>\n+    /// Scaling vector b (rank) - trainable per-layer parameter.\n+    /// </summary>\n+    /// <remarks>\n+    /// This vector scales the intermediate rank-dimensional representation.\n+    /// This is the VeRA component of DVoRA.\n+    /// </remarks>\n+    private Vector<T> _scalingVectorB;\n+\n+    /// <summary>\n+    /// Gradient for magnitude vector computed during backpropagation.\n+    /// </summary>\n+    private Vector<T>? _magnitudeGradient;\n+\n+    /// <summary>\n+    /// Gradient for scaling vector d computed during backpropagation.\n+    /// </summary>\n+    private Vector<T>? _scalingVectorDGradient;\n+\n+    /// <summary>\n+    /// Gradient for scaling vector b computed during backpropagation.\n+    /// </summary>\n+    private Vector<T>? _scalingVectorBGradient;\n+\n+    /// <summary>\n+    /// Cached normalized direction from the last forward pass, used in backpropagation.\n+    /// </summary>\n+    private Matrix<T>? _lastNormalizedDirection;\n+\n+    /// <summary>\n+    /// Stored input from the forward pass, needed for gradient computation.\n+    /// </summary>\n+    private Tensor<T>? _lastInput;\n+\n+    /// <summary>\n+    /// Stored intermediate value from forward pass, needed for backward pass.\n+    /// </summary>\n+    private Matrix<T>? _lastIntermediate;\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters.\n+    /// </summary>\n+    /// <remarks>\n+    /// DVoRA parameters = magnitude (outputSize) + d_scale (outputSize) + b_scale (rank).\n+    /// This is only slightly more than VeRA (adds magnitude vector) but much fewer than DoRA (no full LoRA matrices).\n+    /// </remarks>\n+    public override int ParameterCount\n+    {\n+        get\n+        {\n+            int dvoraParams = _magnitude.Length + _scalingVectorD.Length + _scalingVectorB.Length;\n+            return _freezeBaseLayer ? dvoraParams : (_baseLayer.ParameterCount + dvoraParams);\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Initializes a new DVoRA adapter wrapping an existing layer.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with DVoRA.</param>\n+    /// <param name=\"rank\">The rank of the low-rank decomposition (shared across all DVoRA layers).</param>\n+    /// <param name=\"alpha\">The scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"InvalidOperationException\">Thrown when shared matrices are not initialized.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// Before creating any DVoRA adapters, you must call InitializeSharedMatrices() once to set up\n+    /// the shared random matrices that all DVoRA layers will use.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This creates a DVoRA adapter for a layer. Unlike standard LoRA,\n+    /// you must initialize the shared random matrices first by calling:\n+    ///\n+    /// DVoRAAdapter&lt;T&gt;.InitializeSharedMatrices(inputSize, outputSize, rank);\n+    ///\n+    /// This needs to be done once before creating any DVoRA adapters.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to adapt\n+    /// - rank: How much compression (lower = fewer parameters)\n+    /// - alpha: How strong the adaptation is\n+    /// - freezeBaseLayer: Whether to lock the original layer's weights (usually true)\n+    /// </para>\n+    /// </remarks>\n+    public DVoRAAdapter(ILayer<T> baseLayer, int rank, double alpha = -1, bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        if (baseLayer == null)\n+        {\n+            throw new ArgumentNullException(nameof(baseLayer));\n+        }\n+\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+\n+        // Ensure shared matrices are initialized\n+        if (_sharedMatrixA == null || _sharedMatrixB == null)\n+        {\n+            throw new InvalidOperationException(\n+                \"Shared matrices must be initialized before creating DVoRA adapters. \" +\n+                \"Call DVoRAAdapter<T>.InitializeSharedMatrices(inputSize, outputSize, rank) first.\");\n+        }\n+\n+        // Validate shared matrix dimensions match this layer\n+        if (_sharedMatrixA.Rows != inputSize || _sharedMatrixA.Columns != rank)\n+        {\n+            throw new ArgumentException(\n+                $\"Shared matrix A dimensions ({_sharedMatrixA.Rows}√ó{_sharedMatrixA.Columns}) \" +\n+                $\"do not match required dimensions ({inputSize}√ó{rank})\", nameof(baseLayer));\n+        }\n+\n+        if (_sharedMatrixB.Rows != rank || _sharedMatrixB.Columns != outputSize)\n+        {\n+            throw new ArgumentException(\n+                $\"Shared matrix B dimensions ({_sharedMatrixB.Rows}√ó{_sharedMatrixB.Columns}) \" +\n+                $\"do not match required dimensions ({rank}√ó{outputSize})\", nameof(baseLayer));\n+        }\n+\n+        // Initialize magnitude from base layer weights (DoRA component)\n+        _magnitude = new Vector<T>(outputSize);\n+        DecomposeWeights();\n+\n+        // Initialize scaling vectors to ones (VeRA component - no initial effect)\n+        _scalingVectorD = new Vector<T>(outputSize);\n+        _scalingVectorB = new Vector<T>(rank);\n+\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            _scalingVectorD[i] = NumOps.One;\n+        }\n+\n+        for (int i = 0; i < rank; i++)\n+        {\n+            _scalingVectorB[i] = NumOps.One;\n+        }\n+\n+        // Update parameter vector\n+        Parameters = new Vector<T>(ParameterCount);\n+        UpdateParametersFromComponents();\n+    }\n+\n+    /// <summary>\n+    /// Initializes the shared random matrices used by all DVoRA adapters.\n+    /// </summary>\n+    /// <param name=\"inputSize\">The input dimension for the layers.</param>\n+    /// <param name=\"outputSize\">The output dimension for the layers.</param>\n+    /// <param name=\"rank\">The rank of the low-rank decomposition.</param>\n+    /// <param name=\"seed\">Optional random seed for reproducibility.</param>\n+    /// <remarks>\n+    /// <para>\n+    /// This method must be called once before creating any DVoRA adapters. It initializes the\n+    /// shared matrices A and B with random values that are frozen (never trained).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Call this once at the start before creating any DVoRA layers:\n+    ///\n+    /// // Initialize shared random matrices (do this once)\n+    /// DVoRAAdapter&lt;double&gt;.InitializeSharedMatrices(inputSize: 784, outputSize: 128, rank: 8);\n+    ///\n+    /// // Now create DVoRA adapters (they will use the shared matrices)\n+    /// var adapter1 = new DVoRAAdapter&lt;double&gt;(layer1, rank: 8);\n+    /// var adapter2 = new DVoRAAdapter&lt;double&gt;(layer2, rank: 8);\n+    ///\n+    /// All adapters share the same random A and B matrices, saving memory!\n+    /// </para>\n+    /// </remarks>\n+    public static void InitializeSharedMatrices(int inputSize, int outputSize, int rank, int? seed = null)\n+    {\n+        lock (_initLock)\n+        {\n+            Random rng = seed.HasValue ? new Random(seed.Value) : new Random();\n+            var ops = MathHelper.GetNumericOperations<T>();\n+\n+            // Initialize matrix A (inputSize √ó rank) with Gaussian random values\n+            _sharedMatrixA = new Matrix<T>(inputSize, rank);\n+            T stddevA = ops.Sqrt(ops.Divide(ops.One, ops.FromDouble(rank)));\n+            for (int i = 0; i < inputSize; i++)\n+            {\n+                for (int j = 0; j < rank; j++)\n+                {\n+                    // Box-Muller transform for Gaussian random numbers\n+                    double u1 = rng.NextDouble();\n+                    double u2 = rng.NextDouble();\n+                    double randStdNormal = Math.Sqrt(-2.0 * Math.Log(u1)) * Math.Sin(2.0 * Math.PI * u2);\n+                    _sharedMatrixA[i, j] = ops.Multiply(ops.FromDouble(randStdNormal), stddevA);\n+                }\n+            }\n+\n+            // Initialize matrix B (rank √ó outputSize) with Gaussian random values\n+            _sharedMatrixB = new Matrix<T>(rank, outputSize);\n+            T stddevB = ops.Sqrt(ops.Divide(ops.One, ops.FromDouble(rank)));\n+            for (int i = 0; i < rank; i++)\n+            {\n+                for (int j = 0; j < outputSize; j++)\n+                {\n+                    // Box-Muller transform for Gaussian random numbers\n+                    double u1 = rng.NextDouble();\n+                    double u2 = rng.NextDouble();\n+                    double randStdNormal = Math.Sqrt(-2.0 * Math.Log(u1)) * Math.Sin(2.0 * Math.PI * u2);\n+                    _sharedMatrixB[i, j] = ops.Multiply(ops.FromDouble(randStdNormal), stddevB);\n+                }\n+            }\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Resets the shared matrices (useful for testing or reinitializing).\n+    /// </summary>\n+    public static void ResetSharedMatrices()\n+    {\n+        lock (_initLock)\n+        {\n+            _sharedMatrixA = null;\n+            _sharedMatrixB = null;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Gets whether the shared matrices have been initialized.\n+    /// </summary>\n+    public static bool AreSharedMatricesInitialized => _sharedMatrixA != null && _sharedMatrixB != null;\n+\n+    /// <summary>\n+    /// Decomposes the base layer's weights into magnitude and direction components.\n+    /// </summary>\n+    /// <remarks>\n+    /// This is the DoRA component of DVoRA. For each output neuron:\n+    /// 1. Extract the weight vector\n+    /// 2. Compute the L2 norm (magnitude)\n+    /// 3. Store the magnitude\n+    ///\n+    /// The direction is implicitly W/||W|| and doesn't need to be stored separately.\n+    /// </remarks>\n+    private void DecomposeWeights()\n+    {\n+        Vector<T> baseParams = _baseLayer.GetParameters();\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        int weightCount = inputSize * outputSize;\n+\n+        // For each output neuron, compute the magnitude of its weight vector\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            T sumSquares = NumOps.Zero;\n+\n+            // Sum squares of all weights for this output neuron\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                int idx = i * inputSize + j;\n+                if (idx < weightCount && idx < baseParams.Length)\n+                {\n+                    T weight = baseParams[idx];\n+                    sumSquares = NumOps.Add(sumSquares, NumOps.Multiply(weight, weight));\n+                }\n+            }\n+\n+            // Magnitude is the L2 norm\n+            _magnitude[i] = NumOps.Sqrt(sumSquares);\n+\n+            // Ensure magnitude is never zero (for numerical stability)\n+            if (NumOps.Equals(_magnitude[i], NumOps.Zero))\n+            {\n+                _magnitude[i] = NumOps.FromDouble(1e-8);\n+            }\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Normalizes a matrix row-wise (each row becomes a unit vector).\n+    /// </summary>\n+    /// <param name=\"matrix\">The matrix to normalize.</param>\n+    /// <returns>Row-normalized matrix where each row has unit L2 norm.</returns>\n+    private Matrix<T> NormalizeRows(Matrix<T> matrix)\n+    {\n+        int rows = matrix.Rows;\n+        int cols = matrix.Columns;\n+        Matrix<T> normalized = new Matrix<T>(rows, cols);\n+\n+        for (int i = 0; i < rows; i++)\n+        {\n+            // Compute L2 norm of row\n+            T sumSquares = NumOps.Zero;\n+            for (int j = 0; j < cols; j++)\n+            {\n+                T val = matrix[i, j];\n+                sumSquares = NumOps.Add(sumSquares, NumOps.Multiply(val, val));\n+            }\n+\n+            T norm = NumOps.Sqrt(sumSquares);\n+\n+            // Avoid division by zero\n+            if (NumOps.Equals(norm, NumOps.Zero))\n+            {\n+                norm = NumOps.FromDouble(1e-8);\n+            }\n+\n+            // Normalize row\n+            for (int j = 0; j < cols; j++)\n+            {\n+                normalized[i, j] = NumOps.Divide(matrix[i, j], norm);\n+            }\n+        }\n+\n+        return normalized;\n+    }\n+\n+    /// <summary>\n+    /// Recomposes weights from magnitude and direction components.\n+    /// </summary>\n+    /// <param name=\"direction\">The normalized direction matrix.</param>\n+    /// <returns>The full weight matrix (magnitude * direction).</returns>\n+    private Matrix<T> RecomposeWeights(Matrix<T> direction)\n+    {\n+        int outputSize = direction.Rows;\n+        int inputSize = direction.Columns;\n+        Matrix<T> weights = new Matrix<T>(outputSize, inputSize);\n+\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                weights[i, j] = NumOps.Multiply(_magnitude[i], direction[i, j]);\n+            }\n+        }\n+\n+        return weights;\n+    }\n+\n+    /// <summary>\n+    /// Creates a dummy LoRA layer (not used since DVoRA uses custom logic).\n+    /// </summary>\n+    protected override LoRALayer<T> CreateLoRALayer(int rank, double alpha)\n+    {\n+        // DVoRA doesn't use a standard LoRA layer, but we need to satisfy the base class\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        return new LoRALayer<T>(inputSize, outputSize, rank, alpha);\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass through the DVoRA adapter.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Output combining base layer with DVoRA-adapted weights.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The DVoRA forward pass combines DoRA and VeRA:\n+    /// 1. Gets base layer weights W\n+    /// 2. Computes direction: d = W / ||W|| (DoRA)\n+    /// 3. Applies VeRA to direction: d' = d + d_scale * (B * A * input) * b_scale (VeRA)\n+    /// 4. Normalizes adapted direction: d_norm = d' / ||d'|| (DoRA)\n+    /// 5. Recomposes weights: W' = m * d_norm (DoRA)\n+    /// 6. Computes output: y = input @ W'^T\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is where DVoRA combines both techniques:\n+    ///\n+    /// DoRA part:\n+    /// - Split weights into magnitude (strength) and direction\n+    /// - Keep magnitude separate, work only with direction\n+    ///\n+    /// VeRA part:\n+    /// - Apply shared random matrices + tiny scaling vectors to the direction\n+    ///\n+    /// Final step:\n+    /// - Normalize the adjusted direction\n+    /// - Multiply magnitude back in\n+    /// - Use these hybrid-adapted weights for prediction\n+    ///\n+    /// Result: Stability of DoRA + efficiency of VeRA = best of both worlds!\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        _lastInput = input.Clone();\n+\n+        // Get base layer parameters and extract weights\n+        Vector<T> baseParams = _baseLayer.GetParameters();\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        int weightCount = inputSize * outputSize;\n+\n+        // Extract weight matrix from base layer\n+        Matrix<T> baseWeights = new Matrix<T>(outputSize, inputSize);\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                int weightIdx = i * inputSize + j;\n+                if (weightIdx < weightCount && weightIdx < baseParams.Length)\n+                {\n+                    baseWeights[i, j] = baseParams[weightIdx];\n+                }\n+                else\n+                {\n+                    baseWeights[i, j] = NumOps.Zero;\n+                }\n+            }\n+        }\n+\n+        // Compute base direction (W / ||W||) - DoRA component\n+        Matrix<T> baseDirection = NormalizeRows(baseWeights);\n+\n+        // Apply VeRA to get direction delta\n+        int batchSize = input.Shape[0];\n+        int rank = _scalingVectorB.Length;\n+\n+        // Convert input to matrix [batchSize, inputSize]\n+        Matrix<T> inputMatrix = new Matrix<T>(batchSize, inputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                inputMatrix[i, j] = input[i * inputSize + j];\n+            }\n+        }\n+\n+        // VeRA forward: (B * A * input) with scaling vectors\n+        // Compute: input * A (shared, frozen) ‚Üí [batchSize, rank]\n+        Matrix<T> afterA = inputMatrix.Multiply(_sharedMatrixA!);\n+\n+        // Apply scaling vector b element-wise: afterA * diag(b) ‚Üí [batchSize, rank]\n+        Matrix<T> afterB = new Matrix<T>(batchSize, rank);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < rank; j++)\n+            {\n+                afterB[i, j] = NumOps.Multiply(afterA[i, j], _scalingVectorB[j]);\n+            }\n+        }\n+\n+        // Compute: afterB * B (shared, frozen) ‚Üí [batchSize, outputSize]\n+        Matrix<T> afterSharedB = afterB.Multiply(_sharedMatrixB!);\n+        _lastIntermediate = afterSharedB.Clone();\n+\n+        // Apply scaling vector d element-wise: afterSharedB * diag(d) ‚Üí [batchSize, outputSize]\n+        Matrix<T> veraContribution = new Matrix<T>(batchSize, outputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                veraContribution[i, j] = NumOps.Multiply(afterSharedB[i, j], _scalingVectorD[j]);\n+            }\n+        }\n+\n+        // Apply alpha/rank scaling\n+        T scaling = NumOps.Divide(NumOps.FromDouble(Alpha), NumOps.FromDouble(Rank));\n+\n+        // For direction update, we need the VeRA contribution as a weight delta, not an output\n+        // Average over batch to get per-weight contribution\n+        Matrix<T> veraWeightDelta = new Matrix<T>(outputSize, inputSize);\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                T sum = NumOps.Zero;\n+                for (int b = 0; b < batchSize; b++)\n+                {\n+                    // Approximate weight gradient contribution\n+                    T contrib = NumOps.Multiply(veraContribution[b, i], inputMatrix[b, j]);\n+                    sum = NumOps.Add(sum, contrib);\n+                }\n+                veraWeightDelta[i, j] = NumOps.Multiply(\n+                    NumOps.Divide(sum, NumOps.FromDouble(batchSize)),\n+                    scaling);\n+            }\n+        }\n+\n+        // Add VeRA delta to base direction: d' = d + delta\n+        Matrix<T> adaptedDirection = new Matrix<T>(outputSize, inputSize);\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                adaptedDirection[i, j] = NumOps.Add(baseDirection[i, j], veraWeightDelta[i, j]);\n+            }\n+        }\n+\n+        // Normalize the adapted direction: d_norm = d' / ||d'|| - DoRA component\n+        _lastNormalizedDirection = NormalizeRows(adaptedDirection);\n+\n+        // Recompose weights: W' = m * d_norm - DoRA component\n+        Matrix<T> finalWeights = RecomposeWeights(_lastNormalizedDirection);\n+\n+        // Compute output: y = input @ W'^T\n+        Matrix<T> outputMatrix = inputMatrix.Multiply(finalWeights.Transpose());\n+\n+        // Convert back to tensor\n+        Vector<T> outputData = new Vector<T>(batchSize * outputSize);\n+        int idx = 0;\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                outputData[idx++] = outputMatrix[i, j];\n+            }\n+        }\n+\n+        return new Tensor<T>(new[] { batchSize, outputSize }, outputData);\n+    }\n+\n+    /// <summary>\n+    /// Performs the backward pass through the DVoRA adapter.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient flowing back from the next layer.</param>\n+    /// <returns>Gradient to pass to the previous layer.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The backward pass computes gradients for:\n+    /// 1. Magnitude parameters (DoRA component, one per output neuron)\n+    /// 2. Scaling vectors d and b (VeRA component, per-layer)\n+    /// 3. Base layer weights (if not frozen)\n+    ///\n+    /// The shared matrices A and B remain frozen and are never updated.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is where DVoRA learns! During backpropagation:\n+    /// 1. Compute gradients for magnitude (DoRA learning)\n+    /// 2. Compute gradients for scaling vectors d and b (VeRA learning)\n+    /// 3. Shared matrices A and B stay frozen (VeRA efficiency)\n+    /// 4. Pass gradients back to earlier layers\n+    ///\n+    /// We only train: magnitude + d + b = very few parameters!\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (_lastInput == null || _lastNormalizedDirection == null || _lastIntermediate == null)\n+        {\n+            throw new InvalidOperationException(\"Forward pass must be called before backward pass\");\n+        }\n+\n+        int batchSize = outputGradient.Shape[0];\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        int rank = _scalingVectorB.Length;\n+\n+        // Convert gradient to matrix\n+        Matrix<T> gradMatrix = new Matrix<T>(batchSize, outputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                gradMatrix[i, j] = outputGradient[i * outputSize + j];\n+            }\n+        }\n+\n+        T scaling = NumOps.Divide(NumOps.FromDouble(Alpha), NumOps.FromDouble(Rank));\n+\n+        // Compute magnitude gradients (DoRA component)\n+        _magnitudeGradient = new Vector<T>(outputSize);\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            T gradSum = NumOps.Zero;\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                gradSum = NumOps.Add(gradSum, gradMatrix[b, i]);\n+            }\n+            _magnitudeGradient[i] = gradSum;\n+        }\n+\n+        // Compute gradient for scaling vector d (VeRA component)\n+        _scalingVectorDGradient = new Vector<T>(outputSize);\n+        for (int j = 0; j < outputSize; j++)\n+        {\n+            T sum = NumOps.Zero;\n+            for (int i = 0; i < batchSize; i++)\n+            {\n+                T grad = NumOps.Multiply(gradMatrix[i, j], _lastIntermediate[i, j]);\n+                grad = NumOps.Multiply(grad, scaling);\n+                sum = NumOps.Add(sum, grad);\n+            }\n+            _scalingVectorDGradient[j] = sum;\n+        }\n+\n+        // Propagate gradient back through d scaling\n+        Matrix<T> gradAfterSharedB = new Matrix<T>(batchSize, outputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                gradAfterSharedB[i, j] = NumOps.Multiply(\n+                    NumOps.Multiply(gradMatrix[i, j], _scalingVectorD[j]),\n+                    scaling);\n+            }\n+        }\n+\n+        // Propagate through shared B\n+        Matrix<T> gradAfterB = gradAfterSharedB.Multiply(_sharedMatrixB!.Transpose());\n+\n+        // Convert input to matrix for gradient computation\n+        Matrix<T> inputMatrix = new Matrix<T>(batchSize, inputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                inputMatrix[i, j] = _lastInput[i * inputSize + j];\n+            }\n+        }\n+\n+        // Compute intermediate: input * A\n+        Matrix<T> afterA = inputMatrix.Multiply(_sharedMatrixA!);\n+\n+        // Compute gradient for scaling vector b (VeRA component)\n+        _scalingVectorBGradient = new Vector<T>(rank);\n+        for (int j = 0; j < rank; j++)\n+        {\n+            T sum = NumOps.Zero;\n+            for (int i = 0; i < batchSize; i++)\n+            {\n+                T grad = NumOps.Multiply(gradAfterB[i, j], afterA[i, j]);\n+                sum = NumOps.Add(sum, grad);\n+            }\n+            _scalingVectorBGradient[j] = sum;\n+        }\n+\n+        // Propagate gradient back through b scaling\n+        Matrix<T> gradAfterA = new Matrix<T>(batchSize, rank);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < rank; j++)\n+            {\n+                gradAfterA[i, j] = NumOps.Multiply(gradAfterB[i, j], _scalingVectorB[j]);\n+            }\n+        }\n+\n+        // Propagate through shared A\n+        Matrix<T> veraInputGrad = gradAfterA.Multiply(_sharedMatrixA!.Transpose());\n+\n+        // Backward through base layer (if not frozen)\n+        Tensor<T> baseInputGrad;\n+        if (!_freezeBaseLayer)\n+        {\n+            baseInputGrad = _baseLayer.Backward(outputGradient);\n+        }\n+        else\n+        {\n+            // Create zero gradient for base layer\n+            baseInputGrad = new Tensor<T>(_lastInput.Shape);\n+        }\n+\n+        // Sum input gradients from DVoRA and base layer\n+        Vector<T> inputGradData = new Vector<T>(batchSize * inputSize);\n+        int idx = 0;\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                T dvoraGrad = veraInputGrad[i, j];\n+                T baseGrad = baseInputGrad[i * inputSize + j];\n+                inputGradData[idx++] = NumOps.Add(dvoraGrad, baseGrad);\n+            }\n+        }\n+\n+        // Update parameter gradients\n+        UpdateParameterGradientsFromComponents();\n+\n+        return new Tensor<T>(new[] { batchSize, inputSize }, inputGradData);\n+    }\n+\n+    /// <summary>\n+    /// Updates parameters using the specified learning rate.\n+    /// </summary>\n+    /// <param name=\"learningRate\">The learning rate for parameter updates.</param>\n+    public override void UpdateParameters(T learningRate)\n+    {\n+        if (_magnitudeGradient == null || _scalingVectorDGradient == null || _scalingVectorBGradient == null)\n+        {\n+            return;\n+        }\n+\n+        // Update magnitude parameters (DoRA component)\n+        for (int i = 0; i < _magnitude.Length; i++)\n+        {\n+            T update = NumOps.Multiply(_magnitudeGradient[i], learningRate);\n+            _magnitude[i] = NumOps.Subtract(_magnitude[i], update);\n+\n+            // Ensure magnitude stays positive\n+            if (NumOps.LessThan(_magnitude[i], NumOps.FromDouble(1e-8)))\n+            {\n+                _magnitude[i] = NumOps.FromDouble(1e-8);\n+            }\n+        }\n+\n+        // Update scaling vector d (VeRA component)\n+        for (int i = 0; i < _scalingVectorD.Length; i++)\n+        {\n+            T update = NumOps.Multiply(_scalingVectorDGradient[i], learningRate);\n+            _scalingVectorD[i] = NumOps.Subtract(_scalingVectorD[i], update);\n+        }\n+\n+        // Update scaling vector b (VeRA component)\n+        for (int i = 0; i < _scalingVectorB.Length; i++)\n+        {\n+            T update = NumOps.Multiply(_scalingVectorBGradient[i], learningRate);\n+            _scalingVectorB[i] = NumOps.Subtract(_scalingVectorB[i], update);\n+        }\n+\n+        // Update base layer if not frozen\n+        if (!_freezeBaseLayer)\n+        {\n+            _baseLayer.UpdateParameters(learningRate);\n+        }\n+\n+        // Update parameter vector\n+        UpdateParametersFromComponents();\n+    }\n+\n+    /// <summary>\n+    /// Gets the current parameters as a vector.\n+    /// </summary>\n+    /// <returns>Vector containing all DVoRA parameters (magnitude, d, b).</returns>\n+    public override Vector<T> GetParameters()\n+    {\n+        return Parameters.Clone();\n+    }\n+\n+    /// <summary>\n+    /// Sets the layer parameters from a vector.\n+    /// </summary>\n+    /// <param name=\"parameters\">Vector containing all parameters.</param>\n+    public override void SetParameters(Vector<T> parameters)\n+    {\n+        if (parameters.Length != ParameterCount)\n+        {\n+            throw new ArgumentException($\"Expected {ParameterCount} parameters, got {parameters.Length}\", nameof(parameters));\n+        }\n+\n+        Parameters = parameters.Clone();\n+        UpdateComponentsFromParameters();\n+    }\n+\n+    /// <summary>\n+    /// Updates the parameter vector from the current component states.\n+    /// </summary>\n+    private void UpdateParametersFromComponents()\n+    {\n+        int idx = 0;\n+\n+        // Pack base layer parameters (if not frozen)\n+        if (!_freezeBaseLayer)\n+        {\n+            Vector<T> baseParams = _baseLayer.GetParameters();\n+            for (int i = 0; i < baseParams.Length; i++)\n+            {\n+                Parameters[idx++] = baseParams[i];\n+            }\n+        }\n+\n+        // Pack magnitude parameters\n+        for (int i = 0; i < _magnitude.Length; i++)\n+        {\n+            Parameters[idx++] = _magnitude[i];\n+        }\n+\n+        // Pack scaling vector d\n+        for (int i = 0; i < _scalingVectorD.Length; i++)\n+        {\n+            Parameters[idx++] = _scalingVectorD[i];\n+        }\n+\n+        // Pack scaling vector b\n+        for (int i = 0; i < _scalingVectorB.Length; i++)\n+        {\n+            Parameters[idx++] = _scalingVectorB[i];\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Updates the components from the parameter vector.\n+    /// </summary>\n+    private void UpdateComponentsFromParameters()\n+    {\n+        int idx = 0;\n+\n+        // Unpack base layer parameters (if not frozen)\n+        if (!_freezeBaseLayer)\n+        {\n+            int baseParamCount = _baseLayer.ParameterCount;\n+            Vector<T> baseParams = new Vector<T>(baseParamCount);\n+            for (int i = 0; i < baseParamCount; i++)\n+            {\n+                baseParams[i] = Parameters[idx++];\n+            }\n+            _baseLayer.SetParameters(baseParams);\n+        }\n+\n+        // Unpack magnitude parameters\n+        for (int i = 0; i < _magnitude.Length; i++)\n+        {\n+            _magnitude[i] = Parameters[idx++];\n+        }\n+\n+        // Unpack scaling vector d\n+        for (int i = 0; i < _scalingVectorD.Length; i++)\n+        {\n+            _scalingVectorD[i] = Parameters[idx++];\n+        }\n+\n+        // Unpack scaling vector b\n+        for (int i = 0; i < _scalingVectorB.Length; i++)\n+        {\n+            _scalingVectorB[i] = Parameters[idx++];\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Updates the parameter gradients vector from the component gradients.\n+    /// </summary>\n+    private void UpdateParameterGradientsFromComponents()\n+    {\n+        if (_magnitudeGradient == null || _scalingVectorDGradient == null || _scalingVectorBGradient == null)\n+        {\n+            return;\n+        }\n+\n+        ParameterGradients = new Vector<T>(ParameterCount);\n+        int idx = 0;\n+\n+        // Pack base layer gradients (if not frozen)\n+        if (!_freezeBaseLayer)\n+        {\n+            Vector<T> baseGrads = _baseLayer.GetParameterGradients();\n+            for (int i = 0; i < baseGrads.Length; i++)\n+            {\n+                ParameterGradients[idx++] = baseGrads[i];\n+            }\n+        }\n+\n+        // Pack magnitude gradients\n+        for (int i = 0; i < _magnitudeGradient.Length; i++)\n+        {\n+            ParameterGradients[idx++] = _magnitudeGradient[i];\n+        }\n+\n+        // Pack scaling vector d gradients\n+        for (int i = 0; i < _scalingVectorDGradient.Length; i++)\n+        {\n+            ParameterGradients[idx++] = _scalingVectorDGradient[i];\n+        }\n+\n+        // Pack scaling vector b gradients\n+        for (int i = 0; i < _scalingVectorBGradient.Length; i++)\n+        {\n+            ParameterGradients[idx++] = _scalingVectorBGradient[i];\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Merges the DVoRA adaptation into the base layer and returns the merged layer.\n+    /// </summary>\n+    /// <returns>A new layer with DVoRA weights merged into the base layer's weights.</returns>\n+    /// <exception cref=\"InvalidOperationException\">Thrown when the base layer type is not supported for merging.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// This method creates a final layer with the DVoRA adaptations baked in.\n+    /// The merged weights combine DoRA's magnitude-direction decomposition with VeRA's adaptation:\n+    /// W' = m * normalize(d + VeRA_contribution)\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This \"bakes in\" your DVoRA adaptation for deployment.\n+    ///\n+    /// After training with DVoRA, you probably want to deploy a simpler model without\n+    /// all the DVoRA machinery. This method creates that simpler model by:\n+    /// 1. Computing the VeRA contribution to direction\n+    /// 2. Adding it to the base direction\n+    /// 3. Normalizing the result (DoRA)\n+    /// 4. Multiplying by magnitude (DoRA)\n+    /// 5. Creating a new layer with these merged weights\n+    ///\n+    /// The result is a standard layer that behaves like your DVoRA-adapted model\n+    /// but is faster to run because it doesn't need the DVoRA computation at runtime.\n+    /// </para>\n+    /// </remarks>\n+    public override ILayer<T> MergeToOriginalLayer()\n+    {\n+        if (_sharedMatrixA == null || _sharedMatrixB == null)\n+        {\n+            throw new InvalidOperationException(\"Shared matrices are not initialized\");\n+        }\n+\n+        DenseLayer<T>? denseBase = _baseLayer as DenseLayer<T>;\n+        FullyConnectedLayer<T>? fcBase = _baseLayer as FullyConnectedLayer<T>;\n+\n+        if (denseBase == null && fcBase == null)\n+        {\n+            throw new InvalidOperationException(\"DVoRAAdapter currently only supports DenseLayer or FullyConnectedLayer base layers for merging\");\n+        }\n+\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        int rank = _scalingVectorB.Length;\n+\n+        // Get base layer weights\n+        Vector<T> baseParams = _baseLayer.GetParameters();\n+        Matrix<T> baseWeights = new Matrix<T>(outputSize, inputSize);\n+        int weightCount = inputSize * outputSize;\n+\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                int weightIdx = i * inputSize + j;\n+                if (weightIdx < weightCount && weightIdx < baseParams.Length)\n+                {\n+                    baseWeights[i, j] = baseParams[weightIdx];\n+                }\n+            }\n+        }\n+\n+        // Compute base direction\n+        Matrix<T> baseDirection = NormalizeRows(baseWeights);\n+\n+        // Compute VeRA weight contribution: d * B * A * b * scaling\n+        T scaling = NumOps.Divide(NumOps.FromDouble(Alpha), NumOps.FromDouble(Rank));\n+\n+        // Apply b scaling to A: A_scaled = A * diag(b)\n+        Matrix<T> aScaled = new Matrix<T>(inputSize, rank);\n+        for (int i = 0; i < inputSize; i++)\n+        {\n+            for (int j = 0; j < rank; j++)\n+            {\n+                aScaled[i, j] = NumOps.Multiply(_sharedMatrixA[i, j], _scalingVectorB[j]);\n+            }\n+        }\n+\n+        // Multiply by B: intermediate = A_scaled * B\n+        Matrix<T> intermediate = aScaled.Multiply(_sharedMatrixB);\n+\n+        // Apply d scaling: W_vera = intermediate * diag(d) * scaling\n+        Matrix<T> veraWeights = new Matrix<T>(inputSize, outputSize);\n+        for (int i = 0; i < inputSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                veraWeights[i, j] = NumOps.Multiply(\n+                    NumOps.Multiply(intermediate[i, j], _scalingVectorD[j]),\n+                    scaling);\n+            }\n+        }\n+\n+        // Transpose to match direction matrix format [outputSize, inputSize]\n+        Matrix<T> veraWeightsTransposed = veraWeights.Transpose();\n+\n+        // Add VeRA contribution to base direction\n+        Matrix<T> adaptedDirection = new Matrix<T>(outputSize, inputSize);\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                adaptedDirection[i, j] = NumOps.Add(baseDirection[i, j], veraWeightsTransposed[i, j]);\n+            }\n+        }\n+\n+        // Normalize the adapted direction\n+        Matrix<T> normalizedDirection = NormalizeRows(adaptedDirection);\n+\n+        // Recompose with magnitude: W' = m * d_norm\n+        Matrix<T> finalWeights = RecomposeWeights(normalizedDirection);\n+\n+        // Create merged parameters (weights + biases)\n+        Vector<T> mergedParams = new Vector<T>(baseParams.Length);\n+\n+        // Copy merged weights\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                int weightIdx = i * inputSize + j;\n+                mergedParams[weightIdx] = finalWeights[i, j];\n+            }\n+        }\n+\n+        // Copy biases unchanged\n+        for (int i = weightCount; i < baseParams.Length; i++)\n+        {\n+            mergedParams[i] = baseParams[i];\n+        }\n+\n+        // Create new dense layer with merged parameters\n+        DenseLayer<T> mergedLayer = new DenseLayer<T>(inputSize, outputSize, (IActivationFunction<T>?)null);\n+        mergedLayer.SetParameters(mergedParams);\n+","path":"src/LoRA/Adapters/DVoRAAdapter.cs","commit_id":"489180133c7a9db9460c84a6174f60284717b952","original_commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Carry the activation through when merging.**\n\nThe merged layer is created with `null` activation, losing any non-linear function from the original Dense/FullyConnected layer. Post-merge inference will therefore differ from the adapted model. Please clone the activation (and other essential settings) from the base layer instead of defaulting to `null`.\n\n```diff\n-        DenseLayer<T> mergedLayer = new DenseLayer<T>(inputSize, outputSize, (IActivationFunction<T>?)null);\n+        var activation = denseBase?.ActivationFunction ?? fcBase?.ActivationFunction;\n+        DenseLayer<T> mergedLayer = new DenseLayer<T>(inputSize, outputSize, activation);\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        // Create new dense layer with merged parameters\n        var activation = denseBase?.ActivationFunction ?? fcBase?.ActivationFunction;\n        DenseLayer<T> mergedLayer = new DenseLayer<T>(inputSize, outputSize, activation);\n        mergedLayer.SetParameters(mergedParams);\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/LoRA/Adapters/DVoRAAdapter.cs around lines 1096 to 1099, the merged\nDenseLayer is being constructed with a null activation which drops the original\nlayer's non-linear function; instead, obtain the activation and other essential\nsettings (activation function instance, bias usage, any dropout/normalization\nflags, layer name/metadata) from the original/base dense layer and pass them\ninto the DenseLayer constructor (or assign them immediately after construction)\nso the merged layer preserves the original behavior, then set the merged\nparameters as before.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:42:00Z","updated_at":"2025-11-02T02:42:05Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484118398","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118398"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484118398"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118398/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":1096,"original_start_line":1096,"start_side":"RIGHT","line":1099,"original_line":1099,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":1099,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118403","pull_request_review_id":3408015014,"id":2484118403,"node_id":"PRRC_kwDOKSXUF86UEKOD","diff_hunk":"@@ -0,0 +1,602 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// DyLoRA (Dynamic LoRA) adapter that trains with multiple ranks simultaneously.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// DyLoRA extends the standard LoRA approach by training multiple rank configurations simultaneously\n+/// using a nested dropout technique. This allows a single trained adapter to be deployed at different\n+/// rank levels without retraining, providing flexibility for different hardware constraints or\n+/// performance requirements.\n+/// </para>\n+/// <para>\n+/// The key innovation is nested dropout: during training, for each forward pass, a random rank r\n+/// is selected from the active ranks, and only the first r components of matrices A and B are used.\n+/// This ensures that smaller ranks can function independently and don't rely on higher-rank components.\n+/// </para>\n+/// <para><b>For Beginners:</b> DyLoRA is like LoRA with a superpower - flexibility!\n+///\n+/// Standard LoRA problem:\n+/// - You choose rank=8 and train\n+/// - Later realize rank=4 would work fine (save memory/speed)\n+/// - Or need rank=16 for better quality\n+/// - Must retrain from scratch with the new rank\n+///\n+/// DyLoRA solution:\n+/// - Train once with multiple ranks (e.g., [2, 4, 8, 16])\n+/// - Deploy with ANY of those ranks without retraining\n+/// - Switch between ranks at runtime based on device capabilities\n+///\n+/// How it works:\n+/// 1. Train with MaxRank (e.g., 16) but randomly use smaller ranks during training\n+/// 2. Nested dropout ensures each rank works independently\n+/// 3. After training, pick deployment rank based on needs (2=fastest, 16=best quality)\n+///\n+/// Use cases:\n+/// - Deploy same model to mobile (rank=2) and server (rank=16)\n+/// - Dynamic quality scaling based on battery level\n+/// - A/B testing different rank/quality trade-offs\n+/// - Training once, deploying everywhere\n+///\n+/// Example: Train with ActiveRanks=[2,4,8], deploy with:\n+/// - Rank=2 for mobile devices (98% parameter reduction, good quality)\n+/// - Rank=4 for tablets (95% parameter reduction, better quality)\n+/// - Rank=8 for desktops (90% parameter reduction, best quality)\n+/// </para>\n+/// </remarks>\n+public class DyLoRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Maximum rank for the LoRA decomposition.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This is the highest rank that can be used during inference. The actual matrices A and B\n+    /// are sized for this maximum rank, but smaller ranks can be used by only accessing the\n+    /// first r columns/rows.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is the \"full size\" of your LoRA adapter. You can always\n+    /// use a smaller rank, but you can't exceed this maximum without retraining.\n+    /// </para>\n+    /// </remarks>\n+    private readonly int _maxRank;\n+\n+    /// <summary>\n+    /// Array of ranks to train simultaneously during nested dropout.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// During training, each forward pass randomly selects one of these ranks and only uses\n+    /// that many components. This ensures all these ranks are viable for deployment.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> These are the rank options you can choose from after training.\n+    /// For example, [2, 4, 8, 16] means you can deploy with any of these four ranks.\n+    /// </para>\n+    /// </remarks>\n+    private readonly int[] _activeRanks;\n+\n+    /// <summary>\n+    /// Current rank to use during inference (forward pass in eval mode).\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This determines how many components of the LoRA matrices are used during inference.\n+    /// Can be changed at runtime to trade off between speed and quality.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is the \"deployment rank\" - the actual rank you're using\n+    /// right now for predictions. You can change this at any time without retraining!\n+    /// </para>\n+    /// </remarks>\n+    private int _currentDeploymentRank;\n+\n+    /// <summary>\n+    /// Random number generator for nested dropout during training.\n+    /// </summary>\n+    private readonly Random _random;\n+\n+    /// <summary>\n+    /// Whether the adapter is in training mode (uses nested dropout).\n+    /// </summary>\n+    private bool _isTraining;\n+\n+    /// <summary>\n+    /// Gets the maximum rank of the DyLoRA adapter.\n+    /// </summary>\n+    public int MaxRank => _maxRank;\n+\n+    /// <summary>\n+    /// Gets the array of active ranks used during training.\n+    /// </summary>\n+    public int[] ActiveRanks => _activeRanks.ToArray();\n+\n+    /// <summary>\n+    /// Gets or sets the current deployment rank used during inference.\n+    /// </summary>\n+    /// <exception cref=\"ArgumentException\">Thrown when attempting to set a rank not in ActiveRanks.</exception>\n+    public int CurrentDeploymentRank\n+    {\n+        get => _currentDeploymentRank;\n+        set => SetDeploymentRank(value);\n+    }\n+\n+    /// <summary>\n+    /// Gets or sets whether the adapter is in training mode.\n+    /// </summary>\n+    /// <remarks>\n+    /// When in training mode, nested dropout is applied. In eval mode, the deployment rank is used.\n+    /// </remarks>\n+    public bool IsTraining\n+    {\n+        get => _isTraining;\n+        set => _isTraining = value;\n+    }\n+\n+    /// <summary>\n+    /// Initializes a new DyLoRA adapter with the specified parameters.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with DyLoRA.</param>\n+    /// <param name=\"maxRank\">The maximum rank of the LoRA decomposition.</param>\n+    /// <param name=\"activeRanks\">Array of ranks to train simultaneously (must be sorted ascending and all &lt;= maxRank).</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to maxRank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer or activeRanks is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when activeRanks is invalid.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This creates a DyLoRA adapter that can train and deploy with multiple ranks.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to make flexible and efficient\n+    /// - maxRank: The maximum rank you might need (e.g., 16)\n+    /// - activeRanks: Which ranks to make available (e.g., [2, 4, 8, 16])\n+    /// - alpha: How strong the LoRA adaptation is (usually equals maxRank)\n+    /// - freezeBaseLayer: Whether to lock the original layer (usually true)\n+    ///\n+    /// Example:\n+    /// new DyLoRAAdapter(denseLayer, maxRank: 16, activeRanks: [2, 4, 8, 16])\n+    /// This trains a single adapter that can deploy with ranks 2, 4, 8, or 16.\n+    /// </para>\n+    /// </remarks>\n+    public DyLoRAAdapter(\n+        ILayer<T> baseLayer,\n+        int maxRank,\n+        int[] activeRanks,\n+        double alpha = -1,\n+        bool freezeBaseLayer = true)\n+        : base(baseLayer, maxRank, alpha, freezeBaseLayer)\n+    {\n+        if (activeRanks == null)\n+        {\n+            throw new ArgumentNullException(nameof(activeRanks));\n+        }\n+\n+        if (activeRanks.Length == 0)\n+        {\n+            throw new ArgumentException(\"ActiveRanks must contain at least one rank\", nameof(activeRanks));\n+        }\n+\n+        // Validate activeRanks are sorted and within bounds\n+        for (int i = 0; i < activeRanks.Length; i++)\n+        {\n+            if (activeRanks[i] <= 0)\n+            {\n+                throw new ArgumentException($\"All ranks must be positive, but activeRanks[{i}] = {activeRanks[i]}\", nameof(activeRanks));\n+            }\n+\n+            if (activeRanks[i] > maxRank)\n+            {\n+                throw new ArgumentException($\"All ranks must be <= maxRank ({maxRank}), but activeRanks[{i}] = {activeRanks[i]}\", nameof(activeRanks));\n+            }\n+\n+            if (i > 0 && activeRanks[i] <= activeRanks[i - 1])\n+            {\n+                throw new ArgumentException(\"ActiveRanks must be sorted in ascending order with no duplicates\", nameof(activeRanks));\n+            }\n+        }\n+\n+        _maxRank = maxRank;\n+        _activeRanks = activeRanks.ToArray();\n+        _currentDeploymentRank = activeRanks[activeRanks.Length - 1]; // Default to highest rank\n+        _random = new Random();\n+        _isTraining = true; // Start in training mode\n+    }\n+\n+    /// <summary>\n+    /// Sets the deployment rank for inference.\n+    /// </summary>\n+    /// <param name=\"rank\">The rank to use (must be in ActiveRanks).</param>\n+    /// <exception cref=\"ArgumentException\">Thrown when rank is not in ActiveRanks.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// This allows switching between different ranks at runtime without retraining.\n+    /// The rank must be one of the ActiveRanks that were trained.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This changes the quality/speed trade-off of your model.\n+    /// Higher rank = better quality but slower. Lower rank = faster but slightly lower quality.\n+    ///\n+    /// Example usage:\n+    /// - Battery low? adapter.SetDeploymentRank(2) for speed\n+    /// - Plugged in? adapter.SetDeploymentRank(16) for quality\n+    /// - On mobile? adapter.SetDeploymentRank(4) for balance\n+    /// </para>\n+    /// </remarks>\n+    public void SetDeploymentRank(int rank)\n+    {\n+        if (!_activeRanks.Contains(rank))\n+        {\n+            throw new ArgumentException(\n+                $\"Deployment rank {rank} is not in ActiveRanks [{string.Join(\", \", _activeRanks)}]. \" +\n+                $\"Only trained ranks can be used for deployment.\",\n+                nameof(rank));\n+        }\n+\n+        _currentDeploymentRank = rank;\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass with dynamic rank selection.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Sum of base layer output and DyLoRA output.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// During training, a random rank is selected from ActiveRanks for nested dropout.\n+    /// During inference, the CurrentDeploymentRank is used consistently.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This processes input through both the base layer and DyLoRA:\n+    ///\n+    /// Training mode:\n+    /// - Randomly picks a rank from ActiveRanks each forward pass\n+    /// - Uses only that many components of A and B matrices\n+    /// - This trains all ranks to work independently\n+    ///\n+    /// Inference mode:\n+    /// - Always uses CurrentDeploymentRank\n+    /// - Consistent behavior for production\n+    /// - Can change rank without retraining\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        // Select rank for this forward pass\n+        int activeRank = _isTraining\n+            ? _activeRanks[_random.Next(_activeRanks.Length)]  // Random rank during training\n+            : _currentDeploymentRank;                          // Fixed rank during inference\n+\n+        // Forward through base layer\n+        Tensor<T> baseOutput = _baseLayer.Forward(input);\n+\n+        // Forward through LoRA layer with restricted rank\n+        Tensor<T> loraOutput = ForwardWithRank(input, activeRank);\n+\n+        // Sum the outputs\n+        Tensor<T> result = new Tensor<T>(baseOutput.Shape);\n+        for (int i = 0; i < baseOutput.Length; i++)\n+        {\n+            result[i] = NumOps.Add(baseOutput[i], loraOutput[i]);\n+        }\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Performs forward pass through LoRA layer using only the first 'rank' components.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <param name=\"rank\">Number of components to use.</param>\n+    /// <returns>LoRA output tensor.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This restricts the LoRA computation to use only the first 'rank' columns of A and rows of B,\n+    /// implementing the nested dropout mechanism.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is the core of DyLoRA's flexibility. Instead of using all\n+    /// components of A and B, we only use the first 'rank' of them. This simulates what would happen\n+    /// if we had trained with that specific rank from the start.\n+    /// </para>\n+    /// </remarks>\n+    private Tensor<T> ForwardWithRank(Tensor<T> input, int rank)\n+    {\n+        // Get matrices A and B from the LoRA layer\n+        Matrix<T> fullA = _loraLayer.GetMatrixA();\n+        Matrix<T> fullB = _loraLayer.GetMatrixB();\n+\n+        // Extract submatrices using only the first 'rank' components\n+        // A: [inputSize, maxRank] -> [inputSize, rank]\n+        // B: [maxRank, outputSize] -> [rank, outputSize]\n+        int inputSize = fullA.Rows;\n+        int outputSize = fullB.Columns;\n+\n+        Matrix<T> subA = new Matrix<T>(inputSize, rank);\n+        for (int i = 0; i < inputSize; i++)\n+        {\n+            for (int j = 0; j < rank; j++)\n+            {\n+                subA[i, j] = fullA[i, j];\n+            }\n+        }\n+\n+        Matrix<T> subB = new Matrix<T>(rank, outputSize);\n+        for (int i = 0; i < rank; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                subB[i, j] = fullB[i, j];\n+            }\n+        }\n+\n+        // Compute forward pass with submatrices\n+        int batchSize = input.Shape[0];\n+        Matrix<T> inputMatrix = new Matrix<T>(batchSize, inputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                inputMatrix[i, j] = input[i * inputSize + j];\n+            }\n+        }\n+\n+        // input * A_sub * B_sub * scaling\n+        T scaling = _loraLayer.Scaling;\n+        Matrix<T> intermediate = inputMatrix.Multiply(subA);\n+        Matrix<T> output = intermediate.Multiply(subB).Multiply(scaling);\n+\n+        // Convert back to tensor\n+        Vector<T> outputData = new Vector<T>(batchSize * outputSize);\n+        int idx = 0;\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                outputData[idx++] = output[i, j];\n+            }\n+        }\n+\n+        return new Tensor<T>(new[] { batchSize, outputSize }, outputData);\n+    }\n+\n+    /// <summary>\n+    /// Performs the backward pass with nested dropout training.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient flowing back from the next layer.</param>\n+    /// <returns>Gradient to pass to the previous layer.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// During training, gradients are computed for all components, but the nested dropout ensures\n+    /// that only the active rank's components receive meaningful gradients. This trains all ranks\n+    /// simultaneously while ensuring each smaller rank can function independently.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is where DyLoRA learning happens! During backpropagation:\n+    ///\n+    /// 1. Gradients flow back through whichever rank was used in the forward pass\n+    /// 2. Only those components get updated\n+    /// 3. Over many iterations, all ranks get trained\n+    /// 4. Smaller ranks learn to work without relying on larger rank components\n+    ///\n+    /// This is why you can deploy with any trained rank - each one was trained independently!\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        // The base LoRA backward pass handles gradient computation\n+        // Nested dropout is automatically handled by the forward pass restriction\n+        return base.Backward(outputGradient);\n+    }","path":"src/LoRA/Adapters/DyLoRAAdapter.cs","commit_id":"489180133c7a9db9460c84a6174f60284717b952","original_commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Forward path never primes `_loraLayer`, breaking Backward**\n\n`Forward` bypasses `_loraLayer.Forward`, yet `Backward` delegates to `base.Backward`, which immediately calls `_loraLayer.Backward`. In `LoRAAdapterBase` that method assumes the LoRA layer just ran and has the latest activations cached; here it either blows up (no cached tensors) or accumulates gradients for stale inputs/ranks. Please run the real LoRA forward (or fully override Backward to stay rank-aware) before returning from `Forward`, and mask gradients for components above the sampled rank so the nested-dropout objective is honored.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/LoRA/Adapters/DyLoRAAdapter.cs around lines 262 to 387, Forward currently\ncomputes LoRA output via ForwardWithRank and never calls or primes _loraLayer,\nbut Backward relies on _loraLayer having cached activations; this causes\nmissing/stale activations and incorrect gradient accumulation. Fix by either (A)\ninvoking the actual _loraLayer.Forward (or a new\n_loraLayer.PrimeActivationsForRank(rank)) using the same sub-A/sub-B/rank so the\nlayer caches the correct activations before returning, and ensure those\nactivations reflect the sampled rank; or (B) fully override Backward here: store\nthe sampled rank and any intermediate tensors (inputMatrix/intermediate) in\ninstance fields during ForwardWithRank, then in Backward compute gradients\nmanually and mask gradient updates for components j >= rank so only active\ncomponents receive gradients, and return the gradient to the base input; choose\none approach and implement consistent activation caching and rank-aware gradient\nmasking so nested-dropout semantics are preserved.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:42:00Z","updated_at":"2025-11-02T02:42:05Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484118403","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118403"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484118403"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118403/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":262,"original_start_line":262,"start_side":"RIGHT","line":387,"original_line":387,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":387,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118405","pull_request_review_id":3408015014,"id":2484118405,"node_id":"PRRC_kwDOKSXUF86UEKOF","diff_hunk":"@@ -0,0 +1,290 @@\n+using AiDotNet.Interfaces;\n+using System;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// Implements Flora (Low-Rank Adapters Are Secretly Gradient Compressors) adapter for memory-efficient fine-tuning.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// Flora reinterprets LoRA as a gradient compression mechanism and achieves high-rank updates through\n+/// periodic resampling of projection matrices while maintaining sublinear space complexity for optimizer states.\n+/// </para>\n+/// <para><b>Research Paper:</b> \"Flora: Low-Rank Adapters Are Secretly Gradient Compressors\"\n+/// by Yongchang Hao et al., ICML 2024. arXiv:2402.03293\n+/// </para>\n+/// <para><b>Key Innovation:</b> Unlike standard LoRA which restricts weight updates to a fixed low-rank subspace,\n+/// Flora periodically resamples the projection matrices (A and B), allowing the effective rank of cumulative\n+/// updates to grow over time. This achieves performance comparable to full-rank fine-tuning while maintaining\n+/// the memory efficiency of LoRA.\n+/// </para>\n+/// </remarks>\n+public class FloraAdapter<T> : LoRAAdapterBase<T>\n+{\n+    private readonly int _resamplingInterval;\n+    private readonly int _rank;\n+    private int _currentStep;\n+    private Matrix<T>? _compressedMomentum;\n+    private Matrix<T>? _compressedSecondMoment;\n+    private readonly Random _random;\n+    private readonly double _momentumDecay;\n+    private readonly double _secondMomentDecay;\n+    private readonly bool _useAdaptiveLearningRate;\n+\n+    public FloraAdapter(\n+        ILayer<T> baseLayer,\n+        int rank,\n+        double alpha = -1,\n+        int resamplingInterval = 1000,\n+        double momentumDecay = 0.9,\n+        double secondMomentDecay = 0.999,\n+        bool useAdaptiveLearningRate = true,\n+        bool freezeBaseLayer = true,\n+        int seed = 42)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        if (resamplingInterval < 1)\n+        {\n+            throw new ArgumentException(\"Resampling interval must be at least 1\", nameof(resamplingInterval));\n+        }\n+\n+        _resamplingInterval = resamplingInterval;\n+        _rank = rank;\n+        _currentStep = 0;\n+        _momentumDecay = momentumDecay;\n+        _secondMomentDecay = secondMomentDecay;\n+        _useAdaptiveLearningRate = useAdaptiveLearningRate;\n+        _random = new Random(seed);\n+\n+        int outputSize = GetOutputShape()[0];\n+        _compressedMomentum = new Matrix<T>(rank, outputSize);\n+\n+        if (_useAdaptiveLearningRate)\n+        {\n+            _compressedSecondMoment = new Matrix<T>(rank, outputSize);\n+        }\n+    }\n+\n+    public int ResamplingInterval => _resamplingInterval;\n+    public int CurrentStep => _currentStep;\n+\n+    public override void UpdateParameters(T learningRate)\n+    {\n+        _currentStep++;\n+\n+        if (_currentStep % _resamplingInterval == 0)\n+        {\n+            ResampleProjectionMatrices();\n+        }\n+\n+        Vector<T> loraGradients = _loraLayer.GetParameterGradients();\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+\n+        Matrix<T> gradB = new Matrix<T>(_rank, outputSize);\n+        int bOffset = inputSize * _rank;\n+\n+        for (int i = 0; i < _rank; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                gradB[i, j] = loraGradients[bOffset + i * outputSize + j];\n+            }\n+        }\n+\n+        T beta1 = NumOps.FromDouble(_momentumDecay);\n+        T oneMinusBeta1 = NumOps.FromDouble(1.0 - _momentumDecay);\n+\n+        for (int i = 0; i < _rank; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                T oldMomentum = _compressedMomentum![i, j];\n+                T newMomentum = NumOps.Add(\n+                    NumOps.Multiply(beta1, oldMomentum),\n+                    NumOps.Multiply(oneMinusBeta1, gradB[i, j])\n+                );\n+                _compressedMomentum[i, j] = newMomentum;\n+            }\n+        }\n+\n+        if (_useAdaptiveLearningRate)\n+        {\n+            T beta2 = NumOps.FromDouble(_secondMomentDecay);\n+            T oneMinusBeta2 = NumOps.FromDouble(1.0 - _secondMomentDecay);\n+\n+            for (int i = 0; i < _rank; i++)\n+            {\n+                for (int j = 0; j < outputSize; j++)\n+                {\n+                    T grad = gradB[i, j];\n+                    T gradSquared = NumOps.Multiply(grad, grad);\n+                    T oldSecondMoment = _compressedSecondMoment![i, j];\n+                    T newSecondMoment = NumOps.Add(\n+                        NumOps.Multiply(beta2, oldSecondMoment),\n+                        NumOps.Multiply(oneMinusBeta2, gradSquared)\n+                    );\n+                    _compressedSecondMoment[i, j] = newSecondMoment;\n+                }\n+            }\n+        }\n+\n+        _loraLayer.UpdateParameters(learningRate);\n+\n+        if (!_freezeBaseLayer)\n+        {\n+            _baseLayer.UpdateParameters(learningRate);\n+        }\n+\n+        SyncParametersFromLayers();\n+    }\n+\n+    private void ResampleProjectionMatrices()\n+    {\n+        Vector<T> currentParams = _loraLayer.GetParameters();\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+\n+        Matrix<T> oldA = new Matrix<T>(inputSize, _rank);\n+        for (int i = 0; i < inputSize; i++)\n+        {\n+            for (int j = 0; j < _rank; j++)\n+            {\n+                oldA[i, j] = currentParams[i * _rank + j];\n+            }\n+        }\n+\n+        Matrix<T> newA = new Matrix<T>(inputSize, _rank);\n+        double stddev = 1.0 / Math.Sqrt(_rank);\n+        for (int i = 0; i < inputSize; i++)\n+        {\n+            for (int j = 0; j < _rank; j++)\n+            {\n+                double u1 = 1.0 - _random.NextDouble();\n+                double u2 = 1.0 - _random.NextDouble();\n+                double gaussianValue = Math.Sqrt(-2.0 * Math.Log(u1)) * Math.Cos(2.0 * Math.PI * u2);\n+                newA[i, j] = NumOps.FromDouble(gaussianValue * stddev);\n+            }\n+        }\n+\n+        Matrix<T> transferMatrix = ComputeTransferMatrix(oldA, newA);\n+        Matrix<T> newMomentum = MultiplyMatrices(_compressedMomentum!, transferMatrix);\n+        _compressedMomentum = newMomentum;\n+\n+        if (_useAdaptiveLearningRate && _compressedSecondMoment != null)\n+        {\n+            Matrix<T> newSecondMoment = MultiplyMatrices(_compressedSecondMoment, transferMatrix);\n+            _compressedSecondMoment = newSecondMoment;","path":"src/LoRA/Adapters/FloraAdapter.cs","commit_id":"489180133c7a9db9460c84a6174f60284717b952","original_commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Fix resampled momentum transform order**\n\n`_compressedMomentum` is `rank √ó output` while `transferMatrix` is `rank √ó rank`, so `_compressedMomentum * transferMatrix` violates the inner-dimension check in `MultiplyMatrices` and throws the first time `_currentStep % _resamplingInterval == 0`. Swap the order (and do the same for the second-moment branch) so the multiplication is well-defined and momentum state actually rotates into the new basis.\n\n```diff\n-        Matrix<T> newMomentum = MultiplyMatrices(_compressedMomentum!, transferMatrix);\n+        Matrix<T> newMomentum = MultiplyMatrices(transferMatrix, _compressedMomentum!);\n...\n-            Matrix<T> newSecondMoment = MultiplyMatrices(_compressedSecondMoment, transferMatrix);\n+            Matrix<T> newSecondMoment = MultiplyMatrices(transferMatrix, _compressedSecondMoment);\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        Matrix<T> transferMatrix = ComputeTransferMatrix(oldA, newA);\n        Matrix<T> newMomentum = MultiplyMatrices(transferMatrix, _compressedMomentum!);\n        _compressedMomentum = newMomentum;\n\n        if (_useAdaptiveLearningRate && _compressedSecondMoment != null)\n        {\n            Matrix<T> newSecondMoment = MultiplyMatrices(transferMatrix, _compressedSecondMoment);\n            _compressedSecondMoment = newSecondMoment;\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:42:00Z","updated_at":"2025-11-02T02:42:05Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484118405","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118405"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484118405"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118405/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":172,"original_start_line":172,"start_side":"RIGHT","line":179,"original_line":179,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":179,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118411","pull_request_review_id":3408015014,"id":2484118411,"node_id":"PRRC_kwDOKSXUF86UEKOL","diff_hunk":"@@ -0,0 +1,478 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// Generalized LoRA (GLoRA) implementation that adapts both weights AND activations.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// GLoRA extends standard LoRA by adding adaptation to both the layer's weights and its activations.\n+/// This provides more flexibility for multi-task learning scenarios where different tasks may need\n+/// different feature representations at each layer.\n+/// </para>\n+/// <para>\n+/// The forward pass computes:\n+/// - adapted_weights = base_weights + B_w * A_w (weight adaptation)\n+/// - base_output = input * adapted_weights\n+/// - adapted_output = base_output + B_a * A_a * input (activation adaptation)\n+/// </para>\n+/// <para><b>For Beginners:</b> While standard LoRA only adapts what the layer learns (its weights),\n+/// GLoRA also adapts what the layer produces (its activations). Think of it like this:\n+///\n+/// - Standard LoRA: Adjusts the \"recipe\" (weights) but produces the same type of output\n+/// - GLoRA: Adjusts both the \"recipe\" (weights) AND transforms the output for different uses\n+///\n+/// This is especially useful when:\n+/// 1. Different tasks need different feature representations\n+/// 2. You're doing multi-task learning (e.g., the same base features used differently)\n+/// 3. You need more flexibility than weight-only adaptation provides\n+///\n+/// Key differences from StandardLoRA:\n+/// - WeightAdaptation: Standard LoRA component that modifies layer weights\n+/// - ActivationAdaptation: Additional LoRA component that modifies layer outputs\n+/// - ActivationRank: Can be different from weight rank for fine-tuned control\n+///\n+/// Trade-offs:\n+/// + More flexible: Can adapt representations for different tasks\n+/// + Better for multi-task: Each task can use features differently\n+/// - More parameters: Two LoRA components instead of one\n+/// - Slightly slower: Two adaptation computations per forward pass\n+///\n+/// Example: For a 1000x1000 layer with weight_rank=8 and activation_rank=4:\n+/// - Weight adaptation: 16,000 parameters (same as standard LoRA)\n+/// - Activation adaptation: 8,000 additional parameters\n+/// - Total: 24,000 parameters (still 97.6% reduction from 1M!)\n+/// </para>\n+/// </remarks>\n+public class GLoRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// The LoRA layer that adapts activations (layer outputs).\n+    /// </summary>\n+    private readonly LoRALayer<T> _activationAdaptation;\n+\n+    /// <summary>\n+    /// Gets the weight adaptation LoRA layer.\n+    /// </summary>\n+    /// <remarks>\n+    /// This adapts the layer's weights using standard LoRA (B_w * A_w).\n+    /// </remarks>\n+    public LoRALayer<T> WeightAdaptation => _loraLayer;\n+\n+    /// <summary>\n+    /// Gets the activation adaptation LoRA layer.\n+    /// </summary>\n+    /// <remarks>\n+    /// This adapts the layer's outputs/activations using a second LoRA component (B_a * A_a).\n+    /// </remarks>\n+    public LoRALayer<T> ActivationAdaptation => _activationAdaptation;\n+\n+    /// <summary>\n+    /// Gets the rank of the activation adaptation.\n+    /// </summary>\n+    /// <remarks>\n+    /// This can be different from the weight adaptation rank, allowing for independent\n+    /// control over the complexity of weight vs. activation adaptations.\n+    /// </remarks>\n+    public int ActivationRank => _activationAdaptation.Rank;\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters (both weight and activation adaptations).\n+    /// </summary>\n+    /// <remarks>\n+    /// If the base layer is frozen, this returns the sum of weight and activation LoRA parameters.\n+    /// Otherwise, it includes base layer parameters as well.\n+    /// </remarks>\n+    public override int ParameterCount => _freezeBaseLayer\n+        ? (_loraLayer.ParameterCount + _activationAdaptation.ParameterCount)\n+        : (_baseLayer.ParameterCount + _loraLayer.ParameterCount + _activationAdaptation.ParameterCount);","path":"src/LoRA/Adapters/GLoRAAdapter.cs","commit_id":"489180133c7a9db9460c84a6174f60284717b952","original_commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Prevent `ParameterCount` from dereferencing `_activationAdaptation` before construction.**\n\n`LoRAAdapterBase` calls `ParameterCount` while its ctor is still running, so `_activationAdaptation` is still `null`. The current override therefore throws a `NullReferenceException` before the adapter even finishes constructing. Please guard against the null and defer counting the activation parameters until after `_activationAdaptation` is initialized.\n\n```diff\n-    public override int ParameterCount => _freezeBaseLayer\n-        ? (_loraLayer.ParameterCount + _activationAdaptation.ParameterCount)\n-        : (_baseLayer.ParameterCount + _loraLayer.ParameterCount + _activationAdaptation.ParameterCount);\n+    public override int ParameterCount\n+    {\n+        get\n+        {\n+            int baseParams = _freezeBaseLayer ? 0 : _baseLayer.ParameterCount;\n+            int weightParams = _loraLayer.ParameterCount;\n+            int activationParams = _activationAdaptation != null ? _activationAdaptation.ParameterCount : 0;\n+            return baseParams + weightParams + activationParams;\n+        }\n+    }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    public override int ParameterCount\n    {\n        get\n        {\n            int baseParams = _freezeBaseLayer ? 0 : _baseLayer.ParameterCount;\n            int weightParams = _loraLayer.ParameterCount;\n            int activationParams = _activationAdaptation != null ? _activationAdaptation.ParameterCount : 0;\n            return baseParams + weightParams + activationParams;\n        }\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/LoRA/Adapters/GLoRAAdapter.cs around lines 88 to 90, the ParameterCount\noverride dereferences _activationAdaptation which can be null during\nbase-constructor invocation; update the expression to avoid accessing\n_activationAdaptation when it is null (e.g. conditionally include\n_activationAdaptation.ParameterCount only if _activationAdaptation != null or\nuse a null-coalescing check), preserve the existing freeze-based branching\nlogic, and ensure the property returns the sum of only initialized components so\nit won't throw during construction.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:42:01Z","updated_at":"2025-11-02T02:42:05Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484118411","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118411"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484118411"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118411/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":88,"original_start_line":88,"start_side":"RIGHT","line":90,"original_line":90,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":90,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118417","pull_request_review_id":3408015014,"id":2484118417,"node_id":"PRRC_kwDOKSXUF86UEKOR","diff_hunk":"@@ -0,0 +1,819 @@\n+using AiDotNet.Interfaces;\n+using System.Collections.Generic;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// HRA (Hybrid Rank Adaptation) adapter that combines low-rank and full-rank updates for optimal parameter efficiency.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// HRA addresses a key limitation of standard LoRA: while low-rank updates are efficient, some parameters\n+/// benefit from full-rank updates. HRA uses a hybrid approach:\n+/// - Dense low-rank updates for most parameters (efficient, like LoRA)\n+/// - Sparse full-rank updates for critical parameters (precise, targeted)\n+/// - Importance-based allocation between the two components\n+/// </para>\n+/// <para>\n+/// The forward computation is: output = base_layer(input) + low_rank(input) + sparse_full_rank(input)\n+/// where the hybrid allocation provides the best of both worlds.\n+/// </para>\n+/// <para><b>For Beginners:</b> HRA is like having two tools instead of one:\n+///\n+/// Standard LoRA problem:\n+/// - Uses only low-rank updates (compressed, efficient)\n+/// - Some parameters need precise full-rank updates\n+/// - Full fine-tuning is too expensive\n+/// - Need something in between\n+///\n+/// HRA solution:\n+/// - Most parameters use low-rank updates (efficient, covers 95% of needs)\n+/// - Critical parameters get full-rank updates (precise, covers remaining 5%)\n+/// - Automatically learns which parameters are critical\n+/// - Best quality with minimal parameter overhead\n+///\n+/// Analogy: Think of home renovation:\n+/// - Low-rank updates: Paint the walls (cheap, covers large area, good enough)\n+/// - Full-rank updates: Replace key structural beams (expensive, small area, critical)\n+/// - HRA: Do both where appropriate for best results\n+///\n+/// How it works:\n+/// 1. Start with LoRA-style low-rank matrices (B * A)\n+/// 2. Add sparse full-rank updates for most important parameters\n+/// 3. Track importance scores during training\n+/// 4. Allocate parameter budget optimally between low-rank and sparse full-rank\n+///\n+/// Benefits:\n+/// - Better quality than pure LoRA (full-rank updates where needed)\n+/// - More efficient than full fine-tuning (most updates are low-rank)\n+/// - Adaptive: learns which parameters need full-rank updates\n+/// - Flexible: adjustable sparsity budget for full-rank component\n+///\n+/// Use cases:\n+/// - Tasks where LoRA quality is not quite sufficient\n+/// - Fine-tuning with specific architectural bottlenecks\n+/// - When you have slightly more parameter budget than LoRA but much less than full fine-tuning\n+/// - Domains where certain parameters are known to be critical\n+///\n+/// Example parameter comparison for a 1000x1000 layer:\n+/// - Full fine-tuning: 1,000,000 parameters\n+/// - Standard LoRA (rank=8): 16,000 parameters (98.4% reduction)\n+/// - HRA (rank=8, 1% sparsity): 26,000 parameters (97.4% reduction, better quality)\n+///\n+/// Reference: Based on \"Hybrid Rank Adaptation\" research combining low-rank and sparse full-rank approaches\n+/// </para>\n+/// </remarks>\n+public class HRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Sparse full-rank update matrix storing only non-zero entries.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This dictionary maps (row, col) positions to their update values.\n+    /// Only the most important parameters have non-zero entries here.\n+    /// This provides targeted full-rank updates while maintaining parameter efficiency.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is like a selective paint touch-up kit.\n+    /// Instead of repainting the whole wall (full-rank), we only fix the important spots\n+    /// that need precise attention. The dictionary only stores the spots we're fixing,\n+    /// saving memory.\n+    /// </para>\n+    /// </remarks>\n+    private Dictionary<(int row, int col), T> _sparseFullRankUpdates;\n+\n+    /// <summary>\n+    /// Importance scores for each parameter in the weight matrix.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Each score represents how important that parameter is for the adaptation.\n+    /// Higher scores indicate parameters that should receive full-rank updates.\n+    /// Lower scores indicate parameters that are fine with low-rank updates.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> These scores tell us which parameters are VIPs.\n+    /// High score = this parameter is critical, give it a full-rank update.\n+    /// Low score = this parameter is fine with a low-rank approximation.\n+    /// </para>\n+    /// </remarks>\n+    private Matrix<T> _parameterImportance;\n+\n+    /// <summary>\n+    /// Gradient accumulator for the sparse full-rank component.\n+    /// </summary>\n+    private Dictionary<(int row, int col), T>? _sparseGradients;\n+\n+    /// <summary>\n+    /// Maximum number of sparse full-rank parameters to allocate.\n+    /// </summary>\n+    /// <remarks>\n+    /// Controls the parameter budget for the sparse full-rank component.\n+    /// Typical values: 1-5% of total weight parameters.\n+    /// </remarks>\n+    private readonly int _maxSparseParams;\n+\n+    /// <summary>\n+    /// Sparsity ratio for full-rank updates (0.0 to 1.0).\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Determines what fraction of parameters can receive full-rank updates.\n+    /// For example, 0.01 means 1% of parameters can have full-rank updates.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is your \"special attention budget\".\n+    /// If you have 1000 parameters and sparsity=0.01, you can give 10 parameters\n+    /// the VIP treatment (full-rank updates). Choose wisely!\n+    /// </para>\n+    /// </remarks>\n+    private readonly double _sparsityRatio;\n+\n+    /// <summary>\n+    /// Number of training steps between importance updates.\n+    /// </summary>\n+    private readonly int _importanceUpdateInterval;\n+\n+    /// <summary>\n+    /// Current training step counter.\n+    /// </summary>\n+    private int _stepCount;\n+\n+    /// <summary>\n+    /// Exponential moving average factor for importance score updates.\n+    /// </summary>\n+    /// <remarks>\n+    /// Controls how quickly importance scores adapt to new gradient information.\n+    /// Typical values: 0.9 to 0.99 (higher = more smoothing, lower = faster adaptation).\n+    /// </remarks>\n+    private readonly double _importanceEMA;\n+\n+    /// <summary>\n+    /// Scaling factor for the sparse full-rank component.\n+    /// </summary>\n+    private readonly T _sparseScaling;\n+\n+    /// <summary>\n+    /// Whether to use dynamic importance-based allocation.\n+    /// </summary>\n+    private readonly bool _useDynamicAllocation;\n+\n+    /// <summary>\n+    /// Gets the number of active sparse full-rank parameters.\n+    /// </summary>\n+    public int ActiveSparseParams => _sparseFullRankUpdates.Count;\n+\n+    /// <summary>\n+    /// Gets the maximum allowed sparse parameters.\n+    /// </summary>\n+    public int MaxSparseParams => _maxSparseParams;\n+\n+    /// <summary>\n+    /// Gets the current sparsity ratio.\n+    /// </summary>\n+    public double SparsityRatio => _sparsityRatio;\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters (low-rank + sparse full-rank).\n+    /// </summary>\n+    public override int ParameterCount\n+    {\n+        get\n+        {\n+            int loraParams = _loraLayer.ParameterCount;\n+            int sparseParams = _sparseFullRankUpdates.Count;\n+            int baseParams = _freezeBaseLayer ? 0 : _baseLayer.ParameterCount;\n+            return baseParams + loraParams + sparseParams;\n+        }","path":"src/LoRA/Adapters/HRAAdapter.cs","commit_id":"489180133c7a9db9460c84a6174f60284717b952","original_commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Prevent constructor-time `NullReferenceException` in `ParameterCount`**  \nWhile the base constructor runs, `_sparseFullRankUpdates` is still null. This override dereferences it, so object creation blows up before the derived constructor body executes. Please guard the getter (for example, return `base.ParameterCount` until the dictionary is initialized) before accessing `_sparseFullRankUpdates`.  \n\n```diff\n-    public override int ParameterCount\n-    {\n-        get\n-        {\n-            int loraParams = _loraLayer.ParameterCount;\n-            int sparseParams = _sparseFullRankUpdates.Count;\n-            int baseParams = _freezeBaseLayer ? 0 : _baseLayer.ParameterCount;\n-            return baseParams + loraParams + sparseParams;\n-        }\n-    }\n+    public override int ParameterCount\n+    {\n+        get\n+        {\n+            if (_sparseFullRankUpdates == null)\n+            {\n+                return base.ParameterCount;\n+            }\n+\n+            int loraParams = _loraLayer.ParameterCount;\n+            int sparseParams = _sparseFullRankUpdates.Count;\n+            int baseParams = _freezeBaseLayer ? 0 : _baseLayer.ParameterCount;\n+            return baseParams + loraParams + sparseParams;\n+        }\n+    }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    public override int ParameterCount\n    {\n        get\n        {\n            if (_sparseFullRankUpdates == null)\n            {\n                return base.ParameterCount;\n            }\n\n            int loraParams = _loraLayer.ParameterCount;\n            int sparseParams = _sparseFullRankUpdates.Count;\n            int baseParams = _freezeBaseLayer ? 0 : _baseLayer.ParameterCount;\n            return baseParams + loraParams + sparseParams;\n        }\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/LoRA/Adapters/HRAAdapter.cs around lines 178 to 186, the ParameterCount\ngetter dereferences _sparseFullRankUpdates while the base constructor may run\nbefore that field is initialized, causing a NullReferenceException; update the\ngetter to guard against null (for example: if _sparseFullRankUpdates is null\nthen return base.ParameterCount or the safe sum without touching the dictionary)\nso no member is accessed until the derived constructor has initialized it, and\nkeep the existing logic for _freezeBaseLayer and other counts once the field is\nnon-null.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:42:01Z","updated_at":"2025-11-02T02:42:05Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484118417","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118417"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484118417"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118417/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":178,"original_start_line":178,"start_side":"RIGHT","line":186,"original_line":186,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":186,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118421","pull_request_review_id":3408015014,"id":2484118421,"node_id":"PRRC_kwDOKSXUF86UEKOV","diff_hunk":"@@ -0,0 +1,819 @@\n+using AiDotNet.Interfaces;\n+using System.Collections.Generic;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// HRA (Hybrid Rank Adaptation) adapter that combines low-rank and full-rank updates for optimal parameter efficiency.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// HRA addresses a key limitation of standard LoRA: while low-rank updates are efficient, some parameters\n+/// benefit from full-rank updates. HRA uses a hybrid approach:\n+/// - Dense low-rank updates for most parameters (efficient, like LoRA)\n+/// - Sparse full-rank updates for critical parameters (precise, targeted)\n+/// - Importance-based allocation between the two components\n+/// </para>\n+/// <para>\n+/// The forward computation is: output = base_layer(input) + low_rank(input) + sparse_full_rank(input)\n+/// where the hybrid allocation provides the best of both worlds.\n+/// </para>\n+/// <para><b>For Beginners:</b> HRA is like having two tools instead of one:\n+///\n+/// Standard LoRA problem:\n+/// - Uses only low-rank updates (compressed, efficient)\n+/// - Some parameters need precise full-rank updates\n+/// - Full fine-tuning is too expensive\n+/// - Need something in between\n+///\n+/// HRA solution:\n+/// - Most parameters use low-rank updates (efficient, covers 95% of needs)\n+/// - Critical parameters get full-rank updates (precise, covers remaining 5%)\n+/// - Automatically learns which parameters are critical\n+/// - Best quality with minimal parameter overhead\n+///\n+/// Analogy: Think of home renovation:\n+/// - Low-rank updates: Paint the walls (cheap, covers large area, good enough)\n+/// - Full-rank updates: Replace key structural beams (expensive, small area, critical)\n+/// - HRA: Do both where appropriate for best results\n+///\n+/// How it works:\n+/// 1. Start with LoRA-style low-rank matrices (B * A)\n+/// 2. Add sparse full-rank updates for most important parameters\n+/// 3. Track importance scores during training\n+/// 4. Allocate parameter budget optimally between low-rank and sparse full-rank\n+///\n+/// Benefits:\n+/// - Better quality than pure LoRA (full-rank updates where needed)\n+/// - More efficient than full fine-tuning (most updates are low-rank)\n+/// - Adaptive: learns which parameters need full-rank updates\n+/// - Flexible: adjustable sparsity budget for full-rank component\n+///\n+/// Use cases:\n+/// - Tasks where LoRA quality is not quite sufficient\n+/// - Fine-tuning with specific architectural bottlenecks\n+/// - When you have slightly more parameter budget than LoRA but much less than full fine-tuning\n+/// - Domains where certain parameters are known to be critical\n+///\n+/// Example parameter comparison for a 1000x1000 layer:\n+/// - Full fine-tuning: 1,000,000 parameters\n+/// - Standard LoRA (rank=8): 16,000 parameters (98.4% reduction)\n+/// - HRA (rank=8, 1% sparsity): 26,000 parameters (97.4% reduction, better quality)\n+///\n+/// Reference: Based on \"Hybrid Rank Adaptation\" research combining low-rank and sparse full-rank approaches\n+/// </para>\n+/// </remarks>\n+public class HRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Sparse full-rank update matrix storing only non-zero entries.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This dictionary maps (row, col) positions to their update values.\n+    /// Only the most important parameters have non-zero entries here.\n+    /// This provides targeted full-rank updates while maintaining parameter efficiency.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is like a selective paint touch-up kit.\n+    /// Instead of repainting the whole wall (full-rank), we only fix the important spots\n+    /// that need precise attention. The dictionary only stores the spots we're fixing,\n+    /// saving memory.\n+    /// </para>\n+    /// </remarks>\n+    private Dictionary<(int row, int col), T> _sparseFullRankUpdates;\n+\n+    /// <summary>\n+    /// Importance scores for each parameter in the weight matrix.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Each score represents how important that parameter is for the adaptation.\n+    /// Higher scores indicate parameters that should receive full-rank updates.\n+    /// Lower scores indicate parameters that are fine with low-rank updates.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> These scores tell us which parameters are VIPs.\n+    /// High score = this parameter is critical, give it a full-rank update.\n+    /// Low score = this parameter is fine with a low-rank approximation.\n+    /// </para>\n+    /// </remarks>\n+    private Matrix<T> _parameterImportance;\n+\n+    /// <summary>\n+    /// Gradient accumulator for the sparse full-rank component.\n+    /// </summary>\n+    private Dictionary<(int row, int col), T>? _sparseGradients;\n+\n+    /// <summary>\n+    /// Maximum number of sparse full-rank parameters to allocate.\n+    /// </summary>\n+    /// <remarks>\n+    /// Controls the parameter budget for the sparse full-rank component.\n+    /// Typical values: 1-5% of total weight parameters.\n+    /// </remarks>\n+    private readonly int _maxSparseParams;\n+\n+    /// <summary>\n+    /// Sparsity ratio for full-rank updates (0.0 to 1.0).\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Determines what fraction of parameters can receive full-rank updates.\n+    /// For example, 0.01 means 1% of parameters can have full-rank updates.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is your \"special attention budget\".\n+    /// If you have 1000 parameters and sparsity=0.01, you can give 10 parameters\n+    /// the VIP treatment (full-rank updates). Choose wisely!\n+    /// </para>\n+    /// </remarks>\n+    private readonly double _sparsityRatio;\n+\n+    /// <summary>\n+    /// Number of training steps between importance updates.\n+    /// </summary>\n+    private readonly int _importanceUpdateInterval;\n+\n+    /// <summary>\n+    /// Current training step counter.\n+    /// </summary>\n+    private int _stepCount;\n+\n+    /// <summary>\n+    /// Exponential moving average factor for importance score updates.\n+    /// </summary>\n+    /// <remarks>\n+    /// Controls how quickly importance scores adapt to new gradient information.\n+    /// Typical values: 0.9 to 0.99 (higher = more smoothing, lower = faster adaptation).\n+    /// </remarks>\n+    private readonly double _importanceEMA;\n+\n+    /// <summary>\n+    /// Scaling factor for the sparse full-rank component.\n+    /// </summary>\n+    private readonly T _sparseScaling;\n+\n+    /// <summary>\n+    /// Whether to use dynamic importance-based allocation.\n+    /// </summary>\n+    private readonly bool _useDynamicAllocation;\n+\n+    /// <summary>\n+    /// Gets the number of active sparse full-rank parameters.\n+    /// </summary>\n+    public int ActiveSparseParams => _sparseFullRankUpdates.Count;\n+\n+    /// <summary>\n+    /// Gets the maximum allowed sparse parameters.\n+    /// </summary>\n+    public int MaxSparseParams => _maxSparseParams;\n+\n+    /// <summary>\n+    /// Gets the current sparsity ratio.\n+    /// </summary>\n+    public double SparsityRatio => _sparsityRatio;\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters (low-rank + sparse full-rank).\n+    /// </summary>\n+    public override int ParameterCount\n+    {\n+        get\n+        {\n+            int loraParams = _loraLayer.ParameterCount;\n+            int sparseParams = _sparseFullRankUpdates.Count;\n+            int baseParams = _freezeBaseLayer ? 0 : _baseLayer.ParameterCount;\n+            return baseParams + loraParams + sparseParams;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Initializes a new HRA adapter with hybrid low-rank and sparse full-rank updates.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with HRA.</param>\n+    /// <param name=\"rank\">The rank of the low-rank decomposition.</param>\n+    /// <param name=\"sparsityRatio\">Fraction of parameters for sparse full-rank updates (0.0 to 1.0, default: 0.01).</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <param name=\"importanceUpdateInterval\">Steps between importance recalculation (default: 100).</param>\n+    /// <param name=\"importanceEMA\">EMA factor for importance smoothing (default: 0.95).</param>\n+    /// <param name=\"useDynamicAllocation\">Whether to dynamically reallocate sparse parameters (default: true).</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when parameters are invalid.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This creates an HRA adapter that combines two update strategies.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to adapt\n+    /// - rank: Size of the low-rank component (typical: 8-16)\n+    /// - sparsityRatio: Budget for full-rank updates (0.01 = 1% of parameters get special treatment)\n+    /// - alpha: Strength of the low-rank adaptation\n+    /// - freezeBaseLayer: Lock original weights (usually true)\n+    /// - importanceUpdateInterval: How often to reassess which parameters are important\n+    /// - importanceEMA: How stable importance scores are (higher = more stable)\n+    /// - useDynamicAllocation: Automatically move sparse budget to most important parameters\n+    ///\n+    /// Example:\n+    /// new HRAAdapter(layer, rank: 8, sparsityRatio: 0.01)\n+    /// This gives you LoRA-style updates for most parameters, plus precise updates for the top 1%.\n+    /// </para>\n+    /// </remarks>\n+    public HRAAdapter(\n+        ILayer<T> baseLayer,\n+        int rank,\n+        double sparsityRatio = 0.01,\n+        double alpha = -1,\n+        bool freezeBaseLayer = true,\n+        int importanceUpdateInterval = 100,\n+        double importanceEMA = 0.95,\n+        bool useDynamicAllocation = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        if (sparsityRatio < 0.0 || sparsityRatio > 1.0)\n+        {\n+            throw new ArgumentException(\"Sparsity ratio must be between 0 and 1\", nameof(sparsityRatio));\n+        }\n+\n+        if (importanceEMA <= 0 || importanceEMA >= 1)\n+        {\n+            throw new ArgumentException(\"Importance EMA factor must be between 0 and 1\", nameof(importanceEMA));\n+        }\n+\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        int totalWeightParams = inputSize * outputSize;\n+\n+        _sparsityRatio = sparsityRatio;\n+        _maxSparseParams = (int)(totalWeightParams * sparsityRatio);\n+        _importanceUpdateInterval = importanceUpdateInterval;\n+        _importanceEMA = importanceEMA;\n+        _useDynamicAllocation = useDynamicAllocation;\n+        _stepCount = 0;\n+\n+        // Initialize sparse full-rank updates (empty initially)\n+        _sparseFullRankUpdates = new Dictionary<(int row, int col), T>();\n+\n+        // Initialize importance scores (uniform initially)\n+        _parameterImportance = new Matrix<T>(outputSize, inputSize);\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                _parameterImportance[i, j] = NumOps.Zero;\n+            }\n+        }\n+\n+        // Sparse scaling factor (typically smaller than LoRA scaling)\n+        _sparseScaling = NumOps.FromDouble(0.1);\n+\n+        // Initialize parameters\n+        Parameters = new Vector<T>(ParameterCount);\n+        UpdateParametersFromComponents();\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass through the HRA adapter.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Sum of base layer output, low-rank LoRA output, and sparse full-rank output.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The HRA forward pass computes three components:\n+    /// 1. Base layer output (original behavior)\n+    /// 2. Low-rank LoRA output: scaling * B * A * input\n+    /// 3. Sparse full-rank output: sparse_scaling * S * input (where S is sparse)\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This processes input through three paths and adds them:\n+    /// 1. Original layer (base behavior)\n+    /// 2. LoRA low-rank path (efficient updates for most parameters)\n+    /// 3. Sparse full-rank path (precise updates for VIP parameters)\n+    ///\n+    /// Think of it as a team effort:\n+    /// - Base layer: The foundation\n+    /// - Low-rank: The general workforce (handles most of the load efficiently)\n+    /// - Sparse full-rank: The specialists (handle critical details precisely)\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        // 1. Forward through base layer\n+        Tensor<T> baseOutput = _baseLayer.Forward(input);\n+\n+        // 2. Forward through LoRA layer (low-rank component)\n+        Tensor<T> loraOutput = _loraLayer.Forward(input);\n+\n+        // 3. Forward through sparse full-rank component\n+        Tensor<T> sparseOutput = ForwardSparseFullRank(input);\n+\n+        // Sum all three components\n+        Tensor<T> result = new Tensor<T>(baseOutput.Shape);\n+        for (int i = 0; i < baseOutput.Length; i++)\n+        {\n+            T sum = NumOps.Add(baseOutput[i], loraOutput[i]);\n+            result[i] = NumOps.Add(sum, sparseOutput[i]);\n+        }\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Performs forward pass through the sparse full-rank component.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Sparse full-rank output tensor.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Computes output using only the sparse full-rank parameters.\n+    /// This is a standard matrix multiplication but using a sparse weight matrix.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This applies the \"specialist\" updates.\n+    /// Only the VIP parameters (stored in _sparseFullRankUpdates) are used here.\n+    /// Everything else is treated as zero, maintaining efficiency.\n+    /// </para>\n+    /// </remarks>\n+    private Tensor<T> ForwardSparseFullRank(Tensor<T> input)\n+    {\n+        int batchSize = input.Shape[0];\n+        int inputSize = input.Shape.Length > 1 ? input.Shape[1] : input.Length;\n+        int outputSize = GetOutputShape()[0];\n+\n+        // If no sparse parameters, return zeros\n+        if (_sparseFullRankUpdates.Count == 0)\n+        {\n+            Vector<T> zeroData = new Vector<T>(batchSize * outputSize);\n+            return new Tensor<T>(new[] { batchSize, outputSize }, zeroData);\n+        }\n+\n+        // Convert input to matrix [batchSize, inputSize]\n+        Matrix<T> inputMatrix = new Matrix<T>(batchSize, inputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                inputMatrix[i, j] = input[i * inputSize + j];\n+            }\n+        }\n+\n+        // Compute sparse matrix multiplication\n+        Matrix<T> output = new Matrix<T>(batchSize, outputSize);\n+        foreach (var kvp in _sparseFullRankUpdates)\n+        {\n+            int row = kvp.Key.row;\n+            int col = kvp.Key.col;\n+            T weight = NumOps.Multiply(kvp.Value, _sparseScaling);\n+\n+            // output[b, row] += weight * input[b, col]\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                T contribution = NumOps.Multiply(weight, inputMatrix[b, col]);\n+                output[b, row] = NumOps.Add(output[b, row], contribution);\n+            }\n+        }\n+\n+        // Convert back to tensor\n+        Vector<T> outputData = new Vector<T>(batchSize * outputSize);\n+        int idx = 0;\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                outputData[idx++] = output[i, j];\n+            }\n+        }\n+\n+        return new Tensor<T>(new[] { batchSize, outputSize }, outputData);\n+    }\n+\n+    /// <summary>\n+    /// Performs the backward pass through the HRA adapter.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient flowing back from the next layer.</param>\n+    /// <returns>Gradient to pass to the previous layer.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The backward pass computes gradients for:\n+    /// 1. Low-rank LoRA matrices (A and B)\n+    /// 2. Sparse full-rank parameters\n+    /// 3. Updates importance scores based on gradient magnitudes\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is where HRA learns which parameters are important!\n+    /// During backpropagation:\n+    /// 1. Compute gradients for low-rank component (standard LoRA)\n+    /// 2. Compute gradients for sparse full-rank parameters\n+    /// 3. Track which parameters have large gradients (they're important!)\n+    /// 4. Periodically reassign sparse budget to most important parameters\n+    ///\n+    /// This adaptive approach ensures the sparse full-rank budget is always\n+    /// allocated to the parameters that need it most.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        // Backward through LoRA layer\n+        Tensor<T> loraInputGrad = _loraLayer.Backward(outputGradient);\n+\n+        // Backward through sparse full-rank component\n+        Tensor<T> sparseInputGrad = BackwardSparseFullRank(outputGradient);\n+\n+        // Backward through base layer\n+        Tensor<T> baseInputGrad = _baseLayer.Backward(outputGradient);\n+\n+        // Update importance scores based on gradients\n+        UpdateImportanceScores(outputGradient);\n+\n+        // Increment step and check if we should reallocate sparse parameters\n+        _stepCount++;\n+        if (_useDynamicAllocation && _stepCount % _importanceUpdateInterval == 0)\n+        {\n+            ReallocateSparseParameters();\n+        }\n+\n+        // Sum input gradients\n+        Tensor<T> inputGrad = new Tensor<T>(loraInputGrad.Shape);\n+        for (int i = 0; i < loraInputGrad.Length; i++)\n+        {\n+            T sum = NumOps.Add(loraInputGrad[i], sparseInputGrad[i]);\n+            inputGrad[i] = NumOps.Add(sum, baseInputGrad[i]);\n+        }\n+\n+        return inputGrad;\n+    }\n+\n+    /// <summary>\n+    /// Performs backward pass through the sparse full-rank component.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Output gradient tensor.</param>\n+    /// <returns>Input gradient tensor.</returns>\n+    private Tensor<T> BackwardSparseFullRank(Tensor<T> outputGradient)\n+    {\n+        int batchSize = outputGradient.Shape[0];\n+        int outputSize = outputGradient.Shape.Length > 1 ? outputGradient.Shape[1] : outputGradient.Length;\n+        int inputSize = GetInputShape()[0];\n+\n+        // Initialize sparse gradients\n+        _sparseGradients = new Dictionary<(int row, int col), T>();\n+\n+        // If no sparse parameters, return zeros\n+        if (_sparseFullRankUpdates.Count == 0)\n+        {\n+            Vector<T> zeroData = new Vector<T>(batchSize * inputSize);\n+            return new Tensor<T>(new[] { batchSize, inputSize }, zeroData);\n+        }\n+\n+        // Convert gradient to matrix\n+        Matrix<T> gradMatrix = new Matrix<T>(batchSize, outputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                gradMatrix[i, j] = outputGradient[i * outputSize + j];\n+            }\n+        }\n+\n+        // Compute input gradients and parameter gradients\n+        Matrix<T> inputGradMatrix = new Matrix<T>(batchSize, inputSize);\n+\n+        foreach (var kvp in _sparseFullRankUpdates)\n+        {\n+            int row = kvp.Key.row;\n+            int col = kvp.Key.col;\n+            T weight = NumOps.Multiply(kvp.Value, _sparseScaling);\n+\n+            T paramGrad = NumOps.Zero;\n+\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                // Input gradient: dL/dInput[b, col] += weight * dL/dOutput[b, row]\n+                T grad = NumOps.Multiply(weight, gradMatrix[b, row]);\n+                inputGradMatrix[b, col] = NumOps.Add(inputGradMatrix[b, col], grad);\n+\n+                // Parameter gradient: dL/dWeight[row, col] += input[b, col] * dL/dOutput[b, row]\n+                // Note: We need input from forward pass, stored in base layer\n+                // For simplicity, accumulate gradient magnitude for importance\n+                paramGrad = NumOps.Add(paramGrad, NumOps.Abs(gradMatrix[b, row]));\n+            }\n+\n+            _sparseGradients[kvp.Key] = NumOps.Multiply(paramGrad, _sparseScaling);\n+        }\n+","path":"src/LoRA/Adapters/HRAAdapter.cs","commit_id":"489180133c7a9db9460c84a6174f60284717b952","original_commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Compute sparse gradients with the stored forward input**  \n`BackwardSparseFullRank` never uses the input activations, so it accumulates `|dL/dOutput|` instead of the true gradient `Œ£ input[b, col] * dL/dOutput[b, row]`. Consequently, sparse weights update in the wrong direction (or not at all), breaking learning. Please cache the sparse path‚Äôs input during `ForwardSparseFullRank` and reuse it here to compute the exact gradient for each `(row, col)` entry (and clear the cache afterward).  \n\n```diff\n-        _sparseGradients = new Dictionary<(int row, int col), T>();\n+        _sparseGradients = new Dictionary<(int row, int col), T>();\n@@\n-            T paramGrad = NumOps.Zero;\n-\n-            for (int b = 0; b < batchSize; b++)\n-            {\n-                // Input gradient: dL/dInput[b, col] += weight * dL/dOutput[b, row]\n-                T grad = NumOps.Multiply(weight, gradMatrix[b, row]);\n-                inputGradMatrix[b, col] = NumOps.Add(inputGradMatrix[b, col], grad);\n-\n-                // Parameter gradient: dL/dWeight[row, col] += input[b, col] * dL/dOutput[b, row]\n-                // Note: We need input from forward pass, stored in base layer\n-                // For simplicity, accumulate gradient magnitude for importance\n-                paramGrad = NumOps.Add(paramGrad, NumOps.Abs(gradMatrix[b, row]));\n-            }\n-\n-            _sparseGradients[kvp.Key] = NumOps.Multiply(paramGrad, _sparseScaling);\n+            T paramGrad = NumOps.Zero;\n+\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                T grad = NumOps.Multiply(weight, gradMatrix[b, row]);\n+                inputGradMatrix[b, col] = NumOps.Add(inputGradMatrix[b, col], grad);\n+\n+                T inputVal = _lastSparseInput![b * inputSize + col];\n+                paramGrad = NumOps.Add(paramGrad, NumOps.Multiply(inputVal, gradMatrix[b, row]));\n+            }\n+\n+            _sparseGradients[kvp.Key] = NumOps.Multiply(paramGrad, _sparseScaling);\n```\n*(Assumes you cache the last sparse-forward input in a `_lastSparseInput` tensor and clear it after use.)*\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/LoRA/Adapters/HRAAdapter.cs around lines 472 to 497,\nBackwardSparseFullRank is incorrectly computing parameter gradients by summing\nabsolute output gradients instead of using the forward-input activations; update\nit to read the cached sparse-path inputs (assumed stored as _lastSparseInput\nwith shape [batchSize, inputSize]) and compute paramGrad = Œ£_b\n_lastSparseInput[b, col] * gradMatrix[b, row] (then multiply by _sparseScaling),\ncontinue to compute input gradients as weight * gradMatrix but ensure you index\ninto _lastSparseInput for any needed input values, handle bounds and null-checks\n(throw or fallback if _lastSparseInput is missing), store the computed gradient\ninto _sparseGradients[kvp.Key], and clear _lastSparseInput after the loop to\nfree the cache.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:42:01Z","updated_at":"2025-11-02T02:42:05Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484118421","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118421"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484118421"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118421/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":472,"original_start_line":472,"start_side":"RIGHT","line":497,"original_line":497,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":497,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118427","pull_request_review_id":3408015014,"id":2484118427,"node_id":"PRRC_kwDOKSXUF86UEKOb","diff_hunk":"@@ -0,0 +1,819 @@\n+using AiDotNet.Interfaces;\n+using System.Collections.Generic;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// HRA (Hybrid Rank Adaptation) adapter that combines low-rank and full-rank updates for optimal parameter efficiency.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// HRA addresses a key limitation of standard LoRA: while low-rank updates are efficient, some parameters\n+/// benefit from full-rank updates. HRA uses a hybrid approach:\n+/// - Dense low-rank updates for most parameters (efficient, like LoRA)\n+/// - Sparse full-rank updates for critical parameters (precise, targeted)\n+/// - Importance-based allocation between the two components\n+/// </para>\n+/// <para>\n+/// The forward computation is: output = base_layer(input) + low_rank(input) + sparse_full_rank(input)\n+/// where the hybrid allocation provides the best of both worlds.\n+/// </para>\n+/// <para><b>For Beginners:</b> HRA is like having two tools instead of one:\n+///\n+/// Standard LoRA problem:\n+/// - Uses only low-rank updates (compressed, efficient)\n+/// - Some parameters need precise full-rank updates\n+/// - Full fine-tuning is too expensive\n+/// - Need something in between\n+///\n+/// HRA solution:\n+/// - Most parameters use low-rank updates (efficient, covers 95% of needs)\n+/// - Critical parameters get full-rank updates (precise, covers remaining 5%)\n+/// - Automatically learns which parameters are critical\n+/// - Best quality with minimal parameter overhead\n+///\n+/// Analogy: Think of home renovation:\n+/// - Low-rank updates: Paint the walls (cheap, covers large area, good enough)\n+/// - Full-rank updates: Replace key structural beams (expensive, small area, critical)\n+/// - HRA: Do both where appropriate for best results\n+///\n+/// How it works:\n+/// 1. Start with LoRA-style low-rank matrices (B * A)\n+/// 2. Add sparse full-rank updates for most important parameters\n+/// 3. Track importance scores during training\n+/// 4. Allocate parameter budget optimally between low-rank and sparse full-rank\n+///\n+/// Benefits:\n+/// - Better quality than pure LoRA (full-rank updates where needed)\n+/// - More efficient than full fine-tuning (most updates are low-rank)\n+/// - Adaptive: learns which parameters need full-rank updates\n+/// - Flexible: adjustable sparsity budget for full-rank component\n+///\n+/// Use cases:\n+/// - Tasks where LoRA quality is not quite sufficient\n+/// - Fine-tuning with specific architectural bottlenecks\n+/// - When you have slightly more parameter budget than LoRA but much less than full fine-tuning\n+/// - Domains where certain parameters are known to be critical\n+///\n+/// Example parameter comparison for a 1000x1000 layer:\n+/// - Full fine-tuning: 1,000,000 parameters\n+/// - Standard LoRA (rank=8): 16,000 parameters (98.4% reduction)\n+/// - HRA (rank=8, 1% sparsity): 26,000 parameters (97.4% reduction, better quality)\n+///\n+/// Reference: Based on \"Hybrid Rank Adaptation\" research combining low-rank and sparse full-rank approaches\n+/// </para>\n+/// </remarks>\n+public class HRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Sparse full-rank update matrix storing only non-zero entries.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This dictionary maps (row, col) positions to their update values.\n+    /// Only the most important parameters have non-zero entries here.\n+    /// This provides targeted full-rank updates while maintaining parameter efficiency.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is like a selective paint touch-up kit.\n+    /// Instead of repainting the whole wall (full-rank), we only fix the important spots\n+    /// that need precise attention. The dictionary only stores the spots we're fixing,\n+    /// saving memory.\n+    /// </para>\n+    /// </remarks>\n+    private Dictionary<(int row, int col), T> _sparseFullRankUpdates;\n+\n+    /// <summary>\n+    /// Importance scores for each parameter in the weight matrix.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Each score represents how important that parameter is for the adaptation.\n+    /// Higher scores indicate parameters that should receive full-rank updates.\n+    /// Lower scores indicate parameters that are fine with low-rank updates.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> These scores tell us which parameters are VIPs.\n+    /// High score = this parameter is critical, give it a full-rank update.\n+    /// Low score = this parameter is fine with a low-rank approximation.\n+    /// </para>\n+    /// </remarks>\n+    private Matrix<T> _parameterImportance;\n+\n+    /// <summary>\n+    /// Gradient accumulator for the sparse full-rank component.\n+    /// </summary>\n+    private Dictionary<(int row, int col), T>? _sparseGradients;\n+\n+    /// <summary>\n+    /// Maximum number of sparse full-rank parameters to allocate.\n+    /// </summary>\n+    /// <remarks>\n+    /// Controls the parameter budget for the sparse full-rank component.\n+    /// Typical values: 1-5% of total weight parameters.\n+    /// </remarks>\n+    private readonly int _maxSparseParams;\n+\n+    /// <summary>\n+    /// Sparsity ratio for full-rank updates (0.0 to 1.0).\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Determines what fraction of parameters can receive full-rank updates.\n+    /// For example, 0.01 means 1% of parameters can have full-rank updates.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is your \"special attention budget\".\n+    /// If you have 1000 parameters and sparsity=0.01, you can give 10 parameters\n+    /// the VIP treatment (full-rank updates). Choose wisely!\n+    /// </para>\n+    /// </remarks>\n+    private readonly double _sparsityRatio;\n+\n+    /// <summary>\n+    /// Number of training steps between importance updates.\n+    /// </summary>\n+    private readonly int _importanceUpdateInterval;\n+\n+    /// <summary>\n+    /// Current training step counter.\n+    /// </summary>\n+    private int _stepCount;\n+\n+    /// <summary>\n+    /// Exponential moving average factor for importance score updates.\n+    /// </summary>\n+    /// <remarks>\n+    /// Controls how quickly importance scores adapt to new gradient information.\n+    /// Typical values: 0.9 to 0.99 (higher = more smoothing, lower = faster adaptation).\n+    /// </remarks>\n+    private readonly double _importanceEMA;\n+\n+    /// <summary>\n+    /// Scaling factor for the sparse full-rank component.\n+    /// </summary>\n+    private readonly T _sparseScaling;\n+\n+    /// <summary>\n+    /// Whether to use dynamic importance-based allocation.\n+    /// </summary>\n+    private readonly bool _useDynamicAllocation;\n+\n+    /// <summary>\n+    /// Gets the number of active sparse full-rank parameters.\n+    /// </summary>\n+    public int ActiveSparseParams => _sparseFullRankUpdates.Count;\n+\n+    /// <summary>\n+    /// Gets the maximum allowed sparse parameters.\n+    /// </summary>\n+    public int MaxSparseParams => _maxSparseParams;\n+\n+    /// <summary>\n+    /// Gets the current sparsity ratio.\n+    /// </summary>\n+    public double SparsityRatio => _sparsityRatio;\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters (low-rank + sparse full-rank).\n+    /// </summary>\n+    public override int ParameterCount\n+    {\n+        get\n+        {\n+            int loraParams = _loraLayer.ParameterCount;\n+            int sparseParams = _sparseFullRankUpdates.Count;\n+            int baseParams = _freezeBaseLayer ? 0 : _baseLayer.ParameterCount;\n+            return baseParams + loraParams + sparseParams;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Initializes a new HRA adapter with hybrid low-rank and sparse full-rank updates.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with HRA.</param>\n+    /// <param name=\"rank\">The rank of the low-rank decomposition.</param>\n+    /// <param name=\"sparsityRatio\">Fraction of parameters for sparse full-rank updates (0.0 to 1.0, default: 0.01).</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <param name=\"importanceUpdateInterval\">Steps between importance recalculation (default: 100).</param>\n+    /// <param name=\"importanceEMA\">EMA factor for importance smoothing (default: 0.95).</param>\n+    /// <param name=\"useDynamicAllocation\">Whether to dynamically reallocate sparse parameters (default: true).</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when parameters are invalid.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This creates an HRA adapter that combines two update strategies.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to adapt\n+    /// - rank: Size of the low-rank component (typical: 8-16)\n+    /// - sparsityRatio: Budget for full-rank updates (0.01 = 1% of parameters get special treatment)\n+    /// - alpha: Strength of the low-rank adaptation\n+    /// - freezeBaseLayer: Lock original weights (usually true)\n+    /// - importanceUpdateInterval: How often to reassess which parameters are important\n+    /// - importanceEMA: How stable importance scores are (higher = more stable)\n+    /// - useDynamicAllocation: Automatically move sparse budget to most important parameters\n+    ///\n+    /// Example:\n+    /// new HRAAdapter(layer, rank: 8, sparsityRatio: 0.01)\n+    /// This gives you LoRA-style updates for most parameters, plus precise updates for the top 1%.\n+    /// </para>\n+    /// </remarks>\n+    public HRAAdapter(\n+        ILayer<T> baseLayer,\n+        int rank,\n+        double sparsityRatio = 0.01,\n+        double alpha = -1,\n+        bool freezeBaseLayer = true,\n+        int importanceUpdateInterval = 100,\n+        double importanceEMA = 0.95,\n+        bool useDynamicAllocation = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        if (sparsityRatio < 0.0 || sparsityRatio > 1.0)\n+        {\n+            throw new ArgumentException(\"Sparsity ratio must be between 0 and 1\", nameof(sparsityRatio));\n+        }\n+\n+        if (importanceEMA <= 0 || importanceEMA >= 1)\n+        {\n+            throw new ArgumentException(\"Importance EMA factor must be between 0 and 1\", nameof(importanceEMA));\n+        }\n+\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        int totalWeightParams = inputSize * outputSize;\n+\n+        _sparsityRatio = sparsityRatio;\n+        _maxSparseParams = (int)(totalWeightParams * sparsityRatio);\n+        _importanceUpdateInterval = importanceUpdateInterval;\n+        _importanceEMA = importanceEMA;\n+        _useDynamicAllocation = useDynamicAllocation;\n+        _stepCount = 0;\n+\n+        // Initialize sparse full-rank updates (empty initially)\n+        _sparseFullRankUpdates = new Dictionary<(int row, int col), T>();\n+\n+        // Initialize importance scores (uniform initially)\n+        _parameterImportance = new Matrix<T>(outputSize, inputSize);\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                _parameterImportance[i, j] = NumOps.Zero;\n+            }\n+        }\n+\n+        // Sparse scaling factor (typically smaller than LoRA scaling)\n+        _sparseScaling = NumOps.FromDouble(0.1);\n+\n+        // Initialize parameters\n+        Parameters = new Vector<T>(ParameterCount);\n+        UpdateParametersFromComponents();\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass through the HRA adapter.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Sum of base layer output, low-rank LoRA output, and sparse full-rank output.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The HRA forward pass computes three components:\n+    /// 1. Base layer output (original behavior)\n+    /// 2. Low-rank LoRA output: scaling * B * A * input\n+    /// 3. Sparse full-rank output: sparse_scaling * S * input (where S is sparse)\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This processes input through three paths and adds them:\n+    /// 1. Original layer (base behavior)\n+    /// 2. LoRA low-rank path (efficient updates for most parameters)\n+    /// 3. Sparse full-rank path (precise updates for VIP parameters)\n+    ///\n+    /// Think of it as a team effort:\n+    /// - Base layer: The foundation\n+    /// - Low-rank: The general workforce (handles most of the load efficiently)\n+    /// - Sparse full-rank: The specialists (handle critical details precisely)\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        // 1. Forward through base layer\n+        Tensor<T> baseOutput = _baseLayer.Forward(input);\n+\n+        // 2. Forward through LoRA layer (low-rank component)\n+        Tensor<T> loraOutput = _loraLayer.Forward(input);\n+\n+        // 3. Forward through sparse full-rank component\n+        Tensor<T> sparseOutput = ForwardSparseFullRank(input);\n+\n+        // Sum all three components\n+        Tensor<T> result = new Tensor<T>(baseOutput.Shape);\n+        for (int i = 0; i < baseOutput.Length; i++)\n+        {\n+            T sum = NumOps.Add(baseOutput[i], loraOutput[i]);\n+            result[i] = NumOps.Add(sum, sparseOutput[i]);\n+        }\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Performs forward pass through the sparse full-rank component.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Sparse full-rank output tensor.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Computes output using only the sparse full-rank parameters.\n+    /// This is a standard matrix multiplication but using a sparse weight matrix.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This applies the \"specialist\" updates.\n+    /// Only the VIP parameters (stored in _sparseFullRankUpdates) are used here.\n+    /// Everything else is treated as zero, maintaining efficiency.\n+    /// </para>\n+    /// </remarks>\n+    private Tensor<T> ForwardSparseFullRank(Tensor<T> input)\n+    {\n+        int batchSize = input.Shape[0];\n+        int inputSize = input.Shape.Length > 1 ? input.Shape[1] : input.Length;\n+        int outputSize = GetOutputShape()[0];\n+\n+        // If no sparse parameters, return zeros\n+        if (_sparseFullRankUpdates.Count == 0)\n+        {\n+            Vector<T> zeroData = new Vector<T>(batchSize * outputSize);\n+            return new Tensor<T>(new[] { batchSize, outputSize }, zeroData);\n+        }\n+\n+        // Convert input to matrix [batchSize, inputSize]\n+        Matrix<T> inputMatrix = new Matrix<T>(batchSize, inputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                inputMatrix[i, j] = input[i * inputSize + j];\n+            }\n+        }\n+\n+        // Compute sparse matrix multiplication\n+        Matrix<T> output = new Matrix<T>(batchSize, outputSize);\n+        foreach (var kvp in _sparseFullRankUpdates)\n+        {\n+            int row = kvp.Key.row;\n+            int col = kvp.Key.col;\n+            T weight = NumOps.Multiply(kvp.Value, _sparseScaling);\n+\n+            // output[b, row] += weight * input[b, col]\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                T contribution = NumOps.Multiply(weight, inputMatrix[b, col]);\n+                output[b, row] = NumOps.Add(output[b, row], contribution);\n+            }\n+        }\n+\n+        // Convert back to tensor\n+        Vector<T> outputData = new Vector<T>(batchSize * outputSize);\n+        int idx = 0;\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                outputData[idx++] = output[i, j];\n+            }\n+        }\n+\n+        return new Tensor<T>(new[] { batchSize, outputSize }, outputData);\n+    }\n+\n+    /// <summary>\n+    /// Performs the backward pass through the HRA adapter.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient flowing back from the next layer.</param>\n+    /// <returns>Gradient to pass to the previous layer.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The backward pass computes gradients for:\n+    /// 1. Low-rank LoRA matrices (A and B)\n+    /// 2. Sparse full-rank parameters\n+    /// 3. Updates importance scores based on gradient magnitudes\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is where HRA learns which parameters are important!\n+    /// During backpropagation:\n+    /// 1. Compute gradients for low-rank component (standard LoRA)\n+    /// 2. Compute gradients for sparse full-rank parameters\n+    /// 3. Track which parameters have large gradients (they're important!)\n+    /// 4. Periodically reassign sparse budget to most important parameters\n+    ///\n+    /// This adaptive approach ensures the sparse full-rank budget is always\n+    /// allocated to the parameters that need it most.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        // Backward through LoRA layer\n+        Tensor<T> loraInputGrad = _loraLayer.Backward(outputGradient);\n+\n+        // Backward through sparse full-rank component\n+        Tensor<T> sparseInputGrad = BackwardSparseFullRank(outputGradient);\n+\n+        // Backward through base layer\n+        Tensor<T> baseInputGrad = _baseLayer.Backward(outputGradient);\n+\n+        // Update importance scores based on gradients\n+        UpdateImportanceScores(outputGradient);\n+\n+        // Increment step and check if we should reallocate sparse parameters\n+        _stepCount++;\n+        if (_useDynamicAllocation && _stepCount % _importanceUpdateInterval == 0)\n+        {\n+            ReallocateSparseParameters();\n+        }\n+\n+        // Sum input gradients\n+        Tensor<T> inputGrad = new Tensor<T>(loraInputGrad.Shape);\n+        for (int i = 0; i < loraInputGrad.Length; i++)\n+        {\n+            T sum = NumOps.Add(loraInputGrad[i], sparseInputGrad[i]);\n+            inputGrad[i] = NumOps.Add(sum, baseInputGrad[i]);\n+        }\n+\n+        return inputGrad;\n+    }\n+\n+    /// <summary>\n+    /// Performs backward pass through the sparse full-rank component.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Output gradient tensor.</param>\n+    /// <returns>Input gradient tensor.</returns>\n+    private Tensor<T> BackwardSparseFullRank(Tensor<T> outputGradient)\n+    {\n+        int batchSize = outputGradient.Shape[0];\n+        int outputSize = outputGradient.Shape.Length > 1 ? outputGradient.Shape[1] : outputGradient.Length;\n+        int inputSize = GetInputShape()[0];\n+\n+        // Initialize sparse gradients\n+        _sparseGradients = new Dictionary<(int row, int col), T>();\n+\n+        // If no sparse parameters, return zeros\n+        if (_sparseFullRankUpdates.Count == 0)\n+        {\n+            Vector<T> zeroData = new Vector<T>(batchSize * inputSize);\n+            return new Tensor<T>(new[] { batchSize, inputSize }, zeroData);\n+        }\n+\n+        // Convert gradient to matrix\n+        Matrix<T> gradMatrix = new Matrix<T>(batchSize, outputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                gradMatrix[i, j] = outputGradient[i * outputSize + j];\n+            }\n+        }\n+\n+        // Compute input gradients and parameter gradients\n+        Matrix<T> inputGradMatrix = new Matrix<T>(batchSize, inputSize);\n+\n+        foreach (var kvp in _sparseFullRankUpdates)\n+        {\n+            int row = kvp.Key.row;\n+            int col = kvp.Key.col;\n+            T weight = NumOps.Multiply(kvp.Value, _sparseScaling);\n+\n+            T paramGrad = NumOps.Zero;\n+\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                // Input gradient: dL/dInput[b, col] += weight * dL/dOutput[b, row]\n+                T grad = NumOps.Multiply(weight, gradMatrix[b, row]);\n+                inputGradMatrix[b, col] = NumOps.Add(inputGradMatrix[b, col], grad);\n+\n+                // Parameter gradient: dL/dWeight[row, col] += input[b, col] * dL/dOutput[b, row]\n+                // Note: We need input from forward pass, stored in base layer\n+                // For simplicity, accumulate gradient magnitude for importance\n+                paramGrad = NumOps.Add(paramGrad, NumOps.Abs(gradMatrix[b, row]));\n+            }\n+\n+            _sparseGradients[kvp.Key] = NumOps.Multiply(paramGrad, _sparseScaling);\n+        }\n+\n+        // Convert input gradients back to tensor\n+        Vector<T> inputGradData = new Vector<T>(batchSize * inputSize);\n+        int idx = 0;\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                inputGradData[idx++] = inputGradMatrix[i, j];\n+            }\n+        }\n+\n+        return new Tensor<T>(new[] { batchSize, inputSize }, inputGradData);\n+    }\n+\n+    /// <summary>\n+    /// Updates importance scores based on current gradient magnitudes.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Output gradient from backward pass.</param>\n+    /// <remarks>\n+    /// <para>\n+    /// Importance is computed using exponential moving average of gradient magnitudes.\n+    /// Parameters with consistently high gradients are considered important candidates\n+    /// for sparse full-rank updates.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This identifies which parameters are VIPs.\n+    ///\n+    /// We track gradient magnitudes over time using exponential moving average:\n+    /// - new_importance = 0.95 * old_importance + 0.05 * current_gradient_magnitude\n+    ///\n+    /// Parameters with consistently high gradients get high importance scores.\n+    /// These are the ones that will receive sparse full-rank updates.\n+    /// </para>\n+    /// </remarks>\n+    private void UpdateImportanceScores(Tensor<T> outputGradient)\n+    {\n+        int outputSize = GetOutputShape()[0];\n+        int inputSize = GetInputShape()[0];\n+\n+        // Get LoRA parameter gradients to estimate per-parameter importance\n+        Vector<T> loraGradients = _loraLayer.GetParameterGradients();\n+\n+        // Update importance based on gradient flow through LoRA component\n+        // This is a proxy for which parameters would benefit from full-rank updates\n+        Matrix<T> matrixA = _loraLayer.GetMatrixA();\n+        Matrix<T> matrixB = _loraLayer.GetMatrixB();\n+\n+        T emaFactor = NumOps.FromDouble(_importanceEMA);\n+        T oneMinusEma = NumOps.FromDouble(1.0 - _importanceEMA);\n+\n+        // Estimate per-parameter importance from LoRA gradients\n+        // Higher LoRA gradients suggest that parameter needs more capacity\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                // Compute approximate gradient magnitude for this weight\n+                // by looking at contributions through LoRA paths\n+                T gradMagnitude = NumOps.Zero;\n+\n+                // Sum contributions from all rank components\n+                int rank = Rank;\n+                for (int r = 0; r < rank; r++)\n+                {\n+                    // Gradient flows through A[j,r] and B[r,i]\n+                    int aIndex = j * rank + r;\n+                    int bIndex = r * outputSize + i;\n+\n+                    if (aIndex < loraGradients.Length && bIndex < loraGradients.Length)\n+                    {\n+                        T contribution = NumOps.Multiply(\n+                            NumOps.Abs(loraGradients[aIndex]),\n+                            NumOps.Abs(loraGradients[bIndex]));\n+                        gradMagnitude = NumOps.Add(gradMagnitude, contribution);\n+                    }\n+                }\n+\n+                // Update importance with EMA\n+                T oldImportance = _parameterImportance[i, j];\n+                T newImportance = NumOps.Add(\n+                    NumOps.Multiply(emaFactor, oldImportance),\n+                    NumOps.Multiply(oneMinusEma, gradMagnitude));\n+\n+                _parameterImportance[i, j] = newImportance;\n+            }\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Reallocates sparse full-rank parameters to the most important locations.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This method identifies the top-k most important parameters and assigns\n+    /// sparse full-rank updates to them. Previously allocated parameters that\n+    /// are no longer in the top-k are removed.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is like reassigning specialists to where they're needed most.\n+    ///\n+    /// Every few hundred training steps:\n+    /// 1. Look at all importance scores\n+    /// 2. Find the top 1% most important parameters\n+    /// 3. Assign sparse full-rank budget to those parameters\n+    /// 4. Remove it from parameters that are no longer important\n+    ///\n+    /// This ensures the sparse budget is always optimally allocated.\n+    /// </para>\n+    /// </remarks>\n+    private void ReallocateSparseParameters()\n+    {\n+        int outputSize = GetOutputShape()[0];\n+        int inputSize = GetInputShape()[0];\n+\n+        // Create list of (importance, position) pairs\n+        var importanceList = new List<(T importance, int row, int col)>();\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                importanceList.Add((_parameterImportance[i, j], i, j));\n+            }\n+        }\n+\n+        // Sort by importance (descending)\n+        importanceList.Sort((a, b) =>\n+            Convert.ToDouble(b.importance).CompareTo(Convert.ToDouble(a.importance)));\n+\n+        // Select top-k positions for sparse full-rank updates\n+        var newSparseUpdates = new Dictionary<(int row, int col), T>();\n+        for (int i = 0; i < Math.Min(_maxSparseParams, importanceList.Count); i++)\n+        {\n+            var entry = importanceList[i];\n+            var key = (entry.row, entry.col);\n+\n+            // Preserve existing values if already allocated, otherwise initialize small random\n+            if (_sparseFullRankUpdates.ContainsKey(key))\n+            {\n+                newSparseUpdates[key] = _sparseFullRankUpdates[key];\n+            }\n+            else\n+            {\n+                // Initialize new sparse parameter with small random value\n+                Random rng = new Random();\n+                double randVal = (rng.NextDouble() - 0.5) * 0.02; // Small initialization\n+                newSparseUpdates[key] = NumOps.FromDouble(randVal);\n+            }\n+        }\n+\n+        _sparseFullRankUpdates = newSparseUpdates;\n+    }\n+\n+    /// <summary>\n+    /// Updates parameters using the specified learning rate.\n+    /// </summary>\n+    /// <param name=\"learningRate\">The learning rate for parameter updates.</param>\n+    public override void UpdateParameters(T learningRate)\n+    {\n+        // Update LoRA layer\n+        _loraLayer.UpdateParameters(learningRate);\n+\n+        // Update sparse full-rank parameters\n+        if (_sparseGradients != null)\n+        {\n+            var updatedSparse = new Dictionary<(int row, int col), T>();\n+            foreach (var kvp in _sparseFullRankUpdates)\n+            {\n+                T currentValue = kvp.Value;\n+                T gradient = _sparseGradients.ContainsKey(kvp.Key) ? _sparseGradients[kvp.Key] : NumOps.Zero;\n+                T update = NumOps.Multiply(gradient, learningRate);\n+                T newValue = NumOps.Subtract(currentValue, update);\n+                updatedSparse[kvp.Key] = newValue;\n+            }\n+            _sparseFullRankUpdates = updatedSparse;\n+        }\n+\n+        // Update base layer if not frozen\n+        if (!_freezeBaseLayer)\n+        {\n+            _baseLayer.UpdateParameters(learningRate);\n+        }\n+\n+        // Update parameter vector\n+        UpdateParametersFromComponents();\n+    }\n+\n+    /// <summary>\n+    /// Updates the parameter vector from the current component states.\n+    /// </summary>\n+    private void UpdateParametersFromComponents()\n+    {\n+        Parameters = new Vector<T>(ParameterCount);\n+        int idx = 0;\n+\n+        // Pack base layer parameters if not frozen\n+        if (!_freezeBaseLayer)\n+        {\n+            Vector<T> baseParams = _baseLayer.GetParameters();\n+            for (int i = 0; i < baseParams.Length; i++)\n+            {\n+                Parameters[idx++] = baseParams[i];\n+            }\n+        }\n+\n+        // Pack LoRA parameters\n+        Vector<T> loraParams = _loraLayer.GetParameters();\n+        for (int i = 0; i < loraParams.Length; i++)\n+        {\n+            Parameters[idx++] = loraParams[i];\n+        }\n+\n+        // Pack sparse parameters (just the values, positions are implicit)\n+        foreach (var kvp in _sparseFullRankUpdates)\n+        {\n+            Parameters[idx++] = kvp.Value;\n+        }\n+    }","path":"src/LoRA/Adapters/HRAAdapter.cs","commit_id":"489180133c7a9db9460c84a6174f60284717b952","original_commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**Override `SetParameters` to restore sparse weights**  \n`ParameterCount` now advertises base + LoRA + sparse values, but the inherited `SetParameters` still unpacks only base and LoRA blocks‚Äîevery sparse entry in the tail is silently ignored. Any checkpoint load or optimizer step that calls `SetParameters` will drop the sparse state. Please override `SetParameters` (and, if necessary, `GetParameters`) to pack/unpack the sparse dictionary in the same order as `UpdateParametersFromComponents()`.  \n\n```diff\n+    public override void SetParameters(Vector<T> parameters)\n+    {\n+        if (parameters.Length != ParameterCount)\n+        {\n+            throw new ArgumentException($\"Expected {ParameterCount} parameters, got {parameters.Length}\", nameof(parameters));\n+        }\n+\n+        int idx = 0;\n+        if (!_freezeBaseLayer)\n+        {\n+            int baseCount = _baseLayer.ParameterCount;\n+            Vector<T> baseParams = new Vector<T>(baseCount);\n+            for (int i = 0; i < baseCount; i++)\n+            {\n+                baseParams[i] = parameters[idx++];\n+            }\n+            _baseLayer.SetParameters(baseParams);\n+        }\n+\n+        Vector<T> loraParams = new Vector<T>(_loraLayer.ParameterCount);\n+        for (int i = 0; i < loraParams.Length; i++)\n+        {\n+            loraParams[i] = parameters[idx++];\n+        }\n+        _loraLayer.SetParameters(loraParams);\n+\n+        var sparse = new Dictionary<(int row, int col), T>();\n+        foreach (var key in _sparseFullRankUpdates.Keys.ToList())\n+        {\n+            sparse[key] = parameters[idx++];\n+        }\n+\n+        _sparseFullRankUpdates = sparse;\n+        Parameters = parameters.Clone();\n+    }\n```\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:42:01Z","updated_at":"2025-11-02T02:42:05Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484118427","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118427"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484118427"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118427/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":682,"original_start_line":682,"start_side":"RIGHT","line":712,"original_line":712,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":712,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118429","pull_request_review_id":3408015014,"id":2484118429,"node_id":"PRRC_kwDOKSXUF86UEKOd","diff_hunk":"@@ -0,0 +1,587 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// LongLoRA adapter that efficiently extends LoRA to handle longer context lengths using shifted sparse attention.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// LongLoRA (2023) addresses the challenge of adapting large language models to longer context windows\n+/// in a parameter-efficient manner. While standard LoRA works well for same-length fine-tuning,\n+/// extending context windows naively would require substantial computational resources.\n+/// </para>\n+/// <para>\n+/// LongLoRA introduces two key innovations:\n+/// 1. Shifted Sparse Attention (S¬≤-Attn): During training only, uses shifted group attention patterns\n+///    that are more efficient while maintaining effectiveness for long contexts\n+/// 2. Dense Attention at Inference: At inference time, switches back to standard dense attention\n+///    for full context utilization without the training overhead\n+/// </para>\n+/// <para><b>For Beginners:</b> LongLoRA makes it affordable to train models on longer sequences.\n+///\n+/// The Problem:\n+/// - Standard LoRA works great for adapting models, but extending context length is expensive\n+/// - Full dense attention on long sequences requires O(n¬≤) computation\n+/// - Training on 32k tokens instead of 2k tokens would be 256x slower!\n+///\n+/// LongLoRA's Solution:\n+/// - Uses a clever \"shifted sparse attention\" trick during training\n+/// - Divides the sequence into groups and shifts them to maintain information flow\n+/// - Much cheaper to train: O(n * k) where k is group size (typically 2048)\n+/// - At inference, uses full dense attention to maintain quality\n+///\n+/// Key Parameters:\n+/// - OriginalContextLength: The base model's context window (e.g., 2048)\n+/// - ExtendedContextLength: The target longer context (e.g., 8192 or 32768)\n+/// - UseShiftedAttention: Enable shifted sparse attention (training only)\n+/// - AttentionShiftSize: How many positions to shift attention groups (usually half the group size)\n+///\n+/// Example Use Case:\n+/// You have a model trained on 2k token contexts but need to process 16k token documents.\n+/// LongLoRA lets you extend the context efficiently:\n+/// - Training: Use shifted sparse attention (much faster)\n+/// - Inference: Use full dense attention (full quality)\n+///\n+/// Comparison to Standard LoRA:\n+/// - Standard LoRA: Efficient parameter adaptation, same context length\n+/// - LongLoRA: Efficient parameter adaptation + context length extension\n+/// - Adds minimal overhead (just the attention shift mechanism)\n+///\n+/// Research Background:\n+/// LongLoRA has been successfully used to extend:\n+/// - LLaMA 2 7B from 4k to 32k context (8x extension)\n+/// - LLaMA 2 13B from 4k to 64k context (16x extension)\n+/// - With only ~10% of the training cost compared to full fine-tuning\n+///\n+/// Reference: LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models (2023)\n+/// https://arxiv.org/abs/2309.12307\n+/// </para>\n+/// </remarks>\n+public class LongLoRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// The original context length that the base model was trained on.\n+    /// </summary>\n+    private readonly int _originalContextLength;\n+\n+    /// <summary>\n+    /// The extended context length that this adapter targets.\n+    /// </summary>\n+    private readonly int _extendedContextLength;\n+\n+    /// <summary>\n+    /// Whether to use shifted sparse attention during training (disabled at inference).\n+    /// </summary>\n+    private bool _useShiftedAttention;\n+\n+    /// <summary>\n+    /// The shift size for shifted sparse attention (typically half the group size).\n+    /// </summary>\n+    private readonly int _attentionShiftSize;\n+\n+    /// <summary>\n+    /// Whether the model is currently in training mode.\n+    /// </summary>\n+    private bool _isTraining;\n+\n+    /// <summary>\n+    /// Gets the original context length of the base model.\n+    /// </summary>\n+    /// <remarks>\n+    /// This is the maximum sequence length the base model was originally trained to handle.\n+    /// Typical values: 512, 1024, 2048, 4096.\n+    /// </remarks>\n+    public int OriginalContextLength => _originalContextLength;\n+\n+    /// <summary>\n+    /// Gets the extended context length this adapter targets.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This is the new, longer context window you want to support after adaptation.\n+    /// Should be larger than OriginalContextLength.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is how long of a sequence your adapted model can handle.\n+    /// For example, extending from 2k to 16k tokens means you can process 8x longer documents!\n+    /// </para>\n+    /// </remarks>\n+    public int ExtendedContextLength => _extendedContextLength;\n+\n+    /// <summary>\n+    /// Gets or sets whether to use shifted sparse attention during forward/backward passes.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// When enabled (training mode):\n+    /// - Uses shifted group attention pattern for efficiency\n+    /// - Divides sequence into groups and shifts them\n+    /// - Significantly reduces computational cost\n+    /// </para>\n+    /// <para>\n+    /// When disabled (inference mode):\n+    /// - Uses standard dense attention\n+    /// - Full context utilization\n+    /// - Better quality but slower\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Enable this during training to save compute, disable it\n+    /// during inference to get the best quality. The training trick doesn't hurt the final\n+    /// model's ability to use full attention at inference time!\n+    /// </para>\n+    /// </remarks>\n+    public bool UseShiftedAttention\n+    {\n+        get => _useShiftedAttention;\n+        set => _useShiftedAttention = value;\n+    }\n+\n+    /// <summary>\n+    /// Gets the attention shift size used in shifted sparse attention.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This determines how much groups are shifted to maintain information flow.\n+    /// Typically set to half the group size (e.g., 1024 for 2048 group size).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is the \"sliding window\" amount that ensures\n+    /// different parts of the sequence can communicate across groups. Too small and\n+    /// information doesn't flow well; too large and you lose the efficiency benefit.\n+    /// </para>\n+    /// </remarks>\n+    public int AttentionShiftSize => _attentionShiftSize;\n+\n+    /// <summary>\n+    /// Gets or sets whether the adapter is in training mode.\n+    /// </summary>\n+    /// <remarks>\n+    /// Training mode affects whether shifted attention is applied.\n+    /// Set to false during inference to use standard dense attention.\n+    /// </remarks>\n+    public bool IsTraining\n+    {\n+        get => _isTraining;\n+        set => _isTraining = value;\n+    }\n+\n+    /// <summary>\n+    /// Initializes a new LongLoRA adapter for efficient context length extension.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with LongLoRA.</param>\n+    /// <param name=\"rank\">The rank of the LoRA decomposition.</param>\n+    /// <param name=\"originalContextLength\">The original context length of the base model.</param>\n+    /// <param name=\"extendedContextLength\">The target extended context length.</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"attentionShiftSize\">The shift size for shifted sparse attention (defaults to originalContextLength/2).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when context lengths or shift size are invalid.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This creates a LongLoRA adapter to extend your model's context window.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to adapt (typically attention layers)\n+    /// - rank: How much LoRA compression to use (8-16 is typical)\n+    /// - originalContextLength: How long sequences your base model handles (e.g., 2048)\n+    /// - extendedContextLength: How long you want to extend it to (e.g., 8192 or 16384)\n+    /// - alpha: LoRA strength (usually equals rank)\n+    /// - attentionShiftSize: How much to shift attention groups (auto-calculated if not specified)\n+    /// - freezeBaseLayer: Whether to freeze original weights (usually true for efficiency)\n+    ///\n+    /// The adapter will use shifted sparse attention during training for efficiency,\n+    /// and you can switch to dense attention during inference for quality.\n+    /// </para>\n+    /// </remarks>\n+    public LongLoRAAdapter(\n+        ILayer<T> baseLayer,\n+        int rank,\n+        int originalContextLength,\n+        int extendedContextLength,\n+        double alpha = -1,\n+        int attentionShiftSize = -1,\n+        bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        if (originalContextLength <= 0)\n+        {\n+            throw new ArgumentException(\"Original context length must be positive\", nameof(originalContextLength));\n+        }\n+\n+        if (extendedContextLength <= originalContextLength)\n+        {\n+            throw new ArgumentException(\"Extended context length must be greater than original context length\", nameof(extendedContextLength));\n+        }\n+\n+        _originalContextLength = originalContextLength;\n+        _extendedContextLength = extendedContextLength;\n+        _useShiftedAttention = true;  // Default to shifted attention for training\n+        _isTraining = true;\n+\n+        // Default shift size is half the original context length (typical for shifted sparse attention)\n+        _attentionShiftSize = attentionShiftSize > 0\n+            ? attentionShiftSize\n+            : originalContextLength / 2;\n+\n+        if (_attentionShiftSize >= originalContextLength)\n+        {\n+            throw new ArgumentException(\"Attention shift size must be less than original context length\", nameof(attentionShiftSize));\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass with optional shifted sparse attention.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor of shape [batchSize, sequenceLength, featureDim].</param>\n+    /// <returns>Output tensor with LoRA adaptation applied.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The forward pass behavior depends on the UseShiftedAttention flag:\n+    /// - When true (training): Applies shifted group attention for efficiency\n+    /// - When false (inference): Uses standard dense attention\n+    /// </para>\n+    /// <para>\n+    /// Shifted Sparse Attention Process:\n+    /// 1. Divide the sequence into groups of size OriginalContextLength\n+    /// 2. Shift alternate groups by AttentionShiftSize positions\n+    /// 3. Apply attention within each group\n+    /// 4. Shift back to restore original positions\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This processes your input through the adapted layer.\n+    ///\n+    /// During training (shifted attention enabled):\n+    /// - Breaks long sequence into manageable chunks\n+    /// - Shifts them to allow cross-chunk communication\n+    /// - Much faster than processing the full sequence at once\n+    ///\n+    /// During inference (shifted attention disabled):\n+    /// - Processes the full sequence with complete attention\n+    /// - Slower but gives best quality\n+    ///\n+    /// The magic is that training with the shifted trick still produces a model\n+    /// that works great with full attention at inference!\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        // If not using shifted attention or not in training mode, use standard LoRA forward\n+        if (!_useShiftedAttention || !_isTraining)\n+        {\n+            return base.Forward(input);\n+        }\n+\n+        // Apply shifted sparse attention during training\n+        Tensor<T> shiftedInput = ApplyShiftedAttention(input);\n+\n+        // Forward through base layer with shifted input\n+        Tensor<T> baseOutput = _baseLayer.Forward(shiftedInput);\n+\n+        // Forward through LoRA layer with shifted input\n+        Tensor<T> loraOutput = _loraLayer.Forward(shiftedInput);\n+\n+        // Sum the outputs\n+        Tensor<T> result = new Tensor<T>(baseOutput.Shape);\n+        for (int i = 0; i < baseOutput.Length; i++)\n+        {\n+            result[i] = NumOps.Add(baseOutput[i], loraOutput[i]);\n+        }\n+\n+        // Reverse the shift to restore original sequence positions\n+        result = ReverseShiftedAttention(result);\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Performs the backward pass with optional shifted sparse attention.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient flowing back from the next layer.</param>\n+    /// <returns>Gradient to pass to the previous layer.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The backward pass mirrors the forward pass behavior:\n+    /// - Applies the same shifting pattern to gradients during training\n+    /// - Ensures gradient flow is consistent with the forward pass attention pattern\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This propagates learning signals backward through the network.\n+    /// It uses the same shifted pattern as the forward pass to ensure the gradients match\n+    /// the attention pattern used during the forward pass.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        // If not using shifted attention or not in training mode, use standard LoRA backward\n+        if (!_useShiftedAttention || !_isTraining)\n+        {\n+            return base.Backward(outputGradient);\n+        }\n+\n+        // Apply shift to output gradient to match forward pass shifting\n+        Tensor<T> shiftedGradient = ApplyShiftedAttention(outputGradient);\n+\n+        // Backward through LoRA layer\n+        Tensor<T> loraInputGrad = _loraLayer.Backward(shiftedGradient);\n+\n+        // Backward through base layer\n+        Tensor<T> baseInputGrad = _baseLayer.Backward(shiftedGradient);\n+\n+        // Sum input gradients\n+        Tensor<T> inputGrad = new Tensor<T>(loraInputGrad.Shape);\n+        for (int i = 0; i < loraInputGrad.Length; i++)\n+        {\n+            inputGrad[i] = NumOps.Add(loraInputGrad[i], baseInputGrad[i]);\n+        }\n+\n+        // Reverse the shift to restore original sequence positions\n+        inputGrad = ReverseShiftedAttention(inputGrad);\n+\n+        // Update parameter gradients vector\n+        UpdateParameterGradientsFromLayers();\n+\n+        return inputGrad;\n+    }\n+\n+    /// <summary>\n+    /// Applies shifted sparse attention pattern to the input tensor.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor to shift.</param>\n+    /// <returns>Tensor with shifted attention pattern applied.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The shifting pattern works as follows:\n+    /// 1. Divide sequence into groups of size OriginalContextLength\n+    /// 2. For alternate groups, shift by AttentionShiftSize positions\n+    /// 3. This creates overlapping attention windows that allow information flow\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Imagine sliding windows along a long document.\n+    /// Instead of having fixed non-overlapping windows, we shift every other window\n+    /// by half its size. This ensures that each part of the document can \"see\"\n+    /// parts from neighboring windows, maintaining information flow while keeping\n+    /// computation efficient.\n+    /// </para>\n+    /// </remarks>\n+    private Tensor<T> ApplyShiftedAttention(Tensor<T> input)\n+    {\n+        // For simplicity, this is a conceptual implementation\n+        // In practice, this would integrate with the attention mechanism\n+        // Here we just apply a circular shift to alternate groups\n+\n+        int sequenceLength = input.Shape.Length > 1 ? input.Shape[1] : input.Length;\n+\n+        // If sequence is shorter than group size, no shifting needed\n+        if (sequenceLength <= _originalContextLength)\n+        {\n+            return input.Clone();\n+        }\n+\n+        Tensor<T> shifted = input.Clone();\n+        int groupSize = _originalContextLength;\n+        int numGroups = (sequenceLength + groupSize - 1) / groupSize;\n+\n+        // Apply shift to alternate groups\n+        for (int g = 1; g < numGroups; g += 2)\n+        {\n+            int groupStart = g * groupSize;\n+            int groupEnd = Math.Min(groupStart + groupSize, sequenceLength);\n+\n+            // Circular shift within this group\n+            ShiftGroup(shifted, groupStart, groupEnd, _attentionShiftSize);\n+        }\n+\n+        return shifted;\n+    }\n+\n+    /// <summary>\n+    /// Reverses the shifted sparse attention pattern to restore original positions.\n+    /// </summary>\n+    /// <param name=\"input\">Tensor with shifted attention pattern.</param>\n+    /// <returns>Tensor with original sequence positions restored.</returns>\n+    /// <remarks>\n+    /// This reverses the shifting applied by ApplyShiftedAttention to restore\n+    /// the output to the original sequence order.\n+    /// </remarks>\n+    private Tensor<T> ReverseShiftedAttention(Tensor<T> input)\n+    {\n+        int sequenceLength = input.Shape.Length > 1 ? input.Shape[1] : input.Length;\n+\n+        // If sequence is shorter than group size, no shifting was applied\n+        if (sequenceLength <= _originalContextLength)\n+        {\n+            return input.Clone();\n+        }\n+\n+        Tensor<T> unshifted = input.Clone();\n+        int groupSize = _originalContextLength;\n+        int numGroups = (sequenceLength + groupSize - 1) / groupSize;\n+\n+        // Reverse shift for alternate groups (shift in opposite direction)\n+        for (int g = 1; g < numGroups; g += 2)\n+        {\n+            int groupStart = g * groupSize;\n+            int groupEnd = Math.Min(groupStart + groupSize, sequenceLength);\n+\n+            // Reverse circular shift within this group\n+            ShiftGroup(unshifted, groupStart, groupEnd, -_attentionShiftSize);","path":"src/LoRA/Adapters/LongLoRAAdapter.cs","commit_id":"489180133c7a9db9460c84a6174f60284717b952","original_commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Shifted attention indexing breaks multi-dimensional inputs**\n\n`ApplyShiftedAttention`/`ReverseShiftedAttention`/`ShiftGroup` treat the tensor as a flat 1-D buffer and only rotate the first `sequenceLength` scalars. As soon as the input has the usual `[batch, sequence, hidden]` layout, this means we shift a single hidden feature of the first batch only, while every other feature/batch entry stays untouched. Gradients coming back through `Backward` are distorted in the same way, so the adapter never learns the intended S¬≤-attn pattern and silently corrupts training. Please rewrite the shift helpers to iterate per batch (and per feature/head) using the full tensor strides so that every token across all batches/features is shifted consistently; otherwise LongLoRA cannot function.  \n\n\n\nAlso applies to: 446-480\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/LoRA/Adapters/LongLoRAAdapter.cs around lines 362-423 (and similarly\n446-480), the shift logic currently treats the tensor as a flat 1-D buffer and\nonly rotates the first sequence-length scalars; update the implementation so\nshifting is applied along the sequence axis for every batch and every\nfeature/head channel using the tensor's full shape and strides. Specifically:\ndetect the tensor layout (expecting [batch, sequence, hidden] or at least dims\n>=2), compute stride offsets for each (batch, hidden) pair, and for each group\nalong the sequence axis perform the circular rotation on the contiguous elements\ncorresponding to that (batch,hidden) slot (or use proper slicing/vectorized ops\nif supported by your Tensor<T> API) rather than rotating the flattened array;\napply the same per-(batch,feature) looping in ApplyShiftedAttention,\nReverseShiftedAttention and ShiftGroup (including the opposite-direction shift\nfor reverse) so every token across all batches/features is shifted consistently\nand gradients stay correct.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:42:01Z","updated_at":"2025-11-02T02:42:05Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484118429","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118429"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484118429"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118429/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":362,"original_start_line":362,"start_side":"RIGHT","line":423,"original_line":423,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":423,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118434","pull_request_review_id":3408015014,"id":2484118434,"node_id":"PRRC_kwDOKSXUF86UEKOi","diff_hunk":"@@ -0,0 +1,444 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// Implements MoRA (High-Rank Updating for Parameter-Efficient Fine-Tuning) adapter.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// <b>Paper Reference:</b> \"MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning\"\n+/// by Ting Jiang, Shaohan Huang, et al. (arXiv:2405.12130, May 2024)\n+/// </para>\n+/// <para>\n+/// MoRA addresses a fundamental limitation of LoRA: the low-rank constraint restricts the model's\n+/// ability to learn and memorize new knowledge. While LoRA uses two rectangular matrices (A and B)\n+/// to create low-rank updates, MoRA uses a single square matrix M combined with non-parameter-sharing\n+/// operators to achieve high-rank updates while maintaining the same parameter count.\n+/// </para>\n+/// <para><b>Key Innovations:</b>\n+///\n+/// 1. <b>High-Rank Updates</b>: Unlike LoRA's rank-r updates (r &lt;&lt; d), MoRA achieves rank-rÃÇ\n+///    updates where rÃÇ can equal the full dimension d, enabling the model to learn richer representations.\n+///\n+/// 2. <b>Square Matrix M</b>: Instead of LoRA's A (d√ór) and B (r√ód) matrices, MoRA uses a single\n+///    square matrix M (r√ór) where r = sqrt(d√ód / 2). For the same parameter count as LoRA,\n+///    MoRA achieves much higher effective rank.\n+///\n+/// 3. <b>Non-Parameter-Sharing Operators</b>: MoRA uses rotation, permutation, or other linear\n+///    transformations that don't add trainable parameters but enable dimension compression\n+///    and decompression around the square matrix M.\n+///\n+/// 4. <b>Input Compression / Output Decompression</b>: The architecture is:\n+///    - Compress: Input (d) to Compressed (r) via rotation/permutation\n+///    - Transform: Compressed (r) to Transformed (r) via trainable matrix M\n+///    - Decompress: Transformed (r) to Output (d) via inverse rotation/permutation\n+/// </para>\n+/// <para><b>Architecture Comparison:</b>\n+///\n+/// LoRA: W = W‚ÇÄ + BA where A ‚àà ‚Ñù^(d√ór), B ‚àà ‚Ñù^(r√ód)\n+/// - Parameters: 2dr\n+/// - Rank: r (low-rank constraint)\n+/// - Typical r: 8-64\n+///\n+/// MoRA: W = W‚ÇÄ + R_d^(-1) M R_c where M ‚àà ‚Ñù^(r√ór)\n+/// - Parameters: r¬≤\n+/// - Rank: min(r, d) (can be full-rank)\n+/// - For same param count as LoRA: r = sqrt(2dr), so rank ‚âà sqrt(2dr)\n+/// - Example: LoRA with r=8, d=1024 has 16,384 params and rank 8\n+///            MoRA with same params: r=128, rank 128 (16√ó higher!)\n+/// </para>\n+/// <para><b>Performance (from paper):</b>\n+///\n+/// Compared to LoRA on various tasks:\n+/// - <b>Memory-Intensive Tasks</b>: MoRA significantly outperforms LoRA\n+///   * Continual Pretraining: ~15% better perplexity\n+///   * Instruction Tuning: ~8% better accuracy on knowledge-intensive QA\n+/// - <b>Reasoning Tasks</b>: MoRA performs comparably to LoRA\n+///   * Mathematical Reasoning: Similar performance (within 1-2%)\n+/// - <b>Parameter Efficiency</b>: Same parameter count as LoRA\n+/// - <b>Training Speed</b>: Slightly slower than LoRA due to rotation operations (‚âà5-10% overhead)\n+/// </para>\n+/// <para><b>When to Use MoRA vs LoRA:</b>\n+///\n+/// Use MoRA when:\n+/// - Task requires memorizing new facts or knowledge\n+/// - Domain adaptation with significant vocabulary changes\n+/// - Continual learning scenarios\n+/// - You need the model to \"remember\" rather than just \"adapt\"\n+///\n+/// Use LoRA when:\n+/// - Task is primarily reasoning or pattern recognition\n+/// - Minimal new knowledge acquisition needed\n+/// - Training speed is critical\n+/// - Standard parameter-efficient fine-tuning is sufficient\n+/// </para>\n+/// <para><b>Implementation Details:</b>\n+///\n+/// This implementation uses rotation matrices as the non-parameter-sharing operators:\n+/// - Compression R_c: Projects input from dimension d to dimension r\n+/// - Decompression R_d: Projects from dimension r back to dimension d\n+/// - These are generated using random orthogonal matrices (Gram-Schmidt orthogonalization)\n+/// - They remain fixed during training (non-trainable)\n+///\n+/// Alternative operators mentioned in the paper (not implemented here):\n+/// - RoPE-based rotations (Rotary Position Embeddings)\n+/// - Random permutations\n+/// - Structured rotations (e.g., Hadamard transforms)\n+/// </para>\n+/// <para><b>For Beginners:</b> MoRA is like an upgraded version of LoRA that can learn\n+/// more complex changes to a model while using the same amount of memory.\n+///\n+/// Think of it like this:\n+/// - LoRA is like having 2 small notebooks to write changes (matrices A and B)\n+/// - MoRA is like having 1 square notebook plus a compression/decompression scheme\n+///\n+/// The key insight: By compressing the input, applying changes in compressed space,\n+/// and then decompressing, MoRA can make higher-rank updates that capture more\n+/// complex patterns. This is especially useful when you're teaching the model\n+/// entirely new facts or concepts, not just adapting its existing knowledge.\n+///\n+/// Example: If you're fine-tuning a model to learn medical terminology, MoRA\n+/// will be better at memorizing the new terms, while LoRA might be better at\n+/// learning to reason about medical cases using existing knowledge.\n+/// </para>\n+/// </remarks>\n+public class MoRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Square matrix M for high-rank adaptation (r√ór dimensions).\n+    /// </summary>\n+    /// <remarks>\n+    /// This is the core trainable component of MoRA. Unlike LoRA's rectangular matrices,\n+    /// M is square with dimensions (r√ór), enabling higher-rank updates.\n+    /// </remarks>\n+    private Matrix<T> _matrixM;\n+\n+    /// <summary>\n+    /// Compression matrix that reduces input dimension from d to r (non-trainable).\n+    /// </summary>\n+    /// <remarks>\n+    /// This is a non-trainable orthogonal matrix that compresses the input.\n+    /// It's generated once during initialization using Gram-Schmidt orthogonalization and remains fixed.\n+    /// </remarks>\n+    private readonly Matrix<T> _compressionMatrix;\n+\n+    /// <summary>\n+    /// Decompression matrix that expands dimension from r back to d (non-trainable).\n+    /// </summary>\n+    /// <remarks>\n+    /// This is a non-trainable orthogonal matrix that decompresses the output.\n+    /// In this implementation, it's the transpose of the compression matrix.\n+    /// </remarks>\n+    private readonly Matrix<T> _decompressionMatrix;\n+\n+    /// <summary>\n+    /// The dimension of the square matrix M.\n+    /// </summary>\n+    /// <remarks>\n+    /// For MoRA, this is calculated to match the parameter count of LoRA.\n+    /// If LoRA uses 2dr parameters, MoRA uses r¬≤ = 2dr, so rÃÇ = sqrt(2dr).\n+    /// This gives MoRA a much higher effective rank than LoRA.\n+    /// </remarks>\n+    private readonly int _squareRank;\n+\n+    /// <summary>\n+    /// Gradients for matrix M computed during backpropagation.\n+    /// </summary>\n+    private Matrix<T>? _matrixMGradient;\n+\n+    /// <summary>\n+    /// Stored input from the forward pass, needed for gradient computation.\n+    /// </summary>\n+    private Tensor<T>? _lastInput;\n+\n+    /// <summary>\n+    /// Cached compressed input from forward pass.\n+    /// </summary>\n+    private Matrix<T>? _lastCompressed;\n+\n+    /// <summary>\n+    /// Gets the effective rank of the MoRA adaptation.\n+    /// </summary>\n+    /// <remarks>\n+    /// This is the dimension of the square matrix M, which determines the\n+    /// maximum rank of the updates MoRA can make. Unlike LoRA where this\n+    /// is typically 8-64, MoRA can achieve ranks of 128+ with the same\n+    /// parameter count.\n+    /// </remarks>\n+    public int SquareRank => _squareRank;\n+\n+    public MoRAAdapter(ILayer<T> baseLayer, int rank, double alpha = 1.0, bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+\n+        if (inputSize != outputSize)\n+        {\n+            throw new ArgumentException(\n+                $\"MoRA requires square layers (input size = output size). Got input={inputSize}, output={outputSize}. \" +\n+                \"For non-square layers, use LoRA instead.\", nameof(baseLayer));\n+        }\n+\n+        int dimension = inputSize;\n+        _squareRank = (int)Math.Sqrt(2.0 * dimension * rank);\n+\n+        if (_squareRank < 1)\n+        {\n+            _squareRank = 1;\n+        }\n+\n+        if (_squareRank > dimension)\n+        {\n+            _squareRank = dimension;\n+        }\n+\n+        _matrixM = new Matrix<T>(_squareRank, _squareRank);\n+        InitializeMatrixM();\n+\n+        _compressionMatrix = GenerateOrthogonalMatrix(dimension, _squareRank);\n+        _decompressionMatrix = _compressionMatrix.Transpose();\n+    }\n+\n+    private void InitializeMatrixM()\n+    {\n+        T stddev = NumOps.Sqrt(NumOps.Divide(NumOps.One, NumOps.FromDouble(_squareRank)));\n+\n+        for (int i = 0; i < _matrixM.Rows; i++)\n+        {\n+            for (int j = 0; j < _matrixM.Columns; j++)\n+            {\n+                double u1 = Random.NextDouble();\n+                double u2 = Random.NextDouble();\n+                double randStdNormal = Math.Sqrt(-2.0 * Math.Log(u1)) * Math.Sin(2.0 * Math.PI * u2);\n+                _matrixM[i, j] = NumOps.Multiply(NumOps.FromDouble(randStdNormal), stddev);\n+            }\n+        }\n+    }\n+\n+    private Matrix<T> GenerateOrthogonalMatrix(int rows, int cols)\n+    {\n+        Matrix<T> randomMatrix = new Matrix<T>(rows, cols);\n+        for (int i = 0; i < rows; i++)\n+        {\n+            for (int j = 0; j < cols; j++)\n+            {\n+                double u1 = Random.NextDouble();\n+                double u2 = Random.NextDouble();\n+                double randStdNormal = Math.Sqrt(-2.0 * Math.Log(u1)) * Math.Sin(2.0 * Math.PI * u2);\n+                randomMatrix[i, j] = NumOps.FromDouble(randStdNormal);\n+            }\n+        }\n+\n+        Matrix<T> orthogonal = new Matrix<T>(rows, cols);\n+\n+        for (int j = 0; j < cols; j++)\n+        {\n+            Vector<T> column = new Vector<T>(rows);\n+            for (int i = 0; i < rows; i++)\n+            {\n+                column[i] = randomMatrix[i, j];\n+            }\n+\n+            for (int k = 0; k < j; k++)\n+            {\n+                Vector<T> prevColumn = new Vector<T>(rows);\n+                for (int i = 0; i < rows; i++)\n+                {\n+                    prevColumn[i] = orthogonal[i, k];\n+                }\n+\n+                T dotProduct = NumOps.Zero;\n+                for (int i = 0; i < rows; i++)\n+                {\n+                    dotProduct = NumOps.Add(dotProduct, NumOps.Multiply(column[i], prevColumn[i]));\n+                }\n+\n+                for (int i = 0; i < rows; i++)\n+                {\n+                    column[i] = NumOps.Subtract(column[i], NumOps.Multiply(dotProduct, prevColumn[i]));\n+                }\n+            }\n+\n+            T norm = NumOps.Zero;\n+            for (int i = 0; i < rows; i++)\n+            {\n+                norm = NumOps.Add(norm, NumOps.Multiply(column[i], column[i]));\n+            }\n+            norm = NumOps.Sqrt(norm);\n+\n+            if (NumOps.GreaterThan(norm, NumOps.FromDouble(1e-10)))\n+            {\n+                for (int i = 0; i < rows; i++)\n+                {\n+                    orthogonal[i, j] = NumOps.Divide(column[i], norm);\n+                }\n+            }\n+            else\n+            {\n+                for (int i = 0; i < rows; i++)\n+                {\n+                    orthogonal[i, j] = i == j ? NumOps.One : NumOps.Zero;\n+                }\n+            }\n+        }\n+\n+        return orthogonal;\n+    }\n+\n+    protected override LoRALayer<T> CreateLoRALayer(int rank, double alpha)\n+    {\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        return new LoRALayer<T>(inputSize, outputSize, 1, alpha);\n+    }\n+\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        _lastInput = input.Clone();\n+        Tensor<T> baseOutput = _baseLayer.Forward(input);\n+\n+        int batchSize = input.Shape[0];\n+        int dimension = input.Shape.Length > 1 ? input.Shape[1] : input.Length;\n+\n+        Matrix<T> inputMatrix = new Matrix<T>(batchSize, dimension);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < dimension; j++)\n+            {\n+                inputMatrix[i, j] = input[i * dimension + j];\n+            }\n+        }\n+\n+        Matrix<T> compressed = inputMatrix.Multiply(_compressionMatrix);\n+        _lastCompressed = compressed;\n+\n+        Matrix<T> transformed = compressed.Multiply(_matrixM);\n+        Matrix<T> decompressed = transformed.Multiply(_decompressionMatrix);\n+\n+        T scalingFactor = NumOps.FromDouble(Alpha);\n+        decompressed = decompressed.Multiply(scalingFactor);\n+\n+        Tensor<T> moraOutput = new Tensor<T>(baseOutput.Shape);\n+        int idx = 0;\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < dimension; j++)\n+            {\n+                moraOutput[idx] = decompressed[i, j];\n+                idx++;\n+            }\n+        }\n+\n+        Tensor<T> result = new Tensor<T>(baseOutput.Shape);\n+        for (int i = 0; i < baseOutput.Length; i++)\n+        {\n+            result[i] = NumOps.Add(baseOutput[i], moraOutput[i]);\n+        }\n+\n+        return result;\n+    }\n+\n+    public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (_lastInput == null || _lastCompressed == null)\n+        {\n+            throw new InvalidOperationException(\"Forward pass must be called before backward pass\");\n+        }\n+\n+        int batchSize = _lastInput.Shape[0];\n+        int dimension = _lastInput.Shape.Length > 1 ? _lastInput.Shape[1] : _lastInput.Length;\n+\n+        Matrix<T> gradMatrix = new Matrix<T>(batchSize, dimension);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < dimension; j++)\n+            {\n+                gradMatrix[i, j] = outputGradient[i * dimension + j];\n+            }\n+        }\n+\n+        T scalingFactor = NumOps.FromDouble(Alpha);\n+        Matrix<T> gradTransformed = gradMatrix.Multiply(_decompressionMatrix.Transpose()).Multiply(scalingFactor);\n+        _matrixMGradient = _lastCompressed.Transpose().Multiply(gradTransformed);\n+        Matrix<T> gradCompressed = gradTransformed.Multiply(_matrixM.Transpose());\n+        Matrix<T> moraInputGradient = gradCompressed.Multiply(_compressionMatrix.Transpose());\n+\n+        Tensor<T> baseInputGrad = _baseLayer.Backward(outputGradient);\n+\n+        Tensor<T> inputGrad = new Tensor<T>(_lastInput.Shape);\n+        int idx = 0;\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < dimension; j++)\n+            {\n+                T moraGrad = moraInputGradient[i, j];\n+                inputGrad[idx] = NumOps.Add(baseInputGrad[idx], moraGrad);\n+                idx++;\n+            }\n+        }\n+\n+        return inputGrad;\n+    }\n+\n+    public override void UpdateParameters(T learningRate)\n+    {\n+        if (_matrixMGradient == null)\n+        {\n+            return;\n+        }\n+\n+        for (int i = 0; i < _matrixM.Rows; i++)\n+        {\n+            for (int j = 0; j < _matrixM.Columns; j++)\n+            {\n+                T update = NumOps.Multiply(_matrixMGradient[i, j], learningRate);\n+                _matrixM[i, j] = NumOps.Subtract(_matrixM[i, j], update);\n+            }\n+        }\n+\n+        if (!_freezeBaseLayer)\n+        {\n+            _baseLayer.UpdateParameters(learningRate);\n+        }\n+    }\n+\n+    public override int ParameterCount\n+    {\n+        get\n+        {\n+            int moraParams = _squareRank * _squareRank;\n+            return _freezeBaseLayer ? moraParams : (_baseLayer.ParameterCount + moraParams);\n+        }\n+    }","path":"src/LoRA/Adapters/MoRAAdapter.cs","commit_id":"489180133c7a9db9460c84a6174f60284717b952","original_commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**ParameterCount override causes constructor-time crash**\n\nDuring the base-class constructor, `ParameterCount` is invoked before `_squareRank` is initialized. Because `_squareRank` is still 0 at that moment, you return 0, `Parameters = new Vector<T>(0)`, and `UpdateParametersFromLayers()` immediately writes past the end of that vector, throwing an `IndexOutOfRangeException`. You need to ensure the derived count reflects the LoRA parameters during base construction (e.g., defer the override until after `_squareRank` is set, or store the LoRA parameter count separately and include it alongside the MoRA params).\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/LoRA/Adapters/MoRAAdapter.cs around lines 408‚Äì415, the ParameterCount\noverride reads _squareRank which is still zero during base-class construction,\ncausing a zero-sized Parameters vector and subsequent out-of-range writes;\nchange the implementation so it does not rely on _squareRank being initialized:\ncompute and store the LoRA/MoRA parameter count as a separate field set before\nbase ctor runs (or return a conservative non-zero value during construction and\nupdate the real count after _squareRank is assigned), and make ParameterCount\nreturn baseLayer.ParameterCount + that stored LoRA count (or just the stored\nsum) so the correct size is available during base construction and later updates\ndo not write past the end of the vector.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:42:02Z","updated_at":"2025-11-02T02:42:05Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484118434","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118434"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484118434"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118434/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":408,"original_start_line":408,"start_side":"RIGHT","line":415,"original_line":415,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":415,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118442","pull_request_review_id":3408015014,"id":2484118442,"node_id":"PRRC_kwDOKSXUF86UEKOq","diff_hunk":"@@ -0,0 +1,444 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// Implements MoRA (High-Rank Updating for Parameter-Efficient Fine-Tuning) adapter.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// <b>Paper Reference:</b> \"MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning\"\n+/// by Ting Jiang, Shaohan Huang, et al. (arXiv:2405.12130, May 2024)\n+/// </para>\n+/// <para>\n+/// MoRA addresses a fundamental limitation of LoRA: the low-rank constraint restricts the model's\n+/// ability to learn and memorize new knowledge. While LoRA uses two rectangular matrices (A and B)\n+/// to create low-rank updates, MoRA uses a single square matrix M combined with non-parameter-sharing\n+/// operators to achieve high-rank updates while maintaining the same parameter count.\n+/// </para>\n+/// <para><b>Key Innovations:</b>\n+///\n+/// 1. <b>High-Rank Updates</b>: Unlike LoRA's rank-r updates (r &lt;&lt; d), MoRA achieves rank-rÃÇ\n+///    updates where rÃÇ can equal the full dimension d, enabling the model to learn richer representations.\n+///\n+/// 2. <b>Square Matrix M</b>: Instead of LoRA's A (d√ór) and B (r√ód) matrices, MoRA uses a single\n+///    square matrix M (r√ór) where r = sqrt(d√ód / 2). For the same parameter count as LoRA,\n+///    MoRA achieves much higher effective rank.\n+///\n+/// 3. <b>Non-Parameter-Sharing Operators</b>: MoRA uses rotation, permutation, or other linear\n+///    transformations that don't add trainable parameters but enable dimension compression\n+///    and decompression around the square matrix M.\n+///\n+/// 4. <b>Input Compression / Output Decompression</b>: The architecture is:\n+///    - Compress: Input (d) to Compressed (r) via rotation/permutation\n+///    - Transform: Compressed (r) to Transformed (r) via trainable matrix M\n+///    - Decompress: Transformed (r) to Output (d) via inverse rotation/permutation\n+/// </para>\n+/// <para><b>Architecture Comparison:</b>\n+///\n+/// LoRA: W = W‚ÇÄ + BA where A ‚àà ‚Ñù^(d√ór), B ‚àà ‚Ñù^(r√ód)\n+/// - Parameters: 2dr\n+/// - Rank: r (low-rank constraint)\n+/// - Typical r: 8-64\n+///\n+/// MoRA: W = W‚ÇÄ + R_d^(-1) M R_c where M ‚àà ‚Ñù^(r√ór)\n+/// - Parameters: r¬≤\n+/// - Rank: min(r, d) (can be full-rank)\n+/// - For same param count as LoRA: r = sqrt(2dr), so rank ‚âà sqrt(2dr)\n+/// - Example: LoRA with r=8, d=1024 has 16,384 params and rank 8\n+///            MoRA with same params: r=128, rank 128 (16√ó higher!)\n+/// </para>\n+/// <para><b>Performance (from paper):</b>\n+///\n+/// Compared to LoRA on various tasks:\n+/// - <b>Memory-Intensive Tasks</b>: MoRA significantly outperforms LoRA\n+///   * Continual Pretraining: ~15% better perplexity\n+///   * Instruction Tuning: ~8% better accuracy on knowledge-intensive QA\n+/// - <b>Reasoning Tasks</b>: MoRA performs comparably to LoRA\n+///   * Mathematical Reasoning: Similar performance (within 1-2%)\n+/// - <b>Parameter Efficiency</b>: Same parameter count as LoRA\n+/// - <b>Training Speed</b>: Slightly slower than LoRA due to rotation operations (‚âà5-10% overhead)\n+/// </para>\n+/// <para><b>When to Use MoRA vs LoRA:</b>\n+///\n+/// Use MoRA when:\n+/// - Task requires memorizing new facts or knowledge\n+/// - Domain adaptation with significant vocabulary changes\n+/// - Continual learning scenarios\n+/// - You need the model to \"remember\" rather than just \"adapt\"\n+///\n+/// Use LoRA when:\n+/// - Task is primarily reasoning or pattern recognition\n+/// - Minimal new knowledge acquisition needed\n+/// - Training speed is critical\n+/// - Standard parameter-efficient fine-tuning is sufficient\n+/// </para>\n+/// <para><b>Implementation Details:</b>\n+///\n+/// This implementation uses rotation matrices as the non-parameter-sharing operators:\n+/// - Compression R_c: Projects input from dimension d to dimension r\n+/// - Decompression R_d: Projects from dimension r back to dimension d\n+/// - These are generated using random orthogonal matrices (Gram-Schmidt orthogonalization)\n+/// - They remain fixed during training (non-trainable)\n+///\n+/// Alternative operators mentioned in the paper (not implemented here):\n+/// - RoPE-based rotations (Rotary Position Embeddings)\n+/// - Random permutations\n+/// - Structured rotations (e.g., Hadamard transforms)\n+/// </para>\n+/// <para><b>For Beginners:</b> MoRA is like an upgraded version of LoRA that can learn\n+/// more complex changes to a model while using the same amount of memory.\n+///\n+/// Think of it like this:\n+/// - LoRA is like having 2 small notebooks to write changes (matrices A and B)\n+/// - MoRA is like having 1 square notebook plus a compression/decompression scheme\n+///\n+/// The key insight: By compressing the input, applying changes in compressed space,\n+/// and then decompressing, MoRA can make higher-rank updates that capture more\n+/// complex patterns. This is especially useful when you're teaching the model\n+/// entirely new facts or concepts, not just adapting its existing knowledge.\n+///\n+/// Example: If you're fine-tuning a model to learn medical terminology, MoRA\n+/// will be better at memorizing the new terms, while LoRA might be better at\n+/// learning to reason about medical cases using existing knowledge.\n+/// </para>\n+/// </remarks>\n+public class MoRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Square matrix M for high-rank adaptation (r√ór dimensions).\n+    /// </summary>\n+    /// <remarks>\n+    /// This is the core trainable component of MoRA. Unlike LoRA's rectangular matrices,\n+    /// M is square with dimensions (r√ór), enabling higher-rank updates.\n+    /// </remarks>\n+    private Matrix<T> _matrixM;\n+\n+    /// <summary>\n+    /// Compression matrix that reduces input dimension from d to r (non-trainable).\n+    /// </summary>\n+    /// <remarks>\n+    /// This is a non-trainable orthogonal matrix that compresses the input.\n+    /// It's generated once during initialization using Gram-Schmidt orthogonalization and remains fixed.\n+    /// </remarks>\n+    private readonly Matrix<T> _compressionMatrix;\n+\n+    /// <summary>\n+    /// Decompression matrix that expands dimension from r back to d (non-trainable).\n+    /// </summary>\n+    /// <remarks>\n+    /// This is a non-trainable orthogonal matrix that decompresses the output.\n+    /// In this implementation, it's the transpose of the compression matrix.\n+    /// </remarks>\n+    private readonly Matrix<T> _decompressionMatrix;\n+\n+    /// <summary>\n+    /// The dimension of the square matrix M.\n+    /// </summary>\n+    /// <remarks>\n+    /// For MoRA, this is calculated to match the parameter count of LoRA.\n+    /// If LoRA uses 2dr parameters, MoRA uses r¬≤ = 2dr, so rÃÇ = sqrt(2dr).\n+    /// This gives MoRA a much higher effective rank than LoRA.\n+    /// </remarks>\n+    private readonly int _squareRank;\n+\n+    /// <summary>\n+    /// Gradients for matrix M computed during backpropagation.\n+    /// </summary>\n+    private Matrix<T>? _matrixMGradient;\n+\n+    /// <summary>\n+    /// Stored input from the forward pass, needed for gradient computation.\n+    /// </summary>\n+    private Tensor<T>? _lastInput;\n+\n+    /// <summary>\n+    /// Cached compressed input from forward pass.\n+    /// </summary>\n+    private Matrix<T>? _lastCompressed;\n+\n+    /// <summary>\n+    /// Gets the effective rank of the MoRA adaptation.\n+    /// </summary>\n+    /// <remarks>\n+    /// This is the dimension of the square matrix M, which determines the\n+    /// maximum rank of the updates MoRA can make. Unlike LoRA where this\n+    /// is typically 8-64, MoRA can achieve ranks of 128+ with the same\n+    /// parameter count.\n+    /// </remarks>\n+    public int SquareRank => _squareRank;\n+\n+    public MoRAAdapter(ILayer<T> baseLayer, int rank, double alpha = 1.0, bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+\n+        if (inputSize != outputSize)\n+        {\n+            throw new ArgumentException(\n+                $\"MoRA requires square layers (input size = output size). Got input={inputSize}, output={outputSize}. \" +\n+                \"For non-square layers, use LoRA instead.\", nameof(baseLayer));\n+        }\n+\n+        int dimension = inputSize;\n+        _squareRank = (int)Math.Sqrt(2.0 * dimension * rank);\n+\n+        if (_squareRank < 1)\n+        {\n+            _squareRank = 1;\n+        }\n+\n+        if (_squareRank > dimension)\n+        {\n+            _squareRank = dimension;\n+        }\n+\n+        _matrixM = new Matrix<T>(_squareRank, _squareRank);\n+        InitializeMatrixM();\n+\n+        _compressionMatrix = GenerateOrthogonalMatrix(dimension, _squareRank);\n+        _decompressionMatrix = _compressionMatrix.Transpose();\n+    }\n+\n+    private void InitializeMatrixM()\n+    {\n+        T stddev = NumOps.Sqrt(NumOps.Divide(NumOps.One, NumOps.FromDouble(_squareRank)));\n+\n+        for (int i = 0; i < _matrixM.Rows; i++)\n+        {\n+            for (int j = 0; j < _matrixM.Columns; j++)\n+            {\n+                double u1 = Random.NextDouble();\n+                double u2 = Random.NextDouble();\n+                double randStdNormal = Math.Sqrt(-2.0 * Math.Log(u1)) * Math.Sin(2.0 * Math.PI * u2);\n+                _matrixM[i, j] = NumOps.Multiply(NumOps.FromDouble(randStdNormal), stddev);\n+            }\n+        }\n+    }\n+\n+    private Matrix<T> GenerateOrthogonalMatrix(int rows, int cols)\n+    {\n+        Matrix<T> randomMatrix = new Matrix<T>(rows, cols);\n+        for (int i = 0; i < rows; i++)\n+        {\n+            for (int j = 0; j < cols; j++)\n+            {\n+                double u1 = Random.NextDouble();\n+                double u2 = Random.NextDouble();\n+                double randStdNormal = Math.Sqrt(-2.0 * Math.Log(u1)) * Math.Sin(2.0 * Math.PI * u2);\n+                randomMatrix[i, j] = NumOps.FromDouble(randStdNormal);\n+            }\n+        }\n+\n+        Matrix<T> orthogonal = new Matrix<T>(rows, cols);\n+\n+        for (int j = 0; j < cols; j++)\n+        {\n+            Vector<T> column = new Vector<T>(rows);\n+            for (int i = 0; i < rows; i++)\n+            {\n+                column[i] = randomMatrix[i, j];\n+            }\n+\n+            for (int k = 0; k < j; k++)\n+            {\n+                Vector<T> prevColumn = new Vector<T>(rows);\n+                for (int i = 0; i < rows; i++)\n+                {\n+                    prevColumn[i] = orthogonal[i, k];\n+                }\n+\n+                T dotProduct = NumOps.Zero;\n+                for (int i = 0; i < rows; i++)\n+                {\n+                    dotProduct = NumOps.Add(dotProduct, NumOps.Multiply(column[i], prevColumn[i]));\n+                }\n+\n+                for (int i = 0; i < rows; i++)\n+                {\n+                    column[i] = NumOps.Subtract(column[i], NumOps.Multiply(dotProduct, prevColumn[i]));\n+                }\n+            }\n+\n+            T norm = NumOps.Zero;\n+            for (int i = 0; i < rows; i++)\n+            {\n+                norm = NumOps.Add(norm, NumOps.Multiply(column[i], column[i]));\n+            }\n+            norm = NumOps.Sqrt(norm);\n+\n+            if (NumOps.GreaterThan(norm, NumOps.FromDouble(1e-10)))\n+            {\n+                for (int i = 0; i < rows; i++)\n+                {\n+                    orthogonal[i, j] = NumOps.Divide(column[i], norm);\n+                }\n+            }\n+            else\n+            {\n+                for (int i = 0; i < rows; i++)\n+                {\n+                    orthogonal[i, j] = i == j ? NumOps.One : NumOps.Zero;\n+                }\n+            }\n+        }\n+\n+        return orthogonal;\n+    }\n+\n+    protected override LoRALayer<T> CreateLoRALayer(int rank, double alpha)\n+    {\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        return new LoRALayer<T>(inputSize, outputSize, 1, alpha);\n+    }\n+\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        _lastInput = input.Clone();\n+        Tensor<T> baseOutput = _baseLayer.Forward(input);\n+\n+        int batchSize = input.Shape[0];\n+        int dimension = input.Shape.Length > 1 ? input.Shape[1] : input.Length;\n+\n+        Matrix<T> inputMatrix = new Matrix<T>(batchSize, dimension);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < dimension; j++)\n+            {\n+                inputMatrix[i, j] = input[i * dimension + j];\n+            }\n+        }\n+\n+        Matrix<T> compressed = inputMatrix.Multiply(_compressionMatrix);\n+        _lastCompressed = compressed;\n+\n+        Matrix<T> transformed = compressed.Multiply(_matrixM);\n+        Matrix<T> decompressed = transformed.Multiply(_decompressionMatrix);\n+\n+        T scalingFactor = NumOps.FromDouble(Alpha);\n+        decompressed = decompressed.Multiply(scalingFactor);\n+\n+        Tensor<T> moraOutput = new Tensor<T>(baseOutput.Shape);\n+        int idx = 0;\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < dimension; j++)\n+            {\n+                moraOutput[idx] = decompressed[i, j];\n+                idx++;\n+            }\n+        }\n+\n+        Tensor<T> result = new Tensor<T>(baseOutput.Shape);\n+        for (int i = 0; i < baseOutput.Length; i++)\n+        {\n+            result[i] = NumOps.Add(baseOutput[i], moraOutput[i]);\n+        }\n+\n+        return result;\n+    }\n+\n+    public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (_lastInput == null || _lastCompressed == null)\n+        {\n+            throw new InvalidOperationException(\"Forward pass must be called before backward pass\");\n+        }\n+\n+        int batchSize = _lastInput.Shape[0];\n+        int dimension = _lastInput.Shape.Length > 1 ? _lastInput.Shape[1] : _lastInput.Length;\n+\n+        Matrix<T> gradMatrix = new Matrix<T>(batchSize, dimension);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < dimension; j++)\n+            {\n+                gradMatrix[i, j] = outputGradient[i * dimension + j];\n+            }\n+        }\n+\n+        T scalingFactor = NumOps.FromDouble(Alpha);\n+        Matrix<T> gradTransformed = gradMatrix.Multiply(_decompressionMatrix.Transpose()).Multiply(scalingFactor);\n+        _matrixMGradient = _lastCompressed.Transpose().Multiply(gradTransformed);\n+        Matrix<T> gradCompressed = gradTransformed.Multiply(_matrixM.Transpose());\n+        Matrix<T> moraInputGradient = gradCompressed.Multiply(_compressionMatrix.Transpose());\n+\n+        Tensor<T> baseInputGrad = _baseLayer.Backward(outputGradient);\n+\n+        Tensor<T> inputGrad = new Tensor<T>(_lastInput.Shape);\n+        int idx = 0;\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < dimension; j++)\n+            {\n+                T moraGrad = moraInputGradient[i, j];\n+                inputGrad[idx] = NumOps.Add(baseInputGrad[idx], moraGrad);\n+                idx++;\n+            }\n+        }\n+\n+        return inputGrad;\n+    }\n+\n+    public override void UpdateParameters(T learningRate)\n+    {\n+        if (_matrixMGradient == null)\n+        {\n+            return;\n+        }\n+\n+        for (int i = 0; i < _matrixM.Rows; i++)\n+        {\n+            for (int j = 0; j < _matrixM.Columns; j++)\n+            {\n+                T update = NumOps.Multiply(_matrixMGradient[i, j], learningRate);\n+                _matrixM[i, j] = NumOps.Subtract(_matrixM[i, j], update);\n+            }\n+        }\n+\n+        if (!_freezeBaseLayer)\n+        {\n+            _baseLayer.UpdateParameters(learningRate);\n+        }\n+    }\n+\n+    public override int ParameterCount\n+    {\n+        get\n+        {\n+            int moraParams = _squareRank * _squareRank;\n+            return _freezeBaseLayer ? moraParams : (_baseLayer.ParameterCount + moraParams);\n+        }\n+    }\n+\n+    public override ILayer<T> MergeToOriginalLayer()\n+    {\n+        Matrix<T> temp = _matrixM.Multiply(_compressionMatrix.Transpose());\n+        Matrix<T> fullAdaptation = _decompressionMatrix.Multiply(temp);\n+\n+        T scalingFactor = NumOps.FromDouble(Alpha);\n+        fullAdaptation = fullAdaptation.Multiply(scalingFactor);\n+\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+\n+        IActivationFunction<T> identityActivation = new IdentityActivation<T>();\n+        DenseLayer<T> merged = new DenseLayer<T>(inputSize, outputSize, identityActivation);\n+\n+        Matrix<T> adaptationWeights = fullAdaptation.Transpose();\n+        merged.SetWeights(adaptationWeights);\n+\n+        return merged;","path":"src/LoRA/Adapters/MoRAAdapter.cs","commit_id":"489180133c7a9db9460c84a6174f60284717b952","original_commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Merged layer drops the base weights entirely**\n\n`MergeToOriginalLayer` builds a fresh `DenseLayer` from the adaptation matrix only; it never adds the original base weights or biases. Deploying this merged layer would therefore lose all pretrained knowledge and only keep the MoRA delta, which is catastrophic. Please fetch the base layer parameters, add the decompressed MoRA contribution to the weight block, preserve the biases, and return that merged tensor (mirroring how the other adapters merge).\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/LoRA/Adapters/MoRAAdapter.cs around lines 417 to 434, the method\ncurrently creates a new DenseLayer using only the decompressed MoRA delta and\ndrops the original base weights and biases; instead, retrieve the original\nlayer's weight matrix and bias vector, compute the decompressed fullAdaptation\nas done, transpose/shape it to match the base weight layout, add fullAdaptation\nto the base weight matrix (elementwise), leave the original bias unchanged (or\nadd any bias delta if present), set the merged layer's weights to the summed\nmatrix and biases to the preserved bias vector, and return that merged layer so\npretrained parameters are retained with the MoRA contribution applied.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:42:02Z","updated_at":"2025-11-02T02:42:05Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484118442","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118442"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484118442"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118442/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":417,"original_start_line":417,"start_side":"RIGHT","line":434,"original_line":434,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":434,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118450","pull_request_review_id":3408015014,"id":2484118450,"node_id":"PRRC_kwDOKSXUF86UEKOy","diff_hunk":"@@ -0,0 +1,638 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// Multi-task LoRA adapter that manages multiple task-specific LoRA layers for complex multi-task learning scenarios.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// MultiLoRA extends the basic LoRA concept to handle multiple tasks simultaneously within a single layer.\n+/// Instead of having one LoRA adaptation, it maintains a dictionary of task-specific LoRA layers,\n+/// with a routing mechanism to select the appropriate adapter for each task.\n+/// </para>\n+/// <para>\n+/// Key features:\n+/// - Multiple task-specific LoRA adapters sharing the same base layer\n+/// - Dynamic task switching during inference and training\n+/// - Per-task rank configuration for optimal parameter efficiency\n+/// - Shared base layer weights across all tasks\n+/// - Task-specific merging for deployment\n+/// </para>\n+/// <para><b>For Beginners:</b> Think of MultiLoRA as having one teacher (the base layer) and multiple\n+/// students (task-specific LoRA adapters), each specializing in different subjects.\n+///\n+/// In regular LoRA:\n+/// - You have one base layer (the teacher)\n+/// - One LoRA adapter (one student learning one subject)\n+/// - Output = base + lora_adaptation\n+///\n+/// In MultiLoRA:\n+/// - You have one base layer (the teacher)\n+/// - Multiple LoRA adapters (multiple students, each specializing in different tasks)\n+/// - Output = base + task_specific_lora_adaptation\n+///\n+/// This is powerful for:\n+/// 1. Multi-domain learning: Train on medical, legal, and technical documents simultaneously\n+/// 2. Multi-lingual models: One adapter per language\n+/// 3. Multi-task learning: Sentiment analysis, named entity recognition, question answering, etc.\n+/// 4. Continual learning: Add new tasks without forgetting old ones\n+///\n+/// Example use case:\n+/// - Base: Pre-trained language model\n+/// - Task 1: Sentiment analysis (rank=4)\n+/// - Task 2: Named entity recognition (rank=8)\n+/// - Task 3: Question answering (rank=16)\n+///\n+/// You can switch between tasks at runtime, and each task only trains its specific LoRA weights!\n+/// </para>\n+/// </remarks>\n+public class MultiLoRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Dictionary mapping task names to their specific LoRA layers.\n+    /// </summary>\n+    private readonly Dictionary<string, LoRALayer<T>> _taskAdapters;\n+\n+    /// <summary>\n+    /// The name of the currently active task.\n+    /// </summary>\n+    private string _currentTask;\n+\n+    /// <summary>\n+    /// Gets the dictionary of task-specific LoRA adapters.\n+    /// </summary>\n+    /// <remarks>\n+    /// Each task has its own dedicated LoRA layer with potentially different ranks.\n+    /// This allows for task-specific parameter efficiency optimization.\n+    /// </remarks>\n+    public IReadOnlyDictionary<string, LoRALayer<T>> TaskAdapters => _taskAdapters;\n+\n+    /// <summary>\n+    /// Gets or sets the name of the currently active task.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Changing this property switches which task-specific adapter is used during forward/backward passes.\n+    /// This allows dynamic task switching during inference or training.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is like switching between different \"modes\" of your model.\n+    /// Set it to \"sentiment\" for sentiment analysis, \"ner\" for named entity recognition, etc.\n+    /// The base layer stays the same, but the adaptation changes based on the task.\n+    /// </para>\n+    /// </remarks>\n+    /// <exception cref=\"ArgumentException\">Thrown when trying to set a task that hasn't been added.</exception>\n+    public string CurrentTask\n+    {\n+        get => _currentTask;\n+        set\n+        {\n+            if (!_taskAdapters.ContainsKey(value))\n+            {\n+                throw new ArgumentException($\"Task '{value}' has not been added. Available tasks: {string.Join(\", \", _taskAdapters.Keys)}\", nameof(value));\n+            }\n+            _currentTask = value;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Gets the number of tasks configured in this adapter.\n+    /// </summary>\n+    public int NumberOfTasks => _taskAdapters.Count;\n+\n+    /// <summary>\n+    /// Gets the total parameter count across all task adapters.\n+    /// </summary>\n+    /// <remarks>\n+    /// This includes parameters from the base layer (if not frozen) plus all task-specific LoRA layers.\n+    /// </remarks>\n+    public override int ParameterCount\n+    {\n+        get\n+        {\n+            int totalParams = _freezeBaseLayer ? 0 : _baseLayer.ParameterCount;\n+            foreach (var adapter in _taskAdapters.Values)\n+            {\n+                totalParams += adapter.ParameterCount;\n+            }\n+            return totalParams;\n+        }","path":"src/LoRA/Adapters/MultiLoRAAdapter.cs","commit_id":"489180133c7a9db9460c84a6174f60284717b952","original_commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Guard `ParameterCount` before `_taskAdapters` is initialized**  \nThe base `LoRAAdapterBase<T>` constructor calls `ParameterCount` while `_taskAdapters` is still null, so this override dereferences a null dictionary and throws during object construction. Please guard the override until the dictionary has been created (for example, fall back to `base.ParameterCount` when `_taskAdapters` is null).  \n\n```diff\n-    public override int ParameterCount\n-    {\n-        get\n-        {\n-            int totalParams = _freezeBaseLayer ? 0 : _baseLayer.ParameterCount;\n-            foreach (var adapter in _taskAdapters.Values)\n-            {\n-                totalParams += adapter.ParameterCount;\n-            }\n-            return totalParams;\n-        }\n-    }\n+    public override int ParameterCount\n+    {\n+        get\n+        {\n+            if (_taskAdapters == null)\n+            {\n+                return base.ParameterCount;\n+            }\n+\n+            int totalParams = _freezeBaseLayer ? 0 : _baseLayer.ParameterCount;\n+            foreach (var adapter in _taskAdapters.Values)\n+            {\n+                totalParams += adapter.ParameterCount;\n+            }\n+            return totalParams;\n+        }\n+    }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    public override int ParameterCount\n    {\n        get\n        {\n            if (_taskAdapters == null)\n            {\n                return base.ParameterCount;\n            }\n\n            int totalParams = _freezeBaseLayer ? 0 : _baseLayer.ParameterCount;\n            foreach (var adapter in _taskAdapters.Values)\n            {\n                totalParams += adapter.ParameterCount;\n            }\n            return totalParams;\n        }\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/LoRA/Adapters/MultiLoRAAdapter.cs around lines 114 to 120, the override\nof ParameterCount dereferences _taskAdapters which can be null during base\nconstruction; change the method to guard access to _taskAdapters (e.g. if\n_taskAdapters is null return the fallback/base value such as base.ParameterCount\nor the count computed only from _baseLayer and _freezeBaseLayer) otherwise\niterate and sum adapter.ParameterCount as before, preserving the existing\n_freezeBaseLayer logic.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:42:02Z","updated_at":"2025-11-02T02:42:05Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484118450","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118450"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484118450"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118450/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":114,"original_start_line":114,"start_side":"RIGHT","line":120,"original_line":120,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":120,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118456","pull_request_review_id":3408015014,"id":2484118456,"node_id":"PRRC_kwDOKSXUF86UEKO4","diff_hunk":"@@ -0,0 +1,638 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// Multi-task LoRA adapter that manages multiple task-specific LoRA layers for complex multi-task learning scenarios.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// MultiLoRA extends the basic LoRA concept to handle multiple tasks simultaneously within a single layer.\n+/// Instead of having one LoRA adaptation, it maintains a dictionary of task-specific LoRA layers,\n+/// with a routing mechanism to select the appropriate adapter for each task.\n+/// </para>\n+/// <para>\n+/// Key features:\n+/// - Multiple task-specific LoRA adapters sharing the same base layer\n+/// - Dynamic task switching during inference and training\n+/// - Per-task rank configuration for optimal parameter efficiency\n+/// - Shared base layer weights across all tasks\n+/// - Task-specific merging for deployment\n+/// </para>\n+/// <para><b>For Beginners:</b> Think of MultiLoRA as having one teacher (the base layer) and multiple\n+/// students (task-specific LoRA adapters), each specializing in different subjects.\n+///\n+/// In regular LoRA:\n+/// - You have one base layer (the teacher)\n+/// - One LoRA adapter (one student learning one subject)\n+/// - Output = base + lora_adaptation\n+///\n+/// In MultiLoRA:\n+/// - You have one base layer (the teacher)\n+/// - Multiple LoRA adapters (multiple students, each specializing in different tasks)\n+/// - Output = base + task_specific_lora_adaptation\n+///\n+/// This is powerful for:\n+/// 1. Multi-domain learning: Train on medical, legal, and technical documents simultaneously\n+/// 2. Multi-lingual models: One adapter per language\n+/// 3. Multi-task learning: Sentiment analysis, named entity recognition, question answering, etc.\n+/// 4. Continual learning: Add new tasks without forgetting old ones\n+///\n+/// Example use case:\n+/// - Base: Pre-trained language model\n+/// - Task 1: Sentiment analysis (rank=4)\n+/// - Task 2: Named entity recognition (rank=8)\n+/// - Task 3: Question answering (rank=16)\n+///\n+/// You can switch between tasks at runtime, and each task only trains its specific LoRA weights!\n+/// </para>\n+/// </remarks>\n+public class MultiLoRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Dictionary mapping task names to their specific LoRA layers.\n+    /// </summary>\n+    private readonly Dictionary<string, LoRALayer<T>> _taskAdapters;\n+\n+    /// <summary>\n+    /// The name of the currently active task.\n+    /// </summary>\n+    private string _currentTask;\n+\n+    /// <summary>\n+    /// Gets the dictionary of task-specific LoRA adapters.\n+    /// </summary>\n+    /// <remarks>\n+    /// Each task has its own dedicated LoRA layer with potentially different ranks.\n+    /// This allows for task-specific parameter efficiency optimization.\n+    /// </remarks>\n+    public IReadOnlyDictionary<string, LoRALayer<T>> TaskAdapters => _taskAdapters;\n+\n+    /// <summary>\n+    /// Gets or sets the name of the currently active task.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Changing this property switches which task-specific adapter is used during forward/backward passes.\n+    /// This allows dynamic task switching during inference or training.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is like switching between different \"modes\" of your model.\n+    /// Set it to \"sentiment\" for sentiment analysis, \"ner\" for named entity recognition, etc.\n+    /// The base layer stays the same, but the adaptation changes based on the task.\n+    /// </para>\n+    /// </remarks>\n+    /// <exception cref=\"ArgumentException\">Thrown when trying to set a task that hasn't been added.</exception>\n+    public string CurrentTask\n+    {\n+        get => _currentTask;\n+        set\n+        {\n+            if (!_taskAdapters.ContainsKey(value))\n+            {\n+                throw new ArgumentException($\"Task '{value}' has not been added. Available tasks: {string.Join(\", \", _taskAdapters.Keys)}\", nameof(value));\n+            }\n+            _currentTask = value;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Gets the number of tasks configured in this adapter.\n+    /// </summary>\n+    public int NumberOfTasks => _taskAdapters.Count;\n+\n+    /// <summary>\n+    /// Gets the total parameter count across all task adapters.\n+    /// </summary>\n+    /// <remarks>\n+    /// This includes parameters from the base layer (if not frozen) plus all task-specific LoRA layers.\n+    /// </remarks>\n+    public override int ParameterCount\n+    {\n+        get\n+        {\n+            int totalParams = _freezeBaseLayer ? 0 : _baseLayer.ParameterCount;\n+            foreach (var adapter in _taskAdapters.Values)\n+            {\n+                totalParams += adapter.ParameterCount;\n+            }\n+            return totalParams;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Initializes a new Multi-LoRA adapter with an initial default task.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with multiple LoRA adapters.</param>\n+    /// <param name=\"defaultTaskName\">The name of the default task.</param>\n+    /// <param name=\"defaultRank\">The rank for the default task's LoRA layer.</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer or defaultTaskName is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when defaultTaskName is empty or whitespace.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// The adapter is initialized with one default task. Additional tasks can be added using AddTask().\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This creates a MultiLoRA adapter starting with one task.\n+    /// Think of it like creating a multi-tool that starts with one blade, and you can add more tools later.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The shared foundation layer (like the handle of a multi-tool)\n+    /// - defaultTaskName: A name for your first task (e.g., \"sentiment\", \"translation\")\n+    /// - defaultRank: How complex this task's adaptation is (higher = more parameters)\n+    /// - alpha: Strength of the adaptation\n+    /// - freezeBaseLayer: Whether to lock the base layer (usually true to save memory)\n+    ///\n+    /// After creation, you can add more tasks with different ranks optimized for each task's complexity.\n+    /// </para>\n+    /// </remarks>\n+    public MultiLoRAAdapter(\n+        ILayer<T> baseLayer,\n+        string defaultTaskName,\n+        int defaultRank,\n+        double alpha = -1,\n+        bool freezeBaseLayer = true)\n+        : base(baseLayer, defaultRank, alpha, freezeBaseLayer)\n+    {\n+        if (string.IsNullOrWhiteSpace(defaultTaskName))\n+        {\n+            throw new ArgumentException(\"Default task name cannot be null or whitespace\", nameof(defaultTaskName));\n+        }\n+\n+        _taskAdapters = new Dictionary<string, LoRALayer<T>>();\n+        _currentTask = defaultTaskName;\n+\n+        // Add the default task using the base class's LoRA layer\n+        _taskAdapters[defaultTaskName] = _loraLayer;\n+    }\n+\n+    /// <summary>\n+    /// Adds a new task with its own LoRA adapter.\n+    /// </summary>\n+    /// <param name=\"taskName\">The name of the task (must be unique).</param>\n+    /// <param name=\"rank\">The rank for this task's LoRA layer.</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor for this task (defaults to rank if negative).</param>\n+    /// <exception cref=\"ArgumentException\">Thrown when taskName is null, empty, whitespace, or already exists.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// Each task can have a different rank, allowing you to optimize parameter usage based on task complexity.\n+    /// More complex tasks can use higher ranks, while simpler tasks can use lower ranks.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This adds a new \"mode\" to your model.\n+    ///\n+    /// Example:\n+    /// - Task \"sentiment\" with rank=4: Simple classification (positive/negative/neutral)\n+    /// - Task \"ner\" with rank=8: More complex named entity recognition\n+    /// - Task \"qa\" with rank=16: Even more complex question answering\n+    ///\n+    /// Each task gets its own small set of parameters (determined by rank) that learn task-specific\n+    /// adaptations, while all tasks share the same base layer knowledge.\n+    ///\n+    /// Benefits:\n+    /// - Different ranks for different task complexities\n+    /// - No interference between tasks (each has separate parameters)\n+    /// - Can train tasks independently or simultaneously\n+    /// - Add new tasks without retraining existing ones\n+    /// </para>\n+    /// </remarks>\n+    public void AddTask(string taskName, int rank, double alpha = -1)\n+    {\n+        if (string.IsNullOrWhiteSpace(taskName))\n+        {\n+            throw new ArgumentException(\"Task name cannot be null or whitespace\", nameof(taskName));\n+        }\n+\n+        if (_taskAdapters.ContainsKey(taskName))\n+        {\n+            throw new ArgumentException($\"Task '{taskName}' already exists\", nameof(taskName));\n+        }\n+\n+        // Create a new LoRA layer for this task\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        LoRALayer<T> taskAdapter = new LoRALayer<T>(inputSize, outputSize, rank, alpha);\n+\n+        _taskAdapters[taskName] = taskAdapter;\n+    }\n+\n+    /// <summary>\n+    /// Removes a task and its associated LoRA adapter.\n+    /// </summary>\n+    /// <param name=\"taskName\">The name of the task to remove.</param>\n+    /// <returns>True if the task was removed, false if it didn't exist.</returns>\n+    /// <exception cref=\"InvalidOperationException\">Thrown when trying to remove the last remaining task.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// You cannot remove the last task. At least one task must always be present.\n+    /// If removing the current task, the CurrentTask property will be set to the first remaining task.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This removes a task you no longer need.\n+    /// Like removing a tool from your multi-tool, but you must always keep at least one.\n+    /// If you remove the currently active task, the adapter automatically switches to another available task.\n+    /// </para>\n+    /// </remarks>\n+    public bool RemoveTask(string taskName)\n+    {\n+        if (_taskAdapters.Count <= 1)\n+        {\n+            throw new InvalidOperationException(\"Cannot remove the last task. At least one task must remain.\");\n+        }\n+\n+        bool removed = _taskAdapters.Remove(taskName);\n+\n+        // If we removed the current task, switch to the first available task\n+        if (removed && _currentTask == taskName)\n+        {\n+            _currentTask = _taskAdapters.Keys.First();\n+        }\n+\n+        return removed;\n+    }\n+\n+    /// <summary>\n+    /// Sets the current task for subsequent forward/backward operations.\n+    /// </summary>\n+    /// <param name=\"taskName\">The name of the task to activate.</param>\n+    /// <exception cref=\"ArgumentException\">Thrown when the task doesn't exist.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This switches which task the model is currently working on.\n+    /// Call this before forward() to tell the model what kind of task it should perform.\n+    ///\n+    /// Example usage:\n+    /// ```csharp\n+    /// adapter.SetCurrentTask(\"sentiment\");\n+    /// var sentimentOutput = adapter.Forward(input);\n+    ///\n+    /// adapter.SetCurrentTask(\"ner\");\n+    /// var nerOutput = adapter.Forward(sameInput);\n+    /// ```\n+    ///\n+    /// Same input, different outputs based on which task is active!\n+    /// </para>\n+    /// </remarks>\n+    public void SetCurrentTask(string taskName)\n+    {\n+        CurrentTask = taskName; // Uses property setter for validation\n+    }\n+\n+    /// <summary>\n+    /// Gets the LoRA layer for a specific task.\n+    /// </summary>\n+    /// <param name=\"taskName\">The name of the task.</param>\n+    /// <returns>The LoRA layer for the specified task.</returns>\n+    /// <exception cref=\"ArgumentException\">Thrown when the task doesn't exist.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This lets you access a specific task's LoRA layer directly.\n+    /// Useful for inspecting parameters, getting statistics, or manual manipulation.\n+    /// </para>\n+    /// </remarks>\n+    public LoRALayer<T> GetTaskAdapter(string taskName)\n+    {\n+        if (!_taskAdapters.TryGetValue(taskName, out var adapter))\n+        {\n+            throw new ArgumentException($\"Task '{taskName}' not found. Available tasks: {string.Join(\", \", _taskAdapters.Keys)}\", nameof(taskName));\n+        }\n+        return adapter;\n+    }\n+\n+    /// <summary>\n+    /// Gets the rank of a specific task's LoRA adapter.\n+    /// </summary>\n+    /// <param name=\"taskName\">The name of the task.</param>\n+    /// <returns>The rank of the task's LoRA layer.</returns>\n+    /// <exception cref=\"ArgumentException\">Thrown when the task doesn't exist.</exception>\n+    public int GetTaskRank(string taskName)\n+    {\n+        return GetTaskAdapter(taskName).Rank;\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass using the currently active task's adapter.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Sum of base layer output and current task's LoRA output.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The forward pass computes: output = base_layer(input) + current_task_lora(input)\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This processes data through the model using the current task.\n+    /// 1. Input goes through the base layer (shared knowledge)\n+    /// 2. Input goes through the current task's LoRA layer (task-specific adaptation)\n+    /// 3. Results are added together\n+    ///\n+    /// The magic: Different tasks produce different outputs even though they share the same base layer!\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        // Forward through base layer\n+        Tensor<T> baseOutput = _baseLayer.Forward(input);\n+\n+        // Forward through current task's LoRA layer\n+        LoRALayer<T> currentAdapter = _taskAdapters[_currentTask];\n+        Tensor<T> loraOutput = currentAdapter.Forward(input);\n+\n+        // Sum the outputs\n+        Tensor<T> result = new Tensor<T>(baseOutput.Shape);\n+        for (int i = 0; i < baseOutput.Length; i++)\n+        {\n+            result[i] = NumOps.Add(baseOutput[i], loraOutput[i]);\n+        }\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Performs the backward pass through the current task's adapter.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient flowing back from the next layer.</param>\n+    /// <returns>Gradient to pass to the previous layer.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The backward pass only updates the current task's LoRA parameters. Other tasks are unaffected.\n+    /// This allows task-specific learning without interference.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> During training, this updates only the current task's parameters.\n+    ///\n+    /// Benefits:\n+    /// - Training task A doesn't mess up task B's learning\n+    /// - Can train tasks one at a time or in batches\n+    /// - No \"catastrophic forgetting\" between tasks\n+    ///\n+    /// The gradients flow through:\n+    /// 1. Current task's LoRA layer (gets updated)\n+    /// 2. Base layer (only updated if not frozen)\n+    /// 3. Combined gradients flow back to previous layers\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        // Backward through current task's LoRA layer\n+        LoRALayer<T> currentAdapter = _taskAdapters[_currentTask];\n+        Tensor<T> loraInputGrad = currentAdapter.Backward(outputGradient);\n+\n+        // Backward through base layer\n+        Tensor<T> baseInputGrad = _baseLayer.Backward(outputGradient);\n+\n+        // Sum input gradients\n+        Tensor<T> inputGrad = new Tensor<T>(loraInputGrad.Shape);\n+        for (int i = 0; i < loraInputGrad.Length; i++)\n+        {\n+            inputGrad[i] = NumOps.Add(loraInputGrad[i], baseInputGrad[i]);\n+        }\n+\n+        // Update parameter gradients vector\n+        UpdateParameterGradientsFromLayers();\n+\n+        return inputGrad;\n+    }\n+\n+    /// <summary>\n+    /// Updates parameters for the current task only.\n+    /// </summary>\n+    /// <param name=\"learningRate\">The learning rate for parameter updates.</param>\n+    /// <remarks>\n+    /// <para>\n+    /// Only the current task's LoRA parameters are updated. Other tasks remain unchanged.\n+    /// The base layer is updated only if not frozen.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is where learning happens for the current task.\n+    /// Only the active task's parameters get updated, leaving other tasks untouched.\n+    /// This is key to multi-task learning without interference!\n+    /// </para>\n+    /// </remarks>\n+    public override void UpdateParameters(T learningRate)\n+    {\n+        // Update current task's LoRA layer\n+        LoRALayer<T> currentAdapter = _taskAdapters[_currentTask];\n+        currentAdapter.UpdateParameters(learningRate);\n+\n+        // Update base layer if not frozen\n+        if (!_freezeBaseLayer)\n+        {\n+            _baseLayer.UpdateParameters(learningRate);\n+        }\n+\n+        // Update parameter vector\n+        UpdateParametersFromLayers();\n+    }\n+\n+    /// <summary>\n+    /// Gets the current parameters as a vector.\n+    /// </summary>\n+    /// <returns>Vector containing base parameters (if not frozen) and all task adapters' parameters.</returns>\n+    public override Vector<T> GetParameters()\n+    {\n+        Vector<T> parameters = new Vector<T>(ParameterCount);\n+        int idx = 0;\n+\n+        // Base layer parameters (if not frozen)\n+        if (!_freezeBaseLayer)\n+        {\n+            Vector<T> baseParams = _baseLayer.GetParameters();\n+            for (int i = 0; i < baseParams.Length; i++)\n+            {\n+                parameters[idx++] = baseParams[i];\n+            }\n+        }\n+\n+        // All task adapters' parameters\n+        foreach (var adapter in _taskAdapters.Values)\n+        {\n+            Vector<T> taskParams = adapter.GetParameters();\n+            for (int i = 0; i < taskParams.Length; i++)\n+            {\n+                parameters[idx++] = taskParams[i];\n+            }\n+        }\n+\n+        return parameters;\n+    }\n+\n+    /// <summary>\n+    /// Sets the layer parameters from a vector.\n+    /// </summary>\n+    /// <param name=\"parameters\">Vector containing all parameters.</param>\n+    public override void SetParameters(Vector<T> parameters)\n+    {\n+        if (parameters.Length != ParameterCount)\n+        {\n+            throw new ArgumentException($\"Expected {ParameterCount} parameters, got {parameters.Length}\", nameof(parameters));\n+        }\n+\n+        int idx = 0;\n+\n+        // Base layer parameters (if not frozen)\n+        if (!_freezeBaseLayer)\n+        {\n+            int baseParamCount = _baseLayer.ParameterCount;\n+            Vector<T> baseParams = new Vector<T>(baseParamCount);\n+            for (int i = 0; i < baseParamCount; i++)\n+            {\n+                baseParams[i] = parameters[idx++];\n+            }\n+            _baseLayer.SetParameters(baseParams);\n+        }\n+\n+        // All task adapters' parameters\n+        foreach (var adapter in _taskAdapters.Values)\n+        {\n+            int taskParamCount = adapter.ParameterCount;\n+            Vector<T> taskParams = new Vector<T>(taskParamCount);\n+            for (int i = 0; i < taskParamCount; i++)\n+            {\n+                taskParams[i] = parameters[idx++];\n+            }\n+            adapter.SetParameters(taskParams);\n+        }\n+\n+        Parameters = parameters.Clone();\n+    }\n+\n+    /// <summary>\n+    /// Merges a specific task's LoRA weights into the base layer.\n+    /// </summary>\n+    /// <param name=\"taskName\">The name of the task to merge.</param>\n+    /// <returns>A new layer with the specified task's LoRA weights merged into the base layer.</returns>\n+    /// <exception cref=\"ArgumentException\">Thrown when the task doesn't exist.</exception>\n+    /// <exception cref=\"NotSupportedException\">Thrown when the base layer type doesn't support merging.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// This creates a deployment-ready layer for a specific task by merging its LoRA weights\n+    /// into the base layer. This is useful when you want to deploy a single-task model.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This \"bakes in\" one task's adaptations for deployment.\n+    ///\n+    /// Use case:\n+    /// - You trained a MultiLoRA model with 5 tasks\n+    /// - For production, you only need the \"sentiment\" task\n+    /// - Call MergeTaskToLayer(\"sentiment\") to create a standalone layer\n+    /// - Deploy just that layer (smaller, faster, simpler)\n+    ///\n+    /// The merged layer has the base weights + that task's LoRA weights combined into one.\n+    /// </para>\n+    /// </remarks>\n+    public ILayer<T> MergeTaskToLayer(string taskName)\n+    {\n+        if (!_taskAdapters.TryGetValue(taskName, out var taskAdapter))\n+        {\n+            throw new ArgumentException($\"Task '{taskName}' not found. Available tasks: {string.Join(\", \", _taskAdapters.Keys)}\", nameof(taskName));\n+        }\n+\n+        // This implementation assumes the base layer is a DenseLayer or FullyConnectedLayer\n+        // More sophisticated implementations could support other layer types\n+        DenseLayer<T>? denseBase = _baseLayer as DenseLayer<T>;\n+        FullyConnectedLayer<T>? fcBase = _baseLayer as FullyConnectedLayer<T>;\n+\n+        if (denseBase == null && fcBase == null)\n+        {\n+            throw new NotSupportedException($\"Merging is currently only supported for DenseLayer and FullyConnectedLayer base layers. Base layer type: {_baseLayer.GetType().Name}\");\n+        }\n+\n+        // Get the LoRA weight contribution for this task\n+        Matrix<T> loraWeights = taskAdapter.MergeWeights();\n+\n+        // Get base layer parameters\n+        Vector<T> baseParams = _baseLayer.GetParameters();\n+\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        int weightCount = inputSize * outputSize;\n+\n+        // Create new parameters with merged weights\n+        Vector<T> mergedParams = new Vector<T>(baseParams.Length);\n+\n+        // Merge weights\n+        for (int i = 0; i < weightCount; i++)\n+        {\n+            int row = i / inputSize;\n+            int col = i % inputSize;\n+            mergedParams[i] = NumOps.Add(baseParams[i], loraWeights[row, col]);\n+        }\n+\n+        // Copy biases unchanged\n+        for (int i = weightCount; i < baseParams.Length; i++)\n+        {\n+            mergedParams[i] = baseParams[i];\n+        }\n+\n+        // Create a new dense layer with merged parameters\n+        DenseLayer<T> mergedLayer = new DenseLayer<T>(inputSize, outputSize, (IActivationFunction<T>?)null);\n+        mergedLayer.SetParameters(mergedParams);\n+\n+        return mergedLayer;\n+    }\n+\n+    /// <summary>\n+    /// Merges the currently active task's LoRA weights into the base layer.\n+    /// </summary>\n+    /// <returns>A new layer with current task's LoRA weights merged into the base layer.</returns>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This is a shortcut to merge the current task without specifying its name.\n+    /// Equivalent to calling MergeTaskToLayer(CurrentTask).\n+    /// </para>\n+    /// </remarks>\n+    public override ILayer<T> MergeToOriginalLayer()\n+    {\n+        return MergeTaskToLayer(_currentTask);\n+    }\n+\n+    /// <summary>\n+    /// Updates the parameter vector from the current layer states.\n+    /// </summary>\n+    private void UpdateParametersFromLayers()\n+    {\n+        Parameters = GetParameters();\n+    }\n+\n+    /// <summary>\n+    /// Updates the parameter gradients vector from the layer gradients.\n+    /// </summary>\n+    private void UpdateParameterGradientsFromLayers()\n+    {\n+        ParameterGradients = new Vector<T>(ParameterCount);\n+        int idx = 0;\n+\n+        // Base layer gradients (if not frozen)\n+        if (!_freezeBaseLayer)\n+        {\n+            Vector<T> baseGrads = _baseLayer.GetParameterGradients();\n+            for (int i = 0; i < baseGrads.Length; i++)\n+            {\n+                ParameterGradients[idx++] = baseGrads[i];\n+            }\n+        }\n+\n+        // Current task's gradients\n+        LoRALayer<T> currentAdapter = _taskAdapters[_currentTask];\n+        Vector<T> loraGrads = currentAdapter.GetParameterGradients();\n+        for (int i = 0; i < loraGrads.Length; i++)\n+        {\n+            ParameterGradients[idx++] = loraGrads[i];\n+        }\n+\n+        // Other tasks have zero gradients (they weren't updated)\n+        while (idx < ParameterCount)\n+        {\n+            ParameterGradients[idx++] = NumOps.Zero;","path":"src/LoRA/Adapters/MultiLoRAAdapter.cs","commit_id":"489180133c7a9db9460c84a6174f60284717b952","original_commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Align parameter-gradient packing with the parameter layout**  \n`ParameterGradients` currently writes the active task‚Äôs gradients immediately after the base-layer block and then pads zeros for the remaining slots. Because `GetParameters()` packs every task adapter in dictionary order, any task that precedes `CurrentTask` ends up with gradients belonging to a different adapter‚Äîthe optimizer will update the wrong parameters. Please iterate the tasks in the same order as `GetParameters()` and explicitly emit zeros for non-active tasks in-place.  \n\n```diff\n-        LoRALayer<T> currentAdapter = _taskAdapters[_currentTask];\n-        Vector<T> loraGrads = currentAdapter.GetParameterGradients();\n-        for (int i = 0; i < loraGrads.Length; i++)\n-        {\n-            ParameterGradients[idx++] = loraGrads[i];\n-        }\n-\n-        // Other tasks have zero gradients (they weren't updated)\n-        while (idx < ParameterCount)\n-        {\n-            ParameterGradients[idx++] = NumOps.Zero;\n-        }\n+        LoRALayer<T> currentAdapter = _taskAdapters[_currentTask];\n+        foreach (var adapter in _taskAdapters.Values)\n+        {\n+            Vector<T> grads = ReferenceEquals(adapter, currentAdapter)\n+                ? adapter.GetParameterGradients()\n+                : new Vector<T>(adapter.ParameterCount);\n+\n+            for (int i = 0; i < grads.Length; i++)\n+            {\n+                ParameterGradients[idx++] = grads[i];\n+            }\n+        }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    private void UpdateParameterGradientsFromLayers()\n    {\n        ParameterGradients = new Vector<T>(ParameterCount);\n        int idx = 0;\n\n        // Base layer gradients (if not frozen)\n        if (!_freezeBaseLayer)\n        {\n            Vector<T> baseGrads = _baseLayer.GetParameterGradients();\n            for (int i = 0; i < baseGrads.Length; i++)\n            {\n                ParameterGradients[idx++] = baseGrads[i];\n            }\n        }\n\n        // Current task's gradients\n        LoRALayer<T> currentAdapter = _taskAdapters[_currentTask];\n        foreach (var adapter in _taskAdapters.Values)\n        {\n            Vector<T> grads = ReferenceEquals(adapter, currentAdapter)\n                ? adapter.GetParameterGradients()\n                : new Vector<T>(adapter.ParameterCount);\n\n            for (int i = 0; i < grads.Length; i++)\n            {\n                ParameterGradients[idx++] = grads[i];\n            }\n        }\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/LoRA/Adapters/MultiLoRAAdapter.cs around lines 592 to 618, the\nParameterGradients packing writes the active task's gradients immediately after\nthe base layer then pads zeros to the end, which misaligns gradients relative to\nGetParameters()'s packing order; change the routine to mirror GetParameters()\nexactly: after optionally copying base layer gradients (if !_freezeBaseLayer)\nadvance idx, then iterate the task adapters in the same order used by\nGetParameters() (not just current task), for each task append that task's\ngradients if it is the active/current task or append zeros of the same length if\nit is not, ensuring idx increments correctly so every task slot is filled\nin-place and the overall ParameterGradients ordering matches GetParameters().\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:42:02Z","updated_at":"2025-11-02T02:42:06Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484118456","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118456"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484118456"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118456/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":592,"original_start_line":592,"start_side":"RIGHT","line":618,"original_line":618,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":618,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118467","pull_request_review_id":3408015014,"id":2484118467,"node_id":"PRRC_kwDOKSXUF86UEKPD","diff_hunk":"@@ -0,0 +1,628 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// Quantization-Aware LoRA (QA-LoRA) adapter that combines parameter-efficient fine-tuning with group-wise quantization awareness.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// QA-LoRA extends standard LoRA by being aware of quantization during training. This allows the adapter\n+/// to learn compensations for quantization errors, resulting in better final accuracy compared to\n+/// post-training quantization approaches. The key innovation is simulating quantization during the\n+/// forward pass so that gradients account for quantization effects.\n+/// </para>\n+/// <para><b>For Beginners:</b> QA-LoRA solves a critical problem when deploying models to resource-constrained devices.\n+///\n+/// The Problem:\n+/// - Modern neural networks use high-precision numbers (32-bit floats)\n+/// - Mobile/edge devices need lower precision (4-bit or 8-bit integers) for speed and memory\n+/// - Converting after training (post-training quantization) often loses accuracy\n+///\n+/// QA-LoRA's Solution:\n+/// - Simulates low-precision during training (quantization-aware training)\n+/// - Learns to compensate for quantization errors\n+/// - Uses LoRA for parameter efficiency (only trains the adaptation, not full model)\n+/// - Applies group-wise quantization (groups of weights share scaling factors)\n+///\n+/// Key Concepts:\n+///\n+/// 1. Quantization: Converting high-precision numbers to low-precision\n+///    Example: 32-bit float 0.7234 ‚Üí 4-bit integer 11 (range 0-15)\n+///\n+/// 2. Group-wise Quantization: Instead of one scale for all weights, weights are divided into groups,\n+///    each with its own scale. This preserves more information.\n+///    Example: 64 weights ‚Üí 4 groups of 16 weights each, each group has its own scale\n+///\n+/// 3. Quantization-Aware Training: During training, simulate quantization in forward pass:\n+///    - Convert weights to low-precision (quantize)\n+///    - Immediately convert back to high-precision (dequantize)\n+///    - Use these \"quantized\" values for computation\n+///    - Gradients learn to compensate for the quantization noise\n+///\n+/// 4. Straight-Through Estimator (STE): During backward pass, treat quantization as identity\n+///    - Forward: y = quantize(x)\n+///    - Backward: ‚àÇy/‚àÇx ‚âà 1 (gradient flows through unchanged)\n+///    - This allows gradients to update the full-precision weights\n+///\n+/// Parameters:\n+/// - QuantizationBits: How many bits to use (4-bit, 8-bit, etc.)\n+/// - GroupSize: How many weights per quantization group (e.g., 64, 128)\n+/// - Smaller GroupSize = more scales = better accuracy but more overhead\n+/// - Larger GroupSize = fewer scales = more efficient but less accurate\n+///\n+/// Example Workflow:\n+/// 1. Training: Forward pass uses simulated 4-bit quantization\n+/// 2. Gradients: Backward pass learns to work around quantization errors\n+/// 3. Deployment: Actually quantize the merged weights to 4-bit for inference\n+/// 4. Result: Much better accuracy than quantizing after training\n+///\n+/// Research Context:\n+/// - QLoRA (May 2023): Introduced efficient 4-bit quantization for LoRA\n+/// - QA-LoRA: Extends this with quantization-aware training for better results\n+/// - Typical improvement: 1-3% accuracy gain over post-training quantization\n+///\n+/// Use Cases:\n+/// - Deploying large language models on mobile devices\n+/// - Edge AI applications with strict memory constraints\n+/// - Reducing model size while maintaining accuracy\n+/// - Fine-tuning for deployment on specific hardware (TPUs, specialized accelerators)\n+/// </para>\n+/// </remarks>\n+public class QALoRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Number of bits to use for quantization (e.g., 4, 8).\n+    /// </summary>\n+    private int _quantizationBits;\n+\n+    /// <summary>\n+    /// Number of weights per quantization group.\n+    /// </summary>\n+    /// <remarks>\n+    /// Smaller groups preserve more information but require more scaling factors.\n+    /// Typical values: 64, 128, 256.\n+    /// </remarks>\n+    private int _groupSize;\n+\n+    /// <summary>\n+    /// Whether quantization simulation is currently enabled.\n+    /// </summary>\n+    /// <remarks>\n+    /// Can be disabled during initial warmup or final evaluation.\n+    /// </remarks>\n+    private bool _quantizationEnabled;\n+\n+    /// <summary>\n+    /// Gets or sets the number of bits used for quantization.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Common values:\n+    /// - 4 bits: Extremely memory-efficient, requires careful tuning\n+    /// - 8 bits: Good balance of efficiency and accuracy\n+    /// - 16 bits: Close to full precision, minimal savings\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This controls how much compression you apply.\n+    /// - 4-bit: 8x compression (32-bit ‚Üí 4-bit), more aggressive\n+    /// - 8-bit: 4x compression (32-bit ‚Üí 8-bit), safer choice\n+    /// Lower bits = smaller model but harder to maintain accuracy.\n+    /// </para>\n+    /// </remarks>\n+    public int QuantizationBits\n+    {\n+        get => _quantizationBits;\n+        set\n+        {\n+            if (value < 1 || value > 16)\n+            {\n+                throw new ArgumentException(\"Quantization bits must be between 1 and 16\", nameof(value));\n+            }\n+            _quantizationBits = value;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Gets or sets the group size for group-wise quantization.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Group-wise quantization divides weights into groups, each with independent scaling factors.\n+    /// This preserves more dynamic range than using a single scale for all weights.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Imagine you have 1024 weights to quantize:\n+    /// - GroupSize = 1024: One scale for all weights (simple but loses information)\n+    /// - GroupSize = 128: Eight scales (1024/128 = 8 groups, better accuracy)\n+    /// - GroupSize = 64: Sixteen scales (1024/64 = 16 groups, even better but more overhead)\n+    ///\n+    /// Smaller groups mean each group's weights are more similar, so a single scale per group\n+    /// is more accurate. But you need to store more scales.\n+    /// </para>\n+    /// </remarks>\n+    public int GroupSize\n+    {\n+        get => _groupSize;\n+        set\n+        {\n+            if (value < 1)\n+            {\n+                throw new ArgumentException(\"Group size must be positive\", nameof(value));\n+            }\n+            _groupSize = value;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Gets or sets whether quantization simulation is enabled during forward/backward passes.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Disabling quantization can be useful for:\n+    /// - Initial warmup phases\n+    /// - Evaluating full-precision performance\n+    /// - Debugging training issues\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is like a toggle switch:\n+    /// - Enabled: Simulate low-precision during training (quantization-aware)\n+    /// - Disabled: Use full-precision (standard LoRA training)\n+    /// You might start with it disabled for stability, then enable it partway through training.\n+    /// </para>\n+    /// </remarks>\n+    public bool QuantizationEnabled\n+    {\n+        get => _quantizationEnabled;\n+        set => _quantizationEnabled = value;\n+    }\n+\n+    /// <summary>\n+    /// Initializes a new QA-LoRA adapter with quantization awareness.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with QA-LoRA.</param>\n+    /// <param name=\"rank\">The rank of the LoRA decomposition.</param>\n+    /// <param name=\"quantizationBits\">Number of bits for quantization (e.g., 4, 8).</param>\n+    /// <param name=\"groupSize\">Number of weights per quantization group.</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when quantizationBits or groupSize are invalid.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This creates a QA-LoRA adapter that will train with quantization awareness.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to efficiently fine-tune\n+    /// - rank: How much compression for LoRA (lower = fewer parameters)\n+    /// - quantizationBits: Target precision for deployment (4 or 8 typically)\n+    /// - groupSize: Granularity of quantization (64-128 recommended)\n+    /// - alpha: How strong the LoRA effect is\n+    /// - freezeBaseLayer: Whether to lock the original weights (usually true)\n+    ///\n+    /// Example: QALoRAAdapter(myLayer, rank=8, quantizationBits=4, groupSize=64)\n+    /// - Uses 8-rank LoRA for parameter efficiency\n+    /// - Simulates 4-bit quantization during training\n+    /// - Groups of 64 weights share scaling factors\n+    /// </para>\n+    /// </remarks>\n+    public QALoRAAdapter(\n+        ILayer<T> baseLayer,\n+        int rank,\n+        int quantizationBits,\n+        int groupSize,\n+        double alpha = -1,\n+        bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        if (quantizationBits < 1 || quantizationBits > 16)\n+        {\n+            throw new ArgumentException(\"Quantization bits must be between 1 and 16\", nameof(quantizationBits));\n+        }\n+\n+        if (groupSize < 1)\n+        {\n+            throw new ArgumentException(\"Group size must be positive\", nameof(groupSize));\n+        }\n+\n+        _quantizationBits = quantizationBits;\n+        _groupSize = groupSize;\n+        _quantizationEnabled = true; // Enabled by default\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass through both base and LoRA layers with quantization simulation.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Sum of base layer output and quantized LoRA output.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The forward pass with quantization awareness:\n+    /// 1. Compute base layer output (no quantization)\n+    /// 2. Get LoRA layer parameters\n+    /// 3. Simulate quantization: quantize ‚Üí dequantize (if enabled)\n+    /// 4. Compute LoRA output with quantized parameters\n+    /// 5. Sum base + quantized LoRA outputs\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is where quantization-aware training happens!\n+    ///\n+    /// Normal LoRA forward pass:\n+    /// - base_output = base_layer(input)\n+    /// - lora_output = lora_layer(input)  // Uses full-precision weights\n+    /// - return base_output + lora_output\n+    ///\n+    /// QA-LoRA forward pass:\n+    /// - base_output = base_layer(input)\n+    /// - lora_weights_full = get_lora_weights()  // Full precision\n+    /// - lora_weights_quant = dequantize(quantize(lora_weights_full))  // Simulate quantization\n+    /// - lora_output = compute_with_quantized_weights(input, lora_weights_quant)\n+    /// - return base_output + lora_output\n+    ///\n+    /// The key difference: We temporarily quantize and dequantize the LoRA weights,\n+    /// which adds noise. The gradients will learn to work despite this noise!\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        // Forward through base layer (unchanged)\n+        Tensor<T> baseOutput = _baseLayer.Forward(input);\n+\n+        // Forward through LoRA layer with optional quantization simulation\n+        Tensor<T> loraOutput;\n+\n+        if (_quantizationEnabled)\n+        {\n+            // Simulate quantization on LoRA parameters\n+            Vector<T> originalParams = _loraLayer.GetParameters();\n+            Vector<T> quantizedParams = QuantizeAndDequantize(originalParams);\n+\n+            // Temporarily set quantized parameters\n+            _loraLayer.SetParameters(quantizedParams);\n+\n+            // Forward with quantized parameters\n+            loraOutput = _loraLayer.Forward(input);\n+\n+            // Restore original parameters (important for gradient computation)\n+            _loraLayer.SetParameters(originalParams);\n+        }\n+        else\n+        {\n+            // Standard LoRA forward (no quantization simulation)\n+            loraOutput = _loraLayer.Forward(input);\n+        }\n+\n+        // Sum the outputs\n+        Tensor<T> result = new Tensor<T>(baseOutput.Shape);\n+        for (int i = 0; i < baseOutput.Length; i++)\n+        {\n+            result[i] = NumOps.Add(baseOutput[i], loraOutput[i]);\n+        }\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Performs the backward pass through both layers, accounting for quantization in gradients.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient flowing back from the next layer.</param>\n+    /// <returns>Gradient to pass to the previous layer.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The backward pass uses the Straight-Through Estimator (STE) for quantization:\n+    /// - Forward: y = quantize(x)\n+    /// - Backward: ‚àÇL/‚àÇx = ‚àÇL/‚àÇy (gradient passes through unchanged)\n+    ///\n+    /// This allows gradients to flow to the full-precision weights despite quantization.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is the tricky part of quantization-aware training!\n+    ///\n+    /// The Problem:\n+    /// - Quantization is a discontinuous operation (rounding)\n+    /// - Discontinuous operations have zero or undefined gradients\n+    /// - If gradients can't flow, we can't update weights, so training fails\n+    ///\n+    /// The Solution (Straight-Through Estimator):\n+    /// - Pretend quantization is the identity function during backprop\n+    /// - Forward: actually quantize (add noise)\n+    /// - Backward: pretend we didn't quantize (gradient flows through)\n+    /// - This is mathematically \"wrong\" but works well in practice!\n+    ///\n+    /// Why it works:\n+    /// - The forward pass sees quantized values (learns to compensate)\n+    /// - The backward pass updates full-precision weights (maintains precision)\n+    /// - The network learns weights that work well when quantized\n+    ///\n+    /// Example:\n+    /// Forward: weight = 0.7234 ‚Üí quantize ‚Üí 0.7333 (closest 4-bit value)\n+    /// Backward: gradient flows as if 0.7234 ‚Üí 0.7234 (identity)\n+    /// Update: 0.7234 - learning_rate * gradient (updates full-precision weight)\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        // The Straight-Through Estimator (STE) means we compute gradients\n+        // as if quantization was the identity function.\n+        // The base implementation handles this correctly because:\n+        // 1. We restored original (full-precision) parameters after Forward\n+        // 2. Backward computes gradients w.r.t. those full-precision parameters\n+        // 3. Gradient flow is not blocked by quantization\n+\n+        // Standard LoRA backward pass\n+        Tensor<T> loraInputGrad = _loraLayer.Backward(outputGradient);\n+        Tensor<T> baseInputGrad = _baseLayer.Backward(outputGradient);\n+\n+        // Sum input gradients\n+        Tensor<T> inputGrad = new Tensor<T>(loraInputGrad.Shape);\n+        for (int i = 0; i < loraInputGrad.Length; i++)\n+        {\n+            inputGrad[i] = NumOps.Add(loraInputGrad[i], baseInputGrad[i]);\n+        }\n+\n+        // Update parameter gradients vector\n+        UpdateParameterGradientsFromLayers();\n+\n+        return inputGrad;\n+    }\n+\n+    /// <summary>\n+    /// Simulates quantization and dequantization using group-wise scaling.\n+    /// </summary>\n+    /// <param name=\"parameters\">Full-precision parameters to quantize.</param>\n+    /// <returns>Parameters after quantize‚Üídequantize cycle (simulating quantization noise).</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Group-wise quantization process:\n+    /// 1. Divide parameters into groups of size GroupSize\n+    /// 2. For each group:\n+    ///    a. Find the maximum absolute value in the group\n+    ///    b. Compute scale = max_abs / (2^bits - 1)\n+    ///    c. Quantize: int_value = round(parameter / scale)\n+    ///    d. Clamp to range [0, 2^bits - 1]\n+    ///    e. Dequantize: parameter = int_value * scale\n+    /// 3. Concatenate all groups back together\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is the core of quantization simulation!\n+    ///\n+    /// Step-by-step example with 4-bit quantization, group size 4:\n+    ///\n+    /// Input: [0.8, 0.6, -0.4, 0.2, 0.9, -0.7, 0.3, -0.5]\n+    ///\n+    /// Group 1: [0.8, 0.6, -0.4, 0.2]\n+    ///   - Max absolute value: 0.8\n+    ///   - Range for 4-bit: 0 to 15 (2^4 - 1 = 15)\n+    ///   - Scale: 0.8 / 15 = 0.0533\n+    ///   - Quantize: [15, 11, -8, 4] (divided by scale, rounded)\n+    ///   - Clamp to [0, 15]: [15, 11, 0, 4] (negative values clamped)\n+    ///   - Dequantize: [0.8, 0.5867, 0.0, 0.2133] (multiply by scale)\n+    ///   - Information lost: -0.4 became 0.0, 0.6 became 0.5867\n+    ///\n+    /// Group 2: [0.9, -0.7, 0.3, -0.5]\n+    ///   - Max absolute value: 0.9\n+    ///   - Scale: 0.9 / 15 = 0.06\n+    ///   - Similar process...\n+    ///\n+    /// The network learns to work with these quantized values during training,\n+    /// so when we actually deploy with 4-bit weights, accuracy is maintained!\n+    /// </para>\n+    /// </remarks>\n+    private Vector<T> QuantizeAndDequantize(Vector<T> parameters)\n+    {\n+        int numParams = parameters.Length;\n+        Vector<T> quantized = new Vector<T>(numParams);\n+\n+        // Calculate number of groups\n+        int numGroups = (numParams + _groupSize - 1) / _groupSize; // Ceiling division\n+\n+        // Maximum value for quantization (e.g., 15 for 4-bit, 255 for 8-bit)\n+        double maxQuantizedValue = Math.Pow(2.0, _quantizationBits) - 1.0;\n+\n+        // Process each group\n+        for (int g = 0; g < numGroups; g++)\n+        {\n+            int groupStart = g * _groupSize;\n+            int groupEnd = Math.Min(groupStart + _groupSize, numParams);\n+            int groupActualSize = groupEnd - groupStart;\n+\n+            // Find maximum absolute value in this group\n+            T maxAbs = NumOps.Zero;\n+            for (int i = groupStart; i < groupEnd; i++)\n+            {\n+                T absValue = NumOps.Abs(parameters[i]);\n+                if (NumOps.GreaterThan(absValue, maxAbs))\n+                {\n+                    maxAbs = absValue;\n+                }\n+            }\n+\n+            // Compute scale factor for this group\n+            // scale = max_abs / (2^bits - 1)\n+            // If max_abs is zero, use a small epsilon to avoid division by zero\n+            if (NumOps.Equals(maxAbs, NumOps.Zero))\n+            {\n+                maxAbs = NumOps.FromDouble(1e-8);\n+            }\n+\n+            T scale = NumOps.Divide(maxAbs, NumOps.FromDouble(maxQuantizedValue));\n+\n+            // Quantize and dequantize each parameter in the group\n+            for (int i = groupStart; i < groupEnd; i++)\n+            {\n+                // Quantize: int_value = round(param / scale)\n+                T normalized = NumOps.Divide(parameters[i], scale);\n+                double normalizedDouble = Convert.ToDouble(normalized);\n+                double quantizedDouble = Math.Round(normalizedDouble);\n+\n+                // Clamp to valid range [0, maxQuantizedValue] for unsigned\n+                // Or [-maxQuantizedValue/2, maxQuantizedValue/2] for signed\n+                // Using unsigned for simplicity (common in QLoRA)\n+                quantizedDouble = Math.Max(0.0, Math.Min(maxQuantizedValue, quantizedDouble));\n+","path":"src/LoRA/Adapters/QALoRAAdapter.cs","commit_id":"489180133c7a9db9460c84a6174f60284717b952","original_commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Use a signed quantization range; current clamp zeros every negative weight.**\n\n`quantizedDouble` is clamped to `[0, max]`, so any negative LoRA weight becomes zero after the quantize‚Üídequantize cycle. That completely breaks the quantization-aware simulation (half of the weights vanish), so gradients don‚Äôt reflect deployment behaviour. Switch to a symmetric range (e.g., `[-(2^(bits-1)-1), +(2^(bits-1)-1)]`) and scale accordingly.\n\n```diff\n-        // Maximum value for quantization (e.g., 15 for 4-bit, 255 for 8-bit)\n-        double maxQuantizedValue = Math.Pow(2.0, _quantizationBits) - 1.0;\n+        double maxQuantizedValue = Math.Pow(2.0, _quantizationBits - 1) - 1.0;\n+        double minQuantizedValue = -maxQuantizedValue;\n...\n-            T scale = NumOps.Divide(maxAbs, NumOps.FromDouble(maxQuantizedValue));\n+            T scale = NumOps.Divide(maxAbs, NumOps.FromDouble(maxQuantizedValue));\n...\n-                // Clamp to valid range [0, maxQuantizedValue] for unsigned\n-                // Or [-maxQuantizedValue/2, maxQuantizedValue/2] for signed\n-                // Using unsigned for simplicity (common in QLoRA)\n-                quantizedDouble = Math.Max(0.0, Math.Min(maxQuantizedValue, quantizedDouble));\n+                quantizedDouble = Math.Max(minQuantizedValue, Math.Min(maxQuantizedValue, quantizedDouble));\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n            T scale = NumOps.Divide(maxAbs, NumOps.FromDouble(maxQuantizedValue));\n\n            // Quantize and dequantize each parameter in the group\n            for (int i = groupStart; i < groupEnd; i++)\n            {\n                // Quantize: int_value = round(param / scale)\n                T normalized = NumOps.Divide(parameters[i], scale);\n                double normalizedDouble = Convert.ToDouble(normalized);\n                double quantizedDouble = Math.Round(normalizedDouble);\n\n                // Clamp to valid range [-maxQuantizedValue, maxQuantizedValue] for signed\n                quantizedDouble = Math.Max(minQuantizedValue, Math.Min(maxQuantizedValue, quantizedDouble));\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/LoRA/Adapters/QALoRAAdapter.cs around lines 442 to 456, the quantization\nclamp currently forces quantizedDouble into [0, maxQuantizedValue], which zeros\nall negative weights; change to symmetric signed clamping by computing a\nsignedMax = (2^(bits-1) - 1) (or equivalent for the generic type) and clamp\nquantizedDouble to [-signedMax, +signedMax]; ensure any scale computation uses\nthat signed range (i.e., scale = maxAbs / signedMax) so quantize‚Üídequantize\npreserves negative values and matches signed QLoRA semantics.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:42:02Z","updated_at":"2025-11-02T02:42:06Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484118467","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118467"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484118467"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118467/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":442,"original_start_line":442,"start_side":"RIGHT","line":456,"original_line":456,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":456,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118472","pull_request_review_id":3408015014,"id":2484118472,"node_id":"PRRC_kwDOKSXUF86UEKPI","diff_hunk":"@@ -0,0 +1,821 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// QLoRA (Quantized LoRA) adapter for parameter-efficient fine-tuning with 4-bit quantized base weights.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// QLoRA extends the LoRA (Low-Rank Adaptation) technique by quantizing the base layer's weights\n+/// to 4-bit precision while keeping the LoRA adapter matrices (A and B) in full precision.\n+/// This achieves dramatic memory savings (typically 4x reduction) while maintaining training quality\n+/// comparable to full 16-bit fine-tuning.\n+/// </para>\n+/// <para>\n+/// <b>Key Features:</b>\n+/// - Base layer weights stored in 4-bit precision (INT4 or NF4)\n+/// - LoRA matrices (A and B) remain in full precision for accurate gradient updates\n+/// - Double quantization for constant quantization parameters (further memory savings)\n+/// - Paged optimizers support for handling memory spikes during training\n+/// - Dequantization happens on-the-fly during forward pass\n+/// </para>\n+/// <para>\n+/// <b>Memory Savings:</b>\n+/// For a typical transformer layer with 1000x1000 weights:\n+/// - Standard 16-bit: 2MB for weights\n+/// - QLoRA 4-bit base: 0.5MB for base weights + full precision LoRA (e.g., 32KB for rank 8)\n+/// - Total savings: ~75% memory reduction on base weights\n+/// </para>\n+/// <para>\n+/// <b>Quantization Types:</b>\n+/// - INT4: Uniform 4-bit integer quantization (-8 to 7)\n+/// - NF4 (4-bit Normal Float): Information-theoretically optimal for normally distributed weights\n+/// </para>\n+/// <para>\n+/// <b>For Beginners:</b> QLoRA is an advanced technique that makes fine-tuning large models\n+/// even more memory-efficient than standard LoRA. Here's how it works:\n+///\n+/// Imagine you have a huge model with millions of parameters:\n+/// - Standard LoRA: Freezes the base model, trains small adapters (huge memory savings)\n+/// - QLoRA: Does the same BUT also compresses the base model to 4-bit (even more savings!)\n+///\n+/// Think of it like storing a high-resolution image:\n+/// - Original model: Full 16-bit floating point (2 bytes per number)\n+/// - QLoRA base: Compressed to 4-bit (0.5 bytes per number)\n+/// - LoRA adapters: Still full precision (for accurate learning)\n+///\n+/// The result: You can fine-tune models 4x larger on the same hardware, or use 4x less GPU memory!\n+///\n+/// <b>When to use QLoRA vs Standard LoRA:</b>\n+/// - Use QLoRA when: GPU memory is very limited, model is huge, inference speed is critical\n+/// - Use Standard LoRA when: Memory is not a constraint, maximum accuracy is needed\n+/// - Both achieve similar quality in practice, QLoRA just uses less memory\n+///\n+/// <b>Trade-offs:</b>\n+/// - Pros: 75% less memory, same performance as 16-bit LoRA, faster inference after merging\n+/// - Cons: Slightly slower forward pass (dequantization overhead), more complex implementation\n+/// </para>\n+/// <para>\n+/// <b>Research Background:</b>\n+/// QLoRA was introduced in \"QLoRA: Efficient Finetuning of Quantized LLMs\" (Dettmers et al., 2023).\n+/// It enables fine-tuning of 65B parameter models on a single 48GB GPU by combining:\n+/// 1. 4-bit NormalFloat (NF4) quantization optimized for normally distributed weights\n+/// 2. Double quantization to reduce memory footprint of quantization constants\n+/// 3. Paged optimizers to handle memory spikes during gradient checkpointing\n+/// </para>\n+/// </remarks>\n+public class QLoRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Specifies the type of 4-bit quantization to use for base layer weights.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This determines how we compress numbers from full precision to 4-bit.\n+    /// Think of it like choosing between different image compression algorithms - each has trade-offs.\n+    /// </para>\n+    /// </remarks>\n+    public enum QuantizationType\n+    {\n+        /// <summary>\n+        /// 4-bit integer quantization with uniform spacing (-8 to 7).\n+        /// </summary>\n+        /// <remarks>\n+        /// Simple linear quantization mapping 16 values uniformly across the range.\n+        /// Fast and straightforward, but not optimal for normally distributed weights.\n+        /// </remarks>\n+        INT4,\n+\n+        /// <summary>\n+        /// 4-bit Normal Float quantization optimized for normally distributed weights.\n+        /// </summary>\n+        /// <remarks>\n+        /// Uses information-theoretically optimal quantization levels for normal distributions.\n+        /// Provides better accuracy for typical neural network weights at the same bit width.\n+        /// This is the recommended and default quantization type for QLoRA.\n+        /// </remarks>\n+        NF4\n+    }\n+\n+    /// <summary>\n+    /// The type of quantization used for base layer weights.\n+    /// </summary>\n+    private readonly QuantizationType _quantizationType;\n+\n+    /// <summary>\n+    /// Whether to use double quantization for quantization constants.\n+    /// </summary>\n+    /// <remarks>\n+    /// Double quantization quantizes the quantization constants themselves (e.g., scale factors)\n+    /// to save additional memory. This provides ~3-5% extra memory savings with negligible quality impact.\n+    /// </remarks>\n+    private readonly bool _useDoubleQuantization;\n+\n+    /// <summary>\n+    /// The block size for quantization (number of values sharing the same quantization parameters).\n+    /// </summary>\n+    /// <remarks>\n+    /// Smaller blocks provide finer-grained quantization (better accuracy, more memory for constants).\n+    /// Larger blocks use less memory for constants but may lose precision.\n+    /// Default: 64 (good balance between accuracy and memory).\n+    /// </remarks>\n+    private readonly int _quantizationBlockSize;\n+\n+    /// <summary>\n+    /// Quantized base layer weights stored as 4-bit values.\n+    /// </summary>\n+    /// <remarks>\n+    /// Stored as packed bytes where each byte contains two 4-bit values.\n+    /// Shape matches the base layer's weight matrix.\n+    /// </remarks>\n+    private byte[]? _quantizedWeights;\n+\n+    /// <summary>\n+    /// Scale factors for dequantization (one per quantization block).\n+    /// </summary>\n+    /// <remarks>\n+    /// These scaling factors are used to map 4-bit quantized values back to full precision.\n+    /// For double quantization, these are themselves quantized to save memory.\n+    /// </remarks>\n+    private T[]? _quantizationScales;\n+\n+    /// <summary>\n+    /// Zero points for asymmetric quantization (one per quantization block).\n+    /// </summary>\n+    /// <remarks>\n+    /// Used for asymmetric quantization where the quantization range doesn't center on zero.\n+    /// Optional - set to null for symmetric quantization.\n+    /// </remarks>\n+    private T[]? _quantizationZeroPoints;\n+\n+    /// <summary>\n+    /// Cached dequantized weights for forward pass.\n+    /// </summary>\n+    /// <remarks>\n+    /// Weights are dequantized at the start of forward pass and cached to avoid repeated dequantization.\n+    /// Cleared after backward pass to save memory.\n+    /// </remarks>\n+    private Matrix<T>? _dequantizedWeights;\n+\n+    /// <summary>\n+    /// NF4 quantization lookup table (16 values optimized for normal distribution).\n+    /// </summary>\n+    /// <remarks>\n+    /// These values are derived from optimal quantization for a standard normal distribution.\n+    /// They are NOT evenly spaced - more values near zero where probability mass is concentrated.\n+    /// </remarks>\n+    private static readonly double[] _nf4Table = new double[]\n+    {\n+        -1.0,\n+        -0.6961928009986877,\n+        -0.5250730514526367,\n+        -0.39491748809814453,\n+        -0.28444138169288635,\n+        -0.18477343022823334,\n+        -0.09105003625154495,\n+        0.0,\n+        0.07958029955625534,\n+        0.16093020141124725,\n+        0.24611230194568634,\n+        0.33791524171829224,\n+        0.44070982933044434,\n+        0.5626170039176941,\n+        0.7229568362236023,\n+        1.0\n+    };\n+\n+    /// <summary>\n+    /// Gets the quantization type used for base layer weights.\n+    /// </summary>\n+    public QuantizationType Quantization => _quantizationType;\n+\n+    /// <summary>\n+    /// Gets whether double quantization is enabled.\n+    /// </summary>\n+    public bool UsesDoubleQuantization => _useDoubleQuantization;\n+\n+    /// <summary>\n+    /// Gets the quantization block size.\n+    /// </summary>\n+    public int BlockSize => _quantizationBlockSize;\n+\n+    /// <summary>\n+    /// Initializes a new QLoRA adapter wrapping an existing Dense or FullyConnected layer.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The Dense or FullyConnected layer to adapt with QLoRA.</param>\n+    /// <param name=\"rank\">The rank of the LoRA decomposition.</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"quantizationType\">The type of 4-bit quantization to use (default: NF4).</param>\n+    /// <param name=\"useDoubleQuantization\">Whether to use double quantization for constants (default: true).</param>\n+    /// <param name=\"quantizationBlockSize\">The block size for quantization (default: 64).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training (default: true, recommended for QLoRA).</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when the base layer doesn't have 1D input/output shapes or when block size is invalid.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// The constructor quantizes the base layer's weights immediately to save memory.\n+    /// LoRA matrices are initialized normally and remain in full precision.\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This creates a QLoRA adapter that wraps your existing layer.\n+    ///\n+    /// Parameters explained:\n+    /// - baseLayer: The layer you want to compress and adapt (e.g., a Dense layer)\n+    /// - rank: How many parameters for the LoRA adapter (lower = more efficient)\n+    /// - alpha: How strong the LoRA corrections are\n+    /// - quantizationType: NF4 (recommended) or INT4 (simpler but less accurate)\n+    /// - useDoubleQuantization: true (recommended) saves extra 3-5% memory\n+    /// - quantizationBlockSize: 64 (recommended) balances accuracy and memory\n+    /// - freezeBaseLayer: true (recommended) - only train the LoRA adapter, not the base weights\n+    ///\n+    /// After construction, the base layer's weights are immediately compressed to 4-bit,\n+    /// freeing up 75% of the memory they were using!\n+    /// </para>\n+    /// </remarks>\n+    public QLoRAAdapter(\n+        ILayer<T> baseLayer,\n+        int rank,\n+        double alpha = -1,\n+        QuantizationType quantizationType = QuantizationType.NF4,\n+        bool useDoubleQuantization = true,\n+        int quantizationBlockSize = 64,\n+        bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        // Validate base layer has single-dimensional input/output (specific to Dense layers)\n+        if (baseLayer.GetInputShape().Length != 1 || baseLayer.GetOutputShape().Length != 1)\n+        {\n+            throw new ArgumentException(\"QLoRAAdapter only supports layers with 1D input/output shapes (Dense/FullyConnected layers)\", nameof(baseLayer));\n+        }\n+\n+        if (quantizationBlockSize <= 0)\n+        {\n+            throw new ArgumentException(\"Quantization block size must be positive\", nameof(quantizationBlockSize));\n+        }\n+\n+        _quantizationType = quantizationType;\n+        _useDoubleQuantization = useDoubleQuantization;\n+        _quantizationBlockSize = quantizationBlockSize;\n+\n+        // Quantize base layer weights immediately to save memory\n+        QuantizeBaseLayerWeights();\n+    }\n+\n+    /// <summary>\n+    /// Quantizes the base layer's weights to 4-bit precision.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This method extracts the weight matrix from the base layer and quantizes it\n+    /// using the specified quantization type. The quantized weights and quantization\n+    /// parameters (scales, zero points) are stored for later dequantization.\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This is where the magic happens - we compress the weights\n+    /// from full precision (2 bytes per value) to 4-bit (0.5 bytes per value).\n+    ///\n+    /// The process:\n+    /// 1. Get the full-precision weights from the base layer\n+    /// 2. Split them into blocks (e.g., 64 values per block)\n+    /// 3. For each block, find the best way to map values to 4-bit\n+    /// 4. Store the compressed values and the mapping parameters\n+    /// </para>\n+    /// </remarks>\n+    private void QuantizeBaseLayerWeights()\n+    {\n+        // Get base layer parameters\n+        Vector<T> baseParams = _baseLayer.GetParameters();\n+\n+        // For Dense layers, parameters are stored as [weights..., biases...]\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        int weightCount = inputSize * outputSize;\n+\n+        // Extract weights (skip biases)\n+        T[] weights = new T[weightCount];\n+        for (int i = 0; i < weightCount; i++)\n+        {\n+            weights[i] = baseParams[i];\n+        }\n+\n+        // Quantize weights in blocks\n+        int numBlocks = (weightCount + _quantizationBlockSize - 1) / _quantizationBlockSize;\n+        _quantizedWeights = new byte[(weightCount + 1) / 2]; // 2 values per byte\n+        _quantizationScales = new T[numBlocks];\n+        _quantizationZeroPoints = new T[numBlocks];\n+\n+        for (int blockIdx = 0; blockIdx < numBlocks; blockIdx++)\n+        {\n+            int blockStart = blockIdx * _quantizationBlockSize;\n+            int blockEnd = Math.Min(blockStart + _quantizationBlockSize, weightCount);\n+            int blockLength = blockEnd - blockStart;\n+\n+            // Find min/max for this block\n+            T minVal = weights[blockStart];\n+            T maxVal = weights[blockStart];\n+            for (int i = blockStart + 1; i < blockEnd; i++)\n+            {\n+                if (NumOps.LessThan(weights[i], minVal))\n+                    minVal = weights[i];\n+                if (NumOps.GreaterThan(weights[i], maxVal))\n+                    maxVal = weights[i];\n+            }\n+\n+            // Compute scale and zero point\n+            T range = NumOps.Subtract(maxVal, minVal);\n+            T scale = NumOps.Divide(range, NumOps.FromDouble(15.0)); // 4-bit has 16 levels (0-15)\n+            T zeroPoint = minVal;\n+\n+            _quantizationScales[blockIdx] = scale;\n+            _quantizationZeroPoints[blockIdx] = zeroPoint;\n+\n+            // Quantize values in this block","path":"src/LoRA/Adapters/QLoRAAdapter.cs","commit_id":"489180133c7a9db9460c84a6174f60284717b952","original_commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Prevent divide-by-zero when quantising constant blocks.**\n\nIf a block‚Äôs weights are all identical, `range` becomes zero and `scale = range / 15` collapses to zero, so every later division by `scale` throws at runtime. Constant blocks (e.g. zero-initialised weights) are common, so this blows up immediately. Please fall back to a small epsilon-based scale when the range is zero.\n\n```diff\n-            T range = NumOps.Subtract(maxVal, minVal);\n-            T scale = NumOps.Divide(range, NumOps.FromDouble(15.0)); // 4-bit has 16 levels (0-15)\n+            T range = NumOps.Subtract(maxVal, minVal);\n+            T scale;\n+            if (NumOps.Equals(range, NumOps.Zero))\n+            {\n+                scale = NumOps.FromDouble(1e-8);\n+            }\n+            else\n+            {\n+                scale = NumOps.Divide(range, NumOps.FromDouble(15.0)); // 4-bit has 16 levels (0-15)\n+            }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n            // Compute scale and zero point\n            T range = NumOps.Subtract(maxVal, minVal);\n            T scale;\n            if (NumOps.Equals(range, NumOps.Zero))\n            {\n                scale = NumOps.FromDouble(1e-8);\n            }\n            else\n            {\n                scale = NumOps.Divide(range, NumOps.FromDouble(15.0)); // 4-bit has 16 levels (0-15)\n            }\n            T zeroPoint = minVal;\n\n            _quantizationScales[blockIdx] = scale;\n            _quantizationZeroPoints[blockIdx] = zeroPoint;\n\n            // Quantize values in this block\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/LoRA/Adapters/QLoRAAdapter.cs around lines 325 to 333, computing scale as\nrange/15 can produce zero for constant-weight blocks and later cause\ndivide-by-zero; detect when range is zero (or below a tiny threshold) and\nreplace the computed scale with a small epsilon value (e.g.\nNumOps.FromDouble(1e-8) or similar), keep zeroPoint as minVal, and store that\nnon-zero scale into _quantizationScales[blockIdx] so subsequent divisions are\nsafe.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:42:03Z","updated_at":"2025-11-02T02:42:06Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484118472","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118472"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484118472"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118472/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":325,"original_start_line":325,"start_side":"RIGHT","line":333,"original_line":333,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":333,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118476","pull_request_review_id":3408015014,"id":2484118476,"node_id":"PRRC_kwDOKSXUF86UEKPM","diff_hunk":"@@ -0,0 +1,627 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// Restart LoRA (ReLoRA) adapter that periodically merges and restarts LoRA training for continual learning.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// ReLoRA addresses the challenge of continual learning and long-running training by periodically:\n+/// 1. Merging the LoRA weights into the base layer (accumulating the adaptation)\n+/// 2. Resetting the LoRA matrices to restart training fresh\n+/// 3. Continuing training with a clean slate while preserving previous learning\n+/// </para>\n+/// <para>\n+/// This approach:\n+/// - Prevents catastrophic forgetting by accumulating adaptations into the base layer\n+/// - Allows continuous adaptation to new data without losing old knowledge\n+/// - Maintains parameter efficiency by resetting LoRA to small matrices\n+/// - Enables training on continuously evolving data streams\n+/// </para>\n+/// <para><b>For Beginners:</b> ReLoRA is like having multiple rounds of LoRA training.\n+///\n+/// Imagine you're fine-tuning a model on data that keeps changing:\n+/// - Round 1: Train LoRA on dataset A for 1000 steps\n+/// - Merge: Add the learned changes into the base model\n+/// - Restart: Reset LoRA matrices and train on dataset B for 1000 steps\n+/// - Merge: Add these new changes to the (already updated) base model\n+/// - Repeat...\n+///\n+/// Benefits:\n+/// - Continual learning: Can keep learning from new data indefinitely\n+/// - No catastrophic forgetting: Old knowledge is preserved in the base layer\n+/// - Parameter efficient: LoRA matrices stay small even after many restarts\n+/// - Flexible: Can adapt to distribution shifts and new tasks\n+///\n+/// How it works:\n+/// 1. Train normally with LoRA for N steps (restart interval)\n+/// 2. At step N: Merge LoRA weights ‚Üí AccumulatedWeight += LoRA\n+/// 3. Reset LoRA matrices to zero (fresh start)\n+/// 4. Continue training for another N steps\n+/// 5. Repeat indefinitely\n+///\n+/// Use cases:\n+/// - Training on streaming data (news articles, user behavior, etc.)\n+/// - Adapting to distribution shifts over time\n+/// - Long-running training sessions that need checkpoints\n+/// - Multi-task learning with periodic task switches\n+///\n+/// Reference: \"ReLoRA: High-Rank Training Through Low-Rank Updates\" (2023)\n+/// https://arxiv.org/abs/2307.05695\n+/// </para>\n+/// </remarks>\n+public class ReLoRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Number of training steps between restart operations.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// The restart interval determines how frequently the LoRA weights are merged and reset.\n+    /// Typical values:\n+    /// - Short interval (100-500): Frequent restarts, better for rapidly changing data\n+    /// - Medium interval (1000-2000): Balance between stability and adaptation\n+    /// - Long interval (5000+): Fewer restarts, more thorough learning per cycle\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is how many training steps to run before merging and restarting.\n+    /// Think of it as the length of each training \"session\" before taking a checkpoint.\n+    /// </para>\n+    /// </remarks>\n+    private readonly int _restartInterval;\n+\n+    /// <summary>\n+    /// Current training step counter.\n+    /// </summary>\n+    /// <remarks>\n+    /// This counts up from 0 to restartInterval, then resets to 0 after each restart.\n+    /// </remarks>\n+    private int _currentStep;\n+\n+    /// <summary>\n+    /// Accumulated weight changes from all previous restart cycles.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This matrix accumulates all LoRA adaptations across restart cycles:\n+    /// AccumulatedWeight = sum of all (A * B * scaling) across all cycles.\n+    /// It represents the total learned adaptation that gets added to the base layer.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is like a running total of all the changes made across\n+    /// all restart cycles. Each time we restart, we add the current LoRA changes to this total.\n+    /// This is how we prevent forgetting - all previous learning is saved here.\n+    /// </para>\n+    /// </remarks>\n+    private Matrix<T> _accumulatedWeight;\n+\n+    /// <summary>\n+    /// Total number of restarts that have occurred.\n+    /// </summary>\n+    private int _restartCount;\n+\n+    /// <summary>\n+    /// Whether to use warmup after each restart.\n+    /// </summary>\n+    /// <remarks>\n+    /// When true, the first few steps after restart use a reduced learning rate to stabilize training.\n+    /// </remarks>\n+    private readonly bool _useWarmup;\n+\n+    /// <summary>\n+    /// Number of warmup steps to use after each restart.\n+    /// </summary>\n+    private readonly int _warmupSteps;\n+\n+    /// <summary>\n+    /// Whether to freeze the base layer during training (typical for LoRA).\n+    /// </summary>\n+    private readonly bool _freezeBase;\n+\n+    /// <summary>\n+    /// Gets the number of training steps between restarts.\n+    /// </summary>\n+    public int RestartInterval => _restartInterval;\n+\n+    /// <summary>\n+    /// Gets the current step within the current restart cycle.\n+    /// </summary>\n+    public int CurrentStep => _currentStep;\n+\n+    /// <summary>\n+    /// Gets the total number of restarts that have occurred.\n+    /// </summary>\n+    public int RestartCount => _restartCount;\n+\n+    /// <summary>\n+    /// Gets a copy of the accumulated weight matrix.\n+    /// </summary>\n+    public Matrix<T> GetAccumulatedWeight() => _accumulatedWeight.Clone();\n+\n+    /// <summary>\n+    /// Initializes a new ReLoRA adapter with restart-based continual learning.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with ReLoRA.</param>\n+    /// <param name=\"rank\">The rank of the LoRA decomposition.</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"restartInterval\">Number of steps between restart operations (default: 1000).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training (default: true).</param>\n+    /// <param name=\"useWarmup\">Whether to use warmup after restarts (default: true).</param>\n+    /// <param name=\"warmupSteps\">Number of warmup steps after restart (default: 10).</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when restartInterval is invalid.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This creates a ReLoRA adapter for continual learning.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to adapt continuously\n+    /// - rank: Size of the LoRA matrices (lower = more efficient)\n+    /// - alpha: Strength of the LoRA adaptation\n+    /// - restartInterval: How often to merge and restart (in training steps)\n+    /// - freezeBaseLayer: Lock the base layer weights (typical for LoRA)\n+    /// - useWarmup: Use reduced learning rate after restarts (helps stability)\n+    /// - warmupSteps: How many steps to warm up for\n+    ///\n+    /// The adapter will automatically handle merging and restarting at the specified interval.\n+    /// You just train normally, and it takes care of the restart logic.\n+    /// </para>\n+    /// </remarks>\n+    public ReLoRAAdapter(\n+        ILayer<T> baseLayer,\n+        int rank,\n+        double alpha = -1,\n+        int restartInterval = 1000,\n+        bool freezeBaseLayer = true,\n+        bool useWarmup = true,\n+        int warmupSteps = 10)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        if (restartInterval <= 0)\n+        {\n+            throw new ArgumentException(\"Restart interval must be positive\", nameof(restartInterval));\n+        }\n+\n+        if (warmupSteps < 0)\n+        {\n+            throw new ArgumentException(\"Warmup steps cannot be negative\", nameof(warmupSteps));\n+        }\n+\n+        _restartInterval = restartInterval;\n+        _currentStep = 0;\n+        _restartCount = 0;\n+        _freezeBase = freezeBaseLayer;\n+        _useWarmup = useWarmup;\n+        _warmupSteps = warmupSteps;\n+\n+        // Initialize accumulated weight matrix to zero\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        _accumulatedWeight = new Matrix<T>(outputSize, inputSize);\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                _accumulatedWeight[i, j] = NumOps.Zero;\n+            }\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Checks if a restart should be performed based on the current step count.\n+    /// </summary>\n+    /// <returns>True if current step has reached the restart interval.</returns>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This checks if it's time for a restart.\n+    /// Returns true when we've completed a full training cycle (reached the interval).\n+    /// </para>\n+    /// </remarks>\n+    public bool ShouldRestart()\n+    {\n+        return _currentStep >= _restartInterval;\n+    }\n+\n+    /// <summary>\n+    /// Performs the restart operation: merges current LoRA weights and reinitializes.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// The restart process:\n+    /// 1. Merge current LoRA weights: W_accumulated += W_A * W_B * scaling\n+    /// 2. Reinitialize LoRA matrices: A gets new random values, B reset to zero\n+    /// 3. Reset step counter to 0\n+    /// 4. Increment restart count\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This performs the \"checkpoint and restart\" operation.\n+    ///\n+    /// Steps:\n+    /// 1. Save progress: Add current LoRA changes to the accumulated total\n+    /// 2. Fresh start: Reset LoRA matrices (A gets new random values, B starts at zero)\n+    /// 3. Reset counter: Start counting steps from 0 again\n+    ///\n+    /// After this, training continues normally for another cycle.\n+    /// The accumulated changes are preserved and will be included in the final output.\n+    /// </para>\n+    /// </remarks>\n+    public void RestartLoRA()\n+    {\n+        // Get the current LoRA weight contribution\n+        Matrix<T> loraWeights = _loraLayer.MergeWeights();\n+\n+        // Accumulate the LoRA weights\n+        for (int i = 0; i < _accumulatedWeight.Rows; i++)\n+        {\n+            for (int j = 0; j < _accumulatedWeight.Columns; j++)\n+            {\n+                _accumulatedWeight[i, j] = NumOps.Add(_accumulatedWeight[i, j], loraWeights[i, j]);\n+            }\n+        }\n+\n+        // Reinitialize LoRA matrices\n+        Matrix<T> matrixA = _loraLayer.GetMatrixA();\n+        Matrix<T> matrixB = _loraLayer.GetMatrixB();\n+\n+        // Reinitialize A with random values (same as initial LoRA initialization)\n+        T stddev = NumOps.Sqrt(NumOps.Divide(NumOps.One, NumOps.FromDouble(_loraLayer.Rank)));\n+        for (int i = 0; i < matrixA.Rows; i++)\n+        {\n+            for (int j = 0; j < matrixA.Columns; j++)\n+            {\n+                // Box-Muller transform for Gaussian random numbers\n+                double u1 = Random.NextDouble();\n+                double u2 = Random.NextDouble();\n+                double randStdNormal = Math.Sqrt(-2.0 * Math.Log(u1)) * Math.Sin(2.0 * Math.PI * u2);\n+                matrixA[i, j] = NumOps.Multiply(NumOps.FromDouble(randStdNormal), stddev);\n+            }","path":"src/LoRA/Adapters/ReLoRAAdapter.cs","commit_id":"489180133c7a9db9460c84a6174f60284717b952","original_commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Instantiate an RNG instead of calling `Random.NextDouble()` statically**  \n`Random.NextDouble()` is an instance member; calling it as a static method does not compile. `RestartLoRA()` currently fails to build. Please hold a `Random` instance (or use `Random.Shared`) and invoke `NextDouble()` on it before performing the Box‚ÄìMuller transform.  \n\n```diff\n+    private static readonly Random _rng = new Random();\n@@\n-                double u1 = Random.NextDouble();\n-                double u2 = Random.NextDouble();\n+                double u1 = _rng.NextDouble();\n+                double u2 = _rng.NextDouble();\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/LoRA/Adapters/ReLoRAAdapter.cs around lines 270 to 274, the code\nincorrectly calls Random.NextDouble() as if it were static; replace this with an\ninstance call by holding a Random instance (e.g. use Random.Shared or a private\nRandom field/variable) and call rng.NextDouble() for u1 and u2 before performing\nthe Box‚ÄìMuller transform; ensure the rng instance is accessible in this method\n(or created locally) and remove the static call so the code compiles.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:42:03Z","updated_at":"2025-11-02T02:42:06Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484118476","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118476"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484118476"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118476/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":270,"original_start_line":270,"start_side":"RIGHT","line":274,"original_line":274,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":274,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118482","pull_request_review_id":3408015014,"id":2484118482,"node_id":"PRRC_kwDOKSXUF86UEKPS","diff_hunk":"@@ -0,0 +1,910 @@\n+using AiDotNet.Interfaces;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// S-LoRA adapter for scalable serving of thousands of concurrent LoRA adapters.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// S-LoRA (Scalable LoRA) is a system designed for efficient serving of many LoRA adapters simultaneously.\n+/// Published in November 2023, it addresses the challenge of deploying thousands of task-specific LoRA adapters\n+/// in production environments with limited GPU memory.\n+/// </para>\n+/// <para><b>For Beginners:</b> S-LoRA solves a real-world problem in production AI systems.\n+///\n+/// The problem:\n+/// - You have a large base model (like GPT or LLaMA)\n+/// - You want to serve thousands of different LoRA adapters (one per customer, task, or use case)\n+/// - Each adapter is small (few MB), but thousands of them won't fit in GPU memory\n+/// - Naive approaches either: load one adapter at a time (slow) or reserve memory for all (wasteful)\n+///\n+/// S-LoRA's solution:\n+/// - Unified memory pool: Dynamically manage adapter weights and cache together\n+/// - Batched computation: Process multiple adapters in parallel efficiently\n+/// - Adapter clustering: Group adapters by rank for optimized computation\n+/// - On-demand loading: Fetch adapters from CPU to GPU memory only when needed\n+///\n+/// Key features implemented:\n+/// 1. **Unified Memory Pool**: Single pool for adapter weights (no pre-allocation waste)\n+/// 2. **Adapter Clustering**: Group adapters by rank for batched computation\n+/// 3. **Dynamic Loading**: Load adapters on-demand, evict when not needed\n+/// 4. **Batched Forward Pass**: Process multiple requests with different adapters simultaneously\n+/// 5. **Memory Efficiency**: Serve 100x more adapters than naive approaches\n+///\n+/// Research Paper Reference:\n+/// \"S-LoRA: Serving Thousands of Concurrent LoRA Adapters\"\n+/// Ying Sheng, Shiyi Cao, et al. (November 2023)\n+/// arXiv:2311.03285\n+///\n+/// Performance (from paper):\n+/// - Throughput: 4x improvement over vLLM, 30x over HuggingFace PEFT\n+/// - Adapter capacity: 2,000+ concurrent adapters on single server\n+/// - Memory efficiency: 75-90% GPU memory utilization\n+/// - Scalability: Superlinear throughput scaling with more GPUs\n+///\n+/// Example usage:\n+/// ```csharp\n+/// // Create S-LoRA serving system for base layer\n+/// var sloraAdapter = new SLoRAAdapter&lt;double&gt;(baseLayer, rank: 8);\n+///\n+/// // Register multiple adapters for different tasks\n+/// sloraAdapter.RegisterAdapter(\"customer_1\", adapter1);\n+/// sloraAdapter.RegisterAdapter(\"customer_2\", adapter2);\n+/// sloraAdapter.RegisterAdapter(\"task_classification\", adapter3);\n+///\n+/// // Process batched requests efficiently\n+/// var outputs = sloraAdapter.BatchForward(inputs, adapterIds);\n+/// ```\n+///\n+/// When to use S-LoRA:\n+/// - Serving multiple LoRA adapters in production\n+/// - Multi-tenant AI systems (one adapter per tenant)\n+/// - Task-specific fine-tuning at scale\n+/// - Limited GPU memory but many adapters\n+/// - Need high throughput with many concurrent users\n+///\n+/// Differences from standard LoRA:\n+/// - Standard LoRA: Single adapter, simple forward/backward pass\n+/// - S-LoRA: Multiple adapters, optimized for concurrent serving, memory pooling\n+/// </para>\n+/// </remarks>\n+public class SLoRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Represents an adapter entry in the memory pool.\n+    /// </summary>\n+    private class AdapterEntry\n+    {\n+        /// <summary>\n+        /// The adapter's unique identifier.\n+        /// </summary>\n+        public string Id { get; set; }\n+\n+        /// <summary>\n+        /// The LoRA layer for this adapter.\n+        /// </summary>\n+        public LoRALayer<T> Layer { get; set; }\n+\n+        /// <summary>\n+        /// The rank of this adapter.\n+        /// </summary>\n+        public int Rank { get; set; }\n+\n+        /// <summary>\n+        /// Whether this adapter is currently loaded in \"GPU memory\" (in-memory cache).\n+        /// </summary>\n+        public bool IsLoaded { get; set; }\n+\n+        /// <summary>\n+        /// Last access timestamp for LRU eviction.\n+        /// </summary>\n+        public long LastAccess { get; set; }\n+\n+        /// <summary>\n+        /// Reference count for active requests using this adapter.\n+        /// </summary>\n+        public int ReferenceCount { get; set; }\n+\n+        /// <summary>\n+        /// Initializes a new adapter entry.\n+        /// </summary>\n+        public AdapterEntry(string id, LoRALayer<T> layer, int rank)\n+        {\n+            Id = id ?? string.Empty;\n+            Layer = layer;\n+            Rank = rank;\n+            IsLoaded = false;\n+            LastAccess = 0;\n+            ReferenceCount = 0;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Unified memory pool storing all registered adapters.\n+    /// </summary>\n+    /// <remarks>\n+    /// This simulates S-LoRA's unified memory pool where all adapters reside in CPU memory\n+    /// and are dynamically loaded to GPU memory based on demand.\n+    /// </remarks>\n+    private readonly Dictionary<string, AdapterEntry> _adapterPool;\n+\n+    /// <summary>\n+    /// Adapters currently loaded in \"GPU memory\" (in-memory cache).\n+    /// </summary>\n+    private readonly Dictionary<string, AdapterEntry> _loadedAdapters;\n+\n+    /// <summary>\n+    /// Adapters clustered by rank for efficient batched computation.\n+    /// </summary>\n+    private readonly Dictionary<int, List<string>> _rankClusters;\n+\n+    /// <summary>\n+    /// Maximum number of adapters that can be loaded simultaneously (simulates GPU memory limit).\n+    /// </summary>\n+    private readonly int _maxLoadedAdapters;\n+\n+    /// <summary>\n+    /// Current timestamp for LRU eviction policy.\n+    /// </summary>\n+    private long _timestamp;\n+\n+    /// <summary>\n+    /// Gets the total number of registered adapters in the pool.\n+    /// </summary>\n+    /// <remarks>\n+    /// This represents all adapters in the system, including those not currently loaded.\n+    /// S-LoRA can serve thousands of adapters from a unified pool.\n+    /// </remarks>\n+    public int TotalAdapterCount => _adapterPool.Count;\n+\n+    /// <summary>\n+    /// Gets the number of adapters currently loaded in memory.\n+    /// </summary>\n+    /// <remarks>\n+    /// This represents the \"hot\" adapters actively being used or cached.\n+    /// S-LoRA dynamically loads/evicts adapters based on request patterns.\n+    /// </remarks>\n+    public int LoadedAdapterCount => _loadedAdapters.Count;\n+\n+    /// <summary>\n+    /// Gets the maximum number of adapters that can be loaded simultaneously.\n+    /// </summary>\n+    /// <remarks>\n+    /// This simulates GPU memory constraints. S-LoRA's unified paging mechanism\n+    /// efficiently manages this limited resource.\n+    /// </remarks>\n+    public int MaxLoadedAdapters => _maxLoadedAdapters;\n+\n+    /// <summary>\n+    /// Gets the number of rank clusters for batched computation optimization.\n+    /// </summary>\n+    /// <remarks>\n+    /// Adapters with the same rank are clustered together for efficient batched computation.\n+    /// This is a key optimization in S-LoRA for heterogeneous adapter serving.\n+    /// </remarks>\n+    public int RankClusterCount => _rankClusters.Count;\n+\n+    /// <summary>\n+    /// Initializes a new S-LoRA adapter for scalable multi-adapter serving.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The base layer to adapt with S-LoRA.</param>\n+    /// <param name=\"rank\">The default rank for the primary LoRA decomposition.</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"maxLoadedAdapters\">Maximum number of adapters to keep loaded simultaneously (default: 100).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when maxLoadedAdapters is less than 1.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This creates an S-LoRA serving system for efficient multi-adapter deployment.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The shared base model that all adapters modify\n+    /// - rank: Default rank for new adapters (typical: 8-32)\n+    /// - alpha: Scaling factor for LoRA contributions\n+    /// - maxLoadedAdapters: How many adapters to cache in \"GPU memory\" (100 = good balance)\n+    /// - freezeBaseLayer: Lock base weights (true for serving, false for continued training)\n+    ///\n+    /// How S-LoRA works:\n+    /// 1. One base model shared across all adapters (memory efficient)\n+    /// 2. Thousands of small adapters registered in unified pool\n+    /// 3. Only popular adapters kept loaded in fast memory\n+    /// 4. Unpopular adapters evicted and loaded on-demand\n+    /// 5. Batched computation for multiple adapters simultaneously\n+    ///\n+    /// Example: Serving 10,000 customer-specific adapters:\n+    /// - Base model: 7B parameters (14 GB)\n+    /// - Each adapter: rank 16 (few MB)\n+    /// - Total pool: 10,000 adapters (few GB in CPU memory)\n+    /// - Loaded cache: 100 most-used adapters (hundreds of MB in GPU memory)\n+    /// - Result: Serve 10,000 adapters with GPU memory for 1 base model + 100 adapters!\n+    ///\n+    /// This is 100x more efficient than loading full fine-tuned models.\n+    /// </para>\n+    /// </remarks>\n+    public SLoRAAdapter(\n+        ILayer<T> baseLayer,\n+        int rank,\n+        double alpha = -1,\n+        int maxLoadedAdapters = 100,\n+        bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        if (maxLoadedAdapters < 1)\n+        {\n+            throw new ArgumentException(\"Max loaded adapters must be at least 1\", nameof(maxLoadedAdapters));\n+        }\n+\n+        _adapterPool = new Dictionary<string, AdapterEntry>();\n+        _loadedAdapters = new Dictionary<string, AdapterEntry>();\n+        _rankClusters = new Dictionary<int, List<string>>();\n+        _maxLoadedAdapters = maxLoadedAdapters;\n+        _timestamp = 0;\n+\n+        // Register the primary adapter (from base class)\n+        RegisterAdapter(\"primary\", _loraLayer, rank);\n+        LoadAdapter(\"primary\");\n+    }\n+\n+    /// <summary>\n+    /// Registers a new adapter in the unified memory pool.\n+    /// </summary>\n+    /// <param name=\"adapterId\">Unique identifier for this adapter.</param>\n+    /// <param name=\"loraLayer\">The LoRA layer to register.</param>\n+    /// <param name=\"rank\">The rank of this adapter.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when adapterId or loraLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when an adapter with this ID already exists.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// This method adds a new adapter to S-LoRA's unified memory pool. The adapter is not immediately\n+    /// loaded into GPU memory but is available for on-demand loading when needed.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is like adding a new customer or task-specific adapter to your system.\n+    ///\n+    /// What happens when you register an adapter:\n+    /// 1. Adapter stored in CPU memory pool (cheap storage)\n+    /// 2. Added to rank cluster for batched computation optimization\n+    /// 3. Not loaded to GPU yet (only loaded when first used)\n+    /// 4. Can register thousands of adapters this way\n+    ///\n+    /// Example: Multi-tenant SaaS application\n+    /// ```csharp\n+    /// var slora = new SLoRAAdapter&lt;double&gt;(baseModel, rank: 8, maxLoadedAdapters: 100);\n+    ///\n+    /// // Register 1000 customer adapters\n+    /// for (int i = 0; i &lt; 1000; i++)\n+    /// {\n+    ///     var adapter = LoadCustomerAdapter(i);\n+    ///     slora.RegisterAdapter($\"customer_{i}\", adapter, rank: 8);\n+    /// }\n+    ///\n+    /// // All 1000 adapters registered, but only 100 will be loaded at once\n+    /// // Popular customers get fast GPU-cached access\n+    /// // Inactive customers loaded on-demand from CPU pool\n+    /// ```\n+    ///\n+    /// This enables serving far more adapters than GPU memory allows!\n+    /// </para>\n+    /// </remarks>\n+    public void RegisterAdapter(string adapterId, LoRALayer<T> loraLayer, int rank)\n+    {\n+        if (adapterId == null)\n+        {\n+            throw new ArgumentNullException(nameof(adapterId));\n+        }\n+\n+        if (loraLayer == null)\n+        {\n+            throw new ArgumentNullException(nameof(loraLayer));\n+        }\n+\n+        if (_adapterPool.ContainsKey(adapterId))\n+        {\n+            throw new ArgumentException($\"Adapter with ID '{adapterId}' already exists\", nameof(adapterId));\n+        }\n+\n+        // Create adapter entry\n+        var entry = new AdapterEntry(adapterId, loraLayer, rank);\n+        _adapterPool[adapterId] = entry;\n+\n+        // Add to rank cluster for batched computation\n+        if (!_rankClusters.ContainsKey(rank))\n+        {\n+            _rankClusters[rank] = new List<string>();\n+        }\n+        _rankClusters[rank].Add(adapterId);\n+    }\n+\n+    /// <summary>\n+    /// Loads an adapter from the pool into active memory (simulates GPU loading).\n+    /// </summary>\n+    /// <param name=\"adapterId\">The ID of the adapter to load.</param>\n+    /// <exception cref=\"ArgumentException\">Thrown when adapter ID is not found in pool.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// This method simulates S-LoRA's dynamic adapter loading from CPU to GPU memory.\n+    /// If the loaded adapter cache is full, it evicts the least recently used adapter.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This moves an adapter from slow storage to fast cache.\n+    ///\n+    /// In S-LoRA's architecture:\n+    /// - CPU memory: All adapters stored here (slow but large capacity)\n+    /// - GPU memory: Hot adapters cached here (fast but limited capacity)\n+    ///\n+    /// Loading process:\n+    /// 1. Check if adapter already loaded (if yes, update access time and return)\n+    /// 2. Check if cache is full (if yes, evict least recently used adapter)\n+    /// 3. Load adapter into cache\n+    /// 4. Mark as loaded and update access timestamp\n+    ///\n+    /// LRU eviction policy:\n+    /// - Adapters with oldest last access time evicted first\n+    /// - Adapters with active references (in-flight requests) never evicted\n+    /// - This keeps popular adapters hot in cache\n+    ///\n+    /// Example: Customer request patterns\n+    /// ```\n+    /// Time 0: Customer A requests (load adapter A)\n+    /// Time 1: Customer B requests (load adapter B)\n+    /// ...\n+    /// Time 99: Customer Z requests (load adapter Z, cache now full at 100)\n+    /// Time 100: Customer AA requests (evict least-used, load adapter AA)\n+    /// Time 101: Customer A requests again (adapter A was evicted, reload)\n+    /// ```\n+    ///\n+    /// Popular customers stay cached, inactive ones evicted automatically!\n+    /// </para>\n+    /// </remarks>\n+    public void LoadAdapter(string adapterId)\n+    {\n+        if (!_adapterPool.ContainsKey(adapterId))\n+        {\n+            throw new ArgumentException($\"Adapter '{adapterId}' not found in pool\", nameof(adapterId));\n+        }\n+\n+        var entry = _adapterPool[adapterId];\n+\n+        // If already loaded, just update access time\n+        if (entry.IsLoaded)\n+        {\n+            entry.LastAccess = ++_timestamp;\n+            return;\n+        }\n+\n+        // Evict if cache is full\n+        while (_loadedAdapters.Count >= _maxLoadedAdapters)\n+        {\n+            EvictLRUAdapter();\n+        }\n+\n+        // Load adapter into cache\n+        entry.IsLoaded = true;\n+        entry.LastAccess = ++_timestamp;\n+        _loadedAdapters[adapterId] = entry;\n+    }\n+\n+    /// <summary>\n+    /// Evicts the least recently used adapter from the loaded cache.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This implements S-LoRA's LRU eviction policy for memory management.\n+    /// Adapters with active references (in-flight requests) are not evicted.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This removes the least popular adapter from fast cache to make room.\n+    ///\n+    /// LRU (Least Recently Used) eviction:\n+    /// - Find adapter with oldest last access time\n+    /// - Check it's not actively being used (reference count = 0)\n+    /// - Remove from cache (but keep in pool for future reload)\n+    /// - Frees space for more popular adapters\n+    ///\n+    /// Why this works well:\n+    /// - Popular adapters get accessed frequently (stay cached)\n+    /// - Unpopular adapters get evicted (freed memory for others)\n+    /// - Temporal locality: recent requests predict future requests\n+    /// - Balance between memory usage and performance\n+    ///\n+    /// Example: E-commerce seasonal patterns\n+    /// ```\n+    /// Black Friday: Customer adapters for shoppers cached\n+    /// Normal day: Employee adapters for operations cached\n+    /// Tax season: Accounting adapters cached\n+    /// ```\n+    ///\n+    /// System automatically adapts to workload patterns!\n+    /// </para>\n+    /// </remarks>\n+    private void EvictLRUAdapter()\n+    {\n+        if (_loadedAdapters.Count == 0)\n+        {\n+            return;\n+        }\n+\n+        // Find LRU adapter that's not actively in use\n+        AdapterEntry? lruEntry = null;\n+        long minTimestamp = long.MaxValue;\n+\n+        foreach (var entry in _loadedAdapters.Values)\n+        {\n+            // Don't evict adapters with active references\n+            if (entry.ReferenceCount > 0)\n+            {\n+                continue;\n+            }\n+\n+            if (entry.LastAccess < minTimestamp)\n+            {\n+                minTimestamp = entry.LastAccess;\n+                lruEntry = entry;\n+            }\n+        }\n+\n+        // Evict the LRU adapter\n+        if (lruEntry != null)\n+        {\n+            lruEntry.IsLoaded = false;\n+            _loadedAdapters.Remove(lruEntry.Id);\n+        }","path":"src/LoRA/Adapters/SLoRAAdapter.cs","commit_id":"489180133c7a9db9460c84a6174f60284717b952","original_commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Break the infinite eviction loop when every loaded adapter is in use.**\n\nIf all `_loadedAdapters` have `ReferenceCount > 0`, `EvictLRUAdapter()` returns without evicting anything. The surrounding `while (_loadedAdapters.Count >= _maxLoadedAdapters)` loop never makes progress, so `LoadAdapter` spins forever. Have `EvictLRUAdapter` report success/failure and either back off or throw when no eviction candidate exists.\n\n```diff\n-        while (_loadedAdapters.Count >= _maxLoadedAdapters)\n-        {\n-            EvictLRUAdapter();\n-        }\n+        while (_loadedAdapters.Count >= _maxLoadedAdapters)\n+        {\n+            if (!TryEvictLRUAdapter())\n+            {\n+                throw new InvalidOperationException(\"All loaded adapters are active; cannot load a new adapter until one finishes.\");\n+            }\n+        }\n...\n-    private void EvictLRUAdapter()\n+    private bool TryEvictLRUAdapter()\n     {\n         if (_loadedAdapters.Count == 0)\n         {\n-            return;\n+            return false;\n         }\n...\n-        if (lruEntry != null)\n-        {\n-            lruEntry.IsLoaded = false;\n-            _loadedAdapters.Remove(lruEntry.Id);\n-        }\n+        if (lruEntry == null)\n+        {\n+            return false;\n+        }\n+\n+        lruEntry.IsLoaded = false;\n+        _loadedAdapters.Remove(lruEntry.Id);\n+        return true;\n     }\n```\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/LoRA/Adapters/SLoRAAdapter.cs around lines 378 to 452, EvictLRUAdapter\ncan return without evicting when every loaded adapter has ReferenceCount > 0,\ncausing the caller's while (_loadedAdapters.Count >= _maxLoadedAdapters) loop to\nspin forever; change EvictLRUAdapter to return a bool (true if an adapter was\nevicted, false if no eviction candidate), return false when no non-active\nadapter is found, and update the caller (LoadAdapter) to check that return value\nand handle failure by either throwing a specific exception (e.g.,\nNoEvictableAdapterException) or performing a bounded backoff/retry with a clear\nerror when retries are exhausted so the loop cannot spin indefinitely.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commit fa81503","created_at":"2025-11-02T02:42:03Z","updated_at":"2025-11-02T15:10:33Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484118482","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118482"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484118482"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118482/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":378,"original_start_line":378,"start_side":"RIGHT","line":461,"original_line":452,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":452,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118488","pull_request_review_id":3408015014,"id":2484118488,"node_id":"PRRC_kwDOKSXUF86UEKPY","diff_hunk":"@@ -0,0 +1,145 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// Standard LoRA implementation (original LoRA algorithm).\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// The StandardLoRAAdapter wraps any layer and adds a LoRA layer in parallel.\n+/// During forward pass, both the base layer and LoRA layer process the input, and their outputs are\n+/// summed. The base layer's parameters can be frozen while only the LoRA parameters are trained.\n+/// </para>\n+/// <para><b>For Beginners:</b> This adapter lets you add LoRA to any layer type.\n+/// Think of it like adding a \"correction layer\" that learns what adjustments are needed:\n+///\n+/// - The base layer keeps its original weights (optionally frozen)\n+/// - The LoRA layer learns a small correction\n+/// - The final output is: original_output + lora_correction\n+///\n+/// This is incredibly useful for fine-tuning pre-trained models:\n+/// 1. Load a pre-trained model with any layer type\n+/// 2. Wrap those layers with StandardLoRAAdapter\n+/// 3. Freeze the base layers\n+/// 4. Train only the small LoRA corrections\n+/// 5. Achieve similar results with 100x fewer trainable parameters!\n+///\n+/// Example: If you have a dense layer with 1000x1000 weights, wrapping it with rank=8 LoRA\n+/// (frozen) reduces trainable parameters from 1,000,000 to just 16,000!\n+/// </para>\n+/// </remarks>\n+public class StandardLoRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Initializes a new Standard LoRA adapter wrapping an existing layer.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with LoRA.</param>\n+    /// <param name=\"rank\">The rank of the LoRA decomposition.</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This creates an adapter that adds LoRA to any layer.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to make more efficient to fine-tune\n+    /// - rank: How much compression (lower = fewer parameters, less flexibility)\n+    /// - alpha: How strong the LoRA adaptation is\n+    /// - freezeBaseLayer: Whether to lock the original layer's weights (usually true for efficiency)\n+    ///\n+    /// This adapter works with any layer type:\n+    /// - DenseLayer (fully connected layer)\n+    /// - ConvolutionalLayer (CNN layer)\n+    /// - LSTMLayer (recurrent layer)\n+    /// - Any custom ILayer implementation\n+    ///\n+    /// The standard LoRA algorithm uses 1D matrices for the A and B decomposition,\n+    /// which works well for most layer types.\n+    /// </para>\n+    /// </remarks>\n+    public StandardLoRAAdapter(ILayer<T> baseLayer, int rank, double alpha = -1, bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        // No shape validation - works with any layer type\n+    }\n+\n+    /// <summary>\n+    /// Merges the LoRA adaptation into the base layer and returns the merged layer.\n+    /// </summary>\n+    /// <returns>A new layer with LoRA weights merged into the base layer's weights.</returns>\n+    /// <exception cref=\"InvalidOperationException\">Thrown when the base layer type is not DenseLayer or FullyConnectedLayer.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// This method supports merging for both DenseLayer and FullyConnectedLayer base layers.\n+    /// The LoRA weights are computed and added directly to the base layer's weight matrix.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This \"bakes in\" your LoRA adaptation to create a regular layer.\n+    /// After training with LoRA, you can merge the adaptation into the original weights for:\n+    /// - Faster inference (no need to compute LoRA separately)\n+    /// - Simpler deployment (single layer instead of two)\n+    /// - Compatibility with systems that don't support LoRA\n+    ///\n+    /// Think of it like merging tracked changes in a document - you go from \"original + changes\"\n+    /// to a single updated version.\n+    ///\n+    /// The merging process:\n+    /// 1. Gets the LoRA weight matrix (computed from A and B matrices)\n+    /// 2. Adds these weights to the base layer's existing weights\n+    /// 3. Copies biases unchanged (LoRA doesn't modify biases)\n+    /// 4. Creates a new layer with the merged weights\n+    ///\n+    /// Note: Merging currently only supports DenseLayer and FullyConnectedLayer.\n+    /// For other layer types, you'll need to use the adapter in production or implement\n+    /// custom merging logic.\n+    /// </para>\n+    /// </remarks>\n+    public override ILayer<T> MergeToOriginalLayer()\n+    {\n+        // Support both DenseLayer and FullyConnectedLayer\n+        DenseLayer<T>? denseBase = _baseLayer as DenseLayer<T>;\n+        FullyConnectedLayer<T>? fcBase = _baseLayer as FullyConnectedLayer<T>;\n+\n+        if (denseBase == null && fcBase == null)\n+        {\n+            throw new InvalidOperationException(\"StandardLoRAAdapter merging only supports DenseLayer or FullyConnectedLayer base layers\");\n+        }\n+\n+        // Get the LoRA weight contribution\n+        Matrix<T> loraWeights = _loraLayer.MergeWeights();\n+\n+        // Get base layer parameters (works for both DenseLayer and FullyConnectedLayer)\n+        Vector<T> baseParams = _baseLayer.GetParameters();\n+\n+        // Both DenseLayer and FullyConnectedLayer store parameters as [weights..., biases...]\n+        // We need to add the LoRA weights to the base weights\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        int weightCount = inputSize * outputSize;\n+\n+        // Create new parameters with merged weights\n+        Vector<T> mergedParams = new Vector<T>(baseParams.Length);\n+\n+        // Merge weights\n+        for (int i = 0; i < weightCount; i++)\n+        {\n+            int row = i / inputSize;\n+            int col = i % inputSize;\n+            mergedParams[i] = NumOps.Add(baseParams[i], loraWeights[row, col]);\n+        }\n+\n+        // Copy biases unchanged\n+        for (int i = weightCount; i < baseParams.Length; i++)\n+        {\n+            mergedParams[i] = baseParams[i];\n+        }\n+\n+        // Create a new dense layer with merged parameters\n+        // Always return DenseLayer for consistency\n+        DenseLayer<T> mergedLayer = new DenseLayer<T>(inputSize, outputSize, (IActivationFunction<T>?)null);\n+        mergedLayer.SetParameters(mergedParams);","path":"src/LoRA/Adapters/StandardLoRAAdapter.cs","commit_id":"489180133c7a9db9460c84a6174f60284717b952","original_commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Keep the base layer activation when merging.**\n\nJust like the dense-specific adapter, this path instantiates the merged `DenseLayer` with `null` activation, which strips whatever non-linearity the original layer was using. That changes the model‚Äôs behaviour post-merge. Please reuse the activation configuration from the wrapped layer.\n\n```diff\n-        DenseLayer<T> mergedLayer = new DenseLayer<T>(inputSize, outputSize, (IActivationFunction<T>?)null);\n+        var activation = denseBase?.ActivationFunction ?? fcBase?.ActivationFunction;\n+        DenseLayer<T> mergedLayer = new DenseLayer<T>(inputSize, outputSize, activation);\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        // Create a new dense layer with merged parameters\n        // Always return DenseLayer for consistency\n        var activation = denseBase?.ActivationFunction ?? fcBase?.ActivationFunction;\n        DenseLayer<T> mergedLayer = new DenseLayer<T>(inputSize, outputSize, activation);\n        mergedLayer.SetParameters(mergedParams);\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/LoRA/Adapters/StandardLoRAAdapter.cs around lines 138 to 141, the merged\nDenseLayer is being created with a null activation which drops the original\nlayer's non-linearity; instead obtain the wrapped/base layer's activation\ninstance (e.g., read the activation property or getter from the wrapped\nDenseLayer), pass that activation into the DenseLayer<T> constructor when\ncreating mergedLayer (fall back to null only if the wrapped layer's activation\nis null), then call SetParameters(mergedParams) as before so the merged layer\npreserves the original activation behavior.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:42:03Z","updated_at":"2025-11-02T02:42:06Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484118488","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118488"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484118488"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118488/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":138,"original_start_line":138,"start_side":"RIGHT","line":141,"original_line":141,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":141,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118497","pull_request_review_id":3408015014,"id":2484118497,"node_id":"PRRC_kwDOKSXUF86UEKPh","diff_hunk":"@@ -0,0 +1,872 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// Tied-LoRA adapter - LoRA with weight tying for extreme parameter efficiency across deep networks.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// Tied-LoRA achieves even greater parameter efficiency than standard LoRA by:\n+/// - Sharing the same LoRA matrices (A and B) across multiple layers\n+/// - Training only layer-specific scaling factors\n+/// - Particularly effective for very deep networks with many similar layers\n+/// </para>\n+/// <para>\n+/// The forward computation is: output = base_layer(input) + layerScaling * (B_shared * A_shared * input)\n+/// where layerScaling is a trainable scalar unique to each layer, and A and B are shared trainable matrices.\n+/// </para>\n+/// <para><b>For Beginners:</b> Tied-LoRA is an ultra-efficient variant of LoRA for deep networks.\n+///\n+/// Think of the difference this way:\n+/// - Standard LoRA: Each layer has its own pair of small matrices (A and B) that are trained\n+/// - VeRA: ALL layers share the same random matrices (A and B) which are frozen. Only tiny\n+///   scaling vectors are trained per layer.\n+/// - Tied-LoRA: ALL layers share the same matrices (A and B) which ARE trained. Only a single\n+///   scaling factor is trained per layer.\n+///\n+/// Example parameter comparison for 10 layers of 1000x1000 with rank=8:\n+/// - Full fine-tuning: 10,000,000 parameters\n+/// - Standard LoRA (rank=8): 160,000 parameters (10 layers √ó 16,000 params each)\n+/// - Tied-LoRA (rank=8): ~16,010 parameters (shared 16,000 + 10 scaling factors)\n+///\n+/// Benefits of Tied-LoRA:\n+/// - ‚úÖ Extreme parameter efficiency for deep networks (scales with depth)\n+/// - ‚úÖ Shared matrices enforce consistency across layers\n+/// - ‚úÖ Still trainable (unlike VeRA's frozen matrices)\n+/// - ‚úÖ Very low memory footprint\n+/// - ‚úÖ Faster training (fewer parameters to update)\n+///\n+/// Trade-offs:\n+/// - ‚ö†Ô∏è Less flexible than standard LoRA (shared adaptation across layers)\n+/// - ‚ö†Ô∏è Assumes layers benefit from similar adaptations\n+/// - ‚ö†Ô∏è May underperform standard LoRA on heterogeneous architectures\n+///\n+/// When to use Tied-LoRA:\n+/// - Very deep networks (transformers with many similar layers)\n+/// - Extreme memory constraints\n+/// - When layers have similar structure and function\n+/// - Rapid prototyping with minimal parameter overhead\n+/// - Fine-tuning massive models (GPT, BERT-style architectures)\n+///\n+/// Research insight: Tied-LoRA works well because in deep networks, many layers learn similar\n+/// transformations. By sharing the LoRA matrices and only varying the strength per layer,\n+/// we capture most of the adaptation capability with minimal parameters.\n+/// </para>\n+/// </remarks>\n+public class TiedLoRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Shared trainable matrix A (inputSize √ó rank) used by all Tied-LoRA adapters.\n+    /// </summary>\n+    /// <remarks>\n+    /// This matrix is shared across all Tied-LoRA layers and IS trained during fine-tuning.\n+    /// Unlike VeRA, this matrix is not frozen - it learns the common adaptation pattern.\n+    /// </remarks>\n+    private static Matrix<T>? _sharedMatrixA;\n+\n+    /// <summary>\n+    /// Shared trainable matrix B (rank √ó outputSize) used by all Tied-LoRA adapters.\n+    /// </summary>\n+    /// <remarks>\n+    /// This matrix is shared across all Tied-LoRA layers and IS trained during fine-tuning.\n+    /// Unlike VeRA, this matrix is not frozen - it learns the common adaptation pattern.\n+    /// </remarks>\n+    private static Matrix<T>? _sharedMatrixB;\n+\n+    /// <summary>\n+    /// Gradients for shared matrix A accumulated from all layers.\n+    /// </summary>\n+    private static Matrix<T>? _sharedMatrixAGradient;\n+\n+    /// <summary>\n+    /// Gradients for shared matrix B accumulated from all layers.\n+    /// </summary>\n+    private static Matrix<T>? _sharedMatrixBGradient;\n+\n+    /// <summary>\n+    /// Lock object for thread-safe shared matrix access and updates.\n+    /// </summary>\n+    private static readonly object _sharedLock = new object();\n+\n+    /// <summary>\n+    /// Layer-specific scaling factor - the only trainable parameter unique to this layer.\n+    /// </summary>\n+    /// <remarks>\n+    /// This single scalar value controls how strongly this layer's output is affected by\n+    /// the shared LoRA adaptation. Different layers can have different scaling factors,\n+    /// allowing the network to modulate the shared adaptation per layer.\n+    /// </remarks>\n+    private T _layerScaling;\n+\n+    /// <summary>\n+    /// Gradient for the layer-specific scaling factor.\n+    /// </summary>\n+    private T _layerScalingGradient;\n+\n+    /// <summary>\n+    /// Layer index identifying this adapter's position in the network.\n+    /// </summary>\n+    /// <remarks>\n+    /// This helps track which layer this adapter belongs to, useful for debugging and\n+    /// analysis of how different layers utilize the shared adaptation.\n+    /// </remarks>\n+    private readonly int _layerIndex;\n+\n+    /// <summary>\n+    /// Stored input from the forward pass, needed for gradient computation.\n+    /// </summary>\n+    private Tensor<T>? _lastInput;\n+\n+    /// <summary>\n+    /// Stored intermediate value (B_shared * A_shared * input) from forward pass.\n+    /// </summary>\n+    private Matrix<T>? _lastIntermediate;\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters.\n+    /// </summary>\n+    /// <remarks>\n+    /// Tied-LoRA only trains a single scaling factor per layer (plus the base layer if not frozen).\n+    /// The shared matrices contribute to the parameter count only once across all layers.\n+    /// </remarks>\n+    public override int ParameterCount\n+    {\n+        get\n+        {\n+            // Only the layer scaling factor is unique to this layer\n+            int tiedLoraParams = 1; // Single scaling factor\n+            return _freezeBaseLayer ? tiedLoraParams : (_baseLayer.ParameterCount + tiedLoraParams);\n+        }\n+    }","path":"src/LoRA/Adapters/TiedLoRAAdapter.cs","commit_id":"489180133c7a9db9460c84a6174f60284717b952","original_commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Fix `ParameterCount` during construction to avoid `IndexOutOfRangeException`.**\n\n`LoRAAdapterBase` allocates `Parameters = new Vector<T>(ParameterCount)` and immediately copies `_loraLayer`‚Äôs parameters into it. Because this override reports `ParameterCount == 1`, the base ctor writes past the end of the vector and throws. Defer the reduced count until after construction, then reallocate once your state is ready.\n\n```diff\n-    public override int ParameterCount\n-    {\n-        get\n-        {\n-            int tiedLoraParams = 1; // Single scaling factor\n-            return _freezeBaseLayer ? tiedLoraParams : (_baseLayer.ParameterCount + tiedLoraParams);\n-        }\n-    }\n+    private bool _isInitialized;\n+\n+    public override int ParameterCount\n+    {\n+        get\n+        {\n+            if (!_isInitialized)\n+            {\n+                return base.ParameterCount;\n+            }\n+\n+            int tiedLoraParams = 1;\n+            return _freezeBaseLayer ? tiedLoraParams : (_baseLayer.ParameterCount + tiedLoraParams);\n+        }\n+    }\n...\n-        // Update parameter vector\n-        UpdateParametersFromScaling();\n+        _isInitialized = true;\n+        Parameters = new Vector<T>(ParameterCount);\n+        UpdateParametersFromScaling();\n```\n\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/LoRA/Adapters/TiedLoRAAdapter.cs around lines 135-143, the override of\nParameterCount currently returns 1 during object construction which causes the\nbase constructor to allocate a too-small Parameters vector and copy past its\nend; update the class so that during construction ParameterCount reports the\nfull size (base layer parameters + 1) and only reduce to the tied count after\nconstruction is complete. Concretely: make ParameterCount depend on a\n\"constructed/initialized\" flag (or similar) so the base ctor sees the full\ncount, allow the base ctor to allocate and copy normally, then after you finish\ninitialization set the flag to indicate the freeze/reduced state and reallocate\nor resize Parameters to the smaller tied size and copy/move the preserved\nscaling factor into the new vector. Ensure the reallocation happens only once\nafter construction to avoid further IndexOutOfRangeExceptions.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:42:03Z","updated_at":"2025-11-02T02:42:06Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484118497","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118497"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484118497"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118497/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":135,"original_start_line":135,"start_side":"RIGHT","line":143,"original_line":143,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":143,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118508","pull_request_review_id":3408015014,"id":2484118508,"node_id":"PRRC_kwDOKSXUF86UEKPs","diff_hunk":"@@ -0,0 +1,144 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LoRA.Adapters;\n+\n+namespace AiDotNet.NeuralNetworks.Layers;\n+\n+/// <summary>\n+/// LoRA adapter specifically for Dense and FullyConnected layers with 1D input/output shapes.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// The DenseLoRAAdapter wraps Dense or FullyConnected layers and adds a LoRA layer in parallel.\n+/// During forward pass, both the base layer and LoRA layer process the input, and their outputs are\n+/// summed. The base layer's parameters can be frozen while only the LoRA parameters are trained.\n+/// </para>\n+/// <para><b>For Beginners:</b> This adapter lets you add LoRA to Dense or FullyConnected layers.\n+/// Think of it like adding a \"correction layer\" that learns what adjustments are needed:\n+///\n+/// - The base layer keeps its original weights (optionally frozen)\n+/// - The LoRA layer learns a small correction\n+/// - The final output is: original_output + lora_correction\n+///\n+/// This is incredibly useful for fine-tuning pre-trained models:\n+/// 1. Load a pre-trained model with Dense/FullyConnected layers\n+/// 2. Wrap those layers with DenseLoRAAdapter\n+/// 3. Freeze the base layers\n+/// 4. Train only the small LoRA corrections\n+/// 5. Achieve similar results with 100x fewer trainable parameters!\n+///\n+/// Example: If you have a dense layer with 1000x1000 weights, wrapping it with rank=8 LoRA\n+/// (frozen) reduces trainable parameters from 1,000,000 to just 16,000!\n+/// </para>\n+/// </remarks>\n+public class DenseLoRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Initializes a new Dense LoRA adapter wrapping an existing Dense or FullyConnected layer.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The Dense or FullyConnected layer to adapt with LoRA.</param>\n+    /// <param name=\"rank\">The rank of the LoRA decomposition.</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when the base layer doesn't have 1D input/output shapes.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This creates an adapter that adds LoRA to a Dense or FullyConnected layer.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The Dense or FullyConnected layer you want to make more efficient to fine-tune\n+    /// - rank: How much compression (lower = fewer parameters, less flexibility)\n+    /// - alpha: How strong the LoRA adaptation is\n+    /// - freezeBaseLayer: Whether to lock the original layer's weights (usually true for efficiency)\n+    ///\n+    /// This adapter only works with layers that have 1D input/output shapes, which includes:\n+    /// - DenseLayer (standard fully connected layer)\n+    /// - FullyConnectedLayer (another name for the same thing)\n+    ///\n+    /// It validates that the base layer has compatible shapes before proceeding.\n+    /// </para>\n+    /// </remarks>\n+    public DenseLoRAAdapter(ILayer<T> baseLayer, int rank, double alpha = -1, bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        // Validate base layer has single-dimensional input/output (specific to Dense layers)\n+        if (baseLayer.GetInputShape().Length != 1 || baseLayer.GetOutputShape().Length != 1)\n+        {\n+            throw new ArgumentException(\"DenseLoRAAdapter only supports layers with 1D input/output shapes (Dense/FullyConnected layers)\", nameof(baseLayer));\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Merges the LoRA adaptation into the base layer and returns the merged Dense layer.\n+    /// </summary>\n+    /// <returns>A new DenseLayer with LoRA weights merged into the base layer's weights.</returns>\n+    /// <exception cref=\"InvalidOperationException\">Thrown when the base layer type is not DenseLayer or FullyConnectedLayer.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// This method supports merging for both DenseLayer and FullyConnectedLayer base layers.\n+    /// The LoRA weights are computed and added directly to the base layer's weight matrix.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This \"bakes in\" your LoRA adaptation to create a regular Dense layer.\n+    /// After training with LoRA, you can merge the adaptation into the original weights for:\n+    /// - Faster inference (no need to compute LoRA separately)\n+    /// - Simpler deployment (single layer instead of two)\n+    /// - Compatibility with systems that don't support LoRA\n+    ///\n+    /// Think of it like merging tracked changes in a document - you go from \"original + changes\"\n+    /// to a single updated version.\n+    ///\n+    /// The merging process:\n+    /// 1. Gets the LoRA weight matrix (computed from A and B matrices)\n+    /// 2. Adds these weights to the base layer's existing weights\n+    /// 3. Copies biases unchanged (LoRA doesn't modify biases)\n+    /// 4. Creates a new DenseLayer with the merged weights\n+    /// </para>\n+    /// </remarks>\n+    public override ILayer<T> MergeToOriginalLayer()\n+    {\n+        // Support both DenseLayer and FullyConnectedLayer\n+        DenseLayer<T>? denseBase = _baseLayer as DenseLayer<T>;\n+        FullyConnectedLayer<T>? fcBase = _baseLayer as FullyConnectedLayer<T>;\n+\n+        if (denseBase == null && fcBase == null)\n+        {\n+            throw new InvalidOperationException(\"DenseLoRAAdapter only supports DenseLayer or FullyConnectedLayer base layers\");\n+        }\n+\n+        // Get the LoRA weight contribution\n+        Matrix<T> loraWeights = _loraLayer.MergeWeights();\n+\n+        // Get base layer parameters (works for both DenseLayer and FullyConnectedLayer)\n+        Vector<T> baseParams = _baseLayer.GetParameters();\n+\n+        // Both DenseLayer and FullyConnectedLayer store parameters as [weights..., biases...]\n+        // We need to add the LoRA weights to the base weights\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        int weightCount = inputSize * outputSize;\n+\n+        // Create new parameters with merged weights\n+        Vector<T> mergedParams = new Vector<T>(baseParams.Length);\n+\n+        // Merge weights\n+        for (int i = 0; i < weightCount; i++)\n+        {\n+            int row = i / inputSize;\n+            int col = i % inputSize;\n+            mergedParams[i] = NumOps.Add(baseParams[i], loraWeights[row, col]);\n+        }\n+\n+        // Copy biases unchanged\n+        for (int i = weightCount; i < baseParams.Length; i++)\n+        {\n+            mergedParams[i] = baseParams[i];\n+        }\n+\n+        // Create a new dense layer with merged parameters\n+        // Always return DenseLayer for consistency\n+        DenseLayer<T> mergedLayer = new DenseLayer<T>(inputSize, outputSize, (IActivationFunction<T>?)null);\n+        mergedLayer.SetParameters(mergedParams);\n+","path":"src/NeuralNetworks/Layers/DenseLoRAAdapter.cs","commit_id":"489180133c7a9db9460c84a6174f60284717b952","original_commit_id":"b53b0ee18134117b29aa2b2fa4e8fde2d80ffe80","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Preserve the original activation when merging.**\n\n`MergeToOriginalLayer` always creates the merged `DenseLayer` with a `null` activation, so any non-linear activation configured on the wrapped layer disappears after the merge. That drastically alters inference for anything other than a linear layer. Please carry the existing activation (and other relevant configuration) into the merged instance instead of dropping it.\n\n```diff\n-        DenseLayer<T> mergedLayer = new DenseLayer<T>(inputSize, outputSize, (IActivationFunction<T>?)null);\n+        var activation = denseBase?.ActivationFunction ?? fcBase?.ActivationFunction;\n+        DenseLayer<T> mergedLayer = new DenseLayer<T>(inputSize, outputSize, activation);\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        // Always return DenseLayer for consistency\n        var activation = denseBase?.ActivationFunction ?? fcBase?.ActivationFunction;\n        DenseLayer<T> mergedLayer = new DenseLayer<T>(inputSize, outputSize, activation);\n        mergedLayer.SetParameters(mergedParams);\n\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/Layers/DenseLoRAAdapter.cs around lines 138 to 141, the\nmerged DenseLayer is constructed with a null activation which discards the\nwrapped layer's non-linear activation and other configuration; update the merge\nto read the original/wrapped layer's Activation (and any other relevant settings\nlike Bias, UseBias, or Layer metadata) and pass them into the DenseLayer\nconstructor (or set them on the merged instance) before applying mergedParams so\nthe resulting layer preserves the original activation and configuration.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T02:42:04Z","updated_at":"2025-11-02T02:42:06Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484118508","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118508"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484118508"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484118508/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":138,"original_start_line":138,"start_side":"RIGHT","line":141,"original_line":141,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":141,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140265","pull_request_review_id":3408046172,"id":2484140265,"node_id":"PRRC_kwDOKSXUF86UEPjp","diff_hunk":"@@ -0,0 +1,936 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// LoftQ (LoRA-Fine-Tuning-Quantized) adapter that combines quantization and LoRA with improved initialization.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// LoftQ improves upon QLoRA by using an alternating optimization strategy during initialization\n+/// to find better LoRA adapter parameters for quantized models. Instead of simply quantizing\n+/// a pre-trained model and adding LoRA on top, LoftQ alternates between:\n+/// 1. Optimizing the quantization of the base weights\n+/// 2. Optimizing the LoRA adapter matrices to compensate for quantization error\n+/// </para>\n+/// <para>\n+/// <b>Key Features:</b>\n+/// - Alternating optimization between quantization and LoRA initialization\n+/// - Better initialization than naive quantization + LoRA\n+/// - Supports both 4-bit INT4 and NF4 quantization\n+/// - Reduces the gap between quantized and full-precision fine-tuning\n+/// - Compatible with all QLoRA features (double quantization, block-wise quantization)\n+/// </para>\n+/// <para>\n+/// <b>How LoftQ Differs from QLoRA:</b>\n+/// QLoRA:\n+/// 1. Quantize pre-trained weights\n+/// 2. Initialize LoRA randomly\n+/// 3. Fine-tune LoRA only\n+///\n+/// LoftQ:\n+/// 1. Start with pre-trained weights\n+/// 2. Alternate K times:\n+///    a. Fix LoRA, optimize quantization\n+///    b. Fix quantization, optimize LoRA (via SVD to minimize error)\n+/// 3. Fine-tune LoRA only\n+///\n+/// This alternating initialization creates better starting LoRA parameters that compensate\n+/// for quantization error from the beginning, leading to better final performance.\n+/// </para>\n+/// <para>\n+/// <b>Alternating Optimization Process:</b>\n+/// For K iterations (typically 3-5):\n+/// - Quantization step: Quantize W to get Q, keeping A and B fixed\n+/// - LoRA step: Update A and B to minimize ||W - (Q + AB)||, keeping Q fixed\n+///\n+/// This ensures the LoRA adapter specifically compensates for quantization error,\n+/// rather than learning generic adaptations.\n+/// </para>\n+/// <para>\n+/// <b>Memory Efficiency:</b>\n+/// Same as QLoRA - base weights in 4-bit, LoRA in full precision:\n+/// - 75% memory reduction on base weights\n+/// - Only LoRA parameters trainable (typically 0.1-1% of model size)\n+/// - Additional one-time cost during initialization for alternating optimization\n+/// </para>\n+/// <para>\n+/// <b>For Beginners:</b> LoftQ is an improved version of QLoRA that starts with better settings.\n+///\n+/// Think of it like this:\n+/// - QLoRA: Compress your model, then add random corrections, then train\n+/// - LoftQ: Compress your model, figure out what corrections are needed upfront, then train\n+///\n+/// The key insight: If we're going to compress the weights anyway, let's make sure our\n+/// correction layer (LoRA) is specifically designed to fix compression errors!\n+///\n+/// The process:\n+/// 1. Start with your pre-trained model\n+/// 2. Repeatedly:\n+///    - Try different compressions\n+///    - Adjust LoRA to compensate for compression error\n+///    - Pick the best combination\n+/// 3. Now train LoRA (which already knows how to fix compression issues)\n+///\n+/// Benefits:\n+/// - Better starting point for training\n+/// - Converges faster during fine-tuning\n+/// - Better final accuracy than QLoRA with same memory usage\n+/// - Still only trains LoRA (same efficiency as QLoRA)\n+///\n+/// Trade-offs:\n+/// - Longer initialization time (worth it for better results)\n+/// - Same runtime memory and speed as QLoRA\n+/// - More complex implementation\n+/// </para>\n+/// <para>\n+/// <b>Research Background:</b>\n+/// LoftQ was introduced in \"LoftQ: LoRA-Fine-Tuning-Aware Quantization\" (Li et al., 2023).\n+/// It addresses a key limitation of QLoRA: random LoRA initialization doesn't account for\n+/// the specific quantization errors introduced. By using alternating optimization, LoftQ\n+/// creates LoRA parameters that are \"aware\" of the quantization, leading to better downstream\n+/// fine-tuning performance with no additional runtime cost.\n+/// </para>\n+/// <para>\n+/// <b>When to Use LoftQ vs QLoRA:</b>\n+/// - Use LoftQ when: Training accuracy is critical, willing to spend extra time on initialization\n+/// - Use QLoRA when: Fast experimentation needed, initialization time is critical\n+/// - Both have identical runtime memory and speed characteristics\n+/// </para>\n+/// </remarks>\n+public class LoftQAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Specifies the type of 4-bit quantization to use for base layer weights.\n+    /// </summary>\n+    /// <remarks>\n+    /// Same quantization types as QLoRA. The alternating optimization works with both.\n+    /// </remarks>\n+    public enum QuantizationType\n+    {\n+        /// <summary>\n+        /// 4-bit integer quantization with uniform spacing (-8 to 7).\n+        /// </summary>\n+        INT4,\n+\n+        /// <summary>\n+        /// 4-bit Normal Float quantization optimized for normally distributed weights.\n+        /// </summary>\n+        /// <remarks>\n+        /// Recommended for most neural network weights. NF4 with LoftQ initialization\n+        /// provides the best accuracy-memory trade-off.\n+        /// </remarks>\n+        NF4\n+    }\n+\n+    /// <summary>\n+    /// The type of quantization used for base layer weights.\n+    /// </summary>\n+    private readonly QuantizationType _quantizationType;\n+\n+    /// <summary>\n+    /// Whether to use double quantization for quantization constants.\n+    /// </summary>\n+    private readonly bool _useDoubleQuantization;\n+\n+    /// <summary>\n+    /// The block size for quantization.\n+    /// </summary>\n+    private readonly int _quantizationBlockSize;\n+\n+    /// <summary>\n+    /// Number of alternating optimization iterations during initialization.\n+    /// </summary>\n+    /// <remarks>\n+    /// Typical values: 3-5 iterations. More iterations improve initialization quality\n+    /// but increase initialization time. Empirically, 3-5 iterations provide good\n+    /// balance between quality and speed.\n+    /// </remarks>\n+    private readonly int _numAlternatingIterations;\n+\n+    /// <summary>\n+    /// Quantized base layer weights stored as 4-bit values.\n+    /// </summary>\n+    private byte[]? _quantizedWeights;\n+\n+    /// <summary>\n+    /// Scale factors for dequantization (one per quantization block).\n+    /// </summary>\n+    private T[]? _quantizationScales;\n+\n+    /// <summary>\n+    /// Zero points for asymmetric quantization (one per quantization block).\n+    /// </summary>\n+    private T[]? _quantizationZeroPoints;\n+\n+    /// <summary>\n+    /// Cached dequantized weights for forward pass.\n+    /// </summary>\n+    private Matrix<T>? _dequantizedWeights;\n+\n+    /// <summary>\n+    /// NF4 quantization lookup table (16 values optimized for normal distribution).\n+    /// </summary>\n+    private static readonly double[] _nf4Table = new double[]\n+    {\n+        -1.0,\n+        -0.6961928009986877,\n+        -0.5250730514526367,\n+        -0.39491748809814453,\n+        -0.28444138169288635,\n+        -0.18477343022823334,\n+        -0.09105003625154495,\n+        0.0,\n+        0.07958029955625534,\n+        0.16093020141124725,\n+        0.24611230194568634,\n+        0.33791524171829224,\n+        0.44070982933044434,\n+        0.5626170039176941,\n+        0.7229568362236023,\n+        1.0\n+    };\n+\n+    /// <summary>\n+    /// Gets the quantization type used for base layer weights.\n+    /// </summary>\n+    public QuantizationType Quantization => _quantizationType;\n+\n+    /// <summary>\n+    /// Gets whether double quantization is enabled.\n+    /// </summary>\n+    public bool UsesDoubleQuantization => _useDoubleQuantization;\n+\n+    /// <summary>\n+    /// Gets the quantization block size.\n+    /// </summary>\n+    public int BlockSize => _quantizationBlockSize;\n+\n+    /// <summary>\n+    /// Gets the number of alternating optimization iterations used during initialization.\n+    /// </summary>\n+    public int AlternatingIterations => _numAlternatingIterations;\n+\n+    /// <summary>\n+    /// Initializes a new LoftQ adapter with alternating optimization for improved initialization.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The Dense or FullyConnected layer to adapt with LoftQ.</param>\n+    /// <param name=\"rank\">The rank of the LoRA decomposition.</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"numAlternatingIterations\">Number of alternating optimization iterations for initialization (default: 5).</param>\n+    /// <param name=\"quantizationType\">The type of 4-bit quantization to use (default: NF4).</param>\n+    /// <param name=\"useDoubleQuantization\">Whether to use double quantization for constants (default: true).</param>\n+    /// <param name=\"quantizationBlockSize\">The block size for quantization (default: 64).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training (default: true).</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when the base layer doesn't have 1D input/output shapes or when parameters are invalid.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// This constructor performs LoftQ initialization using alternating optimization:\n+    /// 1. Extracts base layer weights\n+    /// 2. For K iterations:\n+    ///    a. Quantize current weights\n+    ///    b. Compute quantization error\n+    ///    c. Update LoRA to minimize error (via SVD)\n+    ///    d. Update weights = quantized + LoRA\n+    /// 3. Store final quantized weights and LoRA parameters\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> Creating a LoftQ adapter takes longer than QLoRA because\n+    /// we're doing smart initialization. Here's what happens:\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: Your existing layer to compress and adapt\n+    /// - rank: LoRA adapter size (lower = more efficient)\n+    /// - alpha: LoRA strength\n+    /// - numAlternatingIterations: How many times to optimize initialization (3-5 is good)\n+    /// - quantizationType: NF4 recommended for best results\n+    /// - Other parameters: Same as QLoRA\n+    ///\n+    /// Initialization process (this happens once):\n+    /// 1. Look at your original weights\n+    /// 2. Try compressing them\n+    /// 3. See what errors compression creates\n+    /// 4. Adjust LoRA to fix those errors\n+    /// 5. Repeat steps 2-4 several times to find the best combination\n+    /// 6. Save the optimized compression and LoRA\n+    ///\n+    /// This extra work during initialization pays off with better training results!\n+    /// </para>\n+    /// </remarks>\n+    public LoftQAdapter(\n+        ILayer<T> baseLayer,\n+        int rank,\n+        double alpha = -1,\n+        int numAlternatingIterations = 5,\n+        QuantizationType quantizationType = QuantizationType.NF4,\n+        bool useDoubleQuantization = true,\n+        int quantizationBlockSize = 64,\n+        bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        // Validate base layer\n+        if (baseLayer.GetInputShape().Length != 1 || baseLayer.GetOutputShape().Length != 1)\n+        {\n+            throw new ArgumentException(\"LoftQAdapter only supports layers with 1D input/output shapes (Dense/FullyConnected layers)\", nameof(baseLayer));\n+        }\n+\n+        if (quantizationBlockSize <= 0)\n+        {\n+            throw new ArgumentException(\"Quantization block size must be positive\", nameof(quantizationBlockSize));\n+        }\n+\n+        if (numAlternatingIterations < 1)\n+        {\n+            throw new ArgumentException(\"Number of alternating iterations must be at least 1\", nameof(numAlternatingIterations));\n+        }\n+\n+        _quantizationType = quantizationType;\n+        _useDoubleQuantization = useDoubleQuantization;\n+        _quantizationBlockSize = quantizationBlockSize;\n+        _numAlternatingIterations = numAlternatingIterations;\n+\n+        // Perform LoftQ initialization with alternating optimization\n+        PerformLoftQInitialization();\n+    }\n+\n+    /// <summary>\n+    /// Performs LoftQ initialization using alternating optimization between quantization and LoRA.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This is the core LoftQ algorithm:\n+    /// 1. Extract base layer weights W\n+    /// 2. For K iterations:\n+    ///    a. Quantize current weights: Q = Quantize(W_current)\n+    ///    b. Compute residual: R = W - Q\n+    ///    c. Decompose residual via SVD: R ‚âà U * S * V^T\n+    ///    d. Set LoRA matrices: A = V^T[:rank, :], B = U[:, :rank] * S[:rank, :rank]\n+    ///    e. Update: W_current = Q + A * B (scaled by alpha/rank)\n+    /// 3. Store final Q as quantized weights, final A and B as LoRA parameters\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This is where the \"smart initialization\" happens.\n+    ///\n+    /// The algorithm:\n+    /// - Start with your original weights W\n+    /// - Repeat several times:\n+    ///   1. Compress W to get Q (quantized version)\n+    ///   2. Calculate error: R = W - Q (what we lost in compression)\n+    ///   3. Use math (SVD) to find the best LoRA matrices that approximate R\n+    ///   4. Update W = Q + LoRA (compressed + correction)\n+    ///   5. Go back to step 1 with the new W\n+    ///\n+    /// Why alternate?\n+    /// - Each iteration, LoRA learns to fix compression errors better\n+    /// - Each iteration, compression is done knowing LoRA will help\n+    /// - They work together to find the best combination\n+    ///\n+    /// Result: LoRA starts already knowing how to compensate for compression!\n+    /// </para>\n+    /// </remarks>\n+    private void PerformLoftQInitialization()\n+    {\n+        // Get base layer parameters\n+        Vector<T> baseParams = _baseLayer.GetParameters();\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        int weightCount = inputSize * outputSize;\n+\n+        // Extract weights (shape: [outputSize, inputSize])\n+        Matrix<T> weights = new Matrix<T>(outputSize, inputSize);\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                weights[i, j] = baseParams[i * inputSize + j];\n+            }\n+        }\n+\n+        // Store original weights for alternating optimization\n+        Matrix<T> currentWeights = weights.Clone();\n+\n+        // Alternating optimization loop\n+        for (int iter = 0; iter < _numAlternatingIterations; iter++)\n+        {\n+            // Step 1: Quantize current weights\n+            QuantizeWeights(currentWeights);\n+\n+            // Step 2: Dequantize to get Q\n+            Matrix<T> quantizedWeights = DequantizeWeights();\n+\n+            // Step 3: Compute residual R = W - Q\n+            Matrix<T> residual = new Matrix<T>(outputSize, inputSize);\n+            for (int i = 0; i < outputSize; i++)\n+            {\n+                for (int j = 0; j < inputSize; j++)\n+                {\n+                    residual[i, j] = NumOps.Subtract(weights[i, j], quantizedWeights[i, j]);\n+                }\n+            }\n+\n+            // Step 4: Decompose residual via SVD and update LoRA matrices\n+            UpdateLoRAFromResidual(residual);\n+\n+            // Step 5: Update current weights = Q + LoRA (for next iteration)\n+            Matrix<T> loraWeights = _loraLayer.MergeWeights();\n+            for (int i = 0; i < outputSize; i++)\n+            {\n+                for (int j = 0; j < inputSize; j++)\n+                {\n+                    currentWeights[i, j] = NumOps.Add(quantizedWeights[i, j], loraWeights[i, j]);\n+                }\n+            }\n+        }\n+\n+        // Final quantization (already done in last iteration)\n+        // LoRA parameters are also set from last iteration\n+\n+        // Apply double quantization if enabled\n+        if (_useDoubleQuantization)\n+        {\n+            DoubleQuantizeScales();\n+        }\n+\n+        // Update parameter vector\n+        UpdateParametersFromLayers();\n+    }\n+\n+    /// <summary>\n+    /// Updates LoRA matrices A and B to minimize the residual via SVD decomposition.\n+    /// </summary>\n+    /// <param name=\"residual\">The residual matrix to decompose (W - Q).</param>\n+    /// <remarks>\n+    /// <para>\n+    /// Uses SVD to decompose the residual and extract low-rank approximation:\n+    /// - Compute SVD: R = U * S * V^T\n+    /// - Take rank-r approximation: R_approx = U[:, :r] * S[:r, :r] * V^T[:r, :]\n+    /// - Set LoRA matrices: B = U[:, :r] * sqrt(S[:r, :r]), A = sqrt(S[:r, :r]) * V^T[:r, :]\n+    /// - This ensures BA ‚âà R with minimal error in Frobenius norm\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This uses a mathematical technique called SVD to find the best\n+    /// LoRA matrices that approximate the compression error.\n+    ///\n+    /// Think of it like:\n+    /// - You have a big error matrix (difference between original and compressed)\n+    /// - SVD finds the \"most important patterns\" in that error\n+    /// - We keep only the top 'rank' patterns (low-rank approximation)\n+    /// - Split these patterns into two smaller matrices A and B\n+    /// - When multiplied, A * B ‚âà error, but using much fewer parameters!\n+    ///\n+    /// This is mathematically optimal - no other rank-r approximation can do better.\n+    /// </para>\n+    /// </remarks>\n+    private void UpdateLoRAFromResidual(Matrix<T> residual)\n+    {\n+        int outputSize = residual.Rows;\n+        int inputSize = residual.Columns;\n+        int rank = _loraLayer.Rank;\n+\n+        // Compute SVD of residual matrix\n+        // For efficiency, we'll use a simplified approach:\n+        // 1. Compute R * R^T (smaller if outputSize < inputSize)\n+        // 2. Get eigenvalues/eigenvectors\n+        // 3. Construct low-rank approximation\n+\n+        // Compute R * R^T\n+        Matrix<T> rrt = residual.Multiply(residual.Transpose());\n+\n+        // Get eigenvalues and eigenvectors (we'll use power iteration for top-k)\n+        // For a production implementation, use a proper SVD library\n+        // Here we'll use a simplified approach with the full matrices\n+\n+        // Simplified: Just use the residual directly with truncation\n+        // Extract top-rank components\n+\n+        Vector<T> loraParams = _loraLayer.GetParameters();\n+        int aRows = rank;\n+        int aCols = inputSize;\n+        int bRows = outputSize;\n+        int bCols = rank;\n+\n+        // Initialize A and B from truncated residual\n+        // A: [rank, inputSize] - initialized from top rank rows of residual\n+        // B: [outputSize, rank] - initialized to produce low-rank approximation\n+\n+        // Simple initialization: Use first 'rank' singular vectors\n+        // For proper SVD, we'd compute U, S, V and use:\n+        // B = U[:, :rank] * sqrt(S[:rank, :rank])\n+        // A = sqrt(S[:rank, :rank]) * V^T[:rank, :]\n+\n+        // Simplified approach: Initialize A from residual rows, B to scale appropriately\n+        int idx = 0;\n+\n+        // Set A matrix in LoRA parameters (first part)\n+        double scaleFactor = 1.0 / Math.Sqrt(rank); // Simple scaling\n+        for (int i = 0; i < aRows; i++)\n+        {\n+            for (int j = 0; j < aCols; j++)\n+            {\n+                // Take patterns from residual with scaling\n+                int resRow = i % outputSize;\n+                loraParams[idx++] = NumOps.Multiply(residual[resRow, j], NumOps.FromDouble(scaleFactor));\n+            }\n+        }\n+\n+        // Set B matrix in LoRA parameters (second part)\n+        for (int i = 0; i < bRows; i++)\n+        {\n+            for (int j = 0; j < bCols; j++)\n+            {\n+                // Initialize B to create rank-r approximation\n+                T value = NumOps.Zero;\n+                for (int k = 0; k < inputSize; k++)\n+                {\n+                    int aRow = j;\n+                    T aVal = loraParams[aRow * aCols + k];\n+                    value = NumOps.Add(value, NumOps.Multiply(residual[i, k], aVal));\n+                }\n+                loraParams[idx++] = NumOps.Multiply(value, NumOps.FromDouble(scaleFactor));\n+            }\n+        }\n+\n+        // Update LoRA layer with new parameters\n+        _loraLayer.SetParameters(loraParams);\n+    }\n+\n+    /// <summary>\n+    /// Quantizes a weight matrix to 4-bit precision.\n+    /// </summary>\n+    /// <param name=\"weights\">The weight matrix to quantize.</param>\n+    private void QuantizeWeights(Matrix<T> weights)\n+    {\n+        int outputSize = weights.Rows;\n+        int inputSize = weights.Columns;\n+        int weightCount = outputSize * inputSize;\n+\n+        // Flatten weights for quantization\n+        T[] flatWeights = new T[weightCount];\n+        int idx = 0;\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                flatWeights[idx++] = weights[i, j];\n+            }\n+        }\n+\n+        // Quantize in blocks\n+        int numBlocks = (weightCount + _quantizationBlockSize - 1) / _quantizationBlockSize;\n+        _quantizedWeights = new byte[(weightCount + 1) / 2]; // 2 values per byte\n+        _quantizationScales = new T[numBlocks];\n+        _quantizationZeroPoints = new T[numBlocks];\n+\n+        for (int blockIdx = 0; blockIdx < numBlocks; blockIdx++)\n+        {\n+            int blockStart = blockIdx * _quantizationBlockSize;\n+            int blockEnd = Math.Min(blockStart + _quantizationBlockSize, weightCount);\n+\n+            // Find min/max for this block\n+            T minVal = flatWeights[blockStart];\n+            T maxVal = flatWeights[blockStart];\n+            for (int i = blockStart + 1; i < blockEnd; i++)\n+            {\n+                if (NumOps.LessThan(flatWeights[i], minVal))\n+                    minVal = flatWeights[i];\n+                if (NumOps.GreaterThan(flatWeights[i], maxVal))\n+                    maxVal = flatWeights[i];\n+            }\n+\n+            // Compute scale and zero point\n+            T range = NumOps.Subtract(maxVal, minVal);\n+            T scale = NumOps.Divide(range, NumOps.FromDouble(15.0));\n+            T zeroPoint = minVal;\n+\n+            _quantizationScales[blockIdx] = scale;\n+            _quantizationZeroPoints[blockIdx] = zeroPoint;\n+\n+            // Quantize values in this block\n+            for (int i = blockStart; i < blockEnd; i++)\n+            {\n+                byte quantizedValue = QuantizeValue(flatWeights[i], scale, zeroPoint);\n+\n+                // Pack two 4-bit values per byte\n+                int byteIdx = i / 2;\n+                if (i % 2 == 0)\n+                {\n+                    _quantizedWeights[byteIdx] = (byte)(quantizedValue & 0x0F);\n+                }\n+                else\n+                {\n+                    _quantizedWeights[byteIdx] |= (byte)((quantizedValue & 0x0F) << 4);\n+                }\n+            }\n+        }","path":"src/LoRA/Adapters/LoftQAdapter.cs","commit_id":"fa815036ee570162056ba6aca773f34c80f36948","original_commit_id":"8d5a1b6564c1a2bc1b334f1863e6003a6fd142b2","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Guard zero-range quantization blocks to avoid runtime faults**\n\nLine 543 currently computes `scale = range / 15` even when `range` collapses to zero (any block with constant weights‚Äîcommon for freshly reset dense layers). The very next call to `QuantizeValue` divides by `scale`, which produces `‚àû/NaN` and triggers an `OverflowException` when we round/cast the value. That happens before training ever starts, so the adapter cannot be constructed for perfectly valid models. Please ensure we fall back to a non-zero scale when the block variance is zero (or underflows to zero).\n\n```diff\n-            T range = NumOps.Subtract(maxVal, minVal);\n-            T scale = NumOps.Divide(range, NumOps.FromDouble(15.0));\n-            T zeroPoint = minVal;\n+            T range = NumOps.Subtract(maxVal, minVal);\n+            double rangeAbs = Math.Abs(Convert.ToDouble(range));\n+            T zeroPoint = minVal;\n+            T scale;\n+            if (rangeAbs < 1e-12)\n+            {\n+                scale = NumOps.FromDouble(1.0);\n+            }\n+            else\n+            {\n+                scale = NumOps.Divide(range, NumOps.FromDouble(15.0));\n+                if (Convert.ToDouble(scale) == 0d)\n+                {\n+                    scale = NumOps.FromDouble(1.0);\n+                }\n+            }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n            T range = NumOps.Subtract(maxVal, minVal);\n            double rangeAbs = Math.Abs(Convert.ToDouble(range));\n            T zeroPoint = minVal;\n            T scale;\n            if (rangeAbs < 1e-12)\n            {\n                scale = NumOps.FromDouble(1.0);\n            }\n            else\n            {\n                scale = NumOps.Divide(range, NumOps.FromDouble(15.0));\n                if (Convert.ToDouble(scale) == 0d)\n                {\n                    scale = NumOps.FromDouble(1.0);\n                }\n            }\n\n            _quantizationScales[blockIdx] = scale;\n            _quantizationZeroPoints[blockIdx] = zeroPoint;\n\n            // Quantize values in this block\n            for (int i = blockStart; i < blockEnd; i++)\n            {\n                byte quantizedValue = QuantizeValue(flatWeights[i], scale, zeroPoint);\n\n                // Pack two 4-bit values per byte\n                int byteIdx = i / 2;\n                if (i % 2 == 0)\n                {\n                    _quantizedWeights[byteIdx] = (byte)(quantizedValue & 0x0F);\n                }\n                else\n                {\n                    _quantizedWeights[byteIdx] |= (byte)((quantizedValue & 0x0F) << 4);\n                }\n            }\n        }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T03:02:27Z","updated_at":"2025-11-02T03:02:31Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140265","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140265"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140265"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140265/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":543,"original_start_line":543,"start_side":"RIGHT","line":566,"original_line":566,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":566,"position":566,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140270","pull_request_review_id":3408046172,"id":2484140270,"node_id":"PRRC_kwDOKSXUF86UEPju","diff_hunk":"@@ -0,0 +1,903 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// LoHa (Low-Rank Hadamard Product Adaptation) adapter for parameter-efficient fine-tuning.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// LoHa uses element-wise Hadamard products (‚äô) instead of matrix multiplication for adaptation.\n+/// Instead of computing ŒîW = B * A like standard LoRA, LoHa computes:\n+/// ŒîW = sum over rank of (A[i] ‚äô B[i])\n+///\n+/// This formulation can capture element-wise patterns that matrix multiplication may miss,\n+/// making it particularly effective for:\n+/// - Convolutional layers (local spatial patterns)\n+/// - Element-wise transformations\n+/// - Fine-grained weight adjustments\n+/// </para>\n+/// <para><b>Mathematical Formulation:</b>\n+///\n+/// Standard LoRA: ŒîW = B * A where B is rank√óoutput, A is input√órank\n+/// LoHa: ŒîW = Œ£(A[i] ‚äô B[i]) where A[i] and B[i] are both input√óoutput\n+///\n+/// The Hadamard product (‚äô) performs element-wise multiplication, allowing each element\n+/// of the weight matrix to be adjusted independently across the rank dimensions.\n+/// </para>\n+/// <para><b>For Beginners:</b> LoHa is a variant of LoRA that uses element-wise multiplication\n+/// instead of matrix multiplication. Think of it this way:\n+///\n+/// - Standard LoRA: Learns \"row and column patterns\" that combine via matrix multiply\n+/// - LoHa: Learns \"pixel-by-pixel patterns\" that combine via element-wise multiply\n+///\n+/// LoHa is especially good when:\n+/// 1. You need to capture local, element-wise patterns (like in images)\n+/// 2. The weight matrix has spatial structure (like convolutional filters)\n+/// 3. You want each weight to be adjusted somewhat independently\n+///\n+/// Trade-offs compared to LoRA:\n+/// - More parameters: Both A and B must be full-sized (input√óoutput) per rank dimension\n+/// - Different expressiveness: Better for element-wise patterns, different from matrix patterns\n+/// - Better for CNNs: The element-wise nature matches convolutional structure better\n+///\n+/// Example: A 100√ó100 weight matrix with rank=8\n+/// - Standard LoRA: 8√ó100 + 100√ó8 = 1,600 parameters\n+/// - LoHa: 8√ó(100√ó100) + 8√ó(100√ó100) = 160,000 parameters\n+///\n+/// Despite more parameters, LoHa is still far more efficient than full fine-tuning (10,000 params).","path":"src/LoRA/Adapters/LoHaAdapter.cs","commit_id":"fa815036ee570162056ba6aca773f34c80f36948","original_commit_id":"8d5a1b6564c1a2bc1b334f1863e6003a6fd142b2","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Critical documentation error: LoHa is LESS efficient than full fine-tuning in this example.**\n\nThe example claims LoHa with 160,000 parameters is \"far more efficient\" than full fine-tuning with 10,000 parameters. This is mathematically incorrect: 160,000 > 10,000, meaning LoHa uses 16√ó more parameters than full fine-tuning for a 100√ó100 weight matrix.\n\nThis contradicts the entire premise of parameter-efficient fine-tuning. Either:\n1. The parameter count calculation is wrong\n2. The example is incorrect and should use a different matrix size where LoHa is actually beneficial\n3. The LoHa formulation itself is flawed\n\nFor LoHa to be efficient, you'd need very small rank values or the base weight matrix would need to be much larger than 100√ó100.\n\n\n\nConsider using an example where LoHa is actually efficient, such as:\n- 1000√ó1000 matrix with rank=4: LoHa uses 2√ó4√ó1000√ó1000 = 8M parameters vs 1M for full fine-tuning (still 8√ó worse)\n\nOr reconsider whether this Hadamard-based approach is correctly formulated. Standard LoHa implementations typically use different decompositions that maintain efficiency.\n\n```diff\n-/// Example: A 100√ó100 weight matrix with rank=8\n-/// - Standard LoRA: 8√ó100 + 100√ó8 = 1,600 parameters\n-/// - LoHa: 8√ó(100√ó100) + 8√ó(100√ó100) = 160,000 parameters\n-///\n-/// Despite more parameters, LoHa is still far more efficient than full fine-tuning (10,000 params).\n+/// Example: A 100√ó100 weight matrix with rank=8\n+/// - Standard LoRA: 8√ó100 + 100√ó8 = 1,600 parameters\n+/// - LoHa: 8√ó(100√ó100) + 8√ó(100√ó100) = 160,000 parameters\n+/// - Full fine-tuning: 100√ó100 = 10,000 parameters\n+///\n+/// Note: In this example, LoHa uses 16√ó MORE parameters than full fine-tuning.\n+/// LoHa is only beneficial for very large matrices or very small ranks.\n```\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/LoRA/Adapters/LoHaAdapter.cs around lines 45 to 49, the documentation\nexample is incorrect: it computes LoHa as 160,000 parameters for a 100√ó100\nmatrix with rank=8 and then incorrectly claims this is \"far more efficient\" than\nfull fine-tuning with 10,000 parameters; update the text to either (A) correct\nthe arithmetic and remove/replace the misleading claim (e.g., state ‚ÄúLoHa:\n160,000 vs full fine-tuning: 10,000 ‚Äî LoHa uses more parameters in this case‚Äù),\nor (B) replace the example with a valid scenario where LoHa is actually\nparameter-efficient (provide correct formula and numbers), and ensure the\ndescription explains the comparison metric used (parameter counts and\nassumptions about r, m, n) so the example is mathematically accurate.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T03:02:27Z","updated_at":"2025-11-02T03:02:31Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140270","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140270"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140270"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140270/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":45,"original_start_line":45,"start_side":"RIGHT","line":49,"original_line":49,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":49,"position":49,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140275","pull_request_review_id":3408046172,"id":2484140275,"node_id":"PRRC_kwDOKSXUF86UEPjz","diff_hunk":"@@ -0,0 +1,903 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// LoHa (Low-Rank Hadamard Product Adaptation) adapter for parameter-efficient fine-tuning.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// LoHa uses element-wise Hadamard products (‚äô) instead of matrix multiplication for adaptation.\n+/// Instead of computing ŒîW = B * A like standard LoRA, LoHa computes:\n+/// ŒîW = sum over rank of (A[i] ‚äô B[i])\n+///\n+/// This formulation can capture element-wise patterns that matrix multiplication may miss,\n+/// making it particularly effective for:\n+/// - Convolutional layers (local spatial patterns)\n+/// - Element-wise transformations\n+/// - Fine-grained weight adjustments\n+/// </para>\n+/// <para><b>Mathematical Formulation:</b>\n+///\n+/// Standard LoRA: ŒîW = B * A where B is rank√óoutput, A is input√órank\n+/// LoHa: ŒîW = Œ£(A[i] ‚äô B[i]) where A[i] and B[i] are both input√óoutput\n+///\n+/// The Hadamard product (‚äô) performs element-wise multiplication, allowing each element\n+/// of the weight matrix to be adjusted independently across the rank dimensions.\n+/// </para>\n+/// <para><b>For Beginners:</b> LoHa is a variant of LoRA that uses element-wise multiplication\n+/// instead of matrix multiplication. Think of it this way:\n+///\n+/// - Standard LoRA: Learns \"row and column patterns\" that combine via matrix multiply\n+/// - LoHa: Learns \"pixel-by-pixel patterns\" that combine via element-wise multiply\n+///\n+/// LoHa is especially good when:\n+/// 1. You need to capture local, element-wise patterns (like in images)\n+/// 2. The weight matrix has spatial structure (like convolutional filters)\n+/// 3. You want each weight to be adjusted somewhat independently\n+///\n+/// Trade-offs compared to LoRA:\n+/// - More parameters: Both A and B must be full-sized (input√óoutput) per rank dimension\n+/// - Different expressiveness: Better for element-wise patterns, different from matrix patterns\n+/// - Better for CNNs: The element-wise nature matches convolutional structure better\n+///\n+/// Example: A 100√ó100 weight matrix with rank=8\n+/// - Standard LoRA: 8√ó100 + 100√ó8 = 1,600 parameters\n+/// - LoHa: 8√ó(100√ó100) + 8√ó(100√ó100) = 160,000 parameters\n+///\n+/// Despite more parameters, LoHa is still far more efficient than full fine-tuning (10,000 params).\n+/// </para>\n+/// </remarks>\n+public class LoHaAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Low-rank matrices A with dimensions (rank, inputSize, outputSize).\n+    /// Each A[i] is a full-sized matrix for the i-th rank dimension.\n+    /// </summary>\n+    private readonly Matrix<T>[] _matricesA;\n+\n+    /// <summary>\n+    /// Low-rank matrices B with dimensions (rank, inputSize, outputSize).\n+    /// Each B[i] is a full-sized matrix for the i-th rank dimension.\n+    /// </summary>\n+    private readonly Matrix<T>[] _matricesB;\n+\n+    /// <summary>\n+    /// Gradients for matrices A computed during backpropagation.\n+    /// </summary>\n+    private Matrix<T>[]? _matricesAGradient;\n+\n+    /// <summary>\n+    /// Gradients for matrices B computed during backpropagation.\n+    /// </summary>\n+    private Matrix<T>[]? _matricesBGradient;\n+\n+    /// <summary>\n+    /// Stored input from the forward pass, needed for gradient computation.\n+    /// </summary>\n+    private Tensor<T>? _lastInput;\n+\n+    /// <summary>\n+    /// Stored base layer output from the forward pass.\n+    /// </summary>\n+    private Tensor<T>? _lastBaseOutput;\n+\n+    /// <summary>\n+    /// Computed scaling factor (alpha / rank) used during forward pass.\n+    /// </summary>\n+    private readonly T _scaling;\n+\n+    /// <summary>\n+    /// Initializes a new LoHa adapter wrapping an existing layer.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with LoHa.</param>\n+    /// <param name=\"rank\">The rank of the low-rank decomposition.</param>\n+    /// <param name=\"alpha\">The LoHa scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when the base layer doesn't have 1D input/output shapes.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This creates a LoHa adapter for any layer with 1D input/output.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to make more efficient to fine-tune\n+    /// - rank: How many element-wise patterns to learn (more = more flexibility, more parameters)\n+    /// - alpha: How strong the LoHa adaptation is (typically same as rank)\n+    /// - freezeBaseLayer: Whether to lock the original layer's weights (usually true for efficiency)\n+    ///\n+    /// The adapter creates 2√órank full-sized matrices (A and B for each rank dimension),\n+    /// which are combined using element-wise Hadamard products during forward/backward passes.\n+    /// </para>\n+    /// </remarks>\n+    public LoHaAdapter(ILayer<T> baseLayer, int rank, double alpha = -1, bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        // Validate base layer has single-dimensional input/output\n+        if (baseLayer.GetInputShape().Length != 1 || baseLayer.GetOutputShape().Length != 1)\n+        {\n+            throw new ArgumentException(\"LoHaAdapter only supports layers with 1D input/output shapes\", nameof(baseLayer));\n+        }\n+\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+\n+        // Calculate scaling\n+        _scaling = NumOps.Divide(_loraLayer.Alpha, NumOps.FromDouble(rank));\n+\n+        // Initialize LoHa matrices (rank sets of full-sized matrices)\n+        _matricesA = new Matrix<T>[rank];\n+        _matricesB = new Matrix<T>[rank];\n+\n+        for (int r = 0; r < rank; r++)\n+        {\n+            // Initialize A[r] with random values (Gaussian with std = 1/sqrt(rank))\n+            _matricesA[r] = new Matrix<T>(inputSize, outputSize);\n+            T stddev = NumOps.Sqrt(NumOps.Divide(NumOps.One, NumOps.FromDouble(rank)));\n+            for (int i = 0; i < inputSize; i++)\n+            {\n+                for (int j = 0; j < outputSize; j++)\n+                {\n+                    // Box-Muller transform for Gaussian random numbers\n+                    double u1 = Random.NextDouble();\n+                    double u2 = Random.NextDouble();\n+                    double randStdNormal = Math.Sqrt(-2.0 * Math.Log(u1)) * Math.Sin(2.0 * Math.PI * u2);\n+                    _matricesA[r][i, j] = NumOps.Multiply(NumOps.FromDouble(randStdNormal), stddev);\n+                }\n+            }\n+\n+            // Initialize B[r] to zero (so LoHa has no effect initially)\n+            _matricesB[r] = new Matrix<T>(inputSize, outputSize);\n+            for (int i = 0; i < inputSize; i++)\n+            {\n+                for (int j = 0; j < outputSize; j++)\n+                {\n+                    _matricesB[r][i, j] = NumOps.Zero;\n+                }\n+            }\n+        }\n+\n+        // Initialize parameter vector\n+        Parameters = new Vector<T>(ParameterCount);\n+        UpdateParametersFromMatrices();\n+    }\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters.\n+    /// </summary>\n+    /// <remarks>\n+    /// LoHa has 2 * rank * inputSize * outputSize parameters (A and B matrices for each rank).\n+    /// This is more than standard LoRA but still far less than full fine-tuning.\n+    /// </remarks>\n+    public override int ParameterCount\n+    {\n+        get\n+        {\n+            int inputSize = GetInputShape()[0];\n+            int outputSize = GetOutputShape()[0];\n+            int lohaParams = 2 * Rank * inputSize * outputSize;\n+            return _freezeBaseLayer ? lohaParams : (_baseLayer.ParameterCount + lohaParams);\n+        }\n+    }","path":"src/LoRA/Adapters/LoHaAdapter.cs","commit_id":"fa815036ee570162056ba6aca773f34c80f36948","original_commit_id":"8d5a1b6564c1a2bc1b334f1863e6003a6fd142b2","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**ParameterCount is mathematically correct but confirms efficiency concerns.**\n\nThe calculation `2 * Rank * inputSize * outputSize` is accurate for storing rank sets of full-sized [input, output] matrices for both A and B.\n\nHowever, this confirms that LoHa can easily exceed full fine-tuning parameter counts. For a 100√ó100 matrix with rank=8, this yields 160,000 parameters vs 10,000 for full fine-tuning‚Äîa 16√ó increase, not a reduction.\n\nThis raises questions about whether the LoHa formulation in this implementation is correct or beneficial.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/LoRA/Adapters/LoHaAdapter.cs around lines 172 to 181, the LoHa parameter\ncount is computed as 2 * Rank * inputSize * outputSize which is incorrect and\nfar larger than expected; replace that calculation with the correct low‚Äërank\nadapter formula (total LoHa params = Rank * (inputSize + outputSize)) so the\nadapter counts A and B as rank√óinput and output√órank rather than full\ninput√óoutput matrices, then keep the existing _freezeBaseLayer conditional\nreturn but use the corrected lohaParams value and add a small unit test or\nassertion that for known dims (e.g., 100√ó100, rank=8) the LoHa params equal\n8*(100+100)=1600 to validate the fix.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T03:02:27Z","updated_at":"2025-11-02T03:02:31Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140275","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140275"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140275"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140275/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":172,"original_start_line":172,"start_side":"RIGHT","line":181,"original_line":181,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":181,"position":181,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140278","pull_request_review_id":3408046172,"id":2484140278,"node_id":"PRRC_kwDOKSXUF86UEPj2","diff_hunk":"@@ -0,0 +1,903 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// LoHa (Low-Rank Hadamard Product Adaptation) adapter for parameter-efficient fine-tuning.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// LoHa uses element-wise Hadamard products (‚äô) instead of matrix multiplication for adaptation.\n+/// Instead of computing ŒîW = B * A like standard LoRA, LoHa computes:\n+/// ŒîW = sum over rank of (A[i] ‚äô B[i])\n+///\n+/// This formulation can capture element-wise patterns that matrix multiplication may miss,\n+/// making it particularly effective for:\n+/// - Convolutional layers (local spatial patterns)\n+/// - Element-wise transformations\n+/// - Fine-grained weight adjustments\n+/// </para>\n+/// <para><b>Mathematical Formulation:</b>\n+///\n+/// Standard LoRA: ŒîW = B * A where B is rank√óoutput, A is input√órank\n+/// LoHa: ŒîW = Œ£(A[i] ‚äô B[i]) where A[i] and B[i] are both input√óoutput\n+///\n+/// The Hadamard product (‚äô) performs element-wise multiplication, allowing each element\n+/// of the weight matrix to be adjusted independently across the rank dimensions.\n+/// </para>\n+/// <para><b>For Beginners:</b> LoHa is a variant of LoRA that uses element-wise multiplication\n+/// instead of matrix multiplication. Think of it this way:\n+///\n+/// - Standard LoRA: Learns \"row and column patterns\" that combine via matrix multiply\n+/// - LoHa: Learns \"pixel-by-pixel patterns\" that combine via element-wise multiply\n+///\n+/// LoHa is especially good when:\n+/// 1. You need to capture local, element-wise patterns (like in images)\n+/// 2. The weight matrix has spatial structure (like convolutional filters)\n+/// 3. You want each weight to be adjusted somewhat independently\n+///\n+/// Trade-offs compared to LoRA:\n+/// - More parameters: Both A and B must be full-sized (input√óoutput) per rank dimension\n+/// - Different expressiveness: Better for element-wise patterns, different from matrix patterns\n+/// - Better for CNNs: The element-wise nature matches convolutional structure better\n+///\n+/// Example: A 100√ó100 weight matrix with rank=8\n+/// - Standard LoRA: 8√ó100 + 100√ó8 = 1,600 parameters\n+/// - LoHa: 8√ó(100√ó100) + 8√ó(100√ó100) = 160,000 parameters\n+///\n+/// Despite more parameters, LoHa is still far more efficient than full fine-tuning (10,000 params).\n+/// </para>\n+/// </remarks>\n+public class LoHaAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Low-rank matrices A with dimensions (rank, inputSize, outputSize).\n+    /// Each A[i] is a full-sized matrix for the i-th rank dimension.\n+    /// </summary>\n+    private readonly Matrix<T>[] _matricesA;\n+\n+    /// <summary>\n+    /// Low-rank matrices B with dimensions (rank, inputSize, outputSize).\n+    /// Each B[i] is a full-sized matrix for the i-th rank dimension.\n+    /// </summary>\n+    private readonly Matrix<T>[] _matricesB;\n+\n+    /// <summary>\n+    /// Gradients for matrices A computed during backpropagation.\n+    /// </summary>\n+    private Matrix<T>[]? _matricesAGradient;\n+\n+    /// <summary>\n+    /// Gradients for matrices B computed during backpropagation.\n+    /// </summary>\n+    private Matrix<T>[]? _matricesBGradient;\n+\n+    /// <summary>\n+    /// Stored input from the forward pass, needed for gradient computation.\n+    /// </summary>\n+    private Tensor<T>? _lastInput;\n+\n+    /// <summary>\n+    /// Stored base layer output from the forward pass.\n+    /// </summary>\n+    private Tensor<T>? _lastBaseOutput;\n+\n+    /// <summary>\n+    /// Computed scaling factor (alpha / rank) used during forward pass.\n+    /// </summary>\n+    private readonly T _scaling;\n+\n+    /// <summary>\n+    /// Initializes a new LoHa adapter wrapping an existing layer.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with LoHa.</param>\n+    /// <param name=\"rank\">The rank of the low-rank decomposition.</param>\n+    /// <param name=\"alpha\">The LoHa scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when the base layer doesn't have 1D input/output shapes.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This creates a LoHa adapter for any layer with 1D input/output.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to make more efficient to fine-tune\n+    /// - rank: How many element-wise patterns to learn (more = more flexibility, more parameters)\n+    /// - alpha: How strong the LoHa adaptation is (typically same as rank)\n+    /// - freezeBaseLayer: Whether to lock the original layer's weights (usually true for efficiency)\n+    ///\n+    /// The adapter creates 2√órank full-sized matrices (A and B for each rank dimension),\n+    /// which are combined using element-wise Hadamard products during forward/backward passes.\n+    /// </para>\n+    /// </remarks>\n+    public LoHaAdapter(ILayer<T> baseLayer, int rank, double alpha = -1, bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        // Validate base layer has single-dimensional input/output\n+        if (baseLayer.GetInputShape().Length != 1 || baseLayer.GetOutputShape().Length != 1)\n+        {\n+            throw new ArgumentException(\"LoHaAdapter only supports layers with 1D input/output shapes\", nameof(baseLayer));\n+        }\n+\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+\n+        // Calculate scaling\n+        _scaling = NumOps.Divide(_loraLayer.Alpha, NumOps.FromDouble(rank));\n+\n+        // Initialize LoHa matrices (rank sets of full-sized matrices)\n+        _matricesA = new Matrix<T>[rank];\n+        _matricesB = new Matrix<T>[rank];\n+\n+        for (int r = 0; r < rank; r++)\n+        {\n+            // Initialize A[r] with random values (Gaussian with std = 1/sqrt(rank))\n+            _matricesA[r] = new Matrix<T>(inputSize, outputSize);\n+            T stddev = NumOps.Sqrt(NumOps.Divide(NumOps.One, NumOps.FromDouble(rank)));\n+            for (int i = 0; i < inputSize; i++)\n+            {\n+                for (int j = 0; j < outputSize; j++)\n+                {\n+                    // Box-Muller transform for Gaussian random numbers\n+                    double u1 = Random.NextDouble();\n+                    double u2 = Random.NextDouble();\n+                    double randStdNormal = Math.Sqrt(-2.0 * Math.Log(u1)) * Math.Sin(2.0 * Math.PI * u2);\n+                    _matricesA[r][i, j] = NumOps.Multiply(NumOps.FromDouble(randStdNormal), stddev);\n+                }\n+            }\n+\n+            // Initialize B[r] to zero (so LoHa has no effect initially)\n+            _matricesB[r] = new Matrix<T>(inputSize, outputSize);\n+            for (int i = 0; i < inputSize; i++)\n+            {\n+                for (int j = 0; j < outputSize; j++)\n+                {\n+                    _matricesB[r][i, j] = NumOps.Zero;\n+                }\n+            }\n+        }\n+\n+        // Initialize parameter vector\n+        Parameters = new Vector<T>(ParameterCount);\n+        UpdateParametersFromMatrices();\n+    }\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters.\n+    /// </summary>\n+    /// <remarks>\n+    /// LoHa has 2 * rank * inputSize * outputSize parameters (A and B matrices for each rank).\n+    /// This is more than standard LoRA but still far less than full fine-tuning.\n+    /// </remarks>\n+    public override int ParameterCount\n+    {\n+        get\n+        {\n+            int inputSize = GetInputShape()[0];\n+            int outputSize = GetOutputShape()[0];\n+            int lohaParams = 2 * Rank * inputSize * outputSize;\n+            return _freezeBaseLayer ? lohaParams : (_baseLayer.ParameterCount + lohaParams);\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass through both base layer and LoHa adaptation.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Sum of base layer output and LoHa delta (computed via Hadamard products).</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The forward pass computes:\n+    /// 1. base_output = base_layer(input)\n+    /// 2. loha_delta = sum over rank of (input * A[i] ‚äô B[i]) * scaling\n+    /// 3. output = base_output + loha_delta\n+    ///\n+    /// The Hadamard product (‚äô) multiplies corresponding elements, allowing element-wise adaptations.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This runs the input through the original layer and adds a correction.\n+    ///\n+    /// The correction is computed by:\n+    /// 1. Transforming input through each A[i] matrix (one per rank dimension)\n+    /// 2. Multiplying element-wise with corresponding B[i] matrix (Hadamard product)\n+    /// 3. Summing all rank contributions together\n+    /// 4. Scaling by alpha/rank\n+    ///\n+    /// This element-wise approach lets LoHa learn fine-grained adjustments to each weight independently.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        _lastInput = input.Clone();\n+\n+        // Forward through base layer\n+        Tensor<T> baseOutput = _baseLayer.Forward(input);\n+        _lastBaseOutput = baseOutput.Clone();\n+\n+        // Compute LoHa delta using Hadamard products\n+        Tensor<T> lohaDelta = ComputeLoHaDelta(input);\n+\n+        // Sum the outputs: base + loha_delta\n+        Tensor<T> result = new Tensor<T>(baseOutput.Shape);\n+        for (int i = 0; i < baseOutput.Length; i++)\n+        {\n+            result[i] = NumOps.Add(baseOutput[i], lohaDelta[i]);\n+        }\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Computes the LoHa delta using Hadamard products across all rank dimensions.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor of shape [batchSize, inputSize].</param>\n+    /// <returns>LoHa delta tensor of shape [batchSize, outputSize].</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Computes: delta = scaling * sum over rank of (input * A[i]) ‚äô B[i]\n+    ///\n+    /// For each rank dimension i:\n+    /// 1. Multiply input by A[i] matrix: intermediate[i] = input * A[i]\n+    /// 2. Apply Hadamard product with B[i]: result[i] = intermediate[i] ‚äô B[i]\n+    /// 3. Sum all results and scale: delta = scaling * sum(result[i])\n+    /// </para>\n+    /// </remarks>\n+    private Tensor<T> ComputeLoHaDelta(Tensor<T> input)\n+    {\n+        int batchSize = input.Shape[0];\n+        int inputSize = input.Shape.Length > 1 ? input.Shape[1] : input.Length;\n+        int outputSize = GetOutputShape()[0];\n+\n+        // Convert input to matrix [batchSize, inputSize]\n+        Matrix<T> inputMatrix = new Matrix<T>(batchSize, inputSize);\n+        for (int b = 0; b < batchSize; b++)\n+        {\n+            for (int i = 0; i < inputSize; i++)\n+            {\n+                inputMatrix[b, i] = input[b * inputSize + i];\n+            }\n+        }\n+\n+        // Accumulate Hadamard product results across all ranks\n+        Matrix<T> deltaMatrix = new Matrix<T>(batchSize, outputSize);\n+        for (int b = 0; b < batchSize; b++)\n+        {\n+            for (int o = 0; o < outputSize; o++)\n+            {\n+                deltaMatrix[b, o] = NumOps.Zero;\n+            }\n+        }\n+\n+        // Sum over rank: delta += (input * A[r]) ‚äô B[r] for each r\n+        for (int r = 0; r < Rank; r++)\n+        {\n+            // Compute input * A[r] for each batch and output dimension\n+            Matrix<T> intermediate = new Matrix<T>(batchSize, outputSize);\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                for (int o = 0; o < outputSize; o++)\n+                {\n+                    T sum = NumOps.Zero;\n+                    for (int i = 0; i < inputSize; i++)\n+                    {\n+                        // (input * A[r])[b, o] = sum over i of input[b, i] * A[r][i, o]\n+                        sum = NumOps.Add(sum, NumOps.Multiply(inputMatrix[b, i], _matricesA[r][i, o]));\n+                    }\n+                    intermediate[b, o] = sum;\n+                }\n+            }\n+\n+            // Apply Hadamard product with B[r]: result ‚äô= B[r]\n+            Matrix<T> hadamardResult = HadamardProduct(intermediate, _matricesB[r]);\n+\n+            // Accumulate into delta\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                for (int o = 0; o < outputSize; o++)\n+                {\n+                    deltaMatrix[b, o] = NumOps.Add(deltaMatrix[b, o], hadamardResult[b, o]);\n+                }\n+            }\n+        }\n+\n+        // Apply scaling\n+        for (int b = 0; b < batchSize; b++)\n+        {\n+            for (int o = 0; o < outputSize; o++)\n+            {\n+                deltaMatrix[b, o] = NumOps.Multiply(deltaMatrix[b, o], _scaling);\n+            }\n+        }\n+\n+        // Convert back to tensor\n+        Vector<T> deltaData = new Vector<T>(batchSize * outputSize);\n+        int idx = 0;\n+        for (int b = 0; b < batchSize; b++)\n+        {\n+            for (int o = 0; o < outputSize; o++)\n+            {\n+                deltaData[idx++] = deltaMatrix[b, o];\n+            }\n+        }\n+\n+        return new Tensor<T>(new[] { batchSize, outputSize }, deltaData);\n+    }\n+\n+    /// <summary>\n+    /// Computes element-wise Hadamard product between a batch matrix and a weight matrix.\n+    /// </summary>\n+    /// <param name=\"batchMatrix\">Matrix of shape [batchSize, size].</param>\n+    /// <param name=\"weightMatrix\">Matrix of shape [inputSize, outputSize] (broadcasted across batch).</param>\n+    /// <returns>Hadamard product result of same shape as batchMatrix.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// For LoHa, the Hadamard product is applied between the intermediate activations\n+    /// (batchSize √ó outputSize) and the B matrix (inputSize √ó outputSize).\n+    ///\n+    /// Since the intermediate is [batch, output] and B is [input, output], we take the\n+    /// element-wise product along the output dimension.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> The Hadamard product is just element-wise multiplication.\n+    /// For each position (i, j), multiply the corresponding elements: result[i,j] = a[i,j] * b[i,j]\n+    ///\n+    /// This is different from matrix multiplication, which sums over a dimension.\n+    /// Hadamard product keeps dimensions the same and multiplies element-by-element.\n+    /// </para>\n+    /// </remarks>\n+    private Matrix<T> HadamardProduct(Matrix<T> batchMatrix, Matrix<T> weightMatrix)\n+    {\n+        int batchSize = batchMatrix.Rows;\n+        int outputSize = batchMatrix.Columns;\n+\n+        // For LoHa: batchMatrix is [batch, output], weightMatrix is [input, output]\n+        // We broadcast weightMatrix across batch dimension and multiply element-wise along output\n+        Matrix<T> result = new Matrix<T>(batchSize, outputSize);\n+\n+        for (int b = 0; b < batchSize; b++)\n+        {\n+            for (int o = 0; o < outputSize; o++)\n+            {\n+                // Since intermediate is already projected to output space,\n+                // we multiply element-wise with the first row of B\n+                // (This is a simplification; full LoHa may have different broadcasting)\n+                T sum = NumOps.Zero;\n+                for (int i = 0; i < weightMatrix.Rows; i++)\n+                {\n+                    sum = NumOps.Add(sum, weightMatrix[i, o]);\n+                }\n+                // Average across input dimension\n+                T avg = NumOps.Divide(sum, NumOps.FromDouble(weightMatrix.Rows));\n+                result[b, o] = NumOps.Multiply(batchMatrix[b, o], avg);\n+            }\n+        }\n+\n+        return result;\n+    }","path":"src/LoRA/Adapters/LoHaAdapter.cs","commit_id":"fa815036ee570162056ba6aca773f34c80f36948","original_commit_id":"8d5a1b6564c1a2bc1b334f1863e6003a6fd142b2","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Critical: HadamardProduct implementation is mathematically incorrect.**\n\nThis implementation does NOT compute a proper Hadamard product. A Hadamard product (‚äô) is defined as element-wise multiplication between matrices of the **same shape**: (A ‚äô B)[i,j] = A[i,j] √ó B[i,j].\n\nIn this implementation:\n- `batchMatrix` has shape [batch, output]\n- `weightMatrix` has shape [input, output]\n- The code averages `weightMatrix` across the input dimension, then multiplies\n\nThe comment on line 361 admits this: *\"This is a simplification; full LoHa may have different broadcasting\"*\n\nThis is not a minor simplification‚Äîit fundamentally changes the mathematical operation and will produce incorrect results. The gradient computations in `ComputeLoHaGradients` assume proper Hadamard products, so this mismatch will cause training to fail or converge incorrectly.\n\n\n\nThe dimensions suggest a conceptual mismatch. Review the LoHa literature to determine the correct formulation. Typically, LoHa computes:\n```\nŒîW = Œ£(A[r] ‚äô B[r])\n```\nwhere A[r] and B[r] have identical shapes for element-wise multiplication. The current implementation with [batch, output] and [input, output] shapes cannot perform a true Hadamard product.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/LoRA/Adapters/LoHaAdapter.cs around lines 346-374, the HadamardProduct\ncurrently averages weightMatrix across its input dimension then multiplies,\nwhich is not a true Hadamard product. Replace this with a true element-wise\nmultiplication: require that weightMatrix has the same shape as batchMatrix (or\nis broadcastable across the batch dimension), validate shapes up-front and\neither expand/repeat weightMatrix along the batch dimension to match\nbatchMatrix.Rows or throw a clear exception; then compute result[b,o] =\nNumOps.Multiply(batchMatrix[b,o], weightMatrix[b,o]) for all indices. Also\nupdate callers (or upstream code) to supply correctly-shaped matrices (or the\ncorrect projection step) so gradient code in ComputeLoHaGradients remains\nconsistent.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T03:02:27Z","updated_at":"2025-11-02T03:02:31Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140278","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140278"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140278"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140278/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":346,"original_start_line":346,"start_side":"RIGHT","line":374,"original_line":374,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":374,"position":374,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140282","pull_request_review_id":3408046172,"id":2484140282,"node_id":"PRRC_kwDOKSXUF86UEPj6","diff_hunk":"@@ -0,0 +1,903 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// LoHa (Low-Rank Hadamard Product Adaptation) adapter for parameter-efficient fine-tuning.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// LoHa uses element-wise Hadamard products (‚äô) instead of matrix multiplication for adaptation.\n+/// Instead of computing ŒîW = B * A like standard LoRA, LoHa computes:\n+/// ŒîW = sum over rank of (A[i] ‚äô B[i])\n+///\n+/// This formulation can capture element-wise patterns that matrix multiplication may miss,\n+/// making it particularly effective for:\n+/// - Convolutional layers (local spatial patterns)\n+/// - Element-wise transformations\n+/// - Fine-grained weight adjustments\n+/// </para>\n+/// <para><b>Mathematical Formulation:</b>\n+///\n+/// Standard LoRA: ŒîW = B * A where B is rank√óoutput, A is input√órank\n+/// LoHa: ŒîW = Œ£(A[i] ‚äô B[i]) where A[i] and B[i] are both input√óoutput\n+///\n+/// The Hadamard product (‚äô) performs element-wise multiplication, allowing each element\n+/// of the weight matrix to be adjusted independently across the rank dimensions.\n+/// </para>\n+/// <para><b>For Beginners:</b> LoHa is a variant of LoRA that uses element-wise multiplication\n+/// instead of matrix multiplication. Think of it this way:\n+///\n+/// - Standard LoRA: Learns \"row and column patterns\" that combine via matrix multiply\n+/// - LoHa: Learns \"pixel-by-pixel patterns\" that combine via element-wise multiply\n+///\n+/// LoHa is especially good when:\n+/// 1. You need to capture local, element-wise patterns (like in images)\n+/// 2. The weight matrix has spatial structure (like convolutional filters)\n+/// 3. You want each weight to be adjusted somewhat independently\n+///\n+/// Trade-offs compared to LoRA:\n+/// - More parameters: Both A and B must be full-sized (input√óoutput) per rank dimension\n+/// - Different expressiveness: Better for element-wise patterns, different from matrix patterns\n+/// - Better for CNNs: The element-wise nature matches convolutional structure better\n+///\n+/// Example: A 100√ó100 weight matrix with rank=8\n+/// - Standard LoRA: 8√ó100 + 100√ó8 = 1,600 parameters\n+/// - LoHa: 8√ó(100√ó100) + 8√ó(100√ó100) = 160,000 parameters\n+///\n+/// Despite more parameters, LoHa is still far more efficient than full fine-tuning (10,000 params).\n+/// </para>\n+/// </remarks>\n+public class LoHaAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Low-rank matrices A with dimensions (rank, inputSize, outputSize).\n+    /// Each A[i] is a full-sized matrix for the i-th rank dimension.\n+    /// </summary>\n+    private readonly Matrix<T>[] _matricesA;\n+\n+    /// <summary>\n+    /// Low-rank matrices B with dimensions (rank, inputSize, outputSize).\n+    /// Each B[i] is a full-sized matrix for the i-th rank dimension.\n+    /// </summary>\n+    private readonly Matrix<T>[] _matricesB;\n+\n+    /// <summary>\n+    /// Gradients for matrices A computed during backpropagation.\n+    /// </summary>\n+    private Matrix<T>[]? _matricesAGradient;\n+\n+    /// <summary>\n+    /// Gradients for matrices B computed during backpropagation.\n+    /// </summary>\n+    private Matrix<T>[]? _matricesBGradient;\n+\n+    /// <summary>\n+    /// Stored input from the forward pass, needed for gradient computation.\n+    /// </summary>\n+    private Tensor<T>? _lastInput;\n+\n+    /// <summary>\n+    /// Stored base layer output from the forward pass.\n+    /// </summary>\n+    private Tensor<T>? _lastBaseOutput;\n+\n+    /// <summary>\n+    /// Computed scaling factor (alpha / rank) used during forward pass.\n+    /// </summary>\n+    private readonly T _scaling;\n+\n+    /// <summary>\n+    /// Initializes a new LoHa adapter wrapping an existing layer.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with LoHa.</param>\n+    /// <param name=\"rank\">The rank of the low-rank decomposition.</param>\n+    /// <param name=\"alpha\">The LoHa scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when the base layer doesn't have 1D input/output shapes.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This creates a LoHa adapter for any layer with 1D input/output.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to make more efficient to fine-tune\n+    /// - rank: How many element-wise patterns to learn (more = more flexibility, more parameters)\n+    /// - alpha: How strong the LoHa adaptation is (typically same as rank)\n+    /// - freezeBaseLayer: Whether to lock the original layer's weights (usually true for efficiency)\n+    ///\n+    /// The adapter creates 2√órank full-sized matrices (A and B for each rank dimension),\n+    /// which are combined using element-wise Hadamard products during forward/backward passes.\n+    /// </para>\n+    /// </remarks>\n+    public LoHaAdapter(ILayer<T> baseLayer, int rank, double alpha = -1, bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        // Validate base layer has single-dimensional input/output\n+        if (baseLayer.GetInputShape().Length != 1 || baseLayer.GetOutputShape().Length != 1)\n+        {\n+            throw new ArgumentException(\"LoHaAdapter only supports layers with 1D input/output shapes\", nameof(baseLayer));\n+        }\n+\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+\n+        // Calculate scaling\n+        _scaling = NumOps.Divide(_loraLayer.Alpha, NumOps.FromDouble(rank));\n+\n+        // Initialize LoHa matrices (rank sets of full-sized matrices)\n+        _matricesA = new Matrix<T>[rank];\n+        _matricesB = new Matrix<T>[rank];\n+\n+        for (int r = 0; r < rank; r++)\n+        {\n+            // Initialize A[r] with random values (Gaussian with std = 1/sqrt(rank))\n+            _matricesA[r] = new Matrix<T>(inputSize, outputSize);\n+            T stddev = NumOps.Sqrt(NumOps.Divide(NumOps.One, NumOps.FromDouble(rank)));\n+            for (int i = 0; i < inputSize; i++)\n+            {\n+                for (int j = 0; j < outputSize; j++)\n+                {\n+                    // Box-Muller transform for Gaussian random numbers\n+                    double u1 = Random.NextDouble();\n+                    double u2 = Random.NextDouble();\n+                    double randStdNormal = Math.Sqrt(-2.0 * Math.Log(u1)) * Math.Sin(2.0 * Math.PI * u2);\n+                    _matricesA[r][i, j] = NumOps.Multiply(NumOps.FromDouble(randStdNormal), stddev);\n+                }\n+            }\n+\n+            // Initialize B[r] to zero (so LoHa has no effect initially)\n+            _matricesB[r] = new Matrix<T>(inputSize, outputSize);\n+            for (int i = 0; i < inputSize; i++)\n+            {\n+                for (int j = 0; j < outputSize; j++)\n+                {\n+                    _matricesB[r][i, j] = NumOps.Zero;\n+                }\n+            }\n+        }\n+\n+        // Initialize parameter vector\n+        Parameters = new Vector<T>(ParameterCount);\n+        UpdateParametersFromMatrices();\n+    }\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters.\n+    /// </summary>\n+    /// <remarks>\n+    /// LoHa has 2 * rank * inputSize * outputSize parameters (A and B matrices for each rank).\n+    /// This is more than standard LoRA but still far less than full fine-tuning.\n+    /// </remarks>\n+    public override int ParameterCount\n+    {\n+        get\n+        {\n+            int inputSize = GetInputShape()[0];\n+            int outputSize = GetOutputShape()[0];\n+            int lohaParams = 2 * Rank * inputSize * outputSize;\n+            return _freezeBaseLayer ? lohaParams : (_baseLayer.ParameterCount + lohaParams);\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass through both base layer and LoHa adaptation.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Sum of base layer output and LoHa delta (computed via Hadamard products).</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The forward pass computes:\n+    /// 1. base_output = base_layer(input)\n+    /// 2. loha_delta = sum over rank of (input * A[i] ‚äô B[i]) * scaling\n+    /// 3. output = base_output + loha_delta\n+    ///\n+    /// The Hadamard product (‚äô) multiplies corresponding elements, allowing element-wise adaptations.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This runs the input through the original layer and adds a correction.\n+    ///\n+    /// The correction is computed by:\n+    /// 1. Transforming input through each A[i] matrix (one per rank dimension)\n+    /// 2. Multiplying element-wise with corresponding B[i] matrix (Hadamard product)\n+    /// 3. Summing all rank contributions together\n+    /// 4. Scaling by alpha/rank\n+    ///\n+    /// This element-wise approach lets LoHa learn fine-grained adjustments to each weight independently.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        _lastInput = input.Clone();\n+\n+        // Forward through base layer\n+        Tensor<T> baseOutput = _baseLayer.Forward(input);\n+        _lastBaseOutput = baseOutput.Clone();\n+\n+        // Compute LoHa delta using Hadamard products\n+        Tensor<T> lohaDelta = ComputeLoHaDelta(input);\n+\n+        // Sum the outputs: base + loha_delta\n+        Tensor<T> result = new Tensor<T>(baseOutput.Shape);\n+        for (int i = 0; i < baseOutput.Length; i++)\n+        {\n+            result[i] = NumOps.Add(baseOutput[i], lohaDelta[i]);\n+        }\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Computes the LoHa delta using Hadamard products across all rank dimensions.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor of shape [batchSize, inputSize].</param>\n+    /// <returns>LoHa delta tensor of shape [batchSize, outputSize].</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Computes: delta = scaling * sum over rank of (input * A[i]) ‚äô B[i]\n+    ///\n+    /// For each rank dimension i:\n+    /// 1. Multiply input by A[i] matrix: intermediate[i] = input * A[i]\n+    /// 2. Apply Hadamard product with B[i]: result[i] = intermediate[i] ‚äô B[i]\n+    /// 3. Sum all results and scale: delta = scaling * sum(result[i])\n+    /// </para>\n+    /// </remarks>\n+    private Tensor<T> ComputeLoHaDelta(Tensor<T> input)\n+    {\n+        int batchSize = input.Shape[0];\n+        int inputSize = input.Shape.Length > 1 ? input.Shape[1] : input.Length;\n+        int outputSize = GetOutputShape()[0];\n+\n+        // Convert input to matrix [batchSize, inputSize]\n+        Matrix<T> inputMatrix = new Matrix<T>(batchSize, inputSize);\n+        for (int b = 0; b < batchSize; b++)\n+        {\n+            for (int i = 0; i < inputSize; i++)\n+            {\n+                inputMatrix[b, i] = input[b * inputSize + i];\n+            }\n+        }\n+\n+        // Accumulate Hadamard product results across all ranks\n+        Matrix<T> deltaMatrix = new Matrix<T>(batchSize, outputSize);\n+        for (int b = 0; b < batchSize; b++)\n+        {\n+            for (int o = 0; o < outputSize; o++)\n+            {\n+                deltaMatrix[b, o] = NumOps.Zero;\n+            }\n+        }\n+\n+        // Sum over rank: delta += (input * A[r]) ‚äô B[r] for each r\n+        for (int r = 0; r < Rank; r++)\n+        {\n+            // Compute input * A[r] for each batch and output dimension\n+            Matrix<T> intermediate = new Matrix<T>(batchSize, outputSize);\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                for (int o = 0; o < outputSize; o++)\n+                {\n+                    T sum = NumOps.Zero;\n+                    for (int i = 0; i < inputSize; i++)\n+                    {\n+                        // (input * A[r])[b, o] = sum over i of input[b, i] * A[r][i, o]\n+                        sum = NumOps.Add(sum, NumOps.Multiply(inputMatrix[b, i], _matricesA[r][i, o]));\n+                    }\n+                    intermediate[b, o] = sum;\n+                }\n+            }\n+\n+            // Apply Hadamard product with B[r]: result ‚äô= B[r]\n+            Matrix<T> hadamardResult = HadamardProduct(intermediate, _matricesB[r]);\n+\n+            // Accumulate into delta\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                for (int o = 0; o < outputSize; o++)\n+                {\n+                    deltaMatrix[b, o] = NumOps.Add(deltaMatrix[b, o], hadamardResult[b, o]);\n+                }\n+            }\n+        }\n+\n+        // Apply scaling\n+        for (int b = 0; b < batchSize; b++)\n+        {\n+            for (int o = 0; o < outputSize; o++)\n+            {\n+                deltaMatrix[b, o] = NumOps.Multiply(deltaMatrix[b, o], _scaling);\n+            }\n+        }\n+\n+        // Convert back to tensor\n+        Vector<T> deltaData = new Vector<T>(batchSize * outputSize);\n+        int idx = 0;\n+        for (int b = 0; b < batchSize; b++)\n+        {\n+            for (int o = 0; o < outputSize; o++)\n+            {\n+                deltaData[idx++] = deltaMatrix[b, o];\n+            }\n+        }\n+\n+        return new Tensor<T>(new[] { batchSize, outputSize }, deltaData);\n+    }\n+\n+    /// <summary>\n+    /// Computes element-wise Hadamard product between a batch matrix and a weight matrix.\n+    /// </summary>\n+    /// <param name=\"batchMatrix\">Matrix of shape [batchSize, size].</param>\n+    /// <param name=\"weightMatrix\">Matrix of shape [inputSize, outputSize] (broadcasted across batch).</param>\n+    /// <returns>Hadamard product result of same shape as batchMatrix.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// For LoHa, the Hadamard product is applied between the intermediate activations\n+    /// (batchSize √ó outputSize) and the B matrix (inputSize √ó outputSize).\n+    ///\n+    /// Since the intermediate is [batch, output] and B is [input, output], we take the\n+    /// element-wise product along the output dimension.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> The Hadamard product is just element-wise multiplication.\n+    /// For each position (i, j), multiply the corresponding elements: result[i,j] = a[i,j] * b[i,j]\n+    ///\n+    /// This is different from matrix multiplication, which sums over a dimension.\n+    /// Hadamard product keeps dimensions the same and multiplies element-by-element.\n+    /// </para>\n+    /// </remarks>\n+    private Matrix<T> HadamardProduct(Matrix<T> batchMatrix, Matrix<T> weightMatrix)\n+    {\n+        int batchSize = batchMatrix.Rows;\n+        int outputSize = batchMatrix.Columns;\n+\n+        // For LoHa: batchMatrix is [batch, output], weightMatrix is [input, output]\n+        // We broadcast weightMatrix across batch dimension and multiply element-wise along output\n+        Matrix<T> result = new Matrix<T>(batchSize, outputSize);\n+\n+        for (int b = 0; b < batchSize; b++)\n+        {\n+            for (int o = 0; o < outputSize; o++)\n+            {\n+                // Since intermediate is already projected to output space,\n+                // we multiply element-wise with the first row of B\n+                // (This is a simplification; full LoHa may have different broadcasting)\n+                T sum = NumOps.Zero;\n+                for (int i = 0; i < weightMatrix.Rows; i++)\n+                {\n+                    sum = NumOps.Add(sum, weightMatrix[i, o]);\n+                }\n+                // Average across input dimension\n+                T avg = NumOps.Divide(sum, NumOps.FromDouble(weightMatrix.Rows));\n+                result[b, o] = NumOps.Multiply(batchMatrix[b, o], avg);\n+            }\n+        }\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Performs the backward pass through both layers, computing gradients for LoHa matrices.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient flowing back from the next layer.</param>\n+    /// <returns>Gradient to pass to the previous layer.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The backward pass computes gradients using the chain rule for Hadamard products:\n+    ///\n+    /// dL/dA[r] = input^T * (dL/doutput ‚äô B[r]) * scaling\n+    /// dL/dB[r] = (input * A[r]) ‚äô dL/doutput * scaling\n+    /// dL/dinput = base_gradient + sum over rank of (dL/doutput ‚äô B[r]) * A[r]^T * scaling\n+    ///\n+    /// The Hadamard product gradient rule: d/dx (f ‚äô g) = df ‚äô g + f ‚äô dg\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is the learning phase for LoHa. It computes:\n+    ///\n+    /// 1. How to adjust each A[i] matrix to reduce error\n+    /// 2. How to adjust each B[i] matrix to reduce error\n+    /// 3. What gradient to send to earlier layers\n+    ///\n+    /// The math is more complex than standard LoRA because Hadamard products have different\n+    /// derivative rules than matrix multiplication, but the idea is the same: figure out\n+    /// how each parameter contributed to the error and adjust accordingly.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (_lastInput == null || _lastBaseOutput == null)\n+        {\n+            throw new InvalidOperationException(\"Forward pass must be called before backward pass\");\n+        }\n+\n+        // Backward through base layer\n+        Tensor<T> baseInputGrad = _baseLayer.Backward(outputGradient);\n+\n+        // Compute LoHa gradients\n+        Tensor<T> lohaInputGrad = ComputeLoHaGradients(outputGradient);\n+\n+        // Sum input gradients\n+        Tensor<T> inputGrad = new Tensor<T>(lohaInputGrad.Shape);\n+        for (int i = 0; i < lohaInputGrad.Length; i++)\n+        {\n+            inputGrad[i] = NumOps.Add(lohaInputGrad[i], baseInputGrad[i]);\n+        }\n+\n+        // Update parameter gradients vector\n+        UpdateParameterGradientsFromMatrices();\n+\n+        return inputGrad;\n+    }\n+\n+    /// <summary>\n+    /// Computes gradients for LoHa matrices A and B using Hadamard product gradient rules.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient flowing back from next layer.</param>\n+    /// <returns>Input gradient from LoHa path.</returns>\n+    private Tensor<T> ComputeLoHaGradients(Tensor<T> outputGradient)\n+    {\n+        int batchSize = _lastInput!.Shape[0];\n+        int inputSize = _lastInput!.Shape.Length > 1 ? _lastInput.Shape[1] : _lastInput.Length;\n+        int outputSize = GetOutputShape()[0];\n+\n+        // Convert to matrices\n+        Matrix<T> inputMatrix = new Matrix<T>(batchSize, inputSize);\n+        for (int b = 0; b < batchSize; b++)\n+        {\n+            for (int i = 0; i < inputSize; i++)\n+            {\n+                inputMatrix[b, i] = _lastInput[b * inputSize + i];\n+            }\n+        }\n+\n+        Matrix<T> gradMatrix = new Matrix<T>(batchSize, outputSize);\n+        for (int b = 0; b < batchSize; b++)\n+        {\n+            for (int o = 0; o < outputSize; o++)\n+            {\n+                gradMatrix[b, o] = outputGradient[b * outputSize + o];\n+            }\n+        }\n+\n+        // Initialize gradients\n+        _matricesAGradient = new Matrix<T>[Rank];\n+        _matricesBGradient = new Matrix<T>[Rank];\n+        for (int r = 0; r < Rank; r++)\n+        {\n+            _matricesAGradient[r] = new Matrix<T>(inputSize, outputSize);\n+            _matricesBGradient[r] = new Matrix<T>(inputSize, outputSize);\n+        }\n+\n+        // Accumulate input gradients\n+        Matrix<T> inputGradMatrix = new Matrix<T>(batchSize, inputSize);\n+\n+        // For each rank dimension, compute gradients\n+        for (int r = 0; r < Rank; r++)\n+        {\n+            // Compute intermediate = input * A[r]\n+            Matrix<T> intermediate = new Matrix<T>(batchSize, outputSize);\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                for (int o = 0; o < outputSize; o++)\n+                {\n+                    T sum = NumOps.Zero;\n+                    for (int i = 0; i < inputSize; i++)\n+                    {\n+                        sum = NumOps.Add(sum, NumOps.Multiply(inputMatrix[b, i], _matricesA[r][i, o]));\n+                    }\n+                    intermediate[b, o] = sum;\n+                }\n+            }\n+\n+            // Gradient for B[r]: dL/dB[r] = intermediate^T * gradOutput (with Hadamard consideration)\n+            // For element-wise operations: dL/dB = dL/doutput ‚äô intermediate\n+            for (int i = 0; i < inputSize; i++)\n+            {\n+                for (int o = 0; o < outputSize; o++)\n+                {\n+                    T gradSum = NumOps.Zero;\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        // Compute contribution from this batch\n+                        T contribution = NumOps.Multiply(gradMatrix[b, o], intermediate[b, o]);\n+                        gradSum = NumOps.Add(gradSum, contribution);\n+                    }\n+                    _matricesBGradient[r][i, o] = NumOps.Multiply(gradSum, _scaling);\n+                }\n+            }","path":"src/LoRA/Adapters/LoHaAdapter.cs","commit_id":"fa815036ee570162056ba6aca773f34c80f36948","original_commit_id":"8d5a1b6564c1a2bc1b334f1863e6003a6fd142b2","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**Major issue: Gradient computation for B matrices is incorrect.**\n\nLines 490-503 compute the gradient for `_matricesBGradient[r][i, o]` by looping over input index `i`, but the computation inside (lines 494-501) doesn't depend on `i` at all:\n\n```csharp\nfor (int i = 0; i < inputSize; i++)  // <-- loops over i\n{\n    for (int o = 0; o < outputSize; o++)\n    {\n        T gradSum = NumOps.Zero;\n        for (int b = 0; b < batchSize; b++)\n        {\n            T contribution = NumOps.Multiply(gradMatrix[b, o], intermediate[b, o]);\n            // ^^^ intermediate[b,o] is the same for all i!\n            gradSum = NumOps.Add(gradSum, contribution);\n        }\n        _matricesBGradient[r][i, o] = NumOps.Multiply(gradSum, _scaling);\n        // ^^^ Every row i gets the SAME gradient value\n    }\n}\n```\n\nThis means every row of `_matricesBGradient[r]` will be identical, which is mathematically incorrect. Gradients should vary based on which parameters actually contributed to the output.\n\n\n\nThis stems from the dimension mismatch in the forward pass. For proper Hadamard product gradients:\n- If `C = A ‚äô B`, then `dL/dB[i,j] = dL/dC[i,j] √ó A[i,j]`\n\nThe gradient computation needs to properly account for which input dimensions contributed to each output.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/LoRA/Adapters/LoHaAdapter.cs around lines 488-503, the loop computing\n_matricesBGradient[r][i, o] currently uses intermediate[b, o] which does not\ndepend on i, causing every row i to get the same gradient; change the gradient\naccumulation to multiply the output gradient by the intermediate value that\ncorresponds to the i-th input contribution (e.g., use intermediate[b, i] or\nintermediate[b, i, o] depending on how intermediate is shaped) so that inside\nthe inner batch loop you compute contribution = NumOps.Multiply(gradMatrix[b,\no], intermediate[b, i] /* or intermediate[b,i,o] */), sum into gradSum, then set\n_matricesBGradient[r][i, o] = NumOps.Multiply(gradSum, _scaling); ensure the\nchosen intermediate index matches the forward-pass construction so gradients\nvary with i.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T03:02:27Z","updated_at":"2025-11-02T03:02:31Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140282","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140282"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140282"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140282/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":488,"original_start_line":488,"start_side":"RIGHT","line":503,"original_line":503,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":503,"position":503,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140287","pull_request_review_id":3408046172,"id":2484140287,"node_id":"PRRC_kwDOKSXUF86UEPj_","diff_hunk":"@@ -0,0 +1,903 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// LoHa (Low-Rank Hadamard Product Adaptation) adapter for parameter-efficient fine-tuning.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// LoHa uses element-wise Hadamard products (‚äô) instead of matrix multiplication for adaptation.\n+/// Instead of computing ŒîW = B * A like standard LoRA, LoHa computes:\n+/// ŒîW = sum over rank of (A[i] ‚äô B[i])\n+///\n+/// This formulation can capture element-wise patterns that matrix multiplication may miss,\n+/// making it particularly effective for:\n+/// - Convolutional layers (local spatial patterns)\n+/// - Element-wise transformations\n+/// - Fine-grained weight adjustments\n+/// </para>\n+/// <para><b>Mathematical Formulation:</b>\n+///\n+/// Standard LoRA: ŒîW = B * A where B is rank√óoutput, A is input√órank\n+/// LoHa: ŒîW = Œ£(A[i] ‚äô B[i]) where A[i] and B[i] are both input√óoutput\n+///\n+/// The Hadamard product (‚äô) performs element-wise multiplication, allowing each element\n+/// of the weight matrix to be adjusted independently across the rank dimensions.\n+/// </para>\n+/// <para><b>For Beginners:</b> LoHa is a variant of LoRA that uses element-wise multiplication\n+/// instead of matrix multiplication. Think of it this way:\n+///\n+/// - Standard LoRA: Learns \"row and column patterns\" that combine via matrix multiply\n+/// - LoHa: Learns \"pixel-by-pixel patterns\" that combine via element-wise multiply\n+///\n+/// LoHa is especially good when:\n+/// 1. You need to capture local, element-wise patterns (like in images)\n+/// 2. The weight matrix has spatial structure (like convolutional filters)\n+/// 3. You want each weight to be adjusted somewhat independently\n+///\n+/// Trade-offs compared to LoRA:\n+/// - More parameters: Both A and B must be full-sized (input√óoutput) per rank dimension\n+/// - Different expressiveness: Better for element-wise patterns, different from matrix patterns\n+/// - Better for CNNs: The element-wise nature matches convolutional structure better\n+///\n+/// Example: A 100√ó100 weight matrix with rank=8\n+/// - Standard LoRA: 8√ó100 + 100√ó8 = 1,600 parameters\n+/// - LoHa: 8√ó(100√ó100) + 8√ó(100√ó100) = 160,000 parameters\n+///\n+/// Despite more parameters, LoHa is still far more efficient than full fine-tuning (10,000 params).\n+/// </para>\n+/// </remarks>\n+public class LoHaAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Low-rank matrices A with dimensions (rank, inputSize, outputSize).\n+    /// Each A[i] is a full-sized matrix for the i-th rank dimension.\n+    /// </summary>\n+    private readonly Matrix<T>[] _matricesA;\n+\n+    /// <summary>\n+    /// Low-rank matrices B with dimensions (rank, inputSize, outputSize).\n+    /// Each B[i] is a full-sized matrix for the i-th rank dimension.\n+    /// </summary>\n+    private readonly Matrix<T>[] _matricesB;\n+\n+    /// <summary>\n+    /// Gradients for matrices A computed during backpropagation.\n+    /// </summary>\n+    private Matrix<T>[]? _matricesAGradient;\n+\n+    /// <summary>\n+    /// Gradients for matrices B computed during backpropagation.\n+    /// </summary>\n+    private Matrix<T>[]? _matricesBGradient;\n+\n+    /// <summary>\n+    /// Stored input from the forward pass, needed for gradient computation.\n+    /// </summary>\n+    private Tensor<T>? _lastInput;\n+\n+    /// <summary>\n+    /// Stored base layer output from the forward pass.\n+    /// </summary>\n+    private Tensor<T>? _lastBaseOutput;\n+\n+    /// <summary>\n+    /// Computed scaling factor (alpha / rank) used during forward pass.\n+    /// </summary>\n+    private readonly T _scaling;\n+\n+    /// <summary>\n+    /// Initializes a new LoHa adapter wrapping an existing layer.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with LoHa.</param>\n+    /// <param name=\"rank\">The rank of the low-rank decomposition.</param>\n+    /// <param name=\"alpha\">The LoHa scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when the base layer doesn't have 1D input/output shapes.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This creates a LoHa adapter for any layer with 1D input/output.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to make more efficient to fine-tune\n+    /// - rank: How many element-wise patterns to learn (more = more flexibility, more parameters)\n+    /// - alpha: How strong the LoHa adaptation is (typically same as rank)\n+    /// - freezeBaseLayer: Whether to lock the original layer's weights (usually true for efficiency)\n+    ///\n+    /// The adapter creates 2√órank full-sized matrices (A and B for each rank dimension),\n+    /// which are combined using element-wise Hadamard products during forward/backward passes.\n+    /// </para>\n+    /// </remarks>\n+    public LoHaAdapter(ILayer<T> baseLayer, int rank, double alpha = -1, bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        // Validate base layer has single-dimensional input/output\n+        if (baseLayer.GetInputShape().Length != 1 || baseLayer.GetOutputShape().Length != 1)\n+        {\n+            throw new ArgumentException(\"LoHaAdapter only supports layers with 1D input/output shapes\", nameof(baseLayer));\n+        }\n+\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+\n+        // Calculate scaling\n+        _scaling = NumOps.Divide(_loraLayer.Alpha, NumOps.FromDouble(rank));\n+\n+        // Initialize LoHa matrices (rank sets of full-sized matrices)\n+        _matricesA = new Matrix<T>[rank];\n+        _matricesB = new Matrix<T>[rank];\n+\n+        for (int r = 0; r < rank; r++)\n+        {\n+            // Initialize A[r] with random values (Gaussian with std = 1/sqrt(rank))\n+            _matricesA[r] = new Matrix<T>(inputSize, outputSize);\n+            T stddev = NumOps.Sqrt(NumOps.Divide(NumOps.One, NumOps.FromDouble(rank)));\n+            for (int i = 0; i < inputSize; i++)\n+            {\n+                for (int j = 0; j < outputSize; j++)\n+                {\n+                    // Box-Muller transform for Gaussian random numbers\n+                    double u1 = Random.NextDouble();\n+                    double u2 = Random.NextDouble();\n+                    double randStdNormal = Math.Sqrt(-2.0 * Math.Log(u1)) * Math.Sin(2.0 * Math.PI * u2);\n+                    _matricesA[r][i, j] = NumOps.Multiply(NumOps.FromDouble(randStdNormal), stddev);\n+                }\n+            }\n+\n+            // Initialize B[r] to zero (so LoHa has no effect initially)\n+            _matricesB[r] = new Matrix<T>(inputSize, outputSize);\n+            for (int i = 0; i < inputSize; i++)\n+            {\n+                for (int j = 0; j < outputSize; j++)\n+                {\n+                    _matricesB[r][i, j] = NumOps.Zero;\n+                }\n+            }\n+        }\n+\n+        // Initialize parameter vector\n+        Parameters = new Vector<T>(ParameterCount);\n+        UpdateParametersFromMatrices();\n+    }\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters.\n+    /// </summary>\n+    /// <remarks>\n+    /// LoHa has 2 * rank * inputSize * outputSize parameters (A and B matrices for each rank).\n+    /// This is more than standard LoRA but still far less than full fine-tuning.\n+    /// </remarks>\n+    public override int ParameterCount\n+    {\n+        get\n+        {\n+            int inputSize = GetInputShape()[0];\n+            int outputSize = GetOutputShape()[0];\n+            int lohaParams = 2 * Rank * inputSize * outputSize;\n+            return _freezeBaseLayer ? lohaParams : (_baseLayer.ParameterCount + lohaParams);\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass through both base layer and LoHa adaptation.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Sum of base layer output and LoHa delta (computed via Hadamard products).</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The forward pass computes:\n+    /// 1. base_output = base_layer(input)\n+    /// 2. loha_delta = sum over rank of (input * A[i] ‚äô B[i]) * scaling\n+    /// 3. output = base_output + loha_delta\n+    ///\n+    /// The Hadamard product (‚äô) multiplies corresponding elements, allowing element-wise adaptations.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This runs the input through the original layer and adds a correction.\n+    ///\n+    /// The correction is computed by:\n+    /// 1. Transforming input through each A[i] matrix (one per rank dimension)\n+    /// 2. Multiplying element-wise with corresponding B[i] matrix (Hadamard product)\n+    /// 3. Summing all rank contributions together\n+    /// 4. Scaling by alpha/rank\n+    ///\n+    /// This element-wise approach lets LoHa learn fine-grained adjustments to each weight independently.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        _lastInput = input.Clone();\n+\n+        // Forward through base layer\n+        Tensor<T> baseOutput = _baseLayer.Forward(input);\n+        _lastBaseOutput = baseOutput.Clone();\n+\n+        // Compute LoHa delta using Hadamard products\n+        Tensor<T> lohaDelta = ComputeLoHaDelta(input);\n+\n+        // Sum the outputs: base + loha_delta\n+        Tensor<T> result = new Tensor<T>(baseOutput.Shape);\n+        for (int i = 0; i < baseOutput.Length; i++)\n+        {\n+            result[i] = NumOps.Add(baseOutput[i], lohaDelta[i]);\n+        }\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Computes the LoHa delta using Hadamard products across all rank dimensions.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor of shape [batchSize, inputSize].</param>\n+    /// <returns>LoHa delta tensor of shape [batchSize, outputSize].</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Computes: delta = scaling * sum over rank of (input * A[i]) ‚äô B[i]\n+    ///\n+    /// For each rank dimension i:\n+    /// 1. Multiply input by A[i] matrix: intermediate[i] = input * A[i]\n+    /// 2. Apply Hadamard product with B[i]: result[i] = intermediate[i] ‚äô B[i]\n+    /// 3. Sum all results and scale: delta = scaling * sum(result[i])\n+    /// </para>\n+    /// </remarks>\n+    private Tensor<T> ComputeLoHaDelta(Tensor<T> input)\n+    {\n+        int batchSize = input.Shape[0];\n+        int inputSize = input.Shape.Length > 1 ? input.Shape[1] : input.Length;\n+        int outputSize = GetOutputShape()[0];\n+\n+        // Convert input to matrix [batchSize, inputSize]\n+        Matrix<T> inputMatrix = new Matrix<T>(batchSize, inputSize);\n+        for (int b = 0; b < batchSize; b++)\n+        {\n+            for (int i = 0; i < inputSize; i++)\n+            {\n+                inputMatrix[b, i] = input[b * inputSize + i];\n+            }\n+        }\n+\n+        // Accumulate Hadamard product results across all ranks\n+        Matrix<T> deltaMatrix = new Matrix<T>(batchSize, outputSize);\n+        for (int b = 0; b < batchSize; b++)\n+        {\n+            for (int o = 0; o < outputSize; o++)\n+            {\n+                deltaMatrix[b, o] = NumOps.Zero;\n+            }\n+        }\n+\n+        // Sum over rank: delta += (input * A[r]) ‚äô B[r] for each r\n+        for (int r = 0; r < Rank; r++)\n+        {\n+            // Compute input * A[r] for each batch and output dimension\n+            Matrix<T> intermediate = new Matrix<T>(batchSize, outputSize);\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                for (int o = 0; o < outputSize; o++)\n+                {\n+                    T sum = NumOps.Zero;\n+                    for (int i = 0; i < inputSize; i++)\n+                    {\n+                        // (input * A[r])[b, o] = sum over i of input[b, i] * A[r][i, o]\n+                        sum = NumOps.Add(sum, NumOps.Multiply(inputMatrix[b, i], _matricesA[r][i, o]));\n+                    }\n+                    intermediate[b, o] = sum;\n+                }\n+            }\n+\n+            // Apply Hadamard product with B[r]: result ‚äô= B[r]\n+            Matrix<T> hadamardResult = HadamardProduct(intermediate, _matricesB[r]);\n+\n+            // Accumulate into delta\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                for (int o = 0; o < outputSize; o++)\n+                {\n+                    deltaMatrix[b, o] = NumOps.Add(deltaMatrix[b, o], hadamardResult[b, o]);\n+                }\n+            }\n+        }\n+\n+        // Apply scaling\n+        for (int b = 0; b < batchSize; b++)\n+        {\n+            for (int o = 0; o < outputSize; o++)\n+            {\n+                deltaMatrix[b, o] = NumOps.Multiply(deltaMatrix[b, o], _scaling);\n+            }\n+        }\n+\n+        // Convert back to tensor\n+        Vector<T> deltaData = new Vector<T>(batchSize * outputSize);\n+        int idx = 0;\n+        for (int b = 0; b < batchSize; b++)\n+        {\n+            for (int o = 0; o < outputSize; o++)\n+            {\n+                deltaData[idx++] = deltaMatrix[b, o];\n+            }\n+        }\n+\n+        return new Tensor<T>(new[] { batchSize, outputSize }, deltaData);\n+    }\n+\n+    /// <summary>\n+    /// Computes element-wise Hadamard product between a batch matrix and a weight matrix.\n+    /// </summary>\n+    /// <param name=\"batchMatrix\">Matrix of shape [batchSize, size].</param>\n+    /// <param name=\"weightMatrix\">Matrix of shape [inputSize, outputSize] (broadcasted across batch).</param>\n+    /// <returns>Hadamard product result of same shape as batchMatrix.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// For LoHa, the Hadamard product is applied between the intermediate activations\n+    /// (batchSize √ó outputSize) and the B matrix (inputSize √ó outputSize).\n+    ///\n+    /// Since the intermediate is [batch, output] and B is [input, output], we take the\n+    /// element-wise product along the output dimension.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> The Hadamard product is just element-wise multiplication.\n+    /// For each position (i, j), multiply the corresponding elements: result[i,j] = a[i,j] * b[i,j]\n+    ///\n+    /// This is different from matrix multiplication, which sums over a dimension.\n+    /// Hadamard product keeps dimensions the same and multiplies element-by-element.\n+    /// </para>\n+    /// </remarks>\n+    private Matrix<T> HadamardProduct(Matrix<T> batchMatrix, Matrix<T> weightMatrix)\n+    {\n+        int batchSize = batchMatrix.Rows;\n+        int outputSize = batchMatrix.Columns;\n+\n+        // For LoHa: batchMatrix is [batch, output], weightMatrix is [input, output]\n+        // We broadcast weightMatrix across batch dimension and multiply element-wise along output\n+        Matrix<T> result = new Matrix<T>(batchSize, outputSize);\n+\n+        for (int b = 0; b < batchSize; b++)\n+        {\n+            for (int o = 0; o < outputSize; o++)\n+            {\n+                // Since intermediate is already projected to output space,\n+                // we multiply element-wise with the first row of B\n+                // (This is a simplification; full LoHa may have different broadcasting)\n+                T sum = NumOps.Zero;\n+                for (int i = 0; i < weightMatrix.Rows; i++)\n+                {\n+                    sum = NumOps.Add(sum, weightMatrix[i, o]);\n+                }\n+                // Average across input dimension\n+                T avg = NumOps.Divide(sum, NumOps.FromDouble(weightMatrix.Rows));\n+                result[b, o] = NumOps.Multiply(batchMatrix[b, o], avg);\n+            }\n+        }\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Performs the backward pass through both layers, computing gradients for LoHa matrices.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient flowing back from the next layer.</param>\n+    /// <returns>Gradient to pass to the previous layer.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The backward pass computes gradients using the chain rule for Hadamard products:\n+    ///\n+    /// dL/dA[r] = input^T * (dL/doutput ‚äô B[r]) * scaling\n+    /// dL/dB[r] = (input * A[r]) ‚äô dL/doutput * scaling\n+    /// dL/dinput = base_gradient + sum over rank of (dL/doutput ‚äô B[r]) * A[r]^T * scaling\n+    ///\n+    /// The Hadamard product gradient rule: d/dx (f ‚äô g) = df ‚äô g + f ‚äô dg\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is the learning phase for LoHa. It computes:\n+    ///\n+    /// 1. How to adjust each A[i] matrix to reduce error\n+    /// 2. How to adjust each B[i] matrix to reduce error\n+    /// 3. What gradient to send to earlier layers\n+    ///\n+    /// The math is more complex than standard LoRA because Hadamard products have different\n+    /// derivative rules than matrix multiplication, but the idea is the same: figure out\n+    /// how each parameter contributed to the error and adjust accordingly.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (_lastInput == null || _lastBaseOutput == null)\n+        {\n+            throw new InvalidOperationException(\"Forward pass must be called before backward pass\");\n+        }\n+\n+        // Backward through base layer\n+        Tensor<T> baseInputGrad = _baseLayer.Backward(outputGradient);\n+\n+        // Compute LoHa gradients\n+        Tensor<T> lohaInputGrad = ComputeLoHaGradients(outputGradient);\n+\n+        // Sum input gradients\n+        Tensor<T> inputGrad = new Tensor<T>(lohaInputGrad.Shape);\n+        for (int i = 0; i < lohaInputGrad.Length; i++)\n+        {\n+            inputGrad[i] = NumOps.Add(lohaInputGrad[i], baseInputGrad[i]);\n+        }\n+\n+        // Update parameter gradients vector\n+        UpdateParameterGradientsFromMatrices();\n+\n+        return inputGrad;\n+    }\n+\n+    /// <summary>\n+    /// Computes gradients for LoHa matrices A and B using Hadamard product gradient rules.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient flowing back from next layer.</param>\n+    /// <returns>Input gradient from LoHa path.</returns>\n+    private Tensor<T> ComputeLoHaGradients(Tensor<T> outputGradient)\n+    {\n+        int batchSize = _lastInput!.Shape[0];\n+        int inputSize = _lastInput!.Shape.Length > 1 ? _lastInput.Shape[1] : _lastInput.Length;\n+        int outputSize = GetOutputShape()[0];\n+\n+        // Convert to matrices\n+        Matrix<T> inputMatrix = new Matrix<T>(batchSize, inputSize);\n+        for (int b = 0; b < batchSize; b++)\n+        {\n+            for (int i = 0; i < inputSize; i++)\n+            {\n+                inputMatrix[b, i] = _lastInput[b * inputSize + i];\n+            }\n+        }\n+\n+        Matrix<T> gradMatrix = new Matrix<T>(batchSize, outputSize);\n+        for (int b = 0; b < batchSize; b++)\n+        {\n+            for (int o = 0; o < outputSize; o++)\n+            {\n+                gradMatrix[b, o] = outputGradient[b * outputSize + o];\n+            }\n+        }\n+\n+        // Initialize gradients\n+        _matricesAGradient = new Matrix<T>[Rank];\n+        _matricesBGradient = new Matrix<T>[Rank];\n+        for (int r = 0; r < Rank; r++)\n+        {\n+            _matricesAGradient[r] = new Matrix<T>(inputSize, outputSize);\n+            _matricesBGradient[r] = new Matrix<T>(inputSize, outputSize);\n+        }\n+\n+        // Accumulate input gradients\n+        Matrix<T> inputGradMatrix = new Matrix<T>(batchSize, inputSize);\n+\n+        // For each rank dimension, compute gradients\n+        for (int r = 0; r < Rank; r++)\n+        {\n+            // Compute intermediate = input * A[r]\n+            Matrix<T> intermediate = new Matrix<T>(batchSize, outputSize);\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                for (int o = 0; o < outputSize; o++)\n+                {\n+                    T sum = NumOps.Zero;\n+                    for (int i = 0; i < inputSize; i++)\n+                    {\n+                        sum = NumOps.Add(sum, NumOps.Multiply(inputMatrix[b, i], _matricesA[r][i, o]));\n+                    }\n+                    intermediate[b, o] = sum;\n+                }\n+            }\n+\n+            // Gradient for B[r]: dL/dB[r] = intermediate^T * gradOutput (with Hadamard consideration)\n+            // For element-wise operations: dL/dB = dL/doutput ‚äô intermediate\n+            for (int i = 0; i < inputSize; i++)\n+            {\n+                for (int o = 0; o < outputSize; o++)\n+                {\n+                    T gradSum = NumOps.Zero;\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        // Compute contribution from this batch\n+                        T contribution = NumOps.Multiply(gradMatrix[b, o], intermediate[b, o]);\n+                        gradSum = NumOps.Add(gradSum, contribution);\n+                    }\n+                    _matricesBGradient[r][i, o] = NumOps.Multiply(gradSum, _scaling);\n+                }\n+            }\n+\n+            // Gradient for A[r]: dL/dA[r] = input^T * (gradOutput ‚äô B[r])\n+            for (int i = 0; i < inputSize; i++)\n+            {\n+                for (int o = 0; o < outputSize; o++)\n+                {\n+                    T gradSum = NumOps.Zero;\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        // Element-wise gradient with B\n+                        T hadamardGrad = HadamardGradient(gradMatrix[b, o], _matricesB[r], o);\n+                        T contribution = NumOps.Multiply(inputMatrix[b, i], hadamardGrad);\n+                        gradSum = NumOps.Add(gradSum, contribution);\n+                    }\n+                    _matricesAGradient[r][i, o] = NumOps.Multiply(gradSum, _scaling);\n+                }\n+            }\n+\n+            // Input gradient contribution from this rank\n+            // dL/dinput = (gradOutput ‚äô B[r]) * A[r]^T\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                for (int i = 0; i < inputSize; i++)\n+                {\n+                    T gradSum = NumOps.Zero;\n+                    for (int o = 0; o < outputSize; o++)\n+                    {\n+                        T hadamardGrad = HadamardGradient(gradMatrix[b, o], _matricesB[r], o);\n+                        T contribution = NumOps.Multiply(hadamardGrad, _matricesA[r][i, o]);\n+                        gradSum = NumOps.Add(gradSum, contribution);\n+                    }\n+                    T scaled = NumOps.Multiply(gradSum, _scaling);\n+                    inputGradMatrix[b, i] = NumOps.Add(inputGradMatrix[b, i], scaled);\n+                }\n+            }\n+        }\n+\n+        // Convert input gradient back to tensor\n+        Vector<T> inputGradData = new Vector<T>(batchSize * inputSize);\n+        int idx = 0;\n+        for (int b = 0; b < batchSize; b++)\n+        {\n+            for (int i = 0; i < inputSize; i++)\n+            {\n+                inputGradData[idx++] = inputGradMatrix[b, i];\n+            }\n+        }\n+\n+        return new Tensor<T>(new[] { batchSize, inputSize }, inputGradData);\n+    }\n+\n+    /// <summary>\n+    /// Computes the gradient for Hadamard product operation.\n+    /// </summary>\n+    /// <param name=\"outputGrad\">Output gradient scalar.</param>\n+    /// <param name=\"weightMatrix\">Weight matrix B[r].</param>\n+    /// <param name=\"outputIdx\">Output dimension index.</param>\n+    /// <returns>Gradient contribution from Hadamard product.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// For Hadamard product f ‚äô g, the gradient is: d/df (f ‚äô g) = g\n+    /// This method computes the gradient contribution from the weight matrix.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> When you have element-wise multiplication z = x * y,\n+    /// the gradient dL/dx = dL/dz * y. This method computes that for the Hadamard product.\n+    /// </para>\n+    /// </remarks>\n+    private T HadamardGradient(T outputGrad, Matrix<T> weightMatrix, int outputIdx)\n+    {\n+        // For element-wise product, gradient is: dL/dinput = dL/doutput * weight\n+        // Average the weight across input dimension\n+        T sum = NumOps.Zero;\n+        for (int i = 0; i < weightMatrix.Rows; i++)\n+        {\n+            sum = NumOps.Add(sum, weightMatrix[i, outputIdx]);\n+        }\n+        T avg = NumOps.Divide(sum, NumOps.FromDouble(weightMatrix.Rows));\n+        return NumOps.Multiply(outputGrad, avg);\n+    }","path":"src/LoRA/Adapters/LoHaAdapter.cs","commit_id":"fa815036ee570162056ba6aca773f34c80f36948","original_commit_id":"8d5a1b6564c1a2bc1b334f1863e6003a6fd142b2","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**HadamardGradient is mathematically inconsistent.**\n\nThis helper averages the weight matrix across the input dimension (lines 575-580), which doesn't match the mathematical definition of a Hadamard product gradient. For element-wise multiplication `z = x ‚äô y`, the gradient is `dL/dx = dL/dz ‚äô y` (element-wise, not averaged).\n\nThis is consistent with the flawed `HadamardProduct` implementation (lines 346-374), but both need correction to implement proper LoHa.\n\n\n\nThe gradient computation should use the specific matrix element, not an average:\n```diff\n-T sum = NumOps.Zero;\n-for (int i = 0; i < weightMatrix.Rows; i++)\n-{\n-    sum = NumOps.Add(sum, weightMatrix[i, outputIdx]);\n-}\n-T avg = NumOps.Divide(sum, NumOps.FromDouble(weightMatrix.Rows));\n-return NumOps.Multiply(outputGrad, avg);\n+// For proper Hadamard gradient, return the specific element\n+// (This assumes proper shape alignment between gradient and weight matrix)\n+return NumOps.Multiply(outputGrad, weightMatrix[inputIdx, outputIdx]);\n```\n\nHowever, fixing this requires resolving the dimension mismatch in the forward pass first.\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/LoRA/Adapters/LoHaAdapter.cs around lines 571 to 582, HadamardGradient\nincorrectly averages the weight column across the input dimension; change it to\nuse the corresponding per-element weight (no averaging) so the gradient is\noutputGrad * weight[inputIdx, outputIdx]; to do this, adjust the method\nsignature to accept the input index (or the full input vector) so you can\nmultiply outputGrad element-wise by weightMatrix[inputIdx, outputIdx] for each\ninput element, and before that fix the forward HadamardProduct implementation\nand its shapes so the forward pass produces element-wise outputs (same shape as\ninput) and the gradient can index into the matching element rather than\naveraging across rows.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T03:02:27Z","updated_at":"2025-11-02T03:02:31Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140287","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140287"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140287"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140287/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":571,"original_start_line":571,"start_side":"RIGHT","line":582,"original_line":582,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":582,"position":582,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140289","pull_request_review_id":3408046172,"id":2484140289,"node_id":"PRRC_kwDOKSXUF86UEPkB","diff_hunk":"@@ -0,0 +1,903 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// LoHa (Low-Rank Hadamard Product Adaptation) adapter for parameter-efficient fine-tuning.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// LoHa uses element-wise Hadamard products (‚äô) instead of matrix multiplication for adaptation.\n+/// Instead of computing ŒîW = B * A like standard LoRA, LoHa computes:\n+/// ŒîW = sum over rank of (A[i] ‚äô B[i])\n+///\n+/// This formulation can capture element-wise patterns that matrix multiplication may miss,\n+/// making it particularly effective for:\n+/// - Convolutional layers (local spatial patterns)\n+/// - Element-wise transformations\n+/// - Fine-grained weight adjustments\n+/// </para>\n+/// <para><b>Mathematical Formulation:</b>\n+///\n+/// Standard LoRA: ŒîW = B * A where B is rank√óoutput, A is input√órank\n+/// LoHa: ŒîW = Œ£(A[i] ‚äô B[i]) where A[i] and B[i] are both input√óoutput\n+///\n+/// The Hadamard product (‚äô) performs element-wise multiplication, allowing each element\n+/// of the weight matrix to be adjusted independently across the rank dimensions.\n+/// </para>\n+/// <para><b>For Beginners:</b> LoHa is a variant of LoRA that uses element-wise multiplication\n+/// instead of matrix multiplication. Think of it this way:\n+///\n+/// - Standard LoRA: Learns \"row and column patterns\" that combine via matrix multiply\n+/// - LoHa: Learns \"pixel-by-pixel patterns\" that combine via element-wise multiply\n+///\n+/// LoHa is especially good when:\n+/// 1. You need to capture local, element-wise patterns (like in images)\n+/// 2. The weight matrix has spatial structure (like convolutional filters)\n+/// 3. You want each weight to be adjusted somewhat independently\n+///\n+/// Trade-offs compared to LoRA:\n+/// - More parameters: Both A and B must be full-sized (input√óoutput) per rank dimension\n+/// - Different expressiveness: Better for element-wise patterns, different from matrix patterns\n+/// - Better for CNNs: The element-wise nature matches convolutional structure better\n+///\n+/// Example: A 100√ó100 weight matrix with rank=8\n+/// - Standard LoRA: 8√ó100 + 100√ó8 = 1,600 parameters\n+/// - LoHa: 8√ó(100√ó100) + 8√ó(100√ó100) = 160,000 parameters\n+///\n+/// Despite more parameters, LoHa is still far more efficient than full fine-tuning (10,000 params).\n+/// </para>\n+/// </remarks>\n+public class LoHaAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Low-rank matrices A with dimensions (rank, inputSize, outputSize).\n+    /// Each A[i] is a full-sized matrix for the i-th rank dimension.\n+    /// </summary>\n+    private readonly Matrix<T>[] _matricesA;\n+\n+    /// <summary>\n+    /// Low-rank matrices B with dimensions (rank, inputSize, outputSize).\n+    /// Each B[i] is a full-sized matrix for the i-th rank dimension.\n+    /// </summary>\n+    private readonly Matrix<T>[] _matricesB;\n+\n+    /// <summary>\n+    /// Gradients for matrices A computed during backpropagation.\n+    /// </summary>\n+    private Matrix<T>[]? _matricesAGradient;\n+\n+    /// <summary>\n+    /// Gradients for matrices B computed during backpropagation.\n+    /// </summary>\n+    private Matrix<T>[]? _matricesBGradient;\n+\n+    /// <summary>\n+    /// Stored input from the forward pass, needed for gradient computation.\n+    /// </summary>\n+    private Tensor<T>? _lastInput;\n+\n+    /// <summary>\n+    /// Stored base layer output from the forward pass.\n+    /// </summary>\n+    private Tensor<T>? _lastBaseOutput;\n+\n+    /// <summary>\n+    /// Computed scaling factor (alpha / rank) used during forward pass.\n+    /// </summary>\n+    private readonly T _scaling;\n+\n+    /// <summary>\n+    /// Initializes a new LoHa adapter wrapping an existing layer.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with LoHa.</param>\n+    /// <param name=\"rank\">The rank of the low-rank decomposition.</param>\n+    /// <param name=\"alpha\">The LoHa scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when the base layer doesn't have 1D input/output shapes.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This creates a LoHa adapter for any layer with 1D input/output.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to make more efficient to fine-tune\n+    /// - rank: How many element-wise patterns to learn (more = more flexibility, more parameters)\n+    /// - alpha: How strong the LoHa adaptation is (typically same as rank)\n+    /// - freezeBaseLayer: Whether to lock the original layer's weights (usually true for efficiency)\n+    ///\n+    /// The adapter creates 2√órank full-sized matrices (A and B for each rank dimension),\n+    /// which are combined using element-wise Hadamard products during forward/backward passes.\n+    /// </para>\n+    /// </remarks>\n+    public LoHaAdapter(ILayer<T> baseLayer, int rank, double alpha = -1, bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        // Validate base layer has single-dimensional input/output\n+        if (baseLayer.GetInputShape().Length != 1 || baseLayer.GetOutputShape().Length != 1)\n+        {\n+            throw new ArgumentException(\"LoHaAdapter only supports layers with 1D input/output shapes\", nameof(baseLayer));\n+        }\n+\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+\n+        // Calculate scaling\n+        _scaling = NumOps.Divide(_loraLayer.Alpha, NumOps.FromDouble(rank));\n+\n+        // Initialize LoHa matrices (rank sets of full-sized matrices)\n+        _matricesA = new Matrix<T>[rank];\n+        _matricesB = new Matrix<T>[rank];\n+\n+        for (int r = 0; r < rank; r++)\n+        {\n+            // Initialize A[r] with random values (Gaussian with std = 1/sqrt(rank))\n+            _matricesA[r] = new Matrix<T>(inputSize, outputSize);\n+            T stddev = NumOps.Sqrt(NumOps.Divide(NumOps.One, NumOps.FromDouble(rank)));\n+            for (int i = 0; i < inputSize; i++)\n+            {\n+                for (int j = 0; j < outputSize; j++)\n+                {\n+                    // Box-Muller transform for Gaussian random numbers\n+                    double u1 = Random.NextDouble();\n+                    double u2 = Random.NextDouble();\n+                    double randStdNormal = Math.Sqrt(-2.0 * Math.Log(u1)) * Math.Sin(2.0 * Math.PI * u2);\n+                    _matricesA[r][i, j] = NumOps.Multiply(NumOps.FromDouble(randStdNormal), stddev);\n+                }\n+            }\n+\n+            // Initialize B[r] to zero (so LoHa has no effect initially)\n+            _matricesB[r] = new Matrix<T>(inputSize, outputSize);\n+            for (int i = 0; i < inputSize; i++)\n+            {\n+                for (int j = 0; j < outputSize; j++)\n+                {\n+                    _matricesB[r][i, j] = NumOps.Zero;\n+                }\n+            }\n+        }\n+\n+        // Initialize parameter vector\n+        Parameters = new Vector<T>(ParameterCount);\n+        UpdateParametersFromMatrices();\n+    }\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters.\n+    /// </summary>\n+    /// <remarks>\n+    /// LoHa has 2 * rank * inputSize * outputSize parameters (A and B matrices for each rank).\n+    /// This is more than standard LoRA but still far less than full fine-tuning.\n+    /// </remarks>\n+    public override int ParameterCount\n+    {\n+        get\n+        {\n+            int inputSize = GetInputShape()[0];\n+            int outputSize = GetOutputShape()[0];\n+            int lohaParams = 2 * Rank * inputSize * outputSize;\n+            return _freezeBaseLayer ? lohaParams : (_baseLayer.ParameterCount + lohaParams);\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass through both base layer and LoHa adaptation.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Sum of base layer output and LoHa delta (computed via Hadamard products).</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The forward pass computes:\n+    /// 1. base_output = base_layer(input)\n+    /// 2. loha_delta = sum over rank of (input * A[i] ‚äô B[i]) * scaling\n+    /// 3. output = base_output + loha_delta\n+    ///\n+    /// The Hadamard product (‚äô) multiplies corresponding elements, allowing element-wise adaptations.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This runs the input through the original layer and adds a correction.\n+    ///\n+    /// The correction is computed by:\n+    /// 1. Transforming input through each A[i] matrix (one per rank dimension)\n+    /// 2. Multiplying element-wise with corresponding B[i] matrix (Hadamard product)\n+    /// 3. Summing all rank contributions together\n+    /// 4. Scaling by alpha/rank\n+    ///\n+    /// This element-wise approach lets LoHa learn fine-grained adjustments to each weight independently.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        _lastInput = input.Clone();\n+\n+        // Forward through base layer\n+        Tensor<T> baseOutput = _baseLayer.Forward(input);\n+        _lastBaseOutput = baseOutput.Clone();\n+\n+        // Compute LoHa delta using Hadamard products\n+        Tensor<T> lohaDelta = ComputeLoHaDelta(input);\n+\n+        // Sum the outputs: base + loha_delta\n+        Tensor<T> result = new Tensor<T>(baseOutput.Shape);\n+        for (int i = 0; i < baseOutput.Length; i++)\n+        {\n+            result[i] = NumOps.Add(baseOutput[i], lohaDelta[i]);\n+        }\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Computes the LoHa delta using Hadamard products across all rank dimensions.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor of shape [batchSize, inputSize].</param>\n+    /// <returns>LoHa delta tensor of shape [batchSize, outputSize].</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Computes: delta = scaling * sum over rank of (input * A[i]) ‚äô B[i]\n+    ///\n+    /// For each rank dimension i:\n+    /// 1. Multiply input by A[i] matrix: intermediate[i] = input * A[i]\n+    /// 2. Apply Hadamard product with B[i]: result[i] = intermediate[i] ‚äô B[i]\n+    /// 3. Sum all results and scale: delta = scaling * sum(result[i])\n+    /// </para>\n+    /// </remarks>\n+    private Tensor<T> ComputeLoHaDelta(Tensor<T> input)\n+    {\n+        int batchSize = input.Shape[0];\n+        int inputSize = input.Shape.Length > 1 ? input.Shape[1] : input.Length;\n+        int outputSize = GetOutputShape()[0];\n+\n+        // Convert input to matrix [batchSize, inputSize]\n+        Matrix<T> inputMatrix = new Matrix<T>(batchSize, inputSize);\n+        for (int b = 0; b < batchSize; b++)\n+        {\n+            for (int i = 0; i < inputSize; i++)\n+            {\n+                inputMatrix[b, i] = input[b * inputSize + i];\n+            }\n+        }\n+\n+        // Accumulate Hadamard product results across all ranks\n+        Matrix<T> deltaMatrix = new Matrix<T>(batchSize, outputSize);\n+        for (int b = 0; b < batchSize; b++)\n+        {\n+            for (int o = 0; o < outputSize; o++)\n+            {\n+                deltaMatrix[b, o] = NumOps.Zero;\n+            }\n+        }\n+\n+        // Sum over rank: delta += (input * A[r]) ‚äô B[r] for each r\n+        for (int r = 0; r < Rank; r++)\n+        {\n+            // Compute input * A[r] for each batch and output dimension\n+            Matrix<T> intermediate = new Matrix<T>(batchSize, outputSize);\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                for (int o = 0; o < outputSize; o++)\n+                {\n+                    T sum = NumOps.Zero;\n+                    for (int i = 0; i < inputSize; i++)\n+                    {\n+                        // (input * A[r])[b, o] = sum over i of input[b, i] * A[r][i, o]\n+                        sum = NumOps.Add(sum, NumOps.Multiply(inputMatrix[b, i], _matricesA[r][i, o]));\n+                    }\n+                    intermediate[b, o] = sum;\n+                }\n+            }\n+\n+            // Apply Hadamard product with B[r]: result ‚äô= B[r]\n+            Matrix<T> hadamardResult = HadamardProduct(intermediate, _matricesB[r]);\n+\n+            // Accumulate into delta\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                for (int o = 0; o < outputSize; o++)\n+                {\n+                    deltaMatrix[b, o] = NumOps.Add(deltaMatrix[b, o], hadamardResult[b, o]);\n+                }\n+            }\n+        }\n+\n+        // Apply scaling\n+        for (int b = 0; b < batchSize; b++)\n+        {\n+            for (int o = 0; o < outputSize; o++)\n+            {\n+                deltaMatrix[b, o] = NumOps.Multiply(deltaMatrix[b, o], _scaling);\n+            }\n+        }\n+\n+        // Convert back to tensor\n+        Vector<T> deltaData = new Vector<T>(batchSize * outputSize);\n+        int idx = 0;\n+        for (int b = 0; b < batchSize; b++)\n+        {\n+            for (int o = 0; o < outputSize; o++)\n+            {\n+                deltaData[idx++] = deltaMatrix[b, o];\n+            }\n+        }\n+\n+        return new Tensor<T>(new[] { batchSize, outputSize }, deltaData);\n+    }\n+\n+    /// <summary>\n+    /// Computes element-wise Hadamard product between a batch matrix and a weight matrix.\n+    /// </summary>\n+    /// <param name=\"batchMatrix\">Matrix of shape [batchSize, size].</param>\n+    /// <param name=\"weightMatrix\">Matrix of shape [inputSize, outputSize] (broadcasted across batch).</param>\n+    /// <returns>Hadamard product result of same shape as batchMatrix.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// For LoHa, the Hadamard product is applied between the intermediate activations\n+    /// (batchSize √ó outputSize) and the B matrix (inputSize √ó outputSize).\n+    ///\n+    /// Since the intermediate is [batch, output] and B is [input, output], we take the\n+    /// element-wise product along the output dimension.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> The Hadamard product is just element-wise multiplication.\n+    /// For each position (i, j), multiply the corresponding elements: result[i,j] = a[i,j] * b[i,j]\n+    ///\n+    /// This is different from matrix multiplication, which sums over a dimension.\n+    /// Hadamard product keeps dimensions the same and multiplies element-by-element.\n+    /// </para>\n+    /// </remarks>\n+    private Matrix<T> HadamardProduct(Matrix<T> batchMatrix, Matrix<T> weightMatrix)\n+    {\n+        int batchSize = batchMatrix.Rows;\n+        int outputSize = batchMatrix.Columns;\n+\n+        // For LoHa: batchMatrix is [batch, output], weightMatrix is [input, output]\n+        // We broadcast weightMatrix across batch dimension and multiply element-wise along output\n+        Matrix<T> result = new Matrix<T>(batchSize, outputSize);\n+\n+        for (int b = 0; b < batchSize; b++)\n+        {\n+            for (int o = 0; o < outputSize; o++)\n+            {\n+                // Since intermediate is already projected to output space,\n+                // we multiply element-wise with the first row of B\n+                // (This is a simplification; full LoHa may have different broadcasting)\n+                T sum = NumOps.Zero;\n+                for (int i = 0; i < weightMatrix.Rows; i++)\n+                {\n+                    sum = NumOps.Add(sum, weightMatrix[i, o]);\n+                }\n+                // Average across input dimension\n+                T avg = NumOps.Divide(sum, NumOps.FromDouble(weightMatrix.Rows));\n+                result[b, o] = NumOps.Multiply(batchMatrix[b, o], avg);\n+            }\n+        }\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Performs the backward pass through both layers, computing gradients for LoHa matrices.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient flowing back from the next layer.</param>\n+    /// <returns>Gradient to pass to the previous layer.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The backward pass computes gradients using the chain rule for Hadamard products:\n+    ///\n+    /// dL/dA[r] = input^T * (dL/doutput ‚äô B[r]) * scaling\n+    /// dL/dB[r] = (input * A[r]) ‚äô dL/doutput * scaling\n+    /// dL/dinput = base_gradient + sum over rank of (dL/doutput ‚äô B[r]) * A[r]^T * scaling\n+    ///\n+    /// The Hadamard product gradient rule: d/dx (f ‚äô g) = df ‚äô g + f ‚äô dg\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is the learning phase for LoHa. It computes:\n+    ///\n+    /// 1. How to adjust each A[i] matrix to reduce error\n+    /// 2. How to adjust each B[i] matrix to reduce error\n+    /// 3. What gradient to send to earlier layers\n+    ///\n+    /// The math is more complex than standard LoRA because Hadamard products have different\n+    /// derivative rules than matrix multiplication, but the idea is the same: figure out\n+    /// how each parameter contributed to the error and adjust accordingly.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (_lastInput == null || _lastBaseOutput == null)\n+        {\n+            throw new InvalidOperationException(\"Forward pass must be called before backward pass\");\n+        }\n+\n+        // Backward through base layer\n+        Tensor<T> baseInputGrad = _baseLayer.Backward(outputGradient);\n+\n+        // Compute LoHa gradients\n+        Tensor<T> lohaInputGrad = ComputeLoHaGradients(outputGradient);\n+\n+        // Sum input gradients\n+        Tensor<T> inputGrad = new Tensor<T>(lohaInputGrad.Shape);\n+        for (int i = 0; i < lohaInputGrad.Length; i++)\n+        {\n+            inputGrad[i] = NumOps.Add(lohaInputGrad[i], baseInputGrad[i]);\n+        }\n+\n+        // Update parameter gradients vector\n+        UpdateParameterGradientsFromMatrices();\n+\n+        return inputGrad;\n+    }\n+\n+    /// <summary>\n+    /// Computes gradients for LoHa matrices A and B using Hadamard product gradient rules.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient flowing back from next layer.</param>\n+    /// <returns>Input gradient from LoHa path.</returns>\n+    private Tensor<T> ComputeLoHaGradients(Tensor<T> outputGradient)\n+    {\n+        int batchSize = _lastInput!.Shape[0];\n+        int inputSize = _lastInput!.Shape.Length > 1 ? _lastInput.Shape[1] : _lastInput.Length;\n+        int outputSize = GetOutputShape()[0];\n+\n+        // Convert to matrices\n+        Matrix<T> inputMatrix = new Matrix<T>(batchSize, inputSize);\n+        for (int b = 0; b < batchSize; b++)\n+        {\n+            for (int i = 0; i < inputSize; i++)\n+            {\n+                inputMatrix[b, i] = _lastInput[b * inputSize + i];\n+            }\n+        }\n+\n+        Matrix<T> gradMatrix = new Matrix<T>(batchSize, outputSize);\n+        for (int b = 0; b < batchSize; b++)\n+        {\n+            for (int o = 0; o < outputSize; o++)\n+            {\n+                gradMatrix[b, o] = outputGradient[b * outputSize + o];\n+            }\n+        }\n+\n+        // Initialize gradients\n+        _matricesAGradient = new Matrix<T>[Rank];\n+        _matricesBGradient = new Matrix<T>[Rank];\n+        for (int r = 0; r < Rank; r++)\n+        {\n+            _matricesAGradient[r] = new Matrix<T>(inputSize, outputSize);\n+            _matricesBGradient[r] = new Matrix<T>(inputSize, outputSize);\n+        }\n+\n+        // Accumulate input gradients\n+        Matrix<T> inputGradMatrix = new Matrix<T>(batchSize, inputSize);\n+\n+        // For each rank dimension, compute gradients\n+        for (int r = 0; r < Rank; r++)\n+        {\n+            // Compute intermediate = input * A[r]\n+            Matrix<T> intermediate = new Matrix<T>(batchSize, outputSize);\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                for (int o = 0; o < outputSize; o++)\n+                {\n+                    T sum = NumOps.Zero;\n+                    for (int i = 0; i < inputSize; i++)\n+                    {\n+                        sum = NumOps.Add(sum, NumOps.Multiply(inputMatrix[b, i], _matricesA[r][i, o]));\n+                    }\n+                    intermediate[b, o] = sum;\n+                }\n+            }\n+\n+            // Gradient for B[r]: dL/dB[r] = intermediate^T * gradOutput (with Hadamard consideration)\n+            // For element-wise operations: dL/dB = dL/doutput ‚äô intermediate\n+            for (int i = 0; i < inputSize; i++)\n+            {\n+                for (int o = 0; o < outputSize; o++)\n+                {\n+                    T gradSum = NumOps.Zero;\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        // Compute contribution from this batch\n+                        T contribution = NumOps.Multiply(gradMatrix[b, o], intermediate[b, o]);\n+                        gradSum = NumOps.Add(gradSum, contribution);\n+                    }\n+                    _matricesBGradient[r][i, o] = NumOps.Multiply(gradSum, _scaling);\n+                }\n+            }\n+\n+            // Gradient for A[r]: dL/dA[r] = input^T * (gradOutput ‚äô B[r])\n+            for (int i = 0; i < inputSize; i++)\n+            {\n+                for (int o = 0; o < outputSize; o++)\n+                {\n+                    T gradSum = NumOps.Zero;\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        // Element-wise gradient with B\n+                        T hadamardGrad = HadamardGradient(gradMatrix[b, o], _matricesB[r], o);\n+                        T contribution = NumOps.Multiply(inputMatrix[b, i], hadamardGrad);\n+                        gradSum = NumOps.Add(gradSum, contribution);\n+                    }\n+                    _matricesAGradient[r][i, o] = NumOps.Multiply(gradSum, _scaling);\n+                }\n+            }\n+\n+            // Input gradient contribution from this rank\n+            // dL/dinput = (gradOutput ‚äô B[r]) * A[r]^T\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                for (int i = 0; i < inputSize; i++)\n+                {\n+                    T gradSum = NumOps.Zero;\n+                    for (int o = 0; o < outputSize; o++)\n+                    {\n+                        T hadamardGrad = HadamardGradient(gradMatrix[b, o], _matricesB[r], o);\n+                        T contribution = NumOps.Multiply(hadamardGrad, _matricesA[r][i, o]);\n+                        gradSum = NumOps.Add(gradSum, contribution);\n+                    }\n+                    T scaled = NumOps.Multiply(gradSum, _scaling);\n+                    inputGradMatrix[b, i] = NumOps.Add(inputGradMatrix[b, i], scaled);\n+                }\n+            }\n+        }\n+\n+        // Convert input gradient back to tensor\n+        Vector<T> inputGradData = new Vector<T>(batchSize * inputSize);\n+        int idx = 0;\n+        for (int b = 0; b < batchSize; b++)\n+        {\n+            for (int i = 0; i < inputSize; i++)\n+            {\n+                inputGradData[idx++] = inputGradMatrix[b, i];\n+            }\n+        }\n+\n+        return new Tensor<T>(new[] { batchSize, inputSize }, inputGradData);\n+    }\n+\n+    /// <summary>\n+    /// Computes the gradient for Hadamard product operation.\n+    /// </summary>\n+    /// <param name=\"outputGrad\">Output gradient scalar.</param>\n+    /// <param name=\"weightMatrix\">Weight matrix B[r].</param>\n+    /// <param name=\"outputIdx\">Output dimension index.</param>\n+    /// <returns>Gradient contribution from Hadamard product.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// For Hadamard product f ‚äô g, the gradient is: d/df (f ‚äô g) = g\n+    /// This method computes the gradient contribution from the weight matrix.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> When you have element-wise multiplication z = x * y,\n+    /// the gradient dL/dx = dL/dz * y. This method computes that for the Hadamard product.\n+    /// </para>\n+    /// </remarks>\n+    private T HadamardGradient(T outputGrad, Matrix<T> weightMatrix, int outputIdx)\n+    {\n+        // For element-wise product, gradient is: dL/dinput = dL/doutput * weight\n+        // Average the weight across input dimension\n+        T sum = NumOps.Zero;\n+        for (int i = 0; i < weightMatrix.Rows; i++)\n+        {\n+            sum = NumOps.Add(sum, weightMatrix[i, outputIdx]);\n+        }\n+        T avg = NumOps.Divide(sum, NumOps.FromDouble(weightMatrix.Rows));\n+        return NumOps.Multiply(outputGrad, avg);\n+    }\n+\n+    /// <summary>\n+    /// Updates parameters using the specified learning rate.\n+    /// </summary>\n+    /// <param name=\"learningRate\">The learning rate for parameter updates.</param>\n+    public override void UpdateParameters(T learningRate)\n+    {\n+        if (_matricesAGradient == null || _matricesBGradient == null)\n+        {\n+            return;\n+        }\n+\n+        // Update all A and B matrices\n+        for (int r = 0; r < Rank; r++)\n+        {\n+            // Update A[r]\n+            for (int i = 0; i < _matricesA[r].Rows; i++)\n+            {\n+                for (int j = 0; j < _matricesA[r].Columns; j++)\n+                {\n+                    T update = NumOps.Multiply(_matricesAGradient[r][i, j], learningRate);\n+                    _matricesA[r][i, j] = NumOps.Subtract(_matricesA[r][i, j], update);\n+                }\n+            }\n+\n+            // Update B[r]\n+            for (int i = 0; i < _matricesB[r].Rows; i++)\n+            {\n+                for (int j = 0; j < _matricesB[r].Columns; j++)\n+                {\n+                    T update = NumOps.Multiply(_matricesBGradient[r][i, j], learningRate);\n+                    _matricesB[r][i, j] = NumOps.Subtract(_matricesB[r][i, j], update);\n+                }\n+            }\n+        }\n+\n+        // Update base layer if not frozen\n+        if (!_freezeBaseLayer)\n+        {\n+            _baseLayer.UpdateParameters(learningRate);\n+        }\n+\n+        // Update parameter vector\n+        UpdateParametersFromMatrices();\n+    }\n+\n+    /// <summary>\n+    /// Gets the current parameters as a vector.\n+    /// </summary>\n+    /// <returns>Vector containing all LoHa parameters (A and B matrices for all ranks).</returns>\n+    public override Vector<T> GetParameters()\n+    {\n+        return Parameters.Clone();\n+    }\n+\n+    /// <summary>\n+    /// Sets the layer parameters from a vector.\n+    /// </summary>\n+    /// <param name=\"parameters\">Vector containing all LoHa parameters.</param>\n+    public override void SetParameters(Vector<T> parameters)\n+    {\n+        if (parameters.Length != ParameterCount)\n+        {\n+            throw new ArgumentException($\"Expected {ParameterCount} parameters, got {parameters.Length}\", nameof(parameters));\n+        }\n+\n+        Parameters = parameters.Clone();\n+        UpdateMatricesFromParameters();\n+    }\n+\n+    /// <summary>\n+    /// Updates the parameter vector from the current matrix values.\n+    /// </summary>\n+    private void UpdateParametersFromMatrices()\n+    {\n+        int idx = 0;\n+\n+        // Pack base layer parameters if not frozen\n+        if (!_freezeBaseLayer)\n+        {\n+            Vector<T> baseParams = _baseLayer.GetParameters();\n+            for (int i = 0; i < baseParams.Length; i++)\n+            {\n+                Parameters[idx++] = baseParams[i];\n+            }\n+        }\n+\n+        // Pack all A matrices\n+        for (int r = 0; r < Rank; r++)\n+        {\n+            for (int i = 0; i < _matricesA[r].Rows; i++)\n+            {\n+                for (int j = 0; j < _matricesA[r].Columns; j++)\n+                {\n+                    Parameters[idx++] = _matricesA[r][i, j];\n+                }\n+            }\n+        }\n+\n+        // Pack all B matrices\n+        for (int r = 0; r < Rank; r++)\n+        {\n+            for (int i = 0; i < _matricesB[r].Rows; i++)\n+            {\n+                for (int j = 0; j < _matricesB[r].Columns; j++)\n+                {\n+                    Parameters[idx++] = _matricesB[r][i, j];\n+                }\n+            }\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Updates the matrices from the parameter vector.\n+    /// </summary>\n+    private void UpdateMatricesFromParameters()\n+    {\n+        int idx = 0;\n+\n+        // Unpack base layer parameters if not frozen\n+        if (!_freezeBaseLayer)\n+        {\n+            int baseParamCount = _baseLayer.ParameterCount;\n+            Vector<T> baseParams = new Vector<T>(baseParamCount);\n+            for (int i = 0; i < baseParamCount; i++)\n+            {\n+                baseParams[i] = Parameters[idx++];\n+            }\n+            _baseLayer.SetParameters(baseParams);\n+        }\n+\n+        // Unpack all A matrices\n+        for (int r = 0; r < Rank; r++)\n+        {\n+            for (int i = 0; i < _matricesA[r].Rows; i++)\n+            {\n+                for (int j = 0; j < _matricesA[r].Columns; j++)\n+                {\n+                    _matricesA[r][i, j] = Parameters[idx++];\n+                }\n+            }\n+        }\n+\n+        // Unpack all B matrices\n+        for (int r = 0; r < Rank; r++)\n+        {\n+            for (int i = 0; i < _matricesB[r].Rows; i++)\n+            {\n+                for (int j = 0; j < _matricesB[r].Columns; j++)\n+                {\n+                    _matricesB[r][i, j] = Parameters[idx++];\n+                }\n+            }\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Updates the parameter gradients vector from the matrix gradients.\n+    /// </summary>\n+    private void UpdateParameterGradientsFromMatrices()\n+    {\n+        if (_matricesAGradient == null || _matricesBGradient == null)\n+        {\n+            return;\n+        }\n+\n+        ParameterGradients = new Vector<T>(ParameterCount);\n+        int idx = 0;\n+\n+        // Pack base layer gradients if not frozen\n+        if (!_freezeBaseLayer)\n+        {\n+            Vector<T> baseGrads = _baseLayer.GetParameterGradients();\n+            for (int i = 0; i < baseGrads.Length; i++)\n+            {\n+                ParameterGradients[idx++] = baseGrads[i];\n+            }\n+        }\n+\n+        // Pack all A matrix gradients\n+        for (int r = 0; r < Rank; r++)\n+        {\n+            for (int i = 0; i < _matricesAGradient[r].Rows; i++)\n+            {\n+                for (int j = 0; j < _matricesAGradient[r].Columns; j++)\n+                {\n+                    ParameterGradients[idx++] = _matricesAGradient[r][i, j];\n+                }\n+            }\n+        }\n+\n+        // Pack all B matrix gradients\n+        for (int r = 0; r < Rank; r++)\n+        {\n+            for (int i = 0; i < _matricesBGradient[r].Rows; i++)\n+            {\n+                for (int j = 0; j < _matricesBGradient[r].Columns; j++)\n+                {\n+                    ParameterGradients[idx++] = _matricesBGradient[r][i, j];\n+                }\n+            }\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Merges the LoHa adaptation into the base layer and returns the merged layer.\n+    /// </summary>\n+    /// <returns>A new DenseLayer with LoHa weights merged into the base layer's weights.</returns>\n+    /// <exception cref=\"InvalidOperationException\">Thrown when the base layer type is not DenseLayer or FullyConnectedLayer.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// This method computes the full LoHa weight delta by summing all Hadamard products:\n+    /// ŒîW = scaling * sum over rank of (A[i] ‚äô B[i])\n+    ///\n+    /// The delta is then added to the base layer's weights to create a merged layer.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This \"bakes in\" your LoHa adaptation to create a regular Dense layer.\n+    ///\n+    /// The merging process:\n+    /// 1. Computes the full weight delta from all A[i] and B[i] matrices using Hadamard products\n+    /// 2. Adds this delta to the base layer's existing weights\n+    /// 3. Copies biases unchanged (LoHa doesn't modify biases)\n+    /// 4. Creates a new DenseLayer with the merged weights\n+    ///\n+    /// After merging, you have a single layer that includes all the learned adaptations,\n+    /// making inference faster and simpler.\n+    /// </para>\n+    /// </remarks>\n+    public override ILayer<T> MergeToOriginalLayer()\n+    {\n+        // Support both DenseLayer and FullyConnectedLayer\n+        DenseLayer<T>? denseBase = _baseLayer as DenseLayer<T>;\n+        FullyConnectedLayer<T>? fcBase = _baseLayer as FullyConnectedLayer<T>;\n+\n+        if (denseBase == null && fcBase == null)\n+        {\n+            throw new InvalidOperationException(\"LoHaAdapter only supports DenseLayer or FullyConnectedLayer base layers\");\n+        }\n+\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+\n+        // Compute LoHa weight delta: sum over rank of (A[r] ‚äô B[r]) * scaling\n+        Matrix<T> lohaDelta = new Matrix<T>(inputSize, outputSize);\n+        for (int i = 0; i < inputSize; i++)\n+        {\n+            for (int o = 0; o < outputSize; o++)\n+            {\n+                lohaDelta[i, o] = NumOps.Zero;\n+            }\n+        }\n+\n+        for (int r = 0; r < Rank; r++)\n+        {\n+            for (int i = 0; i < inputSize; i++)\n+            {\n+                for (int o = 0; o < outputSize; o++)\n+                {\n+                    // Hadamard product: A[r][i,o] * B[r][i,o]\n+                    T hadamard = NumOps.Multiply(_matricesA[r][i, o], _matricesB[r][i, o]);\n+                    lohaDelta[i, o] = NumOps.Add(lohaDelta[i, o], hadamard);\n+                }\n+            }\n+        }\n+\n+        // Apply scaling\n+        for (int i = 0; i < inputSize; i++)\n+        {\n+            for (int o = 0; o < outputSize; o++)\n+            {\n+                lohaDelta[i, o] = NumOps.Multiply(lohaDelta[i, o], _scaling);\n+            }\n+        }\n+\n+        // Get base layer parameters\n+        Vector<T> baseParams = _baseLayer.GetParameters();\n+        int weightCount = inputSize * outputSize;\n+\n+        // Create new parameters with merged weights\n+        Vector<T> mergedParams = new Vector<T>(baseParams.Length);\n+\n+        // Merge weights (base layer stores weights in row-major order: [output, input])\n+        for (int i = 0; i < weightCount; i++)\n+        {\n+            int row = i / inputSize;  // output index\n+            int col = i % inputSize;   // input index\n+            // lohaDelta is [input, output], so we transpose the indices\n+            mergedParams[i] = NumOps.Add(baseParams[i], lohaDelta[col, row]);\n+        }\n+\n+        // Copy biases unchanged\n+        for (int i = weightCount; i < baseParams.Length; i++)\n+        {\n+            mergedParams[i] = baseParams[i];\n+        }\n+\n+        // Create a new dense layer with merged parameters\n+        DenseLayer<T> mergedLayer = new DenseLayer<T>(inputSize, outputSize, (IActivationFunction<T>?)null);\n+        mergedLayer.SetParameters(mergedParams);\n+\n+        return mergedLayer;\n+    }\n+\n+    /// <summary>\n+    /// Resets the internal state of both the base layer and LoHa adapter.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This clears the memory of the adapter and base layer.\n+    /// It's useful when starting to process a completely new, unrelated batch of data.\n+    /// </para>\n+    /// </remarks>\n+    public override void ResetState()\n+    {\n+        _baseLayer.ResetState();\n+        _loraLayer.ResetState();\n+        _lastInput = null;\n+        _lastBaseOutput = null;\n+        _matricesAGradient = null;\n+        _matricesBGradient = null;\n+    }","path":"src/LoRA/Adapters/LoHaAdapter.cs","commit_id":"fa815036ee570162056ba6aca773f34c80f36948","original_commit_id":"8d5a1b6564c1a2bc1b334f1863e6003a6fd142b2","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Critical: ResetState references non-existent field.**\n\nLine 897 calls `_loraLayer.ResetState()`, but `LoHaAdapter` does not have a `_loraLayer` field. This will cause a compilation error.\n\nThis appears to be copy-pasted from another adapter (like standard `LoRAAdapter`) without proper adaptation for LoHa's architecture.\n\n\n\nRemove the non-existent field reference:\n\n```diff\n public override void ResetState()\n {\n     _baseLayer.ResetState();\n-    _loraLayer.ResetState();\n     _lastInput = null;\n     _lastBaseOutput = null;\n     _matricesAGradient = null;\n     _matricesBGradient = null;\n }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    public override void ResetState()\n    {\n        _baseLayer.ResetState();\n        _lastInput = null;\n        _lastBaseOutput = null;\n        _matricesAGradient = null;\n        _matricesBGradient = null;\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/LoRA/Adapters/LoHaAdapter.cs around lines 894 to 902, the ResetState\nmethod calls a non-existent field _loraLayer.ResetState(), causing a compile\nerror; remove that call and instead reset only actual LoHa fields (keep\n_baseLayer.ResetState() and nulling _lastInput, _lastBaseOutput,\n_matricesAGradient, _matricesBGradient). If LoHaAdapter has its own internal\nlayer(s) that need resetting, call their ResetState methods here using the\ncorrect field names; otherwise simply delete the _loraLayer.ResetState() line.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T03:02:27Z","updated_at":"2025-11-02T03:02:31Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140289","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140289"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140289"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140289/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":894,"original_start_line":894,"start_side":"RIGHT","line":902,"original_line":902,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":902,"position":902,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140294","pull_request_review_id":3408046172,"id":2484140294,"node_id":"PRRC_kwDOKSXUF86UEPkG","diff_hunk":"@@ -0,0 +1,759 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// LoKr (Low-Rank Kronecker Product Adaptation) adapter for parameter-efficient fine-tuning.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// LoKr uses Kronecker products instead of standard matrix multiplication for low-rank adaptation.\n+/// Instead of computing ŒîW = A √ó B (standard LoRA), LoKr computes ŒîW = A ‚äó B where ‚äó is the\n+/// Kronecker product. This is particularly efficient for very large weight matrices.\n+/// </para>\n+/// <para><b>Kronecker Product Definition:</b>\n+/// For matrices A (m√ón) and B (p√óq), the Kronecker product A ‚äó B is an (m√óp) √ó (n√óq) matrix:\n+///\n+/// A ‚äó B = [a‚ÇÅ‚ÇÅB  a‚ÇÅ‚ÇÇB  ...  a‚ÇÅ‚ÇôB]\n+///         [a‚ÇÇ‚ÇÅB  a‚ÇÇ‚ÇÇB  ...  a‚ÇÇ‚ÇôB]\n+///         [  ‚ãÆ     ‚ãÆ    ‚ã±    ‚ãÆ  ]\n+///         [a‚Çò‚ÇÅB  a‚Çò‚ÇÇB  ...  a‚Çò‚ÇôB]\n+///\n+/// Each element a·µ¢‚±º of A is multiplied by the entire matrix B, creating a block structure.\n+/// </para>\n+/// <para><b>For Beginners:</b> LoKr is a variant of LoRA that uses a different mathematical operation\n+/// called the Kronecker product. Think of it this way:\n+///\n+/// - Standard LoRA: Multiplies two small matrices (like 1000√ó8 and 8√ó1000) to approximate changes\n+/// - LoKr: Uses Kronecker product of two even smaller matrices (like 50√ó4 and 20√ó4) to create the same size output\n+///\n+/// The Kronecker product creates a larger matrix by taking every element of the first matrix and\n+/// multiplying it by the entire second matrix. This creates a block pattern that's very efficient\n+/// for representing certain types of structured transformations.\n+///\n+/// <b>When to use LoKr vs standard LoRA:</b>\n+/// - LoKr is better for very wide or very deep layers (e.g., 10000√ó10000 weight matrices)\n+/// - LoKr can achieve similar expressiveness with fewer parameters than LoRA\n+/// - Standard LoRA is simpler and works well for typical layer sizes\n+///\n+/// <b>Parameter Efficiency Example:</b>\n+/// For a 1000√ó1000 weight matrix with rank r=8:\n+/// - Standard LoRA: 1000√ó8 + 8√ó1000 = 16,000 parameters\n+/// - LoKr: 50√ó4 + 20√ó4 = 200 + 80 = 280 parameters (57x fewer!)\n+///   (where 50√ó20 = 1000 for both dimensions)\n+/// </para>\n+/// </remarks>\n+public class LoKrAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// First Kronecker factor matrix A with dimensions (m √ó n).\n+    /// </summary>\n+    /// <remarks>\n+    /// This is one of the two matrices used in the Kronecker product decomposition.\n+    /// </remarks>\n+    private Matrix<T> _matrixA;\n+\n+    /// <summary>\n+    /// Second Kronecker factor matrix B with dimensions (p √ó q).\n+    /// </summary>\n+    /// <remarks>\n+    /// This is the second matrix used in the Kronecker product decomposition.\n+    /// The Kronecker product A ‚äó B produces a (m√óp) √ó (n√óq) matrix.\n+    /// </remarks>\n+    private Matrix<T> _matrixB;\n+\n+    /// <summary>\n+    /// Scaling factor for the LoKr contribution.\n+    /// </summary>\n+    private readonly T _alpha;\n+\n+    /// <summary>\n+    /// Computed scaling factor (alpha / effective_rank) used during forward pass.\n+    /// </summary>\n+    private readonly T _scaling;\n+\n+    /// <summary>\n+    /// Gradients for matrix A computed during backpropagation.\n+    /// </summary>\n+    private Matrix<T>? _gradientA;\n+\n+    /// <summary>\n+    /// Gradients for matrix B computed during backpropagation.\n+    /// </summary>\n+    private Matrix<T>? _gradientB;\n+\n+    /// <summary>\n+    /// Stored input from the forward pass, needed for gradient computation.\n+    /// </summary>\n+    private Tensor<T>? _lastInput;\n+\n+    /// <summary>\n+    /// Dimensions for matrix A (m, n).\n+    /// </summary>\n+    private readonly (int m, int n) _dimsA;\n+\n+    /// <summary>\n+    /// Dimensions for matrix B (p, q).\n+    /// </summary>\n+    private readonly (int p, int q) _dimsB;\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters (elements in A and B matrices).\n+    /// </summary>\n+    public override int ParameterCount => (_matrixA.Rows * _matrixA.Columns) + (_matrixB.Rows * _matrixB.Columns);","path":"src/LoRA/Adapters/LoKrAdapter.cs","commit_id":"fa815036ee570162056ba6aca773f34c80f36948","original_commit_id":"8d5a1b6564c1a2bc1b334f1863e6003a6fd142b2","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**Include base layer parameters in ParameterCount when not frozen.**\n\nThe `ParameterCount` property only returns the sum of A and B matrix parameters, but when `_freezeBaseLayer` is false, the base layer parameters are also trainable and should be included in the count. Looking at LoHaAdapter (reference implementation), its parameter management includes base layer parameters when not frozen. This inconsistency will cause `GetParameters()` and `SetParameters()` to have incorrect vector sizes.\n\n\n\nApply this diff:\n\n```diff\n-    public override int ParameterCount => (_matrixA.Rows * _matrixA.Columns) + (_matrixB.Rows * _matrixB.Columns);\n+    public override int ParameterCount \n+    {\n+        get\n+        {\n+            int count = (_matrixA.Rows * _matrixA.Columns) + (_matrixB.Rows * _matrixB.Columns);\n+            if (!_freezeBaseLayer)\n+            {\n+                count += _baseLayer.ParameterCount;\n+            }\n+            return count;\n+        }\n+    }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    public override int ParameterCount \n    {\n        get\n        {\n            int count = (_matrixA.Rows * _matrixA.Columns) + (_matrixB.Rows * _matrixB.Columns);\n            if (!_freezeBaseLayer)\n            {\n                count += _baseLayer.ParameterCount;\n            }\n            return count;\n        }\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/LoRA/Adapters/LoKrAdapter.cs around line 104, ParameterCount currently\nreturns only the sum of _matrixA and _matrixB parameters; update it to include\nthe base layer parameters when _freezeBaseLayer is false (e.g., add\n(_baseLayer.Rows * _baseLayer.Columns) or use the base layer's own parameter\ncount) so the total matches GetParameters()/SetParameters() vector sizes; ensure\nthe conditional mirrors LoHaAdapter logic and that any null-checks or visibility\nof _baseLayer are handled.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T03:02:28Z","updated_at":"2025-11-02T03:02:32Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140294","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140294"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140294"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140294/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":104,"original_line":104,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":104,"position":104,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140298","pull_request_review_id":3408046172,"id":2484140298,"node_id":"PRRC_kwDOKSXUF86UEPkK","diff_hunk":"@@ -0,0 +1,759 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// LoKr (Low-Rank Kronecker Product Adaptation) adapter for parameter-efficient fine-tuning.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// LoKr uses Kronecker products instead of standard matrix multiplication for low-rank adaptation.\n+/// Instead of computing ŒîW = A √ó B (standard LoRA), LoKr computes ŒîW = A ‚äó B where ‚äó is the\n+/// Kronecker product. This is particularly efficient for very large weight matrices.\n+/// </para>\n+/// <para><b>Kronecker Product Definition:</b>\n+/// For matrices A (m√ón) and B (p√óq), the Kronecker product A ‚äó B is an (m√óp) √ó (n√óq) matrix:\n+///\n+/// A ‚äó B = [a‚ÇÅ‚ÇÅB  a‚ÇÅ‚ÇÇB  ...  a‚ÇÅ‚ÇôB]\n+///         [a‚ÇÇ‚ÇÅB  a‚ÇÇ‚ÇÇB  ...  a‚ÇÇ‚ÇôB]\n+///         [  ‚ãÆ     ‚ãÆ    ‚ã±    ‚ãÆ  ]\n+///         [a‚Çò‚ÇÅB  a‚Çò‚ÇÇB  ...  a‚Çò‚ÇôB]\n+///\n+/// Each element a·µ¢‚±º of A is multiplied by the entire matrix B, creating a block structure.\n+/// </para>\n+/// <para><b>For Beginners:</b> LoKr is a variant of LoRA that uses a different mathematical operation\n+/// called the Kronecker product. Think of it this way:\n+///\n+/// - Standard LoRA: Multiplies two small matrices (like 1000√ó8 and 8√ó1000) to approximate changes\n+/// - LoKr: Uses Kronecker product of two even smaller matrices (like 50√ó4 and 20√ó4) to create the same size output\n+///\n+/// The Kronecker product creates a larger matrix by taking every element of the first matrix and\n+/// multiplying it by the entire second matrix. This creates a block pattern that's very efficient\n+/// for representing certain types of structured transformations.\n+///\n+/// <b>When to use LoKr vs standard LoRA:</b>\n+/// - LoKr is better for very wide or very deep layers (e.g., 10000√ó10000 weight matrices)\n+/// - LoKr can achieve similar expressiveness with fewer parameters than LoRA\n+/// - Standard LoRA is simpler and works well for typical layer sizes\n+///\n+/// <b>Parameter Efficiency Example:</b>\n+/// For a 1000√ó1000 weight matrix with rank r=8:\n+/// - Standard LoRA: 1000√ó8 + 8√ó1000 = 16,000 parameters\n+/// - LoKr: 50√ó4 + 20√ó4 = 200 + 80 = 280 parameters (57x fewer!)\n+///   (where 50√ó20 = 1000 for both dimensions)\n+/// </para>\n+/// </remarks>\n+public class LoKrAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// First Kronecker factor matrix A with dimensions (m √ó n).\n+    /// </summary>\n+    /// <remarks>\n+    /// This is one of the two matrices used in the Kronecker product decomposition.\n+    /// </remarks>\n+    private Matrix<T> _matrixA;\n+\n+    /// <summary>\n+    /// Second Kronecker factor matrix B with dimensions (p √ó q).\n+    /// </summary>\n+    /// <remarks>\n+    /// This is the second matrix used in the Kronecker product decomposition.\n+    /// The Kronecker product A ‚äó B produces a (m√óp) √ó (n√óq) matrix.\n+    /// </remarks>\n+    private Matrix<T> _matrixB;\n+\n+    /// <summary>\n+    /// Scaling factor for the LoKr contribution.\n+    /// </summary>\n+    private readonly T _alpha;\n+\n+    /// <summary>\n+    /// Computed scaling factor (alpha / effective_rank) used during forward pass.\n+    /// </summary>\n+    private readonly T _scaling;\n+\n+    /// <summary>\n+    /// Gradients for matrix A computed during backpropagation.\n+    /// </summary>\n+    private Matrix<T>? _gradientA;\n+\n+    /// <summary>\n+    /// Gradients for matrix B computed during backpropagation.\n+    /// </summary>\n+    private Matrix<T>? _gradientB;\n+\n+    /// <summary>\n+    /// Stored input from the forward pass, needed for gradient computation.\n+    /// </summary>\n+    private Tensor<T>? _lastInput;\n+\n+    /// <summary>\n+    /// Dimensions for matrix A (m, n).\n+    /// </summary>\n+    private readonly (int m, int n) _dimsA;\n+\n+    /// <summary>\n+    /// Dimensions for matrix B (p, q).\n+    /// </summary>\n+    private readonly (int p, int q) _dimsB;\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters (elements in A and B matrices).\n+    /// </summary>\n+    public override int ParameterCount => (_matrixA.Rows * _matrixA.Columns) + (_matrixB.Rows * _matrixB.Columns);\n+\n+    /// <summary>\n+    /// Initializes a new LoKr adapter wrapping an existing layer.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with LoKr.</param>\n+    /// <param name=\"rank\">The effective rank of the decomposition (used to determine factor matrix sizes).</param>\n+    /// <param name=\"alpha\">The LoKr scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when the base layer doesn't have 1D input/output shapes.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// The LoKr matrices are initialized as follows:\n+    /// - Matrix A: Random values from a Gaussian distribution\n+    /// - Matrix B: Zero initialization (so LoKr starts with no effect)\n+    ///\n+    /// The dimensions of A and B are chosen such that A ‚äó B produces a matrix that can be applied\n+    /// to the layer's weights. For a layer with inputSize and outputSize, we factor these dimensions\n+    /// to create A (m√ón) and B (p√óq) where m√óp = outputSize and n√óq = inputSize.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This creates a LoKr adapter for a layer. The rank parameter determines\n+    /// how the weight matrix is factored into two smaller matrices. Lower rank = fewer parameters but\n+    /// less flexibility.\n+    ///\n+    /// The adapter automatically figures out the best sizes for matrices A and B based on your layer's\n+    /// input and output sizes and the rank you specify.\n+    /// </para>\n+    /// </remarks>\n+    public LoKrAdapter(ILayer<T> baseLayer, int rank, double alpha = -1, bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        // Validate base layer has single-dimensional input/output\n+        if (baseLayer.GetInputShape().Length != 1 || baseLayer.GetOutputShape().Length != 1)\n+        {\n+            throw new ArgumentException(\"LoKrAdapter only supports layers with 1D input/output shapes\", nameof(baseLayer));\n+        }\n+\n+        int inputSize = baseLayer.GetInputShape()[0];\n+        int outputSize = baseLayer.GetOutputShape()[0];\n+\n+        // Factor the dimensions to create Kronecker factors\n+        // We want m*p = outputSize and n*q = inputSize, with balanced factors\n+        _dimsA = FactorDimension(outputSize, rank);\n+        _dimsB = (outputSize / _dimsA.m, inputSize / _dimsA.n);\n+\n+        // Verify factorization is valid\n+        if (_dimsA.m * _dimsB.p != outputSize || _dimsA.n * _dimsB.q != inputSize)\n+        {\n+            throw new ArgumentException(\n+                $\"Cannot factor dimensions for LoKr: outputSize={outputSize}, inputSize={inputSize}, rank={rank}. \" +\n+                \"Try a different rank value or use dimensions that are more easily factorizable.\");\n+        }\n+\n+        // Initialize matrices\n+        _matrixA = new Matrix<T>(_dimsA.m, _dimsA.n);\n+        _matrixB = new Matrix<T>(_dimsB.p, _dimsB.q);\n+\n+        // Default alpha to rank if not specified\n+        _alpha = alpha > 0 ? NumOps.FromDouble(alpha) : NumOps.FromDouble(rank);\n+        int effectiveRank = _dimsA.n * _dimsB.q;\n+        _scaling = NumOps.Divide(_alpha, NumOps.FromDouble(effectiveRank));\n+\n+        // Initialize matrix A with random values (Gaussian with std = 1/sqrt(effectiveRank))\n+        T stddev = NumOps.Sqrt(NumOps.Divide(NumOps.One, NumOps.FromDouble(effectiveRank)));\n+        for (int i = 0; i < _matrixA.Rows; i++)\n+        {\n+            for (int j = 0; j < _matrixA.Columns; j++)\n+            {\n+                double u1 = Random.NextDouble();\n+                double u2 = Random.NextDouble();\n+                double randStdNormal = Math.Sqrt(-2.0 * Math.Log(u1)) * Math.Sin(2.0 * Math.PI * u2);\n+                _matrixA[i, j] = NumOps.Multiply(NumOps.FromDouble(randStdNormal), stddev);\n+            }\n+        }\n+\n+        // Initialize matrix B with zeros (so LoKr has no effect initially)\n+        for (int i = 0; i < _matrixB.Rows; i++)\n+        {\n+            for (int j = 0; j < _matrixB.Columns; j++)\n+            {\n+                _matrixB[i, j] = NumOps.Zero;\n+            }\n+        }\n+\n+        // Initialize parameter vector\n+        Parameters = new Vector<T>(ParameterCount);\n+        UpdateParametersFromMatrices();\n+    }\n+\n+    /// <summary>\n+    /// Factors a dimension into two factors based on the desired rank.\n+    /// </summary>\n+    /// <param name=\"size\">The dimension to factor.</param>\n+    /// <param name=\"rank\">The desired effective rank.</param>\n+    /// <returns>Two factors (m, n) such that their product approximates size.</returns>\n+    /// <remarks>\n+    /// This tries to create balanced factors for better numerical stability.\n+    /// </remarks>\n+    private static (int m, int n) FactorDimension(int size, int rank)\n+    {\n+        // Try to find balanced factors based on rank\n+        // We want m and n such that m*p ‚âà size and n is related to rank\n+        int n = Math.Min(rank, (int)Math.Sqrt(size));\n+        int m = size / n;\n+\n+        // Adjust if not evenly divisible\n+        while (size % m != 0 && m > 1)\n+        {\n+            m--;\n+        }\n+        n = size / m;\n+\n+        return (m, n);\n+    }\n+\n+    /// <summary>\n+    /// Computes the Kronecker product of two matrices.\n+    /// </summary>\n+    /// <param name=\"a\">First matrix (m √ó n).</param>\n+    /// <param name=\"b\">Second matrix (p √ó q).</param>\n+    /// <returns>Kronecker product A ‚äó B of size (m√óp) √ó (n√óq).</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The Kronecker product creates a block matrix where each element a[i,j] is multiplied\n+    /// by the entire matrix B. The result has a characteristic block structure.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> The Kronecker product is like creating a grid of copies of matrix B,\n+    /// where each copy is scaled by a different element from matrix A. If A is 2√ó2 and B is 3√ó3,\n+    /// the result is a 6√ó6 matrix with 4 blocks (each 3√ó3).\n+    /// </para>\n+    /// </remarks>\n+    private Matrix<T> KroneckerProduct(Matrix<T> a, Matrix<T> b)\n+    {\n+        int m = a.Rows;\n+        int n = a.Columns;\n+        int p = b.Rows;\n+        int q = b.Columns;\n+\n+        Matrix<T> result = new Matrix<T>(m * p, n * q);\n+\n+        for (int i = 0; i < m; i++)\n+        {\n+            for (int j = 0; j < n; j++)\n+            {\n+                T aij = a[i, j];\n+                for (int k = 0; k < p; k++)\n+                {\n+                    for (int l = 0; l < q; l++)\n+                    {\n+                        result[i * p + k, j * q + l] = NumOps.Multiply(aij, b[k, l]);\n+                    }\n+                }\n+            }\n+        }\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass through both base and LoKr layers.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Sum of base layer output and LoKr output.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The forward pass computes: output = base_layer(input) + (A ‚äó B) * input * scaling\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This runs the input through both the original layer and the\n+    /// LoKr adaptation layer (using Kronecker product), then adds their outputs together.\n+    /// The result is the original behavior plus the learned Kronecker-factored adaptation.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        _lastInput = input.Clone();\n+\n+        // Forward through base layer\n+        Tensor<T> baseOutput = _baseLayer.Forward(input);\n+\n+        // Compute Kronecker product delta = A ‚äó B\n+        Matrix<T> kronDelta = KroneckerProduct(_matrixA, _matrixB);\n+\n+        // Apply to input: delta * input\n+        int batchSize = input.Shape[0];\n+        int inputSize = input.Shape.Length > 1 ? input.Shape[1] : input.Length;\n+        int outputSize = kronDelta.Rows;\n+\n+        // Convert input to matrix [batchSize, inputSize]\n+        Matrix<T> inputMatrix = new Matrix<T>(batchSize, inputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                inputMatrix[i, j] = input[i * inputSize + j];\n+            }\n+        }\n+\n+        // Compute: input * kronDelta^T (because kronDelta is outputSize √ó inputSize)\n+        Matrix<T> deltaOutput = inputMatrix.Multiply(kronDelta.Transpose());\n+\n+        // Apply scaling\n+        deltaOutput = deltaOutput.Multiply(_scaling);\n+\n+        // Convert LoKr output to tensor and add to base output\n+        Tensor<T> result = new Tensor<T>(baseOutput.Shape);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                int idx = i * outputSize + j;\n+                result[idx] = NumOps.Add(baseOutput[idx], deltaOutput[i, j]);\n+            }\n+        }\n+\n+        return result;\n+    }","path":"src/LoRA/Adapters/LoKrAdapter.cs","commit_id":"fa815036ee570162056ba6aca773f34c80f36948","original_commit_id":"8d5a1b6564c1a2bc1b334f1863e6003a6fd142b2","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Critical performance issue: Forward pass materializes full Kronecker product.**\n\nThe forward pass computes the full Kronecker product `A ‚äó B` at Line 285, which creates an `outputSize √ó inputSize` matrix. For a 1000√ó1000 layer, this materializes 1,000,000 elements every forward pass, completely defeating the memory efficiency of Kronecker factorization.\n\nThe LoKr documentation (lines 40-44) claims \"57x fewer parameters\" but this benefit is lost when the full product is materialized. The vec-trick should be used instead: for computing `(A ‚äó B) * vec(X)`, use the property `vec(B * X * A^T)` by reshaping the input, applying B and A^T separately, then reshaping back. This avoids creating the large intermediate matrix.\n\n\n\nConsider refactoring to use efficient Kronecker-vector multiplication:\n\n```diff\n-    // Compute Kronecker product delta = A ‚äó B\n-    Matrix<T> kronDelta = KroneckerProduct(_matrixA, _matrixB);\n-\n-    // Apply to input: delta * input\n     int batchSize = input.Shape[0];\n     int inputSize = input.Shape.Length > 1 ? input.Shape[1] : input.Length;\n-    int outputSize = kronDelta.Rows;\n+    int outputSize = _dimsA.m * _dimsB.p;\n \n     // Convert input to matrix [batchSize, inputSize]\n     Matrix<T> inputMatrix = new Matrix<T>(batchSize, inputSize);\n     for (int i = 0; i < batchSize; i++)\n     {\n         for (int j = 0; j < inputSize; j++)\n         {\n             inputMatrix[i, j] = input[i * inputSize + j];\n         }\n     }\n \n-    // Compute: input * kronDelta^T (because kronDelta is outputSize √ó inputSize)\n-    Matrix<T> deltaOutput = inputMatrix.Multiply(kronDelta.Transpose());\n-\n-    // Apply scaling\n-    deltaOutput = deltaOutput.Multiply(_scaling);\n+    // Use vec-trick: (A ‚äó B) * vec(input) = vec(B * inputReshaped * A^T)\n+    // Reshape input from [batchSize, n*q] to [batchSize*q, n], apply transforms, reshape back\n+    Matrix<T> deltaOutput = ApplyKroneckerProduct(inputMatrix, batchSize);\n```\n\nAdd a helper method:\n\n```csharp\nprivate Matrix<T> ApplyKroneckerProduct(Matrix<T> inputMatrix, int batchSize)\n{\n    // Efficiently compute (A ‚äó B) * input without materializing A ‚äó B\n    // TODO: Implement efficient Kronecker-vector product using reshape operations\n    // This should be O(batchSize * (m*n*q + p*n*q)) without the O(m*p*n*q) materialization cost\n}\n```\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/LoRA/Adapters/LoKrAdapter.cs around lines 277 to 320, the Forward\nimplementation materializes the full Kronecker product with\nKroneckerProduct(_matrixA, _matrixB) which causes huge memory/time costs;\ninstead implement the vec-trick to compute (A ‚äó B) * vec(X) as vec(B *\nX_reshaped * A^T) per batch so you never build A ‚äó B. Replace the Kron\nmaterialization and inputMatrix.Multiply(kronDelta.Transpose()) with a helper\nthat for each batch: reshape the input vector of length inputSize into a matrix\nX of shape [q, p] (matching B and A dims based on how A ‚äó B was intended),\ncompute Y = B.Multiply(X).Multiply(A.Transpose()), multiply Y by _scaling, then\nflatten Y row-wise into the corresponding row of deltaOutput; keep using\n_matrixA, _matrixB and _scaling and return result by adding deltaOutput to\nbaseOutput. Add a private ApplyKroneckerProduct helper to encapsulate the\nper-batch reshape ‚Üí B * X * A^T ‚Üí flatten workflow and call it from Forward.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T03:02:28Z","updated_at":"2025-11-02T03:02:32Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140298","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140298"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140298"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140298/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":277,"original_start_line":277,"start_side":"RIGHT","line":320,"original_line":320,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":320,"position":320,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140300","pull_request_review_id":3408046172,"id":2484140300,"node_id":"PRRC_kwDOKSXUF86UEPkM","diff_hunk":"@@ -0,0 +1,759 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// LoKr (Low-Rank Kronecker Product Adaptation) adapter for parameter-efficient fine-tuning.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// LoKr uses Kronecker products instead of standard matrix multiplication for low-rank adaptation.\n+/// Instead of computing ŒîW = A √ó B (standard LoRA), LoKr computes ŒîW = A ‚äó B where ‚äó is the\n+/// Kronecker product. This is particularly efficient for very large weight matrices.\n+/// </para>\n+/// <para><b>Kronecker Product Definition:</b>\n+/// For matrices A (m√ón) and B (p√óq), the Kronecker product A ‚äó B is an (m√óp) √ó (n√óq) matrix:\n+///\n+/// A ‚äó B = [a‚ÇÅ‚ÇÅB  a‚ÇÅ‚ÇÇB  ...  a‚ÇÅ‚ÇôB]\n+///         [a‚ÇÇ‚ÇÅB  a‚ÇÇ‚ÇÇB  ...  a‚ÇÇ‚ÇôB]\n+///         [  ‚ãÆ     ‚ãÆ    ‚ã±    ‚ãÆ  ]\n+///         [a‚Çò‚ÇÅB  a‚Çò‚ÇÇB  ...  a‚Çò‚ÇôB]\n+///\n+/// Each element a·µ¢‚±º of A is multiplied by the entire matrix B, creating a block structure.\n+/// </para>\n+/// <para><b>For Beginners:</b> LoKr is a variant of LoRA that uses a different mathematical operation\n+/// called the Kronecker product. Think of it this way:\n+///\n+/// - Standard LoRA: Multiplies two small matrices (like 1000√ó8 and 8√ó1000) to approximate changes\n+/// - LoKr: Uses Kronecker product of two even smaller matrices (like 50√ó4 and 20√ó4) to create the same size output\n+///\n+/// The Kronecker product creates a larger matrix by taking every element of the first matrix and\n+/// multiplying it by the entire second matrix. This creates a block pattern that's very efficient\n+/// for representing certain types of structured transformations.\n+///\n+/// <b>When to use LoKr vs standard LoRA:</b>\n+/// - LoKr is better for very wide or very deep layers (e.g., 10000√ó10000 weight matrices)\n+/// - LoKr can achieve similar expressiveness with fewer parameters than LoRA\n+/// - Standard LoRA is simpler and works well for typical layer sizes\n+///\n+/// <b>Parameter Efficiency Example:</b>\n+/// For a 1000√ó1000 weight matrix with rank r=8:\n+/// - Standard LoRA: 1000√ó8 + 8√ó1000 = 16,000 parameters\n+/// - LoKr: 50√ó4 + 20√ó4 = 200 + 80 = 280 parameters (57x fewer!)\n+///   (where 50√ó20 = 1000 for both dimensions)\n+/// </para>\n+/// </remarks>\n+public class LoKrAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// First Kronecker factor matrix A with dimensions (m √ó n).\n+    /// </summary>\n+    /// <remarks>\n+    /// This is one of the two matrices used in the Kronecker product decomposition.\n+    /// </remarks>\n+    private Matrix<T> _matrixA;\n+\n+    /// <summary>\n+    /// Second Kronecker factor matrix B with dimensions (p √ó q).\n+    /// </summary>\n+    /// <remarks>\n+    /// This is the second matrix used in the Kronecker product decomposition.\n+    /// The Kronecker product A ‚äó B produces a (m√óp) √ó (n√óq) matrix.\n+    /// </remarks>\n+    private Matrix<T> _matrixB;\n+\n+    /// <summary>\n+    /// Scaling factor for the LoKr contribution.\n+    /// </summary>\n+    private readonly T _alpha;\n+\n+    /// <summary>\n+    /// Computed scaling factor (alpha / effective_rank) used during forward pass.\n+    /// </summary>\n+    private readonly T _scaling;\n+\n+    /// <summary>\n+    /// Gradients for matrix A computed during backpropagation.\n+    /// </summary>\n+    private Matrix<T>? _gradientA;\n+\n+    /// <summary>\n+    /// Gradients for matrix B computed during backpropagation.\n+    /// </summary>\n+    private Matrix<T>? _gradientB;\n+\n+    /// <summary>\n+    /// Stored input from the forward pass, needed for gradient computation.\n+    /// </summary>\n+    private Tensor<T>? _lastInput;\n+\n+    /// <summary>\n+    /// Dimensions for matrix A (m, n).\n+    /// </summary>\n+    private readonly (int m, int n) _dimsA;\n+\n+    /// <summary>\n+    /// Dimensions for matrix B (p, q).\n+    /// </summary>\n+    private readonly (int p, int q) _dimsB;\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters (elements in A and B matrices).\n+    /// </summary>\n+    public override int ParameterCount => (_matrixA.Rows * _matrixA.Columns) + (_matrixB.Rows * _matrixB.Columns);\n+\n+    /// <summary>\n+    /// Initializes a new LoKr adapter wrapping an existing layer.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with LoKr.</param>\n+    /// <param name=\"rank\">The effective rank of the decomposition (used to determine factor matrix sizes).</param>\n+    /// <param name=\"alpha\">The LoKr scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when the base layer doesn't have 1D input/output shapes.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// The LoKr matrices are initialized as follows:\n+    /// - Matrix A: Random values from a Gaussian distribution\n+    /// - Matrix B: Zero initialization (so LoKr starts with no effect)\n+    ///\n+    /// The dimensions of A and B are chosen such that A ‚äó B produces a matrix that can be applied\n+    /// to the layer's weights. For a layer with inputSize and outputSize, we factor these dimensions\n+    /// to create A (m√ón) and B (p√óq) where m√óp = outputSize and n√óq = inputSize.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This creates a LoKr adapter for a layer. The rank parameter determines\n+    /// how the weight matrix is factored into two smaller matrices. Lower rank = fewer parameters but\n+    /// less flexibility.\n+    ///\n+    /// The adapter automatically figures out the best sizes for matrices A and B based on your layer's\n+    /// input and output sizes and the rank you specify.\n+    /// </para>\n+    /// </remarks>\n+    public LoKrAdapter(ILayer<T> baseLayer, int rank, double alpha = -1, bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        // Validate base layer has single-dimensional input/output\n+        if (baseLayer.GetInputShape().Length != 1 || baseLayer.GetOutputShape().Length != 1)\n+        {\n+            throw new ArgumentException(\"LoKrAdapter only supports layers with 1D input/output shapes\", nameof(baseLayer));\n+        }\n+\n+        int inputSize = baseLayer.GetInputShape()[0];\n+        int outputSize = baseLayer.GetOutputShape()[0];\n+\n+        // Factor the dimensions to create Kronecker factors\n+        // We want m*p = outputSize and n*q = inputSize, with balanced factors\n+        _dimsA = FactorDimension(outputSize, rank);\n+        _dimsB = (outputSize / _dimsA.m, inputSize / _dimsA.n);\n+\n+        // Verify factorization is valid\n+        if (_dimsA.m * _dimsB.p != outputSize || _dimsA.n * _dimsB.q != inputSize)\n+        {\n+            throw new ArgumentException(\n+                $\"Cannot factor dimensions for LoKr: outputSize={outputSize}, inputSize={inputSize}, rank={rank}. \" +\n+                \"Try a different rank value or use dimensions that are more easily factorizable.\");\n+        }\n+\n+        // Initialize matrices\n+        _matrixA = new Matrix<T>(_dimsA.m, _dimsA.n);\n+        _matrixB = new Matrix<T>(_dimsB.p, _dimsB.q);\n+\n+        // Default alpha to rank if not specified\n+        _alpha = alpha > 0 ? NumOps.FromDouble(alpha) : NumOps.FromDouble(rank);\n+        int effectiveRank = _dimsA.n * _dimsB.q;\n+        _scaling = NumOps.Divide(_alpha, NumOps.FromDouble(effectiveRank));\n+\n+        // Initialize matrix A with random values (Gaussian with std = 1/sqrt(effectiveRank))\n+        T stddev = NumOps.Sqrt(NumOps.Divide(NumOps.One, NumOps.FromDouble(effectiveRank)));\n+        for (int i = 0; i < _matrixA.Rows; i++)\n+        {\n+            for (int j = 0; j < _matrixA.Columns; j++)\n+            {\n+                double u1 = Random.NextDouble();\n+                double u2 = Random.NextDouble();\n+                double randStdNormal = Math.Sqrt(-2.0 * Math.Log(u1)) * Math.Sin(2.0 * Math.PI * u2);\n+                _matrixA[i, j] = NumOps.Multiply(NumOps.FromDouble(randStdNormal), stddev);\n+            }\n+        }\n+\n+        // Initialize matrix B with zeros (so LoKr has no effect initially)\n+        for (int i = 0; i < _matrixB.Rows; i++)\n+        {\n+            for (int j = 0; j < _matrixB.Columns; j++)\n+            {\n+                _matrixB[i, j] = NumOps.Zero;\n+            }\n+        }\n+\n+        // Initialize parameter vector\n+        Parameters = new Vector<T>(ParameterCount);\n+        UpdateParametersFromMatrices();\n+    }\n+\n+    /// <summary>\n+    /// Factors a dimension into two factors based on the desired rank.\n+    /// </summary>\n+    /// <param name=\"size\">The dimension to factor.</param>\n+    /// <param name=\"rank\">The desired effective rank.</param>\n+    /// <returns>Two factors (m, n) such that their product approximates size.</returns>\n+    /// <remarks>\n+    /// This tries to create balanced factors for better numerical stability.\n+    /// </remarks>\n+    private static (int m, int n) FactorDimension(int size, int rank)\n+    {\n+        // Try to find balanced factors based on rank\n+        // We want m and n such that m*p ‚âà size and n is related to rank\n+        int n = Math.Min(rank, (int)Math.Sqrt(size));\n+        int m = size / n;\n+\n+        // Adjust if not evenly divisible\n+        while (size % m != 0 && m > 1)\n+        {\n+            m--;\n+        }\n+        n = size / m;\n+\n+        return (m, n);\n+    }\n+\n+    /// <summary>\n+    /// Computes the Kronecker product of two matrices.\n+    /// </summary>\n+    /// <param name=\"a\">First matrix (m √ó n).</param>\n+    /// <param name=\"b\">Second matrix (p √ó q).</param>\n+    /// <returns>Kronecker product A ‚äó B of size (m√óp) √ó (n√óq).</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The Kronecker product creates a block matrix where each element a[i,j] is multiplied\n+    /// by the entire matrix B. The result has a characteristic block structure.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> The Kronecker product is like creating a grid of copies of matrix B,\n+    /// where each copy is scaled by a different element from matrix A. If A is 2√ó2 and B is 3√ó3,\n+    /// the result is a 6√ó6 matrix with 4 blocks (each 3√ó3).\n+    /// </para>\n+    /// </remarks>\n+    private Matrix<T> KroneckerProduct(Matrix<T> a, Matrix<T> b)\n+    {\n+        int m = a.Rows;\n+        int n = a.Columns;\n+        int p = b.Rows;\n+        int q = b.Columns;\n+\n+        Matrix<T> result = new Matrix<T>(m * p, n * q);\n+\n+        for (int i = 0; i < m; i++)\n+        {\n+            for (int j = 0; j < n; j++)\n+            {\n+                T aij = a[i, j];\n+                for (int k = 0; k < p; k++)\n+                {\n+                    for (int l = 0; l < q; l++)\n+                    {\n+                        result[i * p + k, j * q + l] = NumOps.Multiply(aij, b[k, l]);\n+                    }\n+                }\n+            }\n+        }\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass through both base and LoKr layers.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Sum of base layer output and LoKr output.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The forward pass computes: output = base_layer(input) + (A ‚äó B) * input * scaling\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This runs the input through both the original layer and the\n+    /// LoKr adaptation layer (using Kronecker product), then adds their outputs together.\n+    /// The result is the original behavior plus the learned Kronecker-factored adaptation.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        _lastInput = input.Clone();\n+\n+        // Forward through base layer\n+        Tensor<T> baseOutput = _baseLayer.Forward(input);\n+\n+        // Compute Kronecker product delta = A ‚äó B\n+        Matrix<T> kronDelta = KroneckerProduct(_matrixA, _matrixB);\n+\n+        // Apply to input: delta * input\n+        int batchSize = input.Shape[0];\n+        int inputSize = input.Shape.Length > 1 ? input.Shape[1] : input.Length;\n+        int outputSize = kronDelta.Rows;\n+\n+        // Convert input to matrix [batchSize, inputSize]\n+        Matrix<T> inputMatrix = new Matrix<T>(batchSize, inputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                inputMatrix[i, j] = input[i * inputSize + j];\n+            }\n+        }\n+\n+        // Compute: input * kronDelta^T (because kronDelta is outputSize √ó inputSize)\n+        Matrix<T> deltaOutput = inputMatrix.Multiply(kronDelta.Transpose());\n+\n+        // Apply scaling\n+        deltaOutput = deltaOutput.Multiply(_scaling);\n+\n+        // Convert LoKr output to tensor and add to base output\n+        Tensor<T> result = new Tensor<T>(baseOutput.Shape);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                int idx = i * outputSize + j;\n+                result[idx] = NumOps.Add(baseOutput[idx], deltaOutput[i, j]);\n+            }\n+        }\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Performs the backward pass through both layers.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient flowing back from the next layer.</param>\n+    /// <returns>Gradient to pass to the previous layer.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The backward pass computes gradients through the Kronecker product using the vec-trick\n+    /// for efficient gradient computation. The gradients are:\n+    /// - dL/dA uses the Kronecker structure to extract A-specific gradients\n+    /// - dL/dB uses the Kronecker structure to extract B-specific gradients\n+    /// - Input gradients flow through both paths and are summed\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This figures out how to improve both the base layer and the\n+    /// LoKr matrices (A and B). It uses the special structure of the Kronecker product to\n+    /// efficiently compute gradients without having to work with the full Kronecker product matrix.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (_lastInput == null)\n+        {\n+            throw new InvalidOperationException(\"Forward pass must be called before backward pass\");\n+        }\n+\n+        // Backward through base layer\n+        Tensor<T> baseInputGrad = _baseLayer.Backward(outputGradient);\n+\n+        // Compute gradients for LoKr matrices using Kronecker product properties\n+        int batchSize = _lastInput.Shape[0];\n+        int inputSize = _lastInput.Shape.Length > 1 ? _lastInput.Shape[1] : _lastInput.Length;\n+        int outputSize = outputGradient.Shape.Length > 1 ? outputGradient.Shape[1] : outputGradient.Length;\n+\n+        // Convert tensors to matrices\n+        Matrix<T> inputMatrix = new Matrix<T>(batchSize, inputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                inputMatrix[i, j] = _lastInput[i * inputSize + j];\n+            }\n+        }\n+\n+        Matrix<T> gradMatrix = new Matrix<T>(batchSize, outputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                gradMatrix[i, j] = outputGradient[i * outputSize + j];\n+            }\n+        }\n+\n+        // Use vec-trick for Kronecker gradient computation\n+        // For ŒîW = A ‚äó B, the gradients are computed by reshaping and using Kronecker properties\n+        _gradientA = KroneckerGradientA(inputMatrix, gradMatrix, _matrixB);\n+        _gradientB = KroneckerGradientB(inputMatrix, gradMatrix, _matrixA);\n+\n+        // Scale gradients\n+        _gradientA = _gradientA.Multiply(_scaling);\n+        _gradientB = _gradientB.Multiply(_scaling);\n+\n+        // Compute input gradients through Kronecker product\n+        Matrix<T> kronDelta = KroneckerProduct(_matrixA, _matrixB);\n+        Matrix<T> loraInputGrad = gradMatrix.Multiply(kronDelta).Multiply(_scaling);\n+\n+        // Sum input gradients from both paths\n+        Tensor<T> inputGrad = new Tensor<T>(baseInputGrad.Shape);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                int idx = i * inputSize + j;\n+                inputGrad[idx] = NumOps.Add(baseInputGrad[idx], loraInputGrad[i, j]);\n+            }\n+        }\n+\n+        // Update parameter gradients vector\n+        UpdateParameterGradientsFromMatrices();\n+\n+        return inputGrad;\n+    }","path":"src/LoRA/Adapters/LoKrAdapter.cs","commit_id":"fa815036ee570162056ba6aca773f34c80f36948","original_commit_id":"8d5a1b6564c1a2bc1b334f1863e6003a6fd142b2","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Critical performance issue: Backward pass also materializes full Kronecker product.**\n\nLine 384 creates the full Kronecker product for computing input gradients (Line 385). This has the same critical performance issue as the forward pass‚Äîit materializes an `outputSize √ó inputSize` matrix unnecessarily.\n\nNotably, the gradient computations for matrices A and B (lines 376-377) correctly use `KroneckerGradientA` and `KroneckerGradientB` which appear to avoid materialization, but the input gradient computation does not. This creates an inconsistency where weight gradients are efficient but input gradients are not.\n\n\n\nThe input gradient computation should also use the vec-trick to avoid materializing the full Kronecker product.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/LoRA/Adapters/LoKrAdapter.cs around lines 340 to 402, the backward pass\ncurrently materializes the full Kronecker product at line ~384 to compute input\ngradients, which is the same costly mistake as the forward pass; instead compute\nthe input gradients with the vec-trick (i.e., implement/Call a KronVecMultiply\nthat multiplies a matrix or batch of vectors by A ‚äó B without forming it).\nReplace the kronDelta creation and loraInputGrad =\ngradMatrix.Multiply(kronDelta).Multiply(_scaling) with a call that uses\nKronecker properties (reshape/permute or batched mat-vec identity) to compute\nloraInputGrad directly from gradMatrix, _matrixA and _matrixB and then scale by\n_scaling; keep the existing efficient KroneckerGradientA/B usage and\nUpdateParameterGradientsFromMatrices(), and ensure the resulting loraInputGrad\nis written into inputGrad the same way (matching batch and inputSize) without\never allocating the full Kronecker product.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T03:02:28Z","updated_at":"2025-11-02T03:02:32Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140300","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140300"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140300"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140300/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":340,"original_start_line":340,"start_side":"RIGHT","line":402,"original_line":402,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":402,"position":402,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140302","pull_request_review_id":3408046172,"id":2484140302,"node_id":"PRRC_kwDOKSXUF86UEPkO","diff_hunk":"@@ -0,0 +1,759 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// LoKr (Low-Rank Kronecker Product Adaptation) adapter for parameter-efficient fine-tuning.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// LoKr uses Kronecker products instead of standard matrix multiplication for low-rank adaptation.\n+/// Instead of computing ŒîW = A √ó B (standard LoRA), LoKr computes ŒîW = A ‚äó B where ‚äó is the\n+/// Kronecker product. This is particularly efficient for very large weight matrices.\n+/// </para>\n+/// <para><b>Kronecker Product Definition:</b>\n+/// For matrices A (m√ón) and B (p√óq), the Kronecker product A ‚äó B is an (m√óp) √ó (n√óq) matrix:\n+///\n+/// A ‚äó B = [a‚ÇÅ‚ÇÅB  a‚ÇÅ‚ÇÇB  ...  a‚ÇÅ‚ÇôB]\n+///         [a‚ÇÇ‚ÇÅB  a‚ÇÇ‚ÇÇB  ...  a‚ÇÇ‚ÇôB]\n+///         [  ‚ãÆ     ‚ãÆ    ‚ã±    ‚ãÆ  ]\n+///         [a‚Çò‚ÇÅB  a‚Çò‚ÇÇB  ...  a‚Çò‚ÇôB]\n+///\n+/// Each element a·µ¢‚±º of A is multiplied by the entire matrix B, creating a block structure.\n+/// </para>\n+/// <para><b>For Beginners:</b> LoKr is a variant of LoRA that uses a different mathematical operation\n+/// called the Kronecker product. Think of it this way:\n+///\n+/// - Standard LoRA: Multiplies two small matrices (like 1000√ó8 and 8√ó1000) to approximate changes\n+/// - LoKr: Uses Kronecker product of two even smaller matrices (like 50√ó4 and 20√ó4) to create the same size output\n+///\n+/// The Kronecker product creates a larger matrix by taking every element of the first matrix and\n+/// multiplying it by the entire second matrix. This creates a block pattern that's very efficient\n+/// for representing certain types of structured transformations.\n+///\n+/// <b>When to use LoKr vs standard LoRA:</b>\n+/// - LoKr is better for very wide or very deep layers (e.g., 10000√ó10000 weight matrices)\n+/// - LoKr can achieve similar expressiveness with fewer parameters than LoRA\n+/// - Standard LoRA is simpler and works well for typical layer sizes\n+///\n+/// <b>Parameter Efficiency Example:</b>\n+/// For a 1000√ó1000 weight matrix with rank r=8:\n+/// - Standard LoRA: 1000√ó8 + 8√ó1000 = 16,000 parameters\n+/// - LoKr: 50√ó4 + 20√ó4 = 200 + 80 = 280 parameters (57x fewer!)\n+///   (where 50√ó20 = 1000 for both dimensions)\n+/// </para>\n+/// </remarks>\n+public class LoKrAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// First Kronecker factor matrix A with dimensions (m √ó n).\n+    /// </summary>\n+    /// <remarks>\n+    /// This is one of the two matrices used in the Kronecker product decomposition.\n+    /// </remarks>\n+    private Matrix<T> _matrixA;\n+\n+    /// <summary>\n+    /// Second Kronecker factor matrix B with dimensions (p √ó q).\n+    /// </summary>\n+    /// <remarks>\n+    /// This is the second matrix used in the Kronecker product decomposition.\n+    /// The Kronecker product A ‚äó B produces a (m√óp) √ó (n√óq) matrix.\n+    /// </remarks>\n+    private Matrix<T> _matrixB;\n+\n+    /// <summary>\n+    /// Scaling factor for the LoKr contribution.\n+    /// </summary>\n+    private readonly T _alpha;\n+\n+    /// <summary>\n+    /// Computed scaling factor (alpha / effective_rank) used during forward pass.\n+    /// </summary>\n+    private readonly T _scaling;\n+\n+    /// <summary>\n+    /// Gradients for matrix A computed during backpropagation.\n+    /// </summary>\n+    private Matrix<T>? _gradientA;\n+\n+    /// <summary>\n+    /// Gradients for matrix B computed during backpropagation.\n+    /// </summary>\n+    private Matrix<T>? _gradientB;\n+\n+    /// <summary>\n+    /// Stored input from the forward pass, needed for gradient computation.\n+    /// </summary>\n+    private Tensor<T>? _lastInput;\n+\n+    /// <summary>\n+    /// Dimensions for matrix A (m, n).\n+    /// </summary>\n+    private readonly (int m, int n) _dimsA;\n+\n+    /// <summary>\n+    /// Dimensions for matrix B (p, q).\n+    /// </summary>\n+    private readonly (int p, int q) _dimsB;\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters (elements in A and B matrices).\n+    /// </summary>\n+    public override int ParameterCount => (_matrixA.Rows * _matrixA.Columns) + (_matrixB.Rows * _matrixB.Columns);\n+\n+    /// <summary>\n+    /// Initializes a new LoKr adapter wrapping an existing layer.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with LoKr.</param>\n+    /// <param name=\"rank\">The effective rank of the decomposition (used to determine factor matrix sizes).</param>\n+    /// <param name=\"alpha\">The LoKr scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when the base layer doesn't have 1D input/output shapes.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// The LoKr matrices are initialized as follows:\n+    /// - Matrix A: Random values from a Gaussian distribution\n+    /// - Matrix B: Zero initialization (so LoKr starts with no effect)\n+    ///\n+    /// The dimensions of A and B are chosen such that A ‚äó B produces a matrix that can be applied\n+    /// to the layer's weights. For a layer with inputSize and outputSize, we factor these dimensions\n+    /// to create A (m√ón) and B (p√óq) where m√óp = outputSize and n√óq = inputSize.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This creates a LoKr adapter for a layer. The rank parameter determines\n+    /// how the weight matrix is factored into two smaller matrices. Lower rank = fewer parameters but\n+    /// less flexibility.\n+    ///\n+    /// The adapter automatically figures out the best sizes for matrices A and B based on your layer's\n+    /// input and output sizes and the rank you specify.\n+    /// </para>\n+    /// </remarks>\n+    public LoKrAdapter(ILayer<T> baseLayer, int rank, double alpha = -1, bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        // Validate base layer has single-dimensional input/output\n+        if (baseLayer.GetInputShape().Length != 1 || baseLayer.GetOutputShape().Length != 1)\n+        {\n+            throw new ArgumentException(\"LoKrAdapter only supports layers with 1D input/output shapes\", nameof(baseLayer));\n+        }\n+\n+        int inputSize = baseLayer.GetInputShape()[0];\n+        int outputSize = baseLayer.GetOutputShape()[0];\n+\n+        // Factor the dimensions to create Kronecker factors\n+        // We want m*p = outputSize and n*q = inputSize, with balanced factors\n+        _dimsA = FactorDimension(outputSize, rank);\n+        _dimsB = (outputSize / _dimsA.m, inputSize / _dimsA.n);\n+\n+        // Verify factorization is valid\n+        if (_dimsA.m * _dimsB.p != outputSize || _dimsA.n * _dimsB.q != inputSize)\n+        {\n+            throw new ArgumentException(\n+                $\"Cannot factor dimensions for LoKr: outputSize={outputSize}, inputSize={inputSize}, rank={rank}. \" +\n+                \"Try a different rank value or use dimensions that are more easily factorizable.\");\n+        }\n+\n+        // Initialize matrices\n+        _matrixA = new Matrix<T>(_dimsA.m, _dimsA.n);\n+        _matrixB = new Matrix<T>(_dimsB.p, _dimsB.q);\n+\n+        // Default alpha to rank if not specified\n+        _alpha = alpha > 0 ? NumOps.FromDouble(alpha) : NumOps.FromDouble(rank);\n+        int effectiveRank = _dimsA.n * _dimsB.q;\n+        _scaling = NumOps.Divide(_alpha, NumOps.FromDouble(effectiveRank));\n+\n+        // Initialize matrix A with random values (Gaussian with std = 1/sqrt(effectiveRank))\n+        T stddev = NumOps.Sqrt(NumOps.Divide(NumOps.One, NumOps.FromDouble(effectiveRank)));\n+        for (int i = 0; i < _matrixA.Rows; i++)\n+        {\n+            for (int j = 0; j < _matrixA.Columns; j++)\n+            {\n+                double u1 = Random.NextDouble();\n+                double u2 = Random.NextDouble();\n+                double randStdNormal = Math.Sqrt(-2.0 * Math.Log(u1)) * Math.Sin(2.0 * Math.PI * u2);\n+                _matrixA[i, j] = NumOps.Multiply(NumOps.FromDouble(randStdNormal), stddev);\n+            }\n+        }\n+\n+        // Initialize matrix B with zeros (so LoKr has no effect initially)\n+        for (int i = 0; i < _matrixB.Rows; i++)\n+        {\n+            for (int j = 0; j < _matrixB.Columns; j++)\n+            {\n+                _matrixB[i, j] = NumOps.Zero;\n+            }\n+        }\n+\n+        // Initialize parameter vector\n+        Parameters = new Vector<T>(ParameterCount);\n+        UpdateParametersFromMatrices();\n+    }\n+\n+    /// <summary>\n+    /// Factors a dimension into two factors based on the desired rank.\n+    /// </summary>\n+    /// <param name=\"size\">The dimension to factor.</param>\n+    /// <param name=\"rank\">The desired effective rank.</param>\n+    /// <returns>Two factors (m, n) such that their product approximates size.</returns>\n+    /// <remarks>\n+    /// This tries to create balanced factors for better numerical stability.\n+    /// </remarks>\n+    private static (int m, int n) FactorDimension(int size, int rank)\n+    {\n+        // Try to find balanced factors based on rank\n+        // We want m and n such that m*p ‚âà size and n is related to rank\n+        int n = Math.Min(rank, (int)Math.Sqrt(size));\n+        int m = size / n;\n+\n+        // Adjust if not evenly divisible\n+        while (size % m != 0 && m > 1)\n+        {\n+            m--;\n+        }\n+        n = size / m;\n+\n+        return (m, n);\n+    }\n+\n+    /// <summary>\n+    /// Computes the Kronecker product of two matrices.\n+    /// </summary>\n+    /// <param name=\"a\">First matrix (m √ó n).</param>\n+    /// <param name=\"b\">Second matrix (p √ó q).</param>\n+    /// <returns>Kronecker product A ‚äó B of size (m√óp) √ó (n√óq).</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The Kronecker product creates a block matrix where each element a[i,j] is multiplied\n+    /// by the entire matrix B. The result has a characteristic block structure.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> The Kronecker product is like creating a grid of copies of matrix B,\n+    /// where each copy is scaled by a different element from matrix A. If A is 2√ó2 and B is 3√ó3,\n+    /// the result is a 6√ó6 matrix with 4 blocks (each 3√ó3).\n+    /// </para>\n+    /// </remarks>\n+    private Matrix<T> KroneckerProduct(Matrix<T> a, Matrix<T> b)\n+    {\n+        int m = a.Rows;\n+        int n = a.Columns;\n+        int p = b.Rows;\n+        int q = b.Columns;\n+\n+        Matrix<T> result = new Matrix<T>(m * p, n * q);\n+\n+        for (int i = 0; i < m; i++)\n+        {\n+            for (int j = 0; j < n; j++)\n+            {\n+                T aij = a[i, j];\n+                for (int k = 0; k < p; k++)\n+                {\n+                    for (int l = 0; l < q; l++)\n+                    {\n+                        result[i * p + k, j * q + l] = NumOps.Multiply(aij, b[k, l]);\n+                    }\n+                }\n+            }\n+        }\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass through both base and LoKr layers.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Sum of base layer output and LoKr output.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The forward pass computes: output = base_layer(input) + (A ‚äó B) * input * scaling\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This runs the input through both the original layer and the\n+    /// LoKr adaptation layer (using Kronecker product), then adds their outputs together.\n+    /// The result is the original behavior plus the learned Kronecker-factored adaptation.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        _lastInput = input.Clone();\n+\n+        // Forward through base layer\n+        Tensor<T> baseOutput = _baseLayer.Forward(input);\n+\n+        // Compute Kronecker product delta = A ‚äó B\n+        Matrix<T> kronDelta = KroneckerProduct(_matrixA, _matrixB);\n+\n+        // Apply to input: delta * input\n+        int batchSize = input.Shape[0];\n+        int inputSize = input.Shape.Length > 1 ? input.Shape[1] : input.Length;\n+        int outputSize = kronDelta.Rows;\n+\n+        // Convert input to matrix [batchSize, inputSize]\n+        Matrix<T> inputMatrix = new Matrix<T>(batchSize, inputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                inputMatrix[i, j] = input[i * inputSize + j];\n+            }\n+        }\n+\n+        // Compute: input * kronDelta^T (because kronDelta is outputSize √ó inputSize)\n+        Matrix<T> deltaOutput = inputMatrix.Multiply(kronDelta.Transpose());\n+\n+        // Apply scaling\n+        deltaOutput = deltaOutput.Multiply(_scaling);\n+\n+        // Convert LoKr output to tensor and add to base output\n+        Tensor<T> result = new Tensor<T>(baseOutput.Shape);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                int idx = i * outputSize + j;\n+                result[idx] = NumOps.Add(baseOutput[idx], deltaOutput[i, j]);\n+            }\n+        }\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Performs the backward pass through both layers.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient flowing back from the next layer.</param>\n+    /// <returns>Gradient to pass to the previous layer.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The backward pass computes gradients through the Kronecker product using the vec-trick\n+    /// for efficient gradient computation. The gradients are:\n+    /// - dL/dA uses the Kronecker structure to extract A-specific gradients\n+    /// - dL/dB uses the Kronecker structure to extract B-specific gradients\n+    /// - Input gradients flow through both paths and are summed\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This figures out how to improve both the base layer and the\n+    /// LoKr matrices (A and B). It uses the special structure of the Kronecker product to\n+    /// efficiently compute gradients without having to work with the full Kronecker product matrix.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (_lastInput == null)\n+        {\n+            throw new InvalidOperationException(\"Forward pass must be called before backward pass\");\n+        }\n+\n+        // Backward through base layer\n+        Tensor<T> baseInputGrad = _baseLayer.Backward(outputGradient);\n+\n+        // Compute gradients for LoKr matrices using Kronecker product properties\n+        int batchSize = _lastInput.Shape[0];\n+        int inputSize = _lastInput.Shape.Length > 1 ? _lastInput.Shape[1] : _lastInput.Length;\n+        int outputSize = outputGradient.Shape.Length > 1 ? outputGradient.Shape[1] : outputGradient.Length;\n+\n+        // Convert tensors to matrices\n+        Matrix<T> inputMatrix = new Matrix<T>(batchSize, inputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                inputMatrix[i, j] = _lastInput[i * inputSize + j];\n+            }\n+        }\n+\n+        Matrix<T> gradMatrix = new Matrix<T>(batchSize, outputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                gradMatrix[i, j] = outputGradient[i * outputSize + j];\n+            }\n+        }\n+\n+        // Use vec-trick for Kronecker gradient computation\n+        // For ŒîW = A ‚äó B, the gradients are computed by reshaping and using Kronecker properties\n+        _gradientA = KroneckerGradientA(inputMatrix, gradMatrix, _matrixB);\n+        _gradientB = KroneckerGradientB(inputMatrix, gradMatrix, _matrixA);\n+\n+        // Scale gradients\n+        _gradientA = _gradientA.Multiply(_scaling);\n+        _gradientB = _gradientB.Multiply(_scaling);\n+\n+        // Compute input gradients through Kronecker product\n+        Matrix<T> kronDelta = KroneckerProduct(_matrixA, _matrixB);\n+        Matrix<T> loraInputGrad = gradMatrix.Multiply(kronDelta).Multiply(_scaling);\n+\n+        // Sum input gradients from both paths\n+        Tensor<T> inputGrad = new Tensor<T>(baseInputGrad.Shape);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                int idx = i * inputSize + j;\n+                inputGrad[idx] = NumOps.Add(baseInputGrad[idx], loraInputGrad[i, j]);\n+            }\n+        }\n+\n+        // Update parameter gradients vector\n+        UpdateParameterGradientsFromMatrices();\n+\n+        return inputGrad;\n+    }\n+\n+    /// <summary>\n+    /// Computes the gradient for matrix A using Kronecker product properties.\n+    /// </summary>\n+    /// <param name=\"input\">Input matrix [batchSize, inputSize].</param>\n+    /// <param name=\"outputGrad\">Output gradient matrix [batchSize, outputSize].</param>\n+    /// <param name=\"matrixB\">The B matrix in the Kronecker product.</param>\n+    /// <returns>Gradient for matrix A.</returns>\n+    /// <remarks>\n+    /// Uses the vec-trick: vec(A ‚äó B) = (I_m ‚äó B) vec(A), which allows efficient gradient computation.\n+    /// </remarks>\n+    private Matrix<T> KroneckerGradientA(Matrix<T> input, Matrix<T> outputGrad, Matrix<T> matrixB)\n+    {\n+        int batchSize = input.Rows;\n+        Matrix<T> gradA = new Matrix<T>(_dimsA.m, _dimsA.n);\n+\n+        // Reshape output gradient into blocks and compute gradient for A\n+        // This uses the property that ‚àÇ(A ‚äó B)/‚àÇA can be computed efficiently\n+        for (int i = 0; i < _dimsA.m; i++)\n+        {\n+            for (int j = 0; j < _dimsA.n; j++)\n+            {\n+                T sum = NumOps.Zero;\n+\n+                for (int batch = 0; batch < batchSize; batch++)\n+                {\n+                    // Extract the corresponding block from output gradient\n+                    for (int p = 0; p < _dimsB.p; p++)\n+                    {\n+                        for (int q = 0; q < _dimsB.q; q++)\n+                        {\n+                            int outRow = i * _dimsB.p + p;\n+                            int inCol = j * _dimsB.q + q;\n+\n+                            T grad = outputGrad[batch, outRow];\n+                            T inp = input[batch, inCol];\n+                            T b = matrixB[p, q];\n+\n+                            sum = NumOps.Add(sum, NumOps.Multiply(NumOps.Multiply(grad, inp), b));\n+                        }\n+                    }\n+                }\n+\n+                gradA[i, j] = sum;\n+            }\n+        }\n+\n+        return gradA;\n+    }\n+\n+    /// <summary>\n+    /// Computes the gradient for matrix B using Kronecker product properties.\n+    /// </summary>\n+    /// <param name=\"input\">Input matrix [batchSize, inputSize].</param>\n+    /// <param name=\"outputGrad\">Output gradient matrix [batchSize, outputSize].</param>\n+    /// <param name=\"matrixA\">The A matrix in the Kronecker product.</param>\n+    /// <returns>Gradient for matrix B.</returns>\n+    /// <remarks>\n+    /// Uses the vec-trick for efficient gradient computation through the Kronecker structure.\n+    /// </remarks>\n+    private Matrix<T> KroneckerGradientB(Matrix<T> input, Matrix<T> outputGrad, Matrix<T> matrixA)\n+    {\n+        int batchSize = input.Rows;\n+        Matrix<T> gradB = new Matrix<T>(_dimsB.p, _dimsB.q);\n+\n+        // Compute gradient for B using Kronecker product properties\n+        for (int p = 0; p < _dimsB.p; p++)\n+        {\n+            for (int q = 0; q < _dimsB.q; q++)\n+            {\n+                T sum = NumOps.Zero;\n+\n+                for (int batch = 0; batch < batchSize; batch++)\n+                {\n+                    // Extract the corresponding elements using Kronecker structure\n+                    for (int i = 0; i < _dimsA.m; i++)\n+                    {\n+                        for (int j = 0; j < _dimsA.n; j++)\n+                        {\n+                            int outRow = i * _dimsB.p + p;\n+                            int inCol = j * _dimsB.q + q;\n+\n+                            T grad = outputGrad[batch, outRow];\n+                            T inp = input[batch, inCol];\n+                            T a = matrixA[i, j];\n+\n+                            sum = NumOps.Add(sum, NumOps.Multiply(NumOps.Multiply(grad, inp), a));\n+                        }\n+                    }\n+                }\n+\n+                gradB[p, q] = sum;\n+            }\n+        }\n+\n+        return gradB;\n+    }\n+\n+    /// <summary>\n+    /// Updates the layer's parameters using the specified learning rate.\n+    /// </summary>\n+    /// <param name=\"learningRate\">The learning rate for parameter updates.</param>\n+    public override void UpdateParameters(T learningRate)\n+    {\n+        // Always update LoKr matrices\n+        if (_gradientA != null && _gradientB != null)\n+        {\n+            UpdateMatricesWithGradients(learningRate);\n+        }\n+\n+        // Update base layer if not frozen\n+        if (!_freezeBaseLayer)\n+        {\n+            _baseLayer.UpdateParameters(learningRate);\n+        }\n+\n+        // Update parameter vector\n+        UpdateParametersFromMatrices();\n+    }\n+\n+    /// <summary>\n+    /// Updates matrices A and B using their gradients.\n+    /// </summary>\n+    private void UpdateMatricesWithGradients(T learningRate)\n+    {\n+        if (_gradientA == null || _gradientB == null)\n+        {\n+            return;\n+        }\n+\n+        // Update matrix A\n+        for (int i = 0; i < _matrixA.Rows; i++)\n+        {\n+            for (int j = 0; j < _matrixA.Columns; j++)\n+            {\n+                T update = NumOps.Multiply(_gradientA[i, j], learningRate);\n+                _matrixA[i, j] = NumOps.Subtract(_matrixA[i, j], update);\n+            }\n+        }\n+\n+        // Update matrix B\n+        for (int i = 0; i < _matrixB.Rows; i++)\n+        {\n+            for (int j = 0; j < _matrixB.Columns; j++)\n+            {\n+                T update = NumOps.Multiply(_gradientB[i, j], learningRate);\n+                _matrixB[i, j] = NumOps.Subtract(_matrixB[i, j], update);\n+            }\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Merges the LoKr adaptation into the base layer and returns the merged layer.\n+    /// </summary>\n+    /// <returns>A new layer with LoKr weights merged into the base layer's weights.</returns>\n+    /// <exception cref=\"InvalidOperationException\">Thrown when the base layer type is not DenseLayer or FullyConnectedLayer.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// This computes the full Kronecker product A ‚äó B and adds it to the base layer's weights.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This \"bakes in\" your LoKr adaptation to create a regular layer.\n+    /// It computes the full Kronecker product matrix and adds it to the original weights, creating\n+    /// a single merged layer that's faster for inference.\n+    /// </para>\n+    /// </remarks>\n+    public override ILayer<T> MergeToOriginalLayer()\n+    {\n+        // Support both DenseLayer and FullyConnectedLayer\n+        DenseLayer<T>? denseBase = _baseLayer as DenseLayer<T>;\n+        FullyConnectedLayer<T>? fcBase = _baseLayer as FullyConnectedLayer<T>;\n+\n+        if (denseBase == null && fcBase == null)\n+        {\n+            throw new InvalidOperationException(\"LoKrAdapter only supports DenseLayer or FullyConnectedLayer base layers\");\n+        }\n+\n+        // Compute full Kronecker product\n+        Matrix<T> kronWeights = KroneckerProduct(_matrixA, _matrixB);\n+\n+        // Apply scaling\n+        kronWeights = kronWeights.Multiply(_scaling);\n+\n+        // Get base layer parameters\n+        Vector<T> baseParams = _baseLayer.GetParameters();\n+\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        int weightCount = inputSize * outputSize;\n+\n+        // Create new parameters with merged weights\n+        Vector<T> mergedParams = new Vector<T>(baseParams.Length);\n+\n+        // Merge weights (kronWeights is outputSize √ó inputSize, same as base weights)\n+        for (int i = 0; i < weightCount; i++)\n+        {\n+            int row = i / inputSize;\n+            int col = i % inputSize;\n+            mergedParams[i] = NumOps.Add(baseParams[i], kronWeights[row, col]);\n+        }\n+\n+        // Copy biases unchanged\n+        for (int i = weightCount; i < baseParams.Length; i++)\n+        {\n+            mergedParams[i] = baseParams[i];\n+        }\n+\n+        // Create a new dense layer with merged parameters\n+        DenseLayer<T> mergedLayer = new DenseLayer<T>(inputSize, outputSize, (IActivationFunction<T>?)null);\n+        mergedLayer.SetParameters(mergedParams);\n+\n+        return mergedLayer;\n+    }\n+\n+    /// <summary>\n+    /// Gets the current parameters as a vector.\n+    /// </summary>\n+    /// <returns>Vector containing parameters (LoKr only if base is frozen, otherwise both).</returns>\n+    public override Vector<T> GetParameters()\n+    {\n+        return Parameters.Clone();\n+    }\n+\n+    /// <summary>\n+    /// Sets the layer parameters from a vector.\n+    /// </summary>\n+    /// <param name=\"parameters\">Vector containing parameters.</param>\n+    public override void SetParameters(Vector<T> parameters)\n+    {\n+        if (parameters.Length != ParameterCount)\n+        {\n+            throw new ArgumentException($\"Expected {ParameterCount} parameters, got {parameters.Length}\", nameof(parameters));\n+        }\n+\n+        Parameters = parameters.Clone();\n+        UpdateMatricesFromParameters();\n+    }\n+\n+    /// <summary>\n+    /// Updates the parameter vector from the current matrix states.\n+    /// </summary>\n+    private void UpdateParametersFromMatrices()\n+    {\n+        int idx = 0;\n+\n+        // Pack matrix A\n+        for (int i = 0; i < _matrixA.Rows; i++)\n+        {\n+            for (int j = 0; j < _matrixA.Columns; j++)\n+            {\n+                Parameters[idx++] = _matrixA[i, j];\n+            }\n+        }\n+\n+        // Pack matrix B\n+        for (int i = 0; i < _matrixB.Rows; i++)\n+        {\n+            for (int j = 0; j < _matrixB.Columns; j++)\n+            {\n+                Parameters[idx++] = _matrixB[i, j];\n+            }\n+        }\n+    }","path":"src/LoRA/Adapters/LoKrAdapter.cs","commit_id":"fa815036ee570162056ba6aca773f34c80f36948","original_commit_id":"8d5a1b6564c1a2bc1b334f1863e6003a6fd142b2","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**Fix parameter packing to include base layer parameters when not frozen.**\n\n`UpdateParametersFromMatrices` only packs matrices A and B into the `Parameters` vector, but when `_freezeBaseLayer` is false, the base layer parameters are also trainable and should be included. This is inconsistent with LoHaAdapter (lines 655-692) which correctly packs base parameters first when not frozen. \n\nThis causes `GetParameters()` to return an incomplete parameter vector, breaking serialization and any optimizer that relies on the full parameter vector.\n\n\n\nApply this diff:\n\n```diff\n private void UpdateParametersFromMatrices()\n {\n     int idx = 0;\n \n+    // Pack base layer parameters if not frozen\n+    if (!_freezeBaseLayer)\n+    {\n+        Vector<T> baseParams = _baseLayer.GetParameters();\n+        for (int i = 0; i < baseParams.Length; i++)\n+        {\n+            Parameters[idx++] = baseParams[i];\n+        }\n+    }\n+\n     // Pack matrix A\n     for (int i = 0; i < _matrixA.Rows; i++)\n     {\n         for (int j = 0; j < _matrixA.Columns; j++)\n         {\n             Parameters[idx++] = _matrixA[i, j];\n         }\n     }\n \n     // Pack matrix B\n     for (int i = 0; i < _matrixB.Rows; i++)\n     {\n         for (int j = 0; j < _matrixB.Columns; j++)\n         {\n             Parameters[idx++] = _matrixB[i, j];\n         }\n     }\n }\n```\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/LoRA/Adapters/LoKrAdapter.cs around lines 643 to 664,\nUpdateParametersFromMatrices currently only packs _matrixA and _matrixB into\nParameters; when _freezeBaseLayer is false the trainable base-layer parameters\nmust be included first (matching LoHaAdapter). Modify the method to: start idx =\n0; if (!_freezeBaseLayer) loop over the base layer parameter collection (e.g.,\n_baseParameters or the appropriate base parameter matrix/vector) and copy each\nvalue into Parameters[idx++]; then pack _matrixA and _matrixB values into\nParameters as before; keep the existing order and bounds checks so the total\nlength of Parameters matches GetParameters/serialization/optimizer expectations\nand preserve current behavior when _freezeBaseLayer is true.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T03:02:28Z","updated_at":"2025-11-02T03:02:32Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140302","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140302"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140302"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140302/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":643,"original_start_line":643,"start_side":"RIGHT","line":664,"original_line":664,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":664,"position":664,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140303","pull_request_review_id":3408046172,"id":2484140303,"node_id":"PRRC_kwDOKSXUF86UEPkP","diff_hunk":"@@ -0,0 +1,759 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// LoKr (Low-Rank Kronecker Product Adaptation) adapter for parameter-efficient fine-tuning.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// LoKr uses Kronecker products instead of standard matrix multiplication for low-rank adaptation.\n+/// Instead of computing ŒîW = A √ó B (standard LoRA), LoKr computes ŒîW = A ‚äó B where ‚äó is the\n+/// Kronecker product. This is particularly efficient for very large weight matrices.\n+/// </para>\n+/// <para><b>Kronecker Product Definition:</b>\n+/// For matrices A (m√ón) and B (p√óq), the Kronecker product A ‚äó B is an (m√óp) √ó (n√óq) matrix:\n+///\n+/// A ‚äó B = [a‚ÇÅ‚ÇÅB  a‚ÇÅ‚ÇÇB  ...  a‚ÇÅ‚ÇôB]\n+///         [a‚ÇÇ‚ÇÅB  a‚ÇÇ‚ÇÇB  ...  a‚ÇÇ‚ÇôB]\n+///         [  ‚ãÆ     ‚ãÆ    ‚ã±    ‚ãÆ  ]\n+///         [a‚Çò‚ÇÅB  a‚Çò‚ÇÇB  ...  a‚Çò‚ÇôB]\n+///\n+/// Each element a·µ¢‚±º of A is multiplied by the entire matrix B, creating a block structure.\n+/// </para>\n+/// <para><b>For Beginners:</b> LoKr is a variant of LoRA that uses a different mathematical operation\n+/// called the Kronecker product. Think of it this way:\n+///\n+/// - Standard LoRA: Multiplies two small matrices (like 1000√ó8 and 8√ó1000) to approximate changes\n+/// - LoKr: Uses Kronecker product of two even smaller matrices (like 50√ó4 and 20√ó4) to create the same size output\n+///\n+/// The Kronecker product creates a larger matrix by taking every element of the first matrix and\n+/// multiplying it by the entire second matrix. This creates a block pattern that's very efficient\n+/// for representing certain types of structured transformations.\n+///\n+/// <b>When to use LoKr vs standard LoRA:</b>\n+/// - LoKr is better for very wide or very deep layers (e.g., 10000√ó10000 weight matrices)\n+/// - LoKr can achieve similar expressiveness with fewer parameters than LoRA\n+/// - Standard LoRA is simpler and works well for typical layer sizes\n+///\n+/// <b>Parameter Efficiency Example:</b>\n+/// For a 1000√ó1000 weight matrix with rank r=8:\n+/// - Standard LoRA: 1000√ó8 + 8√ó1000 = 16,000 parameters\n+/// - LoKr: 50√ó4 + 20√ó4 = 200 + 80 = 280 parameters (57x fewer!)\n+///   (where 50√ó20 = 1000 for both dimensions)\n+/// </para>\n+/// </remarks>\n+public class LoKrAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// First Kronecker factor matrix A with dimensions (m √ó n).\n+    /// </summary>\n+    /// <remarks>\n+    /// This is one of the two matrices used in the Kronecker product decomposition.\n+    /// </remarks>\n+    private Matrix<T> _matrixA;\n+\n+    /// <summary>\n+    /// Second Kronecker factor matrix B with dimensions (p √ó q).\n+    /// </summary>\n+    /// <remarks>\n+    /// This is the second matrix used in the Kronecker product decomposition.\n+    /// The Kronecker product A ‚äó B produces a (m√óp) √ó (n√óq) matrix.\n+    /// </remarks>\n+    private Matrix<T> _matrixB;\n+\n+    /// <summary>\n+    /// Scaling factor for the LoKr contribution.\n+    /// </summary>\n+    private readonly T _alpha;\n+\n+    /// <summary>\n+    /// Computed scaling factor (alpha / effective_rank) used during forward pass.\n+    /// </summary>\n+    private readonly T _scaling;\n+\n+    /// <summary>\n+    /// Gradients for matrix A computed during backpropagation.\n+    /// </summary>\n+    private Matrix<T>? _gradientA;\n+\n+    /// <summary>\n+    /// Gradients for matrix B computed during backpropagation.\n+    /// </summary>\n+    private Matrix<T>? _gradientB;\n+\n+    /// <summary>\n+    /// Stored input from the forward pass, needed for gradient computation.\n+    /// </summary>\n+    private Tensor<T>? _lastInput;\n+\n+    /// <summary>\n+    /// Dimensions for matrix A (m, n).\n+    /// </summary>\n+    private readonly (int m, int n) _dimsA;\n+\n+    /// <summary>\n+    /// Dimensions for matrix B (p, q).\n+    /// </summary>\n+    private readonly (int p, int q) _dimsB;\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters (elements in A and B matrices).\n+    /// </summary>\n+    public override int ParameterCount => (_matrixA.Rows * _matrixA.Columns) + (_matrixB.Rows * _matrixB.Columns);\n+\n+    /// <summary>\n+    /// Initializes a new LoKr adapter wrapping an existing layer.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with LoKr.</param>\n+    /// <param name=\"rank\">The effective rank of the decomposition (used to determine factor matrix sizes).</param>\n+    /// <param name=\"alpha\">The LoKr scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when the base layer doesn't have 1D input/output shapes.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// The LoKr matrices are initialized as follows:\n+    /// - Matrix A: Random values from a Gaussian distribution\n+    /// - Matrix B: Zero initialization (so LoKr starts with no effect)\n+    ///\n+    /// The dimensions of A and B are chosen such that A ‚äó B produces a matrix that can be applied\n+    /// to the layer's weights. For a layer with inputSize and outputSize, we factor these dimensions\n+    /// to create A (m√ón) and B (p√óq) where m√óp = outputSize and n√óq = inputSize.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This creates a LoKr adapter for a layer. The rank parameter determines\n+    /// how the weight matrix is factored into two smaller matrices. Lower rank = fewer parameters but\n+    /// less flexibility.\n+    ///\n+    /// The adapter automatically figures out the best sizes for matrices A and B based on your layer's\n+    /// input and output sizes and the rank you specify.\n+    /// </para>\n+    /// </remarks>\n+    public LoKrAdapter(ILayer<T> baseLayer, int rank, double alpha = -1, bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        // Validate base layer has single-dimensional input/output\n+        if (baseLayer.GetInputShape().Length != 1 || baseLayer.GetOutputShape().Length != 1)\n+        {\n+            throw new ArgumentException(\"LoKrAdapter only supports layers with 1D input/output shapes\", nameof(baseLayer));\n+        }\n+\n+        int inputSize = baseLayer.GetInputShape()[0];\n+        int outputSize = baseLayer.GetOutputShape()[0];\n+\n+        // Factor the dimensions to create Kronecker factors\n+        // We want m*p = outputSize and n*q = inputSize, with balanced factors\n+        _dimsA = FactorDimension(outputSize, rank);\n+        _dimsB = (outputSize / _dimsA.m, inputSize / _dimsA.n);\n+\n+        // Verify factorization is valid\n+        if (_dimsA.m * _dimsB.p != outputSize || _dimsA.n * _dimsB.q != inputSize)\n+        {\n+            throw new ArgumentException(\n+                $\"Cannot factor dimensions for LoKr: outputSize={outputSize}, inputSize={inputSize}, rank={rank}. \" +\n+                \"Try a different rank value or use dimensions that are more easily factorizable.\");\n+        }\n+\n+        // Initialize matrices\n+        _matrixA = new Matrix<T>(_dimsA.m, _dimsA.n);\n+        _matrixB = new Matrix<T>(_dimsB.p, _dimsB.q);\n+\n+        // Default alpha to rank if not specified\n+        _alpha = alpha > 0 ? NumOps.FromDouble(alpha) : NumOps.FromDouble(rank);\n+        int effectiveRank = _dimsA.n * _dimsB.q;\n+        _scaling = NumOps.Divide(_alpha, NumOps.FromDouble(effectiveRank));\n+\n+        // Initialize matrix A with random values (Gaussian with std = 1/sqrt(effectiveRank))\n+        T stddev = NumOps.Sqrt(NumOps.Divide(NumOps.One, NumOps.FromDouble(effectiveRank)));\n+        for (int i = 0; i < _matrixA.Rows; i++)\n+        {\n+            for (int j = 0; j < _matrixA.Columns; j++)\n+            {\n+                double u1 = Random.NextDouble();\n+                double u2 = Random.NextDouble();\n+                double randStdNormal = Math.Sqrt(-2.0 * Math.Log(u1)) * Math.Sin(2.0 * Math.PI * u2);\n+                _matrixA[i, j] = NumOps.Multiply(NumOps.FromDouble(randStdNormal), stddev);\n+            }\n+        }\n+\n+        // Initialize matrix B with zeros (so LoKr has no effect initially)\n+        for (int i = 0; i < _matrixB.Rows; i++)\n+        {\n+            for (int j = 0; j < _matrixB.Columns; j++)\n+            {\n+                _matrixB[i, j] = NumOps.Zero;\n+            }\n+        }\n+\n+        // Initialize parameter vector\n+        Parameters = new Vector<T>(ParameterCount);\n+        UpdateParametersFromMatrices();\n+    }\n+\n+    /// <summary>\n+    /// Factors a dimension into two factors based on the desired rank.\n+    /// </summary>\n+    /// <param name=\"size\">The dimension to factor.</param>\n+    /// <param name=\"rank\">The desired effective rank.</param>\n+    /// <returns>Two factors (m, n) such that their product approximates size.</returns>\n+    /// <remarks>\n+    /// This tries to create balanced factors for better numerical stability.\n+    /// </remarks>\n+    private static (int m, int n) FactorDimension(int size, int rank)\n+    {\n+        // Try to find balanced factors based on rank\n+        // We want m and n such that m*p ‚âà size and n is related to rank\n+        int n = Math.Min(rank, (int)Math.Sqrt(size));\n+        int m = size / n;\n+\n+        // Adjust if not evenly divisible\n+        while (size % m != 0 && m > 1)\n+        {\n+            m--;\n+        }\n+        n = size / m;\n+\n+        return (m, n);\n+    }\n+\n+    /// <summary>\n+    /// Computes the Kronecker product of two matrices.\n+    /// </summary>\n+    /// <param name=\"a\">First matrix (m √ó n).</param>\n+    /// <param name=\"b\">Second matrix (p √ó q).</param>\n+    /// <returns>Kronecker product A ‚äó B of size (m√óp) √ó (n√óq).</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The Kronecker product creates a block matrix where each element a[i,j] is multiplied\n+    /// by the entire matrix B. The result has a characteristic block structure.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> The Kronecker product is like creating a grid of copies of matrix B,\n+    /// where each copy is scaled by a different element from matrix A. If A is 2√ó2 and B is 3√ó3,\n+    /// the result is a 6√ó6 matrix with 4 blocks (each 3√ó3).\n+    /// </para>\n+    /// </remarks>\n+    private Matrix<T> KroneckerProduct(Matrix<T> a, Matrix<T> b)\n+    {\n+        int m = a.Rows;\n+        int n = a.Columns;\n+        int p = b.Rows;\n+        int q = b.Columns;\n+\n+        Matrix<T> result = new Matrix<T>(m * p, n * q);\n+\n+        for (int i = 0; i < m; i++)\n+        {\n+            for (int j = 0; j < n; j++)\n+            {\n+                T aij = a[i, j];\n+                for (int k = 0; k < p; k++)\n+                {\n+                    for (int l = 0; l < q; l++)\n+                    {\n+                        result[i * p + k, j * q + l] = NumOps.Multiply(aij, b[k, l]);\n+                    }\n+                }\n+            }\n+        }\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass through both base and LoKr layers.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Sum of base layer output and LoKr output.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The forward pass computes: output = base_layer(input) + (A ‚äó B) * input * scaling\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This runs the input through both the original layer and the\n+    /// LoKr adaptation layer (using Kronecker product), then adds their outputs together.\n+    /// The result is the original behavior plus the learned Kronecker-factored adaptation.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        _lastInput = input.Clone();\n+\n+        // Forward through base layer\n+        Tensor<T> baseOutput = _baseLayer.Forward(input);\n+\n+        // Compute Kronecker product delta = A ‚äó B\n+        Matrix<T> kronDelta = KroneckerProduct(_matrixA, _matrixB);\n+\n+        // Apply to input: delta * input\n+        int batchSize = input.Shape[0];\n+        int inputSize = input.Shape.Length > 1 ? input.Shape[1] : input.Length;\n+        int outputSize = kronDelta.Rows;\n+\n+        // Convert input to matrix [batchSize, inputSize]\n+        Matrix<T> inputMatrix = new Matrix<T>(batchSize, inputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                inputMatrix[i, j] = input[i * inputSize + j];\n+            }\n+        }\n+\n+        // Compute: input * kronDelta^T (because kronDelta is outputSize √ó inputSize)\n+        Matrix<T> deltaOutput = inputMatrix.Multiply(kronDelta.Transpose());\n+\n+        // Apply scaling\n+        deltaOutput = deltaOutput.Multiply(_scaling);\n+\n+        // Convert LoKr output to tensor and add to base output\n+        Tensor<T> result = new Tensor<T>(baseOutput.Shape);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                int idx = i * outputSize + j;\n+                result[idx] = NumOps.Add(baseOutput[idx], deltaOutput[i, j]);\n+            }\n+        }\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Performs the backward pass through both layers.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient flowing back from the next layer.</param>\n+    /// <returns>Gradient to pass to the previous layer.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The backward pass computes gradients through the Kronecker product using the vec-trick\n+    /// for efficient gradient computation. The gradients are:\n+    /// - dL/dA uses the Kronecker structure to extract A-specific gradients\n+    /// - dL/dB uses the Kronecker structure to extract B-specific gradients\n+    /// - Input gradients flow through both paths and are summed\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This figures out how to improve both the base layer and the\n+    /// LoKr matrices (A and B). It uses the special structure of the Kronecker product to\n+    /// efficiently compute gradients without having to work with the full Kronecker product matrix.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (_lastInput == null)\n+        {\n+            throw new InvalidOperationException(\"Forward pass must be called before backward pass\");\n+        }\n+\n+        // Backward through base layer\n+        Tensor<T> baseInputGrad = _baseLayer.Backward(outputGradient);\n+\n+        // Compute gradients for LoKr matrices using Kronecker product properties\n+        int batchSize = _lastInput.Shape[0];\n+        int inputSize = _lastInput.Shape.Length > 1 ? _lastInput.Shape[1] : _lastInput.Length;\n+        int outputSize = outputGradient.Shape.Length > 1 ? outputGradient.Shape[1] : outputGradient.Length;\n+\n+        // Convert tensors to matrices\n+        Matrix<T> inputMatrix = new Matrix<T>(batchSize, inputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                inputMatrix[i, j] = _lastInput[i * inputSize + j];\n+            }\n+        }\n+\n+        Matrix<T> gradMatrix = new Matrix<T>(batchSize, outputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                gradMatrix[i, j] = outputGradient[i * outputSize + j];\n+            }\n+        }\n+\n+        // Use vec-trick for Kronecker gradient computation\n+        // For ŒîW = A ‚äó B, the gradients are computed by reshaping and using Kronecker properties\n+        _gradientA = KroneckerGradientA(inputMatrix, gradMatrix, _matrixB);\n+        _gradientB = KroneckerGradientB(inputMatrix, gradMatrix, _matrixA);\n+\n+        // Scale gradients\n+        _gradientA = _gradientA.Multiply(_scaling);\n+        _gradientB = _gradientB.Multiply(_scaling);\n+\n+        // Compute input gradients through Kronecker product\n+        Matrix<T> kronDelta = KroneckerProduct(_matrixA, _matrixB);\n+        Matrix<T> loraInputGrad = gradMatrix.Multiply(kronDelta).Multiply(_scaling);\n+\n+        // Sum input gradients from both paths\n+        Tensor<T> inputGrad = new Tensor<T>(baseInputGrad.Shape);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                int idx = i * inputSize + j;\n+                inputGrad[idx] = NumOps.Add(baseInputGrad[idx], loraInputGrad[i, j]);\n+            }\n+        }\n+\n+        // Update parameter gradients vector\n+        UpdateParameterGradientsFromMatrices();\n+\n+        return inputGrad;\n+    }\n+\n+    /// <summary>\n+    /// Computes the gradient for matrix A using Kronecker product properties.\n+    /// </summary>\n+    /// <param name=\"input\">Input matrix [batchSize, inputSize].</param>\n+    /// <param name=\"outputGrad\">Output gradient matrix [batchSize, outputSize].</param>\n+    /// <param name=\"matrixB\">The B matrix in the Kronecker product.</param>\n+    /// <returns>Gradient for matrix A.</returns>\n+    /// <remarks>\n+    /// Uses the vec-trick: vec(A ‚äó B) = (I_m ‚äó B) vec(A), which allows efficient gradient computation.\n+    /// </remarks>\n+    private Matrix<T> KroneckerGradientA(Matrix<T> input, Matrix<T> outputGrad, Matrix<T> matrixB)\n+    {\n+        int batchSize = input.Rows;\n+        Matrix<T> gradA = new Matrix<T>(_dimsA.m, _dimsA.n);\n+\n+        // Reshape output gradient into blocks and compute gradient for A\n+        // This uses the property that ‚àÇ(A ‚äó B)/‚àÇA can be computed efficiently\n+        for (int i = 0; i < _dimsA.m; i++)\n+        {\n+            for (int j = 0; j < _dimsA.n; j++)\n+            {\n+                T sum = NumOps.Zero;\n+\n+                for (int batch = 0; batch < batchSize; batch++)\n+                {\n+                    // Extract the corresponding block from output gradient\n+                    for (int p = 0; p < _dimsB.p; p++)\n+                    {\n+                        for (int q = 0; q < _dimsB.q; q++)\n+                        {\n+                            int outRow = i * _dimsB.p + p;\n+                            int inCol = j * _dimsB.q + q;\n+\n+                            T grad = outputGrad[batch, outRow];\n+                            T inp = input[batch, inCol];\n+                            T b = matrixB[p, q];\n+\n+                            sum = NumOps.Add(sum, NumOps.Multiply(NumOps.Multiply(grad, inp), b));\n+                        }\n+                    }\n+                }\n+\n+                gradA[i, j] = sum;\n+            }\n+        }\n+\n+        return gradA;\n+    }\n+\n+    /// <summary>\n+    /// Computes the gradient for matrix B using Kronecker product properties.\n+    /// </summary>\n+    /// <param name=\"input\">Input matrix [batchSize, inputSize].</param>\n+    /// <param name=\"outputGrad\">Output gradient matrix [batchSize, outputSize].</param>\n+    /// <param name=\"matrixA\">The A matrix in the Kronecker product.</param>\n+    /// <returns>Gradient for matrix B.</returns>\n+    /// <remarks>\n+    /// Uses the vec-trick for efficient gradient computation through the Kronecker structure.\n+    /// </remarks>\n+    private Matrix<T> KroneckerGradientB(Matrix<T> input, Matrix<T> outputGrad, Matrix<T> matrixA)\n+    {\n+        int batchSize = input.Rows;\n+        Matrix<T> gradB = new Matrix<T>(_dimsB.p, _dimsB.q);\n+\n+        // Compute gradient for B using Kronecker product properties\n+        for (int p = 0; p < _dimsB.p; p++)\n+        {\n+            for (int q = 0; q < _dimsB.q; q++)\n+            {\n+                T sum = NumOps.Zero;\n+\n+                for (int batch = 0; batch < batchSize; batch++)\n+                {\n+                    // Extract the corresponding elements using Kronecker structure\n+                    for (int i = 0; i < _dimsA.m; i++)\n+                    {\n+                        for (int j = 0; j < _dimsA.n; j++)\n+                        {\n+                            int outRow = i * _dimsB.p + p;\n+                            int inCol = j * _dimsB.q + q;\n+\n+                            T grad = outputGrad[batch, outRow];\n+                            T inp = input[batch, inCol];\n+                            T a = matrixA[i, j];\n+\n+                            sum = NumOps.Add(sum, NumOps.Multiply(NumOps.Multiply(grad, inp), a));\n+                        }\n+                    }\n+                }\n+\n+                gradB[p, q] = sum;\n+            }\n+        }\n+\n+        return gradB;\n+    }\n+\n+    /// <summary>\n+    /// Updates the layer's parameters using the specified learning rate.\n+    /// </summary>\n+    /// <param name=\"learningRate\">The learning rate for parameter updates.</param>\n+    public override void UpdateParameters(T learningRate)\n+    {\n+        // Always update LoKr matrices\n+        if (_gradientA != null && _gradientB != null)\n+        {\n+            UpdateMatricesWithGradients(learningRate);\n+        }\n+\n+        // Update base layer if not frozen\n+        if (!_freezeBaseLayer)\n+        {\n+            _baseLayer.UpdateParameters(learningRate);\n+        }\n+\n+        // Update parameter vector\n+        UpdateParametersFromMatrices();\n+    }\n+\n+    /// <summary>\n+    /// Updates matrices A and B using their gradients.\n+    /// </summary>\n+    private void UpdateMatricesWithGradients(T learningRate)\n+    {\n+        if (_gradientA == null || _gradientB == null)\n+        {\n+            return;\n+        }\n+\n+        // Update matrix A\n+        for (int i = 0; i < _matrixA.Rows; i++)\n+        {\n+            for (int j = 0; j < _matrixA.Columns; j++)\n+            {\n+                T update = NumOps.Multiply(_gradientA[i, j], learningRate);\n+                _matrixA[i, j] = NumOps.Subtract(_matrixA[i, j], update);\n+            }\n+        }\n+\n+        // Update matrix B\n+        for (int i = 0; i < _matrixB.Rows; i++)\n+        {\n+            for (int j = 0; j < _matrixB.Columns; j++)\n+            {\n+                T update = NumOps.Multiply(_gradientB[i, j], learningRate);\n+                _matrixB[i, j] = NumOps.Subtract(_matrixB[i, j], update);\n+            }\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Merges the LoKr adaptation into the base layer and returns the merged layer.\n+    /// </summary>\n+    /// <returns>A new layer with LoKr weights merged into the base layer's weights.</returns>\n+    /// <exception cref=\"InvalidOperationException\">Thrown when the base layer type is not DenseLayer or FullyConnectedLayer.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// This computes the full Kronecker product A ‚äó B and adds it to the base layer's weights.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This \"bakes in\" your LoKr adaptation to create a regular layer.\n+    /// It computes the full Kronecker product matrix and adds it to the original weights, creating\n+    /// a single merged layer that's faster for inference.\n+    /// </para>\n+    /// </remarks>\n+    public override ILayer<T> MergeToOriginalLayer()\n+    {\n+        // Support both DenseLayer and FullyConnectedLayer\n+        DenseLayer<T>? denseBase = _baseLayer as DenseLayer<T>;\n+        FullyConnectedLayer<T>? fcBase = _baseLayer as FullyConnectedLayer<T>;\n+\n+        if (denseBase == null && fcBase == null)\n+        {\n+            throw new InvalidOperationException(\"LoKrAdapter only supports DenseLayer or FullyConnectedLayer base layers\");\n+        }\n+\n+        // Compute full Kronecker product\n+        Matrix<T> kronWeights = KroneckerProduct(_matrixA, _matrixB);\n+\n+        // Apply scaling\n+        kronWeights = kronWeights.Multiply(_scaling);\n+\n+        // Get base layer parameters\n+        Vector<T> baseParams = _baseLayer.GetParameters();\n+\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        int weightCount = inputSize * outputSize;\n+\n+        // Create new parameters with merged weights\n+        Vector<T> mergedParams = new Vector<T>(baseParams.Length);\n+\n+        // Merge weights (kronWeights is outputSize √ó inputSize, same as base weights)\n+        for (int i = 0; i < weightCount; i++)\n+        {\n+            int row = i / inputSize;\n+            int col = i % inputSize;\n+            mergedParams[i] = NumOps.Add(baseParams[i], kronWeights[row, col]);\n+        }\n+\n+        // Copy biases unchanged\n+        for (int i = weightCount; i < baseParams.Length; i++)\n+        {\n+            mergedParams[i] = baseParams[i];\n+        }\n+\n+        // Create a new dense layer with merged parameters\n+        DenseLayer<T> mergedLayer = new DenseLayer<T>(inputSize, outputSize, (IActivationFunction<T>?)null);\n+        mergedLayer.SetParameters(mergedParams);\n+\n+        return mergedLayer;\n+    }\n+\n+    /// <summary>\n+    /// Gets the current parameters as a vector.\n+    /// </summary>\n+    /// <returns>Vector containing parameters (LoKr only if base is frozen, otherwise both).</returns>\n+    public override Vector<T> GetParameters()\n+    {\n+        return Parameters.Clone();\n+    }\n+\n+    /// <summary>\n+    /// Sets the layer parameters from a vector.\n+    /// </summary>\n+    /// <param name=\"parameters\">Vector containing parameters.</param>\n+    public override void SetParameters(Vector<T> parameters)\n+    {\n+        if (parameters.Length != ParameterCount)\n+        {\n+            throw new ArgumentException($\"Expected {ParameterCount} parameters, got {parameters.Length}\", nameof(parameters));\n+        }\n+\n+        Parameters = parameters.Clone();\n+        UpdateMatricesFromParameters();\n+    }\n+\n+    /// <summary>\n+    /// Updates the parameter vector from the current matrix states.\n+    /// </summary>\n+    private void UpdateParametersFromMatrices()\n+    {\n+        int idx = 0;\n+\n+        // Pack matrix A\n+        for (int i = 0; i < _matrixA.Rows; i++)\n+        {\n+            for (int j = 0; j < _matrixA.Columns; j++)\n+            {\n+                Parameters[idx++] = _matrixA[i, j];\n+            }\n+        }\n+\n+        // Pack matrix B\n+        for (int i = 0; i < _matrixB.Rows; i++)\n+        {\n+            for (int j = 0; j < _matrixB.Columns; j++)\n+            {\n+                Parameters[idx++] = _matrixB[i, j];\n+            }\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Updates the matrices from the parameter vector.\n+    /// </summary>\n+    private void UpdateMatricesFromParameters()\n+    {\n+        int idx = 0;\n+\n+        // Unpack matrix A\n+        for (int i = 0; i < _matrixA.Rows; i++)\n+        {\n+            for (int j = 0; j < _matrixA.Columns; j++)\n+            {\n+                _matrixA[i, j] = Parameters[idx++];\n+            }\n+        }\n+\n+        // Unpack matrix B\n+        for (int i = 0; i < _matrixB.Rows; i++)\n+        {\n+            for (int j = 0; j < _matrixB.Columns; j++)\n+            {\n+                _matrixB[i, j] = Parameters[idx++];\n+            }\n+        }\n+    }","path":"src/LoRA/Adapters/LoKrAdapter.cs","commit_id":"fa815036ee570162056ba6aca773f34c80f36948","original_commit_id":"8d5a1b6564c1a2bc1b334f1863e6003a6fd142b2","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**Fix parameter unpacking to include base layer parameters when not frozen.**\n\n`UpdateMatricesFromParameters` only unpacks matrices A and B from the `Parameters` vector, but when `_freezeBaseLayer` is false, it should also unpack and restore base layer parameters. This is the inverse operation of `UpdateParametersFromMatrices` and must maintain symmetry. LoHaAdapter correctly handles this (lines 697-740).\n\nWithout this fix, `SetParameters()` will fail to restore base layer state when the base layer is not frozen.\n\n\n\nApply this diff:\n\n```diff\n private void UpdateMatricesFromParameters()\n {\n     int idx = 0;\n \n+    // Unpack base layer parameters if not frozen\n+    if (!_freezeBaseLayer)\n+    {\n+        Vector<T> baseParams = new Vector<T>(_baseLayer.ParameterCount);\n+        for (int i = 0; i < baseParams.Length; i++)\n+        {\n+            baseParams[i] = Parameters[idx++];\n+        }\n+        _baseLayer.SetParameters(baseParams);\n+    }\n+\n     // Unpack matrix A\n     for (int i = 0; i < _matrixA.Rows; i++)\n     {\n         for (int j = 0; j < _matrixA.Columns; j++)\n         {\n             _matrixA[i, j] = Parameters[idx++];\n         }\n     }\n \n     // Unpack matrix B\n     for (int i = 0; i < _matrixB.Rows; i++)\n     {\n         for (int j = 0; j < _matrixB.Columns; j++)\n         {\n             _matrixB[i, j] = Parameters[idx++];\n         }\n     }\n }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    private void UpdateMatricesFromParameters()\n    {\n        int idx = 0;\n\n        // Unpack base layer parameters if not frozen\n        if (!_freezeBaseLayer)\n        {\n            Vector<T> baseParams = new Vector<T>(_baseLayer.ParameterCount);\n            for (int i = 0; i < baseParams.Length; i++)\n            {\n                baseParams[i] = Parameters[idx++];\n            }\n            _baseLayer.SetParameters(baseParams);\n        }\n\n        // Unpack matrix A\n        for (int i = 0; i < _matrixA.Rows; i++)\n        {\n            for (int j = 0; j < _matrixA.Columns; j++)\n            {\n                _matrixA[i, j] = Parameters[idx++];\n            }\n        }\n\n        // Unpack matrix B\n        for (int i = 0; i < _matrixB.Rows; i++)\n        {\n            for (int j = 0; j < _matrixB.Columns; j++)\n            {\n                _matrixB[i, j] = Parameters[idx++];\n            }\n        }\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/LoRA/Adapters/LoKrAdapter.cs around lines 669 to 690,\nUpdateMatricesFromParameters currently only unpacks matrices A and B from the\nParameters vector; you must restore base layer parameters when _freezeBaseLayer\nis false to mirror UpdateParametersFromMatrices. After the existing loops that\nfill _matrixA and _matrixB, add a conditional if (!_freezeBaseLayer) that\niterates over the base layer parameter storage (the same property/array used by\nUpdateParametersFromMatrices, e.g. _baseLayer.Parameters or equivalent) and\nassigns each element from Parameters using the same idx pointer (idx++) until\nall base parameters are consumed, ensuring the unpack order matches the pack\norder exactly.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T03:02:28Z","updated_at":"2025-11-02T03:02:32Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140303","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140303"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140303"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140303/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":669,"original_start_line":669,"start_side":"RIGHT","line":690,"original_line":690,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":690,"position":690,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140307","pull_request_review_id":3408046172,"id":2484140307,"node_id":"PRRC_kwDOKSXUF86UEPkT","diff_hunk":"@@ -0,0 +1,759 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// LoKr (Low-Rank Kronecker Product Adaptation) adapter for parameter-efficient fine-tuning.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// LoKr uses Kronecker products instead of standard matrix multiplication for low-rank adaptation.\n+/// Instead of computing ŒîW = A √ó B (standard LoRA), LoKr computes ŒîW = A ‚äó B where ‚äó is the\n+/// Kronecker product. This is particularly efficient for very large weight matrices.\n+/// </para>\n+/// <para><b>Kronecker Product Definition:</b>\n+/// For matrices A (m√ón) and B (p√óq), the Kronecker product A ‚äó B is an (m√óp) √ó (n√óq) matrix:\n+///\n+/// A ‚äó B = [a‚ÇÅ‚ÇÅB  a‚ÇÅ‚ÇÇB  ...  a‚ÇÅ‚ÇôB]\n+///         [a‚ÇÇ‚ÇÅB  a‚ÇÇ‚ÇÇB  ...  a‚ÇÇ‚ÇôB]\n+///         [  ‚ãÆ     ‚ãÆ    ‚ã±    ‚ãÆ  ]\n+///         [a‚Çò‚ÇÅB  a‚Çò‚ÇÇB  ...  a‚Çò‚ÇôB]\n+///\n+/// Each element a·µ¢‚±º of A is multiplied by the entire matrix B, creating a block structure.\n+/// </para>\n+/// <para><b>For Beginners:</b> LoKr is a variant of LoRA that uses a different mathematical operation\n+/// called the Kronecker product. Think of it this way:\n+///\n+/// - Standard LoRA: Multiplies two small matrices (like 1000√ó8 and 8√ó1000) to approximate changes\n+/// - LoKr: Uses Kronecker product of two even smaller matrices (like 50√ó4 and 20√ó4) to create the same size output\n+///\n+/// The Kronecker product creates a larger matrix by taking every element of the first matrix and\n+/// multiplying it by the entire second matrix. This creates a block pattern that's very efficient\n+/// for representing certain types of structured transformations.\n+///\n+/// <b>When to use LoKr vs standard LoRA:</b>\n+/// - LoKr is better for very wide or very deep layers (e.g., 10000√ó10000 weight matrices)\n+/// - LoKr can achieve similar expressiveness with fewer parameters than LoRA\n+/// - Standard LoRA is simpler and works well for typical layer sizes\n+///\n+/// <b>Parameter Efficiency Example:</b>\n+/// For a 1000√ó1000 weight matrix with rank r=8:\n+/// - Standard LoRA: 1000√ó8 + 8√ó1000 = 16,000 parameters\n+/// - LoKr: 50√ó4 + 20√ó4 = 200 + 80 = 280 parameters (57x fewer!)\n+///   (where 50√ó20 = 1000 for both dimensions)\n+/// </para>\n+/// </remarks>\n+public class LoKrAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// First Kronecker factor matrix A with dimensions (m √ó n).\n+    /// </summary>\n+    /// <remarks>\n+    /// This is one of the two matrices used in the Kronecker product decomposition.\n+    /// </remarks>\n+    private Matrix<T> _matrixA;\n+\n+    /// <summary>\n+    /// Second Kronecker factor matrix B with dimensions (p √ó q).\n+    /// </summary>\n+    /// <remarks>\n+    /// This is the second matrix used in the Kronecker product decomposition.\n+    /// The Kronecker product A ‚äó B produces a (m√óp) √ó (n√óq) matrix.\n+    /// </remarks>\n+    private Matrix<T> _matrixB;\n+\n+    /// <summary>\n+    /// Scaling factor for the LoKr contribution.\n+    /// </summary>\n+    private readonly T _alpha;\n+\n+    /// <summary>\n+    /// Computed scaling factor (alpha / effective_rank) used during forward pass.\n+    /// </summary>\n+    private readonly T _scaling;\n+\n+    /// <summary>\n+    /// Gradients for matrix A computed during backpropagation.\n+    /// </summary>\n+    private Matrix<T>? _gradientA;\n+\n+    /// <summary>\n+    /// Gradients for matrix B computed during backpropagation.\n+    /// </summary>\n+    private Matrix<T>? _gradientB;\n+\n+    /// <summary>\n+    /// Stored input from the forward pass, needed for gradient computation.\n+    /// </summary>\n+    private Tensor<T>? _lastInput;\n+\n+    /// <summary>\n+    /// Dimensions for matrix A (m, n).\n+    /// </summary>\n+    private readonly (int m, int n) _dimsA;\n+\n+    /// <summary>\n+    /// Dimensions for matrix B (p, q).\n+    /// </summary>\n+    private readonly (int p, int q) _dimsB;\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters (elements in A and B matrices).\n+    /// </summary>\n+    public override int ParameterCount => (_matrixA.Rows * _matrixA.Columns) + (_matrixB.Rows * _matrixB.Columns);\n+\n+    /// <summary>\n+    /// Initializes a new LoKr adapter wrapping an existing layer.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with LoKr.</param>\n+    /// <param name=\"rank\">The effective rank of the decomposition (used to determine factor matrix sizes).</param>\n+    /// <param name=\"alpha\">The LoKr scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when the base layer doesn't have 1D input/output shapes.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// The LoKr matrices are initialized as follows:\n+    /// - Matrix A: Random values from a Gaussian distribution\n+    /// - Matrix B: Zero initialization (so LoKr starts with no effect)\n+    ///\n+    /// The dimensions of A and B are chosen such that A ‚äó B produces a matrix that can be applied\n+    /// to the layer's weights. For a layer with inputSize and outputSize, we factor these dimensions\n+    /// to create A (m√ón) and B (p√óq) where m√óp = outputSize and n√óq = inputSize.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This creates a LoKr adapter for a layer. The rank parameter determines\n+    /// how the weight matrix is factored into two smaller matrices. Lower rank = fewer parameters but\n+    /// less flexibility.\n+    ///\n+    /// The adapter automatically figures out the best sizes for matrices A and B based on your layer's\n+    /// input and output sizes and the rank you specify.\n+    /// </para>\n+    /// </remarks>\n+    public LoKrAdapter(ILayer<T> baseLayer, int rank, double alpha = -1, bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        // Validate base layer has single-dimensional input/output\n+        if (baseLayer.GetInputShape().Length != 1 || baseLayer.GetOutputShape().Length != 1)\n+        {\n+            throw new ArgumentException(\"LoKrAdapter only supports layers with 1D input/output shapes\", nameof(baseLayer));\n+        }\n+\n+        int inputSize = baseLayer.GetInputShape()[0];\n+        int outputSize = baseLayer.GetOutputShape()[0];\n+\n+        // Factor the dimensions to create Kronecker factors\n+        // We want m*p = outputSize and n*q = inputSize, with balanced factors\n+        _dimsA = FactorDimension(outputSize, rank);\n+        _dimsB = (outputSize / _dimsA.m, inputSize / _dimsA.n);\n+\n+        // Verify factorization is valid\n+        if (_dimsA.m * _dimsB.p != outputSize || _dimsA.n * _dimsB.q != inputSize)\n+        {\n+            throw new ArgumentException(\n+                $\"Cannot factor dimensions for LoKr: outputSize={outputSize}, inputSize={inputSize}, rank={rank}. \" +\n+                \"Try a different rank value or use dimensions that are more easily factorizable.\");\n+        }\n+\n+        // Initialize matrices\n+        _matrixA = new Matrix<T>(_dimsA.m, _dimsA.n);\n+        _matrixB = new Matrix<T>(_dimsB.p, _dimsB.q);\n+\n+        // Default alpha to rank if not specified\n+        _alpha = alpha > 0 ? NumOps.FromDouble(alpha) : NumOps.FromDouble(rank);\n+        int effectiveRank = _dimsA.n * _dimsB.q;\n+        _scaling = NumOps.Divide(_alpha, NumOps.FromDouble(effectiveRank));\n+\n+        // Initialize matrix A with random values (Gaussian with std = 1/sqrt(effectiveRank))\n+        T stddev = NumOps.Sqrt(NumOps.Divide(NumOps.One, NumOps.FromDouble(effectiveRank)));\n+        for (int i = 0; i < _matrixA.Rows; i++)\n+        {\n+            for (int j = 0; j < _matrixA.Columns; j++)\n+            {\n+                double u1 = Random.NextDouble();\n+                double u2 = Random.NextDouble();\n+                double randStdNormal = Math.Sqrt(-2.0 * Math.Log(u1)) * Math.Sin(2.0 * Math.PI * u2);\n+                _matrixA[i, j] = NumOps.Multiply(NumOps.FromDouble(randStdNormal), stddev);\n+            }\n+        }\n+\n+        // Initialize matrix B with zeros (so LoKr has no effect initially)\n+        for (int i = 0; i < _matrixB.Rows; i++)\n+        {\n+            for (int j = 0; j < _matrixB.Columns; j++)\n+            {\n+                _matrixB[i, j] = NumOps.Zero;\n+            }\n+        }\n+\n+        // Initialize parameter vector\n+        Parameters = new Vector<T>(ParameterCount);\n+        UpdateParametersFromMatrices();\n+    }\n+\n+    /// <summary>\n+    /// Factors a dimension into two factors based on the desired rank.\n+    /// </summary>\n+    /// <param name=\"size\">The dimension to factor.</param>\n+    /// <param name=\"rank\">The desired effective rank.</param>\n+    /// <returns>Two factors (m, n) such that their product approximates size.</returns>\n+    /// <remarks>\n+    /// This tries to create balanced factors for better numerical stability.\n+    /// </remarks>\n+    private static (int m, int n) FactorDimension(int size, int rank)\n+    {\n+        // Try to find balanced factors based on rank\n+        // We want m and n such that m*p ‚âà size and n is related to rank\n+        int n = Math.Min(rank, (int)Math.Sqrt(size));\n+        int m = size / n;\n+\n+        // Adjust if not evenly divisible\n+        while (size % m != 0 && m > 1)\n+        {\n+            m--;\n+        }\n+        n = size / m;\n+\n+        return (m, n);\n+    }\n+\n+    /// <summary>\n+    /// Computes the Kronecker product of two matrices.\n+    /// </summary>\n+    /// <param name=\"a\">First matrix (m √ó n).</param>\n+    /// <param name=\"b\">Second matrix (p √ó q).</param>\n+    /// <returns>Kronecker product A ‚äó B of size (m√óp) √ó (n√óq).</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The Kronecker product creates a block matrix where each element a[i,j] is multiplied\n+    /// by the entire matrix B. The result has a characteristic block structure.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> The Kronecker product is like creating a grid of copies of matrix B,\n+    /// where each copy is scaled by a different element from matrix A. If A is 2√ó2 and B is 3√ó3,\n+    /// the result is a 6√ó6 matrix with 4 blocks (each 3√ó3).\n+    /// </para>\n+    /// </remarks>\n+    private Matrix<T> KroneckerProduct(Matrix<T> a, Matrix<T> b)\n+    {\n+        int m = a.Rows;\n+        int n = a.Columns;\n+        int p = b.Rows;\n+        int q = b.Columns;\n+\n+        Matrix<T> result = new Matrix<T>(m * p, n * q);\n+\n+        for (int i = 0; i < m; i++)\n+        {\n+            for (int j = 0; j < n; j++)\n+            {\n+                T aij = a[i, j];\n+                for (int k = 0; k < p; k++)\n+                {\n+                    for (int l = 0; l < q; l++)\n+                    {\n+                        result[i * p + k, j * q + l] = NumOps.Multiply(aij, b[k, l]);\n+                    }\n+                }\n+            }\n+        }\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass through both base and LoKr layers.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Sum of base layer output and LoKr output.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The forward pass computes: output = base_layer(input) + (A ‚äó B) * input * scaling\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This runs the input through both the original layer and the\n+    /// LoKr adaptation layer (using Kronecker product), then adds their outputs together.\n+    /// The result is the original behavior plus the learned Kronecker-factored adaptation.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        _lastInput = input.Clone();\n+\n+        // Forward through base layer\n+        Tensor<T> baseOutput = _baseLayer.Forward(input);\n+\n+        // Compute Kronecker product delta = A ‚äó B\n+        Matrix<T> kronDelta = KroneckerProduct(_matrixA, _matrixB);\n+\n+        // Apply to input: delta * input\n+        int batchSize = input.Shape[0];\n+        int inputSize = input.Shape.Length > 1 ? input.Shape[1] : input.Length;\n+        int outputSize = kronDelta.Rows;\n+\n+        // Convert input to matrix [batchSize, inputSize]\n+        Matrix<T> inputMatrix = new Matrix<T>(batchSize, inputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                inputMatrix[i, j] = input[i * inputSize + j];\n+            }\n+        }\n+\n+        // Compute: input * kronDelta^T (because kronDelta is outputSize √ó inputSize)\n+        Matrix<T> deltaOutput = inputMatrix.Multiply(kronDelta.Transpose());\n+\n+        // Apply scaling\n+        deltaOutput = deltaOutput.Multiply(_scaling);\n+\n+        // Convert LoKr output to tensor and add to base output\n+        Tensor<T> result = new Tensor<T>(baseOutput.Shape);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                int idx = i * outputSize + j;\n+                result[idx] = NumOps.Add(baseOutput[idx], deltaOutput[i, j]);\n+            }\n+        }\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Performs the backward pass through both layers.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient flowing back from the next layer.</param>\n+    /// <returns>Gradient to pass to the previous layer.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The backward pass computes gradients through the Kronecker product using the vec-trick\n+    /// for efficient gradient computation. The gradients are:\n+    /// - dL/dA uses the Kronecker structure to extract A-specific gradients\n+    /// - dL/dB uses the Kronecker structure to extract B-specific gradients\n+    /// - Input gradients flow through both paths and are summed\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This figures out how to improve both the base layer and the\n+    /// LoKr matrices (A and B). It uses the special structure of the Kronecker product to\n+    /// efficiently compute gradients without having to work with the full Kronecker product matrix.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (_lastInput == null)\n+        {\n+            throw new InvalidOperationException(\"Forward pass must be called before backward pass\");\n+        }\n+\n+        // Backward through base layer\n+        Tensor<T> baseInputGrad = _baseLayer.Backward(outputGradient);\n+\n+        // Compute gradients for LoKr matrices using Kronecker product properties\n+        int batchSize = _lastInput.Shape[0];\n+        int inputSize = _lastInput.Shape.Length > 1 ? _lastInput.Shape[1] : _lastInput.Length;\n+        int outputSize = outputGradient.Shape.Length > 1 ? outputGradient.Shape[1] : outputGradient.Length;\n+\n+        // Convert tensors to matrices\n+        Matrix<T> inputMatrix = new Matrix<T>(batchSize, inputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                inputMatrix[i, j] = _lastInput[i * inputSize + j];\n+            }\n+        }\n+\n+        Matrix<T> gradMatrix = new Matrix<T>(batchSize, outputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                gradMatrix[i, j] = outputGradient[i * outputSize + j];\n+            }\n+        }\n+\n+        // Use vec-trick for Kronecker gradient computation\n+        // For ŒîW = A ‚äó B, the gradients are computed by reshaping and using Kronecker properties\n+        _gradientA = KroneckerGradientA(inputMatrix, gradMatrix, _matrixB);\n+        _gradientB = KroneckerGradientB(inputMatrix, gradMatrix, _matrixA);\n+\n+        // Scale gradients\n+        _gradientA = _gradientA.Multiply(_scaling);\n+        _gradientB = _gradientB.Multiply(_scaling);\n+\n+        // Compute input gradients through Kronecker product\n+        Matrix<T> kronDelta = KroneckerProduct(_matrixA, _matrixB);\n+        Matrix<T> loraInputGrad = gradMatrix.Multiply(kronDelta).Multiply(_scaling);\n+\n+        // Sum input gradients from both paths\n+        Tensor<T> inputGrad = new Tensor<T>(baseInputGrad.Shape);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                int idx = i * inputSize + j;\n+                inputGrad[idx] = NumOps.Add(baseInputGrad[idx], loraInputGrad[i, j]);\n+            }\n+        }\n+\n+        // Update parameter gradients vector\n+        UpdateParameterGradientsFromMatrices();\n+\n+        return inputGrad;\n+    }\n+\n+    /// <summary>\n+    /// Computes the gradient for matrix A using Kronecker product properties.\n+    /// </summary>\n+    /// <param name=\"input\">Input matrix [batchSize, inputSize].</param>\n+    /// <param name=\"outputGrad\">Output gradient matrix [batchSize, outputSize].</param>\n+    /// <param name=\"matrixB\">The B matrix in the Kronecker product.</param>\n+    /// <returns>Gradient for matrix A.</returns>\n+    /// <remarks>\n+    /// Uses the vec-trick: vec(A ‚äó B) = (I_m ‚äó B) vec(A), which allows efficient gradient computation.\n+    /// </remarks>\n+    private Matrix<T> KroneckerGradientA(Matrix<T> input, Matrix<T> outputGrad, Matrix<T> matrixB)\n+    {\n+        int batchSize = input.Rows;\n+        Matrix<T> gradA = new Matrix<T>(_dimsA.m, _dimsA.n);\n+\n+        // Reshape output gradient into blocks and compute gradient for A\n+        // This uses the property that ‚àÇ(A ‚äó B)/‚àÇA can be computed efficiently\n+        for (int i = 0; i < _dimsA.m; i++)\n+        {\n+            for (int j = 0; j < _dimsA.n; j++)\n+            {\n+                T sum = NumOps.Zero;\n+\n+                for (int batch = 0; batch < batchSize; batch++)\n+                {\n+                    // Extract the corresponding block from output gradient\n+                    for (int p = 0; p < _dimsB.p; p++)\n+                    {\n+                        for (int q = 0; q < _dimsB.q; q++)\n+                        {\n+                            int outRow = i * _dimsB.p + p;\n+                            int inCol = j * _dimsB.q + q;\n+\n+                            T grad = outputGrad[batch, outRow];\n+                            T inp = input[batch, inCol];\n+                            T b = matrixB[p, q];\n+\n+                            sum = NumOps.Add(sum, NumOps.Multiply(NumOps.Multiply(grad, inp), b));\n+                        }\n+                    }\n+                }\n+\n+                gradA[i, j] = sum;\n+            }\n+        }\n+\n+        return gradA;\n+    }\n+\n+    /// <summary>\n+    /// Computes the gradient for matrix B using Kronecker product properties.\n+    /// </summary>\n+    /// <param name=\"input\">Input matrix [batchSize, inputSize].</param>\n+    /// <param name=\"outputGrad\">Output gradient matrix [batchSize, outputSize].</param>\n+    /// <param name=\"matrixA\">The A matrix in the Kronecker product.</param>\n+    /// <returns>Gradient for matrix B.</returns>\n+    /// <remarks>\n+    /// Uses the vec-trick for efficient gradient computation through the Kronecker structure.\n+    /// </remarks>\n+    private Matrix<T> KroneckerGradientB(Matrix<T> input, Matrix<T> outputGrad, Matrix<T> matrixA)\n+    {\n+        int batchSize = input.Rows;\n+        Matrix<T> gradB = new Matrix<T>(_dimsB.p, _dimsB.q);\n+\n+        // Compute gradient for B using Kronecker product properties\n+        for (int p = 0; p < _dimsB.p; p++)\n+        {\n+            for (int q = 0; q < _dimsB.q; q++)\n+            {\n+                T sum = NumOps.Zero;\n+\n+                for (int batch = 0; batch < batchSize; batch++)\n+                {\n+                    // Extract the corresponding elements using Kronecker structure\n+                    for (int i = 0; i < _dimsA.m; i++)\n+                    {\n+                        for (int j = 0; j < _dimsA.n; j++)\n+                        {\n+                            int outRow = i * _dimsB.p + p;\n+                            int inCol = j * _dimsB.q + q;\n+\n+                            T grad = outputGrad[batch, outRow];\n+                            T inp = input[batch, inCol];\n+                            T a = matrixA[i, j];\n+\n+                            sum = NumOps.Add(sum, NumOps.Multiply(NumOps.Multiply(grad, inp), a));\n+                        }\n+                    }\n+                }\n+\n+                gradB[p, q] = sum;\n+            }\n+        }\n+\n+        return gradB;\n+    }\n+\n+    /// <summary>\n+    /// Updates the layer's parameters using the specified learning rate.\n+    /// </summary>\n+    /// <param name=\"learningRate\">The learning rate for parameter updates.</param>\n+    public override void UpdateParameters(T learningRate)\n+    {\n+        // Always update LoKr matrices\n+        if (_gradientA != null && _gradientB != null)\n+        {\n+            UpdateMatricesWithGradients(learningRate);\n+        }\n+\n+        // Update base layer if not frozen\n+        if (!_freezeBaseLayer)\n+        {\n+            _baseLayer.UpdateParameters(learningRate);\n+        }\n+\n+        // Update parameter vector\n+        UpdateParametersFromMatrices();\n+    }\n+\n+    /// <summary>\n+    /// Updates matrices A and B using their gradients.\n+    /// </summary>\n+    private void UpdateMatricesWithGradients(T learningRate)\n+    {\n+        if (_gradientA == null || _gradientB == null)\n+        {\n+            return;\n+        }\n+\n+        // Update matrix A\n+        for (int i = 0; i < _matrixA.Rows; i++)\n+        {\n+            for (int j = 0; j < _matrixA.Columns; j++)\n+            {\n+                T update = NumOps.Multiply(_gradientA[i, j], learningRate);\n+                _matrixA[i, j] = NumOps.Subtract(_matrixA[i, j], update);\n+            }\n+        }\n+\n+        // Update matrix B\n+        for (int i = 0; i < _matrixB.Rows; i++)\n+        {\n+            for (int j = 0; j < _matrixB.Columns; j++)\n+            {\n+                T update = NumOps.Multiply(_gradientB[i, j], learningRate);\n+                _matrixB[i, j] = NumOps.Subtract(_matrixB[i, j], update);\n+            }\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Merges the LoKr adaptation into the base layer and returns the merged layer.\n+    /// </summary>\n+    /// <returns>A new layer with LoKr weights merged into the base layer's weights.</returns>\n+    /// <exception cref=\"InvalidOperationException\">Thrown when the base layer type is not DenseLayer or FullyConnectedLayer.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// This computes the full Kronecker product A ‚äó B and adds it to the base layer's weights.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This \"bakes in\" your LoKr adaptation to create a regular layer.\n+    /// It computes the full Kronecker product matrix and adds it to the original weights, creating\n+    /// a single merged layer that's faster for inference.\n+    /// </para>\n+    /// </remarks>\n+    public override ILayer<T> MergeToOriginalLayer()\n+    {\n+        // Support both DenseLayer and FullyConnectedLayer\n+        DenseLayer<T>? denseBase = _baseLayer as DenseLayer<T>;\n+        FullyConnectedLayer<T>? fcBase = _baseLayer as FullyConnectedLayer<T>;\n+\n+        if (denseBase == null && fcBase == null)\n+        {\n+            throw new InvalidOperationException(\"LoKrAdapter only supports DenseLayer or FullyConnectedLayer base layers\");\n+        }\n+\n+        // Compute full Kronecker product\n+        Matrix<T> kronWeights = KroneckerProduct(_matrixA, _matrixB);\n+\n+        // Apply scaling\n+        kronWeights = kronWeights.Multiply(_scaling);\n+\n+        // Get base layer parameters\n+        Vector<T> baseParams = _baseLayer.GetParameters();\n+\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        int weightCount = inputSize * outputSize;\n+\n+        // Create new parameters with merged weights\n+        Vector<T> mergedParams = new Vector<T>(baseParams.Length);\n+\n+        // Merge weights (kronWeights is outputSize √ó inputSize, same as base weights)\n+        for (int i = 0; i < weightCount; i++)\n+        {\n+            int row = i / inputSize;\n+            int col = i % inputSize;\n+            mergedParams[i] = NumOps.Add(baseParams[i], kronWeights[row, col]);\n+        }\n+\n+        // Copy biases unchanged\n+        for (int i = weightCount; i < baseParams.Length; i++)\n+        {\n+            mergedParams[i] = baseParams[i];\n+        }\n+\n+        // Create a new dense layer with merged parameters\n+        DenseLayer<T> mergedLayer = new DenseLayer<T>(inputSize, outputSize, (IActivationFunction<T>?)null);\n+        mergedLayer.SetParameters(mergedParams);\n+\n+        return mergedLayer;\n+    }\n+\n+    /// <summary>\n+    /// Gets the current parameters as a vector.\n+    /// </summary>\n+    /// <returns>Vector containing parameters (LoKr only if base is frozen, otherwise both).</returns>\n+    public override Vector<T> GetParameters()\n+    {\n+        return Parameters.Clone();\n+    }\n+\n+    /// <summary>\n+    /// Sets the layer parameters from a vector.\n+    /// </summary>\n+    /// <param name=\"parameters\">Vector containing parameters.</param>\n+    public override void SetParameters(Vector<T> parameters)\n+    {\n+        if (parameters.Length != ParameterCount)\n+        {\n+            throw new ArgumentException($\"Expected {ParameterCount} parameters, got {parameters.Length}\", nameof(parameters));\n+        }\n+\n+        Parameters = parameters.Clone();\n+        UpdateMatricesFromParameters();\n+    }\n+\n+    /// <summary>\n+    /// Updates the parameter vector from the current matrix states.\n+    /// </summary>\n+    private void UpdateParametersFromMatrices()\n+    {\n+        int idx = 0;\n+\n+        // Pack matrix A\n+        for (int i = 0; i < _matrixA.Rows; i++)\n+        {\n+            for (int j = 0; j < _matrixA.Columns; j++)\n+            {\n+                Parameters[idx++] = _matrixA[i, j];\n+            }\n+        }\n+\n+        // Pack matrix B\n+        for (int i = 0; i < _matrixB.Rows; i++)\n+        {\n+            for (int j = 0; j < _matrixB.Columns; j++)\n+            {\n+                Parameters[idx++] = _matrixB[i, j];\n+            }\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Updates the matrices from the parameter vector.\n+    /// </summary>\n+    private void UpdateMatricesFromParameters()\n+    {\n+        int idx = 0;\n+\n+        // Unpack matrix A\n+        for (int i = 0; i < _matrixA.Rows; i++)\n+        {\n+            for (int j = 0; j < _matrixA.Columns; j++)\n+            {\n+                _matrixA[i, j] = Parameters[idx++];\n+            }\n+        }\n+\n+        // Unpack matrix B\n+        for (int i = 0; i < _matrixB.Rows; i++)\n+        {\n+            for (int j = 0; j < _matrixB.Columns; j++)\n+            {\n+                _matrixB[i, j] = Parameters[idx++];\n+            }\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Updates the parameter gradients vector from the matrix gradients.\n+    /// </summary>\n+    private void UpdateParameterGradientsFromMatrices()\n+    {\n+        if (_gradientA == null || _gradientB == null)\n+        {\n+            return;\n+        }\n+\n+        ParameterGradients = new Vector<T>(ParameterCount);\n+        int idx = 0;\n+\n+        // Pack matrix A gradients\n+        for (int i = 0; i < _gradientA.Rows; i++)\n+        {\n+            for (int j = 0; j < _gradientA.Columns; j++)\n+            {\n+                ParameterGradients[idx++] = _gradientA[i, j];\n+            }\n+        }\n+\n+        // Pack matrix B gradients\n+        for (int i = 0; i < _gradientB.Rows; i++)\n+        {\n+            for (int j = 0; j < _gradientB.Columns; j++)\n+            {\n+                ParameterGradients[idx++] = _gradientB[i, j];\n+            }\n+        }\n+    }","path":"src/LoRA/Adapters/LoKrAdapter.cs","commit_id":"fa815036ee570162056ba6aca773f34c80f36948","original_commit_id":"8d5a1b6564c1a2bc1b334f1863e6003a6fd142b2","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**Fix gradient packing to include base layer gradients when not frozen.**\n\n`UpdateParameterGradientsFromMatrices` only packs gradients for matrices A and B, but when `_freezeBaseLayer` is false, base layer gradients should also be included. LoHaAdapter correctly handles this (lines 741-784). \n\nWithout this, the `ParameterGradients` vector is incomplete when the base layer is not frozen, which breaks gradient-based operations.\n\n\n\nApply this diff:\n\n```diff\n private void UpdateParameterGradientsFromMatrices()\n {\n     if (_gradientA == null || _gradientB == null)\n     {\n         return;\n     }\n \n     ParameterGradients = new Vector<T>(ParameterCount);\n     int idx = 0;\n \n+    // Pack base layer gradients if not frozen\n+    if (!_freezeBaseLayer)\n+    {\n+        Vector<T> baseGrads = _baseLayer.GetParameterGradients();\n+        for (int i = 0; i < baseGrads.Length; i++)\n+        {\n+            ParameterGradients[idx++] = baseGrads[i];\n+        }\n+    }\n+\n     // Pack matrix A gradients\n     for (int i = 0; i < _gradientA.Rows; i++)\n     {\n         for (int j = 0; j < _gradientA.Columns; j++)\n         {\n             ParameterGradients[idx++] = _gradientA[i, j];\n         }\n     }\n \n     // Pack matrix B gradients\n     for (int i = 0; i < _gradientB.Rows; i++)\n     {\n         for (int j = 0; j < _gradientB.Columns; j++)\n         {\n             ParameterGradients[idx++] = _gradientB[i, j];\n         }\n     }\n }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    private void UpdateParameterGradientsFromMatrices()\n    {\n        if (_gradientA == null || _gradientB == null)\n        {\n            return;\n        }\n\n        ParameterGradients = new Vector<T>(ParameterCount);\n        int idx = 0;\n\n        // Pack base layer gradients if not frozen\n        if (!_freezeBaseLayer)\n        {\n            Vector<T> baseGrads = _baseLayer.GetParameterGradients();\n            for (int i = 0; i < baseGrads.Length; i++)\n            {\n                ParameterGradients[idx++] = baseGrads[i];\n            }\n        }\n\n        // Pack matrix A gradients\n        for (int i = 0; i < _gradientA.Rows; i++)\n        {\n            for (int j = 0; j < _gradientA.Columns; j++)\n            {\n                ParameterGradients[idx++] = _gradientA[i, j];\n            }\n        }\n\n        // Pack matrix B gradients\n        for (int i = 0; i < _gradientB.Rows; i++)\n        {\n            for (int j = 0; j < _gradientB.Columns; j++)\n            {\n                ParameterGradients[idx++] = _gradientB[i, j];\n            }\n        }\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/LoRA/Adapters/LoKrAdapter.cs around lines 695 to 722,\nUpdateParameterGradientsFromMatrices currently only packs gradients from\n_gradientA and _gradientB, but when _freezeBaseLayer is false you must also\nappend the base layer gradients to ParameterGradients so the vector is complete;\nmodify the method to, after packing A and B, check if (!_freezeBaseLayer) and\nthen iterate over the base layer gradient storage (same shape/iteration pattern\nused in LoHaAdapter lines ~741-784) copying each base gradient into\nParameterGradients[idx++] so ParameterGradients contains A, then B, then base\ngradients (ensure idx increments and bounds match ParameterCount).\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T03:02:28Z","updated_at":"2025-11-02T03:02:32Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140307","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140307"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140307"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140307/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":695,"original_start_line":695,"start_side":"RIGHT","line":722,"original_line":722,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":722,"position":722,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140310","pull_request_review_id":3408046172,"id":2484140310,"node_id":"PRRC_kwDOKSXUF86UEPkW","diff_hunk":"@@ -0,0 +1,516 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// LoRA-drop implementation: LoRA with dropout regularization.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// LoRA-drop extends standard LoRA by adding dropout to the LoRA components during training.\n+/// During the forward pass in training mode, a random subset of LoRA components are \"dropped out\"\n+/// (set to zero), forcing the model to learn more robust adaptations that don't rely on any\n+/// single component.\n+/// </para>\n+/// <para>\n+/// Key differences from standard LoRA:\n+/// - Applies dropout to LoRA output during training\n+/// - Scales LoRA output by (1 - dropout_rate) during inference\n+/// - Improves generalization and reduces overfitting\n+/// - Particularly useful when adaptation data is limited\n+/// </para>\n+/// <para><b>For Beginners:</b> LoRA-drop adds dropout regularization to LoRA adapters.\n+///\n+/// Dropout is a technique where during training, we randomly \"turn off\" some neurons or components.\n+/// This prevents the model from becoming too dependent on specific components and forces it to\n+/// learn more general patterns.\n+///\n+/// Think of it like practicing a skill with random handicaps:\n+/// - Sometimes you practice with your left hand tied behind your back\n+/// - Sometimes you practice blindfolded\n+/// - This forces you to develop multiple strategies instead of relying on one approach\n+///\n+/// LoRA-drop applies this to LoRA adaptations:\n+/// - During training: Randomly drop some LoRA components (set them to zero)\n+/// - During inference: Use all components but scale them appropriately\n+/// - Result: More robust adaptations that generalize better to new data\n+///\n+/// Recommended dropout rates:\n+/// - 0.1 (10%): Light regularization, good starting point\n+/// - 0.2 (20%): Moderate regularization, common choice\n+/// - 0.3 (30%): Strong regularization, for small adaptation datasets\n+/// - Higher rates (&gt;0.5): Typically too aggressive, may harm performance\n+///\n+/// When to use LoRA-drop over standard LoRA:\n+/// - You have limited adaptation data (risk of overfitting)\n+/// - You need better generalization to unseen data\n+/// - You're fine-tuning on a very specific task but need to maintain general capabilities\n+/// - You've observed overfitting with standard LoRA\n+/// </para>\n+/// </remarks>\n+public class LoRADropAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Dropout rate (probability of dropping a component during training).\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// The dropout rate determines what fraction of LoRA output components are randomly\n+    /// set to zero during each training step. Common values are 0.1-0.3.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is the probability that any given component gets \"turned off\"\n+    /// during training. For example, 0.2 means each component has a 20% chance of being dropped.\n+    /// </para>\n+    /// </remarks>\n+    private readonly double _dropoutRate;\n+\n+    /// <summary>\n+    /// Mask indicating which components to drop in the current forward pass.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This boolean array has the same length as the LoRA output. True means keep the component,\n+    /// false means drop it (set to zero). The mask is regenerated randomly for each forward pass\n+    /// during training.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is like a binary on/off switch for each component.\n+    /// During training, we randomly set some to \"off\" (false) to apply dropout.\n+    /// </para>\n+    /// </remarks>\n+    private bool[]? _dropoutMask;\n+\n+    /// <summary>\n+    /// Indicates whether the layer is in training mode (dropout active) or inference mode (dropout inactive).\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// When true, dropout is applied during forward passes. When false (inference mode),\n+    /// dropout is disabled and outputs are scaled by (1 - dropout_rate) for consistency.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This switch controls whether we're in \"learning mode\" or \"using mode\".\n+    /// During learning (training), we apply dropout. During use (inference), we turn it off.\n+    /// </para>\n+    /// </remarks>\n+    private bool _isTraining;\n+\n+    /// <summary>\n+    /// Random number generator for dropout mask generation.\n+    /// </summary>\n+    private readonly Random _random;\n+\n+    /// <summary>\n+    /// Gets the dropout rate used for regularization.\n+    /// </summary>\n+    public double DropoutRate => _dropoutRate;\n+\n+    /// <summary>\n+    /// Gets or sets whether the layer is in training mode.\n+    /// </summary>\n+    /// <remarks>\n+    /// Set to true during training (dropout active), false during inference (dropout inactive).\n+    /// </remarks>\n+    public bool IsTraining\n+    {\n+        get => _isTraining;\n+        set => _isTraining = value;\n+    }\n+\n+    /// <summary>\n+    /// Initializes a new LoRA-drop adapter with dropout regularization.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with LoRA.</param>\n+    /// <param name=\"rank\">The rank of the LoRA decomposition.</param>\n+    /// <param name=\"dropoutRate\">The dropout rate (probability of dropping a component). Common values: 0.1-0.3.</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <param name=\"seed\">Random seed for reproducible dropout masks (optional).</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when dropoutRate is not in [0, 1) range.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This creates a LoRA adapter with dropout regularization.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to adapt\n+    /// - rank: How much compression to use (same as standard LoRA)\n+    /// - dropoutRate: What fraction to randomly drop during training (0.1 = 10%, 0.2 = 20%, etc.)\n+    /// - alpha: How strong the LoRA adaptation is\n+    /// - freezeBaseLayer: Whether to freeze the original layer (usually true)\n+    /// - seed: Optional random seed for reproducible results\n+    ///\n+    /// Example usage:\n+    /// ```csharp\n+    /// // Create a LoRA-drop adapter with 20% dropout\n+    /// var adapter = new LoRADropAdapter&lt;double&gt;(denseLayer, rank: 8, dropoutRate: 0.2);\n+    ///\n+    /// // Training mode (dropout active)\n+    /// adapter.SetTraining(true);\n+    /// var trainOutput = adapter.Forward(trainInput);\n+    ///\n+    /// // Inference mode (dropout inactive)\n+    /// adapter.SetTraining(false);\n+    /// var testOutput = adapter.Forward(testInput);\n+    /// ```\n+    /// </para>\n+    /// </remarks>\n+    public LoRADropAdapter(ILayer<T> baseLayer, int rank, double dropoutRate, double alpha = -1, bool freezeBaseLayer = true, int? seed = null)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        if (dropoutRate < 0.0 || dropoutRate >= 1.0)\n+        {\n+            throw new ArgumentException(\"Dropout rate must be in the range [0, 1)\", nameof(dropoutRate));\n+        }\n+\n+        _dropoutRate = dropoutRate;\n+        _isTraining = true; // Default to training mode\n+        _random = seed.HasValue ? new Random(seed.Value) : new Random();\n+\n+        // Initialize dropout mask (will be regenerated on each forward pass during training)\n+        int outputSize = GetOutputShape()[0];\n+        _dropoutMask = new bool[outputSize];\n+    }\n+\n+    /// <summary>\n+    /// Sets whether the layer is in training mode or inference mode.\n+    /// </summary>\n+    /// <param name=\"training\">True for training mode (dropout active), false for inference mode (dropout inactive).</param>\n+    /// <remarks>\n+    /// <para>\n+    /// This method should be called to switch between training and inference modes.\n+    /// During training, dropout is applied. During inference, dropout is disabled and\n+    /// outputs are scaled appropriately.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Call this before you start training or testing:\n+    /// - Before training: `adapter.SetTraining(true)`\n+    /// - Before testing/inference: `adapter.SetTraining(false)`\n+    ///\n+    /// This ensures dropout is only used during training, not when making predictions.\n+    /// </para>\n+    /// </remarks>\n+    public void SetTraining(bool training)\n+    {\n+        _isTraining = training;\n+    }\n+\n+    /// <summary>\n+    /// Generates a random dropout mask for the current forward pass.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// For each component, generates a random value and compares it to the dropout rate.\n+    /// If the random value is greater than the dropout rate, the component is kept (true),\n+    /// otherwise it's dropped (false).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This randomly decides which components to keep and which to drop.\n+    /// Think of it like flipping a weighted coin for each component - if you get \"heads\"\n+    /// (random value &gt; dropout rate), you keep it; otherwise you drop it.\n+    /// </para>\n+    /// </remarks>\n+    private void GenerateDropoutMask()\n+    {\n+        if (_dropoutMask == null)\n+        {\n+            return;\n+        }\n+\n+        for (int i = 0; i < _dropoutMask.Length; i++)\n+        {\n+            // Keep the component if random value is greater than dropout rate\n+            _dropoutMask[i] = _random.NextDouble() > _dropoutRate;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass with dropout applied to LoRA output.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Sum of base layer output and dropout-regularized LoRA output.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// During training:\n+    /// 1. Generate new dropout mask\n+    /// 2. Compute LoRA output\n+    /// 3. Apply dropout mask (zero out dropped components)\n+    /// 4. Scale kept components by 1/(1-dropout_rate) to maintain expected value\n+    /// 5. Add to base layer output\n+    ///\n+    /// During inference:\n+    /// 1. Compute LoRA output\n+    /// 2. Scale by (1-dropout_rate) to match training expectation\n+    /// 3. Add to base layer output\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This runs the input through the layer with dropout applied.\n+    ///\n+    /// Training mode:\n+    /// - Randomly drops some LoRA components\n+    /// - Scales up the remaining components to compensate\n+    /// - This forces the model to not rely on any single component\n+    ///\n+    /// Inference mode:\n+    /// - Uses all components\n+    /// - Scales them down to match what the model learned during training\n+    /// - This ensures consistent behavior between training and testing\n+    ///\n+    /// The scaling ensures that the expected output is the same whether or not dropout is active,\n+    /// which is important for stable training and accurate predictions.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        // Forward through base layer\n+        Tensor<T> baseOutput = _baseLayer.Forward(input);\n+\n+        // Forward through LoRA layer\n+        Tensor<T> loraOutput = _loraLayer.Forward(input);\n+\n+        // Apply dropout to LoRA output\n+        if (_isTraining)\n+        {\n+            // Training mode: apply dropout mask\n+            GenerateDropoutMask();\n+\n+            // Scale factor to maintain expected value: 1 / (1 - dropout_rate)\n+            // This compensates for the components we're dropping\n+            T invKeepProb = NumOps.Divide(NumOps.One, NumOps.FromDouble(1.0 - _dropoutRate));\n+\n+            for (int i = 0; i < loraOutput.Length; i++)\n+            {\n+                if (_dropoutMask != null && !_dropoutMask[i % _dropoutMask.Length])\n+                {\n+                    // Drop this component\n+                    loraOutput[i] = NumOps.Zero;\n+                }\n+                else\n+                {\n+                    // Keep this component and scale it\n+                    loraOutput[i] = NumOps.Multiply(loraOutput[i], invKeepProb);\n+                }\n+            }\n+        }\n+        else\n+        {\n+            // Inference mode: no dropout, but scale by (1 - dropout_rate)\n+            // This matches the expected value from training\n+            T scale = NumOps.FromDouble(1.0 - _dropoutRate);\n+            for (int i = 0; i < loraOutput.Length; i++)\n+            {\n+                loraOutput[i] = NumOps.Multiply(loraOutput[i], scale);\n+            }\n+        }","path":"src/LoRA/Adapters/LoRADropAdapter.cs","commit_id":"fa815036ee570162056ba6aca773f34c80f36948","original_commit_id":"8d5a1b6564c1a2bc1b334f1863e6003a6fd142b2","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Fix incorrect inference scaling for inverted dropout.**\n\nThe inference mode scaling is incorrect. When using inverted dropout (scaling by `1/(1-dropout_rate)` during training), inference should apply **no scaling**. The current implementation scales by `(1-dropout_rate)` during inference, which reduces the LoRA contribution incorrectly.\n\nWith `dropout_rate = 0.2`:\n- Training: components scaled by `1/0.8 = 1.25` ‚úì\n- Inference (current): components scaled by `0.8` ‚úó\n- Result: inference LoRA contribution is only 64% of training expectation `(0.8 √ó 1.25 = 1.0 vs 0.8)`\n\n\n\nApply this diff to fix the inference path:\n\n```diff\n         else\n         {\n-            // Inference mode: no dropout, but scale by (1 - dropout_rate)\n-            // This matches the expected value from training\n-            T scale = NumOps.FromDouble(1.0 - _dropoutRate);\n-            for (int i = 0; i < loraOutput.Length; i++)\n-            {\n-                loraOutput[i] = NumOps.Multiply(loraOutput[i], scale);\n-            }\n+            // Inference mode: no dropout, no scaling (inverted dropout)\n+            // Training already scaled by 1/(1-dropout_rate), so inference uses all components as-is\n         }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        else\n        {\n            // Inference mode: no dropout, no scaling (inverted dropout)\n            // Training already scaled by 1/(1-dropout_rate), so inference uses all components as-is\n        }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/LoRA/Adapters/LoRADropAdapter.cs around lines 290 to 299, the inference\nbranch wrongly scales the LoRA output by (1 - _dropoutRate); with inverted\ndropout training already scaling activations by 1/(1 - dropout_rate), inference\nmust not apply any additional scaling. Remove the scaling step so the code\nleaves loraOutput unchanged in inference mode (i.e., do not multiply by scale),\nensuring LoRA contribution during inference matches training.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T03:02:28Z","updated_at":"2025-11-02T03:02:32Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140310","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140310"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140310"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140310/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":290,"original_start_line":290,"start_side":"RIGHT","line":299,"original_line":299,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":299,"position":299,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140312","pull_request_review_id":3408046172,"id":2484140312,"node_id":"PRRC_kwDOKSXUF86UEPkY","diff_hunk":"@@ -0,0 +1,516 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// LoRA-drop implementation: LoRA with dropout regularization.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// LoRA-drop extends standard LoRA by adding dropout to the LoRA components during training.\n+/// During the forward pass in training mode, a random subset of LoRA components are \"dropped out\"\n+/// (set to zero), forcing the model to learn more robust adaptations that don't rely on any\n+/// single component.\n+/// </para>\n+/// <para>\n+/// Key differences from standard LoRA:\n+/// - Applies dropout to LoRA output during training\n+/// - Scales LoRA output by (1 - dropout_rate) during inference\n+/// - Improves generalization and reduces overfitting\n+/// - Particularly useful when adaptation data is limited\n+/// </para>\n+/// <para><b>For Beginners:</b> LoRA-drop adds dropout regularization to LoRA adapters.\n+///\n+/// Dropout is a technique where during training, we randomly \"turn off\" some neurons or components.\n+/// This prevents the model from becoming too dependent on specific components and forces it to\n+/// learn more general patterns.\n+///\n+/// Think of it like practicing a skill with random handicaps:\n+/// - Sometimes you practice with your left hand tied behind your back\n+/// - Sometimes you practice blindfolded\n+/// - This forces you to develop multiple strategies instead of relying on one approach\n+///\n+/// LoRA-drop applies this to LoRA adaptations:\n+/// - During training: Randomly drop some LoRA components (set them to zero)\n+/// - During inference: Use all components but scale them appropriately\n+/// - Result: More robust adaptations that generalize better to new data\n+///\n+/// Recommended dropout rates:\n+/// - 0.1 (10%): Light regularization, good starting point\n+/// - 0.2 (20%): Moderate regularization, common choice\n+/// - 0.3 (30%): Strong regularization, for small adaptation datasets\n+/// - Higher rates (&gt;0.5): Typically too aggressive, may harm performance\n+///\n+/// When to use LoRA-drop over standard LoRA:\n+/// - You have limited adaptation data (risk of overfitting)\n+/// - You need better generalization to unseen data\n+/// - You're fine-tuning on a very specific task but need to maintain general capabilities\n+/// - You've observed overfitting with standard LoRA\n+/// </para>\n+/// </remarks>\n+public class LoRADropAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Dropout rate (probability of dropping a component during training).\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// The dropout rate determines what fraction of LoRA output components are randomly\n+    /// set to zero during each training step. Common values are 0.1-0.3.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is the probability that any given component gets \"turned off\"\n+    /// during training. For example, 0.2 means each component has a 20% chance of being dropped.\n+    /// </para>\n+    /// </remarks>\n+    private readonly double _dropoutRate;\n+\n+    /// <summary>\n+    /// Mask indicating which components to drop in the current forward pass.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This boolean array has the same length as the LoRA output. True means keep the component,\n+    /// false means drop it (set to zero). The mask is regenerated randomly for each forward pass\n+    /// during training.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is like a binary on/off switch for each component.\n+    /// During training, we randomly set some to \"off\" (false) to apply dropout.\n+    /// </para>\n+    /// </remarks>\n+    private bool[]? _dropoutMask;\n+\n+    /// <summary>\n+    /// Indicates whether the layer is in training mode (dropout active) or inference mode (dropout inactive).\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// When true, dropout is applied during forward passes. When false (inference mode),\n+    /// dropout is disabled and outputs are scaled by (1 - dropout_rate) for consistency.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This switch controls whether we're in \"learning mode\" or \"using mode\".\n+    /// During learning (training), we apply dropout. During use (inference), we turn it off.\n+    /// </para>\n+    /// </remarks>\n+    private bool _isTraining;\n+\n+    /// <summary>\n+    /// Random number generator for dropout mask generation.\n+    /// </summary>\n+    private readonly Random _random;\n+\n+    /// <summary>\n+    /// Gets the dropout rate used for regularization.\n+    /// </summary>\n+    public double DropoutRate => _dropoutRate;\n+\n+    /// <summary>\n+    /// Gets or sets whether the layer is in training mode.\n+    /// </summary>\n+    /// <remarks>\n+    /// Set to true during training (dropout active), false during inference (dropout inactive).\n+    /// </remarks>\n+    public bool IsTraining\n+    {\n+        get => _isTraining;\n+        set => _isTraining = value;\n+    }\n+\n+    /// <summary>\n+    /// Initializes a new LoRA-drop adapter with dropout regularization.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with LoRA.</param>\n+    /// <param name=\"rank\">The rank of the LoRA decomposition.</param>\n+    /// <param name=\"dropoutRate\">The dropout rate (probability of dropping a component). Common values: 0.1-0.3.</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <param name=\"seed\">Random seed for reproducible dropout masks (optional).</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when dropoutRate is not in [0, 1) range.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This creates a LoRA adapter with dropout regularization.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to adapt\n+    /// - rank: How much compression to use (same as standard LoRA)\n+    /// - dropoutRate: What fraction to randomly drop during training (0.1 = 10%, 0.2 = 20%, etc.)\n+    /// - alpha: How strong the LoRA adaptation is\n+    /// - freezeBaseLayer: Whether to freeze the original layer (usually true)\n+    /// - seed: Optional random seed for reproducible results\n+    ///\n+    /// Example usage:\n+    /// ```csharp\n+    /// // Create a LoRA-drop adapter with 20% dropout\n+    /// var adapter = new LoRADropAdapter&lt;double&gt;(denseLayer, rank: 8, dropoutRate: 0.2);\n+    ///\n+    /// // Training mode (dropout active)\n+    /// adapter.SetTraining(true);\n+    /// var trainOutput = adapter.Forward(trainInput);\n+    ///\n+    /// // Inference mode (dropout inactive)\n+    /// adapter.SetTraining(false);\n+    /// var testOutput = adapter.Forward(testInput);\n+    /// ```\n+    /// </para>\n+    /// </remarks>\n+    public LoRADropAdapter(ILayer<T> baseLayer, int rank, double dropoutRate, double alpha = -1, bool freezeBaseLayer = true, int? seed = null)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        if (dropoutRate < 0.0 || dropoutRate >= 1.0)\n+        {\n+            throw new ArgumentException(\"Dropout rate must be in the range [0, 1)\", nameof(dropoutRate));\n+        }\n+\n+        _dropoutRate = dropoutRate;\n+        _isTraining = true; // Default to training mode\n+        _random = seed.HasValue ? new Random(seed.Value) : new Random();\n+\n+        // Initialize dropout mask (will be regenerated on each forward pass during training)\n+        int outputSize = GetOutputShape()[0];\n+        _dropoutMask = new bool[outputSize];\n+    }\n+\n+    /// <summary>\n+    /// Sets whether the layer is in training mode or inference mode.\n+    /// </summary>\n+    /// <param name=\"training\">True for training mode (dropout active), false for inference mode (dropout inactive).</param>\n+    /// <remarks>\n+    /// <para>\n+    /// This method should be called to switch between training and inference modes.\n+    /// During training, dropout is applied. During inference, dropout is disabled and\n+    /// outputs are scaled appropriately.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Call this before you start training or testing:\n+    /// - Before training: `adapter.SetTraining(true)`\n+    /// - Before testing/inference: `adapter.SetTraining(false)`\n+    ///\n+    /// This ensures dropout is only used during training, not when making predictions.\n+    /// </para>\n+    /// </remarks>\n+    public void SetTraining(bool training)\n+    {\n+        _isTraining = training;\n+    }\n+\n+    /// <summary>\n+    /// Generates a random dropout mask for the current forward pass.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// For each component, generates a random value and compares it to the dropout rate.\n+    /// If the random value is greater than the dropout rate, the component is kept (true),\n+    /// otherwise it's dropped (false).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This randomly decides which components to keep and which to drop.\n+    /// Think of it like flipping a weighted coin for each component - if you get \"heads\"\n+    /// (random value &gt; dropout rate), you keep it; otherwise you drop it.\n+    /// </para>\n+    /// </remarks>\n+    private void GenerateDropoutMask()\n+    {\n+        if (_dropoutMask == null)\n+        {\n+            return;\n+        }\n+\n+        for (int i = 0; i < _dropoutMask.Length; i++)\n+        {\n+            // Keep the component if random value is greater than dropout rate\n+            _dropoutMask[i] = _random.NextDouble() > _dropoutRate;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass with dropout applied to LoRA output.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Sum of base layer output and dropout-regularized LoRA output.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// During training:\n+    /// 1. Generate new dropout mask\n+    /// 2. Compute LoRA output\n+    /// 3. Apply dropout mask (zero out dropped components)\n+    /// 4. Scale kept components by 1/(1-dropout_rate) to maintain expected value\n+    /// 5. Add to base layer output\n+    ///\n+    /// During inference:\n+    /// 1. Compute LoRA output\n+    /// 2. Scale by (1-dropout_rate) to match training expectation\n+    /// 3. Add to base layer output\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This runs the input through the layer with dropout applied.\n+    ///\n+    /// Training mode:\n+    /// - Randomly drops some LoRA components\n+    /// - Scales up the remaining components to compensate\n+    /// - This forces the model to not rely on any single component\n+    ///\n+    /// Inference mode:\n+    /// - Uses all components\n+    /// - Scales them down to match what the model learned during training\n+    /// - This ensures consistent behavior between training and testing\n+    ///\n+    /// The scaling ensures that the expected output is the same whether or not dropout is active,\n+    /// which is important for stable training and accurate predictions.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        // Forward through base layer\n+        Tensor<T> baseOutput = _baseLayer.Forward(input);\n+\n+        // Forward through LoRA layer\n+        Tensor<T> loraOutput = _loraLayer.Forward(input);\n+\n+        // Apply dropout to LoRA output\n+        if (_isTraining)\n+        {\n+            // Training mode: apply dropout mask\n+            GenerateDropoutMask();\n+\n+            // Scale factor to maintain expected value: 1 / (1 - dropout_rate)\n+            // This compensates for the components we're dropping\n+            T invKeepProb = NumOps.Divide(NumOps.One, NumOps.FromDouble(1.0 - _dropoutRate));\n+\n+            for (int i = 0; i < loraOutput.Length; i++)\n+            {\n+                if (_dropoutMask != null && !_dropoutMask[i % _dropoutMask.Length])\n+                {\n+                    // Drop this component\n+                    loraOutput[i] = NumOps.Zero;\n+                }\n+                else\n+                {\n+                    // Keep this component and scale it\n+                    loraOutput[i] = NumOps.Multiply(loraOutput[i], invKeepProb);\n+                }\n+            }\n+        }\n+        else\n+        {\n+            // Inference mode: no dropout, but scale by (1 - dropout_rate)\n+            // This matches the expected value from training\n+            T scale = NumOps.FromDouble(1.0 - _dropoutRate);\n+            for (int i = 0; i < loraOutput.Length; i++)\n+            {\n+                loraOutput[i] = NumOps.Multiply(loraOutput[i], scale);\n+            }\n+        }\n+\n+        // Sum the outputs\n+        Tensor<T> result = new Tensor<T>(baseOutput.Shape);\n+        for (int i = 0; i < baseOutput.Length; i++)\n+        {\n+            result[i] = NumOps.Add(baseOutput[i], loraOutput[i]);\n+        }\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Performs the backward pass with dropout mask applied to gradients.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient flowing back from the next layer.</param>\n+    /// <returns>Gradient to pass to the previous layer.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// During backpropagation, gradients are only propagated through components that were\n+    /// not dropped during the forward pass. This is achieved by applying the same dropout\n+    /// mask to the gradients and scaling appropriately.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This propagates gradients back through the layer.\n+    ///\n+    /// Key insight: Gradients only flow through the components that were active during\n+    /// the forward pass. If a component was dropped (set to zero), its gradient is also\n+    /// zero - we don't update it based on this training example.\n+    ///\n+    /// This ensures that:\n+    /// - Dropped components don't get updated (they were \"turned off\")\n+    /// - Kept components get normal gradient updates\n+    /// - The scaling from the forward pass is preserved in gradients\n+    ///\n+    /// The result is that the model learns to work with different subsets of components,\n+    /// making it more robust and less prone to overfitting.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        // Create a gradient for the LoRA layer\n+        Tensor<T> loraGradient = new Tensor<T>(outputGradient.Shape);\n+\n+        if (_isTraining)\n+        {\n+            // Apply dropout mask and scaling to gradients\n+            T invKeepProb = NumOps.Divide(NumOps.One, NumOps.FromDouble(1.0 - _dropoutRate));\n+\n+            for (int i = 0; i < outputGradient.Length; i++)\n+            {\n+                if (_dropoutMask != null && !_dropoutMask[i % _dropoutMask.Length])\n+                {\n+                    // This component was dropped - zero gradient\n+                    loraGradient[i] = NumOps.Zero;\n+                }\n+                else\n+                {\n+                    // This component was kept - propagate gradient with scaling\n+                    loraGradient[i] = NumOps.Multiply(outputGradient[i], invKeepProb);\n+                }\n+            }\n+        }\n+        else\n+        {\n+            // Inference mode: scale gradients by (1 - dropout_rate)\n+            T scale = NumOps.FromDouble(1.0 - _dropoutRate);\n+            for (int i = 0; i < outputGradient.Length; i++)\n+            {\n+                loraGradient[i] = NumOps.Multiply(outputGradient[i], scale);\n+            }\n+        }","path":"src/LoRA/Adapters/LoRADropAdapter.cs","commit_id":"fa815036ee570162056ba6aca773f34c80f36948","original_commit_id":"8d5a1b6564c1a2bc1b334f1863e6003a6fd142b2","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Fix incorrect inference gradient scaling.**\n\nThe inference mode gradient scaling has the same bug as the forward pass. When using inverted dropout, inference gradients should not be scaled by `(1-dropout_rate)`.\n\n\nApply this diff:\n\n```diff\n         else\n         {\n-            // Inference mode: scale gradients by (1 - dropout_rate)\n-            T scale = NumOps.FromDouble(1.0 - _dropoutRate);\n-            for (int i = 0; i < outputGradient.Length; i++)\n-            {\n-                loraGradient[i] = NumOps.Multiply(outputGradient[i], scale);\n-            }\n+            // Inference mode: no scaling (inverted dropout)\n+            for (int i = 0; i < outputGradient.Length; i++)\n+            {\n+                loraGradient[i] = outputGradient[i];\n+            }\n         }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        else\n        {\n            // Inference mode: no scaling (inverted dropout)\n            for (int i = 0; i < outputGradient.Length; i++)\n            {\n                loraGradient[i] = outputGradient[i];\n            }\n        }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/LoRA/Adapters/LoRADropAdapter.cs around lines 361 to 369, the\ninference-mode gradient scaling incorrectly multiplies loraGradient by (1 -\n_dropoutRate) (same bug as forward pass); when using inverted dropout you must\nnot scale gradients at inference. Replace the current scaling code with a direct\ncopy/assignment from outputGradient to loraGradient (i.e., remove the\nNumOps.FromDouble(1.0 - _dropoutRate) scale and NumOps.Multiply call and instead\nset loraGradient[i] = outputGradient[i] for each element) so inference gradients\nremain unchanged.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T03:02:29Z","updated_at":"2025-11-02T03:02:32Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140312","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140312"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140312"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140312/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":361,"original_start_line":361,"start_side":"RIGHT","line":369,"original_line":369,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":369,"position":369,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140314","pull_request_review_id":3408046172,"id":2484140314,"node_id":"PRRC_kwDOKSXUF86UEPka","diff_hunk":"@@ -0,0 +1,391 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// LoRA+ adapter that uses optimized learning rates for faster convergence and better performance.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// LoRA+ (February 2024) improves upon standard LoRA by using different learning rates for the A and B matrices.\n+/// The key insight is that matrix B (which starts at zero) needs faster updates than matrix A (which starts random).\n+/// This simple modification leads to significantly faster convergence and improved final performance.\n+/// </para>\n+/// <para><b>For Beginners:</b> LoRA+ is an enhanced version of LoRA that trains faster and better.\n+///\n+/// In standard LoRA:\n+/// - Both matrix A and B are updated with the same learning rate\n+/// - Matrix B starts at zero, so it needs time to \"catch up\"\n+/// - Matrix A starts random, so it's already contributing from the start\n+///\n+/// LoRA+ recognizes this asymmetry:\n+/// - Matrix A is updated with a base learning rate (e.g., 0.0001)\n+/// - Matrix B is updated with a higher learning rate (e.g., 0.0016 = 16x higher)\n+/// - This accelerates learning without instability\n+///\n+/// Key parameters:\n+/// - BaseLearningRate: Learning rate for matrix A (the \"slow\" matrix)\n+/// - LearningRateRatio: Multiplier for matrix B (typically 16.0)\n+/// - ScaledLearningRate: Computed as BaseLearningRate * LearningRateRatio\n+///\n+/// Research shows LoRA+ typically achieves:\n+/// - 2x faster convergence\n+/// - Better final performance\n+/// - No additional parameters compared to standard LoRA\n+///\n+/// Example: If base learning rate is 0.0001 and ratio is 16.0:\n+/// - Matrix A updates with learning rate 0.0001\n+/// - Matrix B updates with learning rate 0.0016\n+///\n+/// Reference: LoRA+: Efficient Low Rank Adaptation of Large Models (February 2024)\n+/// </para>\n+/// </remarks>\n+public class LoRAPlusAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// The ratio of learning rates between matrix B and matrix A.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This ratio determines how much faster matrix B is updated compared to matrix A.\n+    /// Typical values range from 8.0 to 32.0, with 16.0 being the recommended default.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This controls how much faster the B matrix learns.\n+    /// A ratio of 16.0 means B learns 16x faster than A. Higher values mean even faster\n+    /// B updates, but too high can cause instability.\n+    /// </para>\n+    /// </remarks>\n+    private double _learningRateRatio;\n+\n+    /// <summary>\n+    /// The base learning rate applied to matrix A.\n+    /// </summary>\n+    /// <remarks>\n+    /// This is the slower learning rate applied to matrix A, which already has random\n+    /// initialization and contributes from the start of training.\n+    /// </remarks>\n+    private T _baseLearningRate;\n+\n+    /// <summary>\n+    /// The scaled learning rate applied to matrix B (BaseLearningRate * LearningRateRatio).\n+    /// </summary>\n+    /// <remarks>\n+    /// This is the faster learning rate applied to matrix B, which starts at zero\n+    /// and needs accelerated updates to catch up with matrix A.\n+    /// </remarks>\n+    private T _scaledLearningRate;\n+\n+    /// <summary>\n+    /// Gets or sets the learning rate ratio between matrix B and matrix A.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Default value is 16.0 as recommended by the LoRA+ paper. Valid range is typically 1.0 to 32.0.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is the multiplier that makes matrix B learn faster.\n+    /// - 1.0 = same speed as standard LoRA (no benefit)\n+    /// - 8.0 = moderate speedup\n+    /// - 16.0 = recommended default\n+    /// - 32.0 = aggressive speedup (may be unstable)\n+    /// </para>\n+    /// </remarks>\n+    public double LearningRateRatio\n+    {\n+        get => _learningRateRatio;\n+        set\n+        {\n+            if (value < 1.0)\n+            {\n+                throw new ArgumentException(\"Learning rate ratio must be at least 1.0\", nameof(value));\n+            }\n+            _learningRateRatio = value;\n+            UpdateScaledLearningRate();\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Gets the base learning rate for matrix A.\n+    /// </summary>\n+    public T BaseLearningRate => _baseLearningRate;\n+\n+    /// <summary>\n+    /// Gets the scaled learning rate for matrix B.\n+    /// </summary>\n+    public T ScaledLearningRate => _scaledLearningRate;\n+\n+    /// <summary>\n+    /// Initializes a new LoRA+ adapter with optimized dual learning rates.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with LoRA+.</param>\n+    /// <param name=\"rank\">The rank of the LoRA decomposition.</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"learningRateRatio\">The ratio of B's learning rate to A's learning rate (default: 16.0).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when learningRateRatio is less than 1.0.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This creates a LoRA+ adapter that will train faster than standard LoRA.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to efficiently fine-tune\n+    /// - rank: How much compression (lower = fewer parameters)\n+    /// - alpha: How strong the LoRA effect is\n+    /// - learningRateRatio: How much faster B learns than A (16.0 is recommended)\n+    /// - freezeBaseLayer: Whether to lock the original weights (usually true)\n+    ///\n+    /// The learning rate ratio is the key differentiator from standard LoRA. Higher ratios\n+    /// mean faster convergence but require careful tuning to avoid instability.\n+    /// </para>\n+    /// </remarks>\n+    public LoRAPlusAdapter(\n+        ILayer<T> baseLayer,\n+        int rank,\n+        double alpha = -1,\n+        double learningRateRatio = 16.0,\n+        bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        if (learningRateRatio < 1.0)\n+        {\n+            throw new ArgumentException(\"Learning rate ratio must be at least 1.0\", nameof(learningRateRatio));\n+        }\n+\n+        _learningRateRatio = learningRateRatio;\n+        _baseLearningRate = NumOps.Zero;\n+        _scaledLearningRate = NumOps.Zero;\n+    }\n+\n+    /// <summary>\n+    /// Sets the learning rates for this adapter.\n+    /// </summary>\n+    /// <param name=\"baseLearningRate\">The base learning rate for matrix A.</param>\n+    /// <remarks>\n+    /// <para>\n+    /// This method sets the base learning rate and automatically computes the scaled\n+    /// learning rate for matrix B using the current learning rate ratio.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Call this to configure how fast the adapter learns.\n+    /// You only need to provide the base learning rate - the higher learning rate for\n+    /// matrix B is calculated automatically using the ratio you specified.\n+    ///\n+    /// Example: If you call SetLearningRates(0.0001) with ratio 16.0:\n+    /// - Matrix A will use learning rate 0.0001\n+    /// - Matrix B will use learning rate 0.0016 (16x faster)\n+    /// </para>\n+    /// </remarks>\n+    public void SetLearningRates(T baseLearningRate)\n+    {\n+        _baseLearningRate = baseLearningRate;\n+        UpdateScaledLearningRate();\n+    }\n+\n+    /// <summary>\n+    /// Updates the scaled learning rate based on the current base learning rate and ratio.\n+    /// </summary>\n+    private void UpdateScaledLearningRate()\n+    {\n+        _scaledLearningRate = NumOps.Multiply(_baseLearningRate, NumOps.FromDouble(_learningRateRatio));\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass through both base and LoRA layers.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Sum of base layer output and LoRA output.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The forward pass is identical to standard LoRA: output = base_layer(input) + lora_layer(input).\n+    /// The dual learning rate optimization only affects the backward pass and parameter updates.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This works exactly like standard LoRA during the forward pass.\n+    /// The magic of LoRA+ happens during training (backward pass), not inference.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        // Forward pass is identical to base LoRA implementation\n+        return base.Forward(input);\n+    }\n+\n+    /// <summary>\n+    /// Performs the backward pass through both layers with dual learning rate scaling.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient flowing back from the next layer.</param>\n+    /// <returns>Gradient to pass to the previous layer.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The backward pass computes gradients for both matrices but applies different scaling\n+    /// factors to prepare for the dual learning rate update. Matrix B gradients are implicitly\n+    /// prepared for faster updates during the UpdateParameters call.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is where LoRA+ differs from standard LoRA!\n+    /// During backpropagation, we compute gradients for both A and B matrices, but we'll\n+    /// apply different learning rates when actually updating the parameters. This prepares\n+    /// the gradients for the dual learning rate optimization.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        // The base backward implementation computes gradients correctly\n+        // The dual learning rate is applied in UpdateParameters\n+        return base.Backward(outputGradient);\n+    }\n+\n+    /// <summary>\n+    /// Updates parameters using dual learning rates (base rate for A, scaled rate for B).\n+    /// </summary>\n+    /// <param name=\"learningRate\">This parameter is used as the base learning rate for matrix A.</param>\n+    /// <remarks>\n+    /// <para>\n+    /// This method overrides the standard LoRA parameter update to apply different learning rates:\n+    /// - Matrix A is updated with the base learning rate\n+    /// - Matrix B is updated with the scaled learning rate (base * ratio)\n+    /// - Base layer is updated with the base learning rate if not frozen\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is where the dual learning rate magic happens!\n+    /// Instead of updating both matrices at the same speed, we:\n+    /// 1. Update matrix A slowly (with the base learning rate)\n+    /// 2. Update matrix B quickly (with the scaled learning rate)\n+    ///\n+    /// This asymmetry accelerates training because:\n+    /// - Matrix A already has random values and is contributing\n+    /// - Matrix B starts at zero and needs to catch up\n+    /// - Giving B a higher learning rate helps it catch up faster\n+    ///\n+    /// The result is faster convergence and better final performance!\n+    /// </para>\n+    /// </remarks>\n+    public override void UpdateParameters(T learningRate)\n+    {\n+        // Store the base learning rate for matrix A\n+        SetLearningRates(learningRate);\n+\n+        // Get the LoRA layer's parameter gradients\n+        Vector<T> loraGrads = _loraLayer.GetParameterGradients();\n+\n+        // Calculate dimensions\n+        int matrixASize = _loraLayer.GetMatrixA().Rows * _loraLayer.GetMatrixA().Columns;\n+        int matrixBSize = _loraLayer.GetMatrixB().Rows * _loraLayer.GetMatrixB().Columns;\n+\n+        // Get current LoRA parameters\n+        Vector<T> loraParams = _loraLayer.GetParameters();\n+\n+        // Update matrix A with base learning rate\n+        for (int i = 0; i < matrixASize; i++)\n+        {\n+            T update = NumOps.Multiply(loraGrads[i], _baseLearningRate);\n+            loraParams[i] = NumOps.Subtract(loraParams[i], update);\n+        }\n+\n+        // Update matrix B with scaled learning rate (higher rate)\n+        for (int i = matrixASize; i < matrixASize + matrixBSize; i++)\n+        {\n+            T update = NumOps.Multiply(loraGrads[i], _scaledLearningRate);\n+            loraParams[i] = NumOps.Subtract(loraParams[i], update);\n+        }\n+\n+        // Apply updated parameters to LoRA layer\n+        _loraLayer.SetParameters(loraParams);\n+\n+        // Update base layer if not frozen (using base learning rate)\n+        if (!_freezeBaseLayer)\n+        {\n+            _baseLayer.UpdateParameters(_baseLearningRate);\n+        }\n+\n+        // Update the adapter's parameter vector\n+        UpdateParametersFromLayers();\n+    }\n+\n+    /// <summary>\n+    /// Merges the LoRA+ adaptation into the base layer and returns the merged layer.\n+    /// </summary>\n+    /// <returns>A new layer with LoRA weights merged into the base layer's weights.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// For LoRA+, merging works exactly like standard LoRA - the dual learning rates only\n+    /// affect training, not the final merged weights.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> After training with LoRA+, you can merge the weights just like\n+    /// standard LoRA. The faster training doesn't change the final result, it just gets you there quicker!\n+    /// </para>\n+    /// </remarks>\n+    public override ILayer<T> MergeToOriginalLayer()\n+    {\n+        // LoRA+ merging is identical to standard LoRA\n+        // For Dense layers, delegate to DenseLoRAAdapter logic\n+        DenseLayer<T>? denseBase = _baseLayer as DenseLayer<T>;\n+        FullyConnectedLayer<T>? fcBase = _baseLayer as FullyConnectedLayer<T>;\n+\n+        if (denseBase == null && fcBase == null)\n+        {\n+            throw new InvalidOperationException(\"LoRAPlusAdapter currently only supports DenseLayer or FullyConnectedLayer base layers\");\n+        }\n+\n+        // Get the LoRA weight contribution\n+        Matrix<T> loraWeights = _loraLayer.MergeWeights();\n+\n+        // Get base layer parameters\n+        Vector<T> baseParams = _baseLayer.GetParameters();\n+\n+        // Calculate dimensions\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        int weightCount = inputSize * outputSize;\n+\n+        // Create new parameters with merged weights\n+        Vector<T> mergedParams = new Vector<T>(baseParams.Length);\n+\n+        // Merge weights\n+        for (int i = 0; i < weightCount; i++)\n+        {\n+            int row = i / inputSize;\n+            int col = i % inputSize;\n+            mergedParams[i] = NumOps.Add(baseParams[i], loraWeights[row, col]);\n+        }\n+\n+        // Copy biases unchanged\n+        for (int i = weightCount; i < baseParams.Length; i++)\n+        {\n+            mergedParams[i] = baseParams[i];\n+        }\n+\n+        // Create a new dense layer with merged parameters\n+        DenseLayer<T> mergedLayer = new DenseLayer<T>(inputSize, outputSize, (IActivationFunction<T>?)null);\n+        mergedLayer.SetParameters(mergedParams);\n+\n+        return mergedLayer;\n+    }","path":"src/LoRA/Adapters/LoRAPlusAdapter.cs","commit_id":"fa815036ee570162056ba6aca773f34c80f36948","original_commit_id":"8d5a1b6564c1a2bc1b334f1863e6003a6fd142b2","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_üõ†Ô∏è Refactor suggestion_ | _üü† Major_\n\n**Significant code duplication with other adapters.**\n\nThe merge logic is nearly identical to `LoFAAdapter.MergeToOriginalLayer()` (lines 343-391 in LoFAAdapter.cs). Both implementations:\n- Check for DenseLayer/FullyConnectedLayer support\n- Calculate merged weights element-by-element\n- Preserve biases unchanged\n- Create a new DenseLayer with merged parameters\n\nSince multiple LoRA adapter variants share this merge logic, consider extracting it to `LoRAAdapterBase<T>` to eliminate duplication and ensure consistency.\n\n\n\nExample refactoring approach:\n\nAdd to `LoRAAdapterBase<T>`:\n```csharp\nprotected virtual ILayer<T> MergeToDenseOrFullyConnected()\n{\n    DenseLayer<T>? denseBase = _baseLayer as DenseLayer<T>;\n    FullyConnectedLayer<T>? fcBase = _baseLayer as FullyConnectedLayer<T>;\n\n    if (denseBase == null && fcBase == null)\n    {\n        throw new InvalidOperationException($\"{GetType().Name} merging only supports DenseLayer or FullyConnectedLayer base layers\");\n    }\n\n    Matrix<T> loraWeights = _loraLayer.MergeWeights();\n    Vector<T> baseParams = _baseLayer.GetParameters();\n    \n    int inputSize = GetInputShape()[0];\n    int outputSize = GetOutputShape()[0];\n    int weightCount = inputSize * outputSize;\n    \n    Vector<T> mergedParams = new Vector<T>(baseParams.Length);\n    \n    for (int i = 0; i < weightCount; i++)\n    {\n        int row = i / inputSize;\n        int col = i % inputSize;\n        mergedParams[i] = NumOps.Add(baseParams[i], loraWeights[row, col]);\n    }\n    \n    for (int i = weightCount; i < baseParams.Length; i++)\n    {\n        mergedParams[i] = baseParams[i];\n    }\n    \n    DenseLayer<T> mergedLayer = new DenseLayer<T>(inputSize, outputSize, (IActivationFunction<T>?)null);\n    mergedLayer.SetParameters(mergedParams);\n    \n    return mergedLayer;\n}\n```\n\nThen simplify child classes:\n```csharp\npublic override ILayer<T> MergeToOriginalLayer()\n{\n    return MergeToDenseOrFullyConnected();\n}\n```\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/LoRA/Adapters/LoRAPlusAdapter.cs around lines 314 to 359, the\nMergeToOriginalLayer implementation duplicates the same Dense/FullyConnected\nmerge logic found in LoFAAdapter; extract that shared logic into a protected\nmethod on LoRAAdapterBase<T> (e.g. MergeToDenseOrFullyConnected) that performs\nthe Dense/FullyConnected type checks, calls _loraLayer.MergeWeights(), reads\nbase parameters, computes input/output sizes and weightCount, merges weights\nelement-wise while preserving biases, constructs a new DenseLayer with merged\nparameters and throws a clear InvalidOperationException using GetType().Name\nwhen unsupported; then replace the body of MergeToOriginalLayer in\nLoRAPlusAdapter (and LoFAAdapter) with a single call to the new base method to\nremove duplication and ensure consistency.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T03:02:29Z","updated_at":"2025-11-02T03:02:32Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140314","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140314"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140314"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140314/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":314,"original_start_line":314,"start_side":"RIGHT","line":359,"original_line":359,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":359,"position":359,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140316","pull_request_review_id":3408046172,"id":2484140316,"node_id":"PRRC_kwDOKSXUF86UEPkc","diff_hunk":"@@ -0,0 +1,391 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// LoRA+ adapter that uses optimized learning rates for faster convergence and better performance.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// LoRA+ (February 2024) improves upon standard LoRA by using different learning rates for the A and B matrices.\n+/// The key insight is that matrix B (which starts at zero) needs faster updates than matrix A (which starts random).\n+/// This simple modification leads to significantly faster convergence and improved final performance.\n+/// </para>\n+/// <para><b>For Beginners:</b> LoRA+ is an enhanced version of LoRA that trains faster and better.\n+///\n+/// In standard LoRA:\n+/// - Both matrix A and B are updated with the same learning rate\n+/// - Matrix B starts at zero, so it needs time to \"catch up\"\n+/// - Matrix A starts random, so it's already contributing from the start\n+///\n+/// LoRA+ recognizes this asymmetry:\n+/// - Matrix A is updated with a base learning rate (e.g., 0.0001)\n+/// - Matrix B is updated with a higher learning rate (e.g., 0.0016 = 16x higher)\n+/// - This accelerates learning without instability\n+///\n+/// Key parameters:\n+/// - BaseLearningRate: Learning rate for matrix A (the \"slow\" matrix)\n+/// - LearningRateRatio: Multiplier for matrix B (typically 16.0)\n+/// - ScaledLearningRate: Computed as BaseLearningRate * LearningRateRatio\n+///\n+/// Research shows LoRA+ typically achieves:\n+/// - 2x faster convergence\n+/// - Better final performance\n+/// - No additional parameters compared to standard LoRA\n+///\n+/// Example: If base learning rate is 0.0001 and ratio is 16.0:\n+/// - Matrix A updates with learning rate 0.0001\n+/// - Matrix B updates with learning rate 0.0016\n+///\n+/// Reference: LoRA+: Efficient Low Rank Adaptation of Large Models (February 2024)\n+/// </para>\n+/// </remarks>\n+public class LoRAPlusAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// The ratio of learning rates between matrix B and matrix A.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This ratio determines how much faster matrix B is updated compared to matrix A.\n+    /// Typical values range from 8.0 to 32.0, with 16.0 being the recommended default.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This controls how much faster the B matrix learns.\n+    /// A ratio of 16.0 means B learns 16x faster than A. Higher values mean even faster\n+    /// B updates, but too high can cause instability.\n+    /// </para>\n+    /// </remarks>\n+    private double _learningRateRatio;\n+\n+    /// <summary>\n+    /// The base learning rate applied to matrix A.\n+    /// </summary>\n+    /// <remarks>\n+    /// This is the slower learning rate applied to matrix A, which already has random\n+    /// initialization and contributes from the start of training.\n+    /// </remarks>\n+    private T _baseLearningRate;\n+\n+    /// <summary>\n+    /// The scaled learning rate applied to matrix B (BaseLearningRate * LearningRateRatio).\n+    /// </summary>\n+    /// <remarks>\n+    /// This is the faster learning rate applied to matrix B, which starts at zero\n+    /// and needs accelerated updates to catch up with matrix A.\n+    /// </remarks>\n+    private T _scaledLearningRate;\n+\n+    /// <summary>\n+    /// Gets or sets the learning rate ratio between matrix B and matrix A.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Default value is 16.0 as recommended by the LoRA+ paper. Valid range is typically 1.0 to 32.0.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is the multiplier that makes matrix B learn faster.\n+    /// - 1.0 = same speed as standard LoRA (no benefit)\n+    /// - 8.0 = moderate speedup\n+    /// - 16.0 = recommended default\n+    /// - 32.0 = aggressive speedup (may be unstable)\n+    /// </para>\n+    /// </remarks>\n+    public double LearningRateRatio\n+    {\n+        get => _learningRateRatio;\n+        set\n+        {\n+            if (value < 1.0)\n+            {\n+                throw new ArgumentException(\"Learning rate ratio must be at least 1.0\", nameof(value));\n+            }\n+            _learningRateRatio = value;\n+            UpdateScaledLearningRate();\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Gets the base learning rate for matrix A.\n+    /// </summary>\n+    public T BaseLearningRate => _baseLearningRate;\n+\n+    /// <summary>\n+    /// Gets the scaled learning rate for matrix B.\n+    /// </summary>\n+    public T ScaledLearningRate => _scaledLearningRate;\n+\n+    /// <summary>\n+    /// Initializes a new LoRA+ adapter with optimized dual learning rates.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with LoRA+.</param>\n+    /// <param name=\"rank\">The rank of the LoRA decomposition.</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"learningRateRatio\">The ratio of B's learning rate to A's learning rate (default: 16.0).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when learningRateRatio is less than 1.0.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This creates a LoRA+ adapter that will train faster than standard LoRA.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to efficiently fine-tune\n+    /// - rank: How much compression (lower = fewer parameters)\n+    /// - alpha: How strong the LoRA effect is\n+    /// - learningRateRatio: How much faster B learns than A (16.0 is recommended)\n+    /// - freezeBaseLayer: Whether to lock the original weights (usually true)\n+    ///\n+    /// The learning rate ratio is the key differentiator from standard LoRA. Higher ratios\n+    /// mean faster convergence but require careful tuning to avoid instability.\n+    /// </para>\n+    /// </remarks>\n+    public LoRAPlusAdapter(\n+        ILayer<T> baseLayer,\n+        int rank,\n+        double alpha = -1,\n+        double learningRateRatio = 16.0,\n+        bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        if (learningRateRatio < 1.0)\n+        {\n+            throw new ArgumentException(\"Learning rate ratio must be at least 1.0\", nameof(learningRateRatio));\n+        }\n+\n+        _learningRateRatio = learningRateRatio;\n+        _baseLearningRate = NumOps.Zero;\n+        _scaledLearningRate = NumOps.Zero;\n+    }\n+\n+    /// <summary>\n+    /// Sets the learning rates for this adapter.\n+    /// </summary>\n+    /// <param name=\"baseLearningRate\">The base learning rate for matrix A.</param>\n+    /// <remarks>\n+    /// <para>\n+    /// This method sets the base learning rate and automatically computes the scaled\n+    /// learning rate for matrix B using the current learning rate ratio.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Call this to configure how fast the adapter learns.\n+    /// You only need to provide the base learning rate - the higher learning rate for\n+    /// matrix B is calculated automatically using the ratio you specified.\n+    ///\n+    /// Example: If you call SetLearningRates(0.0001) with ratio 16.0:\n+    /// - Matrix A will use learning rate 0.0001\n+    /// - Matrix B will use learning rate 0.0016 (16x faster)\n+    /// </para>\n+    /// </remarks>\n+    public void SetLearningRates(T baseLearningRate)\n+    {\n+        _baseLearningRate = baseLearningRate;\n+        UpdateScaledLearningRate();\n+    }\n+\n+    /// <summary>\n+    /// Updates the scaled learning rate based on the current base learning rate and ratio.\n+    /// </summary>\n+    private void UpdateScaledLearningRate()\n+    {\n+        _scaledLearningRate = NumOps.Multiply(_baseLearningRate, NumOps.FromDouble(_learningRateRatio));\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass through both base and LoRA layers.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Sum of base layer output and LoRA output.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The forward pass is identical to standard LoRA: output = base_layer(input) + lora_layer(input).\n+    /// The dual learning rate optimization only affects the backward pass and parameter updates.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This works exactly like standard LoRA during the forward pass.\n+    /// The magic of LoRA+ happens during training (backward pass), not inference.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        // Forward pass is identical to base LoRA implementation\n+        return base.Forward(input);\n+    }\n+\n+    /// <summary>\n+    /// Performs the backward pass through both layers with dual learning rate scaling.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient flowing back from the next layer.</param>\n+    /// <returns>Gradient to pass to the previous layer.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The backward pass computes gradients for both matrices but applies different scaling\n+    /// factors to prepare for the dual learning rate update. Matrix B gradients are implicitly\n+    /// prepared for faster updates during the UpdateParameters call.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is where LoRA+ differs from standard LoRA!\n+    /// During backpropagation, we compute gradients for both A and B matrices, but we'll\n+    /// apply different learning rates when actually updating the parameters. This prepares\n+    /// the gradients for the dual learning rate optimization.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        // The base backward implementation computes gradients correctly\n+        // The dual learning rate is applied in UpdateParameters\n+        return base.Backward(outputGradient);\n+    }\n+\n+    /// <summary>\n+    /// Updates parameters using dual learning rates (base rate for A, scaled rate for B).\n+    /// </summary>\n+    /// <param name=\"learningRate\">This parameter is used as the base learning rate for matrix A.</param>\n+    /// <remarks>\n+    /// <para>\n+    /// This method overrides the standard LoRA parameter update to apply different learning rates:\n+    /// - Matrix A is updated with the base learning rate\n+    /// - Matrix B is updated with the scaled learning rate (base * ratio)\n+    /// - Base layer is updated with the base learning rate if not frozen\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is where the dual learning rate magic happens!\n+    /// Instead of updating both matrices at the same speed, we:\n+    /// 1. Update matrix A slowly (with the base learning rate)\n+    /// 2. Update matrix B quickly (with the scaled learning rate)\n+    ///\n+    /// This asymmetry accelerates training because:\n+    /// - Matrix A already has random values and is contributing\n+    /// - Matrix B starts at zero and needs to catch up\n+    /// - Giving B a higher learning rate helps it catch up faster\n+    ///\n+    /// The result is faster convergence and better final performance!\n+    /// </para>\n+    /// </remarks>\n+    public override void UpdateParameters(T learningRate)\n+    {\n+        // Store the base learning rate for matrix A\n+        SetLearningRates(learningRate);\n+\n+        // Get the LoRA layer's parameter gradients\n+        Vector<T> loraGrads = _loraLayer.GetParameterGradients();\n+\n+        // Calculate dimensions\n+        int matrixASize = _loraLayer.GetMatrixA().Rows * _loraLayer.GetMatrixA().Columns;\n+        int matrixBSize = _loraLayer.GetMatrixB().Rows * _loraLayer.GetMatrixB().Columns;\n+\n+        // Get current LoRA parameters\n+        Vector<T> loraParams = _loraLayer.GetParameters();\n+\n+        // Update matrix A with base learning rate\n+        for (int i = 0; i < matrixASize; i++)\n+        {\n+            T update = NumOps.Multiply(loraGrads[i], _baseLearningRate);\n+            loraParams[i] = NumOps.Subtract(loraParams[i], update);\n+        }\n+\n+        // Update matrix B with scaled learning rate (higher rate)\n+        for (int i = matrixASize; i < matrixASize + matrixBSize; i++)\n+        {\n+            T update = NumOps.Multiply(loraGrads[i], _scaledLearningRate);\n+            loraParams[i] = NumOps.Subtract(loraParams[i], update);\n+        }\n+\n+        // Apply updated parameters to LoRA layer\n+        _loraLayer.SetParameters(loraParams);\n+\n+        // Update base layer if not frozen (using base learning rate)\n+        if (!_freezeBaseLayer)\n+        {\n+            _baseLayer.UpdateParameters(_baseLearningRate);\n+        }\n+\n+        // Update the adapter's parameter vector\n+        UpdateParametersFromLayers();\n+    }\n+\n+    /// <summary>\n+    /// Merges the LoRA+ adaptation into the base layer and returns the merged layer.\n+    /// </summary>\n+    /// <returns>A new layer with LoRA weights merged into the base layer's weights.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// For LoRA+, merging works exactly like standard LoRA - the dual learning rates only\n+    /// affect training, not the final merged weights.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> After training with LoRA+, you can merge the weights just like\n+    /// standard LoRA. The faster training doesn't change the final result, it just gets you there quicker!\n+    /// </para>\n+    /// </remarks>\n+    public override ILayer<T> MergeToOriginalLayer()\n+    {\n+        // LoRA+ merging is identical to standard LoRA\n+        // For Dense layers, delegate to DenseLoRAAdapter logic\n+        DenseLayer<T>? denseBase = _baseLayer as DenseLayer<T>;\n+        FullyConnectedLayer<T>? fcBase = _baseLayer as FullyConnectedLayer<T>;\n+\n+        if (denseBase == null && fcBase == null)\n+        {\n+            throw new InvalidOperationException(\"LoRAPlusAdapter currently only supports DenseLayer or FullyConnectedLayer base layers\");\n+        }\n+\n+        // Get the LoRA weight contribution\n+        Matrix<T> loraWeights = _loraLayer.MergeWeights();\n+\n+        // Get base layer parameters\n+        Vector<T> baseParams = _baseLayer.GetParameters();\n+\n+        // Calculate dimensions\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        int weightCount = inputSize * outputSize;\n+\n+        // Create new parameters with merged weights\n+        Vector<T> mergedParams = new Vector<T>(baseParams.Length);\n+\n+        // Merge weights\n+        for (int i = 0; i < weightCount; i++)\n+        {\n+            int row = i / inputSize;\n+            int col = i % inputSize;\n+            mergedParams[i] = NumOps.Add(baseParams[i], loraWeights[row, col]);\n+        }\n+\n+        // Copy biases unchanged\n+        for (int i = weightCount; i < baseParams.Length; i++)\n+        {\n+            mergedParams[i] = baseParams[i];\n+        }\n+\n+        // Create a new dense layer with merged parameters\n+        DenseLayer<T> mergedLayer = new DenseLayer<T>(inputSize, outputSize, (IActivationFunction<T>?)null);\n+        mergedLayer.SetParameters(mergedParams);\n+\n+        return mergedLayer;\n+    }\n+\n+    /// <summary>\n+    /// Updates the parameter vector from the current layer states.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This private helper method synchronizes the adapter's parameter vector with the current state\n+    /// of the base and LoRA layers after updates.\n+    /// </para>\n+    /// </remarks>\n+    private void UpdateParametersFromLayers()\n+    {\n+        int idx = 0;\n+\n+        // If base layer is not frozen, pack its parameters first\n+        if (!_freezeBaseLayer)\n+        {\n+            Vector<T> baseParams = _baseLayer.GetParameters();\n+            for (int i = 0; i < baseParams.Length; i++)\n+            {\n+                Parameters[idx++] = baseParams[i];\n+            }\n+        }\n+\n+        // Pack LoRA parameters\n+        Vector<T> loraParams = _loraLayer.GetParameters();\n+        for (int i = 0; i < loraParams.Length; i++)\n+        {\n+            Parameters[idx++] = loraParams[i];\n+        }\n+    }","path":"src/LoRA/Adapters/LoRAPlusAdapter.cs","commit_id":"fa815036ee570162056ba6aca773f34c80f36948","original_commit_id":"8d5a1b6564c1a2bc1b334f1863e6003a6fd142b2","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_üõ†Ô∏è Refactor suggestion_ | _üü† Major_\n\n**Code duplication: identical to LoftQAdapter implementation.**\n\nThe `UpdateParametersFromLayers()` method is identical to the implementation in `LoftQAdapter.cs` (lines 718-736). Since this parameter synchronization logic is common across multiple LoRA adapter variants, it should be moved to `LoRAAdapterBase<T>` as a protected method.\n\n\n\nMove to base class:\n\nIn `LoRAAdapterBase<T>`:\n```csharp\nprotected void UpdateParametersFromLayers()\n{\n    int idx = 0;\n\n    if (!_freezeBaseLayer)\n    {\n        Vector<T> baseParams = _baseLayer.GetParameters();\n        for (int i = 0; i < baseParams.Length; i++)\n        {\n            Parameters[idx++] = baseParams[i];\n        }\n    }\n\n    Vector<T> loraParams = _loraLayer.GetParameters();\n    for (int i = 0; i < loraParams.Length; i++)\n    {\n        Parameters[idx++] = loraParams[i];\n    }\n}\n```\n\nThen remove the private implementation from child adapter classes.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/LoRA/Adapters/LoRAPlusAdapter.cs around lines 370 to 390, the\nUpdateParametersFromLayers() implementation is duplicated from LoftQAdapter;\nmove this shared logic into LoRAAdapterBase<T> as a protected method named\nUpdateParametersFromLayers() and delete the private copies in child adapters.\nImplement the method in LoRAAdapterBase<T> using the same body, ensure the base\nclass exposes or changes the visibility of any referenced members\n(_freezeBaseLayer, _baseLayer, _loraLayer, Parameters) to protected or provide\nprotected accessors so the method compiles, and then remove the private\nimplementations from LoRAPlusAdapter.cs and LoftQAdapter.cs (or any other\nadapters) so they call the base implementation instead.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T03:02:29Z","updated_at":"2025-11-02T03:02:32Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140316","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140316"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140316"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140316/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":370,"original_start_line":370,"start_side":"RIGHT","line":390,"original_line":390,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":390,"position":390,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140318","pull_request_review_id":3408046172,"id":2484140318,"node_id":"PRRC_kwDOKSXUF86UEPke","diff_hunk":"@@ -0,0 +1,928 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// LoRETTA (Low-Rank Economic Tensor-Train Adaptation) adapter for parameter-efficient fine-tuning.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// LoRETTA extends LoRA by using tensor-train decomposition instead of simple matrix factorization.\n+/// Instead of representing weight updates as W = A √ó B, LoRETTA uses a tensor-train decomposition\n+/// that captures higher-order correlations with even fewer parameters.\n+/// </para>\n+/// <para>\n+/// Tensor-train decomposition represents a high-dimensional tensor as a sequence of lower-dimensional\n+/// \"cores\" that are contracted together. For a weight matrix W of size (m √ó n), the tensor-train\n+/// representation is:\n+///\n+/// W[i,j] = G1[i] √ó G2 √ó G3 √ó ... √ó Gd[j]\n+///\n+/// where each core Gk has dimensions (r_{k-1} √ó n_k √ó r_k), and r_k are the TT-ranks.\n+/// The boundary ranks are r_0 = r_d = 1.\n+/// </para>\n+/// <para><b>For Beginners:</b> LoRETTA is an advanced version of LoRA that uses \"tensor-train decomposition\"!\n+///\n+/// Standard LoRA uses two matrices (A and B) to approximate weight changes:\n+/// - Matrix A: Compresses input to rank dimensions\n+/// - Matrix B: Expands back to output dimensions\n+/// - Parameters: inputSize √ó rank + rank √ó outputSize\n+///\n+/// LoRETTA uses multiple small \"cores\" chained together:\n+/// - Instead of 2 large matrices, use many small tensors\n+/// - Each core captures local correlations\n+/// - The cores are \"contracted\" (multiplied in sequence)\n+/// - Can express more complex patterns with fewer parameters\n+///\n+/// Why tensor-train decomposition?\n+/// 1. More expressive: Can capture higher-order correlations\n+/// 2. More efficient: Fewer parameters than matrix factorization\n+/// 3. Better compression: Exploits structure in weight updates\n+/// 4. Scalable: Grows logarithmically with dimensions\n+///\n+/// Example parameter counts for 1000√ó1000 layer:\n+/// - Full update: 1,000,000 parameters\n+/// - Standard LoRA (rank=8): 16,000 parameters (98.4% reduction)\n+/// - LoRETTA (rank=4, 3 cores): ~6,000 parameters (99.4% reduction, even better!)\n+///\n+/// Key parameters:\n+/// - ttRank: Controls compression (like LoRA's rank but more powerful)\n+/// - numCores: How many tensor cores in the chain (typically 3-5)\n+/// - alpha: Scaling factor for the adaptation strength\n+///\n+/// When to use LoRETTA:\n+/// - Maximum parameter efficiency needed\n+/// - Weight updates have higher-order structure\n+/// - You have very large layers to adapt\n+/// - Standard LoRA isn't expressive enough at low ranks\n+///\n+/// Reference:\n+/// Tensor-train decomposition: I. V. Oseledets, \"Tensor-train decomposition,\"\n+/// SIAM J. Scientific Computing, 2011.\n+/// </para>\n+/// </remarks>\n+public class LoRETTAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Tensor-train cores representing the weight decomposition.\n+    /// Core k has shape (ttRanks[k-1], coreShape[k], ttRanks[k]).\n+    /// </summary>\n+    private readonly List<Tensor<T>> _ttCores;\n+\n+    /// <summary>\n+    /// The ranks of the tensor-train decomposition.\n+    /// Length is numCores + 1, with ttRanks[0] = ttRanks[numCores] = 1.\n+    /// </summary>\n+    private readonly int[] _ttRanks;\n+\n+    /// <summary>\n+    /// The shape of each core in the tensor-train.\n+    /// </summary>\n+    private readonly int[] _coreShapes;\n+\n+    /// <summary>\n+    /// Number of cores in the tensor-train.\n+    /// </summary>\n+    private readonly int _numCores;\n+\n+    /// <summary>\n+    /// Gradients for each TT core computed during backpropagation.\n+    /// </summary>\n+    private List<Tensor<T>>? _ttCoreGradients;\n+\n+    /// <summary>\n+    /// Cached intermediate tensors from forward pass, needed for gradient computation.\n+    /// </summary>\n+    private List<Tensor<T>>? _forwardIntermediates;\n+\n+    /// <summary>\n+    /// Gets the tensor-train rank.\n+    /// </summary>\n+    /// <remarks>\n+    /// This is the maximum rank in the tensor-train decomposition. Lower rank means\n+    /// more compression but less expressiveness.\n+    /// </remarks>\n+    public int TTRank => _ttRanks.Max();\n+\n+    /// <summary>\n+    /// Gets the number of cores in the tensor-train.\n+    /// </summary>\n+    public int NumCores => _numCores;\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters in the tensor-train cores.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// The total parameters is the sum of all core sizes:\n+    /// sum_k (ttRanks[k-1] √ó coreShapes[k] √ó ttRanks[k])\n+    /// </para>\n+    /// <para>\n+    /// This is typically much smaller than standard LoRA for the same expressiveness.\n+    /// </para>\n+    /// </remarks>\n+    public override int ParameterCount\n+    {\n+        get\n+        {\n+            int ttParams = 0;\n+            for (int k = 0; k < _numCores; k++)\n+            {\n+                ttParams += _ttRanks[k] * _coreShapes[k] * _ttRanks[k + 1];\n+            }\n+\n+            // Add base layer parameters if not frozen\n+            if (!_freezeBaseLayer)\n+            {\n+                return _baseLayer.ParameterCount + ttParams;\n+            }\n+\n+            return ttParams;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Initializes a new LoRETTA adapter wrapping an existing layer.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with LoRETTA.</param>\n+    /// <param name=\"ttRank\">The rank of the tensor-train decomposition.</param>\n+    /// <param name=\"numCores\">Number of cores in the tensor-train (default: 3).</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to ttRank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when ttRank or numCores are invalid.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This creates a LoRETTA adapter that wraps any layer.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to adapt efficiently\n+    /// - ttRank: Controls compression (lower = fewer parameters, less flexibility)\n+    /// - numCores: How many tensor cores to use (more cores = more expressive but more params)\n+    /// - alpha: How strong the adaptation is\n+    /// - freezeBaseLayer: Whether to lock the original layer's weights (usually true)\n+    ///\n+    /// The cores are initialized carefully:\n+    /// - First and last cores connect to input/output dimensions\n+    /// - Middle cores have uniform shapes\n+    /// - All cores start with small random values (Gaussian initialization)\n+    /// - Designed so initial LoRETTA has minimal effect\n+    ///\n+    /// Recommended settings:\n+    /// - ttRank=4 to 8: Good balance of efficiency and expressiveness\n+    /// - numCores=3: Standard choice (input core, middle core, output core)\n+    /// - numCores=4-5: For very large layers or complex adaptations\n+    /// </para>\n+    /// </remarks>\n+    public LoRETTAAdapter(\n+        ILayer<T> baseLayer,\n+        int ttRank,\n+        int numCores = 3,\n+        double alpha = -1,\n+        bool freezeBaseLayer = true)\n+        : base(baseLayer, ttRank, alpha, freezeBaseLayer)\n+    {\n+        if (ttRank <= 0)\n+        {\n+            throw new ArgumentException(\"TT-rank must be positive\", nameof(ttRank));\n+        }\n+\n+        if (numCores < 2)\n+        {\n+            throw new ArgumentException(\"Number of cores must be at least 2\", nameof(numCores));\n+        }\n+\n+        _numCores = numCores;\n+\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+\n+        // Initialize TT-ranks: [1, ttRank, ttRank, ..., ttRank, 1]\n+        _ttRanks = new int[numCores + 1];\n+        _ttRanks[0] = 1;\n+        _ttRanks[numCores] = 1;\n+        for (int k = 1; k < numCores; k++)\n+        {\n+            _ttRanks[k] = ttRank;\n+        }\n+\n+        // Compute core shapes by factorizing input and output dimensions\n+        _coreShapes = ComputeCoreShapes(inputSize, outputSize, numCores);\n+\n+        // Initialize TT cores\n+        _ttCores = new List<Tensor<T>>(numCores);\n+        InitializeTTCores();\n+\n+        // Update parameter vector\n+        Parameters = new Vector<T>(ParameterCount);\n+        UpdateParametersFromCores();\n+    }\n+\n+    /// <summary>\n+    /// Computes the shape of each core by factorizing the total dimension.\n+    /// </summary>\n+    /// <param name=\"inputSize\">Input dimension.</param>\n+    /// <param name=\"outputSize\">Output dimension.</param>\n+    /// <param name=\"numCores\">Number of cores.</param>\n+    /// <returns>Array of core shapes.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// We need to factorize the total dimensionality (inputSize √ó outputSize) across the cores.\n+    /// The product of all core shapes should approximately equal inputSize √ó outputSize.\n+    ///\n+    /// Strategy: Use geometric decomposition\n+    /// - First core: ~inputSize^(1/2) √ó outputSize^(1/(numCores-1))\n+    /// - Last core: ~inputSize^(1/2) √ó outputSize^(1/(numCores-1))\n+    /// - Middle cores: uniform sizes based on geometric mean\n+    /// </para>\n+    /// </remarks>\n+    private int[] ComputeCoreShapes(int inputSize, int outputSize, int numCores)\n+    {\n+        int[] shapes = new int[numCores];\n+\n+        // Total \"logical\" dimension to decompose\n+        double totalDim = Math.Sqrt((double)inputSize * outputSize);\n+\n+        // Use geometric factorization\n+        double dimPerCore = Math.Pow(totalDim, 2.0 / numCores);\n+\n+        // Ensure each core has at least dimension 2\n+        int baseDim = Math.Max(2, (int)Math.Ceiling(dimPerCore));\n+\n+        // Distribute dimensions\n+        for (int k = 0; k < numCores; k++)\n+        {\n+            shapes[k] = baseDim;\n+        }\n+\n+        // Adjust first and last cores to better match input/output sizes\n+        shapes[0] = Math.Max(2, (int)Math.Ceiling(Math.Sqrt(inputSize)));\n+        shapes[numCores - 1] = Math.Max(2, (int)Math.Ceiling(Math.Sqrt(outputSize)));\n+\n+        return shapes;\n+    }\n+\n+    /// <summary>\n+    /// Initializes all TT cores with small random values.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Each core is initialized with Gaussian noise scaled by 1/sqrt(product of dimensions).\n+    /// This ensures the overall adaptation starts small.\n+    /// </para>\n+    /// </remarks>\n+    private void InitializeTTCores()\n+    {\n+        Random random = new Random(42);\n+\n+        for (int k = 0; k < _numCores; k++)\n+        {\n+            int leftRank = _ttRanks[k];\n+            int coreShape = _coreShapes[k];\n+            int rightRank = _ttRanks[k + 1];\n+\n+            // Core has shape [leftRank, coreShape, rightRank]\n+            int[] shape = new int[] { leftRank, coreShape, rightRank };\n+            Tensor<T> core = new Tensor<T>(shape);\n+\n+            // Initialize with small Gaussian noise\n+            double scale = 1.0 / Math.Sqrt(leftRank * coreShape * rightRank);\n+\n+            for (int i = 0; i < core.Length; i++)\n+            {\n+                // Box-Muller transform for Gaussian random numbers\n+                double u1 = random.NextDouble();\n+                double u2 = random.NextDouble();\n+                double randStdNormal = Math.Sqrt(-2.0 * Math.Log(u1)) * Math.Sin(2.0 * Math.PI * u2);\n+                core[i] = NumOps.Multiply(NumOps.FromDouble(randStdNormal), NumOps.FromDouble(scale));\n+            }\n+\n+            _ttCores.Add(core);\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass through the LoRETTA adapter.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Sum of base layer output and LoRETTA output.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The forward pass computes the tensor-train contraction to produce the adaptation,\n+    /// then adds it to the base layer output.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This processes input through both the original layer and\n+    /// the LoRETTA adaptation, then combines them.\n+    ///\n+    /// The LoRETTA forward pass:\n+    /// 1. Forward through base layer (original behavior)\n+    /// 2. Contract tensor-train cores with input (compute adaptation)\n+    /// 3. Add base output + adaptation output\n+    ///\n+    /// The tensor contraction is done sequentially through the cores, which is efficient\n+    /// even though it looks complex mathematically.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        // Store intermediates for backward pass\n+        _forwardIntermediates = new List<Tensor<T>>();\n+\n+        // Forward through base layer\n+        Tensor<T> baseOutput = _baseLayer.Forward(input);\n+\n+        // Compute LoRETTA adaptation via tensor-train contraction\n+        Tensor<T> ttOutput = ComputeTensorTrainForward(input);\n+\n+        // Sum the outputs\n+        Tensor<T> result = new Tensor<T>(baseOutput.Shape);\n+        for (int i = 0; i < baseOutput.Length; i++)\n+        {\n+            result[i] = NumOps.Add(baseOutput[i], ttOutput[i]);\n+        }\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Computes the forward pass through the tensor-train decomposition.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor of shape [batchSize, inputSize].</param>\n+    /// <returns>Output tensor of shape [batchSize, outputSize].</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This performs the tensor-train contraction:\n+    /// 1. Reshape input to match first core dimensions\n+    /// 2. Contract through each core sequentially\n+    /// 3. Reshape output to match expected output dimensions\n+    /// </para>\n+    /// </remarks>\n+    private Tensor<T> ComputeTensorTrainForward(Tensor<T> input)\n+    {\n+        int batchSize = input.Shape[0];\n+        int inputSize = input.Shape.Length > 1 ? input.Shape[1] : input.Length;\n+\n+        // Start with input reshaped to work with first core\n+        // For simplicity, we'll use a matrix-based contraction approach\n+\n+        // Flatten input to [batchSize √ó inputSize]\n+        Matrix<T> currentMatrix = new Matrix<T>(batchSize, inputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                currentMatrix[i, j] = input[i * inputSize + j];\n+            }\n+        }\n+\n+        // Contract through each core\n+        for (int k = 0; k < _numCores; k++)\n+        {\n+            currentMatrix = ContractWithCore(currentMatrix, _ttCores[k], k);\n+\n+            // Store intermediate for backward pass\n+            if (_forwardIntermediates != null)\n+            {\n+                _forwardIntermediates.Add(TensorFromMatrix(currentMatrix));\n+            }\n+        }\n+\n+        // Extract output\n+        int outputSize = GetOutputShape()[0];\n+        Vector<T> outputData = new Vector<T>(batchSize * outputSize);\n+\n+        int idx = 0;\n+        int currentCols = currentMatrix.Columns;\n+        int outputCols = Math.Min(outputSize, currentCols);\n+\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                if (j < outputCols && i < currentMatrix.Rows)\n+                {\n+                    outputData[idx] = currentMatrix[i, j % currentMatrix.Columns];\n+                }\n+                else\n+                {\n+                    outputData[idx] = NumOps.Zero;\n+                }\n+                idx++;\n+            }\n+        }","path":"src/LoRA/Adapters/LoRETTAAdapter.cs","commit_id":"fa815036ee570162056ba6aca773f34c80f36948","original_commit_id":"8d5a1b6564c1a2bc1b334f1863e6003a6fd142b2","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n<details>\n<summary>üß© Analysis chain</summary>\n\n**Verify correctness of output extraction logic.**\n\nThe output extraction uses modulo operations (`j % currentMatrix.Columns`) to handle potential dimension mismatches, which suggests the tensor-train contraction may not produce the expected output dimensions. This could lead to incorrect outputs or silently reusing values.\n\n\n\n\nThe logic at lines 402-404:\n```csharp\nif (j < outputCols && i < currentMatrix.Rows)\n{\n    outputData[idx] = currentMatrix[i, j % currentMatrix.Columns];\n}\n```\n\nThe modulo operation suggests `j` might exceed `currentMatrix.Columns`, which would incorrectly wrap and reuse earlier column values. This needs verification:\n\n1. Ensure `ContractWithCore` produces output matrices with the correct dimensions\n2. Verify that the final contracted matrix dimensions match `[batchSize, outputSize]`\n3. Remove the modulo operation if dimensions are guaranteed to match\n\nConsider adding assertions to validate dimension consistency:\n\n```diff\n int currentCols = currentMatrix.Columns;\n int outputCols = Math.Min(outputSize, currentCols);\n \n+if (currentCols != outputSize)\n+{\n+    throw new InvalidOperationException(\n+        $\"Tensor-train contraction produced unexpected dimensions: expected {outputSize}, got {currentCols}\");\n+}\n+\n for (int i = 0; i < batchSize; i++)\n {\n     for (int j = 0; j < outputSize; j++)\n     {\n-        if (j < outputCols && i < currentMatrix.Rows)\n-        {\n-            outputData[idx] = currentMatrix[i, j % currentMatrix.Columns];\n-        }\n+        outputData[idx] = currentMatrix[i, j];\n-        else\n-        {\n-            outputData[idx] = NumOps.Zero;\n-        }\n         idx++;\n     }\n }\n```\n\n---\n\nI need to verify the review comment by examining the actual codebase. Let me gather information about the tensor-train contraction logic and dimension handling.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n\n---\n\nüèÅ Script executed:\n\n```shell\n#!/bin/bash\n# Find and examine LoRETTAAdapter.cs file\nfind . -name \"LoRETTAAdapter.cs\" -type f 2>/dev/null | head -5\n```\n\nLength of output: 37\n\n---\n\nüèÅ Script executed:\n\n```shell\n#!/bin/bash\n# Search for the ContractWithCore method and surrounding context\nrg -n \"ContractWithCore\" -A 20 -B 5 --type=cs\n```\n\nLength of output: 3995\n\n---\n\nüèÅ Script executed:\n\n```shell\n#!/bin/bash\n# Search for GetOutputShape method to understand expected dimensions\nrg -n \"GetOutputShape\" -A 5 -B 2 --type=cs\n```\n\nLength of output: 98497\n\n---\n\nüèÅ Script executed:\n\n```shell\n#!/bin/bash\n# Look for the ComputeTensorTrainForward method to see full flow\nrg -n \"ComputeTensorTrainForward\" -A 30 -B 5 --type=cs\n```\n\nLength of output: 4544\n\n---\n\nüèÅ Script executed:\n\n```shell\n#!/bin/bash\n# Search for tests related to LoRETTAAdapter to see expected behavior\nfind . -name \"*LoRETTA*Test*\" -o -name \"*LoRETTA*test*\" | head -10\n```\n\nLength of output: 0\n\n---\n\nüèÅ Script executed:\n\n```shell\n#!/bin/bash\n# Check for any dimension validation or assertions in the contraction logic\nrg -n \"outputSize|currentMatrix.Columns|batchSize\" -B 2 -A 2 --type=cs src/LoRA/Adapters/LoRETTAAdapter.cs\n```\n\nLength of output: 6600\n\n---\n\n```\n\n---\n\nüèÅ Script executed:\n\n```shell\n#!/bin/bash\n# Look for LoRETTA-specific tests\nfind . -type f \\( -name \"*test*\" -o -name \"*Test*\" \\) -exec grep -l \"LoRETTA\" {} \\; 2>/dev/null\n```\n\nLength of output: 0\n\n---\n\nüèÅ Script executed:\n\n```shell\n#!/bin/bash\n# Check the exact formula: does coreShape[numCores-1] ever equal outputSize?\nrg -n \"coreShape.*outputSize|Math.Sqrt|Ceiling.*outputSize\" -B 3 -A 3 --type=cs src/LoRA/Adapters/LoRETTAAdapter.cs\n```\n\nLength of output: 1567\n\n---\n\nüèÅ Script executed:\n\n```shell\n#!/bin/bash\n# Extract complete ComputeCoreShapes to understand dimension logic\nrg -n \"private int\\[\\] ComputeCoreShapes\" -A 25 --type=cs src/LoRA/Adapters/LoRETTAAdapter.cs\n```\n\nLength of output: 972\n\n---\n\nüèÅ Script executed:\n\n```shell\n#!/bin/bash\n# Search for any comments/documentation about modulo wrapping or dimension handling\nrg -n \"modulo|wrap|dimension.*mismatch|simplified\" -B 2 -A 2 --type=cs src/LoRA/Adapters/LoRETTAAdapter.cs\n```\n\nLength of output: 916\n\n---\n\n\n\n</details>\n\n**The output extraction logic produces incorrect results through silent data reuse.**\n\nThe dimension mismatch identified in the review is mathematically confirmed. The final contracted matrix has ~‚àöoutputSize columns (from `shapes[numCores-1] = Math.Ceiling(Math.Sqrt(outputSize))`), but the output loop attempts to extract outputSize values. The modulo operation silently wraps indices, reusing columns multiple times.\n\n**Example**: With outputSize=100, the final matrix has ~10 columns. When extracting columns 0‚Äì99, the loop reuses columns 0‚Äì9 ten times each, producing silently incorrect outputs.\n\nThe proposed fix is correct and necessary:\n\n- Add dimension validation to catch this mismatch explicitly\n- Remove the modulo wrapping (which masks the bug)\n- Either fix the core shape calculation or reshape the final matrix to match expected dimensions\n\nLines 402‚Äì410 require modification as suggested in the review.\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T03:02:29Z","updated_at":"2025-11-02T03:02:32Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140318","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140318"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140318"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140318/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":390,"original_start_line":390,"start_side":"RIGHT","line":412,"original_line":412,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":412,"position":412,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140320","pull_request_review_id":3408046172,"id":2484140320,"node_id":"PRRC_kwDOKSXUF86UEPkg","diff_hunk":"@@ -0,0 +1,928 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// LoRETTA (Low-Rank Economic Tensor-Train Adaptation) adapter for parameter-efficient fine-tuning.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// LoRETTA extends LoRA by using tensor-train decomposition instead of simple matrix factorization.\n+/// Instead of representing weight updates as W = A √ó B, LoRETTA uses a tensor-train decomposition\n+/// that captures higher-order correlations with even fewer parameters.\n+/// </para>\n+/// <para>\n+/// Tensor-train decomposition represents a high-dimensional tensor as a sequence of lower-dimensional\n+/// \"cores\" that are contracted together. For a weight matrix W of size (m √ó n), the tensor-train\n+/// representation is:\n+///\n+/// W[i,j] = G1[i] √ó G2 √ó G3 √ó ... √ó Gd[j]\n+///\n+/// where each core Gk has dimensions (r_{k-1} √ó n_k √ó r_k), and r_k are the TT-ranks.\n+/// The boundary ranks are r_0 = r_d = 1.\n+/// </para>\n+/// <para><b>For Beginners:</b> LoRETTA is an advanced version of LoRA that uses \"tensor-train decomposition\"!\n+///\n+/// Standard LoRA uses two matrices (A and B) to approximate weight changes:\n+/// - Matrix A: Compresses input to rank dimensions\n+/// - Matrix B: Expands back to output dimensions\n+/// - Parameters: inputSize √ó rank + rank √ó outputSize\n+///\n+/// LoRETTA uses multiple small \"cores\" chained together:\n+/// - Instead of 2 large matrices, use many small tensors\n+/// - Each core captures local correlations\n+/// - The cores are \"contracted\" (multiplied in sequence)\n+/// - Can express more complex patterns with fewer parameters\n+///\n+/// Why tensor-train decomposition?\n+/// 1. More expressive: Can capture higher-order correlations\n+/// 2. More efficient: Fewer parameters than matrix factorization\n+/// 3. Better compression: Exploits structure in weight updates\n+/// 4. Scalable: Grows logarithmically with dimensions\n+///\n+/// Example parameter counts for 1000√ó1000 layer:\n+/// - Full update: 1,000,000 parameters\n+/// - Standard LoRA (rank=8): 16,000 parameters (98.4% reduction)\n+/// - LoRETTA (rank=4, 3 cores): ~6,000 parameters (99.4% reduction, even better!)\n+///\n+/// Key parameters:\n+/// - ttRank: Controls compression (like LoRA's rank but more powerful)\n+/// - numCores: How many tensor cores in the chain (typically 3-5)\n+/// - alpha: Scaling factor for the adaptation strength\n+///\n+/// When to use LoRETTA:\n+/// - Maximum parameter efficiency needed\n+/// - Weight updates have higher-order structure\n+/// - You have very large layers to adapt\n+/// - Standard LoRA isn't expressive enough at low ranks\n+///\n+/// Reference:\n+/// Tensor-train decomposition: I. V. Oseledets, \"Tensor-train decomposition,\"\n+/// SIAM J. Scientific Computing, 2011.\n+/// </para>\n+/// </remarks>\n+public class LoRETTAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Tensor-train cores representing the weight decomposition.\n+    /// Core k has shape (ttRanks[k-1], coreShape[k], ttRanks[k]).\n+    /// </summary>\n+    private readonly List<Tensor<T>> _ttCores;\n+\n+    /// <summary>\n+    /// The ranks of the tensor-train decomposition.\n+    /// Length is numCores + 1, with ttRanks[0] = ttRanks[numCores] = 1.\n+    /// </summary>\n+    private readonly int[] _ttRanks;\n+\n+    /// <summary>\n+    /// The shape of each core in the tensor-train.\n+    /// </summary>\n+    private readonly int[] _coreShapes;\n+\n+    /// <summary>\n+    /// Number of cores in the tensor-train.\n+    /// </summary>\n+    private readonly int _numCores;\n+\n+    /// <summary>\n+    /// Gradients for each TT core computed during backpropagation.\n+    /// </summary>\n+    private List<Tensor<T>>? _ttCoreGradients;\n+\n+    /// <summary>\n+    /// Cached intermediate tensors from forward pass, needed for gradient computation.\n+    /// </summary>\n+    private List<Tensor<T>>? _forwardIntermediates;\n+\n+    /// <summary>\n+    /// Gets the tensor-train rank.\n+    /// </summary>\n+    /// <remarks>\n+    /// This is the maximum rank in the tensor-train decomposition. Lower rank means\n+    /// more compression but less expressiveness.\n+    /// </remarks>\n+    public int TTRank => _ttRanks.Max();\n+\n+    /// <summary>\n+    /// Gets the number of cores in the tensor-train.\n+    /// </summary>\n+    public int NumCores => _numCores;\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters in the tensor-train cores.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// The total parameters is the sum of all core sizes:\n+    /// sum_k (ttRanks[k-1] √ó coreShapes[k] √ó ttRanks[k])\n+    /// </para>\n+    /// <para>\n+    /// This is typically much smaller than standard LoRA for the same expressiveness.\n+    /// </para>\n+    /// </remarks>\n+    public override int ParameterCount\n+    {\n+        get\n+        {\n+            int ttParams = 0;\n+            for (int k = 0; k < _numCores; k++)\n+            {\n+                ttParams += _ttRanks[k] * _coreShapes[k] * _ttRanks[k + 1];\n+            }\n+\n+            // Add base layer parameters if not frozen\n+            if (!_freezeBaseLayer)\n+            {\n+                return _baseLayer.ParameterCount + ttParams;\n+            }\n+\n+            return ttParams;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Initializes a new LoRETTA adapter wrapping an existing layer.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with LoRETTA.</param>\n+    /// <param name=\"ttRank\">The rank of the tensor-train decomposition.</param>\n+    /// <param name=\"numCores\">Number of cores in the tensor-train (default: 3).</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to ttRank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when ttRank or numCores are invalid.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This creates a LoRETTA adapter that wraps any layer.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to adapt efficiently\n+    /// - ttRank: Controls compression (lower = fewer parameters, less flexibility)\n+    /// - numCores: How many tensor cores to use (more cores = more expressive but more params)\n+    /// - alpha: How strong the adaptation is\n+    /// - freezeBaseLayer: Whether to lock the original layer's weights (usually true)\n+    ///\n+    /// The cores are initialized carefully:\n+    /// - First and last cores connect to input/output dimensions\n+    /// - Middle cores have uniform shapes\n+    /// - All cores start with small random values (Gaussian initialization)\n+    /// - Designed so initial LoRETTA has minimal effect\n+    ///\n+    /// Recommended settings:\n+    /// - ttRank=4 to 8: Good balance of efficiency and expressiveness\n+    /// - numCores=3: Standard choice (input core, middle core, output core)\n+    /// - numCores=4-5: For very large layers or complex adaptations\n+    /// </para>\n+    /// </remarks>\n+    public LoRETTAAdapter(\n+        ILayer<T> baseLayer,\n+        int ttRank,\n+        int numCores = 3,\n+        double alpha = -1,\n+        bool freezeBaseLayer = true)\n+        : base(baseLayer, ttRank, alpha, freezeBaseLayer)\n+    {\n+        if (ttRank <= 0)\n+        {\n+            throw new ArgumentException(\"TT-rank must be positive\", nameof(ttRank));\n+        }\n+\n+        if (numCores < 2)\n+        {\n+            throw new ArgumentException(\"Number of cores must be at least 2\", nameof(numCores));\n+        }\n+\n+        _numCores = numCores;\n+\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+\n+        // Initialize TT-ranks: [1, ttRank, ttRank, ..., ttRank, 1]\n+        _ttRanks = new int[numCores + 1];\n+        _ttRanks[0] = 1;\n+        _ttRanks[numCores] = 1;\n+        for (int k = 1; k < numCores; k++)\n+        {\n+            _ttRanks[k] = ttRank;\n+        }\n+\n+        // Compute core shapes by factorizing input and output dimensions\n+        _coreShapes = ComputeCoreShapes(inputSize, outputSize, numCores);\n+\n+        // Initialize TT cores\n+        _ttCores = new List<Tensor<T>>(numCores);\n+        InitializeTTCores();\n+\n+        // Update parameter vector\n+        Parameters = new Vector<T>(ParameterCount);\n+        UpdateParametersFromCores();\n+    }\n+\n+    /// <summary>\n+    /// Computes the shape of each core by factorizing the total dimension.\n+    /// </summary>\n+    /// <param name=\"inputSize\">Input dimension.</param>\n+    /// <param name=\"outputSize\">Output dimension.</param>\n+    /// <param name=\"numCores\">Number of cores.</param>\n+    /// <returns>Array of core shapes.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// We need to factorize the total dimensionality (inputSize √ó outputSize) across the cores.\n+    /// The product of all core shapes should approximately equal inputSize √ó outputSize.\n+    ///\n+    /// Strategy: Use geometric decomposition\n+    /// - First core: ~inputSize^(1/2) √ó outputSize^(1/(numCores-1))\n+    /// - Last core: ~inputSize^(1/2) √ó outputSize^(1/(numCores-1))\n+    /// - Middle cores: uniform sizes based on geometric mean\n+    /// </para>\n+    /// </remarks>\n+    private int[] ComputeCoreShapes(int inputSize, int outputSize, int numCores)\n+    {\n+        int[] shapes = new int[numCores];\n+\n+        // Total \"logical\" dimension to decompose\n+        double totalDim = Math.Sqrt((double)inputSize * outputSize);\n+\n+        // Use geometric factorization\n+        double dimPerCore = Math.Pow(totalDim, 2.0 / numCores);\n+\n+        // Ensure each core has at least dimension 2\n+        int baseDim = Math.Max(2, (int)Math.Ceiling(dimPerCore));\n+\n+        // Distribute dimensions\n+        for (int k = 0; k < numCores; k++)\n+        {\n+            shapes[k] = baseDim;\n+        }\n+\n+        // Adjust first and last cores to better match input/output sizes\n+        shapes[0] = Math.Max(2, (int)Math.Ceiling(Math.Sqrt(inputSize)));\n+        shapes[numCores - 1] = Math.Max(2, (int)Math.Ceiling(Math.Sqrt(outputSize)));\n+\n+        return shapes;\n+    }\n+\n+    /// <summary>\n+    /// Initializes all TT cores with small random values.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Each core is initialized with Gaussian noise scaled by 1/sqrt(product of dimensions).\n+    /// This ensures the overall adaptation starts small.\n+    /// </para>\n+    /// </remarks>\n+    private void InitializeTTCores()\n+    {\n+        Random random = new Random(42);\n+\n+        for (int k = 0; k < _numCores; k++)\n+        {\n+            int leftRank = _ttRanks[k];\n+            int coreShape = _coreShapes[k];\n+            int rightRank = _ttRanks[k + 1];\n+\n+            // Core has shape [leftRank, coreShape, rightRank]\n+            int[] shape = new int[] { leftRank, coreShape, rightRank };\n+            Tensor<T> core = new Tensor<T>(shape);\n+\n+            // Initialize with small Gaussian noise\n+            double scale = 1.0 / Math.Sqrt(leftRank * coreShape * rightRank);\n+\n+            for (int i = 0; i < core.Length; i++)\n+            {\n+                // Box-Muller transform for Gaussian random numbers\n+                double u1 = random.NextDouble();\n+                double u2 = random.NextDouble();\n+                double randStdNormal = Math.Sqrt(-2.0 * Math.Log(u1)) * Math.Sin(2.0 * Math.PI * u2);\n+                core[i] = NumOps.Multiply(NumOps.FromDouble(randStdNormal), NumOps.FromDouble(scale));\n+            }\n+\n+            _ttCores.Add(core);\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass through the LoRETTA adapter.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Sum of base layer output and LoRETTA output.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The forward pass computes the tensor-train contraction to produce the adaptation,\n+    /// then adds it to the base layer output.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This processes input through both the original layer and\n+    /// the LoRETTA adaptation, then combines them.\n+    ///\n+    /// The LoRETTA forward pass:\n+    /// 1. Forward through base layer (original behavior)\n+    /// 2. Contract tensor-train cores with input (compute adaptation)\n+    /// 3. Add base output + adaptation output\n+    ///\n+    /// The tensor contraction is done sequentially through the cores, which is efficient\n+    /// even though it looks complex mathematically.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        // Store intermediates for backward pass\n+        _forwardIntermediates = new List<Tensor<T>>();\n+\n+        // Forward through base layer\n+        Tensor<T> baseOutput = _baseLayer.Forward(input);\n+\n+        // Compute LoRETTA adaptation via tensor-train contraction\n+        Tensor<T> ttOutput = ComputeTensorTrainForward(input);\n+\n+        // Sum the outputs\n+        Tensor<T> result = new Tensor<T>(baseOutput.Shape);\n+        for (int i = 0; i < baseOutput.Length; i++)\n+        {\n+            result[i] = NumOps.Add(baseOutput[i], ttOutput[i]);\n+        }\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Computes the forward pass through the tensor-train decomposition.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor of shape [batchSize, inputSize].</param>\n+    /// <returns>Output tensor of shape [batchSize, outputSize].</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This performs the tensor-train contraction:\n+    /// 1. Reshape input to match first core dimensions\n+    /// 2. Contract through each core sequentially\n+    /// 3. Reshape output to match expected output dimensions\n+    /// </para>\n+    /// </remarks>\n+    private Tensor<T> ComputeTensorTrainForward(Tensor<T> input)\n+    {\n+        int batchSize = input.Shape[0];\n+        int inputSize = input.Shape.Length > 1 ? input.Shape[1] : input.Length;\n+\n+        // Start with input reshaped to work with first core\n+        // For simplicity, we'll use a matrix-based contraction approach\n+\n+        // Flatten input to [batchSize √ó inputSize]\n+        Matrix<T> currentMatrix = new Matrix<T>(batchSize, inputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                currentMatrix[i, j] = input[i * inputSize + j];\n+            }\n+        }\n+\n+        // Contract through each core\n+        for (int k = 0; k < _numCores; k++)\n+        {\n+            currentMatrix = ContractWithCore(currentMatrix, _ttCores[k], k);\n+\n+            // Store intermediate for backward pass\n+            if (_forwardIntermediates != null)\n+            {\n+                _forwardIntermediates.Add(TensorFromMatrix(currentMatrix));\n+            }\n+        }\n+\n+        // Extract output\n+        int outputSize = GetOutputShape()[0];\n+        Vector<T> outputData = new Vector<T>(batchSize * outputSize);\n+\n+        int idx = 0;\n+        int currentCols = currentMatrix.Columns;\n+        int outputCols = Math.Min(outputSize, currentCols);\n+\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                if (j < outputCols && i < currentMatrix.Rows)\n+                {\n+                    outputData[idx] = currentMatrix[i, j % currentMatrix.Columns];\n+                }\n+                else\n+                {\n+                    outputData[idx] = NumOps.Zero;\n+                }\n+                idx++;\n+            }\n+        }\n+\n+        // Apply scaling (alpha / rank)\n+        T scaling = NumOps.Divide(\n+            NumOps.FromDouble(Alpha),\n+            NumOps.FromDouble(TTRank)\n+        );\n+\n+        for (int i = 0; i < outputData.Length; i++)\n+        {\n+            outputData[i] = NumOps.Multiply(outputData[i], scaling);\n+        }\n+\n+        return new Tensor<T>(new[] { batchSize, outputSize }, outputData);\n+    }\n+\n+    /// <summary>\n+    /// Contracts a matrix with a tensor-train core.\n+    /// </summary>\n+    /// <param name=\"input\">Input matrix [batchSize, currentDim].</param>\n+    /// <param name=\"core\">TT core tensor [leftRank, coreShape, rightRank].</param>\n+    /// <param name=\"coreIndex\">Index of the core being processed.</param>\n+    /// <returns>Output matrix [batchSize, nextDim].</returns>\n+    private Matrix<T> ContractWithCore(Matrix<T> input, Tensor<T> core, int coreIndex)\n+    {\n+        int batchSize = input.Rows;\n+        int leftRank = _ttRanks[coreIndex];\n+        int coreShape = _coreShapes[coreIndex];\n+        int rightRank = _ttRanks[coreIndex + 1];\n+\n+        // Simplified contraction: treat core as a sequence of matrices\n+        // Core shape: [leftRank, coreShape, rightRank]\n+        // We'll contract by reshaping and matrix multiplication\n+\n+        int inputDim = input.Columns;\n+        int outputDim = coreShape * rightRank;\n+\n+        Matrix<T> output = new Matrix<T>(batchSize, outputDim);\n+\n+        // For each batch element\n+        for (int b = 0; b < batchSize; b++)\n+        {\n+            // Contract input with core\n+            // Simplified: use first 'leftRank' dimensions of input\n+            for (int r = 0; r < rightRank; r++)\n+            {\n+                for (int c = 0; c < coreShape; c++)\n+                {\n+                    T sum = NumOps.Zero;\n+\n+                    for (int l = 0; l < leftRank && l < inputDim; l++)\n+                    {\n+                        int coreIdx = (l * coreShape * rightRank) + (c * rightRank) + r;\n+                        if (coreIdx < core.Length)\n+                        {\n+                            T inputVal = input[b, l];\n+                            T coreVal = core[coreIdx];\n+                            sum = NumOps.Add(sum, NumOps.Multiply(inputVal, coreVal));\n+                        }\n+                    }\n+\n+                    int outIdx = c * rightRank + r;\n+                    if (outIdx < outputDim)\n+                    {\n+                        output[b, outIdx] = sum;\n+                    }\n+                }\n+            }\n+        }\n+\n+        return output;\n+    }\n+\n+    /// <summary>\n+    /// Converts a matrix to a tensor.\n+    /// </summary>\n+    private Tensor<T> TensorFromMatrix(Matrix<T> matrix)\n+    {\n+        Vector<T> data = new Vector<T>(matrix.Rows * matrix.Columns);\n+        int idx = 0;\n+        for (int i = 0; i < matrix.Rows; i++)\n+        {\n+            for (int j = 0; j < matrix.Columns; j++)\n+            {\n+                data[idx++] = matrix[i, j];\n+            }\n+        }\n+        return new Tensor<T>(new[] { matrix.Rows, matrix.Columns }, data);\n+    }\n+\n+    /// <summary>\n+    /// Performs the backward pass through the LoRETTA adapter.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient flowing back from the next layer.</param>\n+    /// <returns>Gradient to pass to the previous layer.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The backward pass computes gradients for all TT cores and propagates gradients\n+    /// back through the tensor-train contraction.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is where learning happens for LoRETTA!\n+    ///\n+    /// The backward pass:\n+    /// 1. Backpropagate through base layer\n+    /// 2. Backpropagate through tensor-train cores\n+    /// 3. Compute gradients for each core\n+    /// 4. Combine input gradients from both paths\n+    ///\n+    /// This is more complex than standard LoRA because we need to backpropagate through\n+    /// multiple cores, but the principle is the same: figure out how each parameter\n+    /// contributed to the error.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        // Backward through base layer\n+        Tensor<T> baseInputGrad = _baseLayer.Backward(outputGradient);\n+\n+        // Backward through tensor-train\n+        Tensor<T> ttInputGrad = ComputeTensorTrainBackward(outputGradient);\n+\n+        // Sum input gradients\n+        Tensor<T> inputGrad = new Tensor<T>(baseInputGrad.Shape);\n+        for (int i = 0; i < baseInputGrad.Length; i++)\n+        {\n+            inputGrad[i] = NumOps.Add(baseInputGrad[i], ttInputGrad[i]);\n+        }\n+\n+        // Update parameter gradients vector\n+        UpdateParameterGradientsFromCores();\n+\n+        return inputGrad;\n+    }\n+\n+    /// <summary>\n+    /// Computes the backward pass through the tensor-train decomposition.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient from the output.</param>\n+    /// <returns>Gradient with respect to input.</returns>\n+    private Tensor<T> ComputeTensorTrainBackward(Tensor<T> outputGradient)\n+    {\n+        // Initialize core gradients\n+        _ttCoreGradients = new List<Tensor<T>>();\n+        for (int k = 0; k < _numCores; k++)\n+        {\n+            _ttCoreGradients.Add(new Tensor<T>(_ttCores[k].Shape));\n+        }\n+\n+        // Simplified backward: compute gradients using finite differences approximation\n+        // For production, would implement proper backpropagation through tensor contractions\n+\n+        int batchSize = outputGradient.Shape[0];\n+        int inputSize = GetInputShape()[0];\n+\n+        // Create zero gradient for input\n+        Tensor<T> inputGradient = new Tensor<T>(new[] { batchSize, inputSize });\n+\n+        // For each core, compute gradient (simplified using the chain rule)\n+        for (int k = 0; k < _numCores; k++)\n+        {\n+            // Gradient computation would use stored intermediates\n+            // For now, initialize with small values\n+            for (int i = 0; i < _ttCoreGradients[k].Length; i++)\n+            {\n+                _ttCoreGradients[k][i] = NumOps.Multiply(\n+                    outputGradient[i % outputGradient.Length],\n+                    NumOps.FromDouble(0.01)\n+                );\n+            }\n+        }\n+\n+        return inputGradient;\n+    }","path":"src/LoRA/Adapters/LoRETTAAdapter.cs","commit_id":"fa815036ee570162056ba6aca773f34c80f36948","original_commit_id":"8d5a1b6564c1a2bc1b334f1863e6003a6fd142b2","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Critical: Backward pass is not properly implemented.**\n\nThe `ComputeTensorTrainBackward` method uses a placeholder implementation that will prevent proper training. The gradients are computed using arbitrary values (`0.01` scaling) and the input gradient is returned as zeros, which means:\n\n1. TT cores will not receive correct gradients and won't learn properly\n2. Input gradients are zeros, breaking gradient flow to earlier layers\n3. The adapter cannot be trained effectively\n\n\n\nThe current implementation explicitly notes this limitation:\n```csharp\n// Simplified backward: compute gradients using finite differences approximation\n// For production, would implement proper backpropagation through tensor contractions\n```\n\nThis needs a proper implementation that:\n1. Backpropagates through the tensor-train contraction using the chain rule\n2. Uses stored `_forwardIntermediates` to compute correct gradients for each core\n3. Returns correct input gradients by backpropagating through the contraction sequence\n\nReference implementations like LoHaAdapter and LoKrAdapter (shown in code snippets) demonstrate proper gradient computation patterns. The tensor-train backward pass should follow similar principles but account for the multi-core sequential contraction.\n\n**This is a blocking issue** that prevents the adapter from being production-ready. The implementation should either:\n- Be completed with proper tensor-train backpropagation, or\n- Be marked as experimental/WIP with clear warnings in the documentation\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/LoRA/Adapters/LoRETTAAdapter.cs around lines 551 to 584, the\nComputeTensorTrainBackward method is a placeholder that assigns arbitrary small\nvalues to _ttCoreGradients and returns a zero input gradient; replace this with\na proper backprop through the tensor-train contraction: use the saved\n_forwardIntermediates for each contraction step, iterate cores in reverse order\napplying the chain rule to compute each core's gradient by contracting the\nupstream gradient with the corresponding left/right intermediates (matching the\npattern used in LoHaAdapter/LoKrAdapter), accumulate gradients with correct\nshape/batch handling via NumOps operations, and compute and return the true\ninputGradient by propagating gradients back through the first contraction(s); if\nyou cannot implement full backprop now, mark the adapter as experimental/WIP and\nadd a clear runtime warning and documentation note instead.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T03:02:29Z","updated_at":"2025-11-02T03:02:32Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140320","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140320"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140320"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140320/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":551,"original_start_line":551,"start_side":"RIGHT","line":584,"original_line":584,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":584,"position":584,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140323","pull_request_review_id":3408046172,"id":2484140323,"node_id":"PRRC_kwDOKSXUF86UEPkj","diff_hunk":"@@ -0,0 +1,928 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// LoRETTA (Low-Rank Economic Tensor-Train Adaptation) adapter for parameter-efficient fine-tuning.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// LoRETTA extends LoRA by using tensor-train decomposition instead of simple matrix factorization.\n+/// Instead of representing weight updates as W = A √ó B, LoRETTA uses a tensor-train decomposition\n+/// that captures higher-order correlations with even fewer parameters.\n+/// </para>\n+/// <para>\n+/// Tensor-train decomposition represents a high-dimensional tensor as a sequence of lower-dimensional\n+/// \"cores\" that are contracted together. For a weight matrix W of size (m √ó n), the tensor-train\n+/// representation is:\n+///\n+/// W[i,j] = G1[i] √ó G2 √ó G3 √ó ... √ó Gd[j]\n+///\n+/// where each core Gk has dimensions (r_{k-1} √ó n_k √ó r_k), and r_k are the TT-ranks.\n+/// The boundary ranks are r_0 = r_d = 1.\n+/// </para>\n+/// <para><b>For Beginners:</b> LoRETTA is an advanced version of LoRA that uses \"tensor-train decomposition\"!\n+///\n+/// Standard LoRA uses two matrices (A and B) to approximate weight changes:\n+/// - Matrix A: Compresses input to rank dimensions\n+/// - Matrix B: Expands back to output dimensions\n+/// - Parameters: inputSize √ó rank + rank √ó outputSize\n+///\n+/// LoRETTA uses multiple small \"cores\" chained together:\n+/// - Instead of 2 large matrices, use many small tensors\n+/// - Each core captures local correlations\n+/// - The cores are \"contracted\" (multiplied in sequence)\n+/// - Can express more complex patterns with fewer parameters\n+///\n+/// Why tensor-train decomposition?\n+/// 1. More expressive: Can capture higher-order correlations\n+/// 2. More efficient: Fewer parameters than matrix factorization\n+/// 3. Better compression: Exploits structure in weight updates\n+/// 4. Scalable: Grows logarithmically with dimensions\n+///\n+/// Example parameter counts for 1000√ó1000 layer:\n+/// - Full update: 1,000,000 parameters\n+/// - Standard LoRA (rank=8): 16,000 parameters (98.4% reduction)\n+/// - LoRETTA (rank=4, 3 cores): ~6,000 parameters (99.4% reduction, even better!)\n+///\n+/// Key parameters:\n+/// - ttRank: Controls compression (like LoRA's rank but more powerful)\n+/// - numCores: How many tensor cores in the chain (typically 3-5)\n+/// - alpha: Scaling factor for the adaptation strength\n+///\n+/// When to use LoRETTA:\n+/// - Maximum parameter efficiency needed\n+/// - Weight updates have higher-order structure\n+/// - You have very large layers to adapt\n+/// - Standard LoRA isn't expressive enough at low ranks\n+///\n+/// Reference:\n+/// Tensor-train decomposition: I. V. Oseledets, \"Tensor-train decomposition,\"\n+/// SIAM J. Scientific Computing, 2011.\n+/// </para>\n+/// </remarks>\n+public class LoRETTAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Tensor-train cores representing the weight decomposition.\n+    /// Core k has shape (ttRanks[k-1], coreShape[k], ttRanks[k]).\n+    /// </summary>\n+    private readonly List<Tensor<T>> _ttCores;\n+\n+    /// <summary>\n+    /// The ranks of the tensor-train decomposition.\n+    /// Length is numCores + 1, with ttRanks[0] = ttRanks[numCores] = 1.\n+    /// </summary>\n+    private readonly int[] _ttRanks;\n+\n+    /// <summary>\n+    /// The shape of each core in the tensor-train.\n+    /// </summary>\n+    private readonly int[] _coreShapes;\n+\n+    /// <summary>\n+    /// Number of cores in the tensor-train.\n+    /// </summary>\n+    private readonly int _numCores;\n+\n+    /// <summary>\n+    /// Gradients for each TT core computed during backpropagation.\n+    /// </summary>\n+    private List<Tensor<T>>? _ttCoreGradients;\n+\n+    /// <summary>\n+    /// Cached intermediate tensors from forward pass, needed for gradient computation.\n+    /// </summary>\n+    private List<Tensor<T>>? _forwardIntermediates;\n+\n+    /// <summary>\n+    /// Gets the tensor-train rank.\n+    /// </summary>\n+    /// <remarks>\n+    /// This is the maximum rank in the tensor-train decomposition. Lower rank means\n+    /// more compression but less expressiveness.\n+    /// </remarks>\n+    public int TTRank => _ttRanks.Max();\n+\n+    /// <summary>\n+    /// Gets the number of cores in the tensor-train.\n+    /// </summary>\n+    public int NumCores => _numCores;\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters in the tensor-train cores.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// The total parameters is the sum of all core sizes:\n+    /// sum_k (ttRanks[k-1] √ó coreShapes[k] √ó ttRanks[k])\n+    /// </para>\n+    /// <para>\n+    /// This is typically much smaller than standard LoRA for the same expressiveness.\n+    /// </para>\n+    /// </remarks>\n+    public override int ParameterCount\n+    {\n+        get\n+        {\n+            int ttParams = 0;\n+            for (int k = 0; k < _numCores; k++)\n+            {\n+                ttParams += _ttRanks[k] * _coreShapes[k] * _ttRanks[k + 1];\n+            }\n+\n+            // Add base layer parameters if not frozen\n+            if (!_freezeBaseLayer)\n+            {\n+                return _baseLayer.ParameterCount + ttParams;\n+            }\n+\n+            return ttParams;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Initializes a new LoRETTA adapter wrapping an existing layer.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with LoRETTA.</param>\n+    /// <param name=\"ttRank\">The rank of the tensor-train decomposition.</param>\n+    /// <param name=\"numCores\">Number of cores in the tensor-train (default: 3).</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to ttRank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when ttRank or numCores are invalid.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This creates a LoRETTA adapter that wraps any layer.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to adapt efficiently\n+    /// - ttRank: Controls compression (lower = fewer parameters, less flexibility)\n+    /// - numCores: How many tensor cores to use (more cores = more expressive but more params)\n+    /// - alpha: How strong the adaptation is\n+    /// - freezeBaseLayer: Whether to lock the original layer's weights (usually true)\n+    ///\n+    /// The cores are initialized carefully:\n+    /// - First and last cores connect to input/output dimensions\n+    /// - Middle cores have uniform shapes\n+    /// - All cores start with small random values (Gaussian initialization)\n+    /// - Designed so initial LoRETTA has minimal effect\n+    ///\n+    /// Recommended settings:\n+    /// - ttRank=4 to 8: Good balance of efficiency and expressiveness\n+    /// - numCores=3: Standard choice (input core, middle core, output core)\n+    /// - numCores=4-5: For very large layers or complex adaptations\n+    /// </para>\n+    /// </remarks>\n+    public LoRETTAAdapter(\n+        ILayer<T> baseLayer,\n+        int ttRank,\n+        int numCores = 3,\n+        double alpha = -1,\n+        bool freezeBaseLayer = true)\n+        : base(baseLayer, ttRank, alpha, freezeBaseLayer)\n+    {\n+        if (ttRank <= 0)\n+        {\n+            throw new ArgumentException(\"TT-rank must be positive\", nameof(ttRank));\n+        }\n+\n+        if (numCores < 2)\n+        {\n+            throw new ArgumentException(\"Number of cores must be at least 2\", nameof(numCores));\n+        }\n+\n+        _numCores = numCores;\n+\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+\n+        // Initialize TT-ranks: [1, ttRank, ttRank, ..., ttRank, 1]\n+        _ttRanks = new int[numCores + 1];\n+        _ttRanks[0] = 1;\n+        _ttRanks[numCores] = 1;\n+        for (int k = 1; k < numCores; k++)\n+        {\n+            _ttRanks[k] = ttRank;\n+        }\n+\n+        // Compute core shapes by factorizing input and output dimensions\n+        _coreShapes = ComputeCoreShapes(inputSize, outputSize, numCores);\n+\n+        // Initialize TT cores\n+        _ttCores = new List<Tensor<T>>(numCores);\n+        InitializeTTCores();\n+\n+        // Update parameter vector\n+        Parameters = new Vector<T>(ParameterCount);\n+        UpdateParametersFromCores();\n+    }\n+\n+    /// <summary>\n+    /// Computes the shape of each core by factorizing the total dimension.\n+    /// </summary>\n+    /// <param name=\"inputSize\">Input dimension.</param>\n+    /// <param name=\"outputSize\">Output dimension.</param>\n+    /// <param name=\"numCores\">Number of cores.</param>\n+    /// <returns>Array of core shapes.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// We need to factorize the total dimensionality (inputSize √ó outputSize) across the cores.\n+    /// The product of all core shapes should approximately equal inputSize √ó outputSize.\n+    ///\n+    /// Strategy: Use geometric decomposition\n+    /// - First core: ~inputSize^(1/2) √ó outputSize^(1/(numCores-1))\n+    /// - Last core: ~inputSize^(1/2) √ó outputSize^(1/(numCores-1))\n+    /// - Middle cores: uniform sizes based on geometric mean\n+    /// </para>\n+    /// </remarks>\n+    private int[] ComputeCoreShapes(int inputSize, int outputSize, int numCores)\n+    {\n+        int[] shapes = new int[numCores];\n+\n+        // Total \"logical\" dimension to decompose\n+        double totalDim = Math.Sqrt((double)inputSize * outputSize);\n+\n+        // Use geometric factorization\n+        double dimPerCore = Math.Pow(totalDim, 2.0 / numCores);\n+\n+        // Ensure each core has at least dimension 2\n+        int baseDim = Math.Max(2, (int)Math.Ceiling(dimPerCore));\n+\n+        // Distribute dimensions\n+        for (int k = 0; k < numCores; k++)\n+        {\n+            shapes[k] = baseDim;\n+        }\n+\n+        // Adjust first and last cores to better match input/output sizes\n+        shapes[0] = Math.Max(2, (int)Math.Ceiling(Math.Sqrt(inputSize)));\n+        shapes[numCores - 1] = Math.Max(2, (int)Math.Ceiling(Math.Sqrt(outputSize)));\n+\n+        return shapes;\n+    }\n+\n+    /// <summary>\n+    /// Initializes all TT cores with small random values.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Each core is initialized with Gaussian noise scaled by 1/sqrt(product of dimensions).\n+    /// This ensures the overall adaptation starts small.\n+    /// </para>\n+    /// </remarks>\n+    private void InitializeTTCores()\n+    {\n+        Random random = new Random(42);\n+\n+        for (int k = 0; k < _numCores; k++)\n+        {\n+            int leftRank = _ttRanks[k];\n+            int coreShape = _coreShapes[k];\n+            int rightRank = _ttRanks[k + 1];\n+\n+            // Core has shape [leftRank, coreShape, rightRank]\n+            int[] shape = new int[] { leftRank, coreShape, rightRank };\n+            Tensor<T> core = new Tensor<T>(shape);\n+\n+            // Initialize with small Gaussian noise\n+            double scale = 1.0 / Math.Sqrt(leftRank * coreShape * rightRank);\n+\n+            for (int i = 0; i < core.Length; i++)\n+            {\n+                // Box-Muller transform for Gaussian random numbers\n+                double u1 = random.NextDouble();\n+                double u2 = random.NextDouble();\n+                double randStdNormal = Math.Sqrt(-2.0 * Math.Log(u1)) * Math.Sin(2.0 * Math.PI * u2);\n+                core[i] = NumOps.Multiply(NumOps.FromDouble(randStdNormal), NumOps.FromDouble(scale));\n+            }\n+\n+            _ttCores.Add(core);\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass through the LoRETTA adapter.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Sum of base layer output and LoRETTA output.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The forward pass computes the tensor-train contraction to produce the adaptation,\n+    /// then adds it to the base layer output.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This processes input through both the original layer and\n+    /// the LoRETTA adaptation, then combines them.\n+    ///\n+    /// The LoRETTA forward pass:\n+    /// 1. Forward through base layer (original behavior)\n+    /// 2. Contract tensor-train cores with input (compute adaptation)\n+    /// 3. Add base output + adaptation output\n+    ///\n+    /// The tensor contraction is done sequentially through the cores, which is efficient\n+    /// even though it looks complex mathematically.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        // Store intermediates for backward pass\n+        _forwardIntermediates = new List<Tensor<T>>();\n+\n+        // Forward through base layer\n+        Tensor<T> baseOutput = _baseLayer.Forward(input);\n+\n+        // Compute LoRETTA adaptation via tensor-train contraction\n+        Tensor<T> ttOutput = ComputeTensorTrainForward(input);\n+\n+        // Sum the outputs\n+        Tensor<T> result = new Tensor<T>(baseOutput.Shape);\n+        for (int i = 0; i < baseOutput.Length; i++)\n+        {\n+            result[i] = NumOps.Add(baseOutput[i], ttOutput[i]);\n+        }\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Computes the forward pass through the tensor-train decomposition.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor of shape [batchSize, inputSize].</param>\n+    /// <returns>Output tensor of shape [batchSize, outputSize].</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This performs the tensor-train contraction:\n+    /// 1. Reshape input to match first core dimensions\n+    /// 2. Contract through each core sequentially\n+    /// 3. Reshape output to match expected output dimensions\n+    /// </para>\n+    /// </remarks>\n+    private Tensor<T> ComputeTensorTrainForward(Tensor<T> input)\n+    {\n+        int batchSize = input.Shape[0];\n+        int inputSize = input.Shape.Length > 1 ? input.Shape[1] : input.Length;\n+\n+        // Start with input reshaped to work with first core\n+        // For simplicity, we'll use a matrix-based contraction approach\n+\n+        // Flatten input to [batchSize √ó inputSize]\n+        Matrix<T> currentMatrix = new Matrix<T>(batchSize, inputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                currentMatrix[i, j] = input[i * inputSize + j];\n+            }\n+        }\n+\n+        // Contract through each core\n+        for (int k = 0; k < _numCores; k++)\n+        {\n+            currentMatrix = ContractWithCore(currentMatrix, _ttCores[k], k);\n+\n+            // Store intermediate for backward pass\n+            if (_forwardIntermediates != null)\n+            {\n+                _forwardIntermediates.Add(TensorFromMatrix(currentMatrix));\n+            }\n+        }\n+\n+        // Extract output\n+        int outputSize = GetOutputShape()[0];\n+        Vector<T> outputData = new Vector<T>(batchSize * outputSize);\n+\n+        int idx = 0;\n+        int currentCols = currentMatrix.Columns;\n+        int outputCols = Math.Min(outputSize, currentCols);\n+\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                if (j < outputCols && i < currentMatrix.Rows)\n+                {\n+                    outputData[idx] = currentMatrix[i, j % currentMatrix.Columns];\n+                }\n+                else\n+                {\n+                    outputData[idx] = NumOps.Zero;\n+                }\n+                idx++;\n+            }\n+        }\n+\n+        // Apply scaling (alpha / rank)\n+        T scaling = NumOps.Divide(\n+            NumOps.FromDouble(Alpha),\n+            NumOps.FromDouble(TTRank)\n+        );\n+\n+        for (int i = 0; i < outputData.Length; i++)\n+        {\n+            outputData[i] = NumOps.Multiply(outputData[i], scaling);\n+        }\n+\n+        return new Tensor<T>(new[] { batchSize, outputSize }, outputData);\n+    }\n+\n+    /// <summary>\n+    /// Contracts a matrix with a tensor-train core.\n+    /// </summary>\n+    /// <param name=\"input\">Input matrix [batchSize, currentDim].</param>\n+    /// <param name=\"core\">TT core tensor [leftRank, coreShape, rightRank].</param>\n+    /// <param name=\"coreIndex\">Index of the core being processed.</param>\n+    /// <returns>Output matrix [batchSize, nextDim].</returns>\n+    private Matrix<T> ContractWithCore(Matrix<T> input, Tensor<T> core, int coreIndex)\n+    {\n+        int batchSize = input.Rows;\n+        int leftRank = _ttRanks[coreIndex];\n+        int coreShape = _coreShapes[coreIndex];\n+        int rightRank = _ttRanks[coreIndex + 1];\n+\n+        // Simplified contraction: treat core as a sequence of matrices\n+        // Core shape: [leftRank, coreShape, rightRank]\n+        // We'll contract by reshaping and matrix multiplication\n+\n+        int inputDim = input.Columns;\n+        int outputDim = coreShape * rightRank;\n+\n+        Matrix<T> output = new Matrix<T>(batchSize, outputDim);\n+\n+        // For each batch element\n+        for (int b = 0; b < batchSize; b++)\n+        {\n+            // Contract input with core\n+            // Simplified: use first 'leftRank' dimensions of input\n+            for (int r = 0; r < rightRank; r++)\n+            {\n+                for (int c = 0; c < coreShape; c++)\n+                {\n+                    T sum = NumOps.Zero;\n+\n+                    for (int l = 0; l < leftRank && l < inputDim; l++)\n+                    {\n+                        int coreIdx = (l * coreShape * rightRank) + (c * rightRank) + r;\n+                        if (coreIdx < core.Length)\n+                        {\n+                            T inputVal = input[b, l];\n+                            T coreVal = core[coreIdx];\n+                            sum = NumOps.Add(sum, NumOps.Multiply(inputVal, coreVal));\n+                        }\n+                    }\n+\n+                    int outIdx = c * rightRank + r;\n+                    if (outIdx < outputDim)\n+                    {\n+                        output[b, outIdx] = sum;\n+                    }\n+                }\n+            }\n+        }\n+\n+        return output;\n+    }\n+\n+    /// <summary>\n+    /// Converts a matrix to a tensor.\n+    /// </summary>\n+    private Tensor<T> TensorFromMatrix(Matrix<T> matrix)\n+    {\n+        Vector<T> data = new Vector<T>(matrix.Rows * matrix.Columns);\n+        int idx = 0;\n+        for (int i = 0; i < matrix.Rows; i++)\n+        {\n+            for (int j = 0; j < matrix.Columns; j++)\n+            {\n+                data[idx++] = matrix[i, j];\n+            }\n+        }\n+        return new Tensor<T>(new[] { matrix.Rows, matrix.Columns }, data);\n+    }\n+\n+    /// <summary>\n+    /// Performs the backward pass through the LoRETTA adapter.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient flowing back from the next layer.</param>\n+    /// <returns>Gradient to pass to the previous layer.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The backward pass computes gradients for all TT cores and propagates gradients\n+    /// back through the tensor-train contraction.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is where learning happens for LoRETTA!\n+    ///\n+    /// The backward pass:\n+    /// 1. Backpropagate through base layer\n+    /// 2. Backpropagate through tensor-train cores\n+    /// 3. Compute gradients for each core\n+    /// 4. Combine input gradients from both paths\n+    ///\n+    /// This is more complex than standard LoRA because we need to backpropagate through\n+    /// multiple cores, but the principle is the same: figure out how each parameter\n+    /// contributed to the error.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        // Backward through base layer\n+        Tensor<T> baseInputGrad = _baseLayer.Backward(outputGradient);\n+\n+        // Backward through tensor-train\n+        Tensor<T> ttInputGrad = ComputeTensorTrainBackward(outputGradient);\n+\n+        // Sum input gradients\n+        Tensor<T> inputGrad = new Tensor<T>(baseInputGrad.Shape);\n+        for (int i = 0; i < baseInputGrad.Length; i++)\n+        {\n+            inputGrad[i] = NumOps.Add(baseInputGrad[i], ttInputGrad[i]);\n+        }\n+\n+        // Update parameter gradients vector\n+        UpdateParameterGradientsFromCores();\n+\n+        return inputGrad;\n+    }\n+\n+    /// <summary>\n+    /// Computes the backward pass through the tensor-train decomposition.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient from the output.</param>\n+    /// <returns>Gradient with respect to input.</returns>\n+    private Tensor<T> ComputeTensorTrainBackward(Tensor<T> outputGradient)\n+    {\n+        // Initialize core gradients\n+        _ttCoreGradients = new List<Tensor<T>>();\n+        for (int k = 0; k < _numCores; k++)\n+        {\n+            _ttCoreGradients.Add(new Tensor<T>(_ttCores[k].Shape));\n+        }\n+\n+        // Simplified backward: compute gradients using finite differences approximation\n+        // For production, would implement proper backpropagation through tensor contractions\n+\n+        int batchSize = outputGradient.Shape[0];\n+        int inputSize = GetInputShape()[0];\n+\n+        // Create zero gradient for input\n+        Tensor<T> inputGradient = new Tensor<T>(new[] { batchSize, inputSize });\n+\n+        // For each core, compute gradient (simplified using the chain rule)\n+        for (int k = 0; k < _numCores; k++)\n+        {\n+            // Gradient computation would use stored intermediates\n+            // For now, initialize with small values\n+            for (int i = 0; i < _ttCoreGradients[k].Length; i++)\n+            {\n+                _ttCoreGradients[k][i] = NumOps.Multiply(\n+                    outputGradient[i % outputGradient.Length],\n+                    NumOps.FromDouble(0.01)\n+                );\n+            }\n+        }\n+\n+        return inputGradient;\n+    }\n+\n+    /// <summary>\n+    /// Updates parameters using the specified learning rate.\n+    /// </summary>\n+    /// <param name=\"learningRate\">The learning rate for parameter updates.</param>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This applies the gradients to update the TT cores.\n+    ///\n+    /// For each core:\n+    /// 1. Get the gradient computed during backpropagation\n+    /// 2. Update: core_new = core_old - learningRate √ó gradient\n+    /// 3. Update base layer if not frozen\n+    ///\n+    /// This is conceptually the same as standard gradient descent, but applied to\n+    /// the tensor-train cores instead of weight matrices.\n+    /// </para>\n+    /// </remarks>\n+    public override void UpdateParameters(T learningRate)\n+    {\n+        if (_ttCoreGradients == null)\n+        {\n+            return;\n+        }\n+\n+        // Update each TT core\n+        for (int k = 0; k < _numCores; k++)\n+        {\n+            for (int i = 0; i < _ttCores[k].Length; i++)\n+            {\n+                T update = NumOps.Multiply(_ttCoreGradients[k][i], learningRate);\n+                _ttCores[k][i] = NumOps.Subtract(_ttCores[k][i], update);\n+            }\n+        }\n+\n+        // Update base layer if not frozen\n+        if (!_freezeBaseLayer)\n+        {\n+            _baseLayer.UpdateParameters(learningRate);\n+        }\n+\n+        // Update parameter vector\n+        UpdateParametersFromCores();\n+    }\n+\n+    /// <summary>\n+    /// Updates the parameter vector from the current TT core values.\n+    /// </summary>\n+    private void UpdateParametersFromCores()\n+    {\n+        int idx = 0;\n+\n+        // If base layer is not frozen, pack its parameters first\n+        if (!_freezeBaseLayer)\n+        {\n+            Vector<T> baseParams = _baseLayer.GetParameters();\n+            for (int i = 0; i < baseParams.Length; i++)\n+            {\n+                Parameters[idx++] = baseParams[i];\n+            }\n+        }\n+\n+        // Pack all TT cores\n+        foreach (Tensor<T> core in _ttCores)\n+        {\n+            for (int i = 0; i < core.Length; i++)\n+            {\n+                Parameters[idx++] = core[i];\n+            }\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Updates the TT cores from the parameter vector.\n+    /// </summary>\n+    private void UpdateCoresFromParameters()\n+    {\n+        int idx = 0;\n+\n+        // If base layer is not frozen, unpack its parameters first\n+        if (!_freezeBaseLayer)\n+        {\n+            int baseParamCount = _baseLayer.ParameterCount;\n+            Vector<T> baseParams = new Vector<T>(baseParamCount);\n+            for (int i = 0; i < baseParamCount; i++)\n+            {\n+                baseParams[i] = Parameters[idx++];\n+            }\n+            _baseLayer.SetParameters(baseParams);\n+        }\n+\n+        // Unpack all TT cores\n+        for (int k = 0; k < _numCores; k++)\n+        {\n+            for (int i = 0; i < _ttCores[k].Length; i++)\n+            {\n+                _ttCores[k][i] = Parameters[idx++];\n+            }\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Updates the parameter gradients vector from the TT core gradients.\n+    /// </summary>\n+    private void UpdateParameterGradientsFromCores()\n+    {\n+        ParameterGradients = new Vector<T>(ParameterCount);\n+        int idx = 0;\n+\n+        // If base layer is not frozen, pack its gradients first\n+        if (!_freezeBaseLayer)\n+        {\n+            Vector<T> baseGrads = _baseLayer.GetParameterGradients();\n+            for (int i = 0; i < baseGrads.Length; i++)\n+            {\n+                ParameterGradients[idx++] = baseGrads[i];\n+            }\n+        }\n+\n+        // Pack TT core gradients\n+        if (_ttCoreGradients != null)\n+        {\n+            foreach (Tensor<T> coreGrad in _ttCoreGradients)\n+            {\n+                for (int i = 0; i < coreGrad.Length; i++)\n+                {\n+                    ParameterGradients[idx++] = coreGrad[i];\n+                }\n+            }\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Gets the current parameters as a vector.\n+    /// </summary>\n+    /// <returns>Vector containing parameters.</returns>\n+    public override Vector<T> GetParameters()\n+    {\n+        return Parameters.Clone();\n+    }\n+\n+    /// <summary>\n+    /// Sets the layer parameters from a vector.\n+    /// </summary>\n+    /// <param name=\"parameters\">Vector containing parameters.</param>\n+    public override void SetParameters(Vector<T> parameters)\n+    {\n+        if (parameters.Length != ParameterCount)\n+        {\n+            throw new ArgumentException(\n+                $\"Expected {ParameterCount} parameters, got {parameters.Length}\",\n+                nameof(parameters));\n+        }\n+\n+        Parameters = parameters.Clone();\n+        UpdateCoresFromParameters();\n+    }\n+\n+    /// <summary>\n+    /// Merges the LoRETTA adaptation into the base layer and returns the merged layer.\n+    /// </summary>\n+    /// <returns>A new layer with LoRETTA weights merged into the base layer's weights.</returns>\n+    /// <exception cref=\"InvalidOperationException\">Thrown when the base layer type is not supported.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This \"bakes in\" your LoRETTA adaptation to create a regular layer.\n+    ///\n+    /// After training:\n+    /// 1. Contract all TT cores to form a full weight matrix\n+    /// 2. Add this matrix to the base layer's weights\n+    /// 3. Create a new layer with the merged weights\n+    ///\n+    /// The result is a standard layer that behaves like your adapted model but:\n+    /// - Faster inference (no tensor-train contraction needed)\n+    /// - Simpler deployment (single layer instead of adapter)\n+    /// - Compatible with any framework\n+    ///\n+    /// The tensor-train cores are contracted to form a full weight update matrix,\n+    /// which is then added to the original weights.\n+    /// </para>\n+    /// </remarks>\n+    public override ILayer<T> MergeToOriginalLayer()\n+    {\n+        // Check base layer type\n+        DenseLayer<T>? denseBase = _baseLayer as DenseLayer<T>;\n+        FullyConnectedLayer<T>? fcBase = _baseLayer as FullyConnectedLayer<T>;\n+\n+        if (denseBase == null && fcBase == null)\n+        {\n+            throw new InvalidOperationException(\n+                \"LoRETTAAdapter merging only supports DenseLayer or FullyConnectedLayer base layers\");\n+        }\n+\n+        // Contract TT cores to form full weight matrix\n+        Matrix<T> ttWeights = ContractTensorTrainToMatrix();\n+\n+        // Get base layer parameters\n+        Vector<T> baseParams = _baseLayer.GetParameters();\n+\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        int weightCount = inputSize * outputSize;\n+\n+        // Create new parameters with merged weights\n+        Vector<T> mergedParams = new Vector<T>(baseParams.Length);\n+\n+        // Merge weights (add LoRETTA contribution to base weights)\n+        for (int i = 0; i < weightCount && i < baseParams.Length; i++)\n+        {\n+            int row = i / inputSize;\n+            int col = i % inputSize;\n+\n+            T ttContribution = NumOps.Zero;\n+            if (row < ttWeights.Rows && col < ttWeights.Columns)\n+            {\n+                ttContribution = ttWeights[row, col];\n+            }\n+\n+            mergedParams[i] = NumOps.Add(baseParams[i], ttContribution);\n+        }\n+\n+        // Copy biases unchanged\n+        for (int i = weightCount; i < baseParams.Length; i++)\n+        {\n+            mergedParams[i] = baseParams[i];\n+        }\n+\n+        // Create a new dense layer with merged parameters\n+        DenseLayer<T> mergedLayer = new DenseLayer<T>(\n+            inputSize,\n+            outputSize,\n+            (IActivationFunction<T>?)null);\n+        mergedLayer.SetParameters(mergedParams);\n+\n+        return mergedLayer;\n+    }\n+\n+    /// <summary>\n+    /// Contracts the tensor-train cores into a full weight matrix.\n+    /// </summary>\n+    /// <returns>Full weight matrix representing the TT decomposition.</returns>\n+    /// <remarks>\n+    /// This performs the full contraction of all TT cores to recover the\n+    /// complete weight update matrix. This is expensive but only needed for merging.\n+    /// </remarks>\n+    private Matrix<T> ContractTensorTrainToMatrix()\n+    {\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+\n+        // Create output matrix\n+        Matrix<T> result = new Matrix<T>(outputSize, inputSize);\n+\n+        // Simplified contraction: use the first and last cores to form a low-rank approximation\n+        // In a full implementation, would contract all cores\n+\n+        // Initialize with zeros\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                result[i, j] = NumOps.Zero;\n+            }\n+        }\n+\n+        // Add contributions from TT cores (simplified)\n+        // For a proper implementation, would perform full tensor contraction\n+        T scale = NumOps.FromDouble(1.0 / _numCores);\n+\n+        for (int k = 0; k < _numCores; k++)\n+        {\n+            Tensor<T> core = _ttCores[k];\n+\n+            for (int i = 0; i < Math.Min(outputSize, core.Length); i++)\n+            {\n+                for (int j = 0; j < Math.Min(inputSize, core.Length); j++)\n+                {\n+                    int idx = (i * inputSize + j) % core.Length;\n+                    result[i, j] = NumOps.Add(\n+                        result[i, j],\n+                        NumOps.Multiply(core[idx], scale)\n+                    );\n+                }\n+            }\n+        }\n+\n+        // Apply scaling\n+        T scaling = NumOps.Divide(\n+            NumOps.FromDouble(Alpha),\n+            NumOps.FromDouble(TTRank)\n+        );\n+\n+        return result.Multiply(scaling);\n+    }","path":"src/LoRA/Adapters/LoRETTAAdapter.cs","commit_id":"fa815036ee570162056ba6aca773f34c80f36948","original_commit_id":"8d5a1b6564c1a2bc1b334f1863e6003a6fd142b2","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Critical: Tensor-train contraction is not properly implemented.**\n\nThe `ContractTensorTrainToMatrix` method uses a simplified placeholder implementation that will not correctly merge the LoRETTA adaptation into the base layer. The current approach:\n\n1. Uses arbitrary scaling (`1.0 / _numCores`) instead of proper tensor contraction\n2. Uses modulo indexing (`(i * inputSize + j) % core.Length`) to handle dimension mismatches\n3. Does not perform the actual tensor-train contraction mathematics\n\n\n\nThe comments explicitly acknowledge this limitation:\n```csharp\n// Simplified contraction: use the first and last cores to form a low-rank approximation\n// In a full implementation, would contract all cores\n// ...\n// For a proper implementation, would perform full tensor contraction\n```\n\nA proper tensor-train contraction should:\n1. Start from the rightmost core and progressively contract with each core moving left\n2. Properly handle the tensor indices and ranks at each contraction step\n3. Produce a matrix that, when applied to inputs, gives the same result as `ComputeTensorTrainForward`\n\nThe merge operation is critical for deployment scenarios where you want to \"bake in\" the adaptation for faster inference. Without correct contraction, the merged layer will not preserve the learned adaptation.\n\n**This is a blocking issue** that prevents the deployment path from working correctly. Consider:\n- Implementing proper tensor-train contraction based on the TT decomposition literature\n- Adding unit tests that verify: `MergeToOriginalLayer().Forward(x)` ‚âà `LoRETTAAdapter.Forward(x)`\n- Or documenting that merge is not yet supported for LoRETTA\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/LoRA/Adapters/LoRETTAAdapter.cs around lines 828 to 876, the current\nContractTensorTrainToMatrix is a placeholder using arbitrary scaling and modulo\nindexing; replace it with a proper tensor-train (TT) contraction: iterate from\nthe rightmost core to the leftmost, at each step contract the running\naccumulator with the next core using the correct TT core shapes (rank_left x\nmode_dim x rank_right), perform index-respecting tensor contractions (no modulo\nhacks), and reshape/interleave dimensions as needed so the final result is an\noutputSize x inputSize matrix equivalent to the full TT-to-matrix flattening;\nensure you correctly use the stored TT ranks and mode dimensions, apply the\nfinal Alpha/TTRank scaling, and add unit tests that assert\nMergeToOriginalLayer().Forward(x) ‚âà LoRETTAAdapter.Forward(x) (or explicitly\ndocument that merging is unsupported if you cannot implement it now).\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T03:02:29Z","updated_at":"2025-11-02T03:02:32Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140323","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140323"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140323"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140323/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":828,"original_start_line":828,"start_side":"RIGHT","line":876,"original_line":876,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":876,"position":876,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140326","pull_request_review_id":3408046172,"id":2484140326,"node_id":"PRRC_kwDOKSXUF86UEPkm","diff_hunk":"@@ -0,0 +1,566 @@\n+using AiDotNet.DecompositionMethods.MatrixDecomposition;\n+using AiDotNet.Enums.AlgorithmTypes;\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// Principal Singular Values and Singular Vectors Adaptation (PiSSA) adapter for parameter-efficient fine-tuning.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// PiSSA (NeurIPS 2024 Spotlight) improves upon standard LoRA by initializing adapter matrices with\n+/// principal components from Singular Value Decomposition (SVD) of pretrained weights, rather than\n+/// random initialization. This results in more effective use of the rank budget and faster convergence.\n+/// </para>\n+/// <para><b>Key Differences from Standard LoRA:</b>\n+/// - Standard LoRA: A initialized randomly, B initialized to zero\n+/// - PiSSA: A and B initialized from top-r singular vectors of pretrained weights\n+/// - Standard LoRA: All weights trainable\n+/// - PiSSA: Residual weights frozen, only top-r components trainable\n+/// </para>\n+/// <para><b>How PiSSA Works:</b>\n+/// 1. Perform SVD on pretrained weights: W = U Œ£ V^T\n+/// 2. Initialize adapter matrices from top-r components:\n+///    - A = V_r^T (top-r right singular vectors)\n+///    - B = U_r Œ£_r (top-r left singular vectors scaled by singular values)\n+/// 3. Freeze residual matrix: W_residual = W - B*A\n+/// 4. During training: output = W_residual * input + B*A*input\n+/// 5. Only B and A are updated; W_residual stays frozen\n+/// </para>\n+/// <para><b>Performance Benefits:</b>\n+/// PiSSA achieves superior performance compared to standard LoRA:\n+/// - GSM8K benchmark: 72.86% (PiSSA) vs 67.7% (LoRA)\n+/// - Better initialization captures important pretrained knowledge\n+/// - More effective gradient updates from the start\n+/// - Faster convergence with fewer training steps\n+/// </para>\n+/// <para><b>For Beginners:</b> Think of PiSSA as \"smart LoRA initialization\".\n+///\n+/// Standard LoRA starts from random:\n+/// - Random A matrix (like throwing darts blindfolded)\n+/// - Zero B matrix (starts with no effect)\n+/// - Learns everything from scratch\n+///\n+/// PiSSA starts from the most important parts of pretrained weights:\n+/// - A and B capture the top-r \"principal directions\" of the pretrained model\n+/// - Starts closer to the optimal solution\n+/// - Like starting a puzzle with the border pieces already connected\n+///\n+/// Example: If you have a pretrained language model with a 4096x4096 weight matrix,\n+/// PiSSA with rank=8 will:\n+/// 1. Find the top 8 most important patterns in those weights via SVD\n+/// 2. Put those patterns into A and B (making them trainable)\n+/// 3. Freeze the remaining \"less important\" patterns\n+/// 4. Train only the top 8 patterns to adapt to your task\n+///\n+/// This is much more efficient than starting from random and achieves better results!\n+/// </para>\n+/// <para><b>References:</b>\n+/// - Paper: \"PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models\"\n+/// - Venue: NeurIPS 2024 (Spotlight)\n+/// - Key Insight: SVD-based initialization > random initialization for low-rank adaptation\n+/// </para>\n+/// </remarks>\n+public class PiSSAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// The frozen residual weights after removing top-r principal components.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This matrix represents W_residual = W - B*A, where W is the original pretrained weights\n+    /// and B*A is the top-r rank approximation. During training, this matrix remains frozen\n+    /// while only the adapter matrices (A and B) are updated.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is the \"leftover\" part of the original weights.\n+    ///\n+    /// Think of the original weights as a complete picture:\n+    /// - The top-r components (in A and B) capture the main features\n+    /// - The residual is what's left after removing those main features\n+    /// - During training, we keep this residual fixed and only adjust the main features\n+    ///\n+    /// This is like keeping the background of a photo fixed while adjusting only the main subject.\n+    /// </para>\n+    /// </remarks>\n+    private Matrix<T>? _residualWeights;\n+\n+    /// <summary>\n+    /// Indicates whether the adapter was initialized from SVD of pretrained weights.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// When true, this adapter was properly initialized using PiSSA's SVD-based initialization.\n+    /// When false, it falls back to standard LoRA random initialization (not recommended for PiSSA).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This flag tells you if the adapter is using PiSSA's smart initialization.\n+    ///\n+    /// True = properly initialized with SVD (recommended)\n+    /// False = using random initialization like standard LoRA (loses PiSSA benefits)\n+    /// </para>\n+    /// </remarks>\n+    private bool _initializedFromSVD;\n+\n+    /// <summary>\n+    /// Gets the frozen residual weights matrix.\n+    /// </summary>\n+    /// <remarks>\n+    /// This matrix is computed during SVD initialization and remains frozen during training.\n+    /// Returns null if SVD initialization was not performed.\n+    /// </remarks>\n+    public Matrix<T>? ResidualWeights => _residualWeights?.Clone();\n+\n+    /// <summary>\n+    /// Gets whether this adapter was initialized from SVD.\n+    /// </summary>\n+    /// <remarks>\n+    /// Returns true if InitializeFromSVD was called successfully, false otherwise.\n+    /// </remarks>\n+    public bool InitializedFromSVD => _initializedFromSVD;\n+\n+    /// <summary>\n+    /// Initializes a new PiSSA adapter wrapping an existing layer.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with PiSSA.</param>\n+    /// <param name=\"rank\">The rank of the low-rank decomposition.</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// This constructor creates a PiSSA adapter. After construction, you should call\n+    /// InitializeFromSVD to properly initialize the adapter matrices from pretrained weights.\n+    /// Without SVD initialization, the adapter behaves like standard LoRA (not recommended).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This creates a PiSSA adapter for any layer type.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to adapt (Dense, Convolutional, etc.)\n+    /// - rank: How many principal components to use (typically 4-32)\n+    /// - alpha: Scaling factor for the adaptation strength\n+    /// - freezeBaseLayer: Usually true to freeze original weights\n+    ///\n+    /// Important: After creating the adapter, call InitializeFromSVD with the pretrained\n+    /// weights to get PiSSA's performance benefits. Otherwise, it's just regular LoRA.\n+    /// </para>\n+    /// </remarks>\n+    public PiSSAAdapter(ILayer<T> baseLayer, int rank, double alpha = -1, bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        _initializedFromSVD = false;\n+    }\n+\n+    /// <summary>\n+    /// Initializes the adapter matrices from SVD of pretrained weights.\n+    /// </summary>\n+    /// <param name=\"pretrainedWeights\">The pretrained weight matrix to decompose.</param>\n+    /// <param name=\"svdAlgorithm\">The SVD algorithm to use (default: GolubReinsch).</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when pretrainedWeights is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when weight matrix dimensions don't match layer dimensions.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// This method performs the core PiSSA initialization:\n+    /// 1. Computes SVD: W = U Œ£ V^T\n+    /// 2. Extracts top-r components: U_r, Œ£_r, V_r\n+    /// 3. Initializes A = V_r^T (right singular vectors)\n+    /// 4. Initializes B = U_r Œ£_r (left singular vectors scaled by singular values)\n+    /// 5. Computes residual: W_residual = W - B*A\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is where the magic happens!\n+    ///\n+    /// The method:\n+    /// 1. Takes your pretrained weights (like from a large language model)\n+    /// 2. Finds the most important patterns using SVD (mathematical technique)\n+    /// 3. Puts those patterns into the adapter matrices A and B\n+    /// 4. Saves the \"leftover\" patterns as frozen residual weights\n+    ///\n+    /// Think of it like:\n+    /// - Original weights = complete painting\n+    /// - SVD = identifying the main strokes vs. minor details\n+    /// - A and B = the main strokes (what we'll adjust)\n+    /// - Residual = the minor details (kept frozen)\n+    ///\n+    /// This initialization is what makes PiSSA better than LoRA - it starts from\n+    /// a smart place instead of random values.\n+    /// </para>\n+    /// </remarks>\n+    public void InitializeFromSVD(Matrix<T> pretrainedWeights, SvdAlgorithmType svdAlgorithm = SvdAlgorithmType.GolubReinsch)\n+    {\n+        if (pretrainedWeights == null)\n+        {\n+            throw new ArgumentNullException(nameof(pretrainedWeights));\n+        }\n+\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+\n+        if (pretrainedWeights.Rows != outputSize || pretrainedWeights.Columns != inputSize)\n+        {\n+            throw new ArgumentException(\n+                $\"Weight matrix dimensions ({pretrainedWeights.Rows}x{pretrainedWeights.Columns}) \" +\n+                $\"do not match layer dimensions ({outputSize}x{inputSize})\",\n+                nameof(pretrainedWeights));\n+        }\n+\n+        // Perform SVD: W = U Œ£ V^T\n+        SvdDecomposition<T> svd = new SvdDecomposition<T>(pretrainedWeights, svdAlgorithm);\n+\n+        // Extract top-r singular values and vectors\n+        int r = Rank;\n+\n+        // Create A matrix from top-r right singular vectors: A = V_r^T\n+        // V^T has dimensions (inputSize x inputSize), we take first r rows\n+        Matrix<T> matrixA = new Matrix<T>(inputSize, r);\n+        for (int i = 0; i < inputSize; i++)\n+        {\n+            for (int j = 0; j < r; j++)\n+            {\n+                matrixA[i, j] = svd.Vt[j, i]; // Transpose: V_r^T\n+            }\n+        }\n+\n+        // Create B matrix from top-r left singular vectors scaled by singular values: B = U_r Œ£_r\n+        // U has dimensions (outputSize x outputSize), we take first r columns\n+        // Œ£ is diagonal, so we scale each column of U_r by the corresponding singular value\n+        Matrix<T> matrixB = new Matrix<T>(r, outputSize);\n+        for (int i = 0; i < r; i++)\n+        {\n+            T singularValue = svd.S[i];\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                matrixB[i, j] = NumOps.Multiply(svd.U[j, i], singularValue);\n+            }\n+        }\n+\n+        // Compute the low-rank approximation: W_rank_r = B*A\n+        // Note: matrixA is [inputSize x r], matrixB is [r x outputSize]\n+        // So B*A would be [r x r], which is wrong. We need A*B^T for proper dimensions.\n+        // Actually, for PiSSA: output = W_residual * input + B * A * input\n+        // Where A: [inputSize x r], B: [r x outputSize]\n+        // So B*A: [r x outputSize] * [inputSize x r] - dimension mismatch!\n+        // Correct formulation: A is applied first (compresses input), then B (expands to output)\n+        // Let's recalculate: we need W ‚âà B^T * A^T in weight space\n+\n+        // For LoRA layer: input -> A -> (rank dims) -> B -> output\n+        // For weight reconstruction: W = B^T * A^T (both transposed)\n+        // Since LoRALayer stores A as [inputSize x rank] and B as [rank x outputSize]\n+        // The weight contribution is: W_lora = A * B (gives [inputSize x outputSize])\n+        // Then transposed to match DenseLayer format [outputSize x inputSize]\n+\n+        // So we need: W = (A * B)^T + W_residual\n+        // Therefore: W_residual = W - (A * B)^T\n+\n+        Matrix<T> lowRankApprox = matrixA.Multiply(matrixB); // [inputSize x rank] * [rank x outputSize] = [inputSize x outputSize]\n+        Matrix<T> lowRankApproxTransposed = lowRankApprox.Transpose(); // [outputSize x inputSize]\n+\n+        // Compute residual: W_residual = W - (B*A approximation)\n+        _residualWeights = new Matrix<T>(outputSize, inputSize);\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                _residualWeights[i, j] = NumOps.Subtract(pretrainedWeights[i, j], lowRankApproxTransposed[i, j]);\n+            }\n+        }\n+\n+        // Set the LoRA layer's A and B matrices\n+        // Note: LoRALayer expects A: [inputSize x rank], B: [rank x outputSize]\n+        Vector<T> loraParams = new Vector<T>(_loraLayer.ParameterCount);\n+        int idx = 0;\n+\n+        // Pack matrix A\n+        for (int i = 0; i < matrixA.Rows; i++)\n+        {\n+            for (int j = 0; j < matrixA.Columns; j++)\n+            {\n+                loraParams[idx++] = matrixA[i, j];\n+            }\n+        }\n+\n+        // Pack matrix B\n+        for (int i = 0; i < matrixB.Rows; i++)\n+        {\n+            for (int j = 0; j < matrixB.Columns; j++)\n+            {\n+                loraParams[idx++] = matrixB[i, j];\n+            }\n+        }\n+\n+        _loraLayer.SetParameters(loraParams);\n+        _initializedFromSVD = true;\n+    }\n+\n+    /// <summary>\n+    /// Creates a PiSSA adapter initialized from SVD of pretrained weights.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with PiSSA.</param>\n+    /// <param name=\"pretrainedWeights\">The pretrained weight matrix to decompose.</param>\n+    /// <param name=\"rank\">The rank of the low-rank decomposition.</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <param name=\"svdAlgorithm\">The SVD algorithm to use (default: GolubReinsch).</param>\n+    /// <returns>A PiSSA adapter initialized from SVD.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This static factory method creates and fully initializes a PiSSA adapter in one step.\n+    /// It combines construction and SVD initialization for convenience.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is the recommended way to create a PiSSA adapter.\n+    ///\n+    /// Instead of:\n+    /// 1. Create adapter\n+    /// 2. Call InitializeFromSVD\n+    ///\n+    /// You can just:\n+    /// 1. Call this method with pretrained weights\n+    ///\n+    /// Example:\n+    /// var adapter = PiSSAAdapter.InitializeFromSVD(myLayer, pretrainedWeights, rank: 8);\n+    /// // Ready to train!\n+    /// </para>\n+    /// </remarks>\n+    public static PiSSAAdapter<T> InitializeFromSVD(\n+        ILayer<T> baseLayer,\n+        Matrix<T> pretrainedWeights,\n+        int rank,\n+        double alpha = -1,\n+        bool freezeBaseLayer = true,\n+        SvdAlgorithmType svdAlgorithm = SvdAlgorithmType.GolubReinsch)\n+    {\n+        PiSSAAdapter<T> adapter = new PiSSAAdapter<T>(baseLayer, rank, alpha, freezeBaseLayer);\n+        adapter.InitializeFromSVD(pretrainedWeights, svdAlgorithm);\n+        return adapter;\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass using residual weights plus trainable PiSSA adaptation.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Output tensor computed as: residual_output + lora_output.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// If initialized from SVD, the forward pass computes:\n+    /// output = W_residual * input + LoRA(input)\n+    ///\n+    /// If not initialized from SVD (falls back to standard LoRA):\n+    /// output = base_layer(input) + LoRA(input)\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This runs input through the adapter.\n+    ///\n+    /// With proper PiSSA initialization:\n+    /// - First applies frozen residual weights (the \"less important\" parts)\n+    /// - Then adds the trainable adaptation (the \"important\" parts from A and B)\n+    /// - Result combines both for the final output\n+    ///\n+    /// Without SVD initialization (not recommended):\n+    /// - Falls back to standard LoRA behavior\n+    /// - Uses base layer output + LoRA correction\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        if (!_initializedFromSVD || _residualWeights == null)\n+        {\n+            // Fall back to standard LoRA behavior if not initialized from SVD\n+            return base.Forward(input);\n+        }\n+\n+        // Get batch size and validate input shape\n+        int batchSize = input.Shape[0];\n+        int inputSize = input.Shape.Length > 1 ? input.Shape[1] : input.Length;\n+\n+        if (inputSize != _residualWeights.Columns)\n+        {\n+            throw new ArgumentException(\n+                $\"Input size {inputSize} does not match residual weights columns {_residualWeights.Columns}\",\n+                nameof(input));\n+        }\n+\n+        // Convert input to matrix [batchSize, inputSize]\n+        Matrix<T> inputMatrix = new Matrix<T>(batchSize, inputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                inputMatrix[i, j] = input[i * inputSize + j];\n+            }\n+        }\n+\n+        // Compute residual output: W_residual * input^T -> [batchSize, outputSize]\n+        Matrix<T> residualOutput = inputMatrix.Multiply(_residualWeights.Transpose());\n+\n+        // Compute LoRA output\n+        Tensor<T> loraOutput = _loraLayer.Forward(input);\n+\n+        // Sum the outputs\n+        int outputSize = _residualWeights.Rows;\n+        Tensor<T> result = new Tensor<T>(new[] { batchSize, outputSize });\n+\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                int idx = i * outputSize + j;\n+                result[idx] = NumOps.Add(residualOutput[i, j], loraOutput[idx]);\n+            }\n+        }\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Performs the backward pass, updating only the trainable adapter matrices (B and A).\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient flowing back from the next layer.</param>\n+    /// <returns>Gradient to pass to the previous layer.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The backward pass propagates gradients through both the frozen residual path and the\n+    /// trainable LoRA path. However, only the LoRA parameters (A and B) are updated;\n+    /// the residual weights remain frozen.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is where learning happens in PiSSA.\n+    ///\n+    /// During backpropagation:\n+    /// - Gradients flow through both the residual path and the LoRA path\n+    /// - But only the LoRA matrices (A and B) get updated\n+    /// - The residual weights stay frozen (no learning)\n+    ///\n+    /// This is the key to PiSSA's efficiency:\n+    /// - We only train the top-r most important components\n+    /// - The rest of the weights stay fixed from pretraining\n+    /// - Fewer parameters to update = faster training and less overfitting\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (!_initializedFromSVD || _residualWeights == null)\n+        {\n+            // Fall back to standard LoRA behavior if not initialized from SVD\n+            return base.Backward(outputGradient);\n+        }\n+\n+        // Backward through LoRA layer (this updates LoRA gradients)\n+        Tensor<T> loraInputGrad = _loraLayer.Backward(outputGradient);\n+\n+        // Backward through frozen residual weights (no parameter updates, just input gradients)\n+        int batchSize = outputGradient.Shape[0];\n+        int outputSize = _residualWeights.Rows;\n+        int inputSize = _residualWeights.Columns;\n+\n+        // Convert output gradient to matrix [batchSize, outputSize]\n+        Matrix<T> gradMatrix = new Matrix<T>(batchSize, outputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                gradMatrix[i, j] = outputGradient[i * outputSize + j];\n+            }\n+        }\n+\n+        // Compute input gradient for residual path: grad * W_residual\n+        Matrix<T> residualInputGrad = gradMatrix.Multiply(_residualWeights);\n+\n+        // Sum input gradients from both paths\n+        Tensor<T> inputGrad = new Tensor<T>(new[] { batchSize, inputSize });\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                int idx = i * inputSize + j;\n+                inputGrad[idx] = NumOps.Add(loraInputGrad[idx], residualInputGrad[i, j]);\n+            }\n+        }\n+\n+        // Update parameter gradients vector (only LoRA parameters, since base is frozen and residual is frozen)\n+        ParameterGradients = _loraLayer.GetParameterGradients();\n+\n+        return inputGrad;\n+    }\n+\n+    /// <summary>\n+    /// Merges the PiSSA adaptation into the original layer.\n+    /// </summary>\n+    /// <returns>A new layer with PiSSA weights merged back into a single weight matrix.</returns>\n+    /// <exception cref=\"InvalidOperationException\">Thrown when the adapter was not initialized from SVD.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// This method reconstructs the full weight matrix by combining:\n+    /// W_merged = W_residual + (A * B)^T\n+    ///\n+    /// This allows you to deploy the adapted model without the PiSSA overhead.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This \"bakes in\" the PiSSA adaptation.\n+    ///\n+    /// After training:\n+    /// - You have: frozen residual weights + trained A and B matrices\n+    /// - Merging combines them: residual + A*B = final weights\n+    /// - Result: a single regular layer with all improvements included\n+    ///\n+    /// Benefits:\n+    /// - Faster inference (no need to compute residual + LoRA separately)\n+    /// - Simpler deployment (just one layer)\n+    /// - Compatible with systems that don't support LoRA/PiSSA\n+    ///\n+    /// Example:\n+    /// var mergedLayer = adapter.MergeToOriginalLayer();\n+    /// // Now you have a standard layer with PiSSA improvements built in!\n+    /// </para>\n+    /// </remarks>\n+    public override ILayer<T> MergeToOriginalLayer()\n+    {\n+        if (!_initializedFromSVD || _residualWeights == null)\n+        {\n+            throw new InvalidOperationException(\n+                \"Cannot merge PiSSA adapter that was not initialized from SVD. \" +\n+                \"Call InitializeFromSVD before merging.\");\n+        }\n+\n+        // Get the LoRA weight contribution: (A * B)^T\n+        Matrix<T> loraWeights = _loraLayer.MergeWeights(); // Already transposed\n+\n+        // Merge: W_final = W_residual + LoRA_weights\n+        int outputSize = _residualWeights.Rows;\n+        int inputSize = _residualWeights.Columns;\n+\n+        Matrix<T> mergedWeights = new Matrix<T>(outputSize, inputSize);\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                mergedWeights[i, j] = NumOps.Add(_residualWeights[i, j], loraWeights[i, j]);\n+            }\n+        }\n+\n+        // Create parameters vector: [merged weights, biases]\n+        // Get biases from base layer\n+        Vector<T> baseParams = _baseLayer.GetParameters();\n+        int weightCount = outputSize * inputSize;\n+        int biasCount = baseParams.Length - weightCount;\n+\n+        Vector<T> mergedParams = new Vector<T>(weightCount + biasCount);\n+\n+        // Pack merged weights\n+        int idx = 0;\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                mergedParams[idx++] = mergedWeights[i, j];\n+            }\n+        }\n+\n+        // Copy biases unchanged\n+        for (int i = weightCount; i < baseParams.Length; i++)\n+        {\n+            mergedParams[idx++] = baseParams[i];\n+        }\n+\n+        // Create a new dense layer with merged parameters\n+        DenseLayer<T> mergedLayer = new DenseLayer<T>(inputSize, outputSize, (IActivationFunction<T>?)null);\n+        mergedLayer.SetParameters(mergedParams);\n+\n+        return mergedLayer;\n+    }\n+}","path":"src/LoRA/Adapters/PiSSAAdapter.cs","commit_id":"fa815036ee570162056ba6aca773f34c80f36948","original_commit_id":"8d5a1b6564c1a2bc1b334f1863e6003a6fd142b2","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**Implementation is mathematically sound despite documentation inconsistencies.**\n\nThe PiSSA adapter implementation correctly:\n- Performs SVD decomposition and extracts top-r components\n- Initializes matrices A and B from singular vectors and values\n- Computes the residual matrix `W_residual = W - (A*B)^T`\n- Implements forward pass as `output = W_residual * input + LoRA(input)`\n- Propagates gradients while updating only LoRA parameters\n- Merges adapted weights for deployment\n\nAll matrix dimensions and transpositions are correct throughout forward, backward, and merge operations.\n\nHowever, the major issue is that documentation comments throughout the file (lines 25-27, 212, 219, 223) incorrectly describe the matrix dimensions as `A = V_r^T` and `B = U_r Œ£_r`, when the actual implementation creates `A = V_r` and `B = Œ£_r * U_r^T`. This mismatch will cause confusion for future maintainers.\n\nBased on the PR objectives noting this will be redesigned, recommend correcting the documentation before any production use.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/LoRA/Adapters/PiSSAAdapter.cs (around lines 25-27, 212, 219, 223 within\nthe 1-566 file), fix the inaccurate documentation about how A and B are\nconstructed: the code builds A as V_r (shape [inputSize x r]) and B as Œ£_r *\nU_r^T (shape [r x outputSize]), and the low-rank weight contribution is (A *\nB)^T; update the XML/summary comments to state these shapes and expressions\n(replace \"A = V_r^T\" and \"B = U_r Œ£_r\" with \"A = V_r (inputSize x r)\" and \"B =\nŒ£_r * U_r^T (r x outputSize)\", and clarify W_residual = W - (A*B)^T and forward\nas output = W_residual * input + LoRA(input)); make analogous wording fixes at\neach referenced line to remove confusion for future maintainers.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T03:02:30Z","updated_at":"2025-11-02T03:02:32Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140326","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140326"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140326"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140326/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":1,"original_start_line":1,"start_side":"RIGHT","line":566,"original_line":566,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":566,"position":566,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140328","pull_request_review_id":3408046172,"id":2484140328,"node_id":"PRRC_kwDOKSXUF86UEPko","diff_hunk":"@@ -0,0 +1,566 @@\n+using AiDotNet.DecompositionMethods.MatrixDecomposition;\n+using AiDotNet.Enums.AlgorithmTypes;\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// Principal Singular Values and Singular Vectors Adaptation (PiSSA) adapter for parameter-efficient fine-tuning.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// PiSSA (NeurIPS 2024 Spotlight) improves upon standard LoRA by initializing adapter matrices with\n+/// principal components from Singular Value Decomposition (SVD) of pretrained weights, rather than\n+/// random initialization. This results in more effective use of the rank budget and faster convergence.\n+/// </para>\n+/// <para><b>Key Differences from Standard LoRA:</b>\n+/// - Standard LoRA: A initialized randomly, B initialized to zero\n+/// - PiSSA: A and B initialized from top-r singular vectors of pretrained weights\n+/// - Standard LoRA: All weights trainable\n+/// - PiSSA: Residual weights frozen, only top-r components trainable\n+/// </para>\n+/// <para><b>How PiSSA Works:</b>\n+/// 1. Perform SVD on pretrained weights: W = U Œ£ V^T\n+/// 2. Initialize adapter matrices from top-r components:\n+///    - A = V_r^T (top-r right singular vectors)\n+///    - B = U_r Œ£_r (top-r left singular vectors scaled by singular values)","path":"src/LoRA/Adapters/PiSSAAdapter.cs","commit_id":"fa815036ee570162056ba6aca773f34c80f36948","original_commit_id":"8d5a1b6564c1a2bc1b334f1863e6003a6fd142b2","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**Misleading documentation: matrix dimension descriptions don't match implementation.**\n\nThe documentation states:\n- `A = V_r^T` (top-r right singular vectors)\n- `B = U_r Œ£_r` (top-r left singular vectors scaled by singular values)\n\nHowever, the actual implementation (lines 214-234) creates:\n- `A = V_r` (dimensions [inputSize √ó rank], the first r columns of V)\n- `B = Œ£_r * U_r^T` (dimensions [rank √ó outputSize], scaled and transposed)\n\nWhile the implementation is mathematically correct (verified that `(A*B)^T = U_r * Œ£_r * V_r^T` equals the rank-r approximation), the mismatch between documentation and code will confuse maintainers trying to understand or debug the SVD initialization.\n\n\n\nApply this diff to correct the documentation:\n\n```diff\n 2. Initialize adapter matrices from top-r components:\n-   - A = V_r^T (top-r right singular vectors)\n-   - B = U_r Œ£_r (top-r left singular vectors scaled by singular values)\n+   - A = V_r (top-r right singular vectors, dimensions: inputSize √ó rank)\n+   - B = Œ£_r * U_r^T (top-r left singular vectors scaled by singular values, dimensions: rank √ó outputSize)\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/LoRA/Adapters/PiSSAAdapter.cs around lines 25 to 27, the doc comment\nincorrectly states A = V_r^T and B = U_r Œ£_r; update it to match the\nimplementation by describing A as V_r (dimensions [inputSize √ó rank], the first\nr columns of V) and B as Œ£_r * U_r^T (dimensions [rank √ó outputSize], i.e., the\ndiagonal matrix of top-r singular values multiplied by U_r transposed), so the\ndocumentation accurately reflects the matrices created in lines 214‚Äì234.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T03:02:30Z","updated_at":"2025-11-02T03:02:32Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140328","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140328"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140328"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140328/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":25,"original_start_line":25,"start_side":"RIGHT","line":27,"original_line":27,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":27,"position":27,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140330","pull_request_review_id":3408046172,"id":2484140330,"node_id":"PRRC_kwDOKSXUF86UEPkq","diff_hunk":"@@ -0,0 +1,566 @@\n+using AiDotNet.DecompositionMethods.MatrixDecomposition;\n+using AiDotNet.Enums.AlgorithmTypes;\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// Principal Singular Values and Singular Vectors Adaptation (PiSSA) adapter for parameter-efficient fine-tuning.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// PiSSA (NeurIPS 2024 Spotlight) improves upon standard LoRA by initializing adapter matrices with\n+/// principal components from Singular Value Decomposition (SVD) of pretrained weights, rather than\n+/// random initialization. This results in more effective use of the rank budget and faster convergence.\n+/// </para>\n+/// <para><b>Key Differences from Standard LoRA:</b>\n+/// - Standard LoRA: A initialized randomly, B initialized to zero\n+/// - PiSSA: A and B initialized from top-r singular vectors of pretrained weights\n+/// - Standard LoRA: All weights trainable\n+/// - PiSSA: Residual weights frozen, only top-r components trainable\n+/// </para>\n+/// <para><b>How PiSSA Works:</b>\n+/// 1. Perform SVD on pretrained weights: W = U Œ£ V^T\n+/// 2. Initialize adapter matrices from top-r components:\n+///    - A = V_r^T (top-r right singular vectors)\n+///    - B = U_r Œ£_r (top-r left singular vectors scaled by singular values)\n+/// 3. Freeze residual matrix: W_residual = W - B*A\n+/// 4. During training: output = W_residual * input + B*A*input\n+/// 5. Only B and A are updated; W_residual stays frozen\n+/// </para>\n+/// <para><b>Performance Benefits:</b>\n+/// PiSSA achieves superior performance compared to standard LoRA:\n+/// - GSM8K benchmark: 72.86% (PiSSA) vs 67.7% (LoRA)\n+/// - Better initialization captures important pretrained knowledge\n+/// - More effective gradient updates from the start\n+/// - Faster convergence with fewer training steps\n+/// </para>\n+/// <para><b>For Beginners:</b> Think of PiSSA as \"smart LoRA initialization\".\n+///\n+/// Standard LoRA starts from random:\n+/// - Random A matrix (like throwing darts blindfolded)\n+/// - Zero B matrix (starts with no effect)\n+/// - Learns everything from scratch\n+///\n+/// PiSSA starts from the most important parts of pretrained weights:\n+/// - A and B capture the top-r \"principal directions\" of the pretrained model\n+/// - Starts closer to the optimal solution\n+/// - Like starting a puzzle with the border pieces already connected\n+///\n+/// Example: If you have a pretrained language model with a 4096x4096 weight matrix,\n+/// PiSSA with rank=8 will:\n+/// 1. Find the top 8 most important patterns in those weights via SVD\n+/// 2. Put those patterns into A and B (making them trainable)\n+/// 3. Freeze the remaining \"less important\" patterns\n+/// 4. Train only the top 8 patterns to adapt to your task\n+///\n+/// This is much more efficient than starting from random and achieves better results!\n+/// </para>\n+/// <para><b>References:</b>\n+/// - Paper: \"PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models\"\n+/// - Venue: NeurIPS 2024 (Spotlight)\n+/// - Key Insight: SVD-based initialization > random initialization for low-rank adaptation\n+/// </para>\n+/// </remarks>\n+public class PiSSAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// The frozen residual weights after removing top-r principal components.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This matrix represents W_residual = W - B*A, where W is the original pretrained weights\n+    /// and B*A is the top-r rank approximation. During training, this matrix remains frozen\n+    /// while only the adapter matrices (A and B) are updated.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is the \"leftover\" part of the original weights.\n+    ///\n+    /// Think of the original weights as a complete picture:\n+    /// - The top-r components (in A and B) capture the main features\n+    /// - The residual is what's left after removing those main features\n+    /// - During training, we keep this residual fixed and only adjust the main features\n+    ///\n+    /// This is like keeping the background of a photo fixed while adjusting only the main subject.\n+    /// </para>\n+    /// </remarks>\n+    private Matrix<T>? _residualWeights;\n+\n+    /// <summary>\n+    /// Indicates whether the adapter was initialized from SVD of pretrained weights.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// When true, this adapter was properly initialized using PiSSA's SVD-based initialization.\n+    /// When false, it falls back to standard LoRA random initialization (not recommended for PiSSA).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This flag tells you if the adapter is using PiSSA's smart initialization.\n+    ///\n+    /// True = properly initialized with SVD (recommended)\n+    /// False = using random initialization like standard LoRA (loses PiSSA benefits)\n+    /// </para>\n+    /// </remarks>\n+    private bool _initializedFromSVD;\n+\n+    /// <summary>\n+    /// Gets the frozen residual weights matrix.\n+    /// </summary>\n+    /// <remarks>\n+    /// This matrix is computed during SVD initialization and remains frozen during training.\n+    /// Returns null if SVD initialization was not performed.\n+    /// </remarks>\n+    public Matrix<T>? ResidualWeights => _residualWeights?.Clone();\n+\n+    /// <summary>\n+    /// Gets whether this adapter was initialized from SVD.\n+    /// </summary>\n+    /// <remarks>\n+    /// Returns true if InitializeFromSVD was called successfully, false otherwise.\n+    /// </remarks>\n+    public bool InitializedFromSVD => _initializedFromSVD;\n+\n+    /// <summary>\n+    /// Initializes a new PiSSA adapter wrapping an existing layer.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with PiSSA.</param>\n+    /// <param name=\"rank\">The rank of the low-rank decomposition.</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// This constructor creates a PiSSA adapter. After construction, you should call\n+    /// InitializeFromSVD to properly initialize the adapter matrices from pretrained weights.\n+    /// Without SVD initialization, the adapter behaves like standard LoRA (not recommended).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This creates a PiSSA adapter for any layer type.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to adapt (Dense, Convolutional, etc.)\n+    /// - rank: How many principal components to use (typically 4-32)\n+    /// - alpha: Scaling factor for the adaptation strength\n+    /// - freezeBaseLayer: Usually true to freeze original weights\n+    ///\n+    /// Important: After creating the adapter, call InitializeFromSVD with the pretrained\n+    /// weights to get PiSSA's performance benefits. Otherwise, it's just regular LoRA.\n+    /// </para>\n+    /// </remarks>\n+    public PiSSAAdapter(ILayer<T> baseLayer, int rank, double alpha = -1, bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        _initializedFromSVD = false;\n+    }\n+\n+    /// <summary>\n+    /// Initializes the adapter matrices from SVD of pretrained weights.\n+    /// </summary>\n+    /// <param name=\"pretrainedWeights\">The pretrained weight matrix to decompose.</param>\n+    /// <param name=\"svdAlgorithm\">The SVD algorithm to use (default: GolubReinsch).</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when pretrainedWeights is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when weight matrix dimensions don't match layer dimensions.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// This method performs the core PiSSA initialization:\n+    /// 1. Computes SVD: W = U Œ£ V^T\n+    /// 2. Extracts top-r components: U_r, Œ£_r, V_r\n+    /// 3. Initializes A = V_r^T (right singular vectors)\n+    /// 4. Initializes B = U_r Œ£_r (left singular vectors scaled by singular values)\n+    /// 5. Computes residual: W_residual = W - B*A\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is where the magic happens!\n+    ///\n+    /// The method:\n+    /// 1. Takes your pretrained weights (like from a large language model)\n+    /// 2. Finds the most important patterns using SVD (mathematical technique)\n+    /// 3. Puts those patterns into the adapter matrices A and B\n+    /// 4. Saves the \"leftover\" patterns as frozen residual weights\n+    ///\n+    /// Think of it like:\n+    /// - Original weights = complete painting\n+    /// - SVD = identifying the main strokes vs. minor details\n+    /// - A and B = the main strokes (what we'll adjust)\n+    /// - Residual = the minor details (kept frozen)\n+    ///\n+    /// This initialization is what makes PiSSA better than LoRA - it starts from\n+    /// a smart place instead of random values.\n+    /// </para>\n+    /// </remarks>\n+    public void InitializeFromSVD(Matrix<T> pretrainedWeights, SvdAlgorithmType svdAlgorithm = SvdAlgorithmType.GolubReinsch)\n+    {\n+        if (pretrainedWeights == null)\n+        {\n+            throw new ArgumentNullException(nameof(pretrainedWeights));\n+        }\n+\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+\n+        if (pretrainedWeights.Rows != outputSize || pretrainedWeights.Columns != inputSize)\n+        {\n+            throw new ArgumentException(\n+                $\"Weight matrix dimensions ({pretrainedWeights.Rows}x{pretrainedWeights.Columns}) \" +\n+                $\"do not match layer dimensions ({outputSize}x{inputSize})\",\n+                nameof(pretrainedWeights));\n+        }\n+\n+        // Perform SVD: W = U Œ£ V^T\n+        SvdDecomposition<T> svd = new SvdDecomposition<T>(pretrainedWeights, svdAlgorithm);\n+\n+        // Extract top-r singular values and vectors\n+        int r = Rank;\n+\n+        // Create A matrix from top-r right singular vectors: A = V_r^T\n+        // V^T has dimensions (inputSize x inputSize), we take first r rows\n+        Matrix<T> matrixA = new Matrix<T>(inputSize, r);\n+        for (int i = 0; i < inputSize; i++)\n+        {\n+            for (int j = 0; j < r; j++)\n+            {\n+                matrixA[i, j] = svd.Vt[j, i]; // Transpose: V_r^T\n+            }\n+        }\n+\n+        // Create B matrix from top-r left singular vectors scaled by singular values: B = U_r Œ£_r\n+        // U has dimensions (outputSize x outputSize), we take first r columns\n+        // Œ£ is diagonal, so we scale each column of U_r by the corresponding singular value\n+        Matrix<T> matrixB = new Matrix<T>(r, outputSize);\n+        for (int i = 0; i < r; i++)\n+        {\n+            T singularValue = svd.S[i];\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                matrixB[i, j] = NumOps.Multiply(svd.U[j, i], singularValue);\n+            }\n+        }","path":"src/LoRA/Adapters/PiSSAAdapter.cs","commit_id":"fa815036ee570162056ba6aca773f34c80f36948","original_commit_id":"8d5a1b6564c1a2bc1b334f1863e6003a6fd142b2","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**Inline comments don't match actual matrix dimensions.**\n\nLine 212 states `A = V_r^T`, but the code creates `A = V_r` (by transposing the first r rows of `svd.Vt` to get the first r columns of V).\n\nLine 223 states `B = U_r Œ£_r`, but the code creates `B = Œ£_r * U_r^T` (by extracting, scaling, and transposing the first r columns of U).\n\nLine 219's comment `// Transpose: V_r^T` is also misleading‚Äîthe result is `V_r`, not `V_r^T`.\n\nWhile the implementation is mathematically correct, these comments will mislead developers trying to understand the initialization logic.\n\n\n\nApply this diff to correct the comments:\n\n```diff\n-    // Create A matrix from top-r right singular vectors: A = V_r^T\n-    // V^T has dimensions (inputSize x inputSize), we take first r rows\n+    // Create A matrix from top-r right singular vectors: A = V_r\n+    // V^T has dimensions (inputSize x inputSize), we take first r rows and transpose to get V_r\n     Matrix<T> matrixA = new Matrix<T>(inputSize, r);\n     for (int i = 0; i < inputSize; i++)\n     {\n         for (int j = 0; j < r; j++)\n         {\n-            matrixA[i, j] = svd.Vt[j, i]; // Transpose: V_r^T\n+            matrixA[i, j] = svd.Vt[j, i]; // Result: V_r (dimensions: inputSize √ó rank)\n         }\n     }\n\n-    // Create B matrix from top-r left singular vectors scaled by singular values: B = U_r Œ£_r\n-    // U has dimensions (outputSize x outputSize), we take first r columns\n-    // Œ£ is diagonal, so we scale each column of U_r by the corresponding singular value\n+    // Create B matrix from top-r left singular vectors scaled by singular values: B = Œ£_r * U_r^T  \n+    // U has dimensions (outputSize x outputSize), we take first r columns, scale, and transpose\n     Matrix<T> matrixB = new Matrix<T>(r, outputSize);\n     for (int i = 0; i < r; i++)\n     {\n         T singularValue = svd.S[i];\n         for (int j = 0; j < outputSize; j++)\n         {\n-            matrixB[i, j] = NumOps.Multiply(svd.U[j, i], singularValue);\n+            matrixB[i, j] = NumOps.Multiply(svd.U[j, i], singularValue); // Result: Œ£_r * U_r^T (dimensions: rank √ó outputSize)\n         }\n     }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        // Create A matrix from top-r right singular vectors: A = V_r\n        // V^T has dimensions (inputSize x inputSize), we take first r rows and transpose to get V_r\n        Matrix<T> matrixA = new Matrix<T>(inputSize, r);\n        for (int i = 0; i < inputSize; i++)\n        {\n            for (int j = 0; j < r; j++)\n            {\n                matrixA[i, j] = svd.Vt[j, i]; // Result: V_r (dimensions: inputSize √ó rank)\n            }\n        }\n\n        // Create B matrix from top-r left singular vectors scaled by singular values: B = Œ£_r * U_r^T  \n        // U has dimensions (outputSize x outputSize), we take first r columns, scale, and transpose\n        Matrix<T> matrixB = new Matrix<T>(r, outputSize);\n        for (int i = 0; i < r; i++)\n        {\n            T singularValue = svd.S[i];\n            for (int j = 0; j < outputSize; j++)\n            {\n                matrixB[i, j] = NumOps.Multiply(svd.U[j, i], singularValue); // Result: Œ£_r * U_r^T (dimensions: rank √ó outputSize)\n            }\n        }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/LoRA/Adapters/PiSSAAdapter.cs around lines 212 to 234, the inline\ncomments incorrectly describe the matrix shapes/transposes: A is built as V_r\n(not V_r^T) and B is built as Œ£_r * U_r^T (not U_r Œ£_r). Update the comments to\naccurately state that matrixA is constructed from the first r columns of V\n(V_r), and that matrixB is constructed as the diagonal singular values scaled\ninto U_r^T (i.e., Œ£_r * U_r^T), and remove or correct the misleading \"//\nTranspose: V_r^T\" note so comments match the implemented dimensions and\noperations.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T03:02:30Z","updated_at":"2025-11-02T03:02:32Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140330","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140330"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140330"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140330/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":212,"original_start_line":212,"start_side":"RIGHT","line":234,"original_line":234,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":234,"position":234,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140333","pull_request_review_id":3408046172,"id":2484140333,"node_id":"PRRC_kwDOKSXUF86UEPkt","diff_hunk":"@@ -0,0 +1,827 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// RoSA (Robust Adaptation) adapter for parameter-efficient fine-tuning with improved robustness to distribution shifts.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// RoSA (Robust Adaptation) extends standard LoRA by combining two complementary components:\n+/// 1. Low-rank component (standard LoRA): Captures common, structured patterns in adaptations\n+/// 2. Sparse component: Captures specific, rare, or outlier patterns that low-rank cannot represent\n+/// </para>\n+/// <para>\n+/// <b>Mathematical Formulation:</b>\n+/// Given input x and pre-trained weights W, RoSA computes:\n+/// - Low-rank component: L = (alpha/rank) * B * A * x\n+/// - Sparse component: S = W_sparse * x (where W_sparse is highly sparse)\n+/// - Final output: y = W*x + L + S\n+///\n+/// The sparse component is maintained through magnitude-based pruning, keeping only the\n+/// most significant weights and zeroing out the rest. This creates a sparse matrix that\n+/// captures specific patterns while remaining parameter-efficient.\n+/// </para>\n+/// <para>\n+/// <b>Research Context:</b>\n+/// RoSA was introduced in January 2024 as a robust alternative to standard LoRA.\n+/// The key insight is that low-rank approximations work well for common patterns but\n+/// struggle with distribution shifts and rare patterns. By adding a sparse component,\n+/// RoSA can capture outliers and domain-specific patterns without significantly\n+/// increasing parameter count.\n+///\n+/// In experiments on domain adaptation tasks, RoSA showed:\n+/// - Better generalization to new domains (+5-10% over standard LoRA)\n+/// - More robust to distribution shifts\n+/// - Ability to capture both global patterns (low-rank) and local exceptions (sparse)\n+/// - Only modest increase in parameters (typically 5-15% more than pure LoRA)\n+/// </para>\n+/// <para>\n+/// <b>For Beginners:</b> RoSA is like LoRA with a safety net for unusual cases.\n+///\n+/// Think of it this way:\n+/// - Low-rank LoRA is like learning general rules (\"most images of cats have pointed ears\")\n+/// - Sparse component is like remembering specific exceptions (\"this one cat breed has round ears\")\n+/// - Together they make a robust model that handles both common and rare cases\n+///\n+/// Why RoSA is more robust:\n+/// - Low-rank component: Efficient for common patterns across domains\n+/// - Sparse component: Handles outliers and domain-specific quirks\n+/// - Result: Better performance when test data differs from training data\n+///\n+/// When to use RoSA over standard LoRA:\n+/// - When you expect distribution shifts (train on news, test on social media)\n+/// - When your data has outliers or rare patterns that matter\n+/// - When you need robustness more than absolute parameter efficiency\n+/// - When adapting to multiple related but distinct domains\n+///\n+/// Trade-offs vs standard LoRA:\n+/// + More robust to distribution shifts\n+/// + Better handles rare patterns\n+/// + More flexible adaptation\n+/// - Slightly more parameters (sparse component adds ~5-15%)\n+/// - Slightly more computation (extra sparse matrix multiply)\n+/// - Requires tuning sparsity ratio\n+/// </para>\n+/// <para>\n+/// <b>Reference:</b>\n+/// \"RoSA: Robust Adaptation through Sparse Regularization\"\n+/// January 2024\n+/// </para>\n+/// </remarks>\n+public class RoSAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Sparse weight matrix that captures specific/rare patterns.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This matrix has the same dimensions as the base layer's weights but is highly sparse\n+    /// (typically 90-99% zeros). It's maintained through magnitude-based pruning during training.\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This is the \"exception handler\" of RoSA.\n+    /// Most of its values are zero, but the few non-zero values capture specific patterns\n+    /// that the low-rank component can't represent efficiently.\n+    /// </para>\n+    /// </remarks>\n+    private Matrix<T> _sparseWeights;\n+\n+    /// <summary>\n+    /// Gradients for the sparse weight component, computed during backpropagation.\n+    /// </summary>\n+    private Matrix<T>? _sparseGradients;\n+\n+    /// <summary>\n+    /// Threshold for magnitude-based pruning of sparse weights.\n+    /// Weights with magnitude below this threshold are set to zero.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This threshold controls the sparsity of the sparse component. Lower values\n+    /// result in more non-zero weights (less sparse), higher values result in\n+    /// fewer non-zero weights (more sparse).\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This is like a \"minimum importance\" cutoff.\n+    /// If a weight's importance is below this value, we zero it out to maintain\n+    /// sparsity. Typical values: 0.001 to 0.1\n+    /// </para>\n+    /// </remarks>\n+    public double SparseThreshold { get; set; }\n+\n+    /// <summary>\n+    /// Target sparsity ratio (fraction of zeros in sparse component).\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This value controls how sparse the sparse component should be.\n+    /// - 0.0 = no sparsity (all weights can be non-zero)\n+    /// - 0.5 = 50% of weights are zero\n+    /// - 0.95 = 95% of weights are zero (very sparse)\n+    /// - 0.99 = 99% of weights are zero (extremely sparse)\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This is the target percentage of zeros we want.\n+    /// Higher values (like 0.95) mean fewer non-zero weights, which keeps the\n+    /// model efficient. Lower values mean more flexibility but more parameters.\n+    ///\n+    /// Typical values:\n+    /// - 0.90 (90% zeros): More flexible, for complex domains\n+    /// - 0.95 (95% zeros): Good balance (recommended starting point)\n+    /// - 0.99 (99% zeros): Very efficient, for simple adaptations\n+    /// </para>\n+    /// </remarks>\n+    public double SparsityRatio { get; set; }\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// RoSA parameters include:\n+    /// - Base layer parameters (if not frozen)\n+    /// - LoRA parameters (rank * (inputSize + outputSize))\n+    /// - Non-zero sparse parameters (varies based on sparsity)\n+    ///\n+    /// For parameter counting, we report the full sparse matrix size, but in practice\n+    /// only the non-zero elements need to be stored and updated.\n+    /// </para>\n+    /// </remarks>\n+    public override int ParameterCount\n+    {\n+        get\n+        {\n+            int baseCount = _freezeBaseLayer ? 0 : _baseLayer.ParameterCount;\n+            int loraCount = _loraLayer.ParameterCount;\n+            int sparseCount = _sparseWeights.Rows * _sparseWeights.Columns;\n+            return baseCount + loraCount + sparseCount;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Initializes a new RoSA adapter wrapping an existing layer.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with RoSA.</param>\n+    /// <param name=\"rank\">The rank of the low-rank LoRA decomposition.</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"sparsityRatio\">Target sparsity ratio (0.0 to 1.0, typically 0.9-0.99).</param>\n+    /// <param name=\"sparseThreshold\">Magnitude threshold for pruning sparse weights (typically 0.001-0.1).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when sparsityRatio is not between 0 and 1.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// The constructor initializes the RoSA adapter by:\n+    /// 1. Setting up the standard LoRA components (via base constructor)\n+    /// 2. Initializing the sparse weight matrix (starts with small random values)\n+    /// 3. Applying initial pruning to enforce sparsity\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This creates a RoSA adapter around your existing layer.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to fine-tune efficiently and robustly\n+    /// - rank: How much compression for the low-rank component (lower = fewer parameters)\n+    /// - alpha: Scaling factor for LoRA contribution (usually equals rank)\n+    /// - sparsityRatio: How sparse the sparse component should be (0.95 = 95% zeros)\n+    /// - sparseThreshold: Minimum importance for keeping a sparse weight (0.01 is typical)\n+    /// - freezeBaseLayer: Usually true - we only train LoRA + sparse, not base weights\n+    ///\n+    /// Example: For a 1000x1000 layer with rank=8 and sparsityRatio=0.95:\n+    /// - Base layer: 1,000,000 parameters (frozen)\n+    /// - LoRA: 16,000 parameters (8 * (1000 + 1000))\n+    /// - Sparse: ~50,000 parameters (5% of 1,000,000)\n+    /// - Total trainable: ~66,000 parameters (vs 1M for full fine-tuning!)\n+    /// </para>\n+    /// </remarks>\n+    public RoSAAdapter(\n+        ILayer<T> baseLayer,\n+        int rank,\n+        double alpha = -1,\n+        double sparsityRatio = 0.95,\n+        double sparseThreshold = 0.01,\n+        bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        if (sparsityRatio < 0.0 || sparsityRatio >= 1.0)\n+        {\n+            throw new ArgumentException(\"Sparsity ratio must be between 0.0 and 1.0 (exclusive of 1.0)\", nameof(sparsityRatio));\n+        }\n+\n+        SparsityRatio = sparsityRatio;\n+        SparseThreshold = sparseThreshold;\n+\n+        // Initialize sparse weights\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        _sparseWeights = new Matrix<T>(outputSize, inputSize);\n+\n+        // Initialize with small random values (will be pruned)\n+        InitializeSparseWeights();\n+\n+        // Apply initial pruning to enforce sparsity\n+        PruneSparseWeights();\n+\n+        // Update parameters to include sparse component\n+        Parameters = new Vector<T>(ParameterCount);\n+        UpdateParametersFromComponents();\n+    }\n+\n+    /// <summary>\n+    /// Initializes sparse weights with small random values.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// The sparse weights are initialized with small random values drawn from a\n+    /// normal distribution with standard deviation 0.01. These values will be\n+    /// pruned based on magnitude to enforce sparsity.\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This gives the sparse component a random starting point.\n+    /// Most of these values will be pruned (set to zero) immediately, but this\n+    /// initialization ensures we start with a diverse set of potential patterns.\n+    /// </para>\n+    /// </remarks>\n+    private void InitializeSparseWeights()\n+    {\n+        Random random = new Random();\n+        for (int i = 0; i < _sparseWeights.Rows; i++)\n+        {\n+            for (int j = 0; j < _sparseWeights.Columns; j++)\n+            {\n+                // Small random initialization\n+                double value = random.NextGaussian(0.0, 0.01);\n+                _sparseWeights[i, j] = NumOps.FromDouble(value);\n+            }\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Prunes sparse weights based on magnitude to maintain target sparsity.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This method implements magnitude-based pruning:\n+    /// 1. Computes magnitude of all sparse weights\n+    /// 2. Determines threshold based on target sparsity ratio\n+    /// 3. Sets weights below threshold to zero\n+    ///\n+    /// This ensures the sparse component maintains its sparsity during training.\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This is like cleaning up the sparse component.\n+    ///\n+    /// We keep only the most important weights:\n+    /// 1. Look at all the weights and their magnitudes\n+    /// 2. Sort them by importance (magnitude)\n+    /// 3. Keep the top X% (based on sparsity ratio)\n+    /// 4. Zero out the rest\n+    ///\n+    /// Example with sparsity ratio 0.95:\n+    /// - We have 1000 weights\n+    /// - We want 95% zeros (950 zeros, 50 non-zeros)\n+    /// - Keep the 50 largest magnitudes\n+    /// - Set the other 950 to zero\n+    ///\n+    /// This is called periodically during training to maintain sparsity.\n+    /// </para>\n+    /// </remarks>\n+    public void PruneSparseWeights()\n+    {\n+        int rows = _sparseWeights.Rows;\n+        int cols = _sparseWeights.Columns;\n+        int totalWeights = rows * cols;\n+\n+        // Collect magnitudes\n+        List<(int row, int col, double magnitude)> magnitudes = new List<(int, int, double)>();\n+        for (int i = 0; i < rows; i++)\n+        {\n+            for (int j = 0; j < cols; j++)\n+            {\n+                double mag = Math.Abs(Convert.ToDouble(_sparseWeights[i, j]));\n+                magnitudes.Add((i, j, mag));\n+            }\n+        }\n+\n+        // Sort by magnitude (descending)\n+        magnitudes.Sort((a, b) => b.magnitude.CompareTo(a.magnitude));\n+\n+        // Determine number of non-zero weights to keep\n+        int keepCount = (int)((1.0 - SparsityRatio) * totalWeights);\n+        keepCount = Math.Max(1, keepCount); // Keep at least one weight\n+\n+        // Also consider threshold-based pruning\n+        double adaptiveThreshold = SparseThreshold;\n+        if (keepCount < magnitudes.Count)\n+        {\n+            // Use the larger of: fixed threshold or magnitude of keepCount-th element\n+            adaptiveThreshold = Math.Max(SparseThreshold, magnitudes[keepCount].magnitude);\n+        }\n+\n+        // Apply pruning: zero out weights below threshold\n+        for (int i = 0; i < rows; i++)\n+        {\n+            for (int j = 0; j < cols; j++)\n+            {\n+                double mag = Math.Abs(Convert.ToDouble(_sparseWeights[i, j]));\n+                if (mag < adaptiveThreshold)\n+                {\n+                    _sparseWeights[i, j] = NumOps.Zero;\n+                }\n+            }\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Gets the current sparsity of the sparse component.\n+    /// </summary>\n+    /// <returns>The fraction of zeros in the sparse weight matrix (0.0 to 1.0).</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method computes the actual sparsity by counting zero and near-zero elements.\n+    /// The result can be compared to SparsityRatio to see how well pruning is working.\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This tells you what percentage of the sparse component is actually zero.\n+    ///\n+    /// If you set SparsityRatio to 0.95, this should return close to 0.95 after pruning.\n+    /// If it's much lower, you might need to adjust the threshold or pruning frequency.\n+    ///\n+    /// Example return values:\n+    /// - 0.95 = 95% zeros (good for target of 0.95)\n+    /// - 0.80 = 80% zeros (less sparse than target)\n+    /// - 0.99 = 99% zeros (more sparse than target)\n+    /// </para>\n+    /// </remarks>\n+    public double GetSparsity()\n+    {\n+        int totalWeights = _sparseWeights.Rows * _sparseWeights.Columns;\n+        int zeroCount = 0;\n+        double epsilon = 1e-10;\n+\n+        for (int i = 0; i < _sparseWeights.Rows; i++)\n+        {\n+            for (int j = 0; j < _sparseWeights.Columns; j++)\n+            {\n+                double val = Math.Abs(Convert.ToDouble(_sparseWeights[i, j]));\n+                if (val < epsilon)\n+                {\n+                    zeroCount++;\n+                }\n+            }\n+        }\n+\n+        return (double)zeroCount / totalWeights;\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass through RoSA adapter.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Output combining base layer, low-rank LoRA, and sparse components.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The RoSA forward pass computes:\n+    /// 1. Base output: y_base = base_layer(input)\n+    /// 2. LoRA output: y_lora = lora_layer(input)\n+    /// 3. Sparse output: y_sparse = input @ sparse_weights^T\n+    /// 4. Final output: y = y_base + y_lora + y_sparse\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This is where all three components work together.\n+    ///\n+    /// Think of it as three parallel processing paths:\n+    /// - Base layer: Original pre-trained knowledge (usually frozen)\n+    /// - LoRA component: Low-rank corrections for common patterns\n+    /// - Sparse component: Specific corrections for rare patterns\n+    ///\n+    /// All three outputs are added together to get the final result.\n+    /// This combination gives RoSA its robustness: the low-rank handles\n+    /// common patterns efficiently, while sparse handles outliers.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        // 1. Forward through base layer\n+        Tensor<T> baseOutput = _baseLayer.Forward(input);\n+\n+        // 2. Forward through LoRA layer (low-rank component)\n+        Tensor<T> loraOutput = _loraLayer.Forward(input);\n+\n+        // 3. Forward through sparse component\n+        // Compute: sparse_output = input @ sparse_weights^T\n+        int batchSize = input.Shape[0];\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+\n+        // Convert input to matrix\n+        Matrix<T> inputMatrix = new Matrix<T>(batchSize, inputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                inputMatrix[i, j] = input[i * inputSize + j];\n+            }\n+        }\n+\n+        // Multiply by sparse weights: [batchSize, inputSize] @ [inputSize, outputSize]\n+        Matrix<T> sparseOutputMatrix = inputMatrix.Multiply(_sparseWeights.Transpose());\n+\n+        // Convert to tensor\n+        Vector<T> sparseOutputData = new Vector<T>(batchSize * outputSize);\n+        int idx = 0;\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                sparseOutputData[idx++] = sparseOutputMatrix[i, j];\n+            }\n+        }\n+        Tensor<T> sparseOutput = new Tensor<T>(new[] { batchSize, outputSize }, sparseOutputData);\n+\n+        // 4. Sum all three outputs\n+        Tensor<T> result = new Tensor<T>(baseOutput.Shape);\n+        for (int i = 0; i < baseOutput.Length; i++)\n+        {\n+            T sum = NumOps.Add(baseOutput[i], loraOutput[i]);\n+            sum = NumOps.Add(sum, sparseOutput[i]);\n+            result[i] = sum;\n+        }\n+\n+        return result;\n+    }","path":"src/LoRA/Adapters/RoSAAdapter.cs","commit_id":"fa815036ee570162056ba6aca773f34c80f36948","original_commit_id":"8d5a1b6564c1a2bc1b334f1863e6003a6fd142b2","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Critical: Missing input caching for backward pass.**\n\nThe forward pass does not cache the input, but the backward pass requires it to compute correct gradients for the sparse weights. The correct gradient formula is `dL/dW_sparse = output_gradient^T @ input`, which requires the input from the forward pass.\n\n\n\nBased on patterns from LoHaAdapter and LoKrAdapter (see relevant code snippets), add input caching:\n\n```diff\n+private Tensor<T>? _lastInput;\n+\n public override Tensor<T> Forward(Tensor<T> input)\n {\n+    // Cache input for backward pass\n+    _lastInput = input.Clone();\n+\n     // 1. Forward through base layer\n     Tensor<T> baseOutput = _baseLayer.Forward(input);\n```\n\nThen update ResetState to clear the cache:\n\n```diff\n public override void ResetState()\n {\n     _baseLayer.ResetState();\n     _loraLayer.ResetState();\n     _sparseGradients = null;\n+    _lastInput = null;\n }\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commit fa81503","created_at":"2025-11-02T03:02:30Z","updated_at":"2025-11-02T15:10:37Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140333","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140333"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140333"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140333/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":414,"original_start_line":405,"start_side":"RIGHT","line":466,"original_line":454,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":454,"position":466,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140336","pull_request_review_id":3408046172,"id":2484140336,"node_id":"PRRC_kwDOKSXUF86UEPkw","diff_hunk":"@@ -0,0 +1,827 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// RoSA (Robust Adaptation) adapter for parameter-efficient fine-tuning with improved robustness to distribution shifts.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// RoSA (Robust Adaptation) extends standard LoRA by combining two complementary components:\n+/// 1. Low-rank component (standard LoRA): Captures common, structured patterns in adaptations\n+/// 2. Sparse component: Captures specific, rare, or outlier patterns that low-rank cannot represent\n+/// </para>\n+/// <para>\n+/// <b>Mathematical Formulation:</b>\n+/// Given input x and pre-trained weights W, RoSA computes:\n+/// - Low-rank component: L = (alpha/rank) * B * A * x\n+/// - Sparse component: S = W_sparse * x (where W_sparse is highly sparse)\n+/// - Final output: y = W*x + L + S\n+///\n+/// The sparse component is maintained through magnitude-based pruning, keeping only the\n+/// most significant weights and zeroing out the rest. This creates a sparse matrix that\n+/// captures specific patterns while remaining parameter-efficient.\n+/// </para>\n+/// <para>\n+/// <b>Research Context:</b>\n+/// RoSA was introduced in January 2024 as a robust alternative to standard LoRA.\n+/// The key insight is that low-rank approximations work well for common patterns but\n+/// struggle with distribution shifts and rare patterns. By adding a sparse component,\n+/// RoSA can capture outliers and domain-specific patterns without significantly\n+/// increasing parameter count.\n+///\n+/// In experiments on domain adaptation tasks, RoSA showed:\n+/// - Better generalization to new domains (+5-10% over standard LoRA)\n+/// - More robust to distribution shifts\n+/// - Ability to capture both global patterns (low-rank) and local exceptions (sparse)\n+/// - Only modest increase in parameters (typically 5-15% more than pure LoRA)\n+/// </para>\n+/// <para>\n+/// <b>For Beginners:</b> RoSA is like LoRA with a safety net for unusual cases.\n+///\n+/// Think of it this way:\n+/// - Low-rank LoRA is like learning general rules (\"most images of cats have pointed ears\")\n+/// - Sparse component is like remembering specific exceptions (\"this one cat breed has round ears\")\n+/// - Together they make a robust model that handles both common and rare cases\n+///\n+/// Why RoSA is more robust:\n+/// - Low-rank component: Efficient for common patterns across domains\n+/// - Sparse component: Handles outliers and domain-specific quirks\n+/// - Result: Better performance when test data differs from training data\n+///\n+/// When to use RoSA over standard LoRA:\n+/// - When you expect distribution shifts (train on news, test on social media)\n+/// - When your data has outliers or rare patterns that matter\n+/// - When you need robustness more than absolute parameter efficiency\n+/// - When adapting to multiple related but distinct domains\n+///\n+/// Trade-offs vs standard LoRA:\n+/// + More robust to distribution shifts\n+/// + Better handles rare patterns\n+/// + More flexible adaptation\n+/// - Slightly more parameters (sparse component adds ~5-15%)\n+/// - Slightly more computation (extra sparse matrix multiply)\n+/// - Requires tuning sparsity ratio\n+/// </para>\n+/// <para>\n+/// <b>Reference:</b>\n+/// \"RoSA: Robust Adaptation through Sparse Regularization\"\n+/// January 2024\n+/// </para>\n+/// </remarks>\n+public class RoSAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Sparse weight matrix that captures specific/rare patterns.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This matrix has the same dimensions as the base layer's weights but is highly sparse\n+    /// (typically 90-99% zeros). It's maintained through magnitude-based pruning during training.\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This is the \"exception handler\" of RoSA.\n+    /// Most of its values are zero, but the few non-zero values capture specific patterns\n+    /// that the low-rank component can't represent efficiently.\n+    /// </para>\n+    /// </remarks>\n+    private Matrix<T> _sparseWeights;\n+\n+    /// <summary>\n+    /// Gradients for the sparse weight component, computed during backpropagation.\n+    /// </summary>\n+    private Matrix<T>? _sparseGradients;\n+\n+    /// <summary>\n+    /// Threshold for magnitude-based pruning of sparse weights.\n+    /// Weights with magnitude below this threshold are set to zero.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This threshold controls the sparsity of the sparse component. Lower values\n+    /// result in more non-zero weights (less sparse), higher values result in\n+    /// fewer non-zero weights (more sparse).\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This is like a \"minimum importance\" cutoff.\n+    /// If a weight's importance is below this value, we zero it out to maintain\n+    /// sparsity. Typical values: 0.001 to 0.1\n+    /// </para>\n+    /// </remarks>\n+    public double SparseThreshold { get; set; }\n+\n+    /// <summary>\n+    /// Target sparsity ratio (fraction of zeros in sparse component).\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This value controls how sparse the sparse component should be.\n+    /// - 0.0 = no sparsity (all weights can be non-zero)\n+    /// - 0.5 = 50% of weights are zero\n+    /// - 0.95 = 95% of weights are zero (very sparse)\n+    /// - 0.99 = 99% of weights are zero (extremely sparse)\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This is the target percentage of zeros we want.\n+    /// Higher values (like 0.95) mean fewer non-zero weights, which keeps the\n+    /// model efficient. Lower values mean more flexibility but more parameters.\n+    ///\n+    /// Typical values:\n+    /// - 0.90 (90% zeros): More flexible, for complex domains\n+    /// - 0.95 (95% zeros): Good balance (recommended starting point)\n+    /// - 0.99 (99% zeros): Very efficient, for simple adaptations\n+    /// </para>\n+    /// </remarks>\n+    public double SparsityRatio { get; set; }\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// RoSA parameters include:\n+    /// - Base layer parameters (if not frozen)\n+    /// - LoRA parameters (rank * (inputSize + outputSize))\n+    /// - Non-zero sparse parameters (varies based on sparsity)\n+    ///\n+    /// For parameter counting, we report the full sparse matrix size, but in practice\n+    /// only the non-zero elements need to be stored and updated.\n+    /// </para>\n+    /// </remarks>\n+    public override int ParameterCount\n+    {\n+        get\n+        {\n+            int baseCount = _freezeBaseLayer ? 0 : _baseLayer.ParameterCount;\n+            int loraCount = _loraLayer.ParameterCount;\n+            int sparseCount = _sparseWeights.Rows * _sparseWeights.Columns;\n+            return baseCount + loraCount + sparseCount;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Initializes a new RoSA adapter wrapping an existing layer.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with RoSA.</param>\n+    /// <param name=\"rank\">The rank of the low-rank LoRA decomposition.</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"sparsityRatio\">Target sparsity ratio (0.0 to 1.0, typically 0.9-0.99).</param>\n+    /// <param name=\"sparseThreshold\">Magnitude threshold for pruning sparse weights (typically 0.001-0.1).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when sparsityRatio is not between 0 and 1.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// The constructor initializes the RoSA adapter by:\n+    /// 1. Setting up the standard LoRA components (via base constructor)\n+    /// 2. Initializing the sparse weight matrix (starts with small random values)\n+    /// 3. Applying initial pruning to enforce sparsity\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This creates a RoSA adapter around your existing layer.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to fine-tune efficiently and robustly\n+    /// - rank: How much compression for the low-rank component (lower = fewer parameters)\n+    /// - alpha: Scaling factor for LoRA contribution (usually equals rank)\n+    /// - sparsityRatio: How sparse the sparse component should be (0.95 = 95% zeros)\n+    /// - sparseThreshold: Minimum importance for keeping a sparse weight (0.01 is typical)\n+    /// - freezeBaseLayer: Usually true - we only train LoRA + sparse, not base weights\n+    ///\n+    /// Example: For a 1000x1000 layer with rank=8 and sparsityRatio=0.95:\n+    /// - Base layer: 1,000,000 parameters (frozen)\n+    /// - LoRA: 16,000 parameters (8 * (1000 + 1000))\n+    /// - Sparse: ~50,000 parameters (5% of 1,000,000)\n+    /// - Total trainable: ~66,000 parameters (vs 1M for full fine-tuning!)\n+    /// </para>\n+    /// </remarks>\n+    public RoSAAdapter(\n+        ILayer<T> baseLayer,\n+        int rank,\n+        double alpha = -1,\n+        double sparsityRatio = 0.95,\n+        double sparseThreshold = 0.01,\n+        bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        if (sparsityRatio < 0.0 || sparsityRatio >= 1.0)\n+        {\n+            throw new ArgumentException(\"Sparsity ratio must be between 0.0 and 1.0 (exclusive of 1.0)\", nameof(sparsityRatio));\n+        }\n+\n+        SparsityRatio = sparsityRatio;\n+        SparseThreshold = sparseThreshold;\n+\n+        // Initialize sparse weights\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        _sparseWeights = new Matrix<T>(outputSize, inputSize);\n+\n+        // Initialize with small random values (will be pruned)\n+        InitializeSparseWeights();\n+\n+        // Apply initial pruning to enforce sparsity\n+        PruneSparseWeights();\n+\n+        // Update parameters to include sparse component\n+        Parameters = new Vector<T>(ParameterCount);\n+        UpdateParametersFromComponents();\n+    }\n+\n+    /// <summary>\n+    /// Initializes sparse weights with small random values.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// The sparse weights are initialized with small random values drawn from a\n+    /// normal distribution with standard deviation 0.01. These values will be\n+    /// pruned based on magnitude to enforce sparsity.\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This gives the sparse component a random starting point.\n+    /// Most of these values will be pruned (set to zero) immediately, but this\n+    /// initialization ensures we start with a diverse set of potential patterns.\n+    /// </para>\n+    /// </remarks>\n+    private void InitializeSparseWeights()\n+    {\n+        Random random = new Random();\n+        for (int i = 0; i < _sparseWeights.Rows; i++)\n+        {\n+            for (int j = 0; j < _sparseWeights.Columns; j++)\n+            {\n+                // Small random initialization\n+                double value = random.NextGaussian(0.0, 0.01);\n+                _sparseWeights[i, j] = NumOps.FromDouble(value);\n+            }\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Prunes sparse weights based on magnitude to maintain target sparsity.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This method implements magnitude-based pruning:\n+    /// 1. Computes magnitude of all sparse weights\n+    /// 2. Determines threshold based on target sparsity ratio\n+    /// 3. Sets weights below threshold to zero\n+    ///\n+    /// This ensures the sparse component maintains its sparsity during training.\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This is like cleaning up the sparse component.\n+    ///\n+    /// We keep only the most important weights:\n+    /// 1. Look at all the weights and their magnitudes\n+    /// 2. Sort them by importance (magnitude)\n+    /// 3. Keep the top X% (based on sparsity ratio)\n+    /// 4. Zero out the rest\n+    ///\n+    /// Example with sparsity ratio 0.95:\n+    /// - We have 1000 weights\n+    /// - We want 95% zeros (950 zeros, 50 non-zeros)\n+    /// - Keep the 50 largest magnitudes\n+    /// - Set the other 950 to zero\n+    ///\n+    /// This is called periodically during training to maintain sparsity.\n+    /// </para>\n+    /// </remarks>\n+    public void PruneSparseWeights()\n+    {\n+        int rows = _sparseWeights.Rows;\n+        int cols = _sparseWeights.Columns;\n+        int totalWeights = rows * cols;\n+\n+        // Collect magnitudes\n+        List<(int row, int col, double magnitude)> magnitudes = new List<(int, int, double)>();\n+        for (int i = 0; i < rows; i++)\n+        {\n+            for (int j = 0; j < cols; j++)\n+            {\n+                double mag = Math.Abs(Convert.ToDouble(_sparseWeights[i, j]));\n+                magnitudes.Add((i, j, mag));\n+            }\n+        }\n+\n+        // Sort by magnitude (descending)\n+        magnitudes.Sort((a, b) => b.magnitude.CompareTo(a.magnitude));\n+\n+        // Determine number of non-zero weights to keep\n+        int keepCount = (int)((1.0 - SparsityRatio) * totalWeights);\n+        keepCount = Math.Max(1, keepCount); // Keep at least one weight\n+\n+        // Also consider threshold-based pruning\n+        double adaptiveThreshold = SparseThreshold;\n+        if (keepCount < magnitudes.Count)\n+        {\n+            // Use the larger of: fixed threshold or magnitude of keepCount-th element\n+            adaptiveThreshold = Math.Max(SparseThreshold, magnitudes[keepCount].magnitude);\n+        }\n+\n+        // Apply pruning: zero out weights below threshold\n+        for (int i = 0; i < rows; i++)\n+        {\n+            for (int j = 0; j < cols; j++)\n+            {\n+                double mag = Math.Abs(Convert.ToDouble(_sparseWeights[i, j]));\n+                if (mag < adaptiveThreshold)\n+                {\n+                    _sparseWeights[i, j] = NumOps.Zero;\n+                }\n+            }\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Gets the current sparsity of the sparse component.\n+    /// </summary>\n+    /// <returns>The fraction of zeros in the sparse weight matrix (0.0 to 1.0).</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method computes the actual sparsity by counting zero and near-zero elements.\n+    /// The result can be compared to SparsityRatio to see how well pruning is working.\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This tells you what percentage of the sparse component is actually zero.\n+    ///\n+    /// If you set SparsityRatio to 0.95, this should return close to 0.95 after pruning.\n+    /// If it's much lower, you might need to adjust the threshold or pruning frequency.\n+    ///\n+    /// Example return values:\n+    /// - 0.95 = 95% zeros (good for target of 0.95)\n+    /// - 0.80 = 80% zeros (less sparse than target)\n+    /// - 0.99 = 99% zeros (more sparse than target)\n+    /// </para>\n+    /// </remarks>\n+    public double GetSparsity()\n+    {\n+        int totalWeights = _sparseWeights.Rows * _sparseWeights.Columns;\n+        int zeroCount = 0;\n+        double epsilon = 1e-10;\n+\n+        for (int i = 0; i < _sparseWeights.Rows; i++)\n+        {\n+            for (int j = 0; j < _sparseWeights.Columns; j++)\n+            {\n+                double val = Math.Abs(Convert.ToDouble(_sparseWeights[i, j]));\n+                if (val < epsilon)\n+                {\n+                    zeroCount++;\n+                }\n+            }\n+        }\n+\n+        return (double)zeroCount / totalWeights;\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass through RoSA adapter.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Output combining base layer, low-rank LoRA, and sparse components.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The RoSA forward pass computes:\n+    /// 1. Base output: y_base = base_layer(input)\n+    /// 2. LoRA output: y_lora = lora_layer(input)\n+    /// 3. Sparse output: y_sparse = input @ sparse_weights^T\n+    /// 4. Final output: y = y_base + y_lora + y_sparse\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This is where all three components work together.\n+    ///\n+    /// Think of it as three parallel processing paths:\n+    /// - Base layer: Original pre-trained knowledge (usually frozen)\n+    /// - LoRA component: Low-rank corrections for common patterns\n+    /// - Sparse component: Specific corrections for rare patterns\n+    ///\n+    /// All three outputs are added together to get the final result.\n+    /// This combination gives RoSA its robustness: the low-rank handles\n+    /// common patterns efficiently, while sparse handles outliers.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        // 1. Forward through base layer\n+        Tensor<T> baseOutput = _baseLayer.Forward(input);\n+\n+        // 2. Forward through LoRA layer (low-rank component)\n+        Tensor<T> loraOutput = _loraLayer.Forward(input);\n+\n+        // 3. Forward through sparse component\n+        // Compute: sparse_output = input @ sparse_weights^T\n+        int batchSize = input.Shape[0];\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+\n+        // Convert input to matrix\n+        Matrix<T> inputMatrix = new Matrix<T>(batchSize, inputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                inputMatrix[i, j] = input[i * inputSize + j];\n+            }\n+        }\n+\n+        // Multiply by sparse weights: [batchSize, inputSize] @ [inputSize, outputSize]\n+        Matrix<T> sparseOutputMatrix = inputMatrix.Multiply(_sparseWeights.Transpose());\n+\n+        // Convert to tensor\n+        Vector<T> sparseOutputData = new Vector<T>(batchSize * outputSize);\n+        int idx = 0;\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                sparseOutputData[idx++] = sparseOutputMatrix[i, j];\n+            }\n+        }\n+        Tensor<T> sparseOutput = new Tensor<T>(new[] { batchSize, outputSize }, sparseOutputData);\n+\n+        // 4. Sum all three outputs\n+        Tensor<T> result = new Tensor<T>(baseOutput.Shape);\n+        for (int i = 0; i < baseOutput.Length; i++)\n+        {\n+            T sum = NumOps.Add(baseOutput[i], loraOutput[i]);\n+            sum = NumOps.Add(sum, sparseOutput[i]);\n+            result[i] = sum;\n+        }\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Performs the backward pass through RoSA adapter.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient flowing back from the next layer.</param>\n+    /// <returns>Gradient to pass to the previous layer.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The backward pass computes gradients for all three components:\n+    /// 1. LoRA component (via LoRA layer's backward)\n+    /// 2. Sparse component (direct gradient computation)\n+    /// 3. Base layer (if not frozen)\n+    ///\n+    /// Gradients are accumulated and input gradients are summed.\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This is where RoSA learns from errors.\n+    ///\n+    /// The backward pass tells each component how to improve:\n+    /// - LoRA component: Update low-rank matrices A and B\n+    /// - Sparse component: Update the sparse weight matrix\n+    /// - Base layer: Update if not frozen (usually frozen)\n+    ///\n+    /// After this, UpdateParameters() will apply the learning using these gradients.\n+    /// The sparse gradients will be pruned to maintain sparsity.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        int batchSize = outputGradient.Shape[0];\n+        int outputSize = GetOutputShape()[0];\n+        int inputSize = GetInputShape()[0];\n+\n+        // 1. Backward through LoRA layer\n+        Tensor<T> loraInputGrad = _loraLayer.Backward(outputGradient);\n+\n+        // 2. Backward through base layer\n+        Tensor<T> baseInputGrad = _baseLayer.Backward(outputGradient);\n+\n+        // 3. Compute gradients for sparse component\n+        // Sparse gradient: dL/dW_sparse = output_gradient^T @ input\n+        // Convert output gradient to matrix\n+        Matrix<T> gradMatrix = new Matrix<T>(batchSize, outputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                gradMatrix[i, j] = outputGradient[i * outputSize + j];\n+            }\n+        }\n+\n+        // Get input from base layer (we'll need to store this in a more complete implementation)\n+        // For now, we'll compute sparse weight gradients from the output gradient\n+        // In practice, you'd cache the input from forward pass\n+        _sparseGradients = new Matrix<T>(outputSize, inputSize);\n+\n+        // Simplified gradient computation (assumes gradients are averaged across batch)\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                T gradSum = NumOps.Zero;\n+                for (int b = 0; b < batchSize; b++)\n+                {\n+                    gradSum = NumOps.Add(gradSum, gradMatrix[b, i]);\n+                }\n+                // Average over batch\n+                _sparseGradients[i, j] = NumOps.Divide(gradSum, NumOps.FromDouble(batchSize));\n+            }\n+        }","path":"src/LoRA/Adapters/RoSAAdapter.cs","commit_id":"fa815036ee570162056ba6aca773f34c80f36948","original_commit_id":"8d5a1b6564c1a2bc1b334f1863e6003a6fd142b2","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Critical: Incorrect sparse weight gradient computation.**\n\nThe sparse gradient computation is mathematically incorrect. The current implementation (lines 512-524) simply averages the output gradient across the batch and doesn't use the input at all. The comment on lines 506-508 acknowledges this is wrong.\n\nThe correct formula for sparse weight gradients is: `dL/dW_sparse = output_gradient^T @ input`\n\nFor each element: `_sparseGradients[i,j] = sum over batch of (output_gradient[b,i] * input[b,j])`\n\n\n\nApply this diff to fix the gradient computation (requires the cached input from the previous fix):\n\n```diff\n-        // Get input from base layer (we'll need to store this in a more complete implementation)\n-        // For now, we'll compute sparse weight gradients from the output gradient\n-        // In practice, you'd cache the input from forward pass\n+        if (_lastInput == null)\n+        {\n+            throw new InvalidOperationException(\"Forward pass must be called before backward pass\");\n+        }\n+\n+        // Convert input to matrix\n+        Matrix<T> inputMatrix = new Matrix<T>(batchSize, inputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                inputMatrix[i, j] = _lastInput[i * inputSize + j];\n+            }\n+        }\n+\n+        // Compute sparse weight gradients: dL/dW_sparse = output_gradient^T @ input\n         _sparseGradients = new Matrix<T>(outputSize, inputSize);\n-\n-        // Simplified gradient computation (assumes gradients are averaged across batch)\n         for (int i = 0; i < outputSize; i++)\n         {\n             for (int j = 0; j < inputSize; j++)\n             {\n                 T gradSum = NumOps.Zero;\n                 for (int b = 0; b < batchSize; b++)\n                 {\n-                    gradSum = NumOps.Add(gradSum, gradMatrix[b, i]);\n+                    T prod = NumOps.Multiply(gradMatrix[b, i], inputMatrix[b, j]);\n+                    gradSum = NumOps.Add(gradSum, prod);\n                 }\n-                // Average over batch\n-                _sparseGradients[i, j] = NumOps.Divide(gradSum, NumOps.FromDouble(batchSize));\n+                _sparseGradients[i, j] = gradSum;\n             }\n         }\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/LoRA/Adapters/RoSAAdapter.cs around lines 494 to 524, the sparse weight\ngradient is computed incorrectly by averaging output gradients and ignoring the\ninput; replace that logic to use the cached input from the forward pass and\ncompute _sparseGradients[i,j] = sum_b (outputGradient[b,i] * input[b,j]) (i.e.\noutput_gradient^T @ input). Specifically, ensure you have the input matrix\ncached (e.g., Matrix<T> cachedInput with shape [batchSize, inputSize]), loop\nover outputSize i and inputSize j, accumulate using NumOps.Multiply and\nNumOps.Add across batch b, and assign the accumulated sum (no standalone\naveraging here unless you intend to divide by batchSize; if averaging is desired\ndivide by NumOps.FromDouble(batchSize) using NumOps.Divide). Use NumOps\noperations for zero, multiplication and addition to maintain generic T handling.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commit fa81503","created_at":"2025-11-02T03:02:30Z","updated_at":"2025-11-02T15:10:39Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140336","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140336"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484140336"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484140336/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":506,"original_start_line":494,"start_side":"RIGHT","line":542,"original_line":524,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":524,"position":542,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484162726","pull_request_review_id":3408080425,"id":2484162726,"node_id":"PRRC_kwDOKSXUF86UEVCm","diff_hunk":"@@ -0,0 +1,660 @@\n+using AiDotNet.Interfaces;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// Chain-of-LoRA adapter that implements sequential composition of multiple LoRA adapters.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// Chain-of-LoRA (COLA) is an advanced LoRA technique that enables sequential composition\n+/// of multiple LoRA adaptations through an iterative optimization framework. Unlike standard\n+/// LoRA which applies a single low-rank adaptation, COLA builds a chain of adaptations where\n+/// each adapter is trained, merged into the model, and then a new adapter is initialized for\n+/// further refinement.\n+/// </para>\n+/// <para>\n+/// This approach bridges the performance gap between standard LoRA and full fine-tuning by\n+/// employing residual learning principles. Each iteration in the chain adds incremental\n+/// improvements to the model's task-specific performance without incurring additional\n+/// computational costs or memory overhead during inference.\n+/// </para>\n+/// <para><b>Key Concepts:</b>\n+///\n+/// <b>Sequential Adaptation:</b>\n+/// Chain-of-LoRA applies adaptations in sequence (Task A ‚Üí Task B ‚Üí Task C), where each\n+/// stage builds upon the previous one. This is inspired by the Frank-Wolfe optimization\n+/// algorithm, which makes greedy updates along the direction of maximum improvement.\n+///\n+/// <b>Merge and Re-initialize:</b>\n+/// After training each LoRA adapter, the learned weights are merged back into the base layer,\n+/// and a new LoRA adapter is initialized. This \"tying a knot\" process allows the model to\n+/// consolidate learned knowledge before adding new adaptations.\n+///\n+/// <b>Knowledge Preservation:</b>\n+/// By freezing the base layer and only training the LoRA components, the chain preserves\n+/// previously learned knowledge while allowing new task-specific adaptations. Each adapter\n+/// in the chain captures a specific aspect of the task or a refinement step.\n+///\n+/// <b>Incremental Fine-tuning Pipeline:</b>\n+/// COLA enables continual learning scenarios where tasks are presented sequentially, and\n+/// the model must adapt to new tasks while maintaining performance on previous ones.\n+/// </para>\n+/// <para><b>Benefits of Chain-of-LoRA:</b>\n+///\n+/// - <b>Better Performance:</b> Achieves up to 6.47% relative accuracy gain over standard LoRA\n+/// - <b>No Extra Overhead:</b> After merging, inference cost is identical to the base model\n+/// - <b>Modular Adaptation:</b> Each adapter can be trained, tested, and validated independently\n+/// - <b>Catastrophic Forgetting Mitigation:</b> Sequential merging helps preserve prior knowledge\n+/// - <b>Task Chaining:</b> Naturally supports multi-task learning and transfer learning scenarios\n+/// - <b>Flexible Deployment:</b> Can deploy the full chain or selected adapters as needed\n+/// </para>\n+/// <para><b>For Beginners:</b>\n+///\n+/// Imagine you're learning a complex skill in stages:\n+/// 1. First, you learn the basics (Adapter 1)\n+/// 2. Then you practice and the basics become automatic (Merge)\n+/// 3. Next, you learn intermediate techniques on top of the basics (Adapter 2)\n+/// 4. Again, you practice until they're automatic (Merge)\n+/// 5. Finally, you learn advanced skills building on everything before (Adapter 3)\n+///\n+/// Chain-of-LoRA works the same way: each adapter learns something new, then it's consolidated\n+/// into the model, and the next adapter can focus on the next refinement. This stepwise approach\n+/// often achieves better results than trying to learn everything at once.\n+/// </para>\n+/// <para><b>Research Reference:</b>\n+///\n+/// Based on \"Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning\"\n+/// (arXiv:2401.04151, January 2024). The paper demonstrates that sequential low-rank adaptations\n+/// can significantly improve task performance compared to single-stage LoRA, especially on\n+/// complex reasoning and multi-step tasks.\n+/// </para>\n+/// <para><b>Usage Example:</b>\n+/// <code>\n+/// // Create a chain with 3 sequential adaptations\n+/// var chain = new ChainLoRAAdapter&lt;double&gt;(baseLayer, rank: 8, chainLength: 3);\n+///\n+/// // Train first adapter on Task A\n+/// chain.SetActiveAdapterIndex(0);\n+/// TrainModel(chain, taskAData);\n+/// chain.MergeActiveAdapter(); // Consolidate Task A knowledge\n+///\n+/// // Train second adapter on Task B\n+/// chain.SetActiveAdapterIndex(1);\n+/// TrainModel(chain, taskBData);\n+/// chain.MergeActiveAdapter(); // Consolidate Task B knowledge\n+///\n+/// // Train third adapter on Task C\n+/// chain.SetActiveAdapterIndex(2);\n+/// TrainModel(chain, taskCData);\n+///\n+/// // Deploy: all adaptations are now part of the model\n+/// ILayer&lt;double&gt; finalLayer = chain.MergeToOriginalLayer();\n+/// </code>\n+/// </para>\n+/// </remarks>\n+public class ChainLoRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// The chain of LoRA adapters applied sequentially.\n+    /// </summary>\n+    private readonly List<LoRALayer<T>> _adapterChain;\n+\n+    /// <summary>\n+    /// The index of the currently active adapter being trained.\n+    /// </summary>\n+    private int _activeAdapterIndex;\n+\n+    /// <summary>\n+    /// Whether each adapter in the chain has been merged.\n+    /// </summary>\n+    private readonly List<bool> _mergedStatus;\n+\n+    /// <summary>\n+    /// The total length of the adapter chain.\n+    /// </summary>\n+    private readonly int _chainLength;\n+\n+    /// <summary>\n+    /// Gets the total number of adapters in the chain.\n+    /// </summary>\n+    /// <remarks>\n+    /// This represents the maximum number of sequential adaptation stages that can be applied.\n+    /// Each adapter can be trained independently and then merged before proceeding to the next.\n+    /// </remarks>\n+    public int ChainLength => _chainLength;\n+\n+    /// <summary>\n+    /// Gets the index of the currently active adapter (0-based).\n+    /// </summary>\n+    /// <remarks>\n+    /// The active adapter is the one currently being trained. Other adapters in the chain\n+    /// are either waiting to be trained (higher indices) or have been merged (lower indices).\n+    /// </remarks>\n+    public int ActiveAdapterIndex => _activeAdapterIndex;\n+\n+    /// <summary>\n+    /// Gets the list of LoRA adapters in the chain.\n+    /// </summary>\n+    /// <remarks>\n+    /// Each adapter in the chain represents one stage of sequential adaptation.\n+    /// Adapters are applied in order during forward passes.\n+    /// </remarks>\n+    public IReadOnlyList<LoRALayer<T>> AdapterChain => _adapterChain.AsReadOnly();\n+\n+    /// <summary>\n+    /// Gets the merged status of each adapter in the chain.\n+    /// </summary>\n+    /// <remarks>\n+    /// True indicates that an adapter has been merged into the base layer and should\n+    /// no longer contribute trainable parameters. Merged adapters still contribute\n+    /// to the forward pass until the entire chain is collapsed.\n+    /// </remarks>\n+    public IReadOnlyList<bool> MergedStatus => _mergedStatus.AsReadOnly();\n+\n+    /// <summary>\n+    /// Initializes a new Chain-of-LoRA adapter with the specified configuration.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with the LoRA chain.</param>\n+    /// <param name=\"rank\">The rank of each LoRA decomposition in the chain.</param>\n+    /// <param name=\"chainLength\">The number of sequential adapters in the chain (default: 3).</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor for each adapter (defaults to rank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training (default: true).</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when chainLength is less than 1.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// Creates a chain of LoRA adapters for sequential fine-tuning. Each adapter in the chain\n+    /// can be trained independently, merged into the model, and then the next adapter can be\n+    /// activated for further refinement.\n+    /// </para>\n+    /// <para><b>For Beginners:</b>\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to adapt (e.g., a dense or convolutional layer)\n+    /// - rank: How compressed each adapter is (lower = fewer parameters per stage)\n+    /// - chainLength: How many sequential adaptation stages you want (typical: 2-5)\n+    /// - alpha: Controls adaptation strength (usually equals rank)\n+    /// - freezeBaseLayer: Lock base weights to preserve pre-trained knowledge (recommended: true)\n+    ///\n+    /// Example: chainLength=3 means you can do three rounds of training and merging,\n+    /// allowing the model to incrementally improve on complex tasks.\n+    /// </para>\n+    /// </remarks>\n+    public ChainLoRAAdapter(\n+        ILayer<T> baseLayer,\n+        int rank,\n+        int chainLength = 3,\n+        double alpha = -1,\n+        bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        if (chainLength < 1)\n+        {\n+            throw new ArgumentException(\"Chain length must be at least 1\", nameof(chainLength));\n+        }\n+\n+        _chainLength = chainLength;\n+        _activeAdapterIndex = 0;\n+        _adapterChain = new List<LoRALayer<T>>(chainLength);\n+        _mergedStatus = new List<bool>(chainLength);\n+\n+        // Create the chain of LoRA adapters\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+\n+        for (int i = 0; i < chainLength; i++)\n+        {\n+            var adapter = new LoRALayer<T>(inputSize, outputSize, rank, alpha);\n+            _adapterChain.Add(adapter);\n+            _mergedStatus.Add(false);\n+        }\n+\n+        // Update parameter count to reflect all unmerged adapters\n+        UpdateParameterCount();\n+    }","path":"src/LoRA/Adapters/ChainLoRAAdapter.cs","commit_id":"fa815036ee570162056ba6aca773f34c80f36948","original_commit_id":"bf7f1556cf26d60d07903cb1a91078b9c9963a87","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**Design issue: unused LoRA layer allocated in base class.**\n\nThe base class `LoRAAdapterBase<T>` constructor creates a single `_loraLayer` (via `CreateLoRALayer()`), but `ChainLoRAAdapter` never uses it‚Äîall actual adapters live in `_adapterChain`. This wastes memory and creates confusion: the `LoRALayer` property (inherited from `ILoRAAdapter<T>`) returns an adapter that doesn't participate in forward/backward passes.\n\nConsider overriding `CreateLoRALayer()` to return a placeholder or refactoring the base class to make `_loraLayer` optional for specialized adapters like chains.\n\n\n\nApply this diff to override and document the unused layer:\n\n```diff\n         _chainLength = chainLength;\n         _activeAdapterIndex = 0;\n         _adapterChain = new List<LoRALayer<T>>(chainLength);\n         _mergedStatus = new List<bool>(chainLength);\n \n         // Create the chain of LoRA adapters\n         int inputSize = GetInputShape()[0];\n         int outputSize = GetOutputShape()[0];\n \n         for (int i = 0; i < chainLength; i++)\n         {\n             var adapter = new LoRALayer<T>(inputSize, outputSize, rank, alpha);\n             _adapterChain.Add(adapter);\n             _mergedStatus.Add(false);\n         }\n \n         // Update parameter count to reflect all unmerged adapters\n         UpdateParameterCount();\n     }\n+\n+    /// <summary>\n+    /// Creates a placeholder LoRA layer to satisfy the base class contract.\n+    /// </summary>\n+    /// <remarks>\n+    /// ChainLoRAAdapter uses _adapterChain instead of the base class _loraLayer.\n+    /// This override prevents allocation of an unused layer.\n+    /// </remarks>\n+    protected override LoRALayer<T> CreateLoRALayer(int rank, double alpha)\n+    {\n+        // Return first adapter from chain as the \"primary\" layer for interface compatibility\n+        // Note: The actual chain logic uses _adapterChain, not this layer\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        return new LoRALayer<T>(inputSize, outputSize, rank, alpha);\n+    }\n```\n\nAlternatively, document that the base `LoRALayer` property returns the first adapter in the chain for interface compatibility.\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T03:40:19Z","updated_at":"2025-11-02T03:40:20Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484162726","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484162726"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484162726"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484162726/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":198,"original_start_line":188,"start_side":"RIGHT","line":229,"original_line":219,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":219,"position":229,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484162727","pull_request_review_id":3408080425,"id":2484162727,"node_id":"PRRC_kwDOKSXUF86UEVCn","diff_hunk":"@@ -0,0 +1,660 @@\n+using AiDotNet.Interfaces;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// Chain-of-LoRA adapter that implements sequential composition of multiple LoRA adapters.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// Chain-of-LoRA (COLA) is an advanced LoRA technique that enables sequential composition\n+/// of multiple LoRA adaptations through an iterative optimization framework. Unlike standard\n+/// LoRA which applies a single low-rank adaptation, COLA builds a chain of adaptations where\n+/// each adapter is trained, merged into the model, and then a new adapter is initialized for\n+/// further refinement.\n+/// </para>\n+/// <para>\n+/// This approach bridges the performance gap between standard LoRA and full fine-tuning by\n+/// employing residual learning principles. Each iteration in the chain adds incremental\n+/// improvements to the model's task-specific performance without incurring additional\n+/// computational costs or memory overhead during inference.\n+/// </para>\n+/// <para><b>Key Concepts:</b>\n+///\n+/// <b>Sequential Adaptation:</b>\n+/// Chain-of-LoRA applies adaptations in sequence (Task A ‚Üí Task B ‚Üí Task C), where each\n+/// stage builds upon the previous one. This is inspired by the Frank-Wolfe optimization\n+/// algorithm, which makes greedy updates along the direction of maximum improvement.\n+///\n+/// <b>Merge and Re-initialize:</b>\n+/// After training each LoRA adapter, the learned weights are merged back into the base layer,\n+/// and a new LoRA adapter is initialized. This \"tying a knot\" process allows the model to\n+/// consolidate learned knowledge before adding new adaptations.\n+///\n+/// <b>Knowledge Preservation:</b>\n+/// By freezing the base layer and only training the LoRA components, the chain preserves\n+/// previously learned knowledge while allowing new task-specific adaptations. Each adapter\n+/// in the chain captures a specific aspect of the task or a refinement step.\n+///\n+/// <b>Incremental Fine-tuning Pipeline:</b>\n+/// COLA enables continual learning scenarios where tasks are presented sequentially, and\n+/// the model must adapt to new tasks while maintaining performance on previous ones.\n+/// </para>\n+/// <para><b>Benefits of Chain-of-LoRA:</b>\n+///\n+/// - <b>Better Performance:</b> Achieves up to 6.47% relative accuracy gain over standard LoRA\n+/// - <b>No Extra Overhead:</b> After merging, inference cost is identical to the base model\n+/// - <b>Modular Adaptation:</b> Each adapter can be trained, tested, and validated independently\n+/// - <b>Catastrophic Forgetting Mitigation:</b> Sequential merging helps preserve prior knowledge\n+/// - <b>Task Chaining:</b> Naturally supports multi-task learning and transfer learning scenarios\n+/// - <b>Flexible Deployment:</b> Can deploy the full chain or selected adapters as needed\n+/// </para>\n+/// <para><b>For Beginners:</b>\n+///\n+/// Imagine you're learning a complex skill in stages:\n+/// 1. First, you learn the basics (Adapter 1)\n+/// 2. Then you practice and the basics become automatic (Merge)\n+/// 3. Next, you learn intermediate techniques on top of the basics (Adapter 2)\n+/// 4. Again, you practice until they're automatic (Merge)\n+/// 5. Finally, you learn advanced skills building on everything before (Adapter 3)\n+///\n+/// Chain-of-LoRA works the same way: each adapter learns something new, then it's consolidated\n+/// into the model, and the next adapter can focus on the next refinement. This stepwise approach\n+/// often achieves better results than trying to learn everything at once.\n+/// </para>\n+/// <para><b>Research Reference:</b>\n+///\n+/// Based on \"Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning\"\n+/// (arXiv:2401.04151, January 2024). The paper demonstrates that sequential low-rank adaptations\n+/// can significantly improve task performance compared to single-stage LoRA, especially on\n+/// complex reasoning and multi-step tasks.\n+/// </para>\n+/// <para><b>Usage Example:</b>\n+/// <code>\n+/// // Create a chain with 3 sequential adaptations\n+/// var chain = new ChainLoRAAdapter&lt;double&gt;(baseLayer, rank: 8, chainLength: 3);\n+///\n+/// // Train first adapter on Task A\n+/// chain.SetActiveAdapterIndex(0);\n+/// TrainModel(chain, taskAData);\n+/// chain.MergeActiveAdapter(); // Consolidate Task A knowledge\n+///\n+/// // Train second adapter on Task B\n+/// chain.SetActiveAdapterIndex(1);\n+/// TrainModel(chain, taskBData);\n+/// chain.MergeActiveAdapter(); // Consolidate Task B knowledge\n+///\n+/// // Train third adapter on Task C\n+/// chain.SetActiveAdapterIndex(2);\n+/// TrainModel(chain, taskCData);\n+///\n+/// // Deploy: all adaptations are now part of the model\n+/// ILayer&lt;double&gt; finalLayer = chain.MergeToOriginalLayer();\n+/// </code>\n+/// </para>\n+/// </remarks>\n+public class ChainLoRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// The chain of LoRA adapters applied sequentially.\n+    /// </summary>\n+    private readonly List<LoRALayer<T>> _adapterChain;\n+\n+    /// <summary>\n+    /// The index of the currently active adapter being trained.\n+    /// </summary>\n+    private int _activeAdapterIndex;\n+\n+    /// <summary>\n+    /// Whether each adapter in the chain has been merged.\n+    /// </summary>\n+    private readonly List<bool> _mergedStatus;\n+\n+    /// <summary>\n+    /// The total length of the adapter chain.\n+    /// </summary>\n+    private readonly int _chainLength;\n+\n+    /// <summary>\n+    /// Gets the total number of adapters in the chain.\n+    /// </summary>\n+    /// <remarks>\n+    /// This represents the maximum number of sequential adaptation stages that can be applied.\n+    /// Each adapter can be trained independently and then merged before proceeding to the next.\n+    /// </remarks>\n+    public int ChainLength => _chainLength;\n+\n+    /// <summary>\n+    /// Gets the index of the currently active adapter (0-based).\n+    /// </summary>\n+    /// <remarks>\n+    /// The active adapter is the one currently being trained. Other adapters in the chain\n+    /// are either waiting to be trained (higher indices) or have been merged (lower indices).\n+    /// </remarks>\n+    public int ActiveAdapterIndex => _activeAdapterIndex;\n+\n+    /// <summary>\n+    /// Gets the list of LoRA adapters in the chain.\n+    /// </summary>\n+    /// <remarks>\n+    /// Each adapter in the chain represents one stage of sequential adaptation.\n+    /// Adapters are applied in order during forward passes.\n+    /// </remarks>\n+    public IReadOnlyList<LoRALayer<T>> AdapterChain => _adapterChain.AsReadOnly();\n+\n+    /// <summary>\n+    /// Gets the merged status of each adapter in the chain.\n+    /// </summary>\n+    /// <remarks>\n+    /// True indicates that an adapter has been merged into the base layer and should\n+    /// no longer contribute trainable parameters. Merged adapters still contribute\n+    /// to the forward pass until the entire chain is collapsed.\n+    /// </remarks>\n+    public IReadOnlyList<bool> MergedStatus => _mergedStatus.AsReadOnly();\n+\n+    /// <summary>\n+    /// Initializes a new Chain-of-LoRA adapter with the specified configuration.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with the LoRA chain.</param>\n+    /// <param name=\"rank\">The rank of each LoRA decomposition in the chain.</param>\n+    /// <param name=\"chainLength\">The number of sequential adapters in the chain (default: 3).</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor for each adapter (defaults to rank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training (default: true).</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when chainLength is less than 1.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// Creates a chain of LoRA adapters for sequential fine-tuning. Each adapter in the chain\n+    /// can be trained independently, merged into the model, and then the next adapter can be\n+    /// activated for further refinement.\n+    /// </para>\n+    /// <para><b>For Beginners:</b>\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to adapt (e.g., a dense or convolutional layer)\n+    /// - rank: How compressed each adapter is (lower = fewer parameters per stage)\n+    /// - chainLength: How many sequential adaptation stages you want (typical: 2-5)\n+    /// - alpha: Controls adaptation strength (usually equals rank)\n+    /// - freezeBaseLayer: Lock base weights to preserve pre-trained knowledge (recommended: true)\n+    ///\n+    /// Example: chainLength=3 means you can do three rounds of training and merging,\n+    /// allowing the model to incrementally improve on complex tasks.\n+    /// </para>\n+    /// </remarks>\n+    public ChainLoRAAdapter(\n+        ILayer<T> baseLayer,\n+        int rank,\n+        int chainLength = 3,\n+        double alpha = -1,\n+        bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        if (chainLength < 1)\n+        {\n+            throw new ArgumentException(\"Chain length must be at least 1\", nameof(chainLength));\n+        }\n+\n+        _chainLength = chainLength;\n+        _activeAdapterIndex = 0;\n+        _adapterChain = new List<LoRALayer<T>>(chainLength);\n+        _mergedStatus = new List<bool>(chainLength);\n+\n+        // Create the chain of LoRA adapters\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+\n+        for (int i = 0; i < chainLength; i++)\n+        {\n+            var adapter = new LoRALayer<T>(inputSize, outputSize, rank, alpha);\n+            _adapterChain.Add(adapter);\n+            _mergedStatus.Add(false);\n+        }\n+\n+        // Update parameter count to reflect all unmerged adapters\n+        UpdateParameterCount();\n+    }\n+\n+    /// <summary>\n+    /// Sets which adapter in the chain is currently active for training.\n+    /// </summary>\n+    /// <param name=\"index\">The 0-based index of the adapter to activate.</param>\n+    /// <exception cref=\"ArgumentOutOfRangeException\">Thrown when index is out of range.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// Only the active adapter receives gradient updates during training. Other adapters\n+    /// are either frozen (already merged) or inactive (waiting to be trained).\n+    /// </para>\n+    /// <para><b>For Beginners:</b>\n+    /// This is like choosing which stage of learning you're currently working on.\n+    /// Set to 0 for the first stage, 1 for the second, etc. Only that stage's adapter\n+    /// will be trained while the others remain frozen.\n+    /// </para>\n+    /// </remarks>\n+    public void SetActiveAdapterIndex(int index)\n+    {\n+        if (index < 0 || index >= _chainLength)\n+        {\n+            throw new ArgumentOutOfRangeException(nameof(index), $\"Index must be between 0 and {_chainLength - 1}\");\n+        }\n+\n+        _activeAdapterIndex = index;\n+    }\n+\n+    /// <summary>\n+    /// Merges the currently active adapter into the base layer representation.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This \"ties a knot\" in the chain by marking the active adapter as merged and frozen.\n+    /// The adapter's weights are conceptually incorporated into the model, allowing the\n+    /// next adapter in the chain to build upon this consolidated knowledge.\n+    /// </para>\n+    /// <para>\n+    /// Note: The actual weight merging into a single layer happens when MergeToOriginalLayer()\n+    /// is called. This method only marks the adapter as merged for training purposes.\n+    /// </para>\n+    /// <para><b>For Beginners:</b>\n+    /// After training an adapter stage, call this to \"lock it in\" before moving to the\n+    /// next stage. It's like saving your progress before starting the next level.\n+    /// </para>\n+    /// </remarks>\n+    public void MergeActiveAdapter()\n+    {\n+        if (_activeAdapterIndex < 0 || _activeAdapterIndex >= _chainLength)\n+        {\n+            throw new InvalidOperationException($\"Invalid active adapter index: {_activeAdapterIndex}\");\n+        }\n+\n+        _mergedStatus[_activeAdapterIndex] = true;\n+        UpdateParameterCount();\n+    }\n+\n+    /// <summary>\n+    /// Unmerges a previously merged adapter, making it trainable again.\n+    /// </summary>\n+    /// <param name=\"index\">The index of the adapter to unmerge.</param>\n+    /// <exception cref=\"ArgumentOutOfRangeException\">Thrown when index is out of range.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// This allows re-training a previously merged adapter if needed for iterative refinement.\n+    /// Useful for scenarios where you want to go back and adjust an earlier stage.\n+    /// </para>\n+    /// </remarks>\n+    public void UnmergeAdapter(int index)\n+    {\n+        if (index < 0 || index >= _chainLength)\n+        {\n+            throw new ArgumentOutOfRangeException(nameof(index), $\"Index must be between 0 and {_chainLength - 1}\");\n+        }\n+\n+        _mergedStatus[index] = false;\n+        UpdateParameterCount();\n+    }\n+\n+    /// <summary>\n+    /// Gets the number of adapters that have been merged.\n+    /// </summary>\n+    /// <returns>Count of merged adapters.</returns>\n+    public int GetMergedCount()\n+    {\n+        return _mergedStatus.Count(merged => merged);\n+    }\n+\n+    /// <summary>\n+    /// Gets the total number of parameters in the chain (base layer + all unmerged adapters).\n+    /// </summary>\n+    /// <remarks>\n+    /// This count includes parameters from the base layer (if not frozen) plus all unmerged adapters in the chain.\n+    /// Merged adapters don't contribute to the parameter count since they've been absorbed into the base weights.\n+    /// </remarks>\n+    public override int ParameterCount\n+    {\n+        get\n+        {\n+            int count = 0;\n+\n+            // Add base layer parameters if not frozen\n+            if (!_freezeBaseLayer)\n+            {\n+                count += _baseLayer.ParameterCount;\n+            }\n+\n+            // Add unmerged adapter parameters\n+            for (int i = 0; i < _chainLength; i++)\n+            {\n+                if (!_mergedStatus[i])\n+                {\n+                    count += _adapterChain[i].ParameterCount;\n+                }\n+            }\n+\n+            return count;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Gets the number of adapters that are still trainable (not merged).\n+    /// </summary>\n+    /// <returns>Count of unmerged adapters.</returns>\n+    public int GetTrainableAdapterCount()\n+    {\n+        return _mergedStatus.Count(merged => !merged);\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass through the base layer and all adapters in the chain.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Output with all adapter contributions summed.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The forward pass computes:\n+    /// output = base_layer(input) + adapter_0(input) + adapter_1(input) + ... + adapter_n(input)\n+    /// </para>\n+    /// <para>\n+    /// All adapters contribute to the output, regardless of merge status. Merged adapters\n+    /// are conceptually part of the model but still computed separately until final merging.\n+    /// </para>\n+    /// <para><b>For Beginners:</b>\n+    /// During inference or training, the input goes through the base layer and ALL adapters\n+    /// in the chain. Their outputs are added together to get the final result. This is how\n+    /// all the sequential adaptations combine to produce the improved output.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        // Forward through base layer\n+        Tensor<T> result = _baseLayer.Forward(input);\n+\n+        // Forward through each adapter in the chain and sum contributions\n+        foreach (var adapter in _adapterChain)\n+        {\n+            Tensor<T> adapterOutput = adapter.Forward(input);\n+\n+            // Add adapter contribution to result\n+            for (int i = 0; i < result.Length; i++)\n+            {\n+                result[i] = NumOps.Add(result[i], adapterOutput[i]);\n+            }\n+        }\n+\n+        return result;\n+    }","path":"src/LoRA/Adapters/ChainLoRAAdapter.cs","commit_id":"fa815036ee570162056ba6aca773f34c80f36948","original_commit_id":"bf7f1556cf26d60d07903cb1a91078b9c9963a87","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**Confusing merge semantics: \"merged\" adapters still computed separately.**\n\nThe forward pass sums contributions from *all* adapters (line 374) regardless of merge status. Per the documentation (lines 359-361), merged adapters are \"conceptually\" part of the model but still computed separately. This creates a misleading API:\n\n1. User calls `MergeActiveAdapter()` expecting weights to merge into the base layer.\n2. The adapter is only marked as frozen (lines 272-273)‚Äîit still participates in every forward/backward pass.\n3. `MergeToOriginalLayer()` (lines 509-523) is a stub that doesn't actually merge weights.\n\nThis design increases inference cost (all adapters computed every forward pass) and violates user expectations. True merging would eliminate per-adapter overhead.\n\n\n\nRecommend one of:\n1. **Implement true merging**: Accumulate adapter weights into the base layer when `MergeActiveAdapter()` is called, then exclude merged adapters from the forward pass.\n2. **Rename methods** to clarify semantics: `FreezeActiveAdapter()` instead of `MergeActiveAdapter()`, making it clear weights are *not* merged until `MergeToOriginalLayer()`.\n3. **Document prominently** (in method docs, not just remarks) that merge is deferred and all adapters remain active during training/inference.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/LoRA/Adapters/ChainLoRAAdapter.cs around lines 368-386, the Forward\nmethod currently sums outputs from every adapter regardless of merge status,\nwhile MergeActiveAdapter only marks adapters frozen and MergeToOriginalLayer is\na stub; implement true merging so MergeActiveAdapter folds the active adapter's\nweight deltas into the base layer weights (apply adapter weight contribution\ndirectly into the base layer parameters and remove or mark the adapter as\nmerged), update Forward to skip merged adapters (so merged adapters are not\ncomputed per forward pass), and implement MergeToOriginalLayer to restore the\noriginal base weights from a saved backup so the merge can be undone; ensure you\ncopy/accumulate tensors correctly, handle device/dtype consistency, and preserve\ngradient/optimizer state or clear adapter gradients when folding to avoid\ndouble-counting.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T03:40:20Z","updated_at":"2025-11-02T03:40:20Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484162727","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484162727"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484162727"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484162727/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":384,"original_start_line":368,"start_side":"RIGHT","line":402,"original_line":386,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":386,"position":402,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484162728","pull_request_review_id":3408080425,"id":2484162728,"node_id":"PRRC_kwDOKSXUF86UEVCo","diff_hunk":"@@ -0,0 +1,660 @@\n+using AiDotNet.Interfaces;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// Chain-of-LoRA adapter that implements sequential composition of multiple LoRA adapters.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// Chain-of-LoRA (COLA) is an advanced LoRA technique that enables sequential composition\n+/// of multiple LoRA adaptations through an iterative optimization framework. Unlike standard\n+/// LoRA which applies a single low-rank adaptation, COLA builds a chain of adaptations where\n+/// each adapter is trained, merged into the model, and then a new adapter is initialized for\n+/// further refinement.\n+/// </para>\n+/// <para>\n+/// This approach bridges the performance gap between standard LoRA and full fine-tuning by\n+/// employing residual learning principles. Each iteration in the chain adds incremental\n+/// improvements to the model's task-specific performance without incurring additional\n+/// computational costs or memory overhead during inference.\n+/// </para>\n+/// <para><b>Key Concepts:</b>\n+///\n+/// <b>Sequential Adaptation:</b>\n+/// Chain-of-LoRA applies adaptations in sequence (Task A ‚Üí Task B ‚Üí Task C), where each\n+/// stage builds upon the previous one. This is inspired by the Frank-Wolfe optimization\n+/// algorithm, which makes greedy updates along the direction of maximum improvement.\n+///\n+/// <b>Merge and Re-initialize:</b>\n+/// After training each LoRA adapter, the learned weights are merged back into the base layer,\n+/// and a new LoRA adapter is initialized. This \"tying a knot\" process allows the model to\n+/// consolidate learned knowledge before adding new adaptations.\n+///\n+/// <b>Knowledge Preservation:</b>\n+/// By freezing the base layer and only training the LoRA components, the chain preserves\n+/// previously learned knowledge while allowing new task-specific adaptations. Each adapter\n+/// in the chain captures a specific aspect of the task or a refinement step.\n+///\n+/// <b>Incremental Fine-tuning Pipeline:</b>\n+/// COLA enables continual learning scenarios where tasks are presented sequentially, and\n+/// the model must adapt to new tasks while maintaining performance on previous ones.\n+/// </para>\n+/// <para><b>Benefits of Chain-of-LoRA:</b>\n+///\n+/// - <b>Better Performance:</b> Achieves up to 6.47% relative accuracy gain over standard LoRA\n+/// - <b>No Extra Overhead:</b> After merging, inference cost is identical to the base model\n+/// - <b>Modular Adaptation:</b> Each adapter can be trained, tested, and validated independently\n+/// - <b>Catastrophic Forgetting Mitigation:</b> Sequential merging helps preserve prior knowledge\n+/// - <b>Task Chaining:</b> Naturally supports multi-task learning and transfer learning scenarios\n+/// - <b>Flexible Deployment:</b> Can deploy the full chain or selected adapters as needed\n+/// </para>\n+/// <para><b>For Beginners:</b>\n+///\n+/// Imagine you're learning a complex skill in stages:\n+/// 1. First, you learn the basics (Adapter 1)\n+/// 2. Then you practice and the basics become automatic (Merge)\n+/// 3. Next, you learn intermediate techniques on top of the basics (Adapter 2)\n+/// 4. Again, you practice until they're automatic (Merge)\n+/// 5. Finally, you learn advanced skills building on everything before (Adapter 3)\n+///\n+/// Chain-of-LoRA works the same way: each adapter learns something new, then it's consolidated\n+/// into the model, and the next adapter can focus on the next refinement. This stepwise approach\n+/// often achieves better results than trying to learn everything at once.\n+/// </para>\n+/// <para><b>Research Reference:</b>\n+///\n+/// Based on \"Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning\"\n+/// (arXiv:2401.04151, January 2024). The paper demonstrates that sequential low-rank adaptations\n+/// can significantly improve task performance compared to single-stage LoRA, especially on\n+/// complex reasoning and multi-step tasks.\n+/// </para>\n+/// <para><b>Usage Example:</b>\n+/// <code>\n+/// // Create a chain with 3 sequential adaptations\n+/// var chain = new ChainLoRAAdapter&lt;double&gt;(baseLayer, rank: 8, chainLength: 3);\n+///\n+/// // Train first adapter on Task A\n+/// chain.SetActiveAdapterIndex(0);\n+/// TrainModel(chain, taskAData);\n+/// chain.MergeActiveAdapter(); // Consolidate Task A knowledge\n+///\n+/// // Train second adapter on Task B\n+/// chain.SetActiveAdapterIndex(1);\n+/// TrainModel(chain, taskBData);\n+/// chain.MergeActiveAdapter(); // Consolidate Task B knowledge\n+///\n+/// // Train third adapter on Task C\n+/// chain.SetActiveAdapterIndex(2);\n+/// TrainModel(chain, taskCData);\n+///\n+/// // Deploy: all adaptations are now part of the model\n+/// ILayer&lt;double&gt; finalLayer = chain.MergeToOriginalLayer();\n+/// </code>\n+/// </para>\n+/// </remarks>\n+public class ChainLoRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// The chain of LoRA adapters applied sequentially.\n+    /// </summary>\n+    private readonly List<LoRALayer<T>> _adapterChain;\n+\n+    /// <summary>\n+    /// The index of the currently active adapter being trained.\n+    /// </summary>\n+    private int _activeAdapterIndex;\n+\n+    /// <summary>\n+    /// Whether each adapter in the chain has been merged.\n+    /// </summary>\n+    private readonly List<bool> _mergedStatus;\n+\n+    /// <summary>\n+    /// The total length of the adapter chain.\n+    /// </summary>\n+    private readonly int _chainLength;\n+\n+    /// <summary>\n+    /// Gets the total number of adapters in the chain.\n+    /// </summary>\n+    /// <remarks>\n+    /// This represents the maximum number of sequential adaptation stages that can be applied.\n+    /// Each adapter can be trained independently and then merged before proceeding to the next.\n+    /// </remarks>\n+    public int ChainLength => _chainLength;\n+\n+    /// <summary>\n+    /// Gets the index of the currently active adapter (0-based).\n+    /// </summary>\n+    /// <remarks>\n+    /// The active adapter is the one currently being trained. Other adapters in the chain\n+    /// are either waiting to be trained (higher indices) or have been merged (lower indices).\n+    /// </remarks>\n+    public int ActiveAdapterIndex => _activeAdapterIndex;\n+\n+    /// <summary>\n+    /// Gets the list of LoRA adapters in the chain.\n+    /// </summary>\n+    /// <remarks>\n+    /// Each adapter in the chain represents one stage of sequential adaptation.\n+    /// Adapters are applied in order during forward passes.\n+    /// </remarks>\n+    public IReadOnlyList<LoRALayer<T>> AdapterChain => _adapterChain.AsReadOnly();\n+\n+    /// <summary>\n+    /// Gets the merged status of each adapter in the chain.\n+    /// </summary>\n+    /// <remarks>\n+    /// True indicates that an adapter has been merged into the base layer and should\n+    /// no longer contribute trainable parameters. Merged adapters still contribute\n+    /// to the forward pass until the entire chain is collapsed.\n+    /// </remarks>\n+    public IReadOnlyList<bool> MergedStatus => _mergedStatus.AsReadOnly();\n+\n+    /// <summary>\n+    /// Initializes a new Chain-of-LoRA adapter with the specified configuration.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with the LoRA chain.</param>\n+    /// <param name=\"rank\">The rank of each LoRA decomposition in the chain.</param>\n+    /// <param name=\"chainLength\">The number of sequential adapters in the chain (default: 3).</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor for each adapter (defaults to rank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training (default: true).</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when chainLength is less than 1.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// Creates a chain of LoRA adapters for sequential fine-tuning. Each adapter in the chain\n+    /// can be trained independently, merged into the model, and then the next adapter can be\n+    /// activated for further refinement.\n+    /// </para>\n+    /// <para><b>For Beginners:</b>\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to adapt (e.g., a dense or convolutional layer)\n+    /// - rank: How compressed each adapter is (lower = fewer parameters per stage)\n+    /// - chainLength: How many sequential adaptation stages you want (typical: 2-5)\n+    /// - alpha: Controls adaptation strength (usually equals rank)\n+    /// - freezeBaseLayer: Lock base weights to preserve pre-trained knowledge (recommended: true)\n+    ///\n+    /// Example: chainLength=3 means you can do three rounds of training and merging,\n+    /// allowing the model to incrementally improve on complex tasks.\n+    /// </para>\n+    /// </remarks>\n+    public ChainLoRAAdapter(\n+        ILayer<T> baseLayer,\n+        int rank,\n+        int chainLength = 3,\n+        double alpha = -1,\n+        bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        if (chainLength < 1)\n+        {\n+            throw new ArgumentException(\"Chain length must be at least 1\", nameof(chainLength));\n+        }\n+\n+        _chainLength = chainLength;\n+        _activeAdapterIndex = 0;\n+        _adapterChain = new List<LoRALayer<T>>(chainLength);\n+        _mergedStatus = new List<bool>(chainLength);\n+\n+        // Create the chain of LoRA adapters\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+\n+        for (int i = 0; i < chainLength; i++)\n+        {\n+            var adapter = new LoRALayer<T>(inputSize, outputSize, rank, alpha);\n+            _adapterChain.Add(adapter);\n+            _mergedStatus.Add(false);\n+        }\n+\n+        // Update parameter count to reflect all unmerged adapters\n+        UpdateParameterCount();\n+    }\n+\n+    /// <summary>\n+    /// Sets which adapter in the chain is currently active for training.\n+    /// </summary>\n+    /// <param name=\"index\">The 0-based index of the adapter to activate.</param>\n+    /// <exception cref=\"ArgumentOutOfRangeException\">Thrown when index is out of range.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// Only the active adapter receives gradient updates during training. Other adapters\n+    /// are either frozen (already merged) or inactive (waiting to be trained).\n+    /// </para>\n+    /// <para><b>For Beginners:</b>\n+    /// This is like choosing which stage of learning you're currently working on.\n+    /// Set to 0 for the first stage, 1 for the second, etc. Only that stage's adapter\n+    /// will be trained while the others remain frozen.\n+    /// </para>\n+    /// </remarks>\n+    public void SetActiveAdapterIndex(int index)\n+    {\n+        if (index < 0 || index >= _chainLength)\n+        {\n+            throw new ArgumentOutOfRangeException(nameof(index), $\"Index must be between 0 and {_chainLength - 1}\");\n+        }\n+\n+        _activeAdapterIndex = index;\n+    }\n+\n+    /// <summary>\n+    /// Merges the currently active adapter into the base layer representation.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This \"ties a knot\" in the chain by marking the active adapter as merged and frozen.\n+    /// The adapter's weights are conceptually incorporated into the model, allowing the\n+    /// next adapter in the chain to build upon this consolidated knowledge.\n+    /// </para>\n+    /// <para>\n+    /// Note: The actual weight merging into a single layer happens when MergeToOriginalLayer()\n+    /// is called. This method only marks the adapter as merged for training purposes.\n+    /// </para>\n+    /// <para><b>For Beginners:</b>\n+    /// After training an adapter stage, call this to \"lock it in\" before moving to the\n+    /// next stage. It's like saving your progress before starting the next level.\n+    /// </para>\n+    /// </remarks>\n+    public void MergeActiveAdapter()\n+    {\n+        if (_activeAdapterIndex < 0 || _activeAdapterIndex >= _chainLength)\n+        {\n+            throw new InvalidOperationException($\"Invalid active adapter index: {_activeAdapterIndex}\");\n+        }\n+\n+        _mergedStatus[_activeAdapterIndex] = true;\n+        UpdateParameterCount();\n+    }\n+\n+    /// <summary>\n+    /// Unmerges a previously merged adapter, making it trainable again.\n+    /// </summary>\n+    /// <param name=\"index\">The index of the adapter to unmerge.</param>\n+    /// <exception cref=\"ArgumentOutOfRangeException\">Thrown when index is out of range.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// This allows re-training a previously merged adapter if needed for iterative refinement.\n+    /// Useful for scenarios where you want to go back and adjust an earlier stage.\n+    /// </para>\n+    /// </remarks>\n+    public void UnmergeAdapter(int index)\n+    {\n+        if (index < 0 || index >= _chainLength)\n+        {\n+            throw new ArgumentOutOfRangeException(nameof(index), $\"Index must be between 0 and {_chainLength - 1}\");\n+        }\n+\n+        _mergedStatus[index] = false;\n+        UpdateParameterCount();\n+    }\n+\n+    /// <summary>\n+    /// Gets the number of adapters that have been merged.\n+    /// </summary>\n+    /// <returns>Count of merged adapters.</returns>\n+    public int GetMergedCount()\n+    {\n+        return _mergedStatus.Count(merged => merged);\n+    }\n+\n+    /// <summary>\n+    /// Gets the total number of parameters in the chain (base layer + all unmerged adapters).\n+    /// </summary>\n+    /// <remarks>\n+    /// This count includes parameters from the base layer (if not frozen) plus all unmerged adapters in the chain.\n+    /// Merged adapters don't contribute to the parameter count since they've been absorbed into the base weights.\n+    /// </remarks>\n+    public override int ParameterCount\n+    {\n+        get\n+        {\n+            int count = 0;\n+\n+            // Add base layer parameters if not frozen\n+            if (!_freezeBaseLayer)\n+            {\n+                count += _baseLayer.ParameterCount;\n+            }\n+\n+            // Add unmerged adapter parameters\n+            for (int i = 0; i < _chainLength; i++)\n+            {\n+                if (!_mergedStatus[i])\n+                {\n+                    count += _adapterChain[i].ParameterCount;\n+                }\n+            }\n+\n+            return count;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Gets the number of adapters that are still trainable (not merged).\n+    /// </summary>\n+    /// <returns>Count of unmerged adapters.</returns>\n+    public int GetTrainableAdapterCount()\n+    {\n+        return _mergedStatus.Count(merged => !merged);\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass through the base layer and all adapters in the chain.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Output with all adapter contributions summed.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The forward pass computes:\n+    /// output = base_layer(input) + adapter_0(input) + adapter_1(input) + ... + adapter_n(input)\n+    /// </para>\n+    /// <para>\n+    /// All adapters contribute to the output, regardless of merge status. Merged adapters\n+    /// are conceptually part of the model but still computed separately until final merging.\n+    /// </para>\n+    /// <para><b>For Beginners:</b>\n+    /// During inference or training, the input goes through the base layer and ALL adapters\n+    /// in the chain. Their outputs are added together to get the final result. This is how\n+    /// all the sequential adaptations combine to produce the improved output.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        // Forward through base layer\n+        Tensor<T> result = _baseLayer.Forward(input);\n+\n+        // Forward through each adapter in the chain and sum contributions\n+        foreach (var adapter in _adapterChain)\n+        {\n+            Tensor<T> adapterOutput = adapter.Forward(input);\n+\n+            // Add adapter contribution to result\n+            for (int i = 0; i < result.Length; i++)\n+            {\n+                result[i] = NumOps.Add(result[i], adapterOutput[i]);\n+            }\n+        }\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Performs the backward pass through all layers in the chain.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient flowing back from the next layer.</param>\n+    /// <returns>Gradient to pass to the previous layer.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Gradients flow through all adapters and the base layer. Only unmerged adapters\n+    /// and the base layer (if not frozen) receive parameter updates.\n+    /// </para>\n+    /// <para><b>For Beginners:</b>\n+    /// During learning, this figures out how to improve each adapter. Only the active,\n+    /// unmerged adapter gets updated - the others are frozen to preserve their knowledge.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        // Initialize input gradient accumulator\n+        Tensor<T> inputGrad = new Tensor<T>(GetInputShape());\n+\n+        // Backward through each adapter in the chain\n+        for (int i = 0; i < _adapterChain.Count; i++)\n+        {\n+            Tensor<T> adapterInputGrad = _adapterChain[i].Backward(outputGradient);\n+\n+            // Accumulate input gradients\n+            for (int j = 0; j < inputGrad.Length; j++)\n+            {\n+                inputGrad[j] = NumOps.Add(inputGrad[j], adapterInputGrad[j]);\n+            }\n+        }\n+\n+        // ALWAYS backward through base layer to get input gradients\n+        // Even when frozen, we need the base layer's Jacobian to propagate gradients to input\n+        // Freezing only prevents parameter updates, not gradient computation\n+        Tensor<T> baseInputGrad = _baseLayer.Backward(outputGradient);\n+\n+        // Accumulate base layer gradients\n+        for (int j = 0; j < inputGrad.Length; j++)\n+        {\n+            inputGrad[j] = NumOps.Add(inputGrad[j], baseInputGrad[j]);\n+        }\n+\n+        // Update parameter gradients\n+        UpdateParameterGradientsFromChain();\n+\n+        return inputGrad;\n+    }\n+\n+    /// <summary>\n+    /// Updates parameters using the specified learning rate.\n+    /// </summary>\n+    /// <param name=\"learningRate\">The learning rate for parameter updates.</param>\n+    /// <remarks>\n+    /// Only the active unmerged adapter receives updates. Merged adapters and the base layer\n+    /// (if frozen) do not receive parameter updates.\n+    /// </remarks>\n+    public override void UpdateParameters(T learningRate)\n+    {\n+        // Update base layer only if not frozen\n+        if (!_freezeBaseLayer)\n+        {\n+            _baseLayer.UpdateParameters(learningRate);\n+        }\n+\n+        // Update only the active unmerged adapter\n+        if (_activeAdapterIndex >= 0 && _activeAdapterIndex < _chainLength && !_mergedStatus[_activeAdapterIndex])\n+        {\n+            _adapterChain[_activeAdapterIndex].UpdateParameters(learningRate);\n+        }\n+\n+        // Update parameter vector\n+        UpdateParametersFromChain();\n+    }\n+\n+    /// <summary>\n+    /// Gets the current parameters as a vector.\n+    /// </summary>\n+    /// <returns>Vector containing parameters from base layer (if not frozen) and all unmerged adapters.</returns>\n+    public override Vector<T> GetParameters()\n+    {\n+        return Parameters.Clone();\n+    }\n+\n+    /// <summary>\n+    /// Sets the layer parameters from a vector.\n+    /// </summary>\n+    /// <param name=\"parameters\">Vector containing parameters.</param>\n+    /// <exception cref=\"ArgumentException\">Thrown when parameter count doesn't match.</exception>\n+    public override void SetParameters(Vector<T> parameters)\n+    {\n+        if (parameters.Length != ParameterCount)\n+        {\n+            throw new ArgumentException($\"Expected {ParameterCount} parameters, got {parameters.Length}\", nameof(parameters));\n+        }\n+\n+        Parameters = parameters.Clone();\n+        UpdateChainFromParameters();\n+    }\n+\n+    /// <summary>\n+    /// Merges all adapters in the chain into the original base layer.\n+    /// </summary>\n+    /// <returns>A new layer with all LoRA adaptations merged into the base weights.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This creates a single layer that includes all the sequential adaptations from the chain.\n+    /// The resulting layer has the same computational cost as the original base layer but\n+    /// includes all the learned improvements from each stage of the chain.\n+    /// </para>\n+    /// <para><b>For Beginners:</b>\n+    /// After training all stages of the chain, call this to create a final optimized layer.\n+    /// The result is a regular layer (no LoRA overhead) that performs as well as the full chain.\n+    /// Perfect for deployment when you want maximum speed with all the learned adaptations.\n+    /// </para>\n+    /// <para><b>Implementation Note:</b>\n+    /// This is a simplified implementation that returns the base layer. In a full implementation,\n+    /// you would merge all adapter weights into a cloned base layer. The merging strategy depends\n+    /// on the specific layer type (Dense, Convolutional, etc.).\n+    /// </para>\n+    /// </remarks>\n+    public override ILayer<T> MergeToOriginalLayer()\n+    {\n+        // Note: This is a simplified implementation that returns the base layer.\n+        // In a production implementation, you would:\n+        // 1. Clone the base layer\n+        // 2. For each adapter in the chain, compute the low-rank update (B √ó A)\n+        // 3. Scale by (alpha / rank)\n+        // 4. Add to the cloned layer's weights\n+        // 5. Return the merged layer\n+        //\n+        // The exact merging process depends on the base layer type and is typically\n+        // implemented by derived classes that specialize for specific layer types.\n+\n+        return _baseLayer;\n+    }","path":"src/LoRA/Adapters/ChainLoRAAdapter.cs","commit_id":"fa815036ee570162056ba6aca773f34c80f36948","original_commit_id":"bf7f1556cf26d60d07903cb1a91078b9c9963a87","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Critical: MergeToOriginalLayer is a non-functional stub.**\n\nThis method returns the base layer unchanged (line 522), ignoring all adapter weights. The comments (lines 511-520) acknowledge this is a placeholder, but deployment without merging defeats a core benefit of LoRA: zero-overhead inference after training.\n\nThe `DenseLoRAAdapter.cs` example (lines 96-142 in relevant snippets) demonstrates proper merging. `ChainLoRAAdapter` should similarly:\n1. Clone the base layer.\n2. For each adapter in `_adapterChain`, compute `B √ó A` scaled by `(alpha / rank)`.\n3. Add each update to the cloned layer's weights.\n4. Return the merged layer.\n\n\n\n\nThis is essential for production use. Do you want me to generate a implementation outline for merging chain adapters into a deployable layer, or open a tracking issue?\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/LoRA/Adapters/ChainLoRAAdapter.cs around lines 509-523,\nMergeToOriginalLayer is a non-functional stub that returns the base layer\nunchanged; replace it with logic that clones the base layer, iterates over\n_adapterChain, computes each adapter's low-rank update (B √ó A), scales it by\n(alpha / rank), adds the scaled update into the cloned layer's weight tensor\n(taking care of matching shapes and broadcasting rules), and returns the cloned\nmerged layer; use the DenseLoRAAdapter merge implementation (lines ~96-142) as a\nreference for cloning, arithmetic, and numeric types, and ensure any\nlayer-type-specific merging is delegated to derived classes or guarded by type\nchecks.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T03:40:20Z","updated_at":"2025-11-02T03:40:20Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484162728","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484162728"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484162728"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484162728/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":525,"original_start_line":509,"start_side":"RIGHT","line":539,"original_line":523,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":523,"position":539,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484862619","pull_request_review_id":3408834322,"id":2484862619,"node_id":"PRRC_kwDOKSXUF86UG_6b","diff_hunk":"@@ -0,0 +1,584 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// Adaptive Low-Rank Adaptation (AdaLoRA) adapter that dynamically allocates parameter budgets among weight matrices.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// AdaLoRA improves upon standard LoRA by dynamically adjusting the rank allocation based on importance scores.\n+/// Instead of using a fixed rank for all weight matrices, AdaLoRA:\n+/// - Starts with a maximum rank and adaptively reduces it during training\n+/// - Computes importance scores for each singular value component\n+/// - Prunes less important components to focus parameter budget on critical adaptations\n+/// - Allows different layers to have different effective ranks\n+/// </para>\n+/// <para>\n+/// This leads to more efficient parameter usage compared to fixed-rank LoRA, especially for large models\n+/// where some layers need more adaptation capacity than others.\n+/// </para>\n+/// <para><b>For Beginners:</b> AdaLoRA is like smart LoRA that learns which parts of the adaptation matter most.\n+///\n+/// Think of standard LoRA as giving every layer the same budget (rank=8 everywhere).\n+/// AdaLoRA is smarter:\n+/// - Some layers get more budget (rank=16) because they're important for the task\n+/// - Other layers get less budget (rank=2) because small changes are enough\n+/// - The model learns this automatically during training\n+///\n+/// How it works:\n+/// 1. Start with a large rank (e.g., maxRank=32)\n+/// 2. During training, track how important each component is\n+/// 3. Prune components with low importance scores\n+/// 4. Focus parameters on what actually helps\n+///\n+/// Benefits:\n+/// - More parameter-efficient than fixed-rank LoRA\n+/// - Better performance with same parameter budget\n+/// - Automatically finds optimal rank per layer\n+///\n+/// Reference: \"Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning\" (ICLR 2023)\n+/// https://arxiv.org/abs/2303.10512\n+/// </para>\n+/// </remarks>\n+public class AdaLoRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Maximum possible rank for this adapter.\n+    /// </summary>\n+    /// <remarks>\n+    /// The adapter starts with this rank and may reduce it during training through pruning.\n+    /// This is the upper bound on the number of singular value components.\n+    /// </remarks>\n+    private readonly int _maxRank;\n+\n+    /// <summary>\n+    /// Current active rank after pruning.\n+    /// </summary>\n+    /// <remarks>\n+    /// This represents the number of singular value components currently being used.\n+    /// It starts at maxRank and decreases as low-importance components are pruned.\n+    /// </remarks>\n+    private int _currentRank;\n+\n+    /// <summary>\n+    /// Importance scores for each singular value component.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Each score represents how important that singular value is for the adaptation.\n+    /// Higher scores indicate more important components that should be retained.\n+    /// These scores are updated during training based on gradient magnitudes.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Think of these as \"usefulness ratings\" for each component.\n+    /// Components with high scores are helping a lot, low scores mean they're not doing much.\n+    /// We keep the high-scoring components and prune the low-scoring ones.\n+    /// </para>\n+    /// </remarks>\n+    private Vector<T> _importanceScores;\n+\n+    /// <summary>\n+    /// Threshold for pruning singular values based on importance.\n+    /// </summary>\n+    /// <remarks>\n+    /// Components with importance scores below this threshold are candidates for pruning.\n+    /// This value is typically set as a small fraction (e.g., 0.01 to 0.1).\n+    /// </remarks>\n+    private readonly double _rankPruningThreshold;\n+\n+    /// <summary>\n+    /// Exponential moving average factor for importance score updates.\n+    /// </summary>\n+    /// <remarks>\n+    /// Controls how quickly importance scores adapt to new gradient information.\n+    /// Typical values: 0.9 to 0.99 (higher = more smoothing, lower = faster adaptation)\n+    /// </remarks>\n+    private readonly double _importanceScoreEMA;\n+\n+    /// <summary>\n+    /// Minimum rank to maintain (prevents pruning below this threshold).\n+    /// </summary>\n+    private readonly int _minRank;\n+\n+    /// <summary>\n+    /// Number of training steps between rank pruning operations.\n+    /// </summary>\n+    private readonly int _pruningInterval;\n+\n+    /// <summary>\n+    /// Current training step counter.\n+    /// </summary>\n+    private int _stepCount;\n+\n+    /// <summary>\n+    /// Gets the maximum rank this adapter can use.\n+    /// </summary>\n+    public int MaxRank => _maxRank;\n+\n+    /// <summary>\n+    /// Gets the current active rank after pruning.\n+    /// </summary>\n+    public int CurrentRank => _currentRank;\n+\n+    /// <summary>\n+    /// Gets a copy of the current importance scores.\n+    /// </summary>\n+    public Vector<T> GetImportanceScores() => _importanceScores.Clone();\n+\n+    /// <summary>\n+    /// Initializes a new AdaLoRA adapter with adaptive rank allocation.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with AdaLoRA.</param>\n+    /// <param name=\"maxRank\">The maximum rank for the LoRA decomposition.</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to maxRank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <param name=\"rankPruningThreshold\">Threshold for pruning based on importance scores (default: 0.05).</param>\n+    /// <param name=\"minRank\">Minimum rank to maintain after pruning (default: 1).</param>\n+    /// <param name=\"pruningInterval\">Number of steps between pruning operations (default: 100).</param>\n+    /// <param name=\"importanceScoreEMA\">EMA factor for importance score updates (default: 0.95).</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when rank parameters are invalid.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This creates an AdaLoRA adapter with smart rank allocation.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to adapt (typically Dense or FullyConnected)\n+    /// - maxRank: Start with this many components (will prune down during training)\n+    /// - alpha: How strong the adaptation is\n+    /// - freezeBaseLayer: Lock the original weights (usually true for efficiency)\n+    /// - rankPruningThreshold: How unimportant a component must be to get pruned (0.05 = bottom 5%)\n+    /// - minRank: Never prune below this rank (safety net)\n+    /// - pruningInterval: How often to check for pruning (in training steps)\n+    /// - importanceScoreEMA: How smooth importance tracking is (higher = more stable)\n+    ///\n+    /// The adapter will automatically adjust its rank during training to focus parameters\n+    /// on the most important components.\n+    /// </para>\n+    /// </remarks>\n+    public AdaLoRAAdapter(\n+        ILayer<T> baseLayer,\n+        int maxRank,\n+        double alpha = -1,\n+        bool freezeBaseLayer = true,\n+        double rankPruningThreshold = 0.05,\n+        int minRank = 1,\n+        int pruningInterval = 100,\n+        double importanceScoreEMA = 0.95)\n+        : base(baseLayer, maxRank, alpha, freezeBaseLayer)\n+    {\n+        if (minRank < 1)\n+        {\n+            throw new ArgumentException(\"Minimum rank must be at least 1\", nameof(minRank));\n+        }\n+\n+        if (minRank > maxRank)\n+        {\n+            throw new ArgumentException($\"Minimum rank ({minRank}) cannot exceed maximum rank ({maxRank})\", nameof(minRank));\n+        }\n+\n+        if (rankPruningThreshold <= 0 || rankPruningThreshold >= 1)\n+        {\n+            throw new ArgumentException(\"Rank pruning threshold must be between 0 and 1\", nameof(rankPruningThreshold));\n+        }\n+\n+        if (importanceScoreEMA <= 0 || importanceScoreEMA >= 1)\n+        {\n+            throw new ArgumentException(\"Importance score EMA factor must be between 0 and 1\", nameof(importanceScoreEMA));\n+        }\n+\n+        _maxRank = maxRank;\n+        _currentRank = maxRank;\n+        _rankPruningThreshold = rankPruningThreshold;\n+        _minRank = minRank;\n+        _pruningInterval = pruningInterval;\n+        _importanceScoreEMA = importanceScoreEMA;\n+        _stepCount = 0;\n+\n+        // Initialize importance scores (start with uniform importance)\n+        _importanceScores = new Vector<T>(maxRank);\n+        T initialScore = NumOps.One;\n+        for (int i = 0; i < maxRank; i++)\n+        {\n+            _importanceScores[i] = initialScore;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass using only the top-k most important singular values.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Sum of base layer output and AdaLoRA output (using current rank).</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Unlike standard LoRA which uses all rank components, AdaLoRA only uses the currentRank\n+    /// most important components based on importance scores. This is more efficient and focuses\n+    /// computation on the most impactful adaptations.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This computes the output using only the important components.\n+    /// If we started with rank=32 but pruned to rank=8, we only use the top 8 most important\n+    /// singular values. This makes computation faster and more focused.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        // Forward through base layer\n+        Tensor<T> baseOutput = _baseLayer.Forward(input);\n+\n+        // Forward through LoRA layer (it will use all components, but we'll mask based on importance)\n+        Tensor<T> loraOutput = _loraLayer.Forward(input);\n+\n+        // If current rank < max rank, we need to mask the output\n+        // This is implicitly handled by the pruned matrices in the LoRA layer\n+        // For simplicity, we use the LoRA output as-is (pruning happens in UpdateParameters)\n+\n+        // Sum the outputs\n+        Tensor<T> result = new Tensor<T>(baseOutput.Shape);\n+        for (int i = 0; i < baseOutput.Length; i++)\n+        {\n+            result[i] = NumOps.Add(baseOutput[i], loraOutput[i]);\n+        }\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Performs the backward pass and updates importance scores based on gradients.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient flowing back from the next layer.</param>\n+    /// <returns>Gradient to pass to the previous layer.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// During backpropagation, AdaLoRA computes importance scores based on the magnitude of\n+    /// gradients for each singular value component. Components with consistently large gradients\n+    /// are considered more important.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is where we learn which components are important!\n+    /// As gradients flow back:\n+    /// 1. We see which components have large gradients (they're actively learning)\n+    /// 2. We update their importance scores (high gradients = high importance)\n+    /// 3. We use exponential moving average to smooth out noise\n+    ///\n+    /// Components that consistently get small gradients aren't helping much,\n+    /// so they'll get low importance scores and eventually be pruned.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        // Backward through both layers\n+        Tensor<T> loraInputGrad = _loraLayer.Backward(outputGradient);\n+        Tensor<T> baseInputGrad = _baseLayer.Backward(outputGradient);\n+\n+        // Update importance scores based on gradient magnitudes\n+        UpdateImportanceScores();\n+\n+        // Increment step count and check if we should prune\n+        _stepCount++;\n+        if (_stepCount % _pruningInterval == 0 && _currentRank > _minRank)\n+        {\n+            PruneRank();\n+        }\n+\n+        // Sum input gradients\n+        Tensor<T> inputGrad = new Tensor<T>(loraInputGrad.Shape);\n+        for (int i = 0; i < loraInputGrad.Length; i++)\n+        {\n+            inputGrad[i] = NumOps.Add(loraInputGrad[i], baseInputGrad[i]);\n+        }\n+\n+        return inputGrad;\n+    }\n+\n+    /// <summary>\n+    /// Updates importance scores based on current gradient magnitudes.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Importance is computed using exponential moving average of gradient magnitudes.\n+    /// For each component i: importance[i] = ema * importance[i] + (1 - ema) * |gradient[i]|\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This updates our \"usefulness ratings\" for each component.\n+    ///\n+    /// We use exponential moving average (EMA) which is like a smoothed average:\n+    /// - New score = 0.95 * old_score + 0.05 * current_gradient_magnitude\n+    ///\n+    /// This way, a component needs to consistently have high gradients to get a high score.\n+    /// A single spike won't cause us to keep an unimportant component.\n+    /// </para>\n+    /// </remarks>\n+    private void UpdateImportanceScores()\n+    {\n+        // Get the LoRA layer's parameter gradients\n+        Vector<T> loraGradients = _loraLayer.GetParameterGradients();\n+\n+        // The LoRA layer stores parameters as [A matrix flattened, B matrix flattened]\n+        // We need to compute importance per rank component\n+        Matrix<T> matrixA = _loraLayer.GetMatrixA();\n+        Matrix<T> matrixB = _loraLayer.GetMatrixB();\n+\n+        int inputSize = matrixA.Rows;\n+        int outputSize = matrixB.Columns;\n+\n+        // For each rank component, compute gradient magnitude\n+        for (int r = 0; r < _currentRank; r++)\n+        {\n+            // Compute L2 norm of gradients for this rank component\n+            T gradMagnitude = NumOps.Zero;\n+\n+            // Gradients from matrix A for column r\n+            for (int i = 0; i < inputSize; i++)\n+            {\n+                T grad = loraGradients[i * _maxRank + r];\n+                gradMagnitude = NumOps.Add(gradMagnitude, NumOps.Multiply(grad, grad));\n+            }\n+\n+            // Gradients from matrix B for row r\n+            int bOffset = inputSize * _maxRank;\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                T grad = loraGradients[bOffset + r * outputSize + j];\n+                gradMagnitude = NumOps.Add(gradMagnitude, NumOps.Multiply(grad, grad));\n+            }\n+\n+            gradMagnitude = NumOps.Sqrt(gradMagnitude);\n+\n+            // Update importance score with EMA\n+            T emaFactor = NumOps.FromDouble(_importanceScoreEMA);\n+            T oneMinusEma = NumOps.FromDouble(1.0 - _importanceScoreEMA);\n+\n+            T oldScore = _importanceScores[r];\n+            T newScore = NumOps.Add(\n+                NumOps.Multiply(emaFactor, oldScore),\n+                NumOps.Multiply(oneMinusEma, gradMagnitude)\n+            );\n+\n+            _importanceScores[r] = newScore;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Prunes low-importance singular value components to reduce rank.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This method identifies components with importance scores below the threshold and removes them.\n+    /// The rank is reduced accordingly, focusing parameters on high-importance components.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This removes components that aren't pulling their weight.\n+    ///\n+    /// Process:\n+    /// 1. Look at all importance scores\n+    /// 2. Find components below the threshold\n+    /// 3. Mark them for removal\n+    /// 4. Reduce the current rank\n+    ///\n+    /// For example, if we have 16 components but 8 have very low importance scores,\n+    /// we can prune those 8 and reduce from rank=16 to rank=8.\n+    ///\n+    /// This makes the model:\n+    /// - Faster (fewer components to compute)\n+    /// - More focused (parameters concentrated on what matters)\n+    /// - More efficient (same or better performance with fewer parameters)\n+    /// </para>\n+    /// </remarks>\n+    private void PruneRank()\n+    {\n+        // Compute threshold value (percentile-based pruning)\n+        // We keep the top (1 - threshold) components\n+\n+        // Create a list of (importance, index) pairs for sorting\n+        var importanceList = new List<(T score, int index)>();\n+        for (int i = 0; i < _currentRank; i++)\n+        {\n+            importanceList.Add((_importanceScores[i], i));\n+        }\n+\n+        // Sort by importance (descending)\n+        // Convert to double for comparison since INumericOperations<T> doesn't have Compare\n+        importanceList.Sort((a, b) =>\n+            Convert.ToDouble(b.score).CompareTo(Convert.ToDouble(a.score)));\n+\n+        // Determine new rank (prune bottom threshold fraction)\n+        int componentsToKeep = Math.Max(_minRank, (int)(_currentRank * (1.0 - _rankPruningThreshold)));\n+\n+        // Only prune if we would actually reduce rank\n+        if (componentsToKeep < _currentRank)\n+        {\n+            // Determine which rank indices to keep (top components by importance)\n+            var keepIndices = new HashSet<int>();\n+            for (int i = 0; i < componentsToKeep; i++)\n+            {\n+                keepIndices.Add(importanceList[i].index);\n+            }\n+\n+            // Zero out pruned components in LoRA matrices\n+            // Get matrices A and B from LoRA layer\n+            Matrix<T> matrixA = _loraLayer.GetMatrixA();\n+            Matrix<T> matrixB = _loraLayer.GetMatrixB();\n+\n+            // Zero columns of A and rows of B for pruned rank components\n+            for (int r = 0; r < _maxRank; r++)\n+            {\n+                if (!keepIndices.Contains(r))\n+                {\n+                    // Zero column r of matrix A [inputSize, rank]\n+                    for (int i = 0; i < matrixA.Rows; i++)\n+                    {\n+                        matrixA[i, r] = NumOps.Zero;\n+                    }\n+\n+                    // Zero row r of matrix B [rank, outputSize]\n+                    for (int j = 0; j < matrixB.Columns; j++)\n+                    {\n+                        matrixB[r, j] = NumOps.Zero;\n+                    }\n+                }\n+            }\n+\n+            // Update LoRA layer parameters with zeroed matrices\n+            // Note: LoRALayer.SetParameters expects flattened A then B\n+            Vector<T> loraParams = new Vector<T>(_loraLayer.ParameterCount);\n+            int idx = 0;\n+\n+            // Pack matrix A\n+            for (int i = 0; i < matrixA.Rows; i++)\n+            {\n+                for (int j = 0; j < matrixA.Columns; j++)\n+                {\n+                    loraParams[idx++] = matrixA[i, j];\n+                }\n+            }\n+\n+            // Pack matrix B\n+            for (int i = 0; i < matrixB.Rows; i++)\n+            {\n+                for (int j = 0; j < matrixB.Columns; j++)\n+                {\n+                    loraParams[idx++] = matrixB[i, j];\n+                }\n+            }\n+\n+            _loraLayer.SetParameters(loraParams);\n+\n+            // Update current rank\n+            _currentRank = componentsToKeep;\n+\n+            // Reorder importance scores to keep only the top components\n+            Vector<T> newImportanceScores = new Vector<T>(_maxRank);\n+            for (int i = 0; i < _currentRank; i++)\n+            {\n+                newImportanceScores[i] = importanceList[i].score;\n+            }\n+            for (int i = _currentRank; i < _maxRank; i++)\n+            {\n+                newImportanceScores[i] = NumOps.Zero;\n+            }\n+            _importanceScores = newImportanceScores;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Expands the rank by adding new components (for cases where more capacity is needed).\n+    /// </summary>\n+    /// <param name=\"additionalRank\">Number of components to add.</param>\n+    /// <remarks>\n+    /// <para>\n+    /// This is the opposite of pruning - it adds new components when the model needs more capacity.\n+    /// New components are initialized with low importance and will need to prove their worth.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Sometimes the model realizes it needs more capacity.\n+    /// This method adds new components, giving the model more flexibility to learn.\n+    ///\n+    /// Think of it like hiring more workers when the team is overloaded.\n+    /// The new components start with low importance and have to earn their keep.\n+    /// </para>\n+    /// </remarks>\n+    public void ExpandRank(int additionalRank)\n+    {\n+        if (additionalRank <= 0)\n+        {\n+            throw new ArgumentException(\"Additional rank must be positive\", nameof(additionalRank));\n+        }\n+\n+        int newRank = Math.Min(_currentRank + additionalRank, _maxRank);\n+\n+        if (newRank > _currentRank)\n+        {\n+            // Initialize new components with low importance\n+            T lowImportance = NumOps.FromDouble(0.01);\n+            for (int i = _currentRank; i < newRank; i++)\n+            {\n+                _importanceScores[i] = lowImportance;\n+            }\n+\n+            _currentRank = newRank;\n+        }\n+    }","path":"src/LoRA/Adapters/AdaLoRAAdapter.cs","commit_id":"fa815036ee570162056ba6aca773f34c80f36948","original_commit_id":"fa815036ee570162056ba6aca773f34c80f36948","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Critical: Expanded rank components remain zeroed and can't learn.**\n\nWhen `ExpandRank` adds capacity, it updates `_currentRank` and initializes importance scores (line 511), but it doesn't reinitialize the matrix elements that were previously zeroed by `PruneRank`. Those columns of A and rows of B remain at zero, so the \"expanded\" components contribute nothing and receive zero gradients.\n\n\n\nApply this fix to reinitialize expanded components:\n\n```diff\n public void ExpandRank(int additionalRank)\n {\n     if (additionalRank <= 0)\n     {\n         throw new ArgumentException(\"Additional rank must be positive\", nameof(additionalRank));\n     }\n\n     int newRank = Math.Min(_currentRank + additionalRank, _maxRank);\n\n     if (newRank > _currentRank)\n     {\n+        // Reinitialize the matrices for expanded components\n+        Matrix<T> matrixA = _loraLayer.GetMatrixA();\n+        Matrix<T> matrixB = _loraLayer.GetMatrixB();\n+        \n+        // Use small random initialization for expanded components\n+        Random rand = new Random();\n+        for (int r = _currentRank; r < newRank; r++)\n+        {\n+            // Reinitialize column r of matrix A\n+            for (int i = 0; i < matrixA.Rows; i++)\n+            {\n+                matrixA[i, r] = NumOps.FromDouble((rand.NextDouble() - 0.5) * 0.01);\n+            }\n+            \n+            // Reinitialize row r of matrix B\n+            for (int j = 0; j < matrixB.Columns; j++)\n+            {\n+                matrixB[r, j] = NumOps.FromDouble((rand.NextDouble() - 0.5) * 0.01);\n+            }\n+        }\n+        \n+        // Update LoRA layer parameters\n+        Vector<T> loraParams = new Vector<T>(_loraLayer.ParameterCount);\n+        int idx = 0;\n+        for (int i = 0; i < matrixA.Rows; i++)\n+        {\n+            for (int j = 0; j < matrixA.Columns; j++)\n+            {\n+                loraParams[idx++] = matrixA[i, j];\n+            }\n+        }\n+        for (int i = 0; i < matrixB.Rows; i++)\n+        {\n+            for (int j = 0; j < matrixB.Columns; j++)\n+            {\n+                loraParams[idx++] = matrixB[i, j];\n+            }\n+        }\n+        _loraLayer.SetParameters(loraParams);\n+\n         // Initialize new components with low importance\n         T lowImportance = NumOps.FromDouble(0.01);\n         for (int i = _currentRank; i < newRank; i++)\n         {\n             _importanceScores[i] = lowImportance;\n         }\n\n         _currentRank = newRank;\n     }\n }\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T15:16:54Z","updated_at":"2025-11-02T15:16:55Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484862619","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484862619"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484862619"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484862619/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":496,"original_start_line":496,"start_side":"RIGHT","line":516,"original_line":516,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":516,"position":516,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484862620","pull_request_review_id":3408834322,"id":2484862620,"node_id":"PRRC_kwDOKSXUF86UG_6c","diff_hunk":"@@ -0,0 +1,584 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// Adaptive Low-Rank Adaptation (AdaLoRA) adapter that dynamically allocates parameter budgets among weight matrices.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// AdaLoRA improves upon standard LoRA by dynamically adjusting the rank allocation based on importance scores.\n+/// Instead of using a fixed rank for all weight matrices, AdaLoRA:\n+/// - Starts with a maximum rank and adaptively reduces it during training\n+/// - Computes importance scores for each singular value component\n+/// - Prunes less important components to focus parameter budget on critical adaptations\n+/// - Allows different layers to have different effective ranks\n+/// </para>\n+/// <para>\n+/// This leads to more efficient parameter usage compared to fixed-rank LoRA, especially for large models\n+/// where some layers need more adaptation capacity than others.\n+/// </para>\n+/// <para><b>For Beginners:</b> AdaLoRA is like smart LoRA that learns which parts of the adaptation matter most.\n+///\n+/// Think of standard LoRA as giving every layer the same budget (rank=8 everywhere).\n+/// AdaLoRA is smarter:\n+/// - Some layers get more budget (rank=16) because they're important for the task\n+/// - Other layers get less budget (rank=2) because small changes are enough\n+/// - The model learns this automatically during training\n+///\n+/// How it works:\n+/// 1. Start with a large rank (e.g., maxRank=32)\n+/// 2. During training, track how important each component is\n+/// 3. Prune components with low importance scores\n+/// 4. Focus parameters on what actually helps\n+///\n+/// Benefits:\n+/// - More parameter-efficient than fixed-rank LoRA\n+/// - Better performance with same parameter budget\n+/// - Automatically finds optimal rank per layer\n+///\n+/// Reference: \"Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning\" (ICLR 2023)\n+/// https://arxiv.org/abs/2303.10512\n+/// </para>\n+/// </remarks>\n+public class AdaLoRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Maximum possible rank for this adapter.\n+    /// </summary>\n+    /// <remarks>\n+    /// The adapter starts with this rank and may reduce it during training through pruning.\n+    /// This is the upper bound on the number of singular value components.\n+    /// </remarks>\n+    private readonly int _maxRank;\n+\n+    /// <summary>\n+    /// Current active rank after pruning.\n+    /// </summary>\n+    /// <remarks>\n+    /// This represents the number of singular value components currently being used.\n+    /// It starts at maxRank and decreases as low-importance components are pruned.\n+    /// </remarks>\n+    private int _currentRank;\n+\n+    /// <summary>\n+    /// Importance scores for each singular value component.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Each score represents how important that singular value is for the adaptation.\n+    /// Higher scores indicate more important components that should be retained.\n+    /// These scores are updated during training based on gradient magnitudes.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Think of these as \"usefulness ratings\" for each component.\n+    /// Components with high scores are helping a lot, low scores mean they're not doing much.\n+    /// We keep the high-scoring components and prune the low-scoring ones.\n+    /// </para>\n+    /// </remarks>\n+    private Vector<T> _importanceScores;\n+\n+    /// <summary>\n+    /// Threshold for pruning singular values based on importance.\n+    /// </summary>\n+    /// <remarks>\n+    /// Components with importance scores below this threshold are candidates for pruning.\n+    /// This value is typically set as a small fraction (e.g., 0.01 to 0.1).\n+    /// </remarks>\n+    private readonly double _rankPruningThreshold;\n+\n+    /// <summary>\n+    /// Exponential moving average factor for importance score updates.\n+    /// </summary>\n+    /// <remarks>\n+    /// Controls how quickly importance scores adapt to new gradient information.\n+    /// Typical values: 0.9 to 0.99 (higher = more smoothing, lower = faster adaptation)\n+    /// </remarks>\n+    private readonly double _importanceScoreEMA;\n+\n+    /// <summary>\n+    /// Minimum rank to maintain (prevents pruning below this threshold).\n+    /// </summary>\n+    private readonly int _minRank;\n+\n+    /// <summary>\n+    /// Number of training steps between rank pruning operations.\n+    /// </summary>\n+    private readonly int _pruningInterval;\n+\n+    /// <summary>\n+    /// Current training step counter.\n+    /// </summary>\n+    private int _stepCount;\n+\n+    /// <summary>\n+    /// Gets the maximum rank this adapter can use.\n+    /// </summary>\n+    public int MaxRank => _maxRank;\n+\n+    /// <summary>\n+    /// Gets the current active rank after pruning.\n+    /// </summary>\n+    public int CurrentRank => _currentRank;\n+\n+    /// <summary>\n+    /// Gets a copy of the current importance scores.\n+    /// </summary>\n+    public Vector<T> GetImportanceScores() => _importanceScores.Clone();\n+\n+    /// <summary>\n+    /// Initializes a new AdaLoRA adapter with adaptive rank allocation.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with AdaLoRA.</param>\n+    /// <param name=\"maxRank\">The maximum rank for the LoRA decomposition.</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to maxRank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <param name=\"rankPruningThreshold\">Threshold for pruning based on importance scores (default: 0.05).</param>\n+    /// <param name=\"minRank\">Minimum rank to maintain after pruning (default: 1).</param>\n+    /// <param name=\"pruningInterval\">Number of steps between pruning operations (default: 100).</param>\n+    /// <param name=\"importanceScoreEMA\">EMA factor for importance score updates (default: 0.95).</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when rank parameters are invalid.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This creates an AdaLoRA adapter with smart rank allocation.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to adapt (typically Dense or FullyConnected)\n+    /// - maxRank: Start with this many components (will prune down during training)\n+    /// - alpha: How strong the adaptation is\n+    /// - freezeBaseLayer: Lock the original weights (usually true for efficiency)\n+    /// - rankPruningThreshold: How unimportant a component must be to get pruned (0.05 = bottom 5%)\n+    /// - minRank: Never prune below this rank (safety net)\n+    /// - pruningInterval: How often to check for pruning (in training steps)\n+    /// - importanceScoreEMA: How smooth importance tracking is (higher = more stable)\n+    ///\n+    /// The adapter will automatically adjust its rank during training to focus parameters\n+    /// on the most important components.\n+    /// </para>\n+    /// </remarks>\n+    public AdaLoRAAdapter(\n+        ILayer<T> baseLayer,\n+        int maxRank,\n+        double alpha = -1,\n+        bool freezeBaseLayer = true,\n+        double rankPruningThreshold = 0.05,\n+        int minRank = 1,\n+        int pruningInterval = 100,\n+        double importanceScoreEMA = 0.95)\n+        : base(baseLayer, maxRank, alpha, freezeBaseLayer)\n+    {\n+        if (minRank < 1)\n+        {\n+            throw new ArgumentException(\"Minimum rank must be at least 1\", nameof(minRank));\n+        }\n+\n+        if (minRank > maxRank)\n+        {\n+            throw new ArgumentException($\"Minimum rank ({minRank}) cannot exceed maximum rank ({maxRank})\", nameof(minRank));\n+        }\n+\n+        if (rankPruningThreshold <= 0 || rankPruningThreshold >= 1)\n+        {\n+            throw new ArgumentException(\"Rank pruning threshold must be between 0 and 1\", nameof(rankPruningThreshold));\n+        }\n+\n+        if (importanceScoreEMA <= 0 || importanceScoreEMA >= 1)\n+        {\n+            throw new ArgumentException(\"Importance score EMA factor must be between 0 and 1\", nameof(importanceScoreEMA));\n+        }\n+\n+        _maxRank = maxRank;\n+        _currentRank = maxRank;\n+        _rankPruningThreshold = rankPruningThreshold;\n+        _minRank = minRank;\n+        _pruningInterval = pruningInterval;\n+        _importanceScoreEMA = importanceScoreEMA;\n+        _stepCount = 0;\n+\n+        // Initialize importance scores (start with uniform importance)\n+        _importanceScores = new Vector<T>(maxRank);\n+        T initialScore = NumOps.One;\n+        for (int i = 0; i < maxRank; i++)\n+        {\n+            _importanceScores[i] = initialScore;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass using only the top-k most important singular values.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Sum of base layer output and AdaLoRA output (using current rank).</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Unlike standard LoRA which uses all rank components, AdaLoRA only uses the currentRank\n+    /// most important components based on importance scores. This is more efficient and focuses\n+    /// computation on the most impactful adaptations.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This computes the output using only the important components.\n+    /// If we started with rank=32 but pruned to rank=8, we only use the top 8 most important\n+    /// singular values. This makes computation faster and more focused.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        // Forward through base layer\n+        Tensor<T> baseOutput = _baseLayer.Forward(input);\n+\n+        // Forward through LoRA layer (it will use all components, but we'll mask based on importance)\n+        Tensor<T> loraOutput = _loraLayer.Forward(input);\n+\n+        // If current rank < max rank, we need to mask the output\n+        // This is implicitly handled by the pruned matrices in the LoRA layer\n+        // For simplicity, we use the LoRA output as-is (pruning happens in UpdateParameters)\n+\n+        // Sum the outputs\n+        Tensor<T> result = new Tensor<T>(baseOutput.Shape);\n+        for (int i = 0; i < baseOutput.Length; i++)\n+        {\n+            result[i] = NumOps.Add(baseOutput[i], loraOutput[i]);\n+        }\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Performs the backward pass and updates importance scores based on gradients.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient flowing back from the next layer.</param>\n+    /// <returns>Gradient to pass to the previous layer.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// During backpropagation, AdaLoRA computes importance scores based on the magnitude of\n+    /// gradients for each singular value component. Components with consistently large gradients\n+    /// are considered more important.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is where we learn which components are important!\n+    /// As gradients flow back:\n+    /// 1. We see which components have large gradients (they're actively learning)\n+    /// 2. We update their importance scores (high gradients = high importance)\n+    /// 3. We use exponential moving average to smooth out noise\n+    ///\n+    /// Components that consistently get small gradients aren't helping much,\n+    /// so they'll get low importance scores and eventually be pruned.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        // Backward through both layers\n+        Tensor<T> loraInputGrad = _loraLayer.Backward(outputGradient);\n+        Tensor<T> baseInputGrad = _baseLayer.Backward(outputGradient);\n+\n+        // Update importance scores based on gradient magnitudes\n+        UpdateImportanceScores();\n+\n+        // Increment step count and check if we should prune\n+        _stepCount++;\n+        if (_stepCount % _pruningInterval == 0 && _currentRank > _minRank)\n+        {\n+            PruneRank();\n+        }\n+\n+        // Sum input gradients\n+        Tensor<T> inputGrad = new Tensor<T>(loraInputGrad.Shape);\n+        for (int i = 0; i < loraInputGrad.Length; i++)\n+        {\n+            inputGrad[i] = NumOps.Add(loraInputGrad[i], baseInputGrad[i]);\n+        }\n+\n+        return inputGrad;\n+    }\n+\n+    /// <summary>\n+    /// Updates importance scores based on current gradient magnitudes.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Importance is computed using exponential moving average of gradient magnitudes.\n+    /// For each component i: importance[i] = ema * importance[i] + (1 - ema) * |gradient[i]|\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This updates our \"usefulness ratings\" for each component.\n+    ///\n+    /// We use exponential moving average (EMA) which is like a smoothed average:\n+    /// - New score = 0.95 * old_score + 0.05 * current_gradient_magnitude\n+    ///\n+    /// This way, a component needs to consistently have high gradients to get a high score.\n+    /// A single spike won't cause us to keep an unimportant component.\n+    /// </para>\n+    /// </remarks>\n+    private void UpdateImportanceScores()\n+    {\n+        // Get the LoRA layer's parameter gradients\n+        Vector<T> loraGradients = _loraLayer.GetParameterGradients();\n+\n+        // The LoRA layer stores parameters as [A matrix flattened, B matrix flattened]\n+        // We need to compute importance per rank component\n+        Matrix<T> matrixA = _loraLayer.GetMatrixA();\n+        Matrix<T> matrixB = _loraLayer.GetMatrixB();\n+\n+        int inputSize = matrixA.Rows;\n+        int outputSize = matrixB.Columns;\n+\n+        // For each rank component, compute gradient magnitude\n+        for (int r = 0; r < _currentRank; r++)\n+        {\n+            // Compute L2 norm of gradients for this rank component\n+            T gradMagnitude = NumOps.Zero;\n+\n+            // Gradients from matrix A for column r\n+            for (int i = 0; i < inputSize; i++)\n+            {\n+                T grad = loraGradients[i * _maxRank + r];\n+                gradMagnitude = NumOps.Add(gradMagnitude, NumOps.Multiply(grad, grad));\n+            }\n+\n+            // Gradients from matrix B for row r\n+            int bOffset = inputSize * _maxRank;\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                T grad = loraGradients[bOffset + r * outputSize + j];\n+                gradMagnitude = NumOps.Add(gradMagnitude, NumOps.Multiply(grad, grad));\n+            }\n+\n+            gradMagnitude = NumOps.Sqrt(gradMagnitude);\n+\n+            // Update importance score with EMA\n+            T emaFactor = NumOps.FromDouble(_importanceScoreEMA);\n+            T oneMinusEma = NumOps.FromDouble(1.0 - _importanceScoreEMA);\n+\n+            T oldScore = _importanceScores[r];\n+            T newScore = NumOps.Add(\n+                NumOps.Multiply(emaFactor, oldScore),\n+                NumOps.Multiply(oneMinusEma, gradMagnitude)\n+            );\n+\n+            _importanceScores[r] = newScore;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Prunes low-importance singular value components to reduce rank.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This method identifies components with importance scores below the threshold and removes them.\n+    /// The rank is reduced accordingly, focusing parameters on high-importance components.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This removes components that aren't pulling their weight.\n+    ///\n+    /// Process:\n+    /// 1. Look at all importance scores\n+    /// 2. Find components below the threshold\n+    /// 3. Mark them for removal\n+    /// 4. Reduce the current rank\n+    ///\n+    /// For example, if we have 16 components but 8 have very low importance scores,\n+    /// we can prune those 8 and reduce from rank=16 to rank=8.\n+    ///\n+    /// This makes the model:\n+    /// - Faster (fewer components to compute)\n+    /// - More focused (parameters concentrated on what matters)\n+    /// - More efficient (same or better performance with fewer parameters)\n+    /// </para>\n+    /// </remarks>\n+    private void PruneRank()\n+    {\n+        // Compute threshold value (percentile-based pruning)\n+        // We keep the top (1 - threshold) components\n+\n+        // Create a list of (importance, index) pairs for sorting\n+        var importanceList = new List<(T score, int index)>();\n+        for (int i = 0; i < _currentRank; i++)\n+        {\n+            importanceList.Add((_importanceScores[i], i));\n+        }\n+\n+        // Sort by importance (descending)\n+        // Convert to double for comparison since INumericOperations<T> doesn't have Compare\n+        importanceList.Sort((a, b) =>\n+            Convert.ToDouble(b.score).CompareTo(Convert.ToDouble(a.score)));\n+\n+        // Determine new rank (prune bottom threshold fraction)\n+        int componentsToKeep = Math.Max(_minRank, (int)(_currentRank * (1.0 - _rankPruningThreshold)));\n+\n+        // Only prune if we would actually reduce rank\n+        if (componentsToKeep < _currentRank)\n+        {\n+            // Determine which rank indices to keep (top components by importance)\n+            var keepIndices = new HashSet<int>();\n+            for (int i = 0; i < componentsToKeep; i++)\n+            {\n+                keepIndices.Add(importanceList[i].index);\n+            }\n+\n+            // Zero out pruned components in LoRA matrices\n+            // Get matrices A and B from LoRA layer\n+            Matrix<T> matrixA = _loraLayer.GetMatrixA();\n+            Matrix<T> matrixB = _loraLayer.GetMatrixB();\n+\n+            // Zero columns of A and rows of B for pruned rank components\n+            for (int r = 0; r < _maxRank; r++)\n+            {\n+                if (!keepIndices.Contains(r))\n+                {\n+                    // Zero column r of matrix A [inputSize, rank]\n+                    for (int i = 0; i < matrixA.Rows; i++)\n+                    {\n+                        matrixA[i, r] = NumOps.Zero;\n+                    }\n+\n+                    // Zero row r of matrix B [rank, outputSize]\n+                    for (int j = 0; j < matrixB.Columns; j++)\n+                    {\n+                        matrixB[r, j] = NumOps.Zero;\n+                    }\n+                }\n+            }\n+\n+            // Update LoRA layer parameters with zeroed matrices\n+            // Note: LoRALayer.SetParameters expects flattened A then B\n+            Vector<T> loraParams = new Vector<T>(_loraLayer.ParameterCount);\n+            int idx = 0;\n+\n+            // Pack matrix A\n+            for (int i = 0; i < matrixA.Rows; i++)\n+            {\n+                for (int j = 0; j < matrixA.Columns; j++)\n+                {\n+                    loraParams[idx++] = matrixA[i, j];\n+                }\n+            }\n+\n+            // Pack matrix B\n+            for (int i = 0; i < matrixB.Rows; i++)\n+            {\n+                for (int j = 0; j < matrixB.Columns; j++)\n+                {\n+                    loraParams[idx++] = matrixB[i, j];\n+                }\n+            }\n+\n+            _loraLayer.SetParameters(loraParams);\n+\n+            // Update current rank\n+            _currentRank = componentsToKeep;\n+\n+            // Reorder importance scores to keep only the top components\n+            Vector<T> newImportanceScores = new Vector<T>(_maxRank);\n+            for (int i = 0; i < _currentRank; i++)\n+            {\n+                newImportanceScores[i] = importanceList[i].score;\n+            }\n+            for (int i = _currentRank; i < _maxRank; i++)\n+            {\n+                newImportanceScores[i] = NumOps.Zero;\n+            }\n+            _importanceScores = newImportanceScores;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Expands the rank by adding new components (for cases where more capacity is needed).\n+    /// </summary>\n+    /// <param name=\"additionalRank\">Number of components to add.</param>\n+    /// <remarks>\n+    /// <para>\n+    /// This is the opposite of pruning - it adds new components when the model needs more capacity.\n+    /// New components are initialized with low importance and will need to prove their worth.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Sometimes the model realizes it needs more capacity.\n+    /// This method adds new components, giving the model more flexibility to learn.\n+    ///\n+    /// Think of it like hiring more workers when the team is overloaded.\n+    /// The new components start with low importance and have to earn their keep.\n+    /// </para>\n+    /// </remarks>\n+    public void ExpandRank(int additionalRank)\n+    {\n+        if (additionalRank <= 0)\n+        {\n+            throw new ArgumentException(\"Additional rank must be positive\", nameof(additionalRank));\n+        }\n+\n+        int newRank = Math.Min(_currentRank + additionalRank, _maxRank);\n+\n+        if (newRank > _currentRank)\n+        {\n+            // Initialize new components with low importance\n+            T lowImportance = NumOps.FromDouble(0.01);\n+            for (int i = _currentRank; i < newRank; i++)\n+            {\n+                _importanceScores[i] = lowImportance;\n+            }\n+\n+            _currentRank = newRank;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Merges the AdaLoRA adaptation into the base layer and returns the merged layer.\n+    /// </summary>\n+    /// <returns>A new layer with AdaLoRA weights merged into the base layer's weights.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// For Dense/FullyConnected layers, this merges the LoRA matrices into the base layer weights.\n+    /// Only the currently active components (based on currentRank) are merged.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This \"bakes in\" your adaptive LoRA to create a regular layer.\n+    /// Only the components that survived pruning (the important ones) are included in the merge.\n+    ///\n+    /// This gives you a final layer that:\n+    /// - Includes only the useful adaptations\n+    /// - Is as fast as a regular layer\n+    /// - Can be deployed without AdaLoRA infrastructure\n+    /// </para>\n+    /// </remarks>\n+    public override ILayer<T> MergeToOriginalLayer()\n+    {\n+        // For now, delegate to the base LoRA layer's merge logic\n+        // The LoRA layer will merge all components; ideally we'd mask by importance\n+        // but for simplicity, we use the current implementation\n+\n+        // Support both DenseLayer and FullyConnected layers\n+        DenseLayer<T>? denseBase = _baseLayer as DenseLayer<T>;\n+        FullyConnectedLayer<T>? fcBase = _baseLayer as FullyConnectedLayer<T>;\n+\n+        if (denseBase == null && fcBase == null)\n+        {\n+            throw new InvalidOperationException(\"AdaLoRAAdapter only supports DenseLayer or FullyConnectedLayer base layers\");\n+        }\n+\n+        // Get the LoRA weight contribution\n+        Matrix<T> loraWeights = _loraLayer.MergeWeights();\n+\n+        // Get base layer parameters\n+        Vector<T> baseParams = _baseLayer.GetParameters();\n+\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        int weightCount = inputSize * outputSize;\n+\n+        // Create new parameters with merged weights\n+        Vector<T> mergedParams = new Vector<T>(baseParams.Length);\n+\n+        // Merge weights\n+        for (int i = 0; i < weightCount; i++)\n+        {\n+            int row = i / inputSize;\n+            int col = i % inputSize;\n+            mergedParams[i] = NumOps.Add(baseParams[i], loraWeights[row, col]);\n+        }","path":"src/LoRA/Adapters/AdaLoRAAdapter.cs","commit_id":"fa815036ee570162056ba6aca773f34c80f36948","original_commit_id":"fa815036ee570162056ba6aca773f34c80f36948","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n<details>\n<summary>üß© Analysis chain</summary>\n\n**Verify parameter layout assumption in weight merging.**\n\nThe indexing at lines 567-568 assumes the base layer stores weights in a specific order where `col` changes faster than `row` (row-major with `col = i % inputSize`). This must match the memory layout used by `DenseLayer` and `FullyConnectedLayer`.\n\n\n\nRun this script to verify the parameter layout matches:\n\n---\n\nüèÅ Script executed:\n\n```shell\n#!/bin/bash\n# Description: Check DenseLayer and FullyConnectedLayer parameter layout order\n\necho \"Searching for DenseLayer GetParameters/SetParameters implementation...\"\nast-grep --pattern $'class DenseLayer$$$<$_> {\n  $$$\n  GetParameters($$$) {\n    $$$\n  }\n  $$$\n}'\n\necho -e \"\\nSearching for FullyConnectedLayer GetParameters/SetParameters implementation...\"\nast-grep --pattern $'class FullyConnectedLayer$$$<$_> {\n  $$$\n  GetParameters($$$) {\n    $$$\n  }\n  $$$\n}'\n\necho -e \"\\nSearching for weight matrix flattening patterns...\"\nrg -nP --type=cs -C5 'for.*Rows.*for.*Columns|for.*\\[i,\\s*j\\]' -g '**/DenseLayer.cs' -g '**/FullyConnectedLayer.cs'\n```\n\nLength of output: 203\n\n---\n\nüèÅ Script executed:\n\n```shell\n#!/bin/bash\n# Description: Locate and examine DenseLayer and FullyConnectedLayer implementations\n\necho \"Finding DenseLayer files...\"\nfd -type f -name \"*DenseLayer.cs\" \n\necho -e \"\\nFinding FullyConnectedLayer files...\"\nfd -type f -name \"*FullyConnectedLayer.cs\"\n\necho -e \"\\nFinding AdaLoRAAdapter...\"\nfd -type f -name \"*AdaLoRAAdapter.cs\"\n\necho -e \"\\nSearching for GetParameters method in all layer files...\"\nrg -n \"GetParameters\" --type=cs -g '**/*Layer*.cs'\n\necho -e \"\\nSearching for SetParameters method in all layer files...\"\nrg -n \"SetParameters\" --type=cs -g '**/*Layer*.cs'\n\necho -e \"\\nSearching for weight flattening patterns...\"\nrg -n \"\\[.*,.*\\]\" --type=cs -C3 -g '**/*Layer*.cs' | head -50\n```\n\nLength of output: 24715\n\n---\n\n</thinking>\n\n---\n\nüèÅ Script executed:\n\n```shell\n#!/bin/bash\n# Examine DenseLayer parameter handling\n\necho \"=== DenseLayer.cs GetParameters (line 497) ===\"\nsed -n '497,530p' src/NeuralNetworks/Layers/DenseLayer.cs\n\necho -e \"\\n=== DenseLayer.cs SetParameters (line 547) ===\"\nsed -n '547,580p' src/NeuralNetworks/Layers/DenseLayer.cs\n\necho -e \"\\n=== FullyConnectedLayer.cs GetParameters (line 535) ===\"\nsed -n '535,570p' src/NeuralNetworks/Layers/FullyConnectedLayer.cs\n\necho -e \"\\n=== FullyConnectedLayer.cs SetParameters (line 587) ===\"\nsed -n '587,620p' src/NeuralNetworks/Layers/FullyConnectedLayer.cs\n```\n\nLength of output: 4901\n\n---\n\nüèÅ Script executed:\n\n```shell\n#!/bin/bash\n# Examine DenseLayer and FullyConnectedLayer parameter handling\n\necho \"=== DenseLayer.cs GetParameters (line 497) ===\"\nsed -n '497,530p' src/NeuralNetworks/Layers/DenseLayer.cs\n\necho -e \"\\n=== DenseLayer.cs SetParameters (line 547) ===\"\nsed -n '547,580p' src/NeuralNetworks/Layers/DenseLayer.cs\n\necho -e \"\\n=== FullyConnectedLayer.cs GetParameters (line 535) ===\"\nsed -n '535,570p' src/NeuralNetworks/Layers/FullyConnectedLayer.cs\n\necho -e \"\\n=== FullyConnectedLayer.cs SetParameters (line 587) ===\"\nsed -n '587,620p' src/NeuralNetworks/Layers/FullyConnectedLayer.cs\n\necho -e \"\\n=== AdaLoRAAdapter.cs MergeToOriginalLayer method (lines 536-583) ===\"\nsed -n '536,583p' src/LoRA/Adapters/AdaLoRAAdapter.cs\n```\n\nLength of output: 6772\n\n---\n\n\n\n</details>\n\n**Fix type preservation in MergeToOriginalLayer‚Äîalways creates DenseLayer regardless of base layer type.**\n\nLine 579 creates `new DenseLayer<T>(...)` unconditionally, even when the base layer is `FullyConnectedLayer<T>` (matched on line 545). The method should preserve the original layer type:\n\n```csharp\n// Create appropriate layer type matching the base layer\nvar mergedLayer = denseBase != null \n    ? new DenseLayer<T>(inputSize, outputSize, (IActivationFunction<T>?)null) \n    : new FullyConnectedLayer<T>(inputSize, outputSize, (IActivationFunction<T>?)null);\n\nmergedLayer.SetParameters(mergedParams);\nreturn mergedLayer;\n```\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/LoRA/Adapters/AdaLoRAAdapter.cs around lines 564 to 570, the merge\nroutine always constructs a DenseLayer<T> which loses the original layer type;\nchange it to instantiate the same concrete type as the base layer (if base was\nDense create DenseLayer<T>, otherwise create FullyConnectedLayer<T>), then call\nSetParameters(mergedParams) on that instance and return it so the original layer\ntype and behavior are preserved.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T15:16:54Z","updated_at":"2025-11-02T15:16:56Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484862620","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484862620"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484862620"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484862620/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":564,"original_start_line":564,"start_side":"RIGHT","line":570,"original_line":570,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":570,"position":570,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484862621","pull_request_review_id":3408834322,"id":2484862621,"node_id":"PRRC_kwDOKSXUF86UG_6d","diff_hunk":"@@ -0,0 +1,584 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// Adaptive Low-Rank Adaptation (AdaLoRA) adapter that dynamically allocates parameter budgets among weight matrices.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// AdaLoRA improves upon standard LoRA by dynamically adjusting the rank allocation based on importance scores.\n+/// Instead of using a fixed rank for all weight matrices, AdaLoRA:\n+/// - Starts with a maximum rank and adaptively reduces it during training\n+/// - Computes importance scores for each singular value component\n+/// - Prunes less important components to focus parameter budget on critical adaptations\n+/// - Allows different layers to have different effective ranks\n+/// </para>\n+/// <para>\n+/// This leads to more efficient parameter usage compared to fixed-rank LoRA, especially for large models\n+/// where some layers need more adaptation capacity than others.\n+/// </para>\n+/// <para><b>For Beginners:</b> AdaLoRA is like smart LoRA that learns which parts of the adaptation matter most.\n+///\n+/// Think of standard LoRA as giving every layer the same budget (rank=8 everywhere).\n+/// AdaLoRA is smarter:\n+/// - Some layers get more budget (rank=16) because they're important for the task\n+/// - Other layers get less budget (rank=2) because small changes are enough\n+/// - The model learns this automatically during training\n+///\n+/// How it works:\n+/// 1. Start with a large rank (e.g., maxRank=32)\n+/// 2. During training, track how important each component is\n+/// 3. Prune components with low importance scores\n+/// 4. Focus parameters on what actually helps\n+///\n+/// Benefits:\n+/// - More parameter-efficient than fixed-rank LoRA\n+/// - Better performance with same parameter budget\n+/// - Automatically finds optimal rank per layer\n+///\n+/// Reference: \"Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning\" (ICLR 2023)\n+/// https://arxiv.org/abs/2303.10512\n+/// </para>\n+/// </remarks>\n+public class AdaLoRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Maximum possible rank for this adapter.\n+    /// </summary>\n+    /// <remarks>\n+    /// The adapter starts with this rank and may reduce it during training through pruning.\n+    /// This is the upper bound on the number of singular value components.\n+    /// </remarks>\n+    private readonly int _maxRank;\n+\n+    /// <summary>\n+    /// Current active rank after pruning.\n+    /// </summary>\n+    /// <remarks>\n+    /// This represents the number of singular value components currently being used.\n+    /// It starts at maxRank and decreases as low-importance components are pruned.\n+    /// </remarks>\n+    private int _currentRank;\n+\n+    /// <summary>\n+    /// Importance scores for each singular value component.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Each score represents how important that singular value is for the adaptation.\n+    /// Higher scores indicate more important components that should be retained.\n+    /// These scores are updated during training based on gradient magnitudes.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Think of these as \"usefulness ratings\" for each component.\n+    /// Components with high scores are helping a lot, low scores mean they're not doing much.\n+    /// We keep the high-scoring components and prune the low-scoring ones.\n+    /// </para>\n+    /// </remarks>\n+    private Vector<T> _importanceScores;\n+\n+    /// <summary>\n+    /// Threshold for pruning singular values based on importance.\n+    /// </summary>\n+    /// <remarks>\n+    /// Components with importance scores below this threshold are candidates for pruning.\n+    /// This value is typically set as a small fraction (e.g., 0.01 to 0.1).\n+    /// </remarks>\n+    private readonly double _rankPruningThreshold;\n+\n+    /// <summary>\n+    /// Exponential moving average factor for importance score updates.\n+    /// </summary>\n+    /// <remarks>\n+    /// Controls how quickly importance scores adapt to new gradient information.\n+    /// Typical values: 0.9 to 0.99 (higher = more smoothing, lower = faster adaptation)\n+    /// </remarks>\n+    private readonly double _importanceScoreEMA;\n+\n+    /// <summary>\n+    /// Minimum rank to maintain (prevents pruning below this threshold).\n+    /// </summary>\n+    private readonly int _minRank;\n+\n+    /// <summary>\n+    /// Number of training steps between rank pruning operations.\n+    /// </summary>\n+    private readonly int _pruningInterval;\n+\n+    /// <summary>\n+    /// Current training step counter.\n+    /// </summary>\n+    private int _stepCount;\n+\n+    /// <summary>\n+    /// Gets the maximum rank this adapter can use.\n+    /// </summary>\n+    public int MaxRank => _maxRank;\n+\n+    /// <summary>\n+    /// Gets the current active rank after pruning.\n+    /// </summary>\n+    public int CurrentRank => _currentRank;\n+\n+    /// <summary>\n+    /// Gets a copy of the current importance scores.\n+    /// </summary>\n+    public Vector<T> GetImportanceScores() => _importanceScores.Clone();\n+\n+    /// <summary>\n+    /// Initializes a new AdaLoRA adapter with adaptive rank allocation.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with AdaLoRA.</param>\n+    /// <param name=\"maxRank\">The maximum rank for the LoRA decomposition.</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to maxRank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <param name=\"rankPruningThreshold\">Threshold for pruning based on importance scores (default: 0.05).</param>\n+    /// <param name=\"minRank\">Minimum rank to maintain after pruning (default: 1).</param>\n+    /// <param name=\"pruningInterval\">Number of steps between pruning operations (default: 100).</param>\n+    /// <param name=\"importanceScoreEMA\">EMA factor for importance score updates (default: 0.95).</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when rank parameters are invalid.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This creates an AdaLoRA adapter with smart rank allocation.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to adapt (typically Dense or FullyConnected)\n+    /// - maxRank: Start with this many components (will prune down during training)\n+    /// - alpha: How strong the adaptation is\n+    /// - freezeBaseLayer: Lock the original weights (usually true for efficiency)\n+    /// - rankPruningThreshold: How unimportant a component must be to get pruned (0.05 = bottom 5%)\n+    /// - minRank: Never prune below this rank (safety net)\n+    /// - pruningInterval: How often to check for pruning (in training steps)\n+    /// - importanceScoreEMA: How smooth importance tracking is (higher = more stable)\n+    ///\n+    /// The adapter will automatically adjust its rank during training to focus parameters\n+    /// on the most important components.\n+    /// </para>\n+    /// </remarks>\n+    public AdaLoRAAdapter(\n+        ILayer<T> baseLayer,\n+        int maxRank,\n+        double alpha = -1,\n+        bool freezeBaseLayer = true,\n+        double rankPruningThreshold = 0.05,\n+        int minRank = 1,\n+        int pruningInterval = 100,\n+        double importanceScoreEMA = 0.95)\n+        : base(baseLayer, maxRank, alpha, freezeBaseLayer)\n+    {\n+        if (minRank < 1)\n+        {\n+            throw new ArgumentException(\"Minimum rank must be at least 1\", nameof(minRank));\n+        }\n+\n+        if (minRank > maxRank)\n+        {\n+            throw new ArgumentException($\"Minimum rank ({minRank}) cannot exceed maximum rank ({maxRank})\", nameof(minRank));\n+        }\n+\n+        if (rankPruningThreshold <= 0 || rankPruningThreshold >= 1)\n+        {\n+            throw new ArgumentException(\"Rank pruning threshold must be between 0 and 1\", nameof(rankPruningThreshold));\n+        }\n+\n+        if (importanceScoreEMA <= 0 || importanceScoreEMA >= 1)\n+        {\n+            throw new ArgumentException(\"Importance score EMA factor must be between 0 and 1\", nameof(importanceScoreEMA));\n+        }\n+\n+        _maxRank = maxRank;\n+        _currentRank = maxRank;\n+        _rankPruningThreshold = rankPruningThreshold;\n+        _minRank = minRank;\n+        _pruningInterval = pruningInterval;\n+        _importanceScoreEMA = importanceScoreEMA;\n+        _stepCount = 0;\n+\n+        // Initialize importance scores (start with uniform importance)\n+        _importanceScores = new Vector<T>(maxRank);\n+        T initialScore = NumOps.One;\n+        for (int i = 0; i < maxRank; i++)\n+        {\n+            _importanceScores[i] = initialScore;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass using only the top-k most important singular values.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Sum of base layer output and AdaLoRA output (using current rank).</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Unlike standard LoRA which uses all rank components, AdaLoRA only uses the currentRank\n+    /// most important components based on importance scores. This is more efficient and focuses\n+    /// computation on the most impactful adaptations.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This computes the output using only the important components.\n+    /// If we started with rank=32 but pruned to rank=8, we only use the top 8 most important\n+    /// singular values. This makes computation faster and more focused.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        // Forward through base layer\n+        Tensor<T> baseOutput = _baseLayer.Forward(input);\n+\n+        // Forward through LoRA layer (it will use all components, but we'll mask based on importance)\n+        Tensor<T> loraOutput = _loraLayer.Forward(input);\n+\n+        // If current rank < max rank, we need to mask the output\n+        // This is implicitly handled by the pruned matrices in the LoRA layer\n+        // For simplicity, we use the LoRA output as-is (pruning happens in UpdateParameters)\n+\n+        // Sum the outputs\n+        Tensor<T> result = new Tensor<T>(baseOutput.Shape);\n+        for (int i = 0; i < baseOutput.Length; i++)\n+        {\n+            result[i] = NumOps.Add(baseOutput[i], loraOutput[i]);\n+        }\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Performs the backward pass and updates importance scores based on gradients.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient flowing back from the next layer.</param>\n+    /// <returns>Gradient to pass to the previous layer.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// During backpropagation, AdaLoRA computes importance scores based on the magnitude of\n+    /// gradients for each singular value component. Components with consistently large gradients\n+    /// are considered more important.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is where we learn which components are important!\n+    /// As gradients flow back:\n+    /// 1. We see which components have large gradients (they're actively learning)\n+    /// 2. We update their importance scores (high gradients = high importance)\n+    /// 3. We use exponential moving average to smooth out noise\n+    ///\n+    /// Components that consistently get small gradients aren't helping much,\n+    /// so they'll get low importance scores and eventually be pruned.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        // Backward through both layers\n+        Tensor<T> loraInputGrad = _loraLayer.Backward(outputGradient);\n+        Tensor<T> baseInputGrad = _baseLayer.Backward(outputGradient);\n+\n+        // Update importance scores based on gradient magnitudes\n+        UpdateImportanceScores();\n+\n+        // Increment step count and check if we should prune\n+        _stepCount++;\n+        if (_stepCount % _pruningInterval == 0 && _currentRank > _minRank)\n+        {\n+            PruneRank();\n+        }\n+\n+        // Sum input gradients\n+        Tensor<T> inputGrad = new Tensor<T>(loraInputGrad.Shape);\n+        for (int i = 0; i < loraInputGrad.Length; i++)\n+        {\n+            inputGrad[i] = NumOps.Add(loraInputGrad[i], baseInputGrad[i]);\n+        }\n+\n+        return inputGrad;\n+    }\n+\n+    /// <summary>\n+    /// Updates importance scores based on current gradient magnitudes.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Importance is computed using exponential moving average of gradient magnitudes.\n+    /// For each component i: importance[i] = ema * importance[i] + (1 - ema) * |gradient[i]|\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This updates our \"usefulness ratings\" for each component.\n+    ///\n+    /// We use exponential moving average (EMA) which is like a smoothed average:\n+    /// - New score = 0.95 * old_score + 0.05 * current_gradient_magnitude\n+    ///\n+    /// This way, a component needs to consistently have high gradients to get a high score.\n+    /// A single spike won't cause us to keep an unimportant component.\n+    /// </para>\n+    /// </remarks>\n+    private void UpdateImportanceScores()\n+    {\n+        // Get the LoRA layer's parameter gradients\n+        Vector<T> loraGradients = _loraLayer.GetParameterGradients();\n+\n+        // The LoRA layer stores parameters as [A matrix flattened, B matrix flattened]\n+        // We need to compute importance per rank component\n+        Matrix<T> matrixA = _loraLayer.GetMatrixA();\n+        Matrix<T> matrixB = _loraLayer.GetMatrixB();\n+\n+        int inputSize = matrixA.Rows;\n+        int outputSize = matrixB.Columns;\n+\n+        // For each rank component, compute gradient magnitude\n+        for (int r = 0; r < _currentRank; r++)\n+        {\n+            // Compute L2 norm of gradients for this rank component\n+            T gradMagnitude = NumOps.Zero;\n+\n+            // Gradients from matrix A for column r\n+            for (int i = 0; i < inputSize; i++)\n+            {\n+                T grad = loraGradients[i * _maxRank + r];\n+                gradMagnitude = NumOps.Add(gradMagnitude, NumOps.Multiply(grad, grad));\n+            }\n+\n+            // Gradients from matrix B for row r\n+            int bOffset = inputSize * _maxRank;\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                T grad = loraGradients[bOffset + r * outputSize + j];\n+                gradMagnitude = NumOps.Add(gradMagnitude, NumOps.Multiply(grad, grad));\n+            }\n+\n+            gradMagnitude = NumOps.Sqrt(gradMagnitude);\n+\n+            // Update importance score with EMA\n+            T emaFactor = NumOps.FromDouble(_importanceScoreEMA);\n+            T oneMinusEma = NumOps.FromDouble(1.0 - _importanceScoreEMA);\n+\n+            T oldScore = _importanceScores[r];\n+            T newScore = NumOps.Add(\n+                NumOps.Multiply(emaFactor, oldScore),\n+                NumOps.Multiply(oneMinusEma, gradMagnitude)\n+            );\n+\n+            _importanceScores[r] = newScore;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Prunes low-importance singular value components to reduce rank.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This method identifies components with importance scores below the threshold and removes them.\n+    /// The rank is reduced accordingly, focusing parameters on high-importance components.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This removes components that aren't pulling their weight.\n+    ///\n+    /// Process:\n+    /// 1. Look at all importance scores\n+    /// 2. Find components below the threshold\n+    /// 3. Mark them for removal\n+    /// 4. Reduce the current rank\n+    ///\n+    /// For example, if we have 16 components but 8 have very low importance scores,\n+    /// we can prune those 8 and reduce from rank=16 to rank=8.\n+    ///\n+    /// This makes the model:\n+    /// - Faster (fewer components to compute)\n+    /// - More focused (parameters concentrated on what matters)\n+    /// - More efficient (same or better performance with fewer parameters)\n+    /// </para>\n+    /// </remarks>\n+    private void PruneRank()\n+    {\n+        // Compute threshold value (percentile-based pruning)\n+        // We keep the top (1 - threshold) components\n+\n+        // Create a list of (importance, index) pairs for sorting\n+        var importanceList = new List<(T score, int index)>();\n+        for (int i = 0; i < _currentRank; i++)\n+        {\n+            importanceList.Add((_importanceScores[i], i));\n+        }\n+\n+        // Sort by importance (descending)\n+        // Convert to double for comparison since INumericOperations<T> doesn't have Compare\n+        importanceList.Sort((a, b) =>\n+            Convert.ToDouble(b.score).CompareTo(Convert.ToDouble(a.score)));\n+\n+        // Determine new rank (prune bottom threshold fraction)\n+        int componentsToKeep = Math.Max(_minRank, (int)(_currentRank * (1.0 - _rankPruningThreshold)));\n+\n+        // Only prune if we would actually reduce rank\n+        if (componentsToKeep < _currentRank)\n+        {\n+            // Determine which rank indices to keep (top components by importance)\n+            var keepIndices = new HashSet<int>();\n+            for (int i = 0; i < componentsToKeep; i++)\n+            {\n+                keepIndices.Add(importanceList[i].index);\n+            }\n+\n+            // Zero out pruned components in LoRA matrices\n+            // Get matrices A and B from LoRA layer\n+            Matrix<T> matrixA = _loraLayer.GetMatrixA();\n+            Matrix<T> matrixB = _loraLayer.GetMatrixB();\n+\n+            // Zero columns of A and rows of B for pruned rank components\n+            for (int r = 0; r < _maxRank; r++)\n+            {\n+                if (!keepIndices.Contains(r))\n+                {\n+                    // Zero column r of matrix A [inputSize, rank]\n+                    for (int i = 0; i < matrixA.Rows; i++)\n+                    {\n+                        matrixA[i, r] = NumOps.Zero;\n+                    }\n+\n+                    // Zero row r of matrix B [rank, outputSize]\n+                    for (int j = 0; j < matrixB.Columns; j++)\n+                    {\n+                        matrixB[r, j] = NumOps.Zero;\n+                    }\n+                }\n+            }\n+\n+            // Update LoRA layer parameters with zeroed matrices\n+            // Note: LoRALayer.SetParameters expects flattened A then B\n+            Vector<T> loraParams = new Vector<T>(_loraLayer.ParameterCount);\n+            int idx = 0;\n+\n+            // Pack matrix A\n+            for (int i = 0; i < matrixA.Rows; i++)\n+            {\n+                for (int j = 0; j < matrixA.Columns; j++)\n+                {\n+                    loraParams[idx++] = matrixA[i, j];\n+                }\n+            }\n+\n+            // Pack matrix B\n+            for (int i = 0; i < matrixB.Rows; i++)\n+            {\n+                for (int j = 0; j < matrixB.Columns; j++)\n+                {\n+                    loraParams[idx++] = matrixB[i, j];\n+                }\n+            }\n+\n+            _loraLayer.SetParameters(loraParams);\n+\n+            // Update current rank\n+            _currentRank = componentsToKeep;\n+\n+            // Reorder importance scores to keep only the top components\n+            Vector<T> newImportanceScores = new Vector<T>(_maxRank);\n+            for (int i = 0; i < _currentRank; i++)\n+            {\n+                newImportanceScores[i] = importanceList[i].score;\n+            }\n+            for (int i = _currentRank; i < _maxRank; i++)\n+            {\n+                newImportanceScores[i] = NumOps.Zero;\n+            }\n+            _importanceScores = newImportanceScores;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Expands the rank by adding new components (for cases where more capacity is needed).\n+    /// </summary>\n+    /// <param name=\"additionalRank\">Number of components to add.</param>\n+    /// <remarks>\n+    /// <para>\n+    /// This is the opposite of pruning - it adds new components when the model needs more capacity.\n+    /// New components are initialized with low importance and will need to prove their worth.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Sometimes the model realizes it needs more capacity.\n+    /// This method adds new components, giving the model more flexibility to learn.\n+    ///\n+    /// Think of it like hiring more workers when the team is overloaded.\n+    /// The new components start with low importance and have to earn their keep.\n+    /// </para>\n+    /// </remarks>\n+    public void ExpandRank(int additionalRank)\n+    {\n+        if (additionalRank <= 0)\n+        {\n+            throw new ArgumentException(\"Additional rank must be positive\", nameof(additionalRank));\n+        }\n+\n+        int newRank = Math.Min(_currentRank + additionalRank, _maxRank);\n+\n+        if (newRank > _currentRank)\n+        {\n+            // Initialize new components with low importance\n+            T lowImportance = NumOps.FromDouble(0.01);\n+            for (int i = _currentRank; i < newRank; i++)\n+            {\n+                _importanceScores[i] = lowImportance;\n+            }\n+\n+            _currentRank = newRank;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Merges the AdaLoRA adaptation into the base layer and returns the merged layer.\n+    /// </summary>\n+    /// <returns>A new layer with AdaLoRA weights merged into the base layer's weights.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// For Dense/FullyConnected layers, this merges the LoRA matrices into the base layer weights.\n+    /// Only the currently active components (based on currentRank) are merged.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This \"bakes in\" your adaptive LoRA to create a regular layer.\n+    /// Only the components that survived pruning (the important ones) are included in the merge.\n+    ///\n+    /// This gives you a final layer that:\n+    /// - Includes only the useful adaptations\n+    /// - Is as fast as a regular layer\n+    /// - Can be deployed without AdaLoRA infrastructure\n+    /// </para>\n+    /// </remarks>\n+    public override ILayer<T> MergeToOriginalLayer()\n+    {\n+        // For now, delegate to the base LoRA layer's merge logic\n+        // The LoRA layer will merge all components; ideally we'd mask by importance\n+        // but for simplicity, we use the current implementation\n+\n+        // Support both DenseLayer and FullyConnected layers\n+        DenseLayer<T>? denseBase = _baseLayer as DenseLayer<T>;\n+        FullyConnectedLayer<T>? fcBase = _baseLayer as FullyConnectedLayer<T>;\n+\n+        if (denseBase == null && fcBase == null)\n+        {\n+            throw new InvalidOperationException(\"AdaLoRAAdapter only supports DenseLayer or FullyConnectedLayer base layers\");\n+        }\n+\n+        // Get the LoRA weight contribution\n+        Matrix<T> loraWeights = _loraLayer.MergeWeights();\n+\n+        // Get base layer parameters\n+        Vector<T> baseParams = _baseLayer.GetParameters();\n+\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        int weightCount = inputSize * outputSize;\n+\n+        // Create new parameters with merged weights\n+        Vector<T> mergedParams = new Vector<T>(baseParams.Length);\n+\n+        // Merge weights\n+        for (int i = 0; i < weightCount; i++)\n+        {\n+            int row = i / inputSize;\n+            int col = i % inputSize;\n+            mergedParams[i] = NumOps.Add(baseParams[i], loraWeights[row, col]);\n+        }\n+\n+        // Copy biases unchanged\n+        for (int i = weightCount; i < baseParams.Length; i++)\n+        {\n+            mergedParams[i] = baseParams[i];\n+        }\n+\n+        // Create a new dense layer with merged parameters\n+        DenseLayer<T> mergedLayer = new DenseLayer<T>(inputSize, outputSize, (IActivationFunction<T>?)null);\n+        mergedLayer.SetParameters(mergedParams);","path":"src/LoRA/Adapters/AdaLoRAAdapter.cs","commit_id":"fa815036ee570162056ba6aca773f34c80f36948","original_commit_id":"fa815036ee570162056ba6aca773f34c80f36948","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**Always creates DenseLayer, losing type information.**\n\nLine 579 always creates a `DenseLayer<T>` even when the base layer is a `FullyConnectedLayer<T>`. This loses type information and may break code that expects the specific layer type.\n\n\n\nApply this fix to preserve the base layer type:\n\n```diff\n-    // Create a new dense layer with merged parameters\n-    DenseLayer<T> mergedLayer = new DenseLayer<T>(inputSize, outputSize, (IActivationFunction<T>?)null);\n-    mergedLayer.SetParameters(mergedParams);\n-\n-    return mergedLayer;\n+    // Create a new layer of the same type as the base layer with merged parameters\n+    if (denseBase != null)\n+    {\n+        DenseLayer<T> mergedLayer = new DenseLayer<T>(inputSize, outputSize, (IActivationFunction<T>?)null);\n+        mergedLayer.SetParameters(mergedParams);\n+        return mergedLayer;\n+    }\n+    else\n+    {\n+        FullyConnectedLayer<T> mergedLayer = new FullyConnectedLayer<T>(inputSize, outputSize, (IActivationFunction<T>?)null);\n+        mergedLayer.SetParameters(mergedParams);\n+        return mergedLayer;\n+    }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        // Create a new layer of the same type as the base layer with merged parameters\n        if (denseBase != null)\n        {\n            DenseLayer<T> mergedLayer = new DenseLayer<T>(inputSize, outputSize, (IActivationFunction<T>?)null);\n            mergedLayer.SetParameters(mergedParams);\n            return mergedLayer;\n        }\n        else\n        {\n            FullyConnectedLayer<T> mergedLayer = new FullyConnectedLayer<T>(inputSize, outputSize, (IActivationFunction<T>?)null);\n            mergedLayer.SetParameters(mergedParams);\n            return mergedLayer;\n        }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/LoRA/Adapters/AdaLoRAAdapter.cs around lines 579-580, the code always\ninstantiates a DenseLayer<T> and thus loses the original layer's concrete type\n(e.g., FullyConnectedLayer<T>); instead detect the base layer's actual type and\ninstantiate that same concrete type with the same constructor shape (inputSize,\noutputSize, activation=null) using reflection or type-checking (if base is\nFullyConnectedLayer<T> create a FullyConnectedLayer<T>, otherwise use\nActivator.CreateInstance(baseLayer.GetType(), inputSize, outputSize,\n(IActivationFunction<T>?)null)); if instantiation fails, fall back to\nDenseLayer<T>; after creating the instance call SetParameters(mergedParams) as\nbefore.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T15:16:54Z","updated_at":"2025-11-02T15:16:56Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484862621","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484862621"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484862621"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484862621/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":579,"original_start_line":579,"start_side":"RIGHT","line":580,"original_line":580,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":580,"position":580,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484862623","pull_request_review_id":3408834322,"id":2484862623,"node_id":"PRRC_kwDOKSXUF86UG_6f","diff_hunk":"@@ -0,0 +1,680 @@\n+using AiDotNet.Interfaces;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// Chain-of-LoRA adapter that implements sequential composition of multiple LoRA adapters.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// Chain-of-LoRA (COLA) is an advanced LoRA technique that enables sequential composition\n+/// of multiple LoRA adaptations through an iterative optimization framework. Unlike standard\n+/// LoRA which applies a single low-rank adaptation, COLA builds a chain of adaptations where\n+/// each adapter is trained, merged into the model, and then a new adapter is initialized for\n+/// further refinement.\n+/// </para>\n+/// <para>\n+/// This approach bridges the performance gap between standard LoRA and full fine-tuning by\n+/// employing residual learning principles. Each iteration in the chain adds incremental\n+/// improvements to the model's task-specific performance without incurring additional\n+/// computational costs or memory overhead during inference.\n+/// </para>\n+/// <para><b>Key Concepts:</b>\n+///\n+/// <b>Sequential Adaptation:</b>\n+/// Chain-of-LoRA applies adaptations in sequence (Task A ‚Üí Task B ‚Üí Task C), where each\n+/// stage builds upon the previous one. This is inspired by the Frank-Wolfe optimization\n+/// algorithm, which makes greedy updates along the direction of maximum improvement.\n+///\n+/// <b>Merge and Re-initialize:</b>\n+/// After training each LoRA adapter, the learned weights are merged back into the base layer,\n+/// and a new LoRA adapter is initialized. This \"tying a knot\" process allows the model to\n+/// consolidate learned knowledge before adding new adaptations.\n+///\n+/// <b>Knowledge Preservation:</b>\n+/// By freezing the base layer and only training the LoRA components, the chain preserves\n+/// previously learned knowledge while allowing new task-specific adaptations. Each adapter\n+/// in the chain captures a specific aspect of the task or a refinement step.\n+///\n+/// <b>Incremental Fine-tuning Pipeline:</b>\n+/// COLA enables continual learning scenarios where tasks are presented sequentially, and\n+/// the model must adapt to new tasks while maintaining performance on previous ones.\n+/// </para>\n+/// <para><b>Benefits of Chain-of-LoRA:</b>\n+///\n+/// - <b>Better Performance:</b> Achieves up to 6.47% relative accuracy gain over standard LoRA\n+/// - <b>No Extra Overhead:</b> After merging, inference cost is identical to the base model\n+/// - <b>Modular Adaptation:</b> Each adapter can be trained, tested, and validated independently\n+/// - <b>Catastrophic Forgetting Mitigation:</b> Sequential merging helps preserve prior knowledge\n+/// - <b>Task Chaining:</b> Naturally supports multi-task learning and transfer learning scenarios\n+/// - <b>Flexible Deployment:</b> Can deploy the full chain or selected adapters as needed\n+/// </para>\n+/// <para><b>For Beginners:</b>\n+///\n+/// Imagine you're learning a complex skill in stages:\n+/// 1. First, you learn the basics (Adapter 1)\n+/// 2. Then you practice and the basics become automatic (Merge)\n+/// 3. Next, you learn intermediate techniques on top of the basics (Adapter 2)\n+/// 4. Again, you practice until they're automatic (Merge)\n+/// 5. Finally, you learn advanced skills building on everything before (Adapter 3)\n+///\n+/// Chain-of-LoRA works the same way: each adapter learns something new, then it's consolidated\n+/// into the model, and the next adapter can focus on the next refinement. This stepwise approach\n+/// often achieves better results than trying to learn everything at once.\n+/// </para>\n+/// <para><b>Research Reference:</b>\n+///\n+/// Based on \"Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning\"\n+/// (arXiv:2401.04151, January 2024). The paper demonstrates that sequential low-rank adaptations\n+/// can significantly improve task performance compared to single-stage LoRA, especially on\n+/// complex reasoning and multi-step tasks.\n+/// </para>\n+/// <para><b>Usage Example:</b>\n+/// <code>\n+/// // Create a chain with 3 sequential adaptations\n+/// var chain = new ChainLoRAAdapter&lt;double&gt;(baseLayer, rank: 8, chainLength: 3);\n+///\n+/// // Train first adapter on Task A\n+/// chain.SetActiveAdapterIndex(0);\n+/// TrainModel(chain, taskAData);\n+/// chain.MergeActiveAdapter(); // Consolidate Task A knowledge\n+///\n+/// // Train second adapter on Task B\n+/// chain.SetActiveAdapterIndex(1);\n+/// TrainModel(chain, taskBData);\n+/// chain.MergeActiveAdapter(); // Consolidate Task B knowledge\n+///\n+/// // Train third adapter on Task C\n+/// chain.SetActiveAdapterIndex(2);\n+/// TrainModel(chain, taskCData);\n+///\n+/// // Deploy: all adaptations are now part of the model\n+/// ILayer&lt;double&gt; finalLayer = chain.MergeToOriginalLayer();\n+/// </code>\n+/// </para>\n+/// </remarks>\n+public class ChainLoRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// The chain of LoRA adapters applied sequentially.\n+    /// </summary>\n+    private readonly List<LoRALayer<T>> _adapterChain;\n+\n+    /// <summary>\n+    /// The index of the currently active adapter being trained.\n+    /// </summary>\n+    private int _activeAdapterIndex;\n+\n+    /// <summary>\n+    /// Whether each adapter in the chain has been merged.\n+    /// </summary>\n+    private readonly List<bool> _mergedStatus;\n+\n+    /// <summary>\n+    /// The total length of the adapter chain.\n+    /// </summary>\n+    private readonly int _chainLength;\n+\n+    /// <summary>\n+    /// Cached parameter count reflecting current chain state.\n+    /// </summary>\n+    /// <remarks>\n+    /// This field is updated whenever adapters are merged/unmerged to avoid\n+    /// recomputing the count on every access and to provide a stable value\n+    /// during base class construction before the chain is fully initialized.\n+    /// </remarks>\n+    private int _currentParameterCount;\n+\n+    /// <summary>\n+    /// Gets the total number of adapters in the chain.\n+    /// </summary>\n+    /// <remarks>\n+    /// This represents the maximum number of sequential adaptation stages that can be applied.\n+    /// Each adapter can be trained independently and then merged before proceeding to the next.\n+    /// </remarks>\n+    public int ChainLength => _chainLength;\n+\n+    /// <summary>\n+    /// Gets the index of the currently active adapter (0-based).\n+    /// </summary>\n+    /// <remarks>\n+    /// The active adapter is the one currently being trained. Other adapters in the chain\n+    /// are either waiting to be trained (higher indices) or have been merged (lower indices).\n+    /// </remarks>\n+    public int ActiveAdapterIndex => _activeAdapterIndex;\n+\n+    /// <summary>\n+    /// Gets the list of LoRA adapters in the chain.\n+    /// </summary>\n+    /// <remarks>\n+    /// Each adapter in the chain represents one stage of sequential adaptation.\n+    /// Adapters are applied in order during forward passes.\n+    /// </remarks>\n+    public IReadOnlyList<LoRALayer<T>> AdapterChain => _adapterChain.AsReadOnly();\n+\n+    /// <summary>\n+    /// Gets the merged status of each adapter in the chain.\n+    /// </summary>\n+    /// <remarks>\n+    /// True indicates that an adapter has been merged into the base layer and should\n+    /// no longer contribute trainable parameters. Merged adapters still contribute\n+    /// to the forward pass until the entire chain is collapsed.\n+    /// </remarks>\n+    public IReadOnlyList<bool> MergedStatus => _mergedStatus.AsReadOnly();\n+\n+    /// <summary>\n+    /// Initializes a new Chain-of-LoRA adapter with the specified configuration.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with the LoRA chain.</param>\n+    /// <param name=\"rank\">The rank of each LoRA decomposition in the chain.</param>\n+    /// <param name=\"chainLength\">The number of sequential adapters in the chain (default: 3).</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor for each adapter (defaults to rank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training (default: true).</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when chainLength is less than 1.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// Creates a chain of LoRA adapters for sequential fine-tuning. Each adapter in the chain\n+    /// can be trained independently, merged into the model, and then the next adapter can be\n+    /// activated for further refinement.\n+    /// </para>\n+    /// <para><b>For Beginners:</b>\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to adapt (e.g., a dense or convolutional layer)\n+    /// - rank: How compressed each adapter is (lower = fewer parameters per stage)\n+    /// - chainLength: How many sequential adaptation stages you want (typical: 2-5)\n+    /// - alpha: Controls adaptation strength (usually equals rank)\n+    /// - freezeBaseLayer: Lock base weights to preserve pre-trained knowledge (recommended: true)\n+    ///\n+    /// Example: chainLength=3 means you can do three rounds of training and merging,\n+    /// allowing the model to incrementally improve on complex tasks.\n+    /// </para>\n+    /// </remarks>\n+    public ChainLoRAAdapter(\n+        ILayer<T> baseLayer,\n+        int rank,\n+        int chainLength = 3,\n+        double alpha = -1,\n+        bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        if (chainLength < 1)\n+        {\n+            throw new ArgumentException(\"Chain length must be at least 1\", nameof(chainLength));\n+        }\n+\n+        _chainLength = chainLength;\n+        _activeAdapterIndex = 0;\n+        _adapterChain = new List<LoRALayer<T>>(chainLength);\n+        _mergedStatus = new List<bool>(chainLength);\n+\n+        // Create the chain of LoRA adapters\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+\n+        for (int i = 0; i < chainLength; i++)\n+        {\n+            var adapter = new LoRALayer<T>(inputSize, outputSize, rank, alpha);\n+            _adapterChain.Add(adapter);\n+            _mergedStatus.Add(false);\n+        }\n+\n+        // Update parameter count to reflect all unmerged adapters\n+        UpdateParameterCount();\n+    }\n+\n+    /// <summary>\n+    /// Sets which adapter in the chain is currently active for training.\n+    /// </summary>\n+    /// <param name=\"index\">The 0-based index of the adapter to activate.</param>\n+    /// <exception cref=\"ArgumentOutOfRangeException\">Thrown when index is out of range.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// Only the active adapter receives gradient updates during training. Other adapters\n+    /// are either frozen (already merged) or inactive (waiting to be trained).\n+    /// </para>\n+    /// <para><b>For Beginners:</b>\n+    /// This is like choosing which stage of learning you're currently working on.\n+    /// Set to 0 for the first stage, 1 for the second, etc. Only that stage's adapter\n+    /// will be trained while the others remain frozen.\n+    /// </para>\n+    /// </remarks>\n+    public void SetActiveAdapterIndex(int index)\n+    {\n+        if (index < 0 || index >= _chainLength)\n+        {\n+            throw new ArgumentOutOfRangeException(nameof(index), $\"Index must be between 0 and {_chainLength - 1}\");\n+        }\n+\n+        _activeAdapterIndex = index;\n+    }\n+\n+    /// <summary>\n+    /// Merges the currently active adapter into the base layer representation.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This \"ties a knot\" in the chain by marking the active adapter as merged and frozen.\n+    /// The adapter's weights are conceptually incorporated into the model, allowing the\n+    /// next adapter in the chain to build upon this consolidated knowledge.\n+    /// </para>\n+    /// <para>\n+    /// Note: The actual weight merging into a single layer happens when MergeToOriginalLayer()\n+    /// is called. This method only marks the adapter as merged for training purposes.\n+    /// </para>\n+    /// <para><b>For Beginners:</b>\n+    /// After training an adapter stage, call this to \"lock it in\" before moving to the\n+    /// next stage. It's like saving your progress before starting the next level.\n+    /// </para>\n+    /// </remarks>\n+    public void MergeActiveAdapter()\n+    {\n+        if (_activeAdapterIndex < 0 || _activeAdapterIndex >= _chainLength)\n+        {\n+            throw new InvalidOperationException($\"Invalid active adapter index: {_activeAdapterIndex}\");\n+        }\n+\n+        _mergedStatus[_activeAdapterIndex] = true;\n+        UpdateParameterCount();\n+    }\n+\n+    /// <summary>\n+    /// Unmerges a previously merged adapter, making it trainable again.\n+    /// </summary>\n+    /// <param name=\"index\">The index of the adapter to unmerge.</param>\n+    /// <exception cref=\"ArgumentOutOfRangeException\">Thrown when index is out of range.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// This allows re-training a previously merged adapter if needed for iterative refinement.\n+    /// Useful for scenarios where you want to go back and adjust an earlier stage.\n+    /// </para>\n+    /// </remarks>\n+    public void UnmergeAdapter(int index)\n+    {\n+        if (index < 0 || index >= _chainLength)\n+        {\n+            throw new ArgumentOutOfRangeException(nameof(index), $\"Index must be between 0 and {_chainLength - 1}\");\n+        }\n+\n+        _mergedStatus[index] = false;\n+        UpdateParameterCount();\n+    }\n+\n+    /// <summary>\n+    /// Gets the number of adapters that have been merged.\n+    /// </summary>\n+    /// <returns>Count of merged adapters.</returns>\n+    public int GetMergedCount()\n+    {\n+        return _mergedStatus.Count(merged => merged);\n+    }\n+\n+    /// <summary>\n+    /// Gets the total number of parameters in the chain (base layer + all unmerged adapters).\n+    /// </summary>\n+    /// <remarks>\n+    /// This count includes parameters from the base layer (if not frozen) plus all unmerged adapters in the chain.\n+    /// Merged adapters don't contribute to the parameter count since they've been absorbed into the base weights.\n+    /// Returns the cached _currentParameterCount once the chain is initialized, or computes it on-the-fly\n+    /// during construction to handle base class initialization.\n+    /// </remarks>\n+    public override int ParameterCount\n+    {\n+        get\n+        {\n+            // If chain is not yet initialized (during base construction), compute on-the-fly\n+            if (_adapterChain == null || _currentParameterCount == 0)\n+            {\n+                int count = 0;\n+\n+                // Add base layer parameters if not frozen and baseLayer exists\n+                if (_baseLayer != null && !_freezeBaseLayer)\n+                {\n+                    count += _baseLayer.ParameterCount;\n+                }\n+\n+                // Add LoRA layer parameters if it exists\n+                if (_loraLayer != null)\n+                {\n+                    count += _loraLayer.ParameterCount;\n+                }\n+\n+                return count;\n+            }\n+\n+            // Otherwise return cached value\n+            return _currentParameterCount;\n+        }\n+    }","path":"src/LoRA/Adapters/ChainLoRAAdapter.cs","commit_id":"fa815036ee570162056ba6aca773f34c80f36948","original_commit_id":"fa815036ee570162056ba6aca773f34c80f36948","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n<details>\n<summary>üß© Analysis chain</summary>\n\n**Verify ParameterCount override correctness.**\n\nThe `ParameterCount` property override now correctly returns `_currentParameterCount`, which is updated whenever adapters are merged/unmerged. This addresses the critical issue from the past review about parameter count mismatches.\n\nHowever, verify that the on-the-fly computation during construction (lines 331-347) correctly handles the case where `_adapterChain` is null but `_loraLayer` exists from the base class initialization.\n\n---\n\nüèÅ Script executed:\n\n```shell\n#!/bin/bash\n# Verify that ParameterCount is used consistently across all parameter operations\n\nrg -n -C3 'ParameterCount' --type cs -g '*ChainLoRAAdapter.cs'\n```\n\nLength of output: 8845\n\n---\n\n</thinking>\n\n---\n\nüèÅ Script executed:\n\n```shell\n#!/bin/bash\n# Understand the relationship between _loraLayer and _adapterChain\n\n# 1. Find where _loraLayer is initialized and used\necho \"=== _loraLayer initialization and usage ===\"\nrg -n '_loraLayer' --type cs -g '*ChainLoRAAdapter.cs' -B2 -A2\n\n# 2. Find where _adapterChain is initialized and used\necho -e \"\\n=== _adapterChain initialization ===\"\nrg -n '_adapterChain\\s*=' --type cs -g '*ChainLoRAAdapter.cs' -B2 -A2\n```\n\nLength of output: 1461\n\n---\n\n</parameter>\n</invoke>\n\n---\n\nüèÅ Script executed:\n\n```shell\n#!/bin/bash\n# Find _loraLayer field declaration and understand its relationship to _adapterChain\n\necho \"=== _loraLayer field declaration ===\"\nrg -n 'private.*_loraLayer\\|protected.*_loraLayer' --type cs -g '*ChainLoRAAdapter.cs' -B3 -A3\n\necho -e \"\\n=== All _loraLayer references (unfiltered) ===\"\nrg -n '_loraLayer' --type cs -g '*ChainLoRAAdapter.cs'\n\necho -e \"\\n=== Where adapters are added to _adapterChain ===\"\nrg -n '_adapterChain.*Add\\|\\.Add.*adapter' --type cs -g '*ChainLoRAAdapter.cs' -B2 -A2\n```\n\nLength of output: 315\n\n---\n\n</parameter>\n</invoke>\n\n---\n\nüèÅ Script executed:\n\n```shell\n#!/bin/bash\n# Find the class declaration and base class for ChainLoRAAdapter\n\necho \"=== ChainLoRAAdapter class declaration and inheritance ===\"\nrg -n 'class ChainLoRAAdapter' --type cs -g '*ChainLoRAAdapter.cs' -A5\n\necho -e \"\\n=== Search for _loraLayer in all LoRA-related files ===\"\nrg -n '_loraLayer' --type cs -g '*.cs' | head -20\n\necho -e \"\\n=== Find base adapter class to understand _loraLayer ===\"\nfd --type f -name '*.cs' -path '*/LoRA/*' | head -10\n```\n\nLength of output: 2852\n\n---\n\n</parameter>\n</invoke>\n\n---\n\nüèÅ Script executed:\n\n```shell\n#!/bin/bash\n# Find LoRAAdapterBase and understand _loraLayer\n\necho \"=== Search for LoRAAdapterBase class definition ===\"\nfd --type f -name '*.cs' -path '*/LoRA/*'\n\necho -e \"\\n=== Find _loraLayer field in base class ===\"\nrg -n 'class LoRAAdapterBase|_loraLayer' --type cs -g '*LoRAAdapterBase*' -B2 -A2 | head -30\n\necho -e \"\\n=== Look for AddAdapter method in ChainLoRAAdapter ===\"\nrg -n 'public.*AddAdapter|void AddAdapter' --type cs -g '*ChainLoRAAdapter.cs' -B2 -A8\n```\n\nLength of output: 2073\n\n---\n\n</parameter>\n</invoke>\n\n---\n\nüèÅ Script executed:\n\n```shell\n#!/bin/bash\n# Find AddAdapter and any _loraLayer initialization in ChainLoRAAdapter\n\necho \"=== Search for AddAdapter in ChainLoRAAdapter ===\"\nrg -n 'AddAdapter' --type cs -g '*ChainLoRAAdapter.cs' -B2 -A15\n\necho -e \"\\n=== Search for constructor and initialization in ChainLoRAAdapter ===\"\nrg -n 'public ChainLoRAAdapter|_loraLayer =' --type cs -g '*ChainLoRAAdapter.cs' -B2 -A5\n```\n\nLength of output: 660\n\n---\n\n</parameter>\n</invoke>\n\n---\n\nüèÅ Script executed:\n\n```shell\n#!/bin/bash\n# Get full constructor body and all methods that modify _adapterChain\n\necho \"=== ChainLoRAAdapter constructor (lines 198-250) ===\"\nsed -n '198,250p' src/LoRA/Adapters/ChainLoRAAdapter.cs\n\necho -e \"\\n=== All public methods in ChainLoRAAdapter ===\"\nrg -n 'public.*\\(' --type cs -g '*ChainLoRAAdapter.cs' | head -20\n\necho -e \"\\n=== Search for .Add calls on _adapterChain ===\"\nrg -n '_adapterChain\\.Add|\\.Add.*LoRA' --type cs -g '*ChainLoRAAdapter.cs'\n```\n\nLength of output: 3577\n\n---\n\n\n\n</details>\n\n**Fix ParameterCount override to consistently use only _adapterChain, not _loraLayer.**\n\nThe on-the-fly computation (lines 331-347) uses `_loraLayer` from the base class, but the actual parameter set comes from `_adapterChain` (initialized in the constructor). These are separate and unrelated adapter instances, causing a semantic mismatch:\n\n- During base construction: Returns `_baseLayer + _loraLayer` (single adapter)\n- After `UpdateParameterCount()`: Returns `_baseLayer + unmerged _adapterChain items` (multiple adapters)\n\nThis causes the parameter count to change after initialization, leading to vector size mismatches. The on-the-fly fallback should compute the count from `_adapterChain` directly instead of relying on the inherited `_loraLayer`:\n\n```csharp\nif (_adapterChain == null || _currentParameterCount == 0)\n{\n    int count = 0;\n    if (_baseLayer != null && !_freezeBaseLayer)\n    {\n        count += _baseLayer.ParameterCount;\n    }\n    // Count unmerged adapters from _adapterChain, not _loraLayer\n    if (_adapterChain != null)\n    {\n        for (int i = 0; i < _adapterChain.Count; i++)\n        {\n            if (!_mergedStatus[i])\n            {\n                count += _adapterChain[i].ParameterCount;\n            }\n        }\n    }\n    return count;\n}\n```\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/LoRA/Adapters/ChainLoRAAdapter.cs around lines 326 to 353, the on-the-fly\nParameterCount fallback incorrectly reads _loraLayer from the base class instead\nof counting the actual adapters in _adapterChain; change the logic to compute\ncount from _adapterChain and _mergedStatus: keep the existing addition of\n_baseLayer.ParameterCount when _baseLayer != null && !_freezeBaseLayer, then if\n_adapterChain != null iterate its entries and for each index where\n!_mergedStatus[index] add _adapterChain[index].ParameterCount to the count,\nensure you guard for nulls and bounds of _mergedStatus, and return that total so\nthe pre-construction fallback matches the post-initialization\nUpdateParameterCount result.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T15:16:54Z","updated_at":"2025-11-02T15:16:56Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484862623","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484862623"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484862623"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484862623/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":326,"original_start_line":326,"start_side":"RIGHT","line":353,"original_line":353,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":353,"position":353,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484862625","pull_request_review_id":3408834322,"id":2484862625,"node_id":"PRRC_kwDOKSXUF86UG_6h","diff_hunk":"@@ -0,0 +1,767 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// DoRA (Weight-Decomposed Low-Rank Adaptation) adapter for parameter-efficient fine-tuning with improved stability.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// DoRA (Weight-Decomposed LoRA) extends standard LoRA by decomposing pre-trained weights into\n+/// magnitude and direction components, then applying LoRA only to the direction component.\n+/// This decomposition leads to more stable training and better convergence compared to standard LoRA.\n+/// </para>\n+/// <para>\n+/// <b>Mathematical Formulation:</b>\n+/// Given pre-trained weights W, DoRA decomposes them as:\n+/// - W = m * d, where m is magnitude (scalar per neuron) and d is direction (unit vector)\n+/// - W' = m * normalize(d + LoRA_delta)\n+/// - LoRA_delta = (alpha/rank) * B * A\n+///\n+/// This ensures that LoRA adaptations primarily affect the direction of weights, not their magnitude,\n+/// which improves training stability and convergence.\n+/// </para>\n+/// <para>\n+/// <b>Research Context:</b>\n+/// DoRA was published in February 2024 and presented as an ICML 2024 Oral paper.\n+/// In experiments on LLaMA-7B, DoRA achieved +3.7% improvement over standard LoRA.\n+/// The key insight is that separating magnitude and direction allows more stable gradient flow\n+/// and better control over the adaptation process.\n+/// </para>\n+/// <para>\n+/// <b>For Beginners:</b> DoRA is an improved version of LoRA that works better in practice.\n+///\n+/// Think of neural network weights as arrows:\n+/// - Each arrow has a length (magnitude) and a direction\n+/// - Standard LoRA adjusts both length and direction at the same time\n+/// - DoRA separates them: it keeps the length fixed and only adjusts the direction\n+/// - This makes training more stable and gives better results\n+///\n+/// Why this matters:\n+/// - More stable training (fewer divergences and NaN errors)\n+/// - Better final performance (+3.7% on LLaMA-7B)\n+/// - Same parameter efficiency as standard LoRA\n+/// - Slightly more computation (due to normalization), but worth it for the stability\n+///\n+/// When to use DoRA over standard LoRA:\n+/// - When training stability is important (large models, complex tasks)\n+/// - When you want the best possible fine-tuning results\n+/// - When you have the computational budget for normalization overhead\n+/// - When adapting very large pre-trained models (LLMs, large vision models)\n+/// </para>\n+/// <para>\n+/// <b>Reference:</b>\n+/// \"DoRA: Weight-Decomposed Low-Rank Adaptation\"\n+/// ICML 2024 Oral\n+/// https://arxiv.org/abs/2402.09353\n+/// </para>\n+/// </remarks>\n+public class DoRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Magnitude component of the decomposed weights (scalar per output neuron).\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// The magnitude vector stores the L2 norm of each weight vector (one per output neuron).\n+    /// During forward pass, this magnitude is applied after normalizing the direction vectors.\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This stores the \"strength\" of each output neuron.\n+    /// When we decompose weights into magnitude and direction, this is the magnitude part.\n+    /// Each output neuron gets one magnitude value.\n+    /// </para>\n+    /// </remarks>\n+    private Vector<T> _magnitude;\n+\n+    /// <summary>\n+    /// Gradients for the magnitude component, computed during backpropagation.\n+    /// </summary>\n+    private Vector<T>? _magnitudeGradient;\n+\n+    /// <summary>\n+    /// Cached normalized direction from the last forward pass, used in backpropagation.\n+    /// </summary>\n+    private Matrix<T>? _lastNormalizedDirection;\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// DoRA adds the magnitude parameters (one per output neuron) to the standard LoRA parameters.\n+    /// Total = (base layer parameters if not frozen) + LoRA parameters + magnitude parameters.\n+    /// </para>\n+    /// </remarks>\n+    public override int ParameterCount\n+    {\n+        get\n+        {\n+            int baseCount = (_baseLayer != null && !_freezeBaseLayer) ? _baseLayer.ParameterCount : 0;\n+            int loraCount = _loraLayer != null ? _loraLayer.ParameterCount : 0;\n+            int magnitudeCount = _magnitude != null ? _magnitude.Length : 0;\n+            return baseCount + loraCount + magnitudeCount;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Initializes a new DoRA adapter wrapping an existing layer.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with DoRA.</param>\n+    /// <param name=\"rank\">The rank of the LoRA decomposition.</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// The constructor initializes the DoRA adapter by:\n+    /// 1. Setting up the standard LoRA components (via base constructor)\n+    /// 2. Decomposing the base layer's initial weights into magnitude and direction\n+    /// 3. Initializing magnitude gradients\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This creates a DoRA adapter around your existing layer.\n+    ///\n+    /// What happens during initialization:\n+    /// - The base class sets up standard LoRA (matrices A and B)\n+    /// - We then decompose the layer's weights into magnitude and direction\n+    /// - The magnitude starts as the actual magnitudes from the original weights\n+    /// - During training, both the LoRA matrices and the magnitudes will be updated\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to fine-tune efficiently\n+    /// - rank: How much compression for LoRA (lower = fewer parameters)\n+    /// - alpha: Scaling factor for LoRA contribution\n+    /// - freezeBaseLayer: Usually true - we only train LoRA + magnitude, not base weights\n+    /// </para>\n+    /// </remarks>\n+    public DoRAAdapter(ILayer<T> baseLayer, int rank, double alpha = -1, bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        // Initialize magnitude from base layer weights\n+        int outputSize = GetOutputShape()[0];\n+        _magnitude = new Vector<T>(outputSize);\n+\n+        // Decompose initial weights to get magnitude\n+        DecomposeWeights();\n+\n+        // Update parameters to include magnitude\n+        Parameters = new Vector<T>(ParameterCount);\n+        UpdateParametersFromComponents();\n+    }\n+\n+    /// <summary>\n+    /// Decomposes the base layer's weights into magnitude and direction components.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// For each output neuron, this method:\n+    /// 1. Extracts the weight vector (all connections to that neuron)\n+    /// 2. Computes the L2 norm (magnitude)\n+    /// 3. Stores the magnitude\n+    ///\n+    /// The direction is implicitly W/||W|| and doesn't need to be stored separately.\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This splits weights into magnitude (length) and direction.\n+    ///\n+    /// Imagine each weight vector as an arrow:\n+    /// - Magnitude = how long the arrow is\n+    /// - Direction = which way the arrow points\n+    ///\n+    /// We store the magnitude separately so we can apply LoRA only to the direction.\n+    /// This is the key innovation of DoRA over standard LoRA.\n+    /// </para>\n+    /// </remarks>\n+    private void DecomposeWeights()\n+    {\n+        // Get base layer parameters\n+        Vector<T> baseParams = _baseLayer.GetParameters();\n+\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        int weightCount = inputSize * outputSize;\n+\n+        // For each output neuron, compute the magnitude of its weight vector\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            T sumSquares = NumOps.Zero;\n+\n+            // Sum squares of all weights for this output neuron\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                int idx = i * inputSize + j;\n+                if (idx < weightCount && idx < baseParams.Length)\n+                {\n+                    T weight = baseParams[idx];\n+                    sumSquares = NumOps.Add(sumSquares, NumOps.Multiply(weight, weight));\n+                }\n+            }\n+\n+            // Magnitude is the L2 norm\n+            _magnitude[i] = NumOps.Sqrt(sumSquares);\n+\n+            // Ensure magnitude is never zero (for numerical stability)\n+            if (NumOps.Equals(_magnitude[i], NumOps.Zero))\n+            {\n+                _magnitude[i] = NumOps.FromDouble(1e-8);\n+            }\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Recomposes weights from magnitude and direction components.\n+    /// </summary>\n+    /// <param name=\"direction\">The normalized direction matrix.</param>\n+    /// <returns>The full weight matrix (magnitude * direction).</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method reconstructs the full weight matrix by scaling each direction vector\n+    /// by its corresponding magnitude value.\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This puts magnitude and direction back together.\n+    ///\n+    /// After we've adjusted the direction with LoRA and have the magnitude stored separately,\n+    /// this combines them back into normal weights. Think of it as:\n+    /// - Take each direction vector (unit vector)\n+    /// - Scale it by its magnitude (scalar)\n+    /// - Result: the full weight vector\n+    ///\n+    /// This is used during forward pass to get the effective weights.\n+    /// </para>\n+    /// </remarks>\n+    private Matrix<T> RecomposeWeights(Matrix<T> direction)\n+    {\n+        int outputSize = direction.Rows;\n+        int inputSize = direction.Columns;\n+\n+        Matrix<T> weights = new Matrix<T>(outputSize, inputSize);\n+\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                weights[i, j] = NumOps.Multiply(_magnitude[i], direction[i, j]);\n+            }\n+        }\n+\n+        return weights;\n+    }\n+\n+    /// <summary>\n+    /// Normalizes a matrix row-wise (each row becomes a unit vector).\n+    /// </summary>\n+    /// <param name=\"matrix\">The matrix to normalize.</param>\n+    /// <returns>Row-normalized matrix where each row has unit L2 norm.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// For each row (weight vector), this computes the L2 norm and divides all elements by it.\n+    /// This ensures each direction vector has unit length.\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This makes each weight vector have length 1.\n+    ///\n+    /// When we separate magnitude and direction, the direction must be a unit vector\n+    /// (length = 1). This method ensures that by dividing each weight vector by its length.\n+    ///\n+    /// Example: vector [3, 4] has length 5, so normalized it becomes [0.6, 0.8]\n+    /// </para>\n+    /// </remarks>\n+    private Matrix<T> NormalizeRows(Matrix<T> matrix)\n+    {\n+        int rows = matrix.Rows;\n+        int cols = matrix.Columns;\n+\n+        Matrix<T> normalized = new Matrix<T>(rows, cols);\n+\n+        for (int i = 0; i < rows; i++)\n+        {\n+            // Compute L2 norm of row\n+            T sumSquares = NumOps.Zero;\n+            for (int j = 0; j < cols; j++)\n+            {\n+                T val = matrix[i, j];\n+                sumSquares = NumOps.Add(sumSquares, NumOps.Multiply(val, val));\n+            }\n+\n+            T norm = NumOps.Sqrt(sumSquares);\n+\n+            // Avoid division by zero\n+            if (NumOps.Equals(norm, NumOps.Zero))\n+            {\n+                norm = NumOps.FromDouble(1e-8);\n+            }\n+\n+            // Normalize row\n+            for (int j = 0; j < cols; j++)\n+            {\n+                normalized[i, j] = NumOps.Divide(matrix[i, j], norm);\n+            }\n+        }\n+\n+        return normalized;\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass through DoRA adapter.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Output combining base layer with DoRA-adapted weights.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The DoRA forward pass:\n+    /// 1. Gets base layer weights W\n+    /// 2. Computes direction: d = W / ||W||\n+    /// 3. Applies LoRA to direction: d' = d + LoRA(input)\n+    /// 4. Normalizes adapted direction: d_norm = d' / ||d'||\n+    /// 5. Recomposes weights: W' = m * d_norm\n+    /// 6. Computes output: y = input @ W'^T\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This is where DoRA's magic happens during prediction.\n+    ///\n+    /// Step by step:\n+    /// 1. Get the original weights from the base layer\n+    /// 2. Split into magnitude (stored) and direction (computed)\n+    /// 3. Apply LoRA's correction to the direction (not the magnitude!)\n+    /// 4. Normalize the new direction to keep it as a unit vector\n+    /// 5. Multiply magnitude back in to get final weights\n+    /// 6. Use these adjusted weights to compute the output\n+    ///\n+    /// The key difference from standard LoRA:\n+    /// - Standard LoRA: output = base_output + lora_output\n+    /// - DoRA: output = input @ (m * normalize(d + lora_output))\n+    ///\n+    /// DoRA's approach gives more stable training because we control magnitude separately.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        // Get base layer parameters and extract weights\n+        Vector<T> baseParams = _baseLayer.GetParameters();\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        int weightCount = inputSize * outputSize;\n+\n+        // Extract weight matrix from base layer (assuming weights come first)\n+        Matrix<T> baseWeights = new Matrix<T>(outputSize, inputSize);\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                int weightIdx = i * inputSize + j;\n+                if (weightIdx < weightCount && weightIdx < baseParams.Length)\n+                {\n+                    baseWeights[i, j] = baseParams[weightIdx];\n+                }\n+                else\n+                {\n+                    baseWeights[i, j] = NumOps.Zero;\n+                }\n+            }\n+        }\n+\n+        // Compute base direction (W / ||W||)\n+        Matrix<T> baseDirection = NormalizeRows(baseWeights);\n+\n+        // Get LoRA contribution (this is already scaled by alpha/rank)\n+        Tensor<T> loraOutput = _loraLayer.Forward(input);\n+\n+        // Convert LoRA output to matrix form (batch_size x output_size)\n+        int batchSize = input.Shape[0];\n+        Matrix<T> loraMatrix = new Matrix<T>(batchSize, outputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                loraMatrix[i, j] = loraOutput[i * outputSize + j];\n+            }\n+        }","path":"src/LoRA/Adapters/DoRAAdapter.cs","commit_id":"fa815036ee570162056ba6aca773f34c80f36948","original_commit_id":"fa815036ee570162056ba6aca773f34c80f36948","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_üõ†Ô∏è Refactor suggestion_ | _üü† Major_\n\n**Remove dead code: LoRA forward pass output is unused.**\n\nLines 370-381 compute `loraOutput` via `_loraLayer.Forward(input)` and convert it to `loraMatrix`, but this matrix is never used. Line 387 correctly obtains the LoRA weight delta directly via `MergeWeights()`. Remove the unused forward call to avoid wasted computation.\n\n\n\nApply this diff:\n\n```diff\n         // Compute base direction (W / ||W||)\n         Matrix<T> baseDirection = NormalizeRows(baseWeights);\n \n-        // Get LoRA contribution (this is already scaled by alpha/rank)\n-        Tensor<T> loraOutput = _loraLayer.Forward(input);\n-\n-        // Convert LoRA output to matrix form (batch_size x output_size)\n-        int batchSize = input.Shape[0];\n-        Matrix<T> loraMatrix = new Matrix<T>(batchSize, outputSize);\n-        for (int i = 0; i < batchSize; i++)\n-        {\n-            for (int j = 0; j < outputSize; j++)\n-            {\n-                loraMatrix[i, j] = loraOutput[i * outputSize + j];\n-            }\n-        }\n-\n         // For DoRA, we need to add LoRA to the direction component, not the output\n         // This requires reconstructing how LoRA affects the weight matrix\n         // LoRA computes: input @ A @ B, which is equivalent to input @ (A @ B)^T\n         // We need (A @ B)^T to add to the direction\n         Matrix<T> loraWeightDelta = _loraLayer.MergeWeights(); // This gives us [outputSize, inputSize]\n \n+        int batchSize = input.Shape[0];\n+\n         // Add LoRA delta to base direction: d' = d + delta\n         Matrix<T> adaptedDirection = new Matrix<T>(outputSize, inputSize);\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        // For DoRA, we need to add LoRA to the direction component, not the output\n        // This requires reconstructing how LoRA affects the weight matrix\n        // LoRA computes: input @ A @ B, which is equivalent to input @ (A @ B)^T\n        // We need (A @ B)^T to add to the direction\n        Matrix<T> loraWeightDelta = _loraLayer.MergeWeights(); // This gives us [outputSize, inputSize]\n\n        int batchSize = input.Shape[0];\n\n        // Add LoRA delta to base direction: d' = d + delta\n        Matrix<T> adaptedDirection = new Matrix<T>(outputSize, inputSize);\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/LoRA/Adapters/DoRAAdapter.cs around lines 370 to 381, the code calls\n_loraLayer.Forward(input) and builds loraMatrix from loraOutput but that result\nis never used; remove the unused forward call and the entire\nloraOutput-to-loraMatrix conversion block (lines 370‚Äì381) so the method relies\non the existing MergeWeights() path that obtains the LoRA weight delta, ensuring\nno remaining references to loraOutput or loraMatrix are left.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T15:16:54Z","updated_at":"2025-11-02T15:16:56Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484862625","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484862625"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484862625"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484862625/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":370,"original_start_line":370,"start_side":"RIGHT","line":381,"original_line":381,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":381,"position":381,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484862627","pull_request_review_id":3408834322,"id":2484862627,"node_id":"PRRC_kwDOKSXUF86UG_6j","diff_hunk":"@@ -0,0 +1,767 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// DoRA (Weight-Decomposed Low-Rank Adaptation) adapter for parameter-efficient fine-tuning with improved stability.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// DoRA (Weight-Decomposed LoRA) extends standard LoRA by decomposing pre-trained weights into\n+/// magnitude and direction components, then applying LoRA only to the direction component.\n+/// This decomposition leads to more stable training and better convergence compared to standard LoRA.\n+/// </para>\n+/// <para>\n+/// <b>Mathematical Formulation:</b>\n+/// Given pre-trained weights W, DoRA decomposes them as:\n+/// - W = m * d, where m is magnitude (scalar per neuron) and d is direction (unit vector)\n+/// - W' = m * normalize(d + LoRA_delta)\n+/// - LoRA_delta = (alpha/rank) * B * A\n+///\n+/// This ensures that LoRA adaptations primarily affect the direction of weights, not their magnitude,\n+/// which improves training stability and convergence.\n+/// </para>\n+/// <para>\n+/// <b>Research Context:</b>\n+/// DoRA was published in February 2024 and presented as an ICML 2024 Oral paper.\n+/// In experiments on LLaMA-7B, DoRA achieved +3.7% improvement over standard LoRA.\n+/// The key insight is that separating magnitude and direction allows more stable gradient flow\n+/// and better control over the adaptation process.\n+/// </para>\n+/// <para>\n+/// <b>For Beginners:</b> DoRA is an improved version of LoRA that works better in practice.\n+///\n+/// Think of neural network weights as arrows:\n+/// - Each arrow has a length (magnitude) and a direction\n+/// - Standard LoRA adjusts both length and direction at the same time\n+/// - DoRA separates them: it keeps the length fixed and only adjusts the direction\n+/// - This makes training more stable and gives better results\n+///\n+/// Why this matters:\n+/// - More stable training (fewer divergences and NaN errors)\n+/// - Better final performance (+3.7% on LLaMA-7B)\n+/// - Same parameter efficiency as standard LoRA\n+/// - Slightly more computation (due to normalization), but worth it for the stability\n+///\n+/// When to use DoRA over standard LoRA:\n+/// - When training stability is important (large models, complex tasks)\n+/// - When you want the best possible fine-tuning results\n+/// - When you have the computational budget for normalization overhead\n+/// - When adapting very large pre-trained models (LLMs, large vision models)\n+/// </para>\n+/// <para>\n+/// <b>Reference:</b>\n+/// \"DoRA: Weight-Decomposed Low-Rank Adaptation\"\n+/// ICML 2024 Oral\n+/// https://arxiv.org/abs/2402.09353\n+/// </para>\n+/// </remarks>\n+public class DoRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Magnitude component of the decomposed weights (scalar per output neuron).\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// The magnitude vector stores the L2 norm of each weight vector (one per output neuron).\n+    /// During forward pass, this magnitude is applied after normalizing the direction vectors.\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This stores the \"strength\" of each output neuron.\n+    /// When we decompose weights into magnitude and direction, this is the magnitude part.\n+    /// Each output neuron gets one magnitude value.\n+    /// </para>\n+    /// </remarks>\n+    private Vector<T> _magnitude;\n+\n+    /// <summary>\n+    /// Gradients for the magnitude component, computed during backpropagation.\n+    /// </summary>\n+    private Vector<T>? _magnitudeGradient;\n+\n+    /// <summary>\n+    /// Cached normalized direction from the last forward pass, used in backpropagation.\n+    /// </summary>\n+    private Matrix<T>? _lastNormalizedDirection;\n+\n+    /// <summary>\n+    /// Gets the total number of trainable parameters.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// DoRA adds the magnitude parameters (one per output neuron) to the standard LoRA parameters.\n+    /// Total = (base layer parameters if not frozen) + LoRA parameters + magnitude parameters.\n+    /// </para>\n+    /// </remarks>\n+    public override int ParameterCount\n+    {\n+        get\n+        {\n+            int baseCount = (_baseLayer != null && !_freezeBaseLayer) ? _baseLayer.ParameterCount : 0;\n+            int loraCount = _loraLayer != null ? _loraLayer.ParameterCount : 0;\n+            int magnitudeCount = _magnitude != null ? _magnitude.Length : 0;\n+            return baseCount + loraCount + magnitudeCount;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Initializes a new DoRA adapter wrapping an existing layer.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The layer to adapt with DoRA.</param>\n+    /// <param name=\"rank\">The rank of the LoRA decomposition.</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// The constructor initializes the DoRA adapter by:\n+    /// 1. Setting up the standard LoRA components (via base constructor)\n+    /// 2. Decomposing the base layer's initial weights into magnitude and direction\n+    /// 3. Initializing magnitude gradients\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This creates a DoRA adapter around your existing layer.\n+    ///\n+    /// What happens during initialization:\n+    /// - The base class sets up standard LoRA (matrices A and B)\n+    /// - We then decompose the layer's weights into magnitude and direction\n+    /// - The magnitude starts as the actual magnitudes from the original weights\n+    /// - During training, both the LoRA matrices and the magnitudes will be updated\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The layer you want to fine-tune efficiently\n+    /// - rank: How much compression for LoRA (lower = fewer parameters)\n+    /// - alpha: Scaling factor for LoRA contribution\n+    /// - freezeBaseLayer: Usually true - we only train LoRA + magnitude, not base weights\n+    /// </para>\n+    /// </remarks>\n+    public DoRAAdapter(ILayer<T> baseLayer, int rank, double alpha = -1, bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        // Initialize magnitude from base layer weights\n+        int outputSize = GetOutputShape()[0];\n+        _magnitude = new Vector<T>(outputSize);\n+\n+        // Decompose initial weights to get magnitude\n+        DecomposeWeights();\n+\n+        // Update parameters to include magnitude\n+        Parameters = new Vector<T>(ParameterCount);\n+        UpdateParametersFromComponents();\n+    }\n+\n+    /// <summary>\n+    /// Decomposes the base layer's weights into magnitude and direction components.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// For each output neuron, this method:\n+    /// 1. Extracts the weight vector (all connections to that neuron)\n+    /// 2. Computes the L2 norm (magnitude)\n+    /// 3. Stores the magnitude\n+    ///\n+    /// The direction is implicitly W/||W|| and doesn't need to be stored separately.\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This splits weights into magnitude (length) and direction.\n+    ///\n+    /// Imagine each weight vector as an arrow:\n+    /// - Magnitude = how long the arrow is\n+    /// - Direction = which way the arrow points\n+    ///\n+    /// We store the magnitude separately so we can apply LoRA only to the direction.\n+    /// This is the key innovation of DoRA over standard LoRA.\n+    /// </para>\n+    /// </remarks>\n+    private void DecomposeWeights()\n+    {\n+        // Get base layer parameters\n+        Vector<T> baseParams = _baseLayer.GetParameters();\n+\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        int weightCount = inputSize * outputSize;\n+\n+        // For each output neuron, compute the magnitude of its weight vector\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            T sumSquares = NumOps.Zero;\n+\n+            // Sum squares of all weights for this output neuron\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                int idx = i * inputSize + j;\n+                if (idx < weightCount && idx < baseParams.Length)\n+                {\n+                    T weight = baseParams[idx];\n+                    sumSquares = NumOps.Add(sumSquares, NumOps.Multiply(weight, weight));\n+                }\n+            }\n+\n+            // Magnitude is the L2 norm\n+            _magnitude[i] = NumOps.Sqrt(sumSquares);\n+\n+            // Ensure magnitude is never zero (for numerical stability)\n+            if (NumOps.Equals(_magnitude[i], NumOps.Zero))\n+            {\n+                _magnitude[i] = NumOps.FromDouble(1e-8);\n+            }\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Recomposes weights from magnitude and direction components.\n+    /// </summary>\n+    /// <param name=\"direction\">The normalized direction matrix.</param>\n+    /// <returns>The full weight matrix (magnitude * direction).</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method reconstructs the full weight matrix by scaling each direction vector\n+    /// by its corresponding magnitude value.\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This puts magnitude and direction back together.\n+    ///\n+    /// After we've adjusted the direction with LoRA and have the magnitude stored separately,\n+    /// this combines them back into normal weights. Think of it as:\n+    /// - Take each direction vector (unit vector)\n+    /// - Scale it by its magnitude (scalar)\n+    /// - Result: the full weight vector\n+    ///\n+    /// This is used during forward pass to get the effective weights.\n+    /// </para>\n+    /// </remarks>\n+    private Matrix<T> RecomposeWeights(Matrix<T> direction)\n+    {\n+        int outputSize = direction.Rows;\n+        int inputSize = direction.Columns;\n+\n+        Matrix<T> weights = new Matrix<T>(outputSize, inputSize);\n+\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                weights[i, j] = NumOps.Multiply(_magnitude[i], direction[i, j]);\n+            }\n+        }\n+\n+        return weights;\n+    }\n+\n+    /// <summary>\n+    /// Normalizes a matrix row-wise (each row becomes a unit vector).\n+    /// </summary>\n+    /// <param name=\"matrix\">The matrix to normalize.</param>\n+    /// <returns>Row-normalized matrix where each row has unit L2 norm.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// For each row (weight vector), this computes the L2 norm and divides all elements by it.\n+    /// This ensures each direction vector has unit length.\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This makes each weight vector have length 1.\n+    ///\n+    /// When we separate magnitude and direction, the direction must be a unit vector\n+    /// (length = 1). This method ensures that by dividing each weight vector by its length.\n+    ///\n+    /// Example: vector [3, 4] has length 5, so normalized it becomes [0.6, 0.8]\n+    /// </para>\n+    /// </remarks>\n+    private Matrix<T> NormalizeRows(Matrix<T> matrix)\n+    {\n+        int rows = matrix.Rows;\n+        int cols = matrix.Columns;\n+\n+        Matrix<T> normalized = new Matrix<T>(rows, cols);\n+\n+        for (int i = 0; i < rows; i++)\n+        {\n+            // Compute L2 norm of row\n+            T sumSquares = NumOps.Zero;\n+            for (int j = 0; j < cols; j++)\n+            {\n+                T val = matrix[i, j];\n+                sumSquares = NumOps.Add(sumSquares, NumOps.Multiply(val, val));\n+            }\n+\n+            T norm = NumOps.Sqrt(sumSquares);\n+\n+            // Avoid division by zero\n+            if (NumOps.Equals(norm, NumOps.Zero))\n+            {\n+                norm = NumOps.FromDouble(1e-8);\n+            }\n+\n+            // Normalize row\n+            for (int j = 0; j < cols; j++)\n+            {\n+                normalized[i, j] = NumOps.Divide(matrix[i, j], norm);\n+            }\n+        }\n+\n+        return normalized;\n+    }\n+\n+    /// <summary>\n+    /// Performs the forward pass through DoRA adapter.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <returns>Output combining base layer with DoRA-adapted weights.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The DoRA forward pass:\n+    /// 1. Gets base layer weights W\n+    /// 2. Computes direction: d = W / ||W||\n+    /// 3. Applies LoRA to direction: d' = d + LoRA(input)\n+    /// 4. Normalizes adapted direction: d_norm = d' / ||d'||\n+    /// 5. Recomposes weights: W' = m * d_norm\n+    /// 6. Computes output: y = input @ W'^T\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This is where DoRA's magic happens during prediction.\n+    ///\n+    /// Step by step:\n+    /// 1. Get the original weights from the base layer\n+    /// 2. Split into magnitude (stored) and direction (computed)\n+    /// 3. Apply LoRA's correction to the direction (not the magnitude!)\n+    /// 4. Normalize the new direction to keep it as a unit vector\n+    /// 5. Multiply magnitude back in to get final weights\n+    /// 6. Use these adjusted weights to compute the output\n+    ///\n+    /// The key difference from standard LoRA:\n+    /// - Standard LoRA: output = base_output + lora_output\n+    /// - DoRA: output = input @ (m * normalize(d + lora_output))\n+    ///\n+    /// DoRA's approach gives more stable training because we control magnitude separately.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Forward(Tensor<T> input)\n+    {\n+        // Get base layer parameters and extract weights\n+        Vector<T> baseParams = _baseLayer.GetParameters();\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        int weightCount = inputSize * outputSize;\n+\n+        // Extract weight matrix from base layer (assuming weights come first)\n+        Matrix<T> baseWeights = new Matrix<T>(outputSize, inputSize);\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                int weightIdx = i * inputSize + j;\n+                if (weightIdx < weightCount && weightIdx < baseParams.Length)\n+                {\n+                    baseWeights[i, j] = baseParams[weightIdx];\n+                }\n+                else\n+                {\n+                    baseWeights[i, j] = NumOps.Zero;\n+                }\n+            }\n+        }\n+\n+        // Compute base direction (W / ||W||)\n+        Matrix<T> baseDirection = NormalizeRows(baseWeights);\n+\n+        // Get LoRA contribution (this is already scaled by alpha/rank)\n+        Tensor<T> loraOutput = _loraLayer.Forward(input);\n+\n+        // Convert LoRA output to matrix form (batch_size x output_size)\n+        int batchSize = input.Shape[0];\n+        Matrix<T> loraMatrix = new Matrix<T>(batchSize, outputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                loraMatrix[i, j] = loraOutput[i * outputSize + j];\n+            }\n+        }\n+\n+        // For DoRA, we need to add LoRA to the direction component, not the output\n+        // This requires reconstructing how LoRA affects the weight matrix\n+        // LoRA computes: input @ A @ B, which is equivalent to input @ (A @ B)^T\n+        // We need (A @ B)^T to add to the direction\n+        Matrix<T> loraWeightDelta = _loraLayer.MergeWeights(); // This gives us [outputSize, inputSize]\n+\n+        // Add LoRA delta to base direction: d' = d + delta\n+        Matrix<T> adaptedDirection = new Matrix<T>(outputSize, inputSize);\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                adaptedDirection[i, j] = NumOps.Add(baseDirection[i, j], loraWeightDelta[i, j]);\n+            }\n+        }\n+\n+        // Normalize the adapted direction: d_norm = d' / ||d'||\n+        _lastNormalizedDirection = NormalizeRows(adaptedDirection);\n+\n+        // Recompose weights: W' = m * d_norm\n+        Matrix<T> finalWeights = RecomposeWeights(_lastNormalizedDirection);\n+\n+        // Compute output: y = input @ W'^T\n+        // Convert input to matrix\n+        Matrix<T> inputMatrix = new Matrix<T>(batchSize, inputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < inputSize; j++)\n+            {\n+                inputMatrix[i, j] = input[i * inputSize + j];\n+            }\n+        }\n+\n+        // Matrix multiply: [batchSize, inputSize] @ [inputSize, outputSize]\n+        Matrix<T> outputMatrix = inputMatrix.Multiply(finalWeights.Transpose());\n+\n+        // Convert back to tensor\n+        Vector<T> outputData = new Vector<T>(batchSize * outputSize);\n+        int idx = 0;\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                outputData[idx++] = outputMatrix[i, j];\n+            }\n+        }\n+\n+        return new Tensor<T>(new[] { batchSize, outputSize }, outputData);\n+    }\n+\n+    /// <summary>\n+    /// Performs the backward pass through DoRA adapter.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">Gradient flowing back from the next layer.</param>\n+    /// <returns>Gradient to pass to the previous layer.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// The backward pass computes gradients for:\n+    /// 1. Magnitude parameters (one per output neuron)\n+    /// 2. LoRA matrices A and B (via LoRA layer's backward)\n+    /// 3. Base layer weights (if not frozen)\n+    ///\n+    /// The key challenge is computing how changes to magnitude and direction affect the loss,\n+    /// given that the direction is normalized during forward pass.\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This is where DoRA learns during training.\n+    ///\n+    /// Backward pass figures out how to improve three things:\n+    /// 1. The magnitude of each output neuron's weights\n+    /// 2. The LoRA matrices that adjust the direction\n+    /// 3. The base layer weights (if we're training them too)\n+    ///\n+    /// The math is complex because we need to account for the normalization step.\n+    /// When we normalize the direction, it creates a dependency between all elements\n+    /// of a weight vector, so the gradients need to account for that.\n+    ///\n+    /// For simplicity, this implementation computes approximate gradients that work well\n+    /// in practice. The exact gradients would require storing more intermediate values\n+    /// from the forward pass.\n+    /// </para>\n+    /// </remarks>\n+    public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (_lastNormalizedDirection == null)\n+        {\n+            throw new InvalidOperationException(\"Forward pass must be called before backward pass\");\n+        }\n+\n+        int batchSize = outputGradient.Shape[0];\n+        int outputSize = GetOutputShape()[0];\n+        int inputSize = GetInputShape()[0];\n+\n+        // Convert output gradient to matrix\n+        Matrix<T> gradMatrix = new Matrix<T>(batchSize, outputSize);\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < outputSize; j++)\n+            {\n+                gradMatrix[i, j] = outputGradient[i * outputSize + j];\n+            }\n+        }\n+\n+        // Compute magnitude gradients\n+        // dL/dm_i = sum over batch of (outputGrad_i * normalizedDirection_i)\n+        _magnitudeGradient = new Vector<T>(_magnitude.Length);\n+        for (int i = 0; i < outputSize; i++)\n+        {\n+            T gradSum = NumOps.Zero;\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                T grad = gradMatrix[b, i];\n+                // Gradient contribution from this output\n+                // Each output is computed as: output_i = m_i * (normalized_direction_i ¬∑ input)\n+                // We need the input, but we can approximate the magnitude gradient\n+                gradSum = NumOps.Add(gradSum, grad);\n+            }\n+            _magnitudeGradient[i] = gradSum;\n+        }","path":"src/LoRA/Adapters/DoRAAdapter.cs","commit_id":"fa815036ee570162056ba6aca773f34c80f36948","original_commit_id":"fa815036ee570162056ba6aca773f34c80f36948","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**Magnitude gradient computation is incomplete.**\n\nThe magnitude gradient simply sums output gradients per neuron but omits the input contribution. The mathematically correct gradient is `dL/dm_i = sum_batch(dL/dout_i * (normalized_direction_i ¬∑ input_batch))`. This approximation may degrade training performance compared to exact gradients. Consider storing the forward-pass input or computing the full gradient for production use.\n\n\n\nFor reference, the correct gradient computation should be:\n\n```python\n# Pseudocode for correct magnitude gradient\nfor i in range(outputSize):\n    gradSum = 0\n    for b in range(batchSize):\n        # Need the dot product of normalized_direction[i] with input[b]\n        dot_product = sum(normalized_direction[i, j] * input[b, j] for j in range(inputSize))\n        gradSum += outputGradient[b, i] * dot_product\n    magnitudeGradient[i] = gradSum\n```\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T15:16:55Z","updated_at":"2025-11-02T15:16:56Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484862627","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484862627"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484862627"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484862627/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":486,"original_start_line":486,"start_side":"RIGHT","line":501,"original_line":501,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":501,"position":501,"subject_type":"line"},{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484862630","pull_request_review_id":3408834322,"id":2484862630,"node_id":"PRRC_kwDOKSXUF86UG_6m","diff_hunk":"@@ -0,0 +1,922 @@\n+using AiDotNet.Interfaces;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.LoRA.Adapters;\n+\n+/// <summary>\n+/// S-LoRA adapter for scalable serving of thousands of concurrent LoRA adapters.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// S-LoRA (Scalable LoRA) is a system designed for efficient serving of many LoRA adapters simultaneously.\n+/// Published in November 2023, it addresses the challenge of deploying thousands of task-specific LoRA adapters\n+/// in production environments with limited GPU memory.\n+/// </para>\n+/// <para><b>For Beginners:</b> S-LoRA solves a real-world problem in production AI systems.\n+///\n+/// The problem:\n+/// - You have a large base model (like GPT or LLaMA)\n+/// - You want to serve thousands of different LoRA adapters (one per customer, task, or use case)\n+/// - Each adapter is small (few MB), but thousands of them won't fit in GPU memory\n+/// - Naive approaches either: load one adapter at a time (slow) or reserve memory for all (wasteful)\n+///\n+/// S-LoRA's solution:\n+/// - Unified memory pool: Dynamically manage adapter weights and cache together\n+/// - Batched computation: Process multiple adapters in parallel efficiently\n+/// - Adapter clustering: Group adapters by rank for optimized computation\n+/// - On-demand loading: Fetch adapters from CPU to GPU memory only when needed\n+///\n+/// Key features implemented:\n+/// 1. **Unified Memory Pool**: Single pool for adapter weights (no pre-allocation waste)\n+/// 2. **Adapter Clustering**: Group adapters by rank for batched computation\n+/// 3. **Dynamic Loading**: Load adapters on-demand, evict when not needed\n+/// 4. **Batched Forward Pass**: Process multiple requests with different adapters simultaneously\n+/// 5. **Memory Efficiency**: Serve 100x more adapters than naive approaches\n+///\n+/// Research Paper Reference:\n+/// \"S-LoRA: Serving Thousands of Concurrent LoRA Adapters\"\n+/// Ying Sheng, Shiyi Cao, et al. (November 2023)\n+/// arXiv:2311.03285\n+///\n+/// Performance (from paper):\n+/// - Throughput: 4x improvement over vLLM, 30x over HuggingFace PEFT\n+/// - Adapter capacity: 2,000+ concurrent adapters on single server\n+/// - Memory efficiency: 75-90% GPU memory utilization\n+/// - Scalability: Superlinear throughput scaling with more GPUs\n+///\n+/// Example usage:\n+/// ```csharp\n+/// // Create S-LoRA serving system for base layer\n+/// var sloraAdapter = new SLoRAAdapter&lt;double&gt;(baseLayer, rank: 8);\n+///\n+/// // Register multiple adapters for different tasks\n+/// sloraAdapter.RegisterAdapter(\"customer_1\", adapter1);\n+/// sloraAdapter.RegisterAdapter(\"customer_2\", adapter2);\n+/// sloraAdapter.RegisterAdapter(\"task_classification\", adapter3);\n+///\n+/// // Process batched requests efficiently\n+/// var outputs = sloraAdapter.BatchForward(inputs, adapterIds);\n+/// ```\n+///\n+/// When to use S-LoRA:\n+/// - Serving multiple LoRA adapters in production\n+/// - Multi-tenant AI systems (one adapter per tenant)\n+/// - Task-specific fine-tuning at scale\n+/// - Limited GPU memory but many adapters\n+/// - Need high throughput with many concurrent users\n+///\n+/// Differences from standard LoRA:\n+/// - Standard LoRA: Single adapter, simple forward/backward pass\n+/// - S-LoRA: Multiple adapters, optimized for concurrent serving, memory pooling\n+/// </para>\n+/// </remarks>\n+public class SLoRAAdapter<T> : LoRAAdapterBase<T>\n+{\n+    /// <summary>\n+    /// Represents an adapter entry in the memory pool.\n+    /// </summary>\n+    private class AdapterEntry\n+    {\n+        /// <summary>\n+        /// The adapter's unique identifier.\n+        /// </summary>\n+        public string Id { get; set; }\n+\n+        /// <summary>\n+        /// The LoRA layer for this adapter.\n+        /// </summary>\n+        public LoRALayer<T> Layer { get; set; }\n+\n+        /// <summary>\n+        /// The rank of this adapter.\n+        /// </summary>\n+        public int Rank { get; set; }\n+\n+        /// <summary>\n+        /// Whether this adapter is currently loaded in \"GPU memory\" (in-memory cache).\n+        /// </summary>\n+        public bool IsLoaded { get; set; }\n+\n+        /// <summary>\n+        /// Last access timestamp for LRU eviction.\n+        /// </summary>\n+        public long LastAccess { get; set; }\n+\n+        /// <summary>\n+        /// Reference count for active requests using this adapter.\n+        /// </summary>\n+        public int ReferenceCount { get; set; }\n+\n+        /// <summary>\n+        /// Initializes a new adapter entry.\n+        /// </summary>\n+        public AdapterEntry(string id, LoRALayer<T> layer, int rank)\n+        {\n+            Id = id ?? string.Empty;\n+            Layer = layer;\n+            Rank = rank;\n+            IsLoaded = false;\n+            LastAccess = 0;\n+            ReferenceCount = 0;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Unified memory pool storing all registered adapters.\n+    /// </summary>\n+    /// <remarks>\n+    /// This simulates S-LoRA's unified memory pool where all adapters reside in CPU memory\n+    /// and are dynamically loaded to GPU memory based on demand.\n+    /// </remarks>\n+    private readonly Dictionary<string, AdapterEntry> _adapterPool;\n+\n+    /// <summary>\n+    /// Adapters currently loaded in \"GPU memory\" (in-memory cache).\n+    /// </summary>\n+    private readonly Dictionary<string, AdapterEntry> _loadedAdapters;\n+\n+    /// <summary>\n+    /// Adapters clustered by rank for efficient batched computation.\n+    /// </summary>\n+    private readonly Dictionary<int, List<string>> _rankClusters;\n+\n+    /// <summary>\n+    /// Maximum number of adapters that can be loaded simultaneously (simulates GPU memory limit).\n+    /// </summary>\n+    private readonly int _maxLoadedAdapters;\n+\n+    /// <summary>\n+    /// Current timestamp for LRU eviction policy.\n+    /// </summary>\n+    private long _timestamp;\n+\n+    /// <summary>\n+    /// Gets the total number of registered adapters in the pool.\n+    /// </summary>\n+    /// <remarks>\n+    /// This represents all adapters in the system, including those not currently loaded.\n+    /// S-LoRA can serve thousands of adapters from a unified pool.\n+    /// </remarks>\n+    public int TotalAdapterCount => _adapterPool.Count;\n+\n+    /// <summary>\n+    /// Gets the number of adapters currently loaded in memory.\n+    /// </summary>\n+    /// <remarks>\n+    /// This represents the \"hot\" adapters actively being used or cached.\n+    /// S-LoRA dynamically loads/evicts adapters based on request patterns.\n+    /// </remarks>\n+    public int LoadedAdapterCount => _loadedAdapters.Count;\n+\n+    /// <summary>\n+    /// Gets the maximum number of adapters that can be loaded simultaneously.\n+    /// </summary>\n+    /// <remarks>\n+    /// This simulates GPU memory constraints. S-LoRA's unified paging mechanism\n+    /// efficiently manages this limited resource.\n+    /// </remarks>\n+    public int MaxLoadedAdapters => _maxLoadedAdapters;\n+\n+    /// <summary>\n+    /// Gets the number of rank clusters for batched computation optimization.\n+    /// </summary>\n+    /// <remarks>\n+    /// Adapters with the same rank are clustered together for efficient batched computation.\n+    /// This is a key optimization in S-LoRA for heterogeneous adapter serving.\n+    /// </remarks>\n+    public int RankClusterCount => _rankClusters.Count;\n+\n+    /// <summary>\n+    /// Initializes a new S-LoRA adapter for scalable multi-adapter serving.\n+    /// </summary>\n+    /// <param name=\"baseLayer\">The base layer to adapt with S-LoRA.</param>\n+    /// <param name=\"rank\">The default rank for the primary LoRA decomposition.</param>\n+    /// <param name=\"alpha\">The LoRA scaling factor (defaults to rank if negative).</param>\n+    /// <param name=\"maxLoadedAdapters\">Maximum number of adapters to keep loaded simultaneously (default: 100).</param>\n+    /// <param name=\"freezeBaseLayer\">Whether to freeze the base layer's parameters during training.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when baseLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when maxLoadedAdapters is less than 1.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This creates an S-LoRA serving system for efficient multi-adapter deployment.\n+    ///\n+    /// Parameters:\n+    /// - baseLayer: The shared base model that all adapters modify\n+    /// - rank: Default rank for new adapters (typical: 8-32)\n+    /// - alpha: Scaling factor for LoRA contributions\n+    /// - maxLoadedAdapters: How many adapters to cache in \"GPU memory\" (100 = good balance)\n+    /// - freezeBaseLayer: Lock base weights (true for serving, false for continued training)\n+    ///\n+    /// How S-LoRA works:\n+    /// 1. One base model shared across all adapters (memory efficient)\n+    /// 2. Thousands of small adapters registered in unified pool\n+    /// 3. Only popular adapters kept loaded in fast memory\n+    /// 4. Unpopular adapters evicted and loaded on-demand\n+    /// 5. Batched computation for multiple adapters simultaneously\n+    ///\n+    /// Example: Serving 10,000 customer-specific adapters:\n+    /// - Base model: 7B parameters (14 GB)\n+    /// - Each adapter: rank 16 (few MB)\n+    /// - Total pool: 10,000 adapters (few GB in CPU memory)\n+    /// - Loaded cache: 100 most-used adapters (hundreds of MB in GPU memory)\n+    /// - Result: Serve 10,000 adapters with GPU memory for 1 base model + 100 adapters!\n+    ///\n+    /// This is 100x more efficient than loading full fine-tuned models.\n+    /// </para>\n+    /// </remarks>\n+    public SLoRAAdapter(\n+        ILayer<T> baseLayer,\n+        int rank,\n+        double alpha = -1,\n+        int maxLoadedAdapters = 100,\n+        bool freezeBaseLayer = true)\n+        : base(baseLayer, rank, alpha, freezeBaseLayer)\n+    {\n+        if (maxLoadedAdapters < 1)\n+        {\n+            throw new ArgumentException(\"Max loaded adapters must be at least 1\", nameof(maxLoadedAdapters));\n+        }\n+\n+        _adapterPool = new Dictionary<string, AdapterEntry>();\n+        _loadedAdapters = new Dictionary<string, AdapterEntry>();\n+        _rankClusters = new Dictionary<int, List<string>>();\n+        _maxLoadedAdapters = maxLoadedAdapters;\n+        _timestamp = 0;\n+\n+        // Register the primary adapter (from base class)\n+        RegisterAdapter(\"primary\", _loraLayer, rank);\n+        LoadAdapter(\"primary\");\n+    }\n+\n+    /// <summary>\n+    /// Registers a new adapter in the unified memory pool.\n+    /// </summary>\n+    /// <param name=\"adapterId\">Unique identifier for this adapter.</param>\n+    /// <param name=\"loraLayer\">The LoRA layer to register.</param>\n+    /// <param name=\"rank\">The rank of this adapter.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when adapterId or loraLayer is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when an adapter with this ID already exists.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// This method adds a new adapter to S-LoRA's unified memory pool. The adapter is not immediately\n+    /// loaded into GPU memory but is available for on-demand loading when needed.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is like adding a new customer or task-specific adapter to your system.\n+    ///\n+    /// What happens when you register an adapter:\n+    /// 1. Adapter stored in CPU memory pool (cheap storage)\n+    /// 2. Added to rank cluster for batched computation optimization\n+    /// 3. Not loaded to GPU yet (only loaded when first used)\n+    /// 4. Can register thousands of adapters this way\n+    ///\n+    /// Example: Multi-tenant SaaS application\n+    /// ```csharp\n+    /// var slora = new SLoRAAdapter&lt;double&gt;(baseModel, rank: 8, maxLoadedAdapters: 100);\n+    ///\n+    /// // Register 1000 customer adapters\n+    /// for (int i = 0; i &lt; 1000; i++)\n+    /// {\n+    ///     var adapter = LoadCustomerAdapter(i);\n+    ///     slora.RegisterAdapter($\"customer_{i}\", adapter, rank: 8);\n+    /// }\n+    ///\n+    /// // All 1000 adapters registered, but only 100 will be loaded at once\n+    /// // Popular customers get fast GPU-cached access\n+    /// // Inactive customers loaded on-demand from CPU pool\n+    /// ```\n+    ///\n+    /// This enables serving far more adapters than GPU memory allows!\n+    /// </para>\n+    /// </remarks>\n+    public void RegisterAdapter(string adapterId, LoRALayer<T> loraLayer, int rank)\n+    {\n+        if (adapterId == null)\n+        {\n+            throw new ArgumentNullException(nameof(adapterId));\n+        }\n+\n+        if (loraLayer == null)\n+        {\n+            throw new ArgumentNullException(nameof(loraLayer));\n+        }\n+\n+        if (_adapterPool.ContainsKey(adapterId))\n+        {\n+            throw new ArgumentException($\"Adapter with ID '{adapterId}' already exists\", nameof(adapterId));\n+        }\n+\n+        // Create adapter entry\n+        var entry = new AdapterEntry(adapterId, loraLayer, rank);\n+        _adapterPool[adapterId] = entry;\n+\n+        // Add to rank cluster for batched computation\n+        if (!_rankClusters.ContainsKey(rank))\n+        {\n+            _rankClusters[rank] = new List<string>();\n+        }\n+        _rankClusters[rank].Add(adapterId);\n+    }\n+\n+    /// <summary>\n+    /// Loads an adapter from the pool into active memory (simulates GPU loading).\n+    /// </summary>\n+    /// <param name=\"adapterId\">The ID of the adapter to load.</param>\n+    /// <exception cref=\"ArgumentException\">Thrown when adapter ID is not found in pool.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// This method simulates S-LoRA's dynamic adapter loading from CPU to GPU memory.\n+    /// If the loaded adapter cache is full, it evicts the least recently used adapter.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This moves an adapter from slow storage to fast cache.\n+    ///\n+    /// In S-LoRA's architecture:\n+    /// - CPU memory: All adapters stored here (slow but large capacity)\n+    /// - GPU memory: Hot adapters cached here (fast but limited capacity)\n+    ///\n+    /// Loading process:\n+    /// 1. Check if adapter already loaded (if yes, update access time and return)\n+    /// 2. Check if cache is full (if yes, evict least recently used adapter)\n+    /// 3. Load adapter into cache\n+    /// 4. Mark as loaded and update access timestamp\n+    ///\n+    /// LRU eviction policy:\n+    /// - Adapters with oldest last access time evicted first\n+    /// - Adapters with active references (in-flight requests) never evicted\n+    /// - This keeps popular adapters hot in cache\n+    ///\n+    /// Example: Customer request patterns\n+    /// ```\n+    /// Time 0: Customer A requests (load adapter A)\n+    /// Time 1: Customer B requests (load adapter B)\n+    /// ...\n+    /// Time 99: Customer Z requests (load adapter Z, cache now full at 100)\n+    /// Time 100: Customer AA requests (evict least-used, load adapter AA)\n+    /// Time 101: Customer A requests again (adapter A was evicted, reload)\n+    /// ```\n+    ///\n+    /// Popular customers stay cached, inactive ones evicted automatically!\n+    /// </para>\n+    /// </remarks>\n+    public void LoadAdapter(string adapterId)\n+    {\n+        if (!_adapterPool.ContainsKey(adapterId))\n+        {\n+            throw new ArgumentException($\"Adapter '{adapterId}' not found in pool\", nameof(adapterId));\n+        }\n+\n+        var entry = _adapterPool[adapterId];\n+\n+        // If already loaded, just update access time\n+        if (entry.IsLoaded)\n+        {\n+            entry.LastAccess = ++_timestamp;\n+            return;\n+        }\n+\n+        // Evict if cache is full\n+        while (_loadedAdapters.Count >= _maxLoadedAdapters)\n+        {\n+            if (!EvictLRUAdapter())\n+            {\n+                // All loaded adapters are pinned (have active references)\n+                throw new InvalidOperationException(\n+                    $\"Cannot load adapter '{adapterId}': cache is full ({_loadedAdapters.Count}/{_maxLoadedAdapters}) \" +\n+                    \"and all loaded adapters are currently in use (pinned with active references). \" +\n+                    \"Consider increasing maxLoadedAdapters or releasing adapter references.\");\n+            }\n+        }\n+\n+        // Load adapter into cache\n+        entry.IsLoaded = true;\n+        entry.LastAccess = ++_timestamp;\n+        _loadedAdapters[adapterId] = entry;\n+    }\n+\n+    /// <summary>\n+    /// Evicts the least recently used adapter from the loaded cache.\n+    /// </summary>\n+    /// <returns>True if an adapter was evicted, false if no adapter could be evicted.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This implements S-LoRA's LRU eviction policy for memory management.\n+    /// Adapters with active references (in-flight requests) are not evicted.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This removes the least popular adapter from fast cache to make room.\n+    ///\n+    /// LRU (Least Recently Used) eviction:\n+    /// - Find adapter with oldest last access time\n+    /// - Check it's not actively being used (reference count = 0)\n+    /// - Remove from cache (but keep in pool for future reload)\n+    /// - Frees space for more popular adapters\n+    ///\n+    /// Why this works well:\n+    /// - Popular adapters get accessed frequently (stay cached)\n+    /// - Unpopular adapters get evicted (freed memory for others)\n+    /// - Temporal locality: recent requests predict future requests\n+    /// - Balance between memory usage and performance\n+    ///\n+    /// Example: E-commerce seasonal patterns\n+    /// ```\n+    /// Black Friday: Customer adapters for shoppers cached\n+    /// Normal day: Employee adapters for operations cached\n+    /// Tax season: Accounting adapters cached\n+    /// ```\n+    ///\n+    /// System automatically adapts to workload patterns!\n+    /// </para>\n+    /// </remarks>\n+    private bool EvictLRUAdapter()\n+    {\n+        if (_loadedAdapters.Count == 0)\n+        {\n+            return false;\n+        }\n+\n+        // Find LRU adapter that's not actively in use\n+        AdapterEntry? lruEntry = null;\n+        long minTimestamp = long.MaxValue;\n+\n+        foreach (var entry in _loadedAdapters.Values)\n+        {\n+            // Don't evict adapters with active references\n+            if (entry.ReferenceCount > 0)\n+            {\n+                continue;\n+            }\n+\n+            if (entry.LastAccess < minTimestamp)\n+            {\n+                minTimestamp = entry.LastAccess;\n+                lruEntry = entry;\n+            }\n+        }\n+\n+        // Evict the LRU adapter if found\n+        if (lruEntry != null)\n+        {\n+            lruEntry.IsLoaded = false;\n+            _loadedAdapters.Remove(lruEntry.Id);\n+            return true;\n+        }\n+\n+        // No adapter could be evicted (all are pinned with active references)\n+        return false;\n+    }\n+\n+    /// <summary>\n+    /// Performs batched forward pass with a specific adapter.\n+    /// </summary>\n+    /// <param name=\"input\">Input tensor.</param>\n+    /// <param name=\"adapterId\">The ID of the adapter to use (default: \"primary\").</param>\n+    /// <returns>Output tensor with adapter applied.</returns>\n+    /// <exception cref=\"ArgumentException\">Thrown when adapter ID is not found.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// This method performs S-LoRA's optimized forward pass with automatic adapter loading\n+    /// and reference tracking.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This runs inference with a specific adapter efficiently.\n+    ///\n+    /// What happens during forward pass:\n+    /// 1. Load adapter if not already cached (automatic on-demand loading)\n+    /// 2. Increment reference count (prevent eviction during processing)\n+    /// 3. Run base model forward pass\n+    /// 4. Run adapter-specific LoRA computation\n+    /// 5. Combine base output + adapter output\n+    /// 6. Decrement reference count (allow eviction if needed)\n+    ///\n+    /// Key S-LoRA optimizations simulated:\n+    /// - Separated base and adapter computation (can batch differently)\n+    /// - Automatic loading from unified pool\n+    /// - Reference counting prevents eviction during processing\n+    /// - LRU access tracking for cache management\n+    ///\n+    /// Example: Multi-customer request handling\n+    /// ```csharp\n+    /// // Request from customer A\n+    /// var outputA = slora.Forward(inputA, \"customer_a\");\n+    ///\n+    /// // Request from customer B (different adapter)\n+    /// var outputB = slora.Forward(inputB, \"customer_b\");\n+    ///\n+    /// // Request from customer A again (adapter still cached)\n+    /// var outputA2 = slora.Forward(inputA2, \"customer_a\");\n+    /// ```\n+    ///\n+    /// Each customer gets their personalized model behavior efficiently!\n+    /// </para>\n+    /// </remarks>\n+    public Tensor<T> Forward(Tensor<T> input, string adapterId = \"primary\")\n+    {\n+        if (!_adapterPool.ContainsKey(adapterId))\n+        {\n+            throw new ArgumentException($\"Adapter '{adapterId}' not found\", nameof(adapterId));\n+        }\n+\n+        // Load adapter if not already loaded\n+        LoadAdapter(adapterId);\n+\n+        var entry = _adapterPool[adapterId];\n+\n+        // Increment reference count\n+        entry.ReferenceCount++;\n+\n+        try\n+        {\n+            // Forward through base layer\n+            Tensor<T> baseOutput = _baseLayer.Forward(input);\n+\n+            // Forward through adapter-specific LoRA layer\n+            Tensor<T> loraOutput = entry.Layer.Forward(input);\n+\n+            // Combine base and adapter outputs\n+            Tensor<T> result = new Tensor<T>(baseOutput.Shape);\n+            for (int i = 0; i < baseOutput.Length; i++)\n+            {\n+                result[i] = NumOps.Add(baseOutput[i], loraOutput[i]);\n+            }\n+\n+            return result;\n+        }\n+        finally\n+        {\n+            // Decrement reference count\n+            entry.ReferenceCount--;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs batched forward pass with multiple adapters simultaneously.\n+    /// </summary>\n+    /// <param name=\"inputs\">Array of input tensors.</param>\n+    /// <param name=\"adapterIds\">Array of adapter IDs corresponding to each input.</param>\n+    /// <returns>Array of output tensors.</returns>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when inputs or adapterIds is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when array lengths don't match or adapter not found.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// This method demonstrates S-LoRA's key innovation: efficient batched computation across\n+    /// heterogeneous adapters. Adapters are clustered by rank for optimized computation.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is S-LoRA's killer feature - processing many requests efficiently!\n+    ///\n+    /// The problem with naive batching:\n+    /// - Request 1: Use customer A's adapter (rank 8)\n+    /// - Request 2: Use customer B's adapter (rank 16)\n+    /// - Request 3: Use customer C's adapter (rank 8)\n+    /// - Naive approach: Process one by one (slow) or merge adapters (memory expensive)\n+    ///\n+    /// S-LoRA's solution:\n+    /// 1. Group requests by adapter rank (rank-based clustering)\n+    /// 2. Process same-rank adapters in optimized batches\n+    /// 3. Use custom kernels for heterogeneous batching\n+    /// 4. Minimize memory overhead and maximize throughput\n+    ///\n+    /// Batching strategy:\n+    /// - Cluster 1 (rank 8): [customer A, customer C] - batch process together\n+    /// - Cluster 2 (rank 16): [customer B] - process separately\n+    /// - Base model: Shared computation for all requests\n+    ///\n+    /// Performance benefits (from paper):\n+    /// - 4x throughput vs. non-batched serving\n+    /// - 30x throughput vs. merging adapters per request\n+    /// - Near-linear scaling with more concurrent requests\n+    /// - 75-90% GPU utilization\n+    ///\n+    /// Example: Multi-tenant API serving\n+    /// ```csharp\n+    /// // Batch of 100 requests from different customers\n+    /// var inputs = new Tensor&lt;T&gt;[100];\n+    /// var adapterIds = new string[100];\n+    ///\n+    /// for (int i = 0; i &lt; 100; i++)\n+    /// {\n+    ///     inputs[i] = GetCustomerRequest(i);\n+    ///     adapterIds[i] = $\"customer_{GetCustomerId(i)}\";\n+    /// }\n+    ///\n+    /// // Process entire batch efficiently (S-LoRA magic!)\n+    /// var outputs = slora.BatchForward(inputs, adapterIds);\n+    /// ```\n+    ///\n+    /// This enables high-throughput multi-tenant AI serving!\n+    /// </para>\n+    /// </remarks>\n+    public Tensor<T>[] BatchForward(Tensor<T>[] inputs, string[] adapterIds)\n+    {\n+        if (inputs == null)\n+        {\n+            throw new ArgumentNullException(nameof(inputs));\n+        }\n+\n+        if (adapterIds == null)\n+        {\n+            throw new ArgumentNullException(nameof(adapterIds));\n+        }\n+\n+        if (inputs.Length != adapterIds.Length)\n+        {\n+            throw new ArgumentException(\"Number of inputs must match number of adapter IDs\", nameof(adapterIds));\n+        }\n+\n+        // Cluster requests by adapter for batched computation\n+        var requestClusters = new Dictionary<string, List<int>>();\n+        for (int i = 0; i < adapterIds.Length; i++)\n+        {\n+            if (!_adapterPool.ContainsKey(adapterIds[i]))\n+            {\n+                throw new ArgumentException($\"Adapter '{adapterIds[i]}' not found\", nameof(adapterIds));\n+            }\n+\n+            if (!requestClusters.ContainsKey(adapterIds[i]))\n+            {\n+                requestClusters[adapterIds[i]] = new List<int>();\n+            }\n+            requestClusters[adapterIds[i]].Add(i);\n+        }\n+\n+        // Prepare output array\n+        Tensor<T>[] outputs = new Tensor<T>[inputs.Length];\n+\n+        // Process each adapter cluster\n+        foreach (var cluster in requestClusters)\n+        {\n+            string adapterId = cluster.Key;\n+            List<int> requestIndices = cluster.Value;\n+\n+            // Load adapter once for entire cluster\n+            LoadAdapter(adapterId);\n+            var entry = _adapterPool[adapterId];\n+\n+            // Increment reference count for this batch\n+            entry.ReferenceCount += requestIndices.Count;\n+\n+            try\n+            {\n+                // Process all requests in this cluster\n+                foreach (int idx in requestIndices)\n+                {\n+                    // Forward through base layer\n+                    Tensor<T> baseOutput = _baseLayer.Forward(inputs[idx]);\n+\n+                    // Forward through adapter-specific LoRA layer\n+                    Tensor<T> loraOutput = entry.Layer.Forward(inputs[idx]);\n+\n+                    // Combine base and adapter outputs\n+                    Tensor<T> result = new Tensor<T>(baseOutput.Shape);\n+                    for (int i = 0; i < baseOutput.Length; i++)\n+                    {\n+                        result[i] = NumOps.Add(baseOutput[i], loraOutput[i]);\n+                    }\n+\n+                    outputs[idx] = result;\n+                }\n+            }\n+            finally\n+            {\n+                // Decrement reference count\n+                entry.ReferenceCount -= requestIndices.Count;\n+            }\n+        }\n+\n+        return outputs;\n+    }\n+\n+    /// <summary>\n+    /// Gets the list of adapter IDs in a specific rank cluster.\n+    /// </summary>\n+    /// <param name=\"rank\">The rank to query.</param>\n+    /// <returns>List of adapter IDs with the specified rank, or empty list if none.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method provides access to S-LoRA's rank-based clustering information.\n+    /// Adapters with the same rank can be batched together more efficiently.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This shows which adapters can be batched together efficiently.\n+    ///\n+    /// Why rank clustering matters:\n+    /// - Adapters with same rank have same computational cost\n+    /// - Can use same CUDA kernels / computation paths\n+    /// - Better memory access patterns\n+    /// - Higher GPU utilization\n+    ///\n+    /// Example: Analyzing your adapter distribution\n+    /// ```csharp\n+    /// var slora = new SLoRAAdapter&lt;double&gt;(baseModel, rank: 8);\n+    ///\n+    /// // Register many adapters with different ranks\n+    /// // ...\n+    ///\n+    /// // See how adapters are distributed\n+    /// var rank8Adapters = slora.GetRankCluster(8);   // Maybe 500 adapters\n+    /// var rank16Adapters = slora.GetRankCluster(16); // Maybe 300 adapters\n+    /// var rank32Adapters = slora.GetRankCluster(32); // Maybe 200 adapters\n+    ///\n+    /// Console.WriteLine($\"Rank 8: {rank8Adapters.Count} adapters\");\n+    /// Console.WriteLine($\"Rank 16: {rank16Adapters.Count} adapters\");\n+    /// Console.WriteLine($\"Rank 32: {rank32Adapters.Count} adapters\");\n+    /// ```\n+    ///\n+    /// This helps optimize batch sizes and resource allocation!\n+    /// </para>\n+    /// </remarks>\n+    public List<string> GetRankCluster(int rank)\n+    {\n+        if (_rankClusters.ContainsKey(rank))\n+        {\n+            return new List<string>(_rankClusters[rank]);\n+        }\n+        return new List<string>();\n+    }\n+\n+    /// <summary>\n+    /// Gets statistics about the current state of the S-LoRA system.\n+    /// </summary>\n+    /// <returns>Dictionary containing system statistics.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method provides detailed statistics about S-LoRA's memory usage, cache efficiency,\n+    /// and adapter distribution.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This gives you insights into how well your S-LoRA system is performing.\n+    ///\n+    /// Key metrics returned:\n+    /// - TotalAdapters: How many adapters registered in pool\n+    /// - LoadedAdapters: How many currently cached in \"GPU memory\"\n+    /// - CacheUtilization: Percentage of cache capacity used\n+    /// - RankClusters: Number of different rank groups\n+    /// - AverageRank: Mean rank across all adapters\n+    /// - ActiveReferences: Adapters currently processing requests\n+    ///\n+    /// Example: Monitoring production system\n+    /// ```csharp\n+    /// var stats = slora.GetStatistics();\n+    ///\n+    /// Console.WriteLine($\"Total adapters: {stats[\"TotalAdapters\"]}\");\n+    /// Console.WriteLine($\"Loaded adapters: {stats[\"LoadedAdapters\"]}\");\n+    /// Console.WriteLine($\"Cache utilization: {stats[\"CacheUtilization\"]}%\");\n+    ///\n+    /// // Alert if cache too small\n+    /// if ((double)stats[\"CacheUtilization\"] &gt; 95)\n+    /// {\n+    ///     Console.WriteLine(\"Warning: Cache nearly full, consider increasing maxLoadedAdapters\");\n+    /// }\n+    /// ```\n+    ///\n+    /// Use this to tune your S-LoRA configuration for optimal performance!\n+    /// </para>\n+    /// </remarks>\n+    public Dictionary<string, double> GetStatistics()\n+    {\n+        var stats = new Dictionary<string, double>();\n+\n+        stats[\"TotalAdapters\"] = _adapterPool.Count;\n+        stats[\"LoadedAdapters\"] = _loadedAdapters.Count;\n+        stats[\"CacheUtilization\"] = (_loadedAdapters.Count / (double)_maxLoadedAdapters) * 100.0;\n+        stats[\"RankClusters\"] = _rankClusters.Count;\n+\n+        // Calculate average rank\n+        if (_adapterPool.Count > 0)\n+        {\n+            double totalRank = _adapterPool.Values.Sum(e => e.Rank);\n+            stats[\"AverageRank\"] = totalRank / _adapterPool.Count;\n+        }\n+        else\n+        {\n+            stats[\"AverageRank\"] = 0;\n+        }\n+\n+        // Count active references\n+        int activeReferences = _loadedAdapters.Values.Sum(e => e.ReferenceCount);\n+        stats[\"ActiveReferences\"] = activeReferences;\n+\n+        return stats;\n+    }\n+\n+    /// <summary>\n+    /// Merges the primary adapter into the base layer and returns the merged layer.\n+    /// </summary>\n+    /// <returns>A new layer with primary LoRA weights merged into the base layer's weights.</returns>\n+    /// <exception cref=\"InvalidOperationException\">Thrown when the base layer type is not supported.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// For S-LoRA, this merges the primary adapter (the one created during initialization).\n+    /// In production S-LoRA deployments, individual adapters typically remain separate for\n+    /// efficient multi-adapter serving rather than being merged.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This merges the default adapter for deployment.\n+    ///\n+    /// When to merge adapters:\n+    /// - Deploying a single-adapter model (no longer need multi-adapter serving)\n+    /// - Want maximum inference speed for one specific adapter\n+    /// - Converting S-LoRA deployment back to standard model\n+    ///\n+    /// When NOT to merge:\n+    /// - Serving multiple adapters (defeats purpose of S-LoRA)\n+    /// - Need to swap adapters dynamically\n+    /// - Want memory efficiency of shared base model\n+    ///\n+    /// S-LoRA's strength is NOT merging:\n+    /// - Keep base model frozen and shared\n+    /// - Keep all adapters separate in pool\n+    /// - Swap adapters per request efficiently\n+    /// - Serve thousands of adapters from one base model\n+    ///\n+    /// This method is mainly for compatibility or transitioning away from S-LoRA architecture.\n+    /// </para>\n+    /// </remarks>\n+    public override ILayer<T> MergeToOriginalLayer()\n+    {\n+        // For S-LoRA, we merge the primary adapter\n+        // In production, adapters typically remain separate\n+        DenseLayer<T>? denseBase = _baseLayer as DenseLayer<T>;\n+        FullyConnectedLayer<T>? fcBase = _baseLayer as FullyConnectedLayer<T>;\n+\n+        if (denseBase == null && fcBase == null)\n+        {\n+            throw new InvalidOperationException(\"SLoRAAdapter currently only supports DenseLayer or FullyConnectedLayer base layers\");\n+        }\n+\n+        // Get the primary LoRA weight contribution\n+        Matrix<T> loraWeights = _loraLayer.MergeWeights();\n+\n+        // Get base layer parameters\n+        Vector<T> baseParams = _baseLayer.GetParameters();\n+\n+        // Calculate dimensions\n+        int inputSize = GetInputShape()[0];\n+        int outputSize = GetOutputShape()[0];\n+        int weightCount = inputSize * outputSize;\n+\n+        // Create new parameters with merged weights\n+        Vector<T> mergedParams = new Vector<T>(baseParams.Length);\n+\n+        // Merge weights\n+        for (int i = 0; i < weightCount; i++)\n+        {\n+            int row = i / inputSize;\n+            int col = i % inputSize;\n+            mergedParams[i] = NumOps.Add(baseParams[i], loraWeights[row, col]);\n+        }\n+\n+        // Copy biases unchanged\n+        for (int i = weightCount; i < baseParams.Length; i++)\n+        {\n+            mergedParams[i] = baseParams[i];\n+        }\n+\n+        // Create a new dense layer with merged parameters\n+        DenseLayer<T> mergedLayer = new DenseLayer<T>(inputSize, outputSize, (IActivationFunction<T>?)null);\n+        mergedLayer.SetParameters(mergedParams);\n+\n+        return mergedLayer;\n+    }","path":"src/LoRA/Adapters/SLoRAAdapter.cs","commit_id":"fa815036ee570162056ba6aca773f34c80f36948","original_commit_id":"fa815036ee570162056ba6aca773f34c80f36948","user":{"login":"coderabbitai[bot]","id":136622811,"node_id":"BOT_kgDOCCSy2w","avatar_url":"https://avatars.githubusercontent.com/in/347564?v=4","gravatar_id":"","url":"https://api.github.com/users/coderabbitai%5Bbot%5D","html_url":"https://github.com/apps/coderabbitai","followers_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/followers","following_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/repos","events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/coderabbitai%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**Incomplete merge implementation for FullyConnectedLayer.**\n\nThe method checks for `FullyConnectedLayer<T>` support (line 834) and includes it in the error message (line 838), but line 870 unconditionally creates a `DenseLayer<T>` and never uses the `fcBase` variable. When the base layer is a `FullyConnectedLayer<T>`, the merge produces the wrong layer type.\n\n\n\nApply this diff to handle both layer types correctly:\n\n```diff\n     // Create a new dense layer with merged parameters\n-    DenseLayer<T> mergedLayer = new DenseLayer<T>(inputSize, outputSize, (IActivationFunction<T>?)null);\n-    mergedLayer.SetParameters(mergedParams);\n-\n-    return mergedLayer;\n+    if (denseBase != null)\n+    {\n+        DenseLayer<T> mergedLayer = new DenseLayer<T>(inputSize, outputSize, (IActivationFunction<T>?)null);\n+        mergedLayer.SetParameters(mergedParams);\n+        return mergedLayer;\n+    }\n+    else\n+    {\n+        FullyConnectedLayer<T> mergedLayer = new FullyConnectedLayer<T>(inputSize, outputSize, (IActivationFunction<T>?)null);\n+        mergedLayer.SetParameters(mergedParams);\n+        return mergedLayer;\n+    }\n```\n\n<!-- This is an auto-generated comment by CodeRabbit -->","created_at":"2025-11-02T15:16:55Z","updated_at":"2025-11-02T15:16:56Z","html_url":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484862630","pull_request_url":"https://api.github.com/repos/ooples/AiDotNet/pulls/256","_links":{"self":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484862630"},"html":{"href":"https://github.com/ooples/AiDotNet/pull/256#discussion_r2484862630"},"pull_request":{"href":"https://api.github.com/repos/ooples/AiDotNet/pulls/256"}},"reactions":{"url":"https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2484862630/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":829,"original_start_line":829,"start_side":"RIGHT","line":874,"original_line":874,"side":"RIGHT","author_association":"CONTRIBUTOR","original_position":874,"position":874,"subject_type":"line"}]