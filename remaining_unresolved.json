[
  {
    "id": "PRRT_kwDOKSXUF85guJzy",
    "isResolved": false,
    "isOutdated": false,
    "comments": {
      "nodes": [
        {
          "path": "src/RetrievalAugmentedGeneration/ContextCompression/DocumentSummarizer.cs",
          "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Clone metadata instead of aliasing the original dictionary**\n\nAssigning `Metadata = doc.Metadata` means the summarized document shares the exact dictionary instance with the source. Any downstream mutation on the summary will silently mutate the original documentΓÇÖs metadata, which breaks expectations for non-destructive compressors.\n\n```diff\n-                var summarizedDoc = new Document<T>(doc.Id, summary)\n-                {\n-                    Metadata = doc.Metadata,\n-                    RelevanceScore = doc.RelevanceScore,\n-                    HasRelevanceScore = doc.HasRelevanceScore\n-                };\n+                var summarizedDoc = new Document<T>(doc.Id, summary)\n+                {\n+                    RelevanceScore = doc.RelevanceScore,\n+                    HasRelevanceScore = doc.HasRelevanceScore\n+                };\n+                foreach (var kvp in doc.Metadata)\n+                {\n+                    summarizedDoc.Metadata[kvp.Key] = kvp.Value;\n+                }\n@@\n-                var summarizedDoc = new Document<T>(doc.Id, summary)\n-                {\n-                    Metadata = doc.Metadata,\n-                    RelevanceScore = doc.RelevanceScore,\n-                    HasRelevanceScore = doc.HasRelevanceScore\n-                };\n+                var summarizedDoc = new Document<T>(doc.Id, summary)\n+                {\n+                    RelevanceScore = doc.RelevanceScore,\n+                    HasRelevanceScore = doc.HasRelevanceScore\n+                };\n+                foreach (var kvp in doc.Metadata)\n+                {\n+                    summarizedDoc.Metadata[kvp.Key] = kvp.Value;\n+                }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n                var summary = SummarizeText(doc.Content, queryTerms);\n                var summarizedDoc = new Document<T>(doc.Id, summary)\n                {\n                    RelevanceScore = doc.RelevanceScore,\n                    HasRelevanceScore = doc.HasRelevanceScore\n                };\n                foreach (var kvp in doc.Metadata)\n                {\n                    summarizedDoc.Metadata[kvp.Key] = kvp.Value;\n                }\n                summarized.Add(summarizedDoc);\n            }\n\n            return summarized;\n        }\n\n        /// <summary>\n        /// Summarizes a list of documents.\n        /// </summary>\n        /// <param name=\"documents\">The documents to summarize.</param>\n        /// <returns>A list of summarized documents.</returns>\n        public List<Document<T>> Summarize(List<Document<T>> documents)\n        {\n            if (documents == null) throw new ArgumentNullException(nameof(documents));\n\n            var summarized = new List<Document<T>>();\n\n            foreach (var doc in documents)\n            {\n                var summary = SummarizeText(doc.Content);\n                var summarizedDoc = new Document<T>(doc.Id, summary)\n                {\n                    RelevanceScore = doc.RelevanceScore,\n                    HasRelevanceScore = doc.HasRelevanceScore\n                };\n                foreach (var kvp in doc.Metadata)\n                {\n                    summarizedDoc.Metadata[kvp.Key] = kvp.Value;\n                }\n                summarized.Add(summarizedDoc);\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/ContextCompression/DocumentSummarizer.cs\naround lines 55 to 88, the code assigns Metadata = doc.Metadata which aliases\nthe original dictionary; instead create a copy so the summarized Document has\nits own dictionary instance. Update both places to set Metadata to a new\ndictionary constructed from doc.Metadata (handling null by leaving Metadata null\nor creating an empty dictionary as appropriate) so mutations to the summary do\nnot affect the original document.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
          "diffHunk": "@@ -0,0 +1,186 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ContextCompression\n+{\n+    /// <summary>\n+    /// Document summarizer for creating concise summaries of retrieved content.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class DocumentSummarizer<T> : ContextCompressorBase<T>\n+    {\n+        private readonly int _maxSummaryLength;\n+        private readonly string _llmEndpoint;\n+        private readonly string _apiKey;\n+\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"DocumentSummarizer{T}\"/> class.\n+        /// </summary>\n+        /// <param name=\"numericOperations\">The numeric operations for type T.</param>\n+        /// <param name=\"maxSummaryLength\">The maximum length of the summary in characters.</param>\n+        /// <param name=\"llmEndpoint\">The LLM API endpoint.</param>\n+        /// <param name=\"apiKey\">The API key for the LLM service.</param>\n+        public DocumentSummarizer(\n+            INumericOperations<T> numericOperations,\n+            int maxSummaryLength = 500,\n+            string llmEndpoint = \"\",\n+            string apiKey = \"\")\n+        {\n+            if (numericOperations == null)\n+                throw new ArgumentNullException(nameof(numericOperations));\n+                \n+            _maxSummaryLength = maxSummaryLength > 0\n+                ? maxSummaryLength\n+                : throw new ArgumentOutOfRangeException(nameof(maxSummaryLength));\n+            _llmEndpoint = llmEndpoint;\n+            _apiKey = apiKey;\n+        }\n+\n+        /// <summary>\n+        /// Compresses documents by summarizing their content with query-aware sentence selection.\n+        /// </summary>\n+        protected override List<Document<T>> CompressCore(\n+            List<Document<T>> documents,\n+            string query,\n+            Dictionary<string, object>? options = null)\n+        {\n+            var summarized = new List<Document<T>>();\n+            var queryTerms = Tokenize(query.ToLowerInvariant());\n+\n+            foreach (var doc in documents)\n+            {\n+                var summary = SummarizeText(doc.Content, queryTerms);\n+                var summarizedDoc = new Document<T>(doc.Id, summary)\n+                {\n+                    Metadata = doc.Metadata,\n+                    RelevanceScore = doc.RelevanceScore,\n+                    HasRelevanceScore = doc.HasRelevanceScore\n+                };\n+                summarized.Add(summarizedDoc);\n+            }\n+\n+            return summarized;\n+        }\n+\n+        /// <summary>\n+        /// Summarizes a list of documents.\n+        /// </summary>\n+        /// <param name=\"documents\">The documents to summarize.</param>\n+        /// <returns>A list of summarized documents.</returns>\n+        public List<Document<T>> Summarize(List<Document<T>> documents)\n+        {\n+            if (documents == null) throw new ArgumentNullException(nameof(documents));\n+\n+            var summarized = new List<Document<T>>();\n+\n+            foreach (var doc in documents)\n+            {\n+                var summary = SummarizeText(doc.Content);\n+                var summarizedDoc = new Document<T>(doc.Id, summary)\n+                {\n+                    Metadata = doc.Metadata,\n+                    RelevanceScore = doc.RelevanceScore,\n+                    HasRelevanceScore = doc.HasRelevanceScore\n+                };\n+                summarized.Add(summarizedDoc);",
          "createdAt": "2025-11-05T19:42:25Z",
          "position": 88
        }
      ]
    }
  },
  {
    "id": "PRRT_kwDOKSXUF85guJz-",
    "isResolved": false,
    "isOutdated": false,
    "comments": {
      "nodes": [
        {
          "path": "src/RetrievalAugmentedGeneration/DocumentStores/ElasticsearchDocumentStore.cs",
          "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Add robust error handling and dispose HTTP objects in GetSimilarCore; avoid .Result.**\n\nCurrently the search call ignores status codes, doesnΓÇÖt dispose response, and blocks with .Result. Wrap in try/catch, dispose HttpResponseMessage, check IsSuccessStatusCode (or EnsureSuccessStatusCode), and prefer GetAwaiter().GetResult() if you must stay sync.\n\nApply this diff:\n\n```diff\n-        var content = new StringContent(\n-            Newtonsoft.Json.JsonConvert.SerializeObject(query),\n-            Encoding.UTF8,\n-            \"application/json\");\n-\n-        var response = _httpClient.PostAsync($\"/{_indexName}/_search\", content).Result;\n-        var responseContent = response.Content.ReadAsStringAsync().Result;\n-        var result = JObject.Parse(responseContent);\n+        using var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(query),\n+            Encoding.UTF8,\n+            \"application/json\");\n+        try\n+        {\n+            using var response = _httpClient.PostAsync($\"/{_indexName}/_search\", content).GetAwaiter().GetResult();\n+            if (!response.IsSuccessStatusCode)\n+            {\n+                var err = response.Content.ReadAsStringAsync().GetAwaiter().GetResult();\n+                throw new InvalidOperationException($\"Elasticsearch search failed: {response.StatusCode}, {err}\");\n+            }\n+            var responseContent = response.Content.ReadAsStringAsync().GetAwaiter().GetResult();\n+            var result = JObject.Parse(responseContent);\n+            // existing parsing belowΓÇª\n+        }\n+        catch (HttpRequestException ex)\n+        {\n+            throw new InvalidOperationException(\"Error calling Elasticsearch _search.\", ex);\n+        }\n```\n\n\nAlso applies to: 233-261\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/ElasticsearchDocumentStore.cs\naround lines 228-236 (and similarly 233-261), the HTTP search call blocks on\n.Result, doesn't check/handle non-success status codes, and fails to dispose\nHTTP objects; wrap the call in a try/catch, use using blocks (or try/finally) to\ndispose the StringContent and HttpResponseMessage, replace .Result with\nGetAwaiter().GetResult() if you must remain synchronous (or await if converting\nto async), call response.EnsureSuccessStatusCode() or explicitly check\nresponse.IsSuccessStatusCode and throw or handle errors, and parse response\ncontent only after confirming success; log or rethrow the caught exception with\ncontext.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:chinchilla -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
          "diffHunk": "@@ -0,0 +1,541 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Net.Http;\n+using System.Text;\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using Newtonsoft.Json.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// Elasticsearch-based document store providing hybrid search capabilities (BM25 + dense vectors).\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Elasticsearch combines traditional full-text search (BM25) with vector similarity search,\n+/// making it ideal for hybrid retrieval scenarios where both keyword matching and semantic\n+/// similarity are important.\n+/// </remarks>\n+public class ElasticsearchDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly HttpClient _httpClient;\n+    private readonly string _indexName;\n+    private int _vectorDimension;\n+    private int _documentCount;\n+    private readonly Dictionary<string, VectorDocument<T>> _cache;\n+\n+    public override int DocumentCount => _documentCount;\n+    public override int VectorDimension => _vectorDimension;\n+\n+    public ElasticsearchDocumentStore(string endpoint, string indexName, string apiKey, string username, string password, int vectorDimension = 1536)\n+    {\n+        if (string.IsNullOrWhiteSpace(endpoint))\n+            throw new ArgumentException(\"Endpoint cannot be empty\", nameof(endpoint));\n+        if (string.IsNullOrWhiteSpace(indexName))\n+            throw new ArgumentException(\"Index name cannot be empty\", nameof(indexName));\n+        if (vectorDimension <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(vectorDimension), \"Vector dimension must be positive\");\n+\n+        var hasApiKey = !string.IsNullOrWhiteSpace(apiKey);\n+        var hasBasicAuth = !string.IsNullOrWhiteSpace(username) && !string.IsNullOrWhiteSpace(password);\n+        if (!hasApiKey && !hasBasicAuth)\n+            throw new ArgumentException(\"Either apiKey or both username and password must be provided for authentication\");\n+\n+        _httpClient = new HttpClient { BaseAddress = new Uri(endpoint) };\n+        \n+        if (hasApiKey)\n+            _httpClient.DefaultRequestHeaders.Add(\"Authorization\", $\"ApiKey {apiKey}\");\n+        else\n+        {\n+            var auth = Convert.ToBase64String(Encoding.UTF8.GetBytes($\"{username}:{password}\"));\n+            _httpClient.DefaultRequestHeaders.Add(\"Authorization\", $\"Basic {auth}\");\n+        }\n+\n+        _indexName = indexName.ToLowerInvariant();\n+        _vectorDimension = vectorDimension;\n+        _documentCount = 0;\n+        _cache = new Dictionary<string, VectorDocument<T>>();\n+\n+        EnsureIndex();\n+    }\n+\n+    private void EnsureIndex()\n+    {\n+        try\n+        {\n+            using var checkResponse = _httpClient.GetAsync($\"/{_indexName}\").GetAwaiter().GetResult();\n+            if (checkResponse.IsSuccessStatusCode) \n+            {\n+                UpdateDocumentCount();\n+                return;\n+            }\n+\n+            var mapping = new\n+            {\n+                mappings = new\n+                {\n+                    properties = new\n+                    {\n+                        id = new { type = \"keyword\" },\n+                        content = new { type = \"text\" },\n+                        embedding = new { type = \"dense_vector\", dims = _vectorDimension },\n+                        metadata = new { type = \"object\", enabled = true }\n+                    }\n+                }\n+            };\n+\n+            using var content = new StringContent(\n+                Newtonsoft.Json.JsonConvert.SerializeObject(mapping),\n+                Encoding.UTF8,\n+                \"application/json\");\n+\n+            using var response = _httpClient.PutAsync($\"/{_indexName}\", content).GetAwaiter().GetResult();\n+            response.EnsureSuccessStatusCode();\n+        }\n+        catch (HttpRequestException ex)\n+        {\n+            throw new InvalidOperationException(\"Failed to ensure Elasticsearch index exists\", ex);\n+        }\n+    }\n+\n+    private void UpdateDocumentCount()\n+    {\n+        using var response = _httpClient.GetAsync($\"/{_indexName}/_count\").GetAwaiter().GetResult();\n+        if (response.IsSuccessStatusCode)\n+        {\n+            var responseContent = response.Content.ReadAsStringAsync().GetAwaiter().GetResult();\n+            var result = JObject.Parse(responseContent);\n+            _documentCount = result[\"count\"]?.Value<int>() ?? 0;\n+        }\n+    }\n+\n+    protected override void AddCore(VectorDocument<T> vectorDocument)\n+    {\n+        if (vectorDocument.Embedding.Length != _vectorDimension)\n+            throw new ArgumentException($\"Document embedding dimension ({vectorDocument.Embedding.Length}) does not match the store's configured dimension ({_vectorDimension}).\");\n+\n+        var embedding = vectorDocument.Embedding.ToArray().Select(v => Convert.ToDouble(v)).ToArray();\n+        \n+        var doc = new\n+        {\n+            id = vectorDocument.Document.Id,\n+            content = vectorDocument.Document.Content,\n+            embedding,\n+            metadata = vectorDocument.Document.Metadata\n+        };\n+\n+        using var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(doc),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        using var response = _httpClient.PutAsync($\"/{_indexName}/_doc/{vectorDocument.Document.Id}\", content).GetAwaiter().GetResult();\n+        response.EnsureSuccessStatusCode();\n+        \n+        _cache[vectorDocument.Document.Id] = vectorDocument;\n+        _documentCount++;\n+    }\n+\n+    protected override void AddBatchCore(IList<VectorDocument<T>> vectorDocuments)\n+    {\n+        if (vectorDocuments.Count == 0) return;\n+\n+        foreach (var vd in vectorDocuments)\n+        {\n+            if (vd.Embedding.Length != _vectorDimension)\n+                throw new ArgumentException($\"Document embedding dimension ({vd.Embedding.Length}) does not match the store's configured dimension ({_vectorDimension}).\");\n+        }\n+\n+        var bulkBody = new StringBuilder();\n+        foreach (var vd in vectorDocuments)\n+        {\n+            var indexAction = new { index = new { _index = _indexName, _id = vd.Document.Id } };\n+            bulkBody.AppendLine(Newtonsoft.Json.JsonConvert.SerializeObject(indexAction));\n+\n+            var embedding = vd.Embedding.ToArray().Select(v => Convert.ToDouble(v)).ToArray();\n+            var doc = new\n+            {\n+                id = vd.Document.Id,\n+                content = vd.Document.Content,\n+                embedding,\n+                metadata = vd.Document.Metadata\n+            };\n+            bulkBody.AppendLine(Newtonsoft.Json.JsonConvert.SerializeObject(doc));\n+        }\n+\n+        using var content = new StringContent(bulkBody.ToString(), Encoding.UTF8, \"application/x-ndjson\");\n+        using var response = _httpClient.PostAsync($\"/{_indexName}/_bulk\", content).GetAwaiter().GetResult();\n+        response.EnsureSuccessStatusCode();\n+        \n+        foreach (var vd in vectorDocuments)\n+            _cache[vd.Document.Id] = vd;\n+        _documentCount += vectorDocuments.Count;\n+    }\n+\n+    protected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)\n+    {\n+        var embedding = queryVector.ToArray().Select(v => Convert.ToDouble(v)).ToArray();\n+        \n+        // Build the query with metadata filters\n+        object queryClause;\n+        if (metadataFilters != null && metadataFilters.Any())\n+        {\n+            var mustClauses = new List<object>();\n+            foreach (var filter in metadataFilters)\n+            {\n+                mustClauses.Add(new\n+                {\n+                    term = new Dictionary<string, object>\n+                    {\n+                        [$\"metadata.{filter.Key}\"] = filter.Value\n+                    }\n+                });\n+            }\n+            queryClause = new\n+            {\n+                @bool = new\n+                {\n+                    must = mustClauses\n+                }\n+            };\n+        }\n+        else\n+        {\n+            queryClause = new { match_all = new { } };\n+        }\n+\n+        var query = new\n+        {\n+            size = topK,\n+            query = new\n+            {\n+                script_score = new\n+                {\n+                    query = queryClause,\n+                    script = new\n+                    {\n+                        source = \"cosineSimilarity(params.query_vector, 'embedding') + 1.0\",\n+                        @params = new { query_vector = embedding }\n+                    }\n+                }\n+            }\n+        };\n+\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(query),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        var response = _httpClient.PostAsync($\"/{_indexName}/_search\", content).Result;\n+        var responseContent = response.Content.ReadAsStringAsync().Result;\n+        var result = JObject.Parse(responseContent);\n+",
          "createdAt": "2025-11-05T19:42:26Z",
          "position": 236
        }
      ]
    }
  },
  {
    "id": "PRRT_kwDOKSXUF85guJ0E",
    "isResolved": false,
    "isOutdated": false,
    "comments": {
      "nodes": [
        {
          "path": "src/RetrievalAugmentedGeneration/DocumentStores/ElasticsearchDocumentStore.cs",
          "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Return null only on 404; throw on other errors and dispose response.**\n\nCurrently all non-success paths return null, hiding server errors. Also not disposing response.\n\nApply this diff:\n\n```diff\n-        try\n-        {\n-            using var response = _httpClient.GetAsync($\"/{_indexName}/_doc/{documentId}\").GetAwaiter().GetResult();\n-            if (!response.IsSuccessStatusCode)\n-                return null;\n+        try\n+        {\n+            using var response = _httpClient.GetAsync($\"/{_indexName}/_doc/{documentId}\").GetAwaiter().GetResult();\n+            if (response.StatusCode == System.Net.HttpStatusCode.NotFound)\n+                return null;\n+            if (!response.IsSuccessStatusCode)\n+            {\n+                var err = response.Content.ReadAsStringAsync().GetAwaiter().GetResult();\n+                throw new InvalidOperationException($\"Get document failed: {response.StatusCode}, {err}\");\n+            }\n \n             var responseContent = response.Content.ReadAsStringAsync().GetAwaiter().GetResult();\n             var result = JObject.Parse(responseContent);\n             \n-            if (result[\"found\"]?.Value<bool>() != true)\n+            if (result[\"found\"]?.Value<bool>() != true)\n                 return null;\n```\n\n<!-- fingerprinting:phantom:poseidon:chinchilla -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
          "diffHunk": "@@ -0,0 +1,541 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Net.Http;\n+using System.Text;\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using Newtonsoft.Json.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// Elasticsearch-based document store providing hybrid search capabilities (BM25 + dense vectors).\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Elasticsearch combines traditional full-text search (BM25) with vector similarity search,\n+/// making it ideal for hybrid retrieval scenarios where both keyword matching and semantic\n+/// similarity are important.\n+/// </remarks>\n+public class ElasticsearchDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly HttpClient _httpClient;\n+    private readonly string _indexName;\n+    private int _vectorDimension;\n+    private int _documentCount;\n+    private readonly Dictionary<string, VectorDocument<T>> _cache;\n+\n+    public override int DocumentCount => _documentCount;\n+    public override int VectorDimension => _vectorDimension;\n+\n+    public ElasticsearchDocumentStore(string endpoint, string indexName, string apiKey, string username, string password, int vectorDimension = 1536)\n+    {\n+        if (string.IsNullOrWhiteSpace(endpoint))\n+            throw new ArgumentException(\"Endpoint cannot be empty\", nameof(endpoint));\n+        if (string.IsNullOrWhiteSpace(indexName))\n+            throw new ArgumentException(\"Index name cannot be empty\", nameof(indexName));\n+        if (vectorDimension <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(vectorDimension), \"Vector dimension must be positive\");\n+\n+        var hasApiKey = !string.IsNullOrWhiteSpace(apiKey);\n+        var hasBasicAuth = !string.IsNullOrWhiteSpace(username) && !string.IsNullOrWhiteSpace(password);\n+        if (!hasApiKey && !hasBasicAuth)\n+            throw new ArgumentException(\"Either apiKey or both username and password must be provided for authentication\");\n+\n+        _httpClient = new HttpClient { BaseAddress = new Uri(endpoint) };\n+        \n+        if (hasApiKey)\n+            _httpClient.DefaultRequestHeaders.Add(\"Authorization\", $\"ApiKey {apiKey}\");\n+        else\n+        {\n+            var auth = Convert.ToBase64String(Encoding.UTF8.GetBytes($\"{username}:{password}\"));\n+            _httpClient.DefaultRequestHeaders.Add(\"Authorization\", $\"Basic {auth}\");\n+        }\n+\n+        _indexName = indexName.ToLowerInvariant();\n+        _vectorDimension = vectorDimension;\n+        _documentCount = 0;\n+        _cache = new Dictionary<string, VectorDocument<T>>();\n+\n+        EnsureIndex();\n+    }\n+\n+    private void EnsureIndex()\n+    {\n+        try\n+        {\n+            using var checkResponse = _httpClient.GetAsync($\"/{_indexName}\").GetAwaiter().GetResult();\n+            if (checkResponse.IsSuccessStatusCode) \n+            {\n+                UpdateDocumentCount();\n+                return;\n+            }\n+\n+            var mapping = new\n+            {\n+                mappings = new\n+                {\n+                    properties = new\n+                    {\n+                        id = new { type = \"keyword\" },\n+                        content = new { type = \"text\" },\n+                        embedding = new { type = \"dense_vector\", dims = _vectorDimension },\n+                        metadata = new { type = \"object\", enabled = true }\n+                    }\n+                }\n+            };\n+\n+            using var content = new StringContent(\n+                Newtonsoft.Json.JsonConvert.SerializeObject(mapping),\n+                Encoding.UTF8,\n+                \"application/json\");\n+\n+            using var response = _httpClient.PutAsync($\"/{_indexName}\", content).GetAwaiter().GetResult();\n+            response.EnsureSuccessStatusCode();\n+        }\n+        catch (HttpRequestException ex)\n+        {\n+            throw new InvalidOperationException(\"Failed to ensure Elasticsearch index exists\", ex);\n+        }\n+    }\n+\n+    private void UpdateDocumentCount()\n+    {\n+        using var response = _httpClient.GetAsync($\"/{_indexName}/_count\").GetAwaiter().GetResult();\n+        if (response.IsSuccessStatusCode)\n+        {\n+            var responseContent = response.Content.ReadAsStringAsync().GetAwaiter().GetResult();\n+            var result = JObject.Parse(responseContent);\n+            _documentCount = result[\"count\"]?.Value<int>() ?? 0;\n+        }\n+    }\n+\n+    protected override void AddCore(VectorDocument<T> vectorDocument)\n+    {\n+        if (vectorDocument.Embedding.Length != _vectorDimension)\n+            throw new ArgumentException($\"Document embedding dimension ({vectorDocument.Embedding.Length}) does not match the store's configured dimension ({_vectorDimension}).\");\n+\n+        var embedding = vectorDocument.Embedding.ToArray().Select(v => Convert.ToDouble(v)).ToArray();\n+        \n+        var doc = new\n+        {\n+            id = vectorDocument.Document.Id,\n+            content = vectorDocument.Document.Content,\n+            embedding,\n+            metadata = vectorDocument.Document.Metadata\n+        };\n+\n+        using var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(doc),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        using var response = _httpClient.PutAsync($\"/{_indexName}/_doc/{vectorDocument.Document.Id}\", content).GetAwaiter().GetResult();\n+        response.EnsureSuccessStatusCode();\n+        \n+        _cache[vectorDocument.Document.Id] = vectorDocument;\n+        _documentCount++;\n+    }\n+\n+    protected override void AddBatchCore(IList<VectorDocument<T>> vectorDocuments)\n+    {\n+        if (vectorDocuments.Count == 0) return;\n+\n+        foreach (var vd in vectorDocuments)\n+        {\n+            if (vd.Embedding.Length != _vectorDimension)\n+                throw new ArgumentException($\"Document embedding dimension ({vd.Embedding.Length}) does not match the store's configured dimension ({_vectorDimension}).\");\n+        }\n+\n+        var bulkBody = new StringBuilder();\n+        foreach (var vd in vectorDocuments)\n+        {\n+            var indexAction = new { index = new { _index = _indexName, _id = vd.Document.Id } };\n+            bulkBody.AppendLine(Newtonsoft.Json.JsonConvert.SerializeObject(indexAction));\n+\n+            var embedding = vd.Embedding.ToArray().Select(v => Convert.ToDouble(v)).ToArray();\n+            var doc = new\n+            {\n+                id = vd.Document.Id,\n+                content = vd.Document.Content,\n+                embedding,\n+                metadata = vd.Document.Metadata\n+            };\n+            bulkBody.AppendLine(Newtonsoft.Json.JsonConvert.SerializeObject(doc));\n+        }\n+\n+        using var content = new StringContent(bulkBody.ToString(), Encoding.UTF8, \"application/x-ndjson\");\n+        using var response = _httpClient.PostAsync($\"/{_indexName}/_bulk\", content).GetAwaiter().GetResult();\n+        response.EnsureSuccessStatusCode();\n+        \n+        foreach (var vd in vectorDocuments)\n+            _cache[vd.Document.Id] = vd;\n+        _documentCount += vectorDocuments.Count;\n+    }\n+\n+    protected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)\n+    {\n+        var embedding = queryVector.ToArray().Select(v => Convert.ToDouble(v)).ToArray();\n+        \n+        // Build the query with metadata filters\n+        object queryClause;\n+        if (metadataFilters != null && metadataFilters.Any())\n+        {\n+            var mustClauses = new List<object>();\n+            foreach (var filter in metadataFilters)\n+            {\n+                mustClauses.Add(new\n+                {\n+                    term = new Dictionary<string, object>\n+                    {\n+                        [$\"metadata.{filter.Key}\"] = filter.Value\n+                    }\n+                });\n+            }\n+            queryClause = new\n+            {\n+                @bool = new\n+                {\n+                    must = mustClauses\n+                }\n+            };\n+        }\n+        else\n+        {\n+            queryClause = new { match_all = new { } };\n+        }\n+\n+        var query = new\n+        {\n+            size = topK,\n+            query = new\n+            {\n+                script_score = new\n+                {\n+                    query = queryClause,\n+                    script = new\n+                    {\n+                        source = \"cosineSimilarity(params.query_vector, 'embedding') + 1.0\",\n+                        @params = new { query_vector = embedding }\n+                    }\n+                }\n+            }\n+        };\n+\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(query),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        var response = _httpClient.PostAsync($\"/{_indexName}/_search\", content).Result;\n+        var responseContent = response.Content.ReadAsStringAsync().Result;\n+        var result = JObject.Parse(responseContent);\n+\n+        var results = new List<Document<T>>();\n+        var hits = result[\"hits\"]?[\"hits\"];\n+        if (hits == null) return results;\n+\n+        foreach (var hit in hits)\n+        {\n+            var source = hit[\"_source\"];\n+            if (source == null) continue;\n+\n+            var id = source[\"id\"]?.ToString() ?? string.Empty;\n+            var docContent = source[\"content\"]?.ToString() ?? string.Empty;\n+            var metadataObj = source[\"metadata\"]?.ToObject<Dictionary<string, object>>() ?? new Dictionary<string, object>();\n+\n+            var score = Convert.ToDouble(hit[\"_score\"]);\n+\n+            var document = new Document<T>(id, docContent, metadataObj)\n+            {\n+                RelevanceScore = NumOps.FromDouble(score),\n+                HasRelevanceScore = true\n+            };\n+\n+            results.Add(document);\n+        }\n+\n+        return results;\n+    }\n+\n+    protected override Document<T>? GetByIdCore(string documentId)\n+    {\n+        if (_cache.TryGetValue(documentId, out var vectorDoc))\n+            return vectorDoc.Document;\n+\n+        try\n+        {\n+            using var response = _httpClient.GetAsync($\"/{_indexName}/_doc/{documentId}\").GetAwaiter().GetResult();\n+            if (!response.IsSuccessStatusCode)\n+                return null;\n+\n+            var responseContent = response.Content.ReadAsStringAsync().GetAwaiter().GetResult();\n+            var result = JObject.Parse(responseContent);\n+            \n+            if (result[\"found\"]?.Value<bool>() != true)\n+                return null;\n+\n+            var source = result[\"_source\"];\n+            var id = source?[\"id\"]?.ToString();\n+            var content = source?[\"content\"]?.ToString();\n+            var metadataObj = source?[\"metadata\"]?.ToObject<Dictionary<string, object>>() ?? new Dictionary<string, object>();\n+\n+            if (id == null || content == null)\n+                return null;\n+\n+            return new Document<T>(id, content, metadataObj);\n+        }\n+        catch (HttpRequestException)\n+        {\n+            return null;\n+        }\n+    }",
          "createdAt": "2025-11-05T19:42:26Z",
          "position": 295
        }
      ]
    }
  },
  {
    "id": "PRRT_kwDOKSXUF85guJ0H",
    "isResolved": false,
    "isOutdated": false,
    "comments": {
      "nodes": [
        {
          "path": "src/RetrievalAugmentedGeneration/DocumentStores/ElasticsearchDocumentStore.cs",
          "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Fix scroll cleanup: current DELETE sends no body; content is set after the call.**\n\nElasticsearch expects scroll_id in DELETE body (or DELETE /_search/scroll/{id}). Use HttpRequestMessage with content.\n\nApply this diff:\n\n```diff\n-        // Clean up scroll context\n+        // Clean up scroll context\n         try\n         {\n-            var deleteScrollRequest = new { scroll_id = new[] { scrollId } };\n-            _httpClient.DeleteAsync(\"_search/scroll\")\n-                .ContinueWith(t =>\n-                {\n-                    if (t.Result.IsSuccessStatusCode)\n-                    {\n-                        t.Result.Content = new StringContent(\n-                            Newtonsoft.Json.JsonConvert.SerializeObject(deleteScrollRequest),\n-                            Encoding.UTF8,\n-                            \"application/json\"\n-                        );\n-                    }\n-                });\n+            if (!string.IsNullOrEmpty(scrollId))\n+            {\n+                var payload = new { scroll_id = new[] { scrollId } };\n+                var req = new HttpRequestMessage(HttpMethod.Delete, \"/_search/scroll\")\n+                {\n+                    Content = new StringContent(\n+                        Newtonsoft.Json.JsonConvert.SerializeObject(payload),\n+                        Encoding.UTF8,\n+                        \"application/json\")\n+                };\n+                using var resp = _httpClient.Send(req);\n+                // ignore non-success; context will expire\n+            }\n         }\n         catch\n         {\n             // Scroll context will expire automatically\n         }\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/ElasticsearchDocumentStore.cs\naround lines 445 to 461, the current DELETE call sends no body because content\nis assigned after calling DeleteAsync; replace this with constructing an\nHttpRequestMessage (HttpMethod.Delete) whose Content is set to a JSON body\ncontaining { \"scroll_id\": [ scrollId ] } (or call DELETE /_search/scroll/{id})\nbefore sending, then send it with _httpClient.SendAsync and await the result,\nhandling success/failure and disposing the request/response appropriately.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:chinchilla -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
          "diffHunk": "@@ -0,0 +1,541 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Net.Http;\n+using System.Text;\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using Newtonsoft.Json.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// Elasticsearch-based document store providing hybrid search capabilities (BM25 + dense vectors).\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Elasticsearch combines traditional full-text search (BM25) with vector similarity search,\n+/// making it ideal for hybrid retrieval scenarios where both keyword matching and semantic\n+/// similarity are important.\n+/// </remarks>\n+public class ElasticsearchDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly HttpClient _httpClient;\n+    private readonly string _indexName;\n+    private int _vectorDimension;\n+    private int _documentCount;\n+    private readonly Dictionary<string, VectorDocument<T>> _cache;\n+\n+    public override int DocumentCount => _documentCount;\n+    public override int VectorDimension => _vectorDimension;\n+\n+    public ElasticsearchDocumentStore(string endpoint, string indexName, string apiKey, string username, string password, int vectorDimension = 1536)\n+    {\n+        if (string.IsNullOrWhiteSpace(endpoint))\n+            throw new ArgumentException(\"Endpoint cannot be empty\", nameof(endpoint));\n+        if (string.IsNullOrWhiteSpace(indexName))\n+            throw new ArgumentException(\"Index name cannot be empty\", nameof(indexName));\n+        if (vectorDimension <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(vectorDimension), \"Vector dimension must be positive\");\n+\n+        var hasApiKey = !string.IsNullOrWhiteSpace(apiKey);\n+        var hasBasicAuth = !string.IsNullOrWhiteSpace(username) && !string.IsNullOrWhiteSpace(password);\n+        if (!hasApiKey && !hasBasicAuth)\n+            throw new ArgumentException(\"Either apiKey or both username and password must be provided for authentication\");\n+\n+        _httpClient = new HttpClient { BaseAddress = new Uri(endpoint) };\n+        \n+        if (hasApiKey)\n+            _httpClient.DefaultRequestHeaders.Add(\"Authorization\", $\"ApiKey {apiKey}\");\n+        else\n+        {\n+            var auth = Convert.ToBase64String(Encoding.UTF8.GetBytes($\"{username}:{password}\"));\n+            _httpClient.DefaultRequestHeaders.Add(\"Authorization\", $\"Basic {auth}\");\n+        }\n+\n+        _indexName = indexName.ToLowerInvariant();\n+        _vectorDimension = vectorDimension;\n+        _documentCount = 0;\n+        _cache = new Dictionary<string, VectorDocument<T>>();\n+\n+        EnsureIndex();\n+    }\n+\n+    private void EnsureIndex()\n+    {\n+        try\n+        {\n+            using var checkResponse = _httpClient.GetAsync($\"/{_indexName}\").GetAwaiter().GetResult();\n+            if (checkResponse.IsSuccessStatusCode) \n+            {\n+                UpdateDocumentCount();\n+                return;\n+            }\n+\n+            var mapping = new\n+            {\n+                mappings = new\n+                {\n+                    properties = new\n+                    {\n+                        id = new { type = \"keyword\" },\n+                        content = new { type = \"text\" },\n+                        embedding = new { type = \"dense_vector\", dims = _vectorDimension },\n+                        metadata = new { type = \"object\", enabled = true }\n+                    }\n+                }\n+            };\n+\n+            using var content = new StringContent(\n+                Newtonsoft.Json.JsonConvert.SerializeObject(mapping),\n+                Encoding.UTF8,\n+                \"application/json\");\n+\n+            using var response = _httpClient.PutAsync($\"/{_indexName}\", content).GetAwaiter().GetResult();\n+            response.EnsureSuccessStatusCode();\n+        }\n+        catch (HttpRequestException ex)\n+        {\n+            throw new InvalidOperationException(\"Failed to ensure Elasticsearch index exists\", ex);\n+        }\n+    }\n+\n+    private void UpdateDocumentCount()\n+    {\n+        using var response = _httpClient.GetAsync($\"/{_indexName}/_count\").GetAwaiter().GetResult();\n+        if (response.IsSuccessStatusCode)\n+        {\n+            var responseContent = response.Content.ReadAsStringAsync().GetAwaiter().GetResult();\n+            var result = JObject.Parse(responseContent);\n+            _documentCount = result[\"count\"]?.Value<int>() ?? 0;\n+        }\n+    }\n+\n+    protected override void AddCore(VectorDocument<T> vectorDocument)\n+    {\n+        if (vectorDocument.Embedding.Length != _vectorDimension)\n+            throw new ArgumentException($\"Document embedding dimension ({vectorDocument.Embedding.Length}) does not match the store's configured dimension ({_vectorDimension}).\");\n+\n+        var embedding = vectorDocument.Embedding.ToArray().Select(v => Convert.ToDouble(v)).ToArray();\n+        \n+        var doc = new\n+        {\n+            id = vectorDocument.Document.Id,\n+            content = vectorDocument.Document.Content,\n+            embedding,\n+            metadata = vectorDocument.Document.Metadata\n+        };\n+\n+        using var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(doc),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        using var response = _httpClient.PutAsync($\"/{_indexName}/_doc/{vectorDocument.Document.Id}\", content).GetAwaiter().GetResult();\n+        response.EnsureSuccessStatusCode();\n+        \n+        _cache[vectorDocument.Document.Id] = vectorDocument;\n+        _documentCount++;\n+    }\n+\n+    protected override void AddBatchCore(IList<VectorDocument<T>> vectorDocuments)\n+    {\n+        if (vectorDocuments.Count == 0) return;\n+\n+        foreach (var vd in vectorDocuments)\n+        {\n+            if (vd.Embedding.Length != _vectorDimension)\n+                throw new ArgumentException($\"Document embedding dimension ({vd.Embedding.Length}) does not match the store's configured dimension ({_vectorDimension}).\");\n+        }\n+\n+        var bulkBody = new StringBuilder();\n+        foreach (var vd in vectorDocuments)\n+        {\n+            var indexAction = new { index = new { _index = _indexName, _id = vd.Document.Id } };\n+            bulkBody.AppendLine(Newtonsoft.Json.JsonConvert.SerializeObject(indexAction));\n+\n+            var embedding = vd.Embedding.ToArray().Select(v => Convert.ToDouble(v)).ToArray();\n+            var doc = new\n+            {\n+                id = vd.Document.Id,\n+                content = vd.Document.Content,\n+                embedding,\n+                metadata = vd.Document.Metadata\n+            };\n+            bulkBody.AppendLine(Newtonsoft.Json.JsonConvert.SerializeObject(doc));\n+        }\n+\n+        using var content = new StringContent(bulkBody.ToString(), Encoding.UTF8, \"application/x-ndjson\");\n+        using var response = _httpClient.PostAsync($\"/{_indexName}/_bulk\", content).GetAwaiter().GetResult();\n+        response.EnsureSuccessStatusCode();\n+        \n+        foreach (var vd in vectorDocuments)\n+            _cache[vd.Document.Id] = vd;\n+        _documentCount += vectorDocuments.Count;\n+    }\n+\n+    protected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)\n+    {\n+        var embedding = queryVector.ToArray().Select(v => Convert.ToDouble(v)).ToArray();\n+        \n+        // Build the query with metadata filters\n+        object queryClause;\n+        if (metadataFilters != null && metadataFilters.Any())\n+        {\n+            var mustClauses = new List<object>();\n+            foreach (var filter in metadataFilters)\n+            {\n+                mustClauses.Add(new\n+                {\n+                    term = new Dictionary<string, object>\n+                    {\n+                        [$\"metadata.{filter.Key}\"] = filter.Value\n+                    }\n+                });\n+            }\n+            queryClause = new\n+            {\n+                @bool = new\n+                {\n+                    must = mustClauses\n+                }\n+            };\n+        }\n+        else\n+        {\n+            queryClause = new { match_all = new { } };\n+        }\n+\n+        var query = new\n+        {\n+            size = topK,\n+            query = new\n+            {\n+                script_score = new\n+                {\n+                    query = queryClause,\n+                    script = new\n+                    {\n+                        source = \"cosineSimilarity(params.query_vector, 'embedding') + 1.0\",\n+                        @params = new { query_vector = embedding }\n+                    }\n+                }\n+            }\n+        };\n+\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(query),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        var response = _httpClient.PostAsync($\"/{_indexName}/_search\", content).Result;\n+        var responseContent = response.Content.ReadAsStringAsync().Result;\n+        var result = JObject.Parse(responseContent);\n+\n+        var results = new List<Document<T>>();\n+        var hits = result[\"hits\"]?[\"hits\"];\n+        if (hits == null) return results;\n+\n+        foreach (var hit in hits)\n+        {\n+            var source = hit[\"_source\"];\n+            if (source == null) continue;\n+\n+            var id = source[\"id\"]?.ToString() ?? string.Empty;\n+            var docContent = source[\"content\"]?.ToString() ?? string.Empty;\n+            var metadataObj = source[\"metadata\"]?.ToObject<Dictionary<string, object>>() ?? new Dictionary<string, object>();\n+\n+            var score = Convert.ToDouble(hit[\"_score\"]);\n+\n+            var document = new Document<T>(id, docContent, metadataObj)\n+            {\n+                RelevanceScore = NumOps.FromDouble(score),\n+                HasRelevanceScore = true\n+            };\n+\n+            results.Add(document);\n+        }\n+\n+        return results;\n+    }\n+\n+    protected override Document<T>? GetByIdCore(string documentId)\n+    {\n+        if (_cache.TryGetValue(documentId, out var vectorDoc))\n+            return vectorDoc.Document;\n+\n+        try\n+        {\n+            using var response = _httpClient.GetAsync($\"/{_indexName}/_doc/{documentId}\").GetAwaiter().GetResult();\n+            if (!response.IsSuccessStatusCode)\n+                return null;\n+\n+            var responseContent = response.Content.ReadAsStringAsync().GetAwaiter().GetResult();\n+            var result = JObject.Parse(responseContent);\n+            \n+            if (result[\"found\"]?.Value<bool>() != true)\n+                return null;\n+\n+            var source = result[\"_source\"];\n+            var id = source?[\"id\"]?.ToString();\n+            var content = source?[\"content\"]?.ToString();\n+            var metadataObj = source?[\"metadata\"]?.ToObject<Dictionary<string, object>>() ?? new Dictionary<string, object>();\n+\n+            if (id == null || content == null)\n+                return null;\n+\n+            return new Document<T>(id, content, metadataObj);\n+        }\n+        catch (HttpRequestException)\n+        {\n+            return null;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Core logic for removing a document from the Elasticsearch index.\n+    /// </summary>\n+    /// <param name=\"documentId\">The validated document ID.</param>\n+    /// <returns>True if the document was found and removed; otherwise, false.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Removes the document from both the cache and the Elasticsearch index via DELETE API.\n+    /// If successful, decrements the document count.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Deletes a document from Elasticsearch.\n+    /// \n+    /// In Elasticsearch terms, this is like:\n+    /// <code>\n+    /// DELETE /index/_doc/document_id\n+    /// </code>\n+    /// \n+    /// Example:\n+    /// <code>\n+    /// if (store.Remove(\"doc-123\"))\n+    ///     Console.WriteLine(\"Document removed from Elasticsearch\");\n+    /// </code>\n+    /// </para>\n+    /// </remarks>\n+    protected override bool RemoveCore(string documentId)\n+    {\n+        using var response = _httpClient.DeleteAsync($\"/{_indexName}/_doc/{documentId}\").GetAwaiter().GetResult();\n+        if (response.IsSuccessStatusCode && _documentCount > 0)\n+        {\n+            _cache.Remove(documentId);\n+            _documentCount--;\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+    /// <summary>\n+    /// Core logic for retrieving all documents from the Elasticsearch index.\n+    /// </summary>\n+    /// <returns>An enumerable of all documents without their vector embeddings.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Uses the Elasticsearch scroll API to efficiently retrieve all documents from the index\n+    /// in batches, avoiding memory issues with large result sets. The scroll API maintains a\n+    /// search context and streams results in manageable batches.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Retrieves all documents using Elasticsearch's scroll API.\n+    ///\n+    /// The scroll API is the recommended way to retrieve large numbers of documents because:\n+    /// - Processes results in batches (default: 1000 documents per batch)\n+    /// - Maintains a search context between requests\n+    /// - Much more memory-efficient than large \"from/size\" pagination\n+    /// - Automatically expires after timeout (default: 1 minute)\n+    ///\n+    /// How it works:\n+    /// 1. Initial scroll request creates search context, returns first batch\n+    /// 2. Subsequent requests using scroll_id return next batches\n+    /// 3. Continues until all documents retrieved\n+    /// 4. Context automatically cleaned up after timeout\n+    ///\n+    /// Example:\n+    /// <code>\n+    /// // Efficiently retrieve all documents\n+    /// var allDocs = store.GetAll().ToList();\n+    /// Console.WriteLine($\"Total documents: {allDocs.Count}\");\n+    ///\n+    /// // Export to JSON\n+    /// var json = JsonConvert.SerializeObject(allDocs);\n+    /// File.WriteAllText(\"elasticsearch_export.json\", json);\n+    /// </code>\n+    /// </para>\n+    /// </remarks>\n+    protected override IEnumerable<Document<T>> GetAllCore()\n+    {\n+        const string scrollTimeout = \"1m\";\n+        const int batchSize = 1000;\n+\n+        // Initial scroll request\n+        var searchRequest = new\n+        {\n+            size = batchSize,\n+            query = new { match_all = new { } }\n+        };\n+\n+        var searchResponse = _httpClient.PostAsync(\n+            $\"{_indexName}/_search?scroll={scrollTimeout}\",\n+            new StringContent(\n+                Newtonsoft.Json.JsonConvert.SerializeObject(searchRequest),\n+                Encoding.UTF8,\n+                \"application/json\"\n+            )\n+        ).Result;\n+\n+        if (!searchResponse.IsSuccessStatusCode)\n+        {\n+            var error = searchResponse.Content.ReadAsStringAsync().Result;\n+            throw new InvalidOperationException($\"Elasticsearch scroll initialization failed: {error}\");\n+        }\n+\n+        var initialResult = JObject.Parse(searchResponse.Content.ReadAsStringAsync().Result);\n+        var scrollId = initialResult[\"_scroll_id\"]?.ToString();\n+\n+        if (string.IsNullOrEmpty(scrollId))\n+            yield break;\n+\n+        // Process initial batch\n+        var hits = initialResult[\"hits\"]?[\"hits\"] as JArray;\n+        if (hits != null)\n+        {\n+            foreach (var hit in hits)\n+            {\n+                var doc = ParseElasticsearchHit(hit as JObject);\n+                if (doc != null)\n+                    yield return doc;\n+            }\n+        }\n+\n+        // Continue scrolling until no more results\n+        while (true)\n+        {\n+            var scrollRequest = new { scroll = scrollTimeout, scroll_id = scrollId };\n+\n+            var scrollResponse = _httpClient.PostAsync(\n+                \"_search/scroll\",\n+                new StringContent(\n+                    Newtonsoft.Json.JsonConvert.SerializeObject(scrollRequest),\n+                    Encoding.UTF8,\n+                    \"application/json\"\n+                )\n+            ).Result;\n+\n+            if (!scrollResponse.IsSuccessStatusCode)\n+                break;\n+\n+            var scrollResult = JObject.Parse(scrollResponse.Content.ReadAsStringAsync().Result);\n+            hits = scrollResult[\"hits\"]?[\"hits\"] as JArray;\n+\n+            if (hits == null || hits.Count == 0)\n+                break;\n+\n+            foreach (var hit in hits)\n+            {\n+                var doc = ParseElasticsearchHit(hit as JObject);\n+                if (doc != null)\n+                    yield return doc;\n+            }\n+        }\n+\n+        // Clean up scroll context\n+        try\n+        {\n+            var deleteScrollRequest = new { scroll_id = new[] { scrollId } };\n+            _httpClient.DeleteAsync(\"_search/scroll\")\n+                .ContinueWith(t =>\n+                {\n+                    if (t.Result.IsSuccessStatusCode)\n+                    {\n+                        t.Result.Content = new StringContent(\n+                            Newtonsoft.Json.JsonConvert.SerializeObject(deleteScrollRequest),\n+                            Encoding.UTF8,\n+                            \"application/json\"\n+                        );\n+                    }\n+                });\n+        }",
          "createdAt": "2025-11-05T19:42:26Z",
          "position": 461
        }
      ]
    }
  },
  {
    "id": "PRRT_kwDOKSXUF85guJ0M",
    "isResolved": false,
    "isOutdated": false,
    "comments": {
      "nodes": [
        {
          "path": "src/RetrievalAugmentedGeneration/Retrievers/ParentDocumentRetriever.cs",
          "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Guard against null `parent_id` metadata.**\n\n`chunk.Metadata[\"parent_id\"].ToString()` will throw when the key exists but the value is null (a common ingestion default). Pattern-match the value to a non-empty string before using it and fall back to `chunk.Id` only when you actually have a usable string.\n\nApply this diff:\n\n```diff\n-            var parentId = chunk.Metadata.ContainsKey(\"parent_id\") \n-                ? chunk.Metadata[\"parent_id\"].ToString() \n-                : chunk.Id;\n-\n-            if (parentId == null) continue;\n+            if (!chunk.Metadata.TryGetValue(\"parent_id\", out var parentIdObj) ||\n+                parentIdObj is not string parentId ||\n+                string.IsNullOrWhiteSpace(parentId))\n+            {\n+                parentId = chunk.Id;\n+            }\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Retrievers/ParentDocumentRetriever.cs around\nlines 210-217, avoid calling ToString() on chunk.Metadata[\"parent_id\"] which\nthrows if the value is null; instead use TryGetValue (or pattern-match) to\nretrieve the object, check it is a non-empty/non-whitespace string (e.g. value\nis string s && !string.IsNullOrWhiteSpace(s)) and assign parentId = s; otherwise\nset parentId = chunk.Id; keep or adjust the subsequent null/empty guard\naccordingly so you only continue when no usable id is available.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
          "diffHunk": "@@ -0,0 +1,270 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers;\n+\n+/// <summary>\n+/// Retrieves precise small chunks for matching but returns complete parent documents for comprehensive context.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for relevance scoring (typically float or double).</typeparam>\n+/// <remarks>\n+/// <para>\n+/// ParentDocumentRetriever solves the \"chunk size dilemma\" in RAG systems: small chunks enable precise matching\n+/// but lack context, while large chunks provide context but reduce precision. This retriever uses a two-tier\n+/// approachΓÇösearch against small chunks (e.g., paragraphs) for accuracy, then return their larger parent documents\n+/// (e.g., full sections or pages) for complete context. This ensures the LLM receives sufficient information to\n+/// generate accurate answers while maintaining high retrieval precision. The retriever can optionally include\n+/// neighboring chunks to expand context boundaries. This pattern is particularly effective for structured content\n+/// (technical docs, research papers, legal documents) where individual paragraphs are meaningful but answers require\n+/// broader context.\n+/// </para>\n+/// <para><b>For Beginners:</b> Think of this like a two-step library search:\n+/// \n+/// The Problem:\n+/// - Small chunks (paragraphs): Easy to match precisely, BUT not enough context\n+/// - Large chunks (whole pages): Plenty of context, BUT hard to match precisely\n+/// \n+/// The Solution - Parent Document Retrieval:\n+/// 1. Search using SMALL chunks for precision\n+/// 2. Return the LARGE parent document for context\n+/// \n+/// Real-world example:\n+/// Query: \"How does photosynthesis work?\"\n+/// \n+/// Small chunk matches: \"...chlorophyll absorbs light energy...\" (paragraph 3)\n+/// Returns: Full page containing introduction + detailed process + diagram\n+/// \n+/// ```csharp\n+/// var retriever = new ParentDocumentRetriever<double>(\n+///     documentStore,\n+///     chunkSize: 256,                    // Small chunks for precision\n+///     parentSize: 2048,                  // Large parents for context\n+///     includeNeighboringChunks: true     // Add nearby chunks too\n+/// );\n+/// \n+/// var results = retriever.Retrieve(\"explain quantum entanglement\", topK: 3);\n+/// // Finds precise paragraphs but returns full sections with complete explanation\n+/// ```\n+/// \n+/// Why use ParentDocumentRetriever:\n+/// - Best of both worlds: precise matching + complete context\n+/// - Ideal for technical documentation and research papers\n+/// - Reduces LLM hallucinations (more context = better answers)\n+/// - Works great with structured content (headings, sections, chapters)\n+/// \n+/// When NOT to use it:\n+/// - Very short documents (chunks = parents already)\n+/// - Documents with redundant content (wastes context window)\n+/// - When you need ONLY the matching excerpt (use regular retrieval)\n+/// - Memory-constrained systems (returns more content per match)\n+/// </para>\n+/// </remarks>\n+public class ParentDocumentRetriever<T> : RetrieverBase<T>\n+{\n+    private readonly IDocumentStore<T> _documentStore;\n+    private readonly IEmbeddingModel<T> _embeddingModel;\n+    private readonly int _chunkSize;\n+    private readonly int _parentSize;\n+    private readonly bool _includeNeighboringChunks;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"ParentDocumentRetriever{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"documentStore\">The document store containing chunked documents with parent metadata.</param>\n+    /// <param name=\"embeddingModel\">The embedding model used to convert text queries into vector embeddings.</param>\n+    /// <param name=\"chunkSize\">Character length of child chunks used for matching (typically 128-512 characters).</param>\n+    /// <param name=\"parentSize\">Character length of parent documents returned (typically 1024-4096 characters).</param>\n+    /// <param name=\"includeNeighboringChunks\">Whether to include adjacent chunks around the matched chunk (expands context boundaries).</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when documentStore or embeddingModel is null.</exception>\n+    /// <exception cref=\"ArgumentOutOfRangeException\">Thrown when chunkSize or parentSize is less than or equal to zero.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when parentSize is less than chunkSize.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This configures how the retriever balances precision vs. context.\n+    /// \n+    /// Recommended configurations:\n+    /// \n+    /// Technical Documentation:\n+    /// - chunkSize: 256 (1-2 paragraphs)\n+    /// - parentSize: 2048 (full section)\n+    /// - includeNeighboringChunks: true (add context before/after)\n+    /// \n+    /// Research Papers:\n+    /// - chunkSize: 512 (paragraph or two)\n+    /// - parentSize: 4096 (entire subsection)\n+    /// - includeNeighboringChunks: false (rely on section boundaries)\n+    /// \n+    /// General Content:\n+    /// - chunkSize: 128 (few sentences)\n+    /// - parentSize: 1024 (multiple paragraphs)\n+    /// - includeNeighboringChunks: true (smooth transitions)\n+    /// \n+    /// The includeNeighboringChunks parameter is helpful when chunk boundaries might split important context.\n+    /// </para>\n+    /// </remarks>\n+    public ParentDocumentRetriever(\n+        IDocumentStore<T> documentStore,\n+        IEmbeddingModel<T> embeddingModel,\n+        int chunkSize,\n+        int parentSize,\n+        bool includeNeighboringChunks)\n+    {\n+        _documentStore = documentStore ?? throw new ArgumentNullException(nameof(documentStore));\n+        _embeddingModel = embeddingModel ?? throw new ArgumentNullException(nameof(embeddingModel));\n+        \n+        if (chunkSize <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(chunkSize), \"Chunk size must be positive\");\n+            \n+        if (parentSize <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(parentSize), \"Parent size must be positive\");\n+            \n+        if (parentSize < chunkSize)\n+            throw new ArgumentException(\"Parent size must be greater than or equal to chunk size\");\n+            \n+        _chunkSize = chunkSize;\n+        _parentSize = parentSize;\n+        _includeNeighboringChunks = includeNeighboringChunks;\n+    }\n+\n+    /// <summary>\n+    /// Retrieves parent documents by matching against child chunks and reconstructing full parent context.\n+    /// </summary>\n+    /// <param name=\"query\">The validated search query (non-empty).</param>\n+    /// <param name=\"topK\">The validated number of parent documents to return (positive integer).</param>\n+    /// <param name=\"metadataFilters\">The validated metadata filters for document selection.</param>\n+    /// <returns>Parent documents ordered by their best child chunk's relevance score (highest first).</returns>\n+    /// <exception cref=\"ArgumentException\">Thrown when query is null or whitespace.</exception>\n+    /// <exception cref=\"ArgumentOutOfRangeException\">Thrown when topK is less than or equal to zero.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// This method implements a hierarchical retrieval pipeline:\n+    /// 1. Oversampling: Retrieves topK * 3 child chunks to ensure sufficient parent document coverage\n+    /// 2. Parent Grouping: Groups chunks by parent_id metadata field\n+    /// 3. Score Aggregation: Assigns each parent the MAXIMUM score of its child chunks (best match wins)\n+    /// 4. Optional Expansion: If includeNeighboringChunks=true, concatenates all matching chunks for each parent\n+    /// 5. Deduplication: Returns unique parent documents (multiple chunks may belong to same parent)\n+    /// \n+    /// The retriever expects chunks to have metadata:\n+    /// - \"parent_id\": Identifier of the parent document\n+    /// - \"chunk_index\": Position of this chunk within parent (optional)\n+    /// - \"chunk_start\": Character offset where chunk begins (optional)\n+    /// \n+    /// Parent documents are reconstructed by combining chunk content and filtering chunk-specific metadata.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Here's how the retrieval process works:\n+    /// \n+    /// Step 1: Find the best matching small chunks\n+    /// - Query: \"neural network backpropagation\"\n+    /// - Matches: [Chunk 5 from Doc A (score=0.9), Chunk 12 from Doc A (score=0.7), Chunk 3 from Doc B (score=0.8)]\n+    /// \n+    /// Step 2: Group chunks by parent document\n+    /// - Doc A: Chunk 5 (0.9), Chunk 12 (0.7)\n+    /// - Doc B: Chunk 3 (0.8)\n+    /// \n+    /// Step 3: Assign parent score = best child score\n+    /// - Doc A: 0.9 (from Chunk 5)\n+    /// - Doc B: 0.8 (from Chunk 3)\n+    /// \n+    /// Step 4: Return parent documents (full context!)\n+    /// - Doc A: Complete section on backpropagation including introduction, math, examples\n+    /// - Doc B: Complete chapter on neural network training\n+    /// \n+    /// This gives your LLM ALL the context it needs to provide a complete answer!\n+    /// </para>\n+    /// </remarks>\n+    protected override IEnumerable<Document<T>> RetrieveCore(\n+        string query,\n+        int topK,\n+        Dictionary<string, object> metadataFilters)\n+    {\n+        if (string.IsNullOrWhiteSpace(query))\n+            throw new ArgumentException(\"Query cannot be null or whitespace\", nameof(query));\n+\n+        if (topK <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(topK), \"topK must be positive\");\n+\n+        // Retrieve chunks at higher K to ensure we get enough parent documents\n+        var chunkK = topK * 3;\n+        \n+        // Embed the query to get query vector\n+        var queryVector = _embeddingModel.Embed(query);\n+        \n+        // Use the document store to find similar chunks\n+        // The chunks should have metadata indicating their parent document\n+        var similarChunks = _documentStore.GetSimilarWithFilters(\n+            queryVector,\n+            chunkK,\n+            metadataFilters ?? new Dictionary<string, object>()\n+        ).ToList();\n+\n+        // Group by parent document ID\n+        var parentDocuments = new Dictionary<string, (Document<T> doc, T maxScore)>();\n+        \n+        foreach (var chunk in similarChunks)\n+        {\n+            // Extract parent document ID from metadata\n+            var parentId = chunk.Metadata.ContainsKey(\"parent_id\") \n+                ? chunk.Metadata[\"parent_id\"].ToString() \n+                : chunk.Id;\n+\n+            if (parentId == null) continue;\n+\n+            var score = chunk.RelevanceScore;",
          "createdAt": "2025-11-05T19:42:26Z",
          "position": 217
        }
      ]
    }
  },
  {
    "id": "PRRT_kwDOKSXUF85guJ0S",
    "isResolved": false,
    "isOutdated": false,
    "comments": {
      "nodes": [
        {
          "path": "src/RetrievalAugmentedGeneration/Retrievers/ParentDocumentRetriever.cs",
          "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Returned ΓÇ£parentΓÇ¥ docs are just chunks (and out of order).**\n\n`CreateParentDocument` seeds the parent with the first chunkΓÇÖs text, and unless `_includeNeighboringChunks` is set you never add anything elseΓÇöso the retriever returns the same single chunk it matched. When you do append, you concatenate chunks in retrieval-score order instead of `chunk_index`, scrambling the parent content. Fetch the actual parent document (e.g., via `_documentStore.GetById`) or at least collect all child chunks for that parent, sort by their original index, and trim to `_parentSize` before emitting. As written, the retriever fails its stated contract.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Retrievers/ParentDocumentRetriever.cs around\nlines 221-239, the code currently seeds parent documents with a single chunk and\nappends neighboring chunks in retrieval-score order, producing truncated or\nscrambled parent content; instead fetch the real parent document (e.g.\n_documentStore.GetById(parentId)) or collect all child chunks for this parent,\nsort them by their original chunk index, take/trim to _parentSize, concatenate\nin index order to build the parent content, and store that assembled document\n(preserving the highest score as maxScore); ensure this happens whether or not\n_includeNeighboringChunks is set so the returned parent reflects actual parent\ncontent and maintain correct ordering and trimming before emitting.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
          "diffHunk": "@@ -0,0 +1,270 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers;\n+\n+/// <summary>\n+/// Retrieves precise small chunks for matching but returns complete parent documents for comprehensive context.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for relevance scoring (typically float or double).</typeparam>\n+/// <remarks>\n+/// <para>\n+/// ParentDocumentRetriever solves the \"chunk size dilemma\" in RAG systems: small chunks enable precise matching\n+/// but lack context, while large chunks provide context but reduce precision. This retriever uses a two-tier\n+/// approachΓÇösearch against small chunks (e.g., paragraphs) for accuracy, then return their larger parent documents\n+/// (e.g., full sections or pages) for complete context. This ensures the LLM receives sufficient information to\n+/// generate accurate answers while maintaining high retrieval precision. The retriever can optionally include\n+/// neighboring chunks to expand context boundaries. This pattern is particularly effective for structured content\n+/// (technical docs, research papers, legal documents) where individual paragraphs are meaningful but answers require\n+/// broader context.\n+/// </para>\n+/// <para><b>For Beginners:</b> Think of this like a two-step library search:\n+/// \n+/// The Problem:\n+/// - Small chunks (paragraphs): Easy to match precisely, BUT not enough context\n+/// - Large chunks (whole pages): Plenty of context, BUT hard to match precisely\n+/// \n+/// The Solution - Parent Document Retrieval:\n+/// 1. Search using SMALL chunks for precision\n+/// 2. Return the LARGE parent document for context\n+/// \n+/// Real-world example:\n+/// Query: \"How does photosynthesis work?\"\n+/// \n+/// Small chunk matches: \"...chlorophyll absorbs light energy...\" (paragraph 3)\n+/// Returns: Full page containing introduction + detailed process + diagram\n+/// \n+/// ```csharp\n+/// var retriever = new ParentDocumentRetriever<double>(\n+///     documentStore,\n+///     chunkSize: 256,                    // Small chunks for precision\n+///     parentSize: 2048,                  // Large parents for context\n+///     includeNeighboringChunks: true     // Add nearby chunks too\n+/// );\n+/// \n+/// var results = retriever.Retrieve(\"explain quantum entanglement\", topK: 3);\n+/// // Finds precise paragraphs but returns full sections with complete explanation\n+/// ```\n+/// \n+/// Why use ParentDocumentRetriever:\n+/// - Best of both worlds: precise matching + complete context\n+/// - Ideal for technical documentation and research papers\n+/// - Reduces LLM hallucinations (more context = better answers)\n+/// - Works great with structured content (headings, sections, chapters)\n+/// \n+/// When NOT to use it:\n+/// - Very short documents (chunks = parents already)\n+/// - Documents with redundant content (wastes context window)\n+/// - When you need ONLY the matching excerpt (use regular retrieval)\n+/// - Memory-constrained systems (returns more content per match)\n+/// </para>\n+/// </remarks>\n+public class ParentDocumentRetriever<T> : RetrieverBase<T>\n+{\n+    private readonly IDocumentStore<T> _documentStore;\n+    private readonly IEmbeddingModel<T> _embeddingModel;\n+    private readonly int _chunkSize;\n+    private readonly int _parentSize;\n+    private readonly bool _includeNeighboringChunks;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"ParentDocumentRetriever{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"documentStore\">The document store containing chunked documents with parent metadata.</param>\n+    /// <param name=\"embeddingModel\">The embedding model used to convert text queries into vector embeddings.</param>\n+    /// <param name=\"chunkSize\">Character length of child chunks used for matching (typically 128-512 characters).</param>\n+    /// <param name=\"parentSize\">Character length of parent documents returned (typically 1024-4096 characters).</param>\n+    /// <param name=\"includeNeighboringChunks\">Whether to include adjacent chunks around the matched chunk (expands context boundaries).</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when documentStore or embeddingModel is null.</exception>\n+    /// <exception cref=\"ArgumentOutOfRangeException\">Thrown when chunkSize or parentSize is less than or equal to zero.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when parentSize is less than chunkSize.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This configures how the retriever balances precision vs. context.\n+    /// \n+    /// Recommended configurations:\n+    /// \n+    /// Technical Documentation:\n+    /// - chunkSize: 256 (1-2 paragraphs)\n+    /// - parentSize: 2048 (full section)\n+    /// - includeNeighboringChunks: true (add context before/after)\n+    /// \n+    /// Research Papers:\n+    /// - chunkSize: 512 (paragraph or two)\n+    /// - parentSize: 4096 (entire subsection)\n+    /// - includeNeighboringChunks: false (rely on section boundaries)\n+    /// \n+    /// General Content:\n+    /// - chunkSize: 128 (few sentences)\n+    /// - parentSize: 1024 (multiple paragraphs)\n+    /// - includeNeighboringChunks: true (smooth transitions)\n+    /// \n+    /// The includeNeighboringChunks parameter is helpful when chunk boundaries might split important context.\n+    /// </para>\n+    /// </remarks>\n+    public ParentDocumentRetriever(\n+        IDocumentStore<T> documentStore,\n+        IEmbeddingModel<T> embeddingModel,\n+        int chunkSize,\n+        int parentSize,\n+        bool includeNeighboringChunks)\n+    {\n+        _documentStore = documentStore ?? throw new ArgumentNullException(nameof(documentStore));\n+        _embeddingModel = embeddingModel ?? throw new ArgumentNullException(nameof(embeddingModel));\n+        \n+        if (chunkSize <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(chunkSize), \"Chunk size must be positive\");\n+            \n+        if (parentSize <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(parentSize), \"Parent size must be positive\");\n+            \n+        if (parentSize < chunkSize)\n+            throw new ArgumentException(\"Parent size must be greater than or equal to chunk size\");\n+            \n+        _chunkSize = chunkSize;\n+        _parentSize = parentSize;\n+        _includeNeighboringChunks = includeNeighboringChunks;\n+    }\n+\n+    /// <summary>\n+    /// Retrieves parent documents by matching against child chunks and reconstructing full parent context.\n+    /// </summary>\n+    /// <param name=\"query\">The validated search query (non-empty).</param>\n+    /// <param name=\"topK\">The validated number of parent documents to return (positive integer).</param>\n+    /// <param name=\"metadataFilters\">The validated metadata filters for document selection.</param>\n+    /// <returns>Parent documents ordered by their best child chunk's relevance score (highest first).</returns>\n+    /// <exception cref=\"ArgumentException\">Thrown when query is null or whitespace.</exception>\n+    /// <exception cref=\"ArgumentOutOfRangeException\">Thrown when topK is less than or equal to zero.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// This method implements a hierarchical retrieval pipeline:\n+    /// 1. Oversampling: Retrieves topK * 3 child chunks to ensure sufficient parent document coverage\n+    /// 2. Parent Grouping: Groups chunks by parent_id metadata field\n+    /// 3. Score Aggregation: Assigns each parent the MAXIMUM score of its child chunks (best match wins)\n+    /// 4. Optional Expansion: If includeNeighboringChunks=true, concatenates all matching chunks for each parent\n+    /// 5. Deduplication: Returns unique parent documents (multiple chunks may belong to same parent)\n+    /// \n+    /// The retriever expects chunks to have metadata:\n+    /// - \"parent_id\": Identifier of the parent document\n+    /// - \"chunk_index\": Position of this chunk within parent (optional)\n+    /// - \"chunk_start\": Character offset where chunk begins (optional)\n+    /// \n+    /// Parent documents are reconstructed by combining chunk content and filtering chunk-specific metadata.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Here's how the retrieval process works:\n+    /// \n+    /// Step 1: Find the best matching small chunks\n+    /// - Query: \"neural network backpropagation\"\n+    /// - Matches: [Chunk 5 from Doc A (score=0.9), Chunk 12 from Doc A (score=0.7), Chunk 3 from Doc B (score=0.8)]\n+    /// \n+    /// Step 2: Group chunks by parent document\n+    /// - Doc A: Chunk 5 (0.9), Chunk 12 (0.7)\n+    /// - Doc B: Chunk 3 (0.8)\n+    /// \n+    /// Step 3: Assign parent score = best child score\n+    /// - Doc A: 0.9 (from Chunk 5)\n+    /// - Doc B: 0.8 (from Chunk 3)\n+    /// \n+    /// Step 4: Return parent documents (full context!)\n+    /// - Doc A: Complete section on backpropagation including introduction, math, examples\n+    /// - Doc B: Complete chapter on neural network training\n+    /// \n+    /// This gives your LLM ALL the context it needs to provide a complete answer!\n+    /// </para>\n+    /// </remarks>\n+    protected override IEnumerable<Document<T>> RetrieveCore(\n+        string query,\n+        int topK,\n+        Dictionary<string, object> metadataFilters)\n+    {\n+        if (string.IsNullOrWhiteSpace(query))\n+            throw new ArgumentException(\"Query cannot be null or whitespace\", nameof(query));\n+\n+        if (topK <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(topK), \"topK must be positive\");\n+\n+        // Retrieve chunks at higher K to ensure we get enough parent documents\n+        var chunkK = topK * 3;\n+        \n+        // Embed the query to get query vector\n+        var queryVector = _embeddingModel.Embed(query);\n+        \n+        // Use the document store to find similar chunks\n+        // The chunks should have metadata indicating their parent document\n+        var similarChunks = _documentStore.GetSimilarWithFilters(\n+            queryVector,\n+            chunkK,\n+            metadataFilters ?? new Dictionary<string, object>()\n+        ).ToList();\n+\n+        // Group by parent document ID\n+        var parentDocuments = new Dictionary<string, (Document<T> doc, T maxScore)>();\n+        \n+        foreach (var chunk in similarChunks)\n+        {\n+            // Extract parent document ID from metadata\n+            var parentId = chunk.Metadata.ContainsKey(\"parent_id\") \n+                ? chunk.Metadata[\"parent_id\"].ToString() \n+                : chunk.Id;\n+\n+            if (parentId == null) continue;\n+\n+            var score = chunk.RelevanceScore;\n+            \n+            if (!parentDocuments.ContainsKey(parentId))\n+            {\n+                // Create parent document by combining chunks\n+                var parentDoc = CreateParentDocument(chunk, parentId);\n+                parentDocuments[parentId] = (parentDoc, score);\n+            }\n+            else\n+            {\n+                // Update if this chunk has better score\n+                var existing = parentDocuments[parentId];\n+                if (NumOps.GreaterThan(score, existing.maxScore))\n+                {\n+                    parentDocuments[parentId] = (existing.doc, score);\n+                }\n+                \n+                // Append chunk content if including neighboring chunks\n+                if (_includeNeighboringChunks)\n+                {\n+                    parentDocuments[parentId].doc.Content += \"\\n\\n\" + chunk.Content;\n+                }\n+            }",
          "createdAt": "2025-11-05T19:42:26Z",
          "position": 239
        }
      ]
    }
  },
  {
    "id": "PRRT_kwDOKSXUF85guJ0Z",
    "isResolved": false,
    "isOutdated": false,
    "comments": {
      "nodes": [
        {
          "path": "src/RetrievalAugmentedGeneration/Retrievers/TFIDFRetriever.cs",
          "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Guard ordering on generic T to avoid runtime comparer failures.**\n\nOrderByDescending on T requires comparability. Add a constraint or sort by a convertible numeric.\n\nApply one of:\n- Add constraint (simple):\n```diff\n-    public class TFIDFRetriever<T> : RetrieverBase<T>\n+    public class TFIDFRetriever<T> : RetrieverBase<T> where T : IComparable<T>\n```\n- Or sort by double (if NumOps exposes ToDouble).\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Retrievers/TFIDFRetriever.cs around lines 27\nto 35, the class uses OrderByDescending on generic T which can fail at runtime\nbecause T may not be comparable; either add a generic constraint to ensure\ncomparability (e.g., declare the class with where T : IComparable<T> or where T\n: IComparable) so LINQ ordering is valid, or change the sorting key to a numeric\nrepresentation (convert the T values to double via an existing NumOps/ToDouble\nhelper or Convert.ToDouble and sort by that) and keep the generic unconstrained.\nImplement one of these fixes and update any related method signatures so the\ncomparer operation is guaranteed at compile time.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:chinchilla -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
          "diffHunk": "@@ -0,0 +1,204 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\n+{\n+    /// <summary>\n+    /// TF-IDF (Term Frequency-Inverse Document Frequency) retrieval strategy with cached statistics.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    /// <remarks>\n+    /// <para>\n+    /// Implements production-ready TF-IDF retrieval with intelligent caching to avoid recomputing\n+    /// statistics on every query. The cache is automatically invalidated when the document count changes,\n+    /// ensuring accuracy while maximizing performance.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> TF-IDF ranks documents by how unique and frequent terms are.\n+    /// \n+    /// This implementation uses a cache to avoid recalculating term statistics on every search,\n+    /// dramatically improving performance for repeated queries. The cache is automatically refreshed\n+    /// when documents are added or removed.\n+    /// </para>\n+    /// </remarks>\n+    public class TFIDFRetriever<T> : RetrieverBase<T>\n+    {\n+        private readonly IDocumentStore<T> _documentStore;\n+        private readonly object _cacheLock = new object();\n+        \n+        private Dictionary<string, Dictionary<string, T>>? _cachedTfidf;\n+        private Dictionary<string, T>? _cachedIdf;\n+        private int _cachedDocumentCount;\n+",
          "createdAt": "2025-11-05T19:42:26Z",
          "position": 35
        }
      ]
    }
  },
  {
    "id": "PRRT_kwDOKSXUF85guNSB",
    "isResolved": false,
    "isOutdated": false,
    "comments": {
      "nodes": [
        {
          "path": "src/RetrievalAugmentedGeneration/DocumentStores/InMemoryDocumentStore.cs",
          "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Remove unused constructor parameters or store them for actual use.**\n\nThe constructor requires `databasePath` and `tableName` parameters with validation, but these values are never stored or used anywhere in the implementation. The documentation claims `tableName` is \"used for debugging/logging,\" but there's no evidence of this in the code.\n\nAdditionally, setting `_vectorDimension` in the constructor creates inconsistency with other document store implementations (e.g., `FAISSDocumentStore` determines dimension from the first document) and results in dead code in `AddCore` and `AddBatchCore` (see separate comments).\n\n\n\nConsider one of these approaches:\n\n**Option 1 (Recommended): Simplify to match other stores**\n```diff\n-public InMemoryDocumentStore(string databasePath, string tableName, int vectorDimension)\n+public InMemoryDocumentStore(int initialCapacity = 1000)\n {\n-    if (string.IsNullOrWhiteSpace(databasePath))\n-        throw new ArgumentException(\"Database path cannot be empty\", nameof(databasePath));\n-    if (string.IsNullOrWhiteSpace(tableName))\n-        throw new ArgumentException(\"Table name cannot be empty\", nameof(tableName));\n-    if (vectorDimension <= 0)\n-        throw new ArgumentException(\"Vector dimension must be positive\", nameof(vectorDimension));\n-\n-    _store = new Dictionary<string, VectorDocument<T>>();\n-    _vectorDimension = vectorDimension;\n+    _store = new Dictionary<string, VectorDocument<T>>(initialCapacity);\n+    _vectorDimension = 0;\n }\n```\n\n**Option 2: If parameters are needed for future features, store them**\n```diff\n+private readonly string _databasePath;\n+private readonly string _tableName;\n+\n public InMemoryDocumentStore(string databasePath, string tableName, int vectorDimension)\n {\n     // ... validation ...\n     \n+    _databasePath = databasePath;\n+    _tableName = tableName;\n     _store = new Dictionary<string, VectorDocument<T>>();\n-    _vectorDimension = vectorDimension;\n+    _vectorDimension = 0; // Set from first document\n }\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/InMemoryDocumentStore.cs\naround lines 81 to 92, the constructor validates databasePath and tableName but\nnever stores or uses them and sets a fixed _vectorDimension which conflicts with\nother stores and causes dead code; either remove the unused parameters and the\n_vectorDimension field to simplify behavior to match other stores (determine\ndimension from the first added document), or persist databasePath and tableName\nto private readonly fields and use tableName for logging/debugging and postpone\nvector dimension assignment (derive it from the first document added) so\nAddCore/AddBatchCore remain consistent; update constructor and class fields\naccordingly and adjust any logging or dimension-check logic to use the chosen\napproach.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
          "diffHunk": "@@ -1,209 +1,336 @@\n using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n using AiDotNet.LinearAlgebra;\n using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n \n namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n \n /// <summary>\n-/// An in-memory document store implementation using cosine similarity for retrieval.\n+/// High-performance in-memory vector document store optimized for rapid prototyping and small-to-medium datasets.\n /// </summary>\n /// <remarks>\n /// <para>\n-/// This implementation stores all documents and their embeddings in memory using dictionaries.\n-/// It provides fast similarity search using cosine similarity and is suitable for development,\n-/// testing, and small to medium-sized document collections (up to ~100K documents).\n-/// For larger collections or persistent storage, consider using a dedicated vector database.\n+/// This implementation provides a pure in-memory vector store using .NET's high-performance collections,\n+/// offering extremely fast vector similarity search with minimal configuration. Ideal for development,\n+/// testing, caching layers, and applications with datasets that fit comfortably in RAM.\n /// </para>\n-/// <para><b>For Beginners:</b> This is a simple document storage that keeps everything in RAM.\n+/// <para><b>For Beginners:</b> An in-memory store keeps all your data in RAM (like variables in your program).\n /// \n-/// Think of it like a filing cabinet in your office:\n-/// - All documents are stored in memory (RAM), not on disk\n-/// - Very fast for searching (no database queries needed)\n-/// - Lost when program restarts (not persistent)\n-/// - Limited by available RAM\n+/// Think of it like a super-fast notebook that lives in your computer's memory:\n+/// - Lightning-fast access (microseconds vs milliseconds)\n+/// - Zero configuration - just create and use\n+/// - Perfect for prototyping and testing\n+/// - Data is lost when the program stops\n /// \n-/// Good for:\n+/// Best used for:\n /// - Development and testing\n-/// - Small document collections (< 100K documents)\n-/// - Prototyping RAG applications\n-/// - Unit tests\n+/// - Session-based temporary data\n+/// - Caching frequently accessed vectors\n+/// - Small to medium datasets (< 100K documents)\n /// \n-/// Not ideal for:\n-/// - Production with large collections (use FAISS, Milvus, etc.)\n-/// - When persistence is required (data survives restarts)\n-/// - Distributed systems (this is single-process only)\n+/// Key characteristics:\n+/// - O(n) similarity search using cosine distance\n+/// - Thread-safe concurrent access\n+/// - Low memory overhead with efficient storage\n+/// - Simple serialization/deserialization support\n /// </para>\n /// </remarks>\n-/// <typeparam name=\"T\">The numeric data type used for vector calculations (typically float or double).</typeparam>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n public class InMemoryDocumentStore<T> : DocumentStoreBase<T>\n {\n-    private readonly Dictionary<string, VectorDocument<T>> _documents;\n+    private readonly Dictionary<string, VectorDocument<T>> _store;\n     private int _vectorDimension;\n \n     /// <summary>\n-    /// Gets the number of documents currently stored in the document store.\n+    /// Gets the number of documents currently stored in the database.\n     /// </summary>\n-    public override int DocumentCount => _documents.Count;\n+    public override int DocumentCount => _store.Count;\n \n     /// <summary>\n-    /// Gets the dimensionality of the vectors stored in this document store.\n+    /// Gets the dimensionality of vectors stored in this database.\n     /// </summary>\n     public override int VectorDimension => _vectorDimension;\n \n     /// <summary>\n     /// Initializes a new instance of the InMemoryDocumentStore class.\n     /// </summary>\n-    /// <param name=\"initialCapacity\">The initial capacity for the internal dictionary (default: 1000).</param>\n-    public InMemoryDocumentStore(int initialCapacity = 1000)\n+    /// <param name=\"databasePath\">Logical identifier for this store instance (not used for file storage).</param>\n+    /// <param name=\"tableName\">Logical name for the document collection (used for debugging/logging).</param>\n+    /// <param name=\"vectorDimension\">The dimension of vector embeddings.</param>\n+    /// <exception cref=\"ArgumentException\">Thrown when parameters are invalid.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> Creates a new in-memory vector store that keeps all data in RAM.\n+    /// \n+    /// Example:\n+    /// <code>\n+    /// // Create a store with 384-dimensional embeddings\n+    /// var store = new InMemoryDocumentStore&lt;float&gt;(\n+    ///     \"session-cache\",\n+    ///     \"documents\",\n+    ///     vectorDimension: 384\n+    /// );\n+    /// </code>\n+    /// \n+    /// All data is stored in memory and will be lost when the program exits.\n+    /// Vector dimension must match your embedding model's output dimension.\n+    /// </para>\n+    /// </remarks>\n+    public InMemoryDocumentStore(string databasePath, string tableName, int vectorDimension)\n     {\n-        if (initialCapacity <= 0)\n-            throw new ArgumentException(\"Initial capacity must be greater than zero\", nameof(initialCapacity));\n+        if (string.IsNullOrWhiteSpace(databasePath))\n+            throw new ArgumentException(\"Database path cannot be empty\", nameof(databasePath));\n+        if (string.IsNullOrWhiteSpace(tableName))\n+            throw new ArgumentException(\"Table name cannot be empty\", nameof(tableName));\n+        if (vectorDimension <= 0)\n+            throw new ArgumentException(\"Vector dimension must be positive\", nameof(vectorDimension));\n \n-        _documents = new Dictionary<string, VectorDocument<T>>(initialCapacity);\n-        _vectorDimension = 0;\n+        _store = new Dictionary<string, VectorDocument<T>>();\n+        _vectorDimension = vectorDimension;\n     }",
          "createdAt": "2025-11-05T19:50:24Z",
          "position": 131
        }
      ]
    }
  }
]
