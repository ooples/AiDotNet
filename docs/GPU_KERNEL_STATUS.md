# GPU Kernel Implementation Status

This document tracks all GPU kernels needed for full GPU-resident training across CUDA, HIP (AMD), and OpenCL backends.

## Kernel Categories

### Legend
| Symbol | Meaning |
|--------|---------|
| âœ… | Implemented in all backends (CUDA, HIP, OpenCL) |
| âš ï¸ | Implemented in some backends (see notes) |
| âŒ | Not implemented in any backend |
| ğŸ”§ | Exists but needs fixes/improvements |

---

## 1. Activation Forward Kernels

| Kernel | CUDA | HIP | OpenCL | Notes |
|--------|------|-----|--------|-------|
| relu | âœ… | âœ… | âœ… | |
| leaky_relu | âœ… | âœ… | âœ… | |
| sigmoid | âœ… | âœ… | âœ… | |
| tanh | âœ… | âœ… | âœ… | |
| gelu | âœ… | âœ… | âœ… | |
| swish/silu | âœ… | âœ… | âœ… | |
| softmax | âœ… | âœ… | âœ… | |
| elu | âœ… | âœ… | âœ… | |
| mish | âœ… | âœ… | âœ… | |
| softplus | âœ… | âœ… | âœ… | |
| hardswish | âœ… | âœ… | âœ… | |

## 2. Activation Backward Kernels

| Kernel | CUDA | HIP | OpenCL | Unblocks |
|--------|------|-----|--------|----------|
| relu_backward | âœ… | âœ… | âœ… | ActivationLayer, all ReLU layers |
| leaky_relu_backward | âœ… | âœ… | âœ… | LeakyReLU activations |
| sigmoid_backward | âœ… | âœ… | âœ… | Sigmoid activations, gates |
| tanh_backward | âœ… | âœ… | âœ… | Tanh activations, LSTM/GRU |
| gelu_backward | âœ… | âœ… | âœ… | Transformers, BERT |
| softmax_backward | âœ… | âœ… | âœ… | Attention, classification |
| elu_backward | âœ… | âœ… | âœ… | ELU activations |
| swish_backward | âœ… | âœ… | âœ… | Swish/SiLU activations |
| mish_backward | âŒ | âŒ | âŒ | Mish activation |
| softplus_backward | âŒ | âŒ | âŒ | Softplus activation |
| hardswish_backward | âŒ | âŒ | âŒ | HardSwish activation |

## 3. Convolution Kernels

| Kernel | CUDA | HIP | OpenCL | Unblocks |
|--------|------|-----|--------|----------|
| **Forward** |
| im2col | âœ… | âœ… | âœ… | All conv layers |
| conv2d_direct | âœ… | âœ… | âœ… | ConvolutionalLayer |
| depthwise_conv2d | âœ… | âœ… | âœ… | DepthwiseSeparable |
| conv_transpose2d | âœ… | âœ… | âœ… | DeconvolutionalLayer |
| conv3d_direct | âœ… | âœ… | âœ… | Conv3DLayer |
| **Backward** |
| col2im | âœ… | âœ… | âœ… | All conv backward |
| conv2d_backward_input | âœ… | âœ… | âœ… | ConvolutionalLayer backward |
| conv2d_backward_weights | âœ… | âœ… | âœ… | ConvolutionalLayer backward |
| conv3d_backward_input | âŒ | âŒ | âŒ | Conv3DLayer backward |
| conv3d_backward_weights | âŒ | âŒ | âŒ | Conv3DLayer backward |
| deconv_backward_input | âŒ | âŒ | âŒ | DeconvolutionalLayer backward |
| deconv_backward_weights | âŒ | âŒ | âŒ | DeconvolutionalLayer backward |
| depthwise_conv2d_backward | âŒ | âŒ | âŒ | DepthwiseSeparable backward |
| dilated_conv2d_backward | âŒ | âŒ | âŒ | DilatedConvolutionalLayer |

## 4. Normalization Kernels

| Kernel | CUDA | HIP | OpenCL | Unblocks |
|--------|------|-----|--------|----------|
| **Forward** |
| batchnorm_forward | âœ… | âœ… | âœ… | BatchNormalizationLayer |
| layernorm_forward | âœ… | âœ… | âœ… | LayerNormalizationLayer |
| groupnorm_forward | âœ… | âœ… | âœ… | GroupNormalizationLayer |
| instancenorm_forward | âœ… | âœ… | âœ… | InstanceNormalizationLayer |
| rmsnorm_forward | âœ… | âœ… | âœ… | RMSNorm (LLaMA style) |
| **Backward** |
| batchnorm_backward | âœ… | âœ… | âœ… | BatchNormalizationLayer |
| layernorm_backward | âœ… | âœ… | âœ… | LayerNormalizationLayer |
| layernorm_grad_params | âœ… | âœ… | âœ… | LayerNorm gamma/beta grads |
| groupnorm_backward | âŒ | âŒ | âŒ | GroupNormalizationLayer |
| instancenorm_backward | âŒ | âŒ | âŒ | InstanceNormalizationLayer |
| rmsnorm_backward | âŒ | âŒ | âœ… | RMSNorm backward |
| rmsnorm_grad_gamma | âŒ | âŒ | âœ… | RMSNorm gamma gradient |

## 5. Pooling Kernels

| Kernel | CUDA | HIP | OpenCL | Unblocks |
|--------|------|-----|--------|----------|
| **Forward** |
| maxpool2d | âœ… | âœ… | âœ… | MaxPoolingLayer |
| avgpool2d | âœ… | âœ… | âœ… | AveragePoolingLayer |
| global_avgpool2d | âœ… | âœ… | âœ… | GlobalPoolingLayer |
| global_maxpool2d | âœ… | âœ… | âœ… | GlobalPoolingLayer |
| adaptive_avgpool2d | âœ… | âœ… | âœ… | AdaptiveAveragePoolingLayer |
| **Backward** |
| maxpool2d_backward | âœ… | âœ… | âœ… | MaxPoolingLayer âœ“ |
| avgpool2d_backward | âœ… | âœ… | âœ… | AveragePoolingLayer âœ“ |
| global_avgpool2d_backward | âŒ | âŒ | âŒ | GlobalPoolingLayer |
| global_maxpool2d_backward | âŒ | âŒ | âŒ | GlobalPoolingLayer |
| adaptive_avgpool2d_backward | âŒ | âŒ | âŒ | AdaptiveAveragePoolingLayer |
| maxpool3d_backward | âŒ | âŒ | âŒ | MaxPool3DLayer |
| avgpool3d_backward | âŒ | âŒ | âŒ | AveragePool3DLayer |

## 6. Attention Kernels

| Kernel | CUDA | HIP | OpenCL | Unblocks |
|--------|------|-----|--------|----------|
| **Forward** |
| scaled_dot_product_attention | âœ… | âœ… | âœ… | AttentionLayer, MHA |
| flash_attention_v2 | âœ… | âœ… | âœ… | Memory-efficient attention |
| grouped_query_attention | âœ… | âœ… | âœ… | GQA (LLaMA 2 style) |
| **Backward** |
| flash_attention_backward | âœ… | âœ… | âœ… | All attention layers |
| grouped_query_attention_backward | âœ… | âœ… | âœ… | GQA backward |
| cross_attention_backward | âŒ | âŒ | âŒ | CrossAttentionLayer |
| multi_head_attention_backward | âŒ | âŒ | âŒ | QKV projection grads |

## 7. Loss Function Kernels

| Kernel | CUDA | HIP | OpenCL | Unblocks |
|--------|------|-----|--------|----------|
| **Forward** |
| mse_loss | âœ… | âœ… | âœ… | MeanSquaredErrorLoss |
| cross_entropy_loss | âœ… | âœ… | âœ… | CrossEntropyLoss |
| bce_loss | âœ… | âœ… | âœ… | BinaryCrossEntropyLoss |
| smooth_l1_loss | âœ… | âœ… | âœ… | HuberLoss |
| **Backward** |
| mse_backward | âœ… | âœ… | âœ… | MeanSquaredErrorLoss |
| cross_entropy_backward | âœ… | âœ… | âœ… | CrossEntropyLoss |
| bce_backward | âœ… | âœ… | âœ… | BinaryCrossEntropyLoss |
| smooth_l1_backward | âœ… | âœ… | âœ… | HuberLoss |
| focal_loss | âŒ | âŒ | âŒ | FocalLoss |
| focal_loss_backward | âŒ | âŒ | âŒ | FocalLoss |
| triplet_loss | âŒ | âŒ | âŒ | TripletLoss |
| triplet_loss_backward | âŒ | âŒ | âŒ | TripletLoss |
| contrastive_loss | âŒ | âŒ | âŒ | ContrastiveLoss |
| contrastive_loss_backward | âŒ | âŒ | âŒ | ContrastiveLoss |

## 8. Optimizer Kernels

| Kernel | CUDA | HIP | OpenCL | Unblocks |
|--------|------|-----|--------|----------|
| sgd_step | âœ… | âœ… | âœ… | SGDOptimizer |
| sgd_momentum_update | âŒ | âŒ | âœ… | MomentumOptimizer |
| adam_step | âœ… | âœ… | âœ… | AdamOptimizer |
| adamw_step | âœ… | âœ… | âœ… | AdamWOptimizer |
| rmsprop_step | âŒ | âŒ | âŒ | RMSpropOptimizer |
| adagrad_step | âŒ | âŒ | âŒ | AdagradOptimizer |
| nag_step | âŒ | âŒ | âŒ | NesterovOptimizer |
| lars_step | âŒ | âŒ | âŒ | LARSOptimizer |
| lamb_step | âŒ | âŒ | âŒ | LAMBOptimizer |
| gradient_clip_norm | âŒ | âŒ | âŒ | All optimizers |
| gradient_clip_value | âŒ | âŒ | âŒ | All optimizers |

## 9. Embedding Kernels

| Kernel | CUDA | HIP | OpenCL | Unblocks |
|--------|------|-----|--------|----------|
| embedding_forward | âœ… | âœ… | âœ… | EmbeddingLayer |
| embedding_backward | âœ… | âœ… | âœ… | EmbeddingLayer (sparse scatter) |
| gather_kernel | âŒ | âŒ | âœ… | General gather ops |
| scatter_add_kernel | âŒ | âŒ | âœ… | Sparse gradient accumulation |

## 10. Recurrent Kernels (LSTM/GRU)

| Kernel | CUDA | HIP | OpenCL | Unblocks |
|--------|------|-----|--------|----------|
| **LSTM** |
| lstm_forward | âŒ | âŒ | âŒ | LSTMLayer |
| lstm_backward | âŒ | âŒ | âŒ | LSTMLayer (BPTT) |
| lstm_cell_forward | âŒ | âŒ | âŒ | Single LSTM step |
| lstm_cell_backward | âŒ | âŒ | âŒ | Single LSTM backward |
| **GRU** |
| gru_forward | âŒ | âŒ | âŒ | GRULayer |
| gru_backward | âŒ | âŒ | âŒ | GRULayer (BPTT) |
| gru_cell_forward | âŒ | âŒ | âŒ | Single GRU step |
| gru_cell_backward | âŒ | âŒ | âŒ | Single GRU backward |
| **ConvLSTM** |
| convlstm_forward | âŒ | âŒ | âŒ | ConvLSTMLayer |
| convlstm_backward | âŒ | âŒ | âŒ | ConvLSTMLayer (Issue #700) |

## 11. Utility Kernels

| Kernel | CUDA | HIP | OpenCL | Notes |
|--------|------|-----|--------|-------|
| transpose_2d | âœ… | âœ… | âœ… | |
| batched_transpose | âœ… | âœ… | âœ… | |
| permute_general | âœ… | âœ… | âŒ | General axis permutation |
| copy_buffer | âœ… | âœ… | âœ… | |
| fill_buffer | âŒ | âŒ | âœ… | Zero initialization |
| dropout_forward | âœ… | âœ… | âœ… | |
| dropout_backward | âœ… | âœ… | âœ… | |
| clamp | âœ… | âœ… | âœ… | |
| where_cond | âœ… | âœ… | âœ… | |
| argmax_axis | âœ… | âœ… | âœ… | |
| argmin_axis | âœ… | âœ… | âœ… | |
| reduce_sum | âœ… | âœ… | âŒ | Needs OpenCL impl |
| reduce_max | âœ… | âœ… | âŒ | Needs OpenCL impl |

## 12. Fused Kernels

| Kernel | CUDA | HIP | OpenCL | Notes |
|--------|------|-----|--------|-------|
| gemm_bias_relu | âœ… | âœ… | âœ… | |
| gemm_bias_gelu | âœ… | âœ… | âœ… | |
| gemm_bias_sigmoid | âœ… | âœ… | âœ… | |
| gemm_bias_tanh | âœ… | âœ… | âœ… | |
| gemm_bias | âœ… | âœ… | âœ… | |
| gemm_bias_swish | âœ… | âœ… | âŒ | |
| layernorm_relu | âœ… | âœ… | âœ… | |
| layernorm_gelu | âœ… | âœ… | âœ… | |
| residual_layernorm | âœ… | âœ… | âœ… | |
| bias_dropout | âœ… | âœ… | âœ… | |

## 13. Graph Neural Network Kernels

| Kernel | CUDA | HIP | OpenCL | Unblocks |
|--------|------|-----|--------|----------|
| sparse_mm_forward | âŒ | âŒ | âš ï¸ | GCN, GAT, GraphSAGE |
| sparse_mm_backward | âŒ | âŒ | âŒ | All GNN backward |
| message_passing_forward | âŒ | âŒ | âŒ | MessagePassingLayer |
| message_passing_backward | âŒ | âŒ | âŒ | MessagePassingLayer |
| scatter_add | âŒ | âŒ | âœ… | Graph aggregation |
| scatter_max | âŒ | âŒ | âŒ | Graph aggregation |
| scatter_mean | âŒ | âŒ | âŒ | Graph aggregation |
| edge_softmax | âŒ | âŒ | âŒ | GAT attention |
| diffusion_conv_forward | âŒ | âŒ | âŒ | DiffusionConvLayer |
| diffusion_conv_backward | âŒ | âŒ | âŒ | DiffusionConvLayer (Issue #700) |

## 14. 3D/Mesh Kernels

| Kernel | CUDA | HIP | OpenCL | Unblocks |
|--------|------|-----|--------|----------|
| upsample3d_nearest | âŒ | âŒ | âŒ | Upsample3DLayer |
| upsample3d_nearest_backward | âŒ | âŒ | âŒ | Upsample3DLayer |
| mesh_conv_forward | âŒ | âŒ | âŒ | MeshEdgeConvLayer |
| mesh_conv_backward | âŒ | âŒ | âŒ | MeshEdgeConvLayer |
| spiral_conv_forward | âŒ | âŒ | âŒ | SpiralConvLayer |
| spiral_conv_backward | âŒ | âŒ | âŒ | SpiralConvLayer |

---

## Priority Kernel Implementation Order

### Tier 0: Blockers for Basic Training (CRITICAL)
These must be implemented first to enable any GPU training:

1. **GEMM backward (for DenseLayer)** - Already available via transpose + GEMM
2. **Activation backward** - âœ… Already implemented (relu, sigmoid, tanh, gelu, softmax)
3. **Loss backward** - âœ… Already implemented (mse, cross_entropy, bce)
4. **SGD/Adam update** - âœ… Already implemented

**Status: UNBLOCKED** - Basic training infrastructure kernels exist!

### Tier 1: CNN Training
1. conv2d_backward_input - âœ… Exists
2. conv2d_backward_weights - âœ… Exists
3. batchnorm_backward - âœ… Exists
4. pooling backward - âœ… Exists (max, avg)

**Status: UNBLOCKED** - CNN training kernels exist!

### Tier 2: Transformer Training
1. layernorm_backward - âœ… Exists
2. attention backward - âœ… flash_attention_backward exists
3. embedding_backward - âœ… Exists

**Status: UNBLOCKED** - Transformer training kernels exist!

### Tier 3: Recurrent Network Training (BLOCKERS)
1. âŒ lstm_cell_forward
2. âŒ lstm_cell_backward  
3. âŒ gru_cell_forward
4. âŒ gru_cell_backward

**Status: BLOCKED** - Need LSTM/GRU kernels

### Tier 4: Graph Neural Network Training (BLOCKERS)
1. âŒ sparse_mm_backward
2. âŒ scatter_add (CUDA/HIP)
3. âŒ message_passing_backward

**Status: BLOCKED** - Need sparse/scatter kernels

### Tier 5: Missing Backward Kernels (Low Priority)
1. âŒ conv3d_backward
2. âŒ groupnorm_backward
3. âŒ instancenorm_backward
4. âŒ global_pool_backward
5. âŒ mish_backward, softplus_backward, hardswish_backward

---

## Backend Parity Gaps

### OpenCL Missing (compared to CUDA/HIP)
- reduce_sum, reduce_max
- permute_general
- gemm_bias_swish
- sgd_momentum_update (uses different name)

### CUDA/HIP Missing (compared to OpenCL)
- rmsnorm_backward, rmsnorm_grad_gamma
- scatter_add_kernel, gather_kernel
- fill_buffer

---

## Summary Statistics

| Category | Total Kernels | Implemented | Missing | % Complete |
|----------|--------------|-------------|---------|------------|
| Activation Forward | 11 | 11 | 0 | 100% |
| Activation Backward | 11 | 8 | 3 | 73% |
| Convolution | 14 | 8 | 6 | 57% |
| Normalization | 12 | 9 | 3 | 75% |
| Pooling | 12 | 6 | 6 | 50% |
| Attention | 7 | 6 | 1 | 86% |
| Loss Functions | 10 | 8 | 2 | 80% |
| Optimizer | 11 | 4 | 7 | 36% |
| Embedding | 4 | 3 | 1 | 75% |
| Recurrent (LSTM/GRU) | 10 | 0 | 10 | 0% |
| Graph Neural Networks | 10 | 1 | 9 | 10% |
| 3D/Mesh | 6 | 0 | 6 | 0% |
| **TOTAL** | **118** | **64** | **54** | **54%** |

## Key Findings

### Good News
1. **Basic training is UNBLOCKED**: Dense, Conv2D, BatchNorm, Attention, Loss functions all have backward kernels
2. **Optimizer kernels exist**: SGD and Adam are implemented
3. **Good backend parity**: CUDA, HIP, OpenCL have similar coverage

### Blockers to Address
1. **LSTM/GRU kernels**: 0% complete - blocks all recurrent layer training
2. **GNN kernels**: 10% complete - blocks graph neural network training  
3. **Conv3D backward**: Missing - blocks 3D CNN training
4. **Some optimizers**: RMSprop, Adagrad, LARS, LAMB missing

### Recommended Implementation Order
1. LSTM cell forward/backward (unblocks LSTMLayer, ConvLSTMLayer)
2. GRU cell forward/backward (unblocks GRULayer)
3. scatter_add for CUDA/HIP (unblocks GNN layers)
4. sparse_mm_backward (unblocks GNN training)
5. Conv3D backward (unblocks 3D CNNs)
6. Remaining optimizer kernels
