<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
      <title>Interface IActivationFunction&lt;T&gt; | AiDotNet Documentation </title>
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <meta name="title" content="Interface IActivationFunction&lt;T&gt; | AiDotNet Documentation ">
      
      <meta name="description" content="Defines an interface for activation functions used in neural networks and other machine learning algorithms.">
      <link rel="icon" href="../favicon.ico">
      <link rel="stylesheet" href="../public/docfx.min.css">
      <link rel="stylesheet" href="../public/main.css">
      <meta name="docfx:navrel" content="../toc.html">
      <meta name="docfx:tocrel" content="toc.html">
      
      <meta name="docfx:rel" content="../">
      
      
      <meta name="docfx:docurl" content="https://github.com/ooples/AiDotNet/new/master/apiSpec/new?filename=AiDotNet_Interfaces_IActivationFunction_1.md&amp;value=---%0Auid%3A%20AiDotNet.Interfaces.IActivationFunction%601%0Asummary%3A%20&#39;*You%20can%20override%20summary%20for%20the%20API%20here%20using%20*MARKDOWN*%20syntax&#39;%0A---%0A%0A*Please%20type%20below%20more%20information%20about%20this%20API%3A*%0A%0A">
      <meta name="loc:inThisArticle" content="In this article">
      <meta name="loc:searchResultsCount" content="{count} results for &quot;{query}&quot;">
      <meta name="loc:searchNoResults" content="No results for &quot;{query}&quot;">
      <meta name="loc:tocFilter" content="Filter by title">
      <meta name="loc:nextArticle" content="Next">
      <meta name="loc:prevArticle" content="Previous">
      <meta name="loc:themeLight" content="Light">
      <meta name="loc:themeDark" content="Dark">
      <meta name="loc:themeAuto" content="Auto">
      <meta name="loc:changeTheme" content="Change theme">
      <meta name="loc:copy" content="Copy">
      <meta name="loc:downloadPdf" content="Download PDF">

      <script type="module" src="./../public/docfx.min.js"></script>

      <script>
        const theme = localStorage.getItem('theme') || 'auto'
        document.documentElement.setAttribute('data-bs-theme', theme === 'auto' ? (window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light') : theme)
      </script>

  </head>

  <body class="tex2jax_ignore" data-layout="" data-yaml-mime="ManagedReference">
    <header class="bg-body border-bottom">
      <nav id="autocollapse" class="navbar navbar-expand-md" role="navigation">
        <div class="container-xxl flex-nowrap">
          <a class="navbar-brand" href="../index.html">
            <img id="logo" class="svg" src="../logo.svg" alt="AiDotNet">
            AiDotNet
          </a>
          <button class="btn btn-lg d-md-none border-0" type="button" data-bs-toggle="collapse" data-bs-target="#navpanel" aria-controls="navpanel" aria-expanded="false" aria-label="Toggle navigation">
            <i class="bi bi-three-dots"></i>
          </button>
          <div class="collapse navbar-collapse" id="navpanel">
            <div id="navbar">
              <form class="search" role="search" id="search">
                <i class="bi bi-search"></i>
                <input class="form-control" id="search-query" type="search" disabled placeholder="Search" autocomplete="off" aria-label="Search">
              </form>
            </div>
          </div>
        </div>
      </nav>
    </header>

    <main class="container-xxl">
      <div class="toc-offcanvas">
        <div class="offcanvas-md offcanvas-start" tabindex="-1" id="tocOffcanvas" aria-labelledby="tocOffcanvasLabel">
          <div class="offcanvas-header">
            <h5 class="offcanvas-title" id="tocOffcanvasLabel">Table of Contents</h5>
            <button type="button" class="btn-close" data-bs-dismiss="offcanvas" data-bs-target="#tocOffcanvas" aria-label="Close"></button>
          </div>
          <div class="offcanvas-body">
            <nav class="toc" id="toc"></nav>
          </div>
        </div>
      </div>

      <div class="content">
        <div class="actionbar">
          <button class="btn btn-lg border-0 d-md-none" type="button" data-bs-toggle="offcanvas" data-bs-target="#tocOffcanvas" aria-controls="tocOffcanvas" aria-expanded="false" aria-label="Show table of contents">
            <i class="bi bi-list"></i>
          </button>

          <nav id="breadcrumb"></nav>
        </div>

        <article data-uid="AiDotNet.Interfaces.IActivationFunction`1">



  <h1 id="AiDotNet_Interfaces_IActivationFunction_1" data-uid="AiDotNet.Interfaces.IActivationFunction`1" class="text-break">
Interface IActivationFunction&lt;T&gt;  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/Interfaces/IActivationFunction.cs/#L27"><i class="bi bi-code-slash"></i></a>
  </h1>

  <div class="facts text-secondary">
    <dl><dt>Namespace</dt><dd><a class="xref" href="AiDotNet.html">AiDotNet</a>.<a class="xref" href="AiDotNet.Interfaces.html">Interfaces</a></dd></dl>
  <dl><dt>Assembly</dt><dd>AiDotNet.dll</dd></dl>
  </div>

  <div class="markdown summary"><p>Defines an interface for activation functions used in neural networks and other machine learning algorithms.</p>
</div>
  <div class="markdown conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public interface IActivationFunction&lt;T&gt;</code></pre>
  </div>



  <h4 class="section">Type Parameters</h4>
  <dl class="parameters">
    <dt><code>T</code></dt>
    <dd><p>The numeric type used for calculations (e.g., double, float).</p>
</dd>
  </dl>








  <h2 id="AiDotNet_Interfaces_IActivationFunction_1_remarks">Remarks</h2>
  <div class="markdown level0 remarks"><p><b>For Beginners:</b> An activation function is like a decision-maker in a neural network.</p>
<p>Imagine each neuron (node) in a neural network receives a number as input. The activation
function decides how strongly that neuron should &quot;fire&quot; or activate based on that input.</p>
<p>For example:</p>
<ul>
<li>If the input is very negative, the neuron might not activate at all (output = 0)</li>
<li>If the input is very positive, the neuron might activate fully (output = 1)</li>
<li>If the input is around zero, the neuron might activate partially</li>
</ul>
<p>Different activation functions create different patterns of activation, which helps
neural networks learn different types of patterns in data. Common activation functions
include Sigmoid, ReLU (Rectified Linear Unit), and Tanh (Hyperbolic Tangent).</p>
<p>This interface defines the standard methods that all activation functions must implement.</p>
</div>


  <h2 class="section" id="properties">Properties
</h2>


  <a id="AiDotNet_Interfaces_IActivationFunction_1_SupportsGpuTraining_" data-uid="AiDotNet.Interfaces.IActivationFunction`1.SupportsGpuTraining*"></a>

  <h3 id="AiDotNet_Interfaces_IActivationFunction_1_SupportsGpuTraining" data-uid="AiDotNet.Interfaces.IActivationFunction`1.SupportsGpuTraining">
  SupportsGpuTraining
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/Interfaces/IActivationFunction.cs/#L171"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Gets whether this activation function supports GPU-resident training.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">bool SupportsGpuTraining { get; }</code></pre>
  </div>





  <h4 class="section">Property Value</h4>
  <dl class="parameters">
    <dt><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.boolean">bool</a></dt>
    <dd><p>True if the activation can perform forward and backward passes entirely on GPU.</p>
</dd>
  </dl>




  <h4 class="section" id="AiDotNet_Interfaces_IActivationFunction_1_SupportsGpuTraining_remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
Activation functions return false if:
- The GPU backend does not have kernels for this activation type
- The activation has dynamic behavior that cannot be executed on GPU
</p>
<p>
<b>For Beginners:</b> GPU-resident training keeps all data on the graphics card during
training, avoiding slow data transfers between CPU and GPU memory. This property indicates
whether this activation function can participate in GPU-resident training.
</p>
</div>




  <a id="AiDotNet_Interfaces_IActivationFunction_1_SupportsJitCompilation_" data-uid="AiDotNet.Interfaces.IActivationFunction`1.SupportsJitCompilation*"></a>

  <h3 id="AiDotNet_Interfaces_IActivationFunction_1_SupportsJitCompilation" data-uid="AiDotNet.Interfaces.IActivationFunction`1.SupportsJitCompilation">
  SupportsJitCompilation
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/Interfaces/IActivationFunction.cs/#L117"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Gets whether this activation function supports JIT compilation.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">bool SupportsJitCompilation { get; }</code></pre>
  </div>





  <h4 class="section">Property Value</h4>
  <dl class="parameters">
    <dt><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.boolean">bool</a></dt>
    <dd><p>True if the activation can be applied to computation graphs for JIT compilation.</p>
</dd>
  </dl>




  <h4 class="section" id="AiDotNet_Interfaces_IActivationFunction_1_SupportsJitCompilation_remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
Activation functions return false if:
- Gradient computation (backward pass) is not yet implemented
- The activation uses operations not supported by TensorOperations
- The activation has dynamic behavior that cannot be represented in a static graph
</p>
<p>
Once gradient computation is implemented and tested, set this to true.
</p>
<p>
<b>For Beginners:</b> JIT (Just-In-Time) compilation is an advanced optimization technique
that pre-compiles the neural network's operations into a faster execution graph.
This property indicates whether this activation function is ready to be part of that
optimized execution. If false, the activation will fall back to the standard execution path.
</p>
</div>




  <h2 class="section" id="methods">Methods
</h2>


  <a id="AiDotNet_Interfaces_IActivationFunction_1_Activate_" data-uid="AiDotNet.Interfaces.IActivationFunction`1.Activate*"></a>

  <h3 id="AiDotNet_Interfaces_IActivationFunction_1_Activate_AiDotNet_Tensors_LinearAlgebra_Tensor__0__" data-uid="AiDotNet.Interfaces.IActivationFunction`1.Activate(AiDotNet.Tensors.LinearAlgebra.Tensor{`0})">
  Activate(Tensor&lt;T&gt;)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/Interfaces/IActivationFunction.cs/#L87"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Applies the activation function to each element in a tensor.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">Tensor&lt;T&gt; Activate(Tensor&lt;T&gt; input)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>input</code> <a class="xref" href="AiDotNet.Tensors.LinearAlgebra.Tensor-1.html">Tensor</a>&lt;T&gt;</dt>
    <dd><p>The input tensor.</p>
</dd>
  </dl>

  <h4 class="section">Returns</h4>
  <dl class="parameters">
    <dt><a class="xref" href="AiDotNet.Tensors.LinearAlgebra.Tensor-1.html">Tensor</a>&lt;T&gt;</dt>
    <dd><p>A new tensor with the activation function applied to each element.</p>
</dd>
  </dl>











  <a id="AiDotNet_Interfaces_IActivationFunction_1_Activate_" data-uid="AiDotNet.Interfaces.IActivationFunction`1.Activate*"></a>

  <h3 id="AiDotNet_Interfaces_IActivationFunction_1_Activate_AiDotNet_Tensors_LinearAlgebra_Vector__0__" data-uid="AiDotNet.Interfaces.IActivationFunction`1.Activate(AiDotNet.Tensors.LinearAlgebra.Vector{`0})">
  Activate(Vector&lt;T&gt;)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/Interfaces/IActivationFunction.cs/#L73"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Applies the activation function to each element in a vector.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">Vector&lt;T&gt; Activate(Vector&lt;T&gt; input)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>input</code> <a class="xref" href="AiDotNet.Tensors.LinearAlgebra.Vector-1.html">Vector</a>&lt;T&gt;</dt>
    <dd><p>The input vector.</p>
</dd>
  </dl>

  <h4 class="section">Returns</h4>
  <dl class="parameters">
    <dt><a class="xref" href="AiDotNet.Tensors.LinearAlgebra.Vector-1.html">Vector</a>&lt;T&gt;</dt>
    <dd><p>A new vector with the activation function applied to each element.</p>
</dd>
  </dl>











  <a id="AiDotNet_Interfaces_IActivationFunction_1_Activate_" data-uid="AiDotNet.Interfaces.IActivationFunction`1.Activate*"></a>

  <h3 id="AiDotNet_Interfaces_IActivationFunction_1_Activate__0_" data-uid="AiDotNet.Interfaces.IActivationFunction`1.Activate(`0)">
  Activate(T)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/Interfaces/IActivationFunction.cs/#L45"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Applies the activation function to the input value.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">T Activate(T input)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>input</code> <span class="xref">T</span></dt>
    <dd><p>The input value to the activation function.</p>
</dd>
  </dl>

  <h4 class="section">Returns</h4>
  <dl class="parameters">
    <dt><span class="xref">T</span></dt>
    <dd><p>The activated output value.</p>
</dd>
  </dl>







  <h4 class="section" id="AiDotNet_Interfaces_IActivationFunction_1_Activate__0__remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p><b>For Beginners:</b> This method takes a number (which could be positive, negative, or zero)
and transforms it according to the specific activation function's rule.</p>
<p>For example, with the ReLU activation function:</p>
<ul>
<li>If input is negative, output is 0</li>
<li>If input is positive, output is the same as the input</li>
</ul>
<p>This transformation helps neural networks model complex, non-linear relationships
in data, which is essential for tasks like image recognition or language processing.</p>
</div>




  <a id="AiDotNet_Interfaces_IActivationFunction_1_ApplyToGraph_" data-uid="AiDotNet.Interfaces.IActivationFunction`1.ApplyToGraph*"></a>

  <h3 id="AiDotNet_Interfaces_IActivationFunction_1_ApplyToGraph_AiDotNet_Autodiff_ComputationNode__0__" data-uid="AiDotNet.Interfaces.IActivationFunction`1.ApplyToGraph(AiDotNet.Autodiff.ComputationNode{`0})">
  ApplyToGraph(ComputationNode&lt;T&gt;)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/Interfaces/IActivationFunction.cs/#L136"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Applies this activation function to a computation graph node.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">ComputationNode&lt;T&gt; ApplyToGraph(ComputationNode&lt;T&gt; input)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>input</code> <a class="xref" href="AiDotNet.Autodiff.ComputationNode-1.html">ComputationNode</a>&lt;T&gt;</dt>
    <dd><p>The computation node to apply the activation to.</p>
</dd>
  </dl>

  <h4 class="section">Returns</h4>
  <dl class="parameters">
    <dt><a class="xref" href="AiDotNet.Autodiff.ComputationNode-1.html">ComputationNode</a>&lt;T&gt;</dt>
    <dd><p>A new computation node with the activation applied.</p>
</dd>
  </dl>







  <h4 class="section" id="AiDotNet_Interfaces_IActivationFunction_1_ApplyToGraph_AiDotNet_Autodiff_ComputationNode__0___remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
This method maps the activation to the corresponding TensorOperations method.
For example, ReLU returns TensorOperations&lt;T&gt;.ReLU(input).
</p>
<p>
<b>For Beginners:</b> This method adds the activation function to the computation graph,
which is a data structure that represents all the operations in the neural network.
The graph can then be optimized and executed more efficiently through JIT compilation.
</p>
</div>

  <h4 class="section">Exceptions</h4>
  <dl class="parameters">
    <dt><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.notsupportedexception">NotSupportedException</a></dt>
    <dd><p>Thrown if SupportsJitCompilation is false.</p>
</dd>
  </dl>



  <a id="AiDotNet_Interfaces_IActivationFunction_1_Backward_" data-uid="AiDotNet.Interfaces.IActivationFunction`1.Backward*"></a>

  <h3 id="AiDotNet_Interfaces_IActivationFunction_1_Backward_AiDotNet_Tensors_LinearAlgebra_Tensor__0__AiDotNet_Tensors_LinearAlgebra_Tensor__0__" data-uid="AiDotNet.Interfaces.IActivationFunction`1.Backward(AiDotNet.Tensors.LinearAlgebra.Tensor{`0},AiDotNet.Tensors.LinearAlgebra.Tensor{`0})">
  Backward(Tensor&lt;T&gt;, Tensor&lt;T&gt;)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/Interfaces/IActivationFunction.cs/#L151"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Calculates the backward pass gradient for this activation function.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">Tensor&lt;T&gt; Backward(Tensor&lt;T&gt; input, Tensor&lt;T&gt; outputGradient)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>input</code> <a class="xref" href="AiDotNet.Tensors.LinearAlgebra.Tensor-1.html">Tensor</a>&lt;T&gt;</dt>
    <dd><p>The input tensor that was used in the forward pass.</p>
</dd>
    <dt><code>outputGradient</code> <a class="xref" href="AiDotNet.Tensors.LinearAlgebra.Tensor-1.html">Tensor</a>&lt;T&gt;</dt>
    <dd><p>The gradient flowing back from the next layer.</p>
</dd>
  </dl>

  <h4 class="section">Returns</h4>
  <dl class="parameters">
    <dt><a class="xref" href="AiDotNet.Tensors.LinearAlgebra.Tensor-1.html">Tensor</a>&lt;T&gt;</dt>
    <dd><p>The gradient with respect to the input.</p>
</dd>
  </dl>







  <h4 class="section" id="AiDotNet_Interfaces_IActivationFunction_1_Backward_AiDotNet_Tensors_LinearAlgebra_Tensor__0__AiDotNet_Tensors_LinearAlgebra_Tensor__0___remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
This method computes dL/dx = dL/dy * dy/dx.
For element-wise activations (ReLU, Sigmoid), this is element-wise multiplication.
For vector activations (Softmax), this involves Jacobian multiplication.
</p>
</div>




  <a id="AiDotNet_Interfaces_IActivationFunction_1_BackwardGpu_" data-uid="AiDotNet.Interfaces.IActivationFunction`1.BackwardGpu*"></a>

  <h3 id="AiDotNet_Interfaces_IActivationFunction_1_BackwardGpu_AiDotNet_Tensors_Engines_DirectGpu_IDirectGpuBackend_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_System_Int32_" data-uid="AiDotNet.Interfaces.IActivationFunction`1.BackwardGpu(AiDotNet.Tensors.Engines.DirectGpu.IDirectGpuBackend,AiDotNet.Tensors.Engines.DirectGpu.IGpuBuffer,AiDotNet.Tensors.Engines.DirectGpu.IGpuBuffer,AiDotNet.Tensors.Engines.DirectGpu.IGpuBuffer,AiDotNet.Tensors.Engines.DirectGpu.IGpuBuffer,System.Int32)">
  BackwardGpu(IDirectGpuBackend, IGpuBuffer, IGpuBuffer?, IGpuBuffer?, IGpuBuffer, int)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/Interfaces/IActivationFunction.cs/#L217"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Calculates the backward pass gradient on GPU.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">void BackwardGpu(IDirectGpuBackend backend, IGpuBuffer gradOutput, IGpuBuffer? input, IGpuBuffer? output, IGpuBuffer gradInput, int size)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>backend</code> <a class="xref" href="AiDotNet.Tensors.Engines.DirectGpu.IDirectGpuBackend.html">IDirectGpuBackend</a></dt>
    <dd><p>The GPU backend to use for execution.</p>
</dd>
    <dt><code>gradOutput</code> <a class="xref" href="AiDotNet.Tensors.Engines.DirectGpu.IGpuBuffer.html">IGpuBuffer</a></dt>
    <dd><p>The gradient flowing back from the next layer.</p>
</dd>
    <dt><code>input</code> <a class="xref" href="AiDotNet.Tensors.Engines.DirectGpu.IGpuBuffer.html">IGpuBuffer</a></dt>
    <dd><p>The input buffer from the forward pass (needed for ReLU, GELU, Swish, LeakyReLU).</p>
</dd>
    <dt><code>output</code> <a class="xref" href="AiDotNet.Tensors.Engines.DirectGpu.IGpuBuffer.html">IGpuBuffer</a></dt>
    <dd><p>The output buffer from the forward pass (needed for Sigmoid, Tanh).</p>
</dd>
    <dt><code>gradInput</code> <a class="xref" href="AiDotNet.Tensors.Engines.DirectGpu.IGpuBuffer.html">IGpuBuffer</a></dt>
    <dd><p>The output buffer to store the input gradient.</p>
</dd>
    <dt><code>size</code> <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.int32">int</a></dt>
    <dd><p>The number of elements to process.</p>
</dd>
  </dl>








  <h4 class="section" id="AiDotNet_Interfaces_IActivationFunction_1_BackwardGpu_AiDotNet_Tensors_Engines_DirectGpu_IDirectGpuBackend_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_System_Int32__remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
This method computes the activation gradient entirely on GPU.
Different activation functions require different cached values from forward pass:
- ReLU, LeakyReLU, GELU, Swish: Need the input from forward pass
- Sigmoid, Tanh: Need the output from forward pass
</p>
<p>
<b>For Beginners:</b> During training, we need to compute gradients to update the network.
This method computes the gradient of the activation function on GPU, which is essential
for efficient GPU-resident training.
</p>
</div>

  <h4 class="section">Exceptions</h4>
  <dl class="parameters">
    <dt><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.notsupportedexception">NotSupportedException</a></dt>
    <dd><p>Thrown if SupportsGpuTraining is false.</p>
</dd>
  </dl>



  <a id="AiDotNet_Interfaces_IActivationFunction_1_Derivative_" data-uid="AiDotNet.Interfaces.IActivationFunction`1.Derivative*"></a>

  <h3 id="AiDotNet_Interfaces_IActivationFunction_1_Derivative_AiDotNet_Tensors_LinearAlgebra_Tensor__0__" data-uid="AiDotNet.Interfaces.IActivationFunction`1.Derivative(AiDotNet.Tensors.LinearAlgebra.Tensor{`0})">
  Derivative(Tensor&lt;T&gt;)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/Interfaces/IActivationFunction.cs/#L94"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Calculates the derivative for each element in a tensor.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">Tensor&lt;T&gt; Derivative(Tensor&lt;T&gt; input)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>input</code> <a class="xref" href="AiDotNet.Tensors.LinearAlgebra.Tensor-1.html">Tensor</a>&lt;T&gt;</dt>
    <dd><p>The input tensor.</p>
</dd>
  </dl>

  <h4 class="section">Returns</h4>
  <dl class="parameters">
    <dt><a class="xref" href="AiDotNet.Tensors.LinearAlgebra.Tensor-1.html">Tensor</a>&lt;T&gt;</dt>
    <dd><p>A new tensor containing derivatives for each input element.</p>
</dd>
  </dl>











  <a id="AiDotNet_Interfaces_IActivationFunction_1_Derivative_" data-uid="AiDotNet.Interfaces.IActivationFunction`1.Derivative*"></a>

  <h3 id="AiDotNet_Interfaces_IActivationFunction_1_Derivative_AiDotNet_Tensors_LinearAlgebra_Vector__0__" data-uid="AiDotNet.Interfaces.IActivationFunction`1.Derivative(AiDotNet.Tensors.LinearAlgebra.Vector{`0})">
  Derivative(Vector&lt;T&gt;)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/Interfaces/IActivationFunction.cs/#L80"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Calculates the derivative matrix for a vector input.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">Matrix&lt;T&gt; Derivative(Vector&lt;T&gt; input)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>input</code> <a class="xref" href="AiDotNet.Tensors.LinearAlgebra.Vector-1.html">Vector</a>&lt;T&gt;</dt>
    <dd><p>The input vector.</p>
</dd>
  </dl>

  <h4 class="section">Returns</h4>
  <dl class="parameters">
    <dt><a class="xref" href="AiDotNet.Tensors.LinearAlgebra.Matrix-1.html">Matrix</a>&lt;T&gt;</dt>
    <dd><p>A diagonal matrix containing derivatives for each input element.</p>
</dd>
  </dl>











  <a id="AiDotNet_Interfaces_IActivationFunction_1_Derivative_" data-uid="AiDotNet.Interfaces.IActivationFunction`1.Derivative*"></a>

  <h3 id="AiDotNet_Interfaces_IActivationFunction_1_Derivative__0_" data-uid="AiDotNet.Interfaces.IActivationFunction`1.Derivative(`0)">
  Derivative(T)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/Interfaces/IActivationFunction.cs/#L66"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Calculates the derivative (slope) of the activation function at the given input value.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">T Derivative(T input)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>input</code> <span class="xref">T</span></dt>
    <dd><p>The input value at which to calculate the derivative.</p>
</dd>
  </dl>

  <h4 class="section">Returns</h4>
  <dl class="parameters">
    <dt><span class="xref">T</span></dt>
    <dd><p>The derivative value of the activation function at the input point.</p>
</dd>
  </dl>







  <h4 class="section" id="AiDotNet_Interfaces_IActivationFunction_1_Derivative__0__remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p><b>For Beginners:</b> The derivative tells us how quickly the activation function's output
changes when we make a small change to the input.</p>
<p>Think of it as the &quot;slope&quot; or &quot;steepness&quot; at a particular point on the activation function's curve.</p>
<p>This is crucial for training neural networks because:</p>
<ul>
<li>It helps determine how much to adjust the network's weights during learning</li>
<li>A higher derivative means a stronger signal for learning</li>
<li>A derivative of zero means no learning signal (which can be a problem known as &quot;vanishing gradient&quot;)</li>
</ul>
<p>During training, the neural network uses this derivative to figure out how to adjust
its internal parameters to improve its predictions.</p>
</div>




  <a id="AiDotNet_Interfaces_IActivationFunction_1_ForwardGpu_" data-uid="AiDotNet.Interfaces.IActivationFunction`1.ForwardGpu*"></a>

  <h3 id="AiDotNet_Interfaces_IActivationFunction_1_ForwardGpu_AiDotNet_Tensors_Engines_DirectGpu_IDirectGpuBackend_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_System_Int32_" data-uid="AiDotNet.Interfaces.IActivationFunction`1.ForwardGpu(AiDotNet.Tensors.Engines.DirectGpu.IDirectGpuBackend,AiDotNet.Tensors.Engines.DirectGpu.IGpuBuffer,AiDotNet.Tensors.Engines.DirectGpu.IGpuBuffer,System.Int32)">
  ForwardGpu(IDirectGpuBackend, IGpuBuffer, IGpuBuffer, int)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/Interfaces/IActivationFunction.cs/#L192"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Applies the activation function on GPU.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">void ForwardGpu(IDirectGpuBackend backend, IGpuBuffer input, IGpuBuffer output, int size)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>backend</code> <a class="xref" href="AiDotNet.Tensors.Engines.DirectGpu.IDirectGpuBackend.html">IDirectGpuBackend</a></dt>
    <dd><p>The GPU backend to use for execution.</p>
</dd>
    <dt><code>input</code> <a class="xref" href="AiDotNet.Tensors.Engines.DirectGpu.IGpuBuffer.html">IGpuBuffer</a></dt>
    <dd><p>The input GPU buffer.</p>
</dd>
    <dt><code>output</code> <a class="xref" href="AiDotNet.Tensors.Engines.DirectGpu.IGpuBuffer.html">IGpuBuffer</a></dt>
    <dd><p>The output GPU buffer to store the activated values.</p>
</dd>
    <dt><code>size</code> <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.int32">int</a></dt>
    <dd><p>The number of elements to process.</p>
</dd>
  </dl>








  <h4 class="section" id="AiDotNet_Interfaces_IActivationFunction_1_ForwardGpu_AiDotNet_Tensors_Engines_DirectGpu_IDirectGpuBackend_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_System_Int32__remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
This method applies the activation function entirely on GPU, avoiding CPU-GPU data transfers.
The input and output buffers may be the same for in-place operations if supported.
</p>
<p>
<b>For Beginners:</b> This is the GPU-accelerated version of the Activate method.
Instead of processing data on the CPU, this runs thousands of calculations in parallel
on the GPU, making it much faster for large tensors.
</p>
</div>

  <h4 class="section">Exceptions</h4>
  <dl class="parameters">
    <dt><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.notsupportedexception">NotSupportedException</a></dt>
    <dd><p>Thrown if SupportsGpuTraining is false.</p>
</dd>
  </dl>




</article>

        <div class="contribution d-print-none">
          <a href="https://github.com/ooples/AiDotNet/blob/master/src/Interfaces/IActivationFunction.cs/#L27" class="edit-link">Edit this page</a>
        </div>


      </div>

      <div class="affix">
        <nav id="affix"></nav>
      </div>
    </main>

    <div class="container-xxl search-results" id="search-results"></div>

    <footer class="border-top text-secondary">
      <div class="container-xxl">
        <div class="flex-fill">
          AiDotNet - Enterprise AI/ML Library for .NET
        </div>
      </div>
    </footer>
  </body>
</html>
