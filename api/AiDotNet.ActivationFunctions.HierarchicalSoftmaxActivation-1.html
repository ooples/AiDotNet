<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
      <title>Class HierarchicalSoftmaxActivation&lt;T&gt; | AiDotNet Documentation </title>
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <meta name="title" content="Class HierarchicalSoftmaxActivation&lt;T&gt; | AiDotNet Documentation ">
      
      <meta name="description" content="Implements the Hierarchical Softmax activation function for neural networks.">
      <link rel="icon" href="../favicon.ico">
      <link rel="stylesheet" href="../public/docfx.min.css">
      <link rel="stylesheet" href="../public/main.css">
      <meta name="docfx:navrel" content="../toc.html">
      <meta name="docfx:tocrel" content="toc.html">
      
      <meta name="docfx:rel" content="../">
      
      
      <meta name="docfx:docurl" content="https://github.com/ooples/AiDotNet/new/master/apiSpec/new?filename=AiDotNet_ActivationFunctions_HierarchicalSoftmaxActivation_1.md&amp;value=---%0Auid%3A%20AiDotNet.ActivationFunctions.HierarchicalSoftmaxActivation%601%0Asummary%3A%20&#39;*You%20can%20override%20summary%20for%20the%20API%20here%20using%20*MARKDOWN*%20syntax&#39;%0A---%0A%0A*Please%20type%20below%20more%20information%20about%20this%20API%3A*%0A%0A">
      <meta name="loc:inThisArticle" content="In this article">
      <meta name="loc:searchResultsCount" content="{count} results for &quot;{query}&quot;">
      <meta name="loc:searchNoResults" content="No results for &quot;{query}&quot;">
      <meta name="loc:tocFilter" content="Filter by title">
      <meta name="loc:nextArticle" content="Next">
      <meta name="loc:prevArticle" content="Previous">
      <meta name="loc:themeLight" content="Light">
      <meta name="loc:themeDark" content="Dark">
      <meta name="loc:themeAuto" content="Auto">
      <meta name="loc:changeTheme" content="Change theme">
      <meta name="loc:copy" content="Copy">
      <meta name="loc:downloadPdf" content="Download PDF">

      <script type="module" src="./../public/docfx.min.js"></script>

      <script>
        const theme = localStorage.getItem('theme') || 'auto'
        document.documentElement.setAttribute('data-bs-theme', theme === 'auto' ? (window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light') : theme)
      </script>

  </head>

  <body class="tex2jax_ignore" data-layout="" data-yaml-mime="ManagedReference">
    <header class="bg-body border-bottom">
      <nav id="autocollapse" class="navbar navbar-expand-md" role="navigation">
        <div class="container-xxl flex-nowrap">
          <a class="navbar-brand" href="../index.html">
            <img id="logo" class="svg" src="../logo.svg" alt="AiDotNet">
            AiDotNet
          </a>
          <button class="btn btn-lg d-md-none border-0" type="button" data-bs-toggle="collapse" data-bs-target="#navpanel" aria-controls="navpanel" aria-expanded="false" aria-label="Toggle navigation">
            <i class="bi bi-three-dots"></i>
          </button>
          <div class="collapse navbar-collapse" id="navpanel">
            <div id="navbar">
              <form class="search" role="search" id="search">
                <i class="bi bi-search"></i>
                <input class="form-control" id="search-query" type="search" disabled placeholder="Search" autocomplete="off" aria-label="Search">
              </form>
            </div>
          </div>
        </div>
      </nav>
    </header>

    <main class="container-xxl">
      <div class="toc-offcanvas">
        <div class="offcanvas-md offcanvas-start" tabindex="-1" id="tocOffcanvas" aria-labelledby="tocOffcanvasLabel">
          <div class="offcanvas-header">
            <h5 class="offcanvas-title" id="tocOffcanvasLabel">Table of Contents</h5>
            <button type="button" class="btn-close" data-bs-dismiss="offcanvas" data-bs-target="#tocOffcanvas" aria-label="Close"></button>
          </div>
          <div class="offcanvas-body">
            <nav class="toc" id="toc"></nav>
          </div>
        </div>
      </div>

      <div class="content">
        <div class="actionbar">
          <button class="btn btn-lg border-0 d-md-none" type="button" data-bs-toggle="offcanvas" data-bs-target="#tocOffcanvas" aria-controls="tocOffcanvas" aria-expanded="false" aria-label="Show table of contents">
            <i class="bi bi-list"></i>
          </button>

          <nav id="breadcrumb"></nav>
        </div>

        <article data-uid="AiDotNet.ActivationFunctions.HierarchicalSoftmaxActivation`1">



  <h1 id="AiDotNet_ActivationFunctions_HierarchicalSoftmaxActivation_1" data-uid="AiDotNet.ActivationFunctions.HierarchicalSoftmaxActivation`1" class="text-break">
Class HierarchicalSoftmaxActivation&lt;T&gt;  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/ActivationFunctions/HierarchicalSoftmaxActivation.cs/#L30"><i class="bi bi-code-slash"></i></a>
  </h1>

  <div class="facts text-secondary">
    <dl><dt>Namespace</dt><dd><a class="xref" href="AiDotNet.html">AiDotNet</a>.<a class="xref" href="AiDotNet.ActivationFunctions.html">ActivationFunctions</a></dd></dl>
  <dl><dt>Assembly</dt><dd>AiDotNet.dll</dd></dl>
  </div>

  <div class="markdown summary"><p>Implements the Hierarchical Softmax activation function for neural networks.</p>
</div>
  <div class="markdown conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public class HierarchicalSoftmaxActivation&lt;T&gt; : ActivationFunctionBase&lt;T&gt;, IActivationFunction&lt;T&gt;, IVectorActivationFunction&lt;T&gt;</code></pre>
  </div>



  <h4 class="section">Type Parameters</h4>
  <dl class="parameters">
    <dt><code>T</code></dt>
    <dd><p>The numeric type used for calculations (e.g., float, double).</p>
</dd>
  </dl>

  <dl class="typelist inheritance">
    <dt>Inheritance</dt>
    <dd>
      <div><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object">object</a></div>
      <div><a class="xref" href="AiDotNet.ActivationFunctions.ActivationFunctionBase-1.html">ActivationFunctionBase</a>&lt;T&gt;</div>
      <div><span class="xref">HierarchicalSoftmaxActivation&lt;T&gt;</span></div>
    </dd>
  </dl>

  <dl class="typelist implements">
    <dt>Implements</dt>
    <dd>
      <div><a class="xref" href="AiDotNet.Interfaces.IActivationFunction-1.html">IActivationFunction</a>&lt;T&gt;</div>
      <div><a class="xref" href="AiDotNet.Interfaces.IVectorActivationFunction-1.html">IVectorActivationFunction</a>&lt;T&gt;</div>
    </dd>
  </dl>


  <dl class="typelist inheritedMembers">
    <dt>Inherited Members</dt>
    <dd>
    <div>
      <a class="xref" href="AiDotNet.ActivationFunctions.ActivationFunctionBase-1.html#AiDotNet_ActivationFunctions_ActivationFunctionBase_1_NumOps">ActivationFunctionBase&lt;T&gt;.NumOps</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.ActivationFunctions.ActivationFunctionBase-1.html#AiDotNet_ActivationFunctions_ActivationFunctionBase_1_Engine">ActivationFunctionBase&lt;T&gt;.Engine</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.ActivationFunctions.ActivationFunctionBase-1.html#AiDotNet_ActivationFunctions_ActivationFunctionBase_1_Activate__0_">ActivationFunctionBase&lt;T&gt;.Activate(T)</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.ActivationFunctions.ActivationFunctionBase-1.html#AiDotNet_ActivationFunctions_ActivationFunctionBase_1_Derivative__0_">ActivationFunctionBase&lt;T&gt;.Derivative(T)</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.ActivationFunctions.ActivationFunctionBase-1.html#AiDotNet_ActivationFunctions_ActivationFunctionBase_1_Activate_AiDotNet_Tensors_LinearAlgebra_Tensor__0__">ActivationFunctionBase&lt;T&gt;.Activate(Tensor&lt;T&gt;)</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.ActivationFunctions.ActivationFunctionBase-1.html#AiDotNet_ActivationFunctions_ActivationFunctionBase_1_Derivative_AiDotNet_Tensors_LinearAlgebra_Tensor__0__">ActivationFunctionBase&lt;T&gt;.Derivative(Tensor&lt;T&gt;)</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.ActivationFunctions.ActivationFunctionBase-1.html#AiDotNet_ActivationFunctions_ActivationFunctionBase_1_Backward_AiDotNet_Tensors_LinearAlgebra_Tensor__0__AiDotNet_Tensors_LinearAlgebra_Tensor__0__">ActivationFunctionBase&lt;T&gt;.Backward(Tensor&lt;T&gt;, Tensor&lt;T&gt;)</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.ActivationFunctions.ActivationFunctionBase-1.html#AiDotNet_ActivationFunctions_ActivationFunctionBase_1_SupportsGpuTraining">ActivationFunctionBase&lt;T&gt;.SupportsGpuTraining</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.ActivationFunctions.ActivationFunctionBase-1.html#AiDotNet_ActivationFunctions_ActivationFunctionBase_1_ForwardGpu_AiDotNet_Tensors_Engines_DirectGpu_IDirectGpuBackend_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_System_Int32_">ActivationFunctionBase&lt;T&gt;.ForwardGpu(IDirectGpuBackend, IGpuBuffer, IGpuBuffer, int)</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.ActivationFunctions.ActivationFunctionBase-1.html#AiDotNet_ActivationFunctions_ActivationFunctionBase_1_BackwardGpu_AiDotNet_Tensors_Engines_DirectGpu_IDirectGpuBackend_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_System_Int32_">ActivationFunctionBase&lt;T&gt;.BackwardGpu(IDirectGpuBackend, IGpuBuffer, IGpuBuffer, IGpuBuffer, IGpuBuffer, int)</a>
    </div>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.equals#system-object-equals(system-object)">object.Equals(object)</a>
    </div>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.equals#system-object-equals(system-object-system-object)">object.Equals(object, object)</a>
    </div>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.gethashcode">object.GetHashCode()</a>
    </div>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.gettype">object.GetType()</a>
    </div>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.memberwiseclone">object.MemberwiseClone()</a>
    </div>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.referenceequals">object.ReferenceEquals(object, object)</a>
    </div>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.tostring">object.ToString()</a>
    </div>
  </dd></dl>




  <h2 id="AiDotNet_ActivationFunctions_HierarchicalSoftmaxActivation_1_remarks">Remarks</h2>
  <div class="markdown level0 remarks"><p>
<b>For Beginners:</b> Hierarchical Softmax is an efficient alternative to the standard Softmax function,
especially when dealing with a large number of output classes (like thousands of words in language models).
<p>While regular Softmax calculates probabilities for all possible classes at once (which can be slow
with many classes), Hierarchical Softmax organizes classes in a tree structure:</p>
<ul>
<li>Think of it like a &quot;20 Questions&quot; game where each question narrows down the possibilities</li>
<li>Each node in the tree represents a binary decision (left or right)</li>
<li>The final probability is calculated by multiplying probabilities along the path to a class</li>
</ul>
<p>This approach reduces computation from O(N) to O(log N), where N is the number of classes,
making it much faster for problems with many output classes.</p>
<p>Common uses include:</p>
<ul>
<li>Natural language processing (predicting words from vocabularies)</li>
<li>Classification problems with many categories</li>
<li>Any task where computing standard Softmax would be too slow</li>
</ul>

</div>


  <h2 class="section" id="constructors">Constructors
</h2>


  <a id="AiDotNet_ActivationFunctions_HierarchicalSoftmaxActivation_1__ctor_" data-uid="AiDotNet.ActivationFunctions.HierarchicalSoftmaxActivation`1.#ctor*"></a>

  <h3 id="AiDotNet_ActivationFunctions_HierarchicalSoftmaxActivation_1__ctor_System_Int32_" data-uid="AiDotNet.ActivationFunctions.HierarchicalSoftmaxActivation`1.#ctor(System.Int32)">
  HierarchicalSoftmaxActivation(int)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/ActivationFunctions/HierarchicalSoftmaxActivation.cs/#L77"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Initializes a new instance of the Hierarchical Softmax activation function.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public HierarchicalSoftmaxActivation(int numClasses)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>numClasses</code> <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.int32">int</a></dt>
    <dd><p>The number of output classes to support.</p>
</dd>
  </dl>








  <h4 class="section" id="AiDotNet_ActivationFunctions_HierarchicalSoftmaxActivation_1__ctor_System_Int32__remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
<b>For Beginners:</b> This constructor sets up the hierarchical structure needed for efficient probability calculation.
<p>It creates a binary tree where:</p>
<ul>
<li>The number of levels (tree depth) is calculated based on the number of classes</li>
<li>Each node in the tree gets its own set of weights</li>
<li>Weights are initialized randomly to start the learning process</li>
</ul>
<p>For example, if you have 8 classes, it creates a 3-level tree (because 2Â³=8),
allowing the model to make 3 binary decisions to reach any of the 8 classes.</p>

</div>




  <h2 class="section" id="properties">Properties
</h2>


  <a id="AiDotNet_ActivationFunctions_HierarchicalSoftmaxActivation_1_NodeWeightsTensor_" data-uid="AiDotNet.ActivationFunctions.HierarchicalSoftmaxActivation`1.NodeWeightsTensor*"></a>

  <h3 id="AiDotNet_ActivationFunctions_HierarchicalSoftmaxActivation_1_NodeWeightsTensor" data-uid="AiDotNet.ActivationFunctions.HierarchicalSoftmaxActivation`1.NodeWeightsTensor">
  NodeWeightsTensor
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/ActivationFunctions/HierarchicalSoftmaxActivation.cs/#L58"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Gets the node weights as a tensor for use in computation graphs.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public Tensor&lt;T&gt; NodeWeightsTensor { get; }</code></pre>
  </div>





  <h4 class="section">Property Value</h4>
  <dl class="parameters">
    <dt><a class="xref" href="AiDotNet.Tensors.LinearAlgebra.Tensor-1.html">Tensor</a>&lt;T&gt;</dt>
    <dd><p>A tensor containing the node weights with shape [treeDepth, numClasses].</p>
</dd>
  </dl>




  <h4 class="section" id="AiDotNet_ActivationFunctions_HierarchicalSoftmaxActivation_1_NodeWeightsTensor_remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
<b>For Beginners:</b> This property provides access to the internal weights used by the hierarchical
tree structure. When using JIT compilation, you can wrap these weights in a ComputationNode
to enable gradient computation and weight updates during training.
</p>
</div>




  <a id="AiDotNet_ActivationFunctions_HierarchicalSoftmaxActivation_1_SupportsJitCompilation_" data-uid="AiDotNet.ActivationFunctions.HierarchicalSoftmaxActivation`1.SupportsJitCompilation*"></a>

  <h3 id="AiDotNet_ActivationFunctions_HierarchicalSoftmaxActivation_1_SupportsJitCompilation" data-uid="AiDotNet.ActivationFunctions.HierarchicalSoftmaxActivation`1.SupportsJitCompilation">
  SupportsJitCompilation
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/ActivationFunctions/HierarchicalSoftmaxActivation.cs/#L259"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Gets whether this activation function supports JIT compilation.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public override bool SupportsJitCompilation { get; }</code></pre>
  </div>





  <h4 class="section">Property Value</h4>
  <dl class="parameters">
    <dt><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.boolean">bool</a></dt>
    <dd><p>True because TensorOperations.HierarchicalSoftmax provides full forward and backward pass support.</p>
</dd>
  </dl>




  <h4 class="section" id="AiDotNet_ActivationFunctions_HierarchicalSoftmaxActivation_1_SupportsJitCompilation_remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
HierarchicalSoftmax supports JIT compilation with gradient computation through the binary tree structure.
The backward pass computes gradients for both the input and the node weights, enabling end-to-end training.
</p>
<p>
The node weights are exposed via <a class="xref" href="AiDotNet.ActivationFunctions.HierarchicalSoftmaxActivation-1.html#AiDotNet_ActivationFunctions_HierarchicalSoftmaxActivation_1_NodeWeightsTensor">NodeWeightsTensor</a> for use in computation graphs.
For training, wrap the weights in a ComputationNode to track gradients.
</p>
</div>




  <h2 class="section" id="methods">Methods
</h2>


  <a id="AiDotNet_ActivationFunctions_HierarchicalSoftmaxActivation_1_Activate_" data-uid="AiDotNet.ActivationFunctions.HierarchicalSoftmaxActivation`1.Activate*"></a>

  <h3 id="AiDotNet_ActivationFunctions_HierarchicalSoftmaxActivation_1_Activate_AiDotNet_Tensors_LinearAlgebra_Vector__0__" data-uid="AiDotNet.ActivationFunctions.HierarchicalSoftmaxActivation`1.Activate(AiDotNet.Tensors.LinearAlgebra.Vector{`0})">
  Activate(Vector&lt;T&gt;)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/ActivationFunctions/HierarchicalSoftmaxActivation.cs/#L110"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Applies the Hierarchical Softmax activation function to transform input vectors into class probabilities.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public override Vector&lt;T&gt; Activate(Vector&lt;T&gt; input)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>input</code> <a class="xref" href="AiDotNet.Tensors.LinearAlgebra.Vector-1.html">Vector</a>&lt;T&gt;</dt>
    <dd><p>The input vector to transform.</p>
</dd>
  </dl>

  <h4 class="section">Returns</h4>
  <dl class="parameters">
    <dt><a class="xref" href="AiDotNet.Tensors.LinearAlgebra.Vector-1.html">Vector</a>&lt;T&gt;</dt>
    <dd><p>A vector containing probabilities for each output class.</p>
</dd>
  </dl>







  <h4 class="section" id="AiDotNet_ActivationFunctions_HierarchicalSoftmaxActivation_1_Activate_AiDotNet_Tensors_LinearAlgebra_Vector__0___remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
<b>For Beginners:</b> This method converts your neural network's raw output (numbers) into probabilities
that sum to approximately 1, making them suitable for classification tasks.
<p>For each possible class:</p>
<ol>
<li>It traces a unique path through the binary tree</li>
<li>At each node, it calculates the probability of going left or right</li>
<li>It multiplies these probabilities to get the final probability for that class</li>
</ol>
<p>Unlike standard Softmax which computes all classes at once, this method calculates
each class probability independently by following its path through the tree.</p>

</div>




  <a id="AiDotNet_ActivationFunctions_HierarchicalSoftmaxActivation_1_ApplyToGraph_" data-uid="AiDotNet.ActivationFunctions.HierarchicalSoftmaxActivation`1.ApplyToGraph*"></a>

  <h3 id="AiDotNet_ActivationFunctions_HierarchicalSoftmaxActivation_1_ApplyToGraph_AiDotNet_Autodiff_ComputationNode__0__" data-uid="AiDotNet.ActivationFunctions.HierarchicalSoftmaxActivation`1.ApplyToGraph(AiDotNet.Autodiff.ComputationNode{`0})">
  ApplyToGraph(ComputationNode&lt;T&gt;)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/ActivationFunctions/HierarchicalSoftmaxActivation.cs/#L278"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Applies this activation function to a computation graph node.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public override ComputationNode&lt;T&gt; ApplyToGraph(ComputationNode&lt;T&gt; input)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>input</code> <a class="xref" href="AiDotNet.Autodiff.ComputationNode-1.html">ComputationNode</a>&lt;T&gt;</dt>
    <dd><p>The computation node to apply the activation to.</p>
</dd>
  </dl>

  <h4 class="section">Returns</h4>
  <dl class="parameters">
    <dt><a class="xref" href="AiDotNet.Autodiff.ComputationNode-1.html">ComputationNode</a>&lt;T&gt;</dt>
    <dd><p>A new computation node with HierarchicalSoftmax activation applied.</p>
</dd>
  </dl>







  <h4 class="section" id="AiDotNet_ActivationFunctions_HierarchicalSoftmaxActivation_1_ApplyToGraph_AiDotNet_Autodiff_ComputationNode__0___remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
This method maps to TensorOperations&lt;T&gt;.HierarchicalSoftmax which handles both
forward and backward passes for JIT compilation.
</p>
<p>
The internal node weights are wrapped in a ComputationNode to enable gradient tracking.
For full training support with weight updates, use <a class="xref" href="AiDotNet.ActivationFunctions.HierarchicalSoftmaxActivation-1.html#AiDotNet_ActivationFunctions_HierarchicalSoftmaxActivation_1_ApplyToGraph_AiDotNet_Autodiff_ComputationNode__0__AiDotNet_Autodiff_ComputationNode__0__">ApplyToGraph(ComputationNode&lt;T&gt;, ComputationNode&lt;T&gt;)</a>
with externally managed weights.
</p>
</div>

  <h4 class="section">Exceptions</h4>
  <dl class="parameters">
    <dt><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.argumentnullexception">ArgumentNullException</a></dt>
    <dd><p>Thrown if input is null.</p>
</dd>
  </dl>



  <a id="AiDotNet_ActivationFunctions_HierarchicalSoftmaxActivation_1_ApplyToGraph_" data-uid="AiDotNet.ActivationFunctions.HierarchicalSoftmaxActivation`1.ApplyToGraph*"></a>

  <h3 id="AiDotNet_ActivationFunctions_HierarchicalSoftmaxActivation_1_ApplyToGraph_AiDotNet_Autodiff_ComputationNode__0__AiDotNet_Autodiff_ComputationNode__0__" data-uid="AiDotNet.ActivationFunctions.HierarchicalSoftmaxActivation`1.ApplyToGraph(AiDotNet.Autodiff.ComputationNode{`0},AiDotNet.Autodiff.ComputationNode{`0})">
  ApplyToGraph(ComputationNode&lt;T&gt;, ComputationNode&lt;T&gt;)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/ActivationFunctions/HierarchicalSoftmaxActivation.cs/#L309"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Applies Hierarchical Softmax with externally provided weights for full training support.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public ComputationNode&lt;T&gt; ApplyToGraph(ComputationNode&lt;T&gt; input, ComputationNode&lt;T&gt; nodeWeights)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>input</code> <a class="xref" href="AiDotNet.Autodiff.ComputationNode-1.html">ComputationNode</a>&lt;T&gt;</dt>
    <dd><p>The computation node containing the input features.</p>
</dd>
    <dt><code>nodeWeights</code> <a class="xref" href="AiDotNet.Autodiff.ComputationNode-1.html">ComputationNode</a>&lt;T&gt;</dt>
    <dd><p>The computation node containing the tree node weights.</p>
</dd>
  </dl>

  <h4 class="section">Returns</h4>
  <dl class="parameters">
    <dt><a class="xref" href="AiDotNet.Autodiff.ComputationNode-1.html">ComputationNode</a>&lt;T&gt;</dt>
    <dd><p>A new computation node with HierarchicalSoftmax activation applied.</p>
</dd>
  </dl>







  <h4 class="section" id="AiDotNet_ActivationFunctions_HierarchicalSoftmaxActivation_1_ApplyToGraph_AiDotNet_Autodiff_ComputationNode__0__AiDotNet_Autodiff_ComputationNode__0___remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
<b>For Beginners:</b> Use this overload when you want to train the hierarchical softmax weights
as part of your model. By providing the weights as a ComputationNode, gradients will flow
through them during backpropagation, allowing the optimizer to update them.
</p>
<p>
Example usage:
<pre><code class="lang-csharp">var weightsNode = new ComputationNode&lt;float&gt;(activation.NodeWeightsTensor, requiresGrad: true);
var output = activation.ApplyToGraph(input, weightsNode);</code></pre>

</div>

  <h4 class="section">Exceptions</h4>
  <dl class="parameters">
    <dt><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.argumentnullexception">ArgumentNullException</a></dt>
    <dd><p>Thrown if input or nodeWeights is null.</p>
</dd>
  </dl>



  <a id="AiDotNet_ActivationFunctions_HierarchicalSoftmaxActivation_1_Derivative_" data-uid="AiDotNet.ActivationFunctions.HierarchicalSoftmaxActivation`1.Derivative*"></a>

  <h3 id="AiDotNet_ActivationFunctions_HierarchicalSoftmaxActivation_1_Derivative_AiDotNet_Tensors_LinearAlgebra_Vector__0__" data-uid="AiDotNet.ActivationFunctions.HierarchicalSoftmaxActivation`1.Derivative(AiDotNet.Tensors.LinearAlgebra.Vector{`0})">
  Derivative(Vector&lt;T&gt;)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/ActivationFunctions/HierarchicalSoftmaxActivation.cs/#L140"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Calculates the derivative (gradient) of the Hierarchical Softmax function.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public override Matrix&lt;T&gt; Derivative(Vector&lt;T&gt; input)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>input</code> <a class="xref" href="AiDotNet.Tensors.LinearAlgebra.Vector-1.html">Vector</a>&lt;T&gt;</dt>
    <dd><p>The input vector at which to calculate the derivative.</p>
</dd>
  </dl>

  <h4 class="section">Returns</h4>
  <dl class="parameters">
    <dt><a class="xref" href="AiDotNet.Tensors.LinearAlgebra.Matrix-1.html">Matrix</a>&lt;T&gt;</dt>
    <dd><p>A Jacobian matrix containing the partial derivatives.</p>
</dd>
  </dl>







  <h4 class="section" id="AiDotNet_ActivationFunctions_HierarchicalSoftmaxActivation_1_Derivative_AiDotNet_Tensors_LinearAlgebra_Vector__0___remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
<b>For Beginners:</b> The derivative tells us how much the output probabilities change when we slightly
change each input value. This information is essential during neural network training.
<p>This method:</p>
<ol>
<li>Creates a matrix where each row represents how one output class is affected by changes in each input</li>
<li>For each class, it calculates how changes to the input affect the probability of that class</li>
<li>These calculations help the neural network learn by adjusting weights in the right direction</li>
</ol>
<p>The &quot;Jacobian matrix&quot; is simply a collection of all these derivatives organized in rows and columns,
where each row corresponds to an output class and each column to an input dimension.</p>

</div>




  <a id="AiDotNet_ActivationFunctions_HierarchicalSoftmaxActivation_1_SupportsScalarOperations_" data-uid="AiDotNet.ActivationFunctions.HierarchicalSoftmaxActivation`1.SupportsScalarOperations*"></a>

  <h3 id="AiDotNet_ActivationFunctions_HierarchicalSoftmaxActivation_1_SupportsScalarOperations" data-uid="AiDotNet.ActivationFunctions.HierarchicalSoftmaxActivation`1.SupportsScalarOperations">
  SupportsScalarOperations()
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/ActivationFunctions/HierarchicalSoftmaxActivation.cs/#L89"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Indicates whether this activation function can operate on individual scalar values.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">protected override bool SupportsScalarOperations()</code></pre>
  </div>


  <h4 class="section">Returns</h4>
  <dl class="parameters">
    <dt><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.boolean">bool</a></dt>
    <dd><p>Always returns false as Hierarchical Softmax requires vector operations.</p>
</dd>
  </dl>












</article>

        <div class="contribution d-print-none">
          <a href="https://github.com/ooples/AiDotNet/blob/master/src/ActivationFunctions/HierarchicalSoftmaxActivation.cs/#L30" class="edit-link">Edit this page</a>
        </div>


      </div>

      <div class="affix">
        <nav id="affix"></nav>
      </div>
    </main>

    <div class="container-xxl search-results" id="search-results"></div>

    <footer class="border-top text-secondary">
      <div class="container-xxl">
        <div class="flex-fill">
          AiDotNet - Enterprise AI/ML Library for .NET
        </div>
      </div>
    </footer>
  </body>
</html>
