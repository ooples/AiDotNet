<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
      <title>Class PrecisionRecallCurveFitDetectorOptions | AiDotNet Documentation </title>
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <meta name="title" content="Class PrecisionRecallCurveFitDetectorOptions | AiDotNet Documentation ">
      
      <meta name="description" content="Configuration options for the Precision-Recall Curve Fit Detector, which evaluates model quality using precision-recall metrics particularly valuable for imbalanced classification problems.">
      <link rel="icon" href="../favicon.ico">
      <link rel="stylesheet" href="../public/docfx.min.css">
      <link rel="stylesheet" href="../public/main.css">
      <meta name="docfx:navrel" content="../toc.html">
      <meta name="docfx:tocrel" content="toc.html">
      
      <meta name="docfx:rel" content="../">
      
      
      <meta name="docfx:docurl" content="https://github.com/ooples/AiDotNet/new/master/apiSpec/new?filename=AiDotNet_Models_Options_PrecisionRecallCurveFitDetectorOptions.md&amp;value=---%0Auid%3A%20AiDotNet.Models.Options.PrecisionRecallCurveFitDetectorOptions%0Asummary%3A%20&#39;*You%20can%20override%20summary%20for%20the%20API%20here%20using%20*MARKDOWN*%20syntax&#39;%0A---%0A%0A*Please%20type%20below%20more%20information%20about%20this%20API%3A*%0A%0A">
      <meta name="loc:inThisArticle" content="In this article">
      <meta name="loc:searchResultsCount" content="{count} results for &quot;{query}&quot;">
      <meta name="loc:searchNoResults" content="No results for &quot;{query}&quot;">
      <meta name="loc:tocFilter" content="Filter by title">
      <meta name="loc:nextArticle" content="Next">
      <meta name="loc:prevArticle" content="Previous">
      <meta name="loc:themeLight" content="Light">
      <meta name="loc:themeDark" content="Dark">
      <meta name="loc:themeAuto" content="Auto">
      <meta name="loc:changeTheme" content="Change theme">
      <meta name="loc:copy" content="Copy">
      <meta name="loc:downloadPdf" content="Download PDF">

      <script type="module" src="./../public/docfx.min.js"></script>

      <script>
        const theme = localStorage.getItem('theme') || 'auto'
        document.documentElement.setAttribute('data-bs-theme', theme === 'auto' ? (window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light') : theme)
      </script>

  </head>

  <body class="tex2jax_ignore" data-layout="" data-yaml-mime="ManagedReference">
    <header class="bg-body border-bottom">
      <nav id="autocollapse" class="navbar navbar-expand-md" role="navigation">
        <div class="container-xxl flex-nowrap">
          <a class="navbar-brand" href="../index.html">
            <img id="logo" class="svg" src="../logo.svg" alt="AiDotNet">
            AiDotNet
          </a>
          <button class="btn btn-lg d-md-none border-0" type="button" data-bs-toggle="collapse" data-bs-target="#navpanel" aria-controls="navpanel" aria-expanded="false" aria-label="Toggle navigation">
            <i class="bi bi-three-dots"></i>
          </button>
          <div class="collapse navbar-collapse" id="navpanel">
            <div id="navbar">
              <form class="search" role="search" id="search">
                <i class="bi bi-search"></i>
                <input class="form-control" id="search-query" type="search" disabled placeholder="Search" autocomplete="off" aria-label="Search">
              </form>
            </div>
          </div>
        </div>
      </nav>
    </header>

    <main class="container-xxl">
      <div class="toc-offcanvas">
        <div class="offcanvas-md offcanvas-start" tabindex="-1" id="tocOffcanvas" aria-labelledby="tocOffcanvasLabel">
          <div class="offcanvas-header">
            <h5 class="offcanvas-title" id="tocOffcanvasLabel">Table of Contents</h5>
            <button type="button" class="btn-close" data-bs-dismiss="offcanvas" data-bs-target="#tocOffcanvas" aria-label="Close"></button>
          </div>
          <div class="offcanvas-body">
            <nav class="toc" id="toc"></nav>
          </div>
        </div>
      </div>

      <div class="content">
        <div class="actionbar">
          <button class="btn btn-lg border-0 d-md-none" type="button" data-bs-toggle="offcanvas" data-bs-target="#tocOffcanvas" aria-controls="tocOffcanvas" aria-expanded="false" aria-label="Show table of contents">
            <i class="bi bi-list"></i>
          </button>

          <nav id="breadcrumb"></nav>
        </div>

        <article data-uid="AiDotNet.Models.Options.PrecisionRecallCurveFitDetectorOptions">



  <h1 id="AiDotNet_Models_Options_PrecisionRecallCurveFitDetectorOptions" data-uid="AiDotNet.Models.Options.PrecisionRecallCurveFitDetectorOptions" class="text-break">
Class PrecisionRecallCurveFitDetectorOptions  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/Models/Options/PrecisionRecallCurveFitDetectorOptions.cs/#L40"><i class="bi bi-code-slash"></i></a>
  </h1>

  <div class="facts text-secondary">
    <dl><dt>Namespace</dt><dd><a class="xref" href="AiDotNet.html">AiDotNet</a>.<a class="xref" href="AiDotNet.Models.html">Models</a>.<a class="xref" href="AiDotNet.Models.Options.html">Options</a></dd></dl>
  <dl><dt>Assembly</dt><dd>AiDotNet.dll</dd></dl>
  </div>

  <div class="markdown summary"><p>Configuration options for the Precision-Recall Curve Fit Detector, which evaluates model quality
using precision-recall metrics particularly valuable for imbalanced classification problems.</p>
</div>
  <div class="markdown conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public class PrecisionRecallCurveFitDetectorOptions</code></pre>
  </div>




  <dl class="typelist inheritance">
    <dt>Inheritance</dt>
    <dd>
      <div><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object">object</a></div>
      <div><span class="xref">PrecisionRecallCurveFitDetectorOptions</span></div>
    </dd>
  </dl>



  <dl class="typelist inheritedMembers">
    <dt>Inherited Members</dt>
    <dd>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.equals#system-object-equals(system-object)">object.Equals(object)</a>
    </div>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.equals#system-object-equals(system-object-system-object)">object.Equals(object, object)</a>
    </div>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.gethashcode">object.GetHashCode()</a>
    </div>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.gettype">object.GetType()</a>
    </div>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.memberwiseclone">object.MemberwiseClone()</a>
    </div>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.referenceequals">object.ReferenceEquals(object, object)</a>
    </div>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.tostring">object.ToString()</a>
    </div>
  </dd></dl>




  <h2 id="AiDotNet_Models_Options_PrecisionRecallCurveFitDetectorOptions_remarks">Remarks</h2>
  <div class="markdown level0 remarks"><p>
The Precision-Recall Curve Fit Detector assesses model performance using metrics derived from the
precision-recall curve, which plots precision against recall at various classification thresholds.
Unlike accuracy, which can be misleading for imbalanced datasets, precision and recall metrics provide
more meaningful insights into model performance when class distributions are skewed. The Area Under
the Precision-Recall Curve (AUC-PR) and F1 Score are combined with customizable weights to produce
a composite fitness score. This detector is particularly valuable for applications where false positives
and false negatives have different implications, such as fraud detection, medical diagnosis, or anomaly
detection. The thresholds and weights configured in this class determine whether a model is considered
adequately fitted based on these metrics.
</p>
<p><b>For Beginners:</b> The Precision-Recall Curve Fit Detector helps evaluate how well your model is performing, especially when you have imbalanced data.
<p>Imagine you're building a system to detect rare events (like fraud):</p>
<ul>
<li>You might have 1,000 normal transactions for every 1 fraudulent one</li>
<li>A model that always predicts &quot;not fraud&quot; would be 99.9% accurate, but useless!</li>
<li>This is why we need better ways to evaluate models with imbalanced data</li>
</ul>
<p>Instead of simple accuracy, this detector uses two important metrics:</p>
<ol>
<li><p>Precision: When the model predicts something is positive (like fraud), how often is it correct?</p>
<ul>
<li>High precision means fewer false alarms</li>
<li>Think of it as: &quot;When the model raises an alert, how trustworthy is that alert?&quot;</li>
</ul>
</li>
<li><p>Recall: Out of all the actual positive cases, how many did the model correctly identify?</p>
<ul>
<li>High recall means fewer missed cases</li>
<li>Think of it as: &quot;What percentage of fraudulent transactions did the model catch?&quot;</li>
</ul>
</li>
</ol>
<p>The precision-recall curve shows the trade-off between these metrics at different thresholds.
This class lets you configure how the detector evaluates model quality based on these metrics.</p>

</div>


  <h2 class="section" id="properties">Properties
</h2>


  <a id="AiDotNet_Models_Options_PrecisionRecallCurveFitDetectorOptions_AreaUnderCurveThreshold_" data-uid="AiDotNet.Models.Options.PrecisionRecallCurveFitDetectorOptions.AreaUnderCurveThreshold*"></a>

  <h3 id="AiDotNet_Models_Options_PrecisionRecallCurveFitDetectorOptions_AreaUnderCurveThreshold" data-uid="AiDotNet.Models.Options.PrecisionRecallCurveFitDetectorOptions.AreaUnderCurveThreshold">
  AreaUnderCurveThreshold
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/Models/Options/PrecisionRecallCurveFitDetectorOptions.cs/#L82"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Gets or sets the minimum acceptable Area Under the Precision-Recall Curve (AUC-PR) value.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public double AreaUnderCurveThreshold { get; set; }</code></pre>
  </div>





  <h4 class="section">Property Value</h4>
  <dl class="parameters">
    <dt><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.double">double</a></dt>
    <dd><p>The AUC-PR threshold, defaulting to 0.7.</p>
</dd>
  </dl>




  <h4 class="section" id="AiDotNet_Models_Options_PrecisionRecallCurveFitDetectorOptions_AreaUnderCurveThreshold_remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
This parameter defines the threshold for the Area Under the Precision-Recall Curve (AUC-PR)
below which a model may be considered underperforming. The AUC-PR measures the overall
quality of the model across all possible classification thresholds, with values ranging
from 0 to 1. Higher values indicate better performance. Unlike the ROC curve, the baseline
for the PR curve depends on the class imbalance ratio, making it particularly suitable for
imbalanced datasets. The appropriate threshold depends on the specific domain, the degree
of class imbalance, and the consequences of classification errors in the application.
</p>
<p><b>For Beginners:</b> This setting defines the minimum acceptable score for the Area Under the Precision-Recall Curve.
<p>The default value of 0.7 means:</p>
<ul>
<li>The model's AUC-PR score should be at least 0.7 (on a scale from 0 to 1)</li>
<li>Below this threshold, the model might be considered inadequate</li>
</ul>
<p>Think of AUC-PR like a student's overall test score:</p>
<ul>
<li>1.0 is a perfect score (the model perfectly separates positive and negative cases)</li>
<li>0.5 might be mediocre performance (for moderately imbalanced datasets)</li>
<li>0.0 is terrible performance (the model gets everything wrong)</li>
<li>The default threshold of 0.7 is like requiring at least a &quot;C&quot; grade</li>
</ul>
<p>You might want a higher threshold (like 0.8 or 0.9):</p>
<ul>
<li>For critical applications where mistakes are costly</li>
<li>When you have high-quality data that should enable better models</li>
<li>When previous models have consistently achieved higher scores</li>
</ul>
<p>You might accept a lower threshold (like 0.6 or 0.5):</p>
<ul>
<li>For extremely imbalanced datasets where even good models have lower AUC-PR</li>
<li>In early stages of model development</li>
<li>When the problem is inherently difficult to predict</li>
</ul>
<p>Note: Unlike accuracy, AUC-PR accounts for class imbalance, so a &quot;good&quot; score
depends on your specific dataset and domain.</p>

</div>




  <a id="AiDotNet_Models_Options_PrecisionRecallCurveFitDetectorOptions_AucWeight_" data-uid="AiDotNet.Models.Options.PrecisionRecallCurveFitDetectorOptions.AucWeight*"></a>

  <h3 id="AiDotNet_Models_Options_PrecisionRecallCurveFitDetectorOptions_AucWeight" data-uid="AiDotNet.Models.Options.PrecisionRecallCurveFitDetectorOptions.AucWeight">
  AucWeight
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/Models/Options/PrecisionRecallCurveFitDetectorOptions.cs/#L165"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Gets or sets the weight applied to the Area Under the Precision-Recall Curve in the composite fitness score.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public double AucWeight { get; set; }</code></pre>
  </div>





  <h4 class="section">Property Value</h4>
  <dl class="parameters">
    <dt><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.double">double</a></dt>
    <dd><p>The AUC-PR weight, defaulting to 0.6.</p>
</dd>
  </dl>




  <h4 class="section" id="AiDotNet_Models_Options_PrecisionRecallCurveFitDetectorOptions_AucWeight_remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
This parameter controls the relative importance of the Area Under the Precision-Recall Curve
in the composite fitness score. The composite score is calculated as a weighted average
of the AUC-PR and F1 Score, with AucWeight and F1ScoreWeight determining the contribution of
each metric. Higher values give more influence to the AUC-PR, which evaluates model performance
across all possible thresholds. This weighting should reflect the relative importance of
overall model quality versus performance at the specific operating point in the application context.
Note that AucWeight and F1ScoreWeight should sum to 1.0 to maintain a consistent scale.
</p>
<p><b>For Beginners:</b> This setting controls how much importance is given to the AUC-PR score when calculating the overall model quality.
<p>The default value of 0.6 means:</p>
<ul>
<li>The AUC-PR contributes 60% to the final quality score</li>
<li>The F1 Score contributes the remaining 40%</li>
</ul>
<p>Think of it like grading a student:</p>
<ul>
<li>AUC-PR is like their overall course performance across many assignments</li>
<li>F1 Score is like their performance on the final exam</li>
<li>This weight determines if you care more about consistent performance (AUC-PR) or performance at a specific threshold (F1 Score)</li>
</ul>
<p>You might want a higher AUC weight (like 0.8):</p>
<ul>
<li>When you want to reward models that perform well across many thresholds</li>
<li>When you're still exploring the best threshold to use in production</li>
<li>When you might need to adjust thresholds frequently based on changing conditions</li>
</ul>
<p>You might want a lower AUC weight (like 0.4 or 0.2):</p>
<ul>
<li>When you care more about performance at your specific operating threshold</li>
<li>When you have a fixed threshold that won't change in production</li>
<li>When optimizing for the F1 Score is more important in your application</li>
</ul>
<p>Note: AucWeight and F1ScoreWeight should add up to 1.0 to maintain a proper scale.</p>

</div>




  <a id="AiDotNet_Models_Options_PrecisionRecallCurveFitDetectorOptions_F1ScoreThreshold_" data-uid="AiDotNet.Models.Options.PrecisionRecallCurveFitDetectorOptions.F1ScoreThreshold*"></a>

  <h3 id="AiDotNet_Models_Options_PrecisionRecallCurveFitDetectorOptions_F1ScoreThreshold" data-uid="AiDotNet.Models.Options.PrecisionRecallCurveFitDetectorOptions.F1ScoreThreshold">
  F1ScoreThreshold
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/Models/Options/PrecisionRecallCurveFitDetectorOptions.cs/#L125"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Gets or sets the minimum acceptable F1 Score for the model.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public double F1ScoreThreshold { get; set; }</code></pre>
  </div>





  <h4 class="section">Property Value</h4>
  <dl class="parameters">
    <dt><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.double">double</a></dt>
    <dd><p>The F1 Score threshold, defaulting to 0.6.</p>
</dd>
  </dl>




  <h4 class="section" id="AiDotNet_Models_Options_PrecisionRecallCurveFitDetectorOptions_F1ScoreThreshold_remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
This parameter defines the threshold for the F1 Score below which a model may be considered
underperforming. The F1 Score is the harmonic mean of precision and recall, providing a
single metric that balances these two aspects of classification performance. Values range
from 0 to 1, with higher values indicating better performance. Unlike the AUC-PR, which
evaluates performance across all possible thresholds, the F1 Score is typically calculated
at a specific classification threshold (often 0.5). This makes it a more practical metric
for evaluating model performance at the operating point that will be used in production.
</p>
<p><b>For Beginners:</b> This setting defines the minimum acceptable F1 Score for your model.
<p>The default value of 0.6 means:</p>
<ul>
<li>The model's F1 Score should be at least 0.6 (on a scale from 0 to 1)</li>
<li>Below this threshold, the model might be considered inadequate</li>
</ul>
<p>The F1 Score combines precision and recall into a single number:</p>
<ul>
<li>It gives you a balanced view of both metrics</li>
<li>It's especially useful when you care about both false positives and false negatives</li>
<li>A high F1 Score means both good precision AND good recall</li>
</ul>
<p>Think of it like a balanced meal score:</p>
<ul>
<li>You need both proteins (precision) and vegetables (recall) for a healthy meal</li>
<li>The F1 Score ensures you're not just loading up on one and ignoring the other</li>
<li>A score of 1.0 means perfect precision and recall</li>
<li>A score of 0.0 means either precision or recall (or both) is terrible</li>
</ul>
<p>You might want a higher threshold (like 0.7 or 0.8):</p>
<ul>
<li>When balanced performance is critical to your application</li>
<li>In mature systems where models should achieve good results on both metrics</li>
</ul>
<p>You might accept a lower threshold (like 0.5 or 0.4):</p>
<ul>
<li>In highly imbalanced datasets where even good models have lower F1 Scores</li>
<li>When you're more concerned about one metric (precision or recall) than perfect balance</li>
<li>In early development phases</li>
</ul>

</div>




  <a id="AiDotNet_Models_Options_PrecisionRecallCurveFitDetectorOptions_F1ScoreWeight_" data-uid="AiDotNet.Models.Options.PrecisionRecallCurveFitDetectorOptions.F1ScoreWeight*"></a>

  <h3 id="AiDotNet_Models_Options_PrecisionRecallCurveFitDetectorOptions_F1ScoreWeight" data-uid="AiDotNet.Models.Options.PrecisionRecallCurveFitDetectorOptions.F1ScoreWeight">
  F1ScoreWeight
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/Models/Options/PrecisionRecallCurveFitDetectorOptions.cs/#L205"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Gets or sets the weight applied to the F1 Score in the composite fitness score.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public double F1ScoreWeight { get; set; }</code></pre>
  </div>





  <h4 class="section">Property Value</h4>
  <dl class="parameters">
    <dt><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.double">double</a></dt>
    <dd><p>The F1 Score weight, defaulting to 0.4.</p>
</dd>
  </dl>




  <h4 class="section" id="AiDotNet_Models_Options_PrecisionRecallCurveFitDetectorOptions_F1ScoreWeight_remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
This parameter controls the relative importance of the F1 Score in the composite fitness score.
The composite score is calculated as a weighted average of the AUC-PR and F1 Score, with
AucWeight and F1ScoreWeight determining the contribution of each metric. Higher values give
more influence to the F1 Score, which evaluates model performance at a specific operating point.
This weighting should reflect the relative importance of performance at the specific operating
point versus overall model quality across all thresholds. Note that AucWeight and F1ScoreWeight
should sum to 1.0 to maintain a consistent scale.
</p>
<p><b>For Beginners:</b> This setting controls how much importance is given to the F1 Score when calculating the overall model quality.
<p>The default value of 0.4 means:</p>
<ul>
<li>The F1 Score contributes 40% to the final quality score</li>
<li>The AUC-PR contributes the remaining 60%</li>
</ul>
<p>Think of it like hiring an employee:</p>
<ul>
<li>The F1 Score is like how well they perform on the specific job tasks they'll do daily</li>
<li>AUC-PR is like their overall skill set across many potential tasks</li>
<li>This weight determines if you care more about specific job performance (F1 Score) or overall capabilities (AUC-PR)</li>
</ul>
<p>You might want a higher F1 Score weight (like 0.6 or 0.8):</p>
<ul>
<li>When you have a fixed classification threshold in production</li>
<li>When you care most about performance at that specific threshold</li>
<li>When balance between precision and recall at your operating point is critical</li>
</ul>
<p>You might want a lower F1 Score weight (like 0.2):</p>
<ul>
<li>When you want to emphasize overall model quality across all thresholds</li>
<li>When you frequently adjust classification thresholds</li>
<li>When exploring different models during development</li>
</ul>
<p>Note: AucWeight and F1ScoreWeight should add up to 1.0 to maintain a proper scale.</p>

</div>





</article>

        <div class="contribution d-print-none">
          <a href="https://github.com/ooples/AiDotNet/blob/master/src/Models/Options/PrecisionRecallCurveFitDetectorOptions.cs/#L40" class="edit-link">Edit this page</a>
        </div>


      </div>

      <div class="affix">
        <nav id="affix"></nav>
      </div>
    </main>

    <div class="container-xxl search-results" id="search-results"></div>

    <footer class="border-top text-secondary">
      <div class="container-xxl">
        <div class="flex-fill">
          AiDotNet - Enterprise AI/ML Library for .NET
        </div>
      </div>
    </footer>
  </body>
</html>
