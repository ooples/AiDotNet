<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
      <title>Enum LanguageModelBackbone | AiDotNet Documentation </title>
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <meta name="title" content="Enum LanguageModelBackbone | AiDotNet Documentation ">
      
      <meta name="description" content="Defines the language model backbone types used in multimodal neural networks.">
      <link rel="icon" href="../favicon.ico">
      <link rel="stylesheet" href="../public/docfx.min.css">
      <link rel="stylesheet" href="../public/main.css">
      <meta name="docfx:navrel" content="../toc.html">
      <meta name="docfx:tocrel" content="toc.html">
      
      <meta name="docfx:rel" content="../">
      
      
      <meta name="docfx:docurl" content="https://github.com/ooples/AiDotNet/new/master/apiSpec/new?filename=AiDotNet_Enums_LanguageModelBackbone.md&amp;value=---%0Auid%3A%20AiDotNet.Enums.LanguageModelBackbone%0Asummary%3A%20&#39;*You%20can%20override%20summary%20for%20the%20API%20here%20using%20*MARKDOWN*%20syntax&#39;%0A---%0A%0A*Please%20type%20below%20more%20information%20about%20this%20API%3A*%0A%0A">
      <meta name="loc:inThisArticle" content="In this article">
      <meta name="loc:searchResultsCount" content="{count} results for &quot;{query}&quot;">
      <meta name="loc:searchNoResults" content="No results for &quot;{query}&quot;">
      <meta name="loc:tocFilter" content="Filter by title">
      <meta name="loc:nextArticle" content="Next">
      <meta name="loc:prevArticle" content="Previous">
      <meta name="loc:themeLight" content="Light">
      <meta name="loc:themeDark" content="Dark">
      <meta name="loc:themeAuto" content="Auto">
      <meta name="loc:changeTheme" content="Change theme">
      <meta name="loc:copy" content="Copy">
      <meta name="loc:downloadPdf" content="Download PDF">

      <script type="module" src="./../public/docfx.min.js"></script>

      <script>
        const theme = localStorage.getItem('theme') || 'auto'
        document.documentElement.setAttribute('data-bs-theme', theme === 'auto' ? (window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light') : theme)
      </script>

  </head>

  <body class="tex2jax_ignore" data-layout="" data-yaml-mime="ManagedReference">
    <header class="bg-body border-bottom">
      <nav id="autocollapse" class="navbar navbar-expand-md" role="navigation">
        <div class="container-xxl flex-nowrap">
          <a class="navbar-brand" href="../index.html">
            <img id="logo" class="svg" src="../logo.svg" alt="AiDotNet">
            AiDotNet
          </a>
          <button class="btn btn-lg d-md-none border-0" type="button" data-bs-toggle="collapse" data-bs-target="#navpanel" aria-controls="navpanel" aria-expanded="false" aria-label="Toggle navigation">
            <i class="bi bi-three-dots"></i>
          </button>
          <div class="collapse navbar-collapse" id="navpanel">
            <div id="navbar">
              <form class="search" role="search" id="search">
                <i class="bi bi-search"></i>
                <input class="form-control" id="search-query" type="search" disabled placeholder="Search" autocomplete="off" aria-label="Search">
              </form>
            </div>
          </div>
        </div>
      </nav>
    </header>

    <main class="container-xxl">
      <div class="toc-offcanvas">
        <div class="offcanvas-md offcanvas-start" tabindex="-1" id="tocOffcanvas" aria-labelledby="tocOffcanvasLabel">
          <div class="offcanvas-header">
            <h5 class="offcanvas-title" id="tocOffcanvasLabel">Table of Contents</h5>
            <button type="button" class="btn-close" data-bs-dismiss="offcanvas" data-bs-target="#tocOffcanvas" aria-label="Close"></button>
          </div>
          <div class="offcanvas-body">
            <nav class="toc" id="toc"></nav>
          </div>
        </div>
      </div>

      <div class="content">
        <div class="actionbar">
          <button class="btn btn-lg border-0 d-md-none" type="button" data-bs-toggle="offcanvas" data-bs-target="#tocOffcanvas" aria-controls="tocOffcanvas" aria-expanded="false" aria-label="Show table of contents">
            <i class="bi bi-list"></i>
          </button>

          <nav id="breadcrumb"></nav>
        </div>

        <article data-uid="AiDotNet.Enums.LanguageModelBackbone">




  <h1 id="AiDotNet_Enums_LanguageModelBackbone" data-uid="AiDotNet.Enums.LanguageModelBackbone" class="text-break">
Enum LanguageModelBackbone  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/Enums/LanguageModelBackbone.cs/#L31"><i class="bi bi-code-slash"></i></a>
  </h1>

  <div class="facts text-secondary">
    <dl><dt>Namespace</dt><dd><a class="xref" href="AiDotNet.html">AiDotNet</a>.<a class="xref" href="AiDotNet.Enums.html">Enums</a></dd></dl>
  <dl><dt>Assembly</dt><dd>AiDotNet.dll</dd></dl>
  </div>

  <div class="markdown summary"><p>Defines the language model backbone types used in multimodal neural networks.</p>
</div>
  <div class="markdown conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public enum LanguageModelBackbone</code></pre>
  </div>









  <h2 id="fields">Fields
</h2>
  <dl class="parameters">
    <dt id="AiDotNet_Enums_LanguageModelBackbone_Chinchilla"><code>Chinchilla = 5</code></dt>
  <dd><p>Chinchilla by DeepMind - compute-optimal language model.</p>
<p>
<b>For Beginners:</b> Chinchilla is DeepMind's language model optimized for
the right balance of model size and training data.
<p>Key characteristics:</p>
<ul>
<li>70B parameters trained on 1.4T tokens</li>
<li>Optimized for training efficiency</li>
<li>Used as backbone in Flamingo</li>
<li>Excellent multimodal learning capabilities</li>
</ul>

</dd>
  
    <dt id="AiDotNet_Enums_LanguageModelBackbone_FlanT5"><code>FlanT5 = 1</code></dt>
  <dd><p>Flan-T5 by Google - instruction-tuned T5 model.</p>
<p>
<b>For Beginners:</b> Flan-T5 is Google's T5 model fine-tuned on a mixture of
instruction-following tasks. It's an encoder-decoder model.
<p>Key characteristics:</p>
<ul>
<li>Encoder-decoder architecture</li>
<li>Excellent at following instructions</li>
<li>Better for question-answering tasks</li>
<li>Hidden dimension: 2048 for Flan-T5-XL</li>
<li>Commonly used in BLIP-2 for instruction-following variants</li>
</ul>

</dd>
  
    <dt id="AiDotNet_Enums_LanguageModelBackbone_LLaMA"><code>LLaMA = 2</code></dt>
  <dd><p>LLaMA (Large Language Model Meta AI) by Meta.</p>
<p>
<b>For Beginners:</b> LLaMA is Meta's efficient open-source language model
that achieves strong performance with fewer parameters.
<p>Key characteristics:</p>
<ul>
<li>Decoder-only architecture</li>
<li>Very efficient for its size</li>
<li>Base model for many fine-tuned variants</li>
<li>Commonly used in LLaVA</li>
<li>Available in 7B, 13B, 33B, 65B sizes</li>
</ul>

</dd>
  
    <dt id="AiDotNet_Enums_LanguageModelBackbone_Mistral"><code>Mistral = 4</code></dt>
  <dd><p>Mistral - efficient open-source language model.</p>
<p>
<b>For Beginners:</b> Mistral is a newer, highly efficient language model
that outperforms LLaMA-2 on many benchmarks despite being smaller.
<p>Key characteristics:</p>
<ul>
<li>Uses sliding window attention for efficiency</li>
<li>Strong performance at 7B parameter scale</li>
<li>Good for resource-constrained scenarios</li>
<li>Increasingly used in newer LLaVA variants</li>
</ul>

</dd>
  
    <dt id="AiDotNet_Enums_LanguageModelBackbone_OPT"><code>OPT = 0</code></dt>
  <dd><p>OPT (Open Pre-trained Transformer) by Meta AI.</p>
<p>
<b>For Beginners:</b> OPT is a family of decoder-only language models from Meta AI
that range from 125M to 175B parameters. It's commonly used in BLIP-2.
<p>Key characteristics:</p>
<ul>
<li>Decoder-only architecture (like GPT)</li>
<li>Good for general text generation</li>
<li>Available in various sizes (OPT-2.7B is common for BLIP-2)</li>
<li>Hidden dimension: 2560 for OPT-2.7B</li>
</ul>

</dd>
  
    <dt id="AiDotNet_Enums_LanguageModelBackbone_Phi"><code>Phi = 6</code></dt>
  <dd><p>Phi by Microsoft - small but capable language model.</p>
<p>
<b>For Beginners:</b> Phi models are Microsoft's small language models
that achieve impressive performance for their size.
<p>Key characteristics:</p>
<ul>
<li>Very small (1.3B to 3B parameters)</li>
<li>Trained on high-quality &quot;textbook&quot; data</li>
<li>Fast inference on limited hardware</li>
<li>Good for lightweight multimodal applications</li>
</ul>

</dd>
  
    <dt id="AiDotNet_Enums_LanguageModelBackbone_Qwen"><code>Qwen = 7</code></dt>
  <dd><p>Qwen by Alibaba - multilingual language model.</p>
<p>
<b>For Beginners:</b> Qwen is Alibaba's multilingual language model
with strong Chinese and English capabilities.
<p>Key characteristics:</p>
<ul>
<li>Strong multilingual support</li>
<li>Good for international applications</li>
<li>Available in various sizes</li>
<li>Used in Qwen-VL for vision-language tasks</li>
</ul>

</dd>
  
    <dt id="AiDotNet_Enums_LanguageModelBackbone_RoBERTa"><code>RoBERTa = 8</code></dt>
  <dd><p>RoBERTa - robustly optimized BERT-style encoder.</p>
<p>
<b>For Beginners:</b> RoBERTa is an improved BERT model that uses the same
encoder-only architecture but trains longer on more data for better results.
It is commonly used in document understanding models like LayoutLMv3.
</p>
</dd>
  
    <dt id="AiDotNet_Enums_LanguageModelBackbone_Vicuna"><code>Vicuna = 3</code></dt>
  <dd><p>Vicuna - LLaMA fine-tuned on conversational data.</p>
<p>
<b>For Beginners:</b> Vicuna is LLaMA fine-tuned on user conversations,
making it better at natural dialogue and instruction-following.
<p>Key characteristics:</p>
<ul>
<li>Based on LLaMA architecture</li>
<li>Fine-tuned for conversation</li>
<li>Better at following complex instructions</li>
<li>Popular choice for LLaVA-1.5+</li>
</ul>

</dd>
  
  </dl>


  <h2 id="AiDotNet_Enums_LanguageModelBackbone_remarks">Remarks</h2>
  <div class="markdown level0 remarks"><p>
This enum specifies which language model architecture is used as the backbone for text generation
and understanding in multimodal models like BLIP-2, LLaVA, and Flamingo. The backbone determines
the model's capacity, vocabulary, and generation capabilities.
</p>
<p><b>For Beginners:</b> Think of the language model backbone as the "brain" that processes
and generates text in vision-language models.
<p>When a model like BLIP-2 needs to describe an image or answer a question about it:</p>
<ol>
<li>The vision encoder extracts features from the image</li>
<li>The Q-Former/adapter bridges vision and language</li>
<li>The language model backbone generates the actual text response</li>
</ol>
<p>Different backbones have different strengths:</p>
<ul>
<li><b>OPT</b>: Good for general text generation, used in BLIP-2</li>
<li><b>FlanT5</b>: Better for instruction-following, used in BLIP-2</li>
<li><b>LLaMA</b>: Efficient and powerful, used in LLaVA</li>
<li><b>Vicuna</b>: LLaMA fine-tuned for conversations, used in LLaVA</li>
<li><b>Mistral</b>: Fast and efficient, newer alternative for LLaVA</li>
<li><b>Chinchilla</b>: Used in Flamingo, optimized for multimodal learning</li>
</ul>
<p>The choice affects model size, speed, and quality of text generation.</p>

</div>

</article>

        <div class="contribution d-print-none">
          <a href="https://github.com/ooples/AiDotNet/blob/master/src/Enums/LanguageModelBackbone.cs/#L31" class="edit-link">Edit this page</a>
        </div>


      </div>

      <div class="affix">
        <nav id="affix"></nav>
      </div>
    </main>

    <div class="container-xxl search-results" id="search-results"></div>

    <footer class="border-top text-secondary">
      <div class="container-xxl">
        <div class="flex-fill">
          AiDotNet - Enterprise AI/ML Library for .NET
        </div>
      </div>
    </footer>
  </body>
</html>
