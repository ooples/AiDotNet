<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
      <title>Class LeakyReLUActivation&lt;T&gt; | AiDotNet Documentation </title>
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <meta name="title" content="Class LeakyReLUActivation&lt;T&gt; | AiDotNet Documentation ">
      
      <meta name="description" content="Implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function for neural networks.">
      <link rel="icon" href="../favicon.ico">
      <link rel="stylesheet" href="../public/docfx.min.css">
      <link rel="stylesheet" href="../public/main.css">
      <meta name="docfx:navrel" content="../toc.html">
      <meta name="docfx:tocrel" content="toc.html">
      
      <meta name="docfx:rel" content="../">
      
      
      <meta name="docfx:docurl" content="https://github.com/ooples/AiDotNet/new/master/apiSpec/new?filename=AiDotNet_ActivationFunctions_LeakyReLUActivation_1.md&amp;value=---%0Auid%3A%20AiDotNet.ActivationFunctions.LeakyReLUActivation%601%0Asummary%3A%20&#39;*You%20can%20override%20summary%20for%20the%20API%20here%20using%20*MARKDOWN*%20syntax&#39;%0A---%0A%0A*Please%20type%20below%20more%20information%20about%20this%20API%3A*%0A%0A">
      <meta name="loc:inThisArticle" content="In this article">
      <meta name="loc:searchResultsCount" content="{count} results for &quot;{query}&quot;">
      <meta name="loc:searchNoResults" content="No results for &quot;{query}&quot;">
      <meta name="loc:tocFilter" content="Filter by title">
      <meta name="loc:nextArticle" content="Next">
      <meta name="loc:prevArticle" content="Previous">
      <meta name="loc:themeLight" content="Light">
      <meta name="loc:themeDark" content="Dark">
      <meta name="loc:themeAuto" content="Auto">
      <meta name="loc:changeTheme" content="Change theme">
      <meta name="loc:copy" content="Copy">
      <meta name="loc:downloadPdf" content="Download PDF">

      <script type="module" src="./../public/docfx.min.js"></script>

      <script>
        const theme = localStorage.getItem('theme') || 'auto'
        document.documentElement.setAttribute('data-bs-theme', theme === 'auto' ? (window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light') : theme)
      </script>

  </head>

  <body class="tex2jax_ignore" data-layout="" data-yaml-mime="ManagedReference">
    <header class="bg-body border-bottom">
      <nav id="autocollapse" class="navbar navbar-expand-md" role="navigation">
        <div class="container-xxl flex-nowrap">
          <a class="navbar-brand" href="../index.html">
            <img id="logo" class="svg" src="../logo.svg" alt="AiDotNet">
            AiDotNet
          </a>
          <button class="btn btn-lg d-md-none border-0" type="button" data-bs-toggle="collapse" data-bs-target="#navpanel" aria-controls="navpanel" aria-expanded="false" aria-label="Toggle navigation">
            <i class="bi bi-three-dots"></i>
          </button>
          <div class="collapse navbar-collapse" id="navpanel">
            <div id="navbar">
              <form class="search" role="search" id="search">
                <i class="bi bi-search"></i>
                <input class="form-control" id="search-query" type="search" disabled placeholder="Search" autocomplete="off" aria-label="Search">
              </form>
            </div>
          </div>
        </div>
      </nav>
    </header>

    <main class="container-xxl">
      <div class="toc-offcanvas">
        <div class="offcanvas-md offcanvas-start" tabindex="-1" id="tocOffcanvas" aria-labelledby="tocOffcanvasLabel">
          <div class="offcanvas-header">
            <h5 class="offcanvas-title" id="tocOffcanvasLabel">Table of Contents</h5>
            <button type="button" class="btn-close" data-bs-dismiss="offcanvas" data-bs-target="#tocOffcanvas" aria-label="Close"></button>
          </div>
          <div class="offcanvas-body">
            <nav class="toc" id="toc"></nav>
          </div>
        </div>
      </div>

      <div class="content">
        <div class="actionbar">
          <button class="btn btn-lg border-0 d-md-none" type="button" data-bs-toggle="offcanvas" data-bs-target="#tocOffcanvas" aria-controls="tocOffcanvas" aria-expanded="false" aria-label="Show table of contents">
            <i class="bi bi-list"></i>
          </button>

          <nav id="breadcrumb"></nav>
        </div>

        <article data-uid="AiDotNet.ActivationFunctions.LeakyReLUActivation`1">



  <h1 id="AiDotNet_ActivationFunctions_LeakyReLUActivation_1" data-uid="AiDotNet.ActivationFunctions.LeakyReLUActivation`1" class="text-break">
Class LeakyReLUActivation&lt;T&gt;  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/ActivationFunctions/LeakyReLUActivation.cs/#L27"><i class="bi bi-code-slash"></i></a>
  </h1>

  <div class="facts text-secondary">
    <dl><dt>Namespace</dt><dd><a class="xref" href="AiDotNet.html">AiDotNet</a>.<a class="xref" href="AiDotNet.ActivationFunctions.html">ActivationFunctions</a></dd></dl>
  <dl><dt>Assembly</dt><dd>AiDotNet.dll</dd></dl>
  </div>

  <div class="markdown summary"><p>Implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function for neural networks.</p>
</div>
  <div class="markdown conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public class LeakyReLUActivation&lt;T&gt; : ActivationFunctionBase&lt;T&gt;, IActivationFunction&lt;T&gt;, IVectorActivationFunction&lt;T&gt;</code></pre>
  </div>



  <h4 class="section">Type Parameters</h4>
  <dl class="parameters">
    <dt><code>T</code></dt>
    <dd><p>The numeric type used for calculations (e.g., float, double).</p>
</dd>
  </dl>

  <dl class="typelist inheritance">
    <dt>Inheritance</dt>
    <dd>
      <div><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object">object</a></div>
      <div><a class="xref" href="AiDotNet.ActivationFunctions.ActivationFunctionBase-1.html">ActivationFunctionBase</a>&lt;T&gt;</div>
      <div><span class="xref">LeakyReLUActivation&lt;T&gt;</span></div>
    </dd>
  </dl>

  <dl class="typelist implements">
    <dt>Implements</dt>
    <dd>
      <div><a class="xref" href="AiDotNet.Interfaces.IActivationFunction-1.html">IActivationFunction</a>&lt;T&gt;</div>
      <div><a class="xref" href="AiDotNet.Interfaces.IVectorActivationFunction-1.html">IVectorActivationFunction</a>&lt;T&gt;</div>
    </dd>
  </dl>


  <dl class="typelist inheritedMembers">
    <dt>Inherited Members</dt>
    <dd>
    <div>
      <a class="xref" href="AiDotNet.ActivationFunctions.ActivationFunctionBase-1.html#AiDotNet_ActivationFunctions_ActivationFunctionBase_1_NumOps">ActivationFunctionBase&lt;T&gt;.NumOps</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.ActivationFunctions.ActivationFunctionBase-1.html#AiDotNet_ActivationFunctions_ActivationFunctionBase_1_Engine">ActivationFunctionBase&lt;T&gt;.Engine</a>
    </div>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.equals#system-object-equals(system-object)">object.Equals(object)</a>
    </div>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.equals#system-object-equals(system-object-system-object)">object.Equals(object, object)</a>
    </div>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.gethashcode">object.GetHashCode()</a>
    </div>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.gettype">object.GetType()</a>
    </div>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.memberwiseclone">object.MemberwiseClone()</a>
    </div>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.referenceequals">object.ReferenceEquals(object, object)</a>
    </div>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.tostring">object.ToString()</a>
    </div>
  </dd></dl>




  <h2 id="AiDotNet_ActivationFunctions_LeakyReLUActivation_1_remarks">Remarks</h2>
  <div class="markdown level0 remarks"><p>
<b>For Beginners:</b> The Leaky ReLU activation function is a variation of the standard ReLU function.
<p>How it works:</p>
<ul>
<li>For positive inputs (x &gt; 0): It returns the input unchanged (like a straight line)</li>
<li>For negative inputs (x = 0): It returns a small fraction of the input (a * x)</li>
</ul>
<p>The main advantage of Leaky ReLU over standard ReLU is that it never completely &quot;turns off&quot;
neurons for negative inputs. Instead, it allows a small gradient to flow through, which helps
prevent the &quot;dying ReLU&quot; problem where neurons can stop learning during training.</p>
<p>Think of it like a water pipe that:</p>
<ul>
<li>Allows full flow when the input is positive</li>
<li>Allows a small &quot;leak&quot; when the input is negative (controlled by the alpha parameter)</li>
</ul>

</div>


  <h2 class="section" id="constructors">Constructors
</h2>


  <a id="AiDotNet_ActivationFunctions_LeakyReLUActivation_1__ctor_" data-uid="AiDotNet.ActivationFunctions.LeakyReLUActivation`1.#ctor*"></a>

  <h3 id="AiDotNet_ActivationFunctions_LeakyReLUActivation_1__ctor_System_Double_" data-uid="AiDotNet.ActivationFunctions.LeakyReLUActivation`1.#ctor(System.Double)">
  LeakyReLUActivation(double)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/ActivationFunctions/LeakyReLUActivation.cs/#L58"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Initializes a new instance of the Leaky ReLU activation function with the specified alpha parameter.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public LeakyReLUActivation(double alpha = 0.01)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>alpha</code> <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.double">double</a></dt>
    <dd><p>The slope coefficient for negative input values. Default value is 0.01.</p>
</dd>
  </dl>








  <h4 class="section" id="AiDotNet_ActivationFunctions_LeakyReLUActivation_1__ctor_System_Double__remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
<b>For Beginners:</b> The alpha parameter determines how much of the negative inputs "leak through":
<ul>
<li>With alpha = 0.01 (default), negative inputs are multiplied by 0.01 (reduced to 1% of their value)</li>
<li>With alpha = 0.1, negative inputs are multiplied by 0.1 (reduced to 10% of their value)</li>
<li>With alpha = 0.001, negative inputs are multiplied by 0.001 (reduced to 0.1% of their value)</li>
</ul>
<p>A larger alpha means more information flows through for negative inputs, which can help with learning
but might make the network less focused on positive features. The default value of 0.01 works well
for most applications, but you can adjust it based on your specific needs.</p>

</div>




  <h2 class="section" id="properties">Properties
</h2>


  <a id="AiDotNet_ActivationFunctions_LeakyReLUActivation_1_Alpha_" data-uid="AiDotNet.ActivationFunctions.LeakyReLUActivation`1.Alpha*"></a>

  <h3 id="AiDotNet_ActivationFunctions_LeakyReLUActivation_1_Alpha" data-uid="AiDotNet.ActivationFunctions.LeakyReLUActivation`1.Alpha">
  Alpha
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/ActivationFunctions/LeakyReLUActivation.cs/#L37"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Gets the slope coefficient for negative input values.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public T Alpha { get; }</code></pre>
  </div>





  <h4 class="section">Property Value</h4>
  <dl class="parameters">
    <dt><span class="xref">T</span></dt>
    <dd></dd>
  </dl>








  <a id="AiDotNet_ActivationFunctions_LeakyReLUActivation_1_SupportsGpuTraining_" data-uid="AiDotNet.ActivationFunctions.LeakyReLUActivation`1.SupportsGpuTraining*"></a>

  <h3 id="AiDotNet_ActivationFunctions_LeakyReLUActivation_1_SupportsGpuTraining" data-uid="AiDotNet.ActivationFunctions.LeakyReLUActivation`1.SupportsGpuTraining">
  SupportsGpuTraining
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/ActivationFunctions/LeakyReLUActivation.cs/#L257"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Gets whether LeakyReLU supports GPU-resident training.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public override bool SupportsGpuTraining { get; }</code></pre>
  </div>





  <h4 class="section">Property Value</h4>
  <dl class="parameters">
    <dt><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.boolean">bool</a></dt>
    <dd><p>True because LeakyReLU has GPU kernels for both forward and backward passes.</p>
</dd>
  </dl>








  <a id="AiDotNet_ActivationFunctions_LeakyReLUActivation_1_SupportsJitCompilation_" data-uid="AiDotNet.ActivationFunctions.LeakyReLUActivation`1.SupportsJitCompilation*"></a>

  <h3 id="AiDotNet_ActivationFunctions_LeakyReLUActivation_1_SupportsJitCompilation" data-uid="AiDotNet.ActivationFunctions.LeakyReLUActivation`1.SupportsJitCompilation">
  SupportsJitCompilation
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/ActivationFunctions/LeakyReLUActivation.cs/#L227"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Gets whether this activation function supports JIT compilation.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public override bool SupportsJitCompilation { get; }</code></pre>
  </div>





  <h4 class="section">Property Value</h4>
  <dl class="parameters">
    <dt><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.boolean">bool</a></dt>
    <dd><p>True because gradient computation is fully implemented in TensorOperations.LeakyReLU.</p>
</dd>
  </dl>




  <h4 class="section" id="AiDotNet_ActivationFunctions_LeakyReLUActivation_1_SupportsJitCompilation_remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
LeakyReLU supports JIT compilation because:
- The gradient computation (backward pass) is fully implemented in TensorOperations
- The operation uses IEngine for GPU acceleration
- It can be represented as a static computation graph node
</p>
</div>




  <h2 class="section" id="methods">Methods
</h2>


  <a id="AiDotNet_ActivationFunctions_LeakyReLUActivation_1_Activate_" data-uid="AiDotNet.ActivationFunctions.LeakyReLUActivation`1.Activate*"></a>

  <h3 id="AiDotNet_ActivationFunctions_LeakyReLUActivation_1_Activate_AiDotNet_Tensors_LinearAlgebra_Tensor__0__" data-uid="AiDotNet.ActivationFunctions.LeakyReLUActivation`1.Activate(AiDotNet.Tensors.LinearAlgebra.Tensor{`0})">
  Activate(Tensor&lt;T&gt;)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/ActivationFunctions/LeakyReLUActivation.cs/#L180"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Applies the Leaky ReLU activation function to each element in a tensor.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public override Tensor&lt;T&gt; Activate(Tensor&lt;T&gt; input)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>input</code> <a class="xref" href="AiDotNet.Tensors.LinearAlgebra.Tensor-1.html">Tensor</a>&lt;T&gt;</dt>
    <dd><p>The input tensor to activate.</p>
</dd>
  </dl>

  <h4 class="section">Returns</h4>
  <dl class="parameters">
    <dt><a class="xref" href="AiDotNet.Tensors.LinearAlgebra.Tensor-1.html">Tensor</a>&lt;T&gt;</dt>
    <dd><p>A new tensor with the Leaky ReLU function applied to each element.</p>
</dd>
  </dl>











  <a id="AiDotNet_ActivationFunctions_LeakyReLUActivation_1_Activate_" data-uid="AiDotNet.ActivationFunctions.LeakyReLUActivation`1.Activate*"></a>

  <h3 id="AiDotNet_ActivationFunctions_LeakyReLUActivation_1_Activate_AiDotNet_Tensors_LinearAlgebra_Vector__0__" data-uid="AiDotNet.ActivationFunctions.LeakyReLUActivation`1.Activate(AiDotNet.Tensors.LinearAlgebra.Vector{`0})">
  Activate(Vector&lt;T&gt;)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/ActivationFunctions/LeakyReLUActivation.cs/#L103"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Applies the Leaky ReLU activation function to a vector of values.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public override Vector&lt;T&gt; Activate(Vector&lt;T&gt; input)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>input</code> <a class="xref" href="AiDotNet.Tensors.LinearAlgebra.Vector-1.html">Vector</a>&lt;T&gt;</dt>
    <dd><p>The input vector.</p>
</dd>
  </dl>

  <h4 class="section">Returns</h4>
  <dl class="parameters">
    <dt><a class="xref" href="AiDotNet.Tensors.LinearAlgebra.Vector-1.html">Vector</a>&lt;T&gt;</dt>
    <dd><p>A new vector with the Leaky ReLU function applied to each element.</p>
</dd>
  </dl>







  <h4 class="section" id="AiDotNet_ActivationFunctions_LeakyReLUActivation_1_Activate_AiDotNet_Tensors_LinearAlgebra_Vector__0___remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p><b>For Beginners:</b> This method applies the Leaky ReLU function to each value in a collection (vector)
of inputs. It processes each number individually using the same rules as the single-value version.</p>
</div>




  <a id="AiDotNet_ActivationFunctions_LeakyReLUActivation_1_Activate_" data-uid="AiDotNet.ActivationFunctions.LeakyReLUActivation`1.Activate*"></a>

  <h3 id="AiDotNet_ActivationFunctions_LeakyReLUActivation_1_Activate__0_" data-uid="AiDotNet.ActivationFunctions.LeakyReLUActivation`1.Activate(`0)">
  Activate(T)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/ActivationFunctions/LeakyReLUActivation.cs/#L89"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Applies the Leaky ReLU activation function to a single value.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public override T Activate(T input)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>input</code> <span class="xref">T</span></dt>
    <dd><p>The input value.</p>
</dd>
  </dl>

  <h4 class="section">Returns</h4>
  <dl class="parameters">
    <dt><span class="xref">T</span></dt>
    <dd><p>The input value if it's positive, or the input value multiplied by alpha if it's negative or zero.</p>
</dd>
  </dl>







  <h4 class="section" id="AiDotNet_ActivationFunctions_LeakyReLUActivation_1_Activate__0__remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
<b>For Beginners:</b> This method transforms an input value using the formula:
<p>f(x) = x        if x &gt; 0
f(x) = a * x    if x = 0</p>
<p>For example, with the default a = 0.01:</p>
<ul>
<li>Input of 5 ? Output of 5 (unchanged)</li>
<li>Input of 0 ? Output of 0</li>
<li>Input of -5 ? Output of -0.05 (5 * 0.01)</li>
</ul>

</div>




  <a id="AiDotNet_ActivationFunctions_LeakyReLUActivation_1_ApplyToGraph_" data-uid="AiDotNet.ActivationFunctions.LeakyReLUActivation`1.ApplyToGraph*"></a>

  <h3 id="AiDotNet_ActivationFunctions_LeakyReLUActivation_1_ApplyToGraph_AiDotNet_Autodiff_ComputationNode__0__" data-uid="AiDotNet.ActivationFunctions.LeakyReLUActivation`1.ApplyToGraph(AiDotNet.Autodiff.ComputationNode{`0})">
  ApplyToGraph(ComputationNode&lt;T&gt;)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/ActivationFunctions/LeakyReLUActivation.cs/#L241"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Applies this activation function to a computation graph node.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public override ComputationNode&lt;T&gt; ApplyToGraph(ComputationNode&lt;T&gt; input)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>input</code> <a class="xref" href="AiDotNet.Autodiff.ComputationNode-1.html">ComputationNode</a>&lt;T&gt;</dt>
    <dd><p>The computation node to apply the activation to.</p>
</dd>
  </dl>

  <h4 class="section">Returns</h4>
  <dl class="parameters">
    <dt><a class="xref" href="AiDotNet.Autodiff.ComputationNode-1.html">ComputationNode</a>&lt;T&gt;</dt>
    <dd><p>A new computation node with LeakyReLU activation applied.</p>
</dd>
  </dl>







  <h4 class="section" id="AiDotNet_ActivationFunctions_LeakyReLUActivation_1_ApplyToGraph_AiDotNet_Autodiff_ComputationNode__0___remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
This method maps the LeakyReLU activation to TensorOperations&lt;T&gt;.LeakyReLU(input, alpha),
which handles both forward and backward passes for JIT compilation.
</p>
</div>

  <h4 class="section">Exceptions</h4>
  <dl class="parameters">
    <dt><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.argumentnullexception">ArgumentNullException</a></dt>
    <dd><p>Thrown if input is null.</p>
</dd>
  </dl>



  <a id="AiDotNet_ActivationFunctions_LeakyReLUActivation_1_Backward_" data-uid="AiDotNet.ActivationFunctions.LeakyReLUActivation`1.Backward*"></a>

  <h3 id="AiDotNet_ActivationFunctions_LeakyReLUActivation_1_Backward_AiDotNet_Tensors_LinearAlgebra_Tensor__0__AiDotNet_Tensors_LinearAlgebra_Tensor__0__" data-uid="AiDotNet.ActivationFunctions.LeakyReLUActivation`1.Backward(AiDotNet.Tensors.LinearAlgebra.Tensor{`0},AiDotNet.Tensors.LinearAlgebra.Tensor{`0})">
  Backward(Tensor&lt;T&gt;, Tensor&lt;T&gt;)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/ActivationFunctions/LeakyReLUActivation.cs/#L210"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Calculates the backward pass gradient for Leaky ReLU using GPU-accelerated fused operation.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public override Tensor&lt;T&gt; Backward(Tensor&lt;T&gt; input, Tensor&lt;T&gt; outputGradient)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>input</code> <a class="xref" href="AiDotNet.Tensors.LinearAlgebra.Tensor-1.html">Tensor</a>&lt;T&gt;</dt>
    <dd><p>The input tensor that was used in the forward pass.</p>
</dd>
    <dt><code>outputGradient</code> <a class="xref" href="AiDotNet.Tensors.LinearAlgebra.Tensor-1.html">Tensor</a>&lt;T&gt;</dt>
    <dd><p>The gradient flowing back from the next layer.</p>
</dd>
  </dl>

  <h4 class="section">Returns</h4>
  <dl class="parameters">
    <dt><a class="xref" href="AiDotNet.Tensors.LinearAlgebra.Tensor-1.html">Tensor</a>&lt;T&gt;</dt>
    <dd><p>The gradient with respect to the input.</p>
</dd>
  </dl>







  <h4 class="section" id="AiDotNet_ActivationFunctions_LeakyReLUActivation_1_Backward_AiDotNet_Tensors_LinearAlgebra_Tensor__0__AiDotNet_Tensors_LinearAlgebra_Tensor__0___remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p><b>For Beginners:</b> This method uses a single GPU kernel to compute the gradient,
which is faster than computing derivative and gradient multiplication separately.</p>
</div>




  <a id="AiDotNet_ActivationFunctions_LeakyReLUActivation_1_BackwardGpu_" data-uid="AiDotNet.ActivationFunctions.LeakyReLUActivation`1.BackwardGpu*"></a>

  <h3 id="AiDotNet_ActivationFunctions_LeakyReLUActivation_1_BackwardGpu_AiDotNet_Tensors_Engines_DirectGpu_IDirectGpuBackend_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_System_Int32_" data-uid="AiDotNet.ActivationFunctions.LeakyReLUActivation`1.BackwardGpu(AiDotNet.Tensors.Engines.DirectGpu.IDirectGpuBackend,AiDotNet.Tensors.Engines.DirectGpu.IGpuBuffer,AiDotNet.Tensors.Engines.DirectGpu.IGpuBuffer,AiDotNet.Tensors.Engines.DirectGpu.IGpuBuffer,AiDotNet.Tensors.Engines.DirectGpu.IGpuBuffer,System.Int32)">
  BackwardGpu(IDirectGpuBackend, IGpuBuffer, IGpuBuffer?, IGpuBuffer?, IGpuBuffer, int)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/ActivationFunctions/LeakyReLUActivation.cs/#L287"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Calculates the Leaky ReLU backward pass gradient on GPU.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public override void BackwardGpu(IDirectGpuBackend backend, IGpuBuffer gradOutput, IGpuBuffer? input, IGpuBuffer? output, IGpuBuffer gradInput, int size)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>backend</code> <a class="xref" href="AiDotNet.Tensors.Engines.DirectGpu.IDirectGpuBackend.html">IDirectGpuBackend</a></dt>
    <dd><p>The GPU backend to use for execution.</p>
</dd>
    <dt><code>gradOutput</code> <a class="xref" href="AiDotNet.Tensors.Engines.DirectGpu.IGpuBuffer.html">IGpuBuffer</a></dt>
    <dd><p>The gradient flowing back from the next layer.</p>
</dd>
    <dt><code>input</code> <a class="xref" href="AiDotNet.Tensors.Engines.DirectGpu.IGpuBuffer.html">IGpuBuffer</a></dt>
    <dd><p>The input buffer from the forward pass.</p>
</dd>
    <dt><code>output</code> <a class="xref" href="AiDotNet.Tensors.Engines.DirectGpu.IGpuBuffer.html">IGpuBuffer</a></dt>
    <dd><p>Not used for LeakyReLU (can be null). LeakyReLU backward uses forward input.</p>
</dd>
    <dt><code>gradInput</code> <a class="xref" href="AiDotNet.Tensors.Engines.DirectGpu.IGpuBuffer.html">IGpuBuffer</a></dt>
    <dd><p>The output buffer to store the input gradient.</p>
</dd>
    <dt><code>size</code> <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.int32">int</a></dt>
    <dd><p>The number of elements to process.</p>
</dd>
  </dl>








  <h4 class="section" id="AiDotNet_ActivationFunctions_LeakyReLUActivation_1_BackwardGpu_AiDotNet_Tensors_Engines_DirectGpu_IDirectGpuBackend_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_System_Int32__remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>LeakyReLU backward on GPU: gradInput[i] = gradOutput[i] * (input[i] &gt; 0 ? 1 : alpha)</p>
</div>




  <a id="AiDotNet_ActivationFunctions_LeakyReLUActivation_1_Derivative_" data-uid="AiDotNet.ActivationFunctions.LeakyReLUActivation`1.Derivative*"></a>

  <h3 id="AiDotNet_ActivationFunctions_LeakyReLUActivation_1_Derivative_AiDotNet_Tensors_LinearAlgebra_Tensor__0__" data-uid="AiDotNet.ActivationFunctions.LeakyReLUActivation`1.Derivative(AiDotNet.Tensors.LinearAlgebra.Tensor{`0})">
  Derivative(Tensor&lt;T&gt;)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/ActivationFunctions/LeakyReLUActivation.cs/#L190"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Calculates the derivative of the Leaky ReLU function for each element in a tensor.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public override Tensor&lt;T&gt; Derivative(Tensor&lt;T&gt; input)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>input</code> <a class="xref" href="AiDotNet.Tensors.LinearAlgebra.Tensor-1.html">Tensor</a>&lt;T&gt;</dt>
    <dd><p>The input tensor to calculate the derivative for.</p>
</dd>
  </dl>

  <h4 class="section">Returns</h4>
  <dl class="parameters">
    <dt><a class="xref" href="AiDotNet.Tensors.LinearAlgebra.Tensor-1.html">Tensor</a>&lt;T&gt;</dt>
    <dd><p>A new tensor containing the derivatives of the Leaky ReLU function for each input element.</p>
</dd>
  </dl>











  <a id="AiDotNet_ActivationFunctions_LeakyReLUActivation_1_Derivative_" data-uid="AiDotNet.ActivationFunctions.LeakyReLUActivation`1.Derivative*"></a>

  <h3 id="AiDotNet_ActivationFunctions_LeakyReLUActivation_1_Derivative_AiDotNet_Tensors_LinearAlgebra_Vector__0__" data-uid="AiDotNet.ActivationFunctions.LeakyReLUActivation`1.Derivative(AiDotNet.Tensors.LinearAlgebra.Vector{`0})">
  Derivative(Vector&lt;T&gt;)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/ActivationFunctions/LeakyReLUActivation.cs/#L152"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Calculates the derivative (gradient) of the Leaky ReLU function for a vector of values.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public override Matrix&lt;T&gt; Derivative(Vector&lt;T&gt; input)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>input</code> <a class="xref" href="AiDotNet.Tensors.LinearAlgebra.Vector-1.html">Vector</a>&lt;T&gt;</dt>
    <dd><p>The input vector at which to calculate the derivative.</p>
</dd>
  </dl>

  <h4 class="section">Returns</h4>
  <dl class="parameters">
    <dt><a class="xref" href="AiDotNet.Tensors.LinearAlgebra.Matrix-1.html">Matrix</a>&lt;T&gt;</dt>
    <dd><p>A diagonal matrix containing the derivatives for each input value.</p>
</dd>
  </dl>







  <h4 class="section" id="AiDotNet_ActivationFunctions_LeakyReLUActivation_1_Derivative_AiDotNet_Tensors_LinearAlgebra_Vector__0___remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
<b>For Beginners:</b> This method calculates how the output vector changes when we slightly change each input value.
<p>The result is a special matrix called a &quot;Jacobian matrix&quot; where:</p>
<ul>
<li>Values on the main diagonal (top-left to bottom-right) are the derivatives for each input</li>
<li>All other values are 0</li>
</ul>
<p>This diagonal structure indicates that each output is affected only by its corresponding input,
with no cross-interactions between different elements.</p>
<p>For Leaky ReLU, each diagonal value will be either:</p>
<ul>
<li>1 (for inputs &gt; 0)</li>
<li>alpha (for inputs = 0)</li>
</ul>

</div>




  <a id="AiDotNet_ActivationFunctions_LeakyReLUActivation_1_Derivative_" data-uid="AiDotNet.ActivationFunctions.LeakyReLUActivation`1.Derivative*"></a>

  <h3 id="AiDotNet_ActivationFunctions_LeakyReLUActivation_1_Derivative__0_" data-uid="AiDotNet.ActivationFunctions.LeakyReLUActivation`1.Derivative(`0)">
  Derivative(T)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/ActivationFunctions/LeakyReLUActivation.cs/#L126"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Calculates the derivative (gradient) of the Leaky ReLU function for a single value.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public override T Derivative(T input)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>input</code> <span class="xref">T</span></dt>
    <dd><p>The input value at which to calculate the derivative.</p>
</dd>
  </dl>

  <h4 class="section">Returns</h4>
  <dl class="parameters">
    <dt><span class="xref">T</span></dt>
    <dd><p>1 if the input is positive, or alpha if the input is negative or zero.</p>
</dd>
  </dl>







  <h4 class="section" id="AiDotNet_ActivationFunctions_LeakyReLUActivation_1_Derivative__0__remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
<b>For Beginners:</b> The derivative tells us how much the output changes when we slightly change the input.
This information is crucial during neural network training.
<p>For the Leaky ReLU function, the derivative is very simple:</p>
<ul>
<li>For positive inputs (x &gt; 0): The derivative is 1 (output changes at the same rate as input)</li>
<li>For negative inputs (x = 0): The derivative is alpha (output changes at alpha times the rate of input)</li>
</ul>
<p>Unlike some other activation functions, Leaky ReLU's derivative never becomes zero,
which helps prevent neurons from &quot;dying&quot; during training.</p>

</div>




  <a id="AiDotNet_ActivationFunctions_LeakyReLUActivation_1_ForwardGpu_" data-uid="AiDotNet.ActivationFunctions.LeakyReLUActivation`1.ForwardGpu*"></a>

  <h3 id="AiDotNet_ActivationFunctions_LeakyReLUActivation_1_ForwardGpu_AiDotNet_Tensors_Engines_DirectGpu_IDirectGpuBackend_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_System_Int32_" data-uid="AiDotNet.ActivationFunctions.LeakyReLUActivation`1.ForwardGpu(AiDotNet.Tensors.Engines.DirectGpu.IDirectGpuBackend,AiDotNet.Tensors.Engines.DirectGpu.IGpuBuffer,AiDotNet.Tensors.Engines.DirectGpu.IGpuBuffer,System.Int32)">
  ForwardGpu(IDirectGpuBackend, IGpuBuffer, IGpuBuffer, int)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/ActivationFunctions/LeakyReLUActivation.cs/#L269"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Applies the Leaky ReLU activation function on GPU.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public override void ForwardGpu(IDirectGpuBackend backend, IGpuBuffer input, IGpuBuffer output, int size)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>backend</code> <a class="xref" href="AiDotNet.Tensors.Engines.DirectGpu.IDirectGpuBackend.html">IDirectGpuBackend</a></dt>
    <dd><p>The GPU backend to use for execution.</p>
</dd>
    <dt><code>input</code> <a class="xref" href="AiDotNet.Tensors.Engines.DirectGpu.IGpuBuffer.html">IGpuBuffer</a></dt>
    <dd><p>The input GPU buffer.</p>
</dd>
    <dt><code>output</code> <a class="xref" href="AiDotNet.Tensors.Engines.DirectGpu.IGpuBuffer.html">IGpuBuffer</a></dt>
    <dd><p>The output GPU buffer to store the activated values.</p>
</dd>
    <dt><code>size</code> <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.int32">int</a></dt>
    <dd><p>The number of elements to process.</p>
</dd>
  </dl>








  <h4 class="section" id="AiDotNet_ActivationFunctions_LeakyReLUActivation_1_ForwardGpu_AiDotNet_Tensors_Engines_DirectGpu_IDirectGpuBackend_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_System_Int32__remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>LeakyReLU on GPU: output[i] = input[i] &gt; 0 ? input[i] : alpha * input[i]</p>
</div>




  <a id="AiDotNet_ActivationFunctions_LeakyReLUActivation_1_SupportsScalarOperations_" data-uid="AiDotNet.ActivationFunctions.LeakyReLUActivation`1.SupportsScalarOperations*"></a>

  <h3 id="AiDotNet_ActivationFunctions_LeakyReLUActivation_1_SupportsScalarOperations" data-uid="AiDotNet.ActivationFunctions.LeakyReLUActivation`1.SupportsScalarOperations">
  SupportsScalarOperations()
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/ActivationFunctions/LeakyReLUActivation.cs/#L67"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Indicates whether this activation function can operate on individual scalar values.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">protected override bool SupportsScalarOperations()</code></pre>
  </div>


  <h4 class="section">Returns</h4>
  <dl class="parameters">
    <dt><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.boolean">bool</a></dt>
    <dd><p>Always returns true as the Leaky ReLU function can be applied to individual values.</p>
</dd>
  </dl>












</article>

        <div class="contribution d-print-none">
          <a href="https://github.com/ooples/AiDotNet/blob/master/src/ActivationFunctions/LeakyReLUActivation.cs/#L27" class="edit-link">Edit this page</a>
        </div>


      </div>

      <div class="affix">
        <nav id="affix"></nav>
      </div>
    </main>

    <div class="container-xxl search-results" id="search-results"></div>

    <footer class="border-top text-secondary">
      <div class="container-xxl">
        <div class="flex-fill">
          AiDotNet - Enterprise AI/ML Library for .NET
        </div>
      </div>
    </footer>
  </body>
</html>
