<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
      <title>Enum PretrainedTokenizerModel | AiDotNet Documentation </title>
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <meta name="title" content="Enum PretrainedTokenizerModel | AiDotNet Documentation ">
      
      <meta name="description" content="Specifies pretrained tokenizer models available from HuggingFace Hub.">
      <link rel="icon" href="../favicon.ico">
      <link rel="stylesheet" href="../public/docfx.min.css">
      <link rel="stylesheet" href="../public/main.css">
      <meta name="docfx:navrel" content="../toc.html">
      <meta name="docfx:tocrel" content="toc.html">
      
      <meta name="docfx:rel" content="../">
      
      
      <meta name="docfx:docurl" content="https://github.com/ooples/AiDotNet/new/master/apiSpec/new?filename=AiDotNet_Tokenization_Configuration_PretrainedTokenizerModel.md&amp;value=---%0Auid%3A%20AiDotNet.Tokenization.Configuration.PretrainedTokenizerModel%0Asummary%3A%20&#39;*You%20can%20override%20summary%20for%20the%20API%20here%20using%20*MARKDOWN*%20syntax&#39;%0A---%0A%0A*Please%20type%20below%20more%20information%20about%20this%20API%3A*%0A%0A">
      <meta name="loc:inThisArticle" content="In this article">
      <meta name="loc:searchResultsCount" content="{count} results for &quot;{query}&quot;">
      <meta name="loc:searchNoResults" content="No results for &quot;{query}&quot;">
      <meta name="loc:tocFilter" content="Filter by title">
      <meta name="loc:nextArticle" content="Next">
      <meta name="loc:prevArticle" content="Previous">
      <meta name="loc:themeLight" content="Light">
      <meta name="loc:themeDark" content="Dark">
      <meta name="loc:themeAuto" content="Auto">
      <meta name="loc:changeTheme" content="Change theme">
      <meta name="loc:copy" content="Copy">
      <meta name="loc:downloadPdf" content="Download PDF">

      <script type="module" src="./../public/docfx.min.js"></script>

      <script>
        const theme = localStorage.getItem('theme') || 'auto'
        document.documentElement.setAttribute('data-bs-theme', theme === 'auto' ? (window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light') : theme)
      </script>

  </head>

  <body class="tex2jax_ignore" data-layout="" data-yaml-mime="ManagedReference">
    <header class="bg-body border-bottom">
      <nav id="autocollapse" class="navbar navbar-expand-md" role="navigation">
        <div class="container-xxl flex-nowrap">
          <a class="navbar-brand" href="../index.html">
            <img id="logo" class="svg" src="../logo.svg" alt="AiDotNet">
            AiDotNet
          </a>
          <button class="btn btn-lg d-md-none border-0" type="button" data-bs-toggle="collapse" data-bs-target="#navpanel" aria-controls="navpanel" aria-expanded="false" aria-label="Toggle navigation">
            <i class="bi bi-three-dots"></i>
          </button>
          <div class="collapse navbar-collapse" id="navpanel">
            <div id="navbar">
              <form class="search" role="search" id="search">
                <i class="bi bi-search"></i>
                <input class="form-control" id="search-query" type="search" disabled placeholder="Search" autocomplete="off" aria-label="Search">
              </form>
            </div>
          </div>
        </div>
      </nav>
    </header>

    <main class="container-xxl">
      <div class="toc-offcanvas">
        <div class="offcanvas-md offcanvas-start" tabindex="-1" id="tocOffcanvas" aria-labelledby="tocOffcanvasLabel">
          <div class="offcanvas-header">
            <h5 class="offcanvas-title" id="tocOffcanvasLabel">Table of Contents</h5>
            <button type="button" class="btn-close" data-bs-dismiss="offcanvas" data-bs-target="#tocOffcanvas" aria-label="Close"></button>
          </div>
          <div class="offcanvas-body">
            <nav class="toc" id="toc"></nav>
          </div>
        </div>
      </div>

      <div class="content">
        <div class="actionbar">
          <button class="btn btn-lg border-0 d-md-none" type="button" data-bs-toggle="offcanvas" data-bs-target="#tocOffcanvas" aria-controls="tocOffcanvas" aria-expanded="false" aria-label="Show table of contents">
            <i class="bi bi-list"></i>
          </button>

          <nav id="breadcrumb"></nav>
        </div>

        <article data-uid="AiDotNet.Tokenization.Configuration.PretrainedTokenizerModel">




  <h1 id="AiDotNet_Tokenization_Configuration_PretrainedTokenizerModel" data-uid="AiDotNet.Tokenization.Configuration.PretrainedTokenizerModel" class="text-break">
Enum PretrainedTokenizerModel  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/Tokenization/Configuration/PretrainedTokenizerModel.cs/#L16"><i class="bi bi-code-slash"></i></a>
  </h1>

  <div class="facts text-secondary">
    <dl><dt>Namespace</dt><dd><a class="xref" href="AiDotNet.html">AiDotNet</a>.<a class="xref" href="AiDotNet.Tokenization.html">Tokenization</a>.<a class="xref" href="AiDotNet.Tokenization.Configuration.html">Configuration</a></dd></dl>
  <dl><dt>Assembly</dt><dd>AiDotNet.dll</dd></dl>
  </div>

  <div class="markdown summary"><p>Specifies pretrained tokenizer models available from HuggingFace Hub.</p>
</div>
  <div class="markdown conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public enum PretrainedTokenizerModel</code></pre>
  </div>








  <dl class="typelist extensionMethods">
    <dt>Extension Methods</dt>
    <dd>
  <div>
      <a class="xref" href="AiDotNet.Tokenization.Configuration.PretrainedTokenizerModelExtensions.html#AiDotNet_Tokenization_Configuration_PretrainedTokenizerModelExtensions_ToModelId_AiDotNet_Tokenization_Configuration_PretrainedTokenizerModel_">PretrainedTokenizerModelExtensions.ToModelId(PretrainedTokenizerModel)</a>
  </div>
  </dd></dl>

  <h2 id="fields">Fields
</h2>
  <dl class="parameters">
    <dt id="AiDotNet_Tokenization_Configuration_PretrainedTokenizerModel_AlbertBaseV2"><code>AlbertBaseV2 = 14</code></dt>
  
  <dd><p>ALBERT Base v2 - A Lite BERT with parameter sharing.
Much smaller model size with competitive performance.</p>
</dd>
    <dt id="AiDotNet_Tokenization_Configuration_PretrainedTokenizerModel_BertBaseCased"><code>BertBaseCased = 1</code></dt>
  
  <dd><p>BERT Base Cased - Preserves case information.
Best when capitalization matters (e.g., named entity recognition).</p>
</dd>
    <dt id="AiDotNet_Tokenization_Configuration_PretrainedTokenizerModel_BertBaseUncased"><code>BertBaseUncased = 0</code></dt>
  
  <dd><p>BERT Base Uncased - The default choice for most NLP tasks.
Vocabulary: 30,522 tokens. Case-insensitive.</p>
</dd>
    <dt id="AiDotNet_Tokenization_Configuration_PretrainedTokenizerModel_BertLargeCased"><code>BertLargeCased = 3</code></dt>
  
  <dd><p>BERT Large Cased - Large model preserving case.
Best accuracy for case-sensitive tasks.</p>
</dd>
    <dt id="AiDotNet_Tokenization_Configuration_PretrainedTokenizerModel_BertLargeUncased"><code>BertLargeUncased = 2</code></dt>
  
  <dd><p>BERT Large Uncased - Larger model with better accuracy.
Vocabulary: 30,522 tokens. More compute intensive.</p>
</dd>
    <dt id="AiDotNet_Tokenization_Configuration_PretrainedTokenizerModel_CodeBertBase"><code>CodeBertBase = 18</code></dt>
  
  <dd><p>CodeBERT Base - BERT for programming languages.
Best for code understanding tasks.</p>
</dd>
    <dt id="AiDotNet_Tokenization_Configuration_PretrainedTokenizerModel_DistilBertBaseCased"><code>DistilBertBaseCased = 10</code></dt>
  
  <dd><p>DistilBERT Base Cased - Distilled BERT preserving case.
Fast and case-sensitive.</p>
</dd>
    <dt id="AiDotNet_Tokenization_Configuration_PretrainedTokenizerModel_DistilBertBaseUncased"><code>DistilBertBaseUncased = 9</code></dt>
  
  <dd><p>DistilBERT Base Uncased - Distilled BERT (40% smaller, 60% faster).
Good balance of speed and accuracy.</p>
</dd>
    <dt id="AiDotNet_Tokenization_Configuration_PretrainedTokenizerModel_ElectraBase"><code>ElectraBase = 17</code></dt>
  
  <dd><p>Electra Base - Efficient pretraining approach (base size).
Good accuracy with efficient training.</p>
</dd>
    <dt id="AiDotNet_Tokenization_Configuration_PretrainedTokenizerModel_ElectraSmall"><code>ElectraSmall = 16</code></dt>
  
  <dd><p>Electra Small - Efficient pretraining approach.
Very efficient for its size.</p>
</dd>
    <dt id="AiDotNet_Tokenization_Configuration_PretrainedTokenizerModel_Gpt2"><code>Gpt2 = 4</code></dt>
  
  <dd><p>GPT-2 - OpenAI's text generation model.
Vocabulary: 50,257 tokens. Best for text generation.</p>
</dd>
    <dt id="AiDotNet_Tokenization_Configuration_PretrainedTokenizerModel_Gpt2Large"><code>Gpt2Large = 6</code></dt>
  
  <dd><p>GPT-2 Large - Even larger GPT-2 variant.
High quality generation for demanding applications.</p>
</dd>
    <dt id="AiDotNet_Tokenization_Configuration_PretrainedTokenizerModel_Gpt2Medium"><code>Gpt2Medium = 5</code></dt>
  
  <dd><p>GPT-2 Medium - Larger GPT-2 variant.
Better quality generation, more compute required.</p>
</dd>
    <dt id="AiDotNet_Tokenization_Configuration_PretrainedTokenizerModel_GraphCodeBert"><code>GraphCodeBert = 20</code></dt>
  
  <dd><p>GraphCodeBERT - Code model with data flow.
Enhanced code understanding with graph structure.</p>
</dd>
    <dt id="AiDotNet_Tokenization_Configuration_PretrainedTokenizerModel_MicrosoftCodeBert"><code>MicrosoftCodeBert = 19</code></dt>
  
  <dd><p>Microsoft CodeBERT - Multi-language code model.
Supports multiple programming languages.</p>
</dd>
    <dt id="AiDotNet_Tokenization_Configuration_PretrainedTokenizerModel_RobertaBase"><code>RobertaBase = 7</code></dt>
  
  <dd><p>RoBERTa Base - Robustly optimized BERT.
Often outperforms BERT on benchmarks.</p>
</dd>
    <dt id="AiDotNet_Tokenization_Configuration_PretrainedTokenizerModel_RobertaLarge"><code>RobertaLarge = 8</code></dt>
  
  <dd><p>RoBERTa Large - Large RoBERTa model.
State-of-the-art performance on many tasks.</p>
</dd>
    <dt id="AiDotNet_Tokenization_Configuration_PretrainedTokenizerModel_T5Base"><code>T5Base = 12</code></dt>
  
  <dd><p>T5 Base - Text-to-Text Transfer Transformer (base).
Good balance of performance and efficiency.</p>
</dd>
    <dt id="AiDotNet_Tokenization_Configuration_PretrainedTokenizerModel_T5Large"><code>T5Large = 13</code></dt>
  
  <dd><p>T5 Large - Text-to-Text Transfer Transformer (large).
High performance for complex tasks.</p>
</dd>
    <dt id="AiDotNet_Tokenization_Configuration_PretrainedTokenizerModel_T5Small"><code>T5Small = 11</code></dt>
  
  <dd><p>T5 Small - Text-to-Text Transfer Transformer (small).
Versatile for many NLP tasks.</p>
</dd>
    <dt id="AiDotNet_Tokenization_Configuration_PretrainedTokenizerModel_XlnetBaseCased"><code>XlnetBaseCased = 15</code></dt>
  
  <dd><p>XLNet Base Cased - Autoregressive pretraining.
Strong performance on long-context tasks.</p>
</dd>
  </dl>


  <h2 id="AiDotNet_Tokenization_Configuration_PretrainedTokenizerModel_remarks">Remarks</h2>
  <div class="markdown level0 remarks"><p><b>For Beginners:</b> These are industry-standard tokenizers that have been trained
on large text corpora. Each is designed for different use cases:</p>
<ul>
<li>BERT models: Best for understanding text (classification, Q&amp;A, NER)</li>
<li>GPT models: Best for text generation</li>
<li>RoBERTa: Improved BERT with better training</li>
<li>T5: Versatile text-to-text model</li>
<li>DistilBERT: Faster, smaller BERT</li>
</ul>
</div>

</article>

        <div class="contribution d-print-none">
          <a href="https://github.com/ooples/AiDotNet/blob/master/src/Tokenization/Configuration/PretrainedTokenizerModel.cs/#L16" class="edit-link">Edit this page</a>
        </div>


      </div>

      <div class="affix">
        <nav id="affix"></nav>
      </div>
    </main>

    <div class="container-xxl search-results" id="search-results"></div>

    <footer class="border-top text-secondary">
      <div class="container-xxl">
        <div class="flex-fill">
          AiDotNet - Enterprise AI/ML Library for .NET
        </div>
      </div>
    </footer>
  </body>
</html>
