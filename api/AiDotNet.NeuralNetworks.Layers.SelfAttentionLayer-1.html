<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
      <title>Class SelfAttentionLayer&lt;T&gt; | AiDotNet Documentation </title>
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <meta name="title" content="Class SelfAttentionLayer&lt;T&gt; | AiDotNet Documentation ">
      
      <meta name="description" content="Represents a self-attention layer that allows a sequence to attend to itself, capturing relationships between elements.">
      <link rel="icon" href="../favicon.ico">
      <link rel="stylesheet" href="../public/docfx.min.css">
      <link rel="stylesheet" href="../public/main.css">
      <meta name="docfx:navrel" content="../toc.html">
      <meta name="docfx:tocrel" content="toc.html">
      
      <meta name="docfx:rel" content="../">
      
      
      <meta name="docfx:docurl" content="https://github.com/ooples/AiDotNet/new/master/apiSpec/new?filename=AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1.md&amp;value=---%0Auid%3A%20AiDotNet.NeuralNetworks.Layers.SelfAttentionLayer%601%0Asummary%3A%20&#39;*You%20can%20override%20summary%20for%20the%20API%20here%20using%20*MARKDOWN*%20syntax&#39;%0A---%0A%0A*Please%20type%20below%20more%20information%20about%20this%20API%3A*%0A%0A">
      <meta name="loc:inThisArticle" content="In this article">
      <meta name="loc:searchResultsCount" content="{count} results for &quot;{query}&quot;">
      <meta name="loc:searchNoResults" content="No results for &quot;{query}&quot;">
      <meta name="loc:tocFilter" content="Filter by title">
      <meta name="loc:nextArticle" content="Next">
      <meta name="loc:prevArticle" content="Previous">
      <meta name="loc:themeLight" content="Light">
      <meta name="loc:themeDark" content="Dark">
      <meta name="loc:themeAuto" content="Auto">
      <meta name="loc:changeTheme" content="Change theme">
      <meta name="loc:copy" content="Copy">
      <meta name="loc:downloadPdf" content="Download PDF">

      <script type="module" src="./../public/docfx.min.js"></script>

      <script>
        const theme = localStorage.getItem('theme') || 'auto'
        document.documentElement.setAttribute('data-bs-theme', theme === 'auto' ? (window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light') : theme)
      </script>

  </head>

  <body class="tex2jax_ignore" data-layout="" data-yaml-mime="ManagedReference">
    <header class="bg-body border-bottom">
      <nav id="autocollapse" class="navbar navbar-expand-md" role="navigation">
        <div class="container-xxl flex-nowrap">
          <a class="navbar-brand" href="../index.html">
            <img id="logo" class="svg" src="../logo.svg" alt="AiDotNet">
            AiDotNet
          </a>
          <button class="btn btn-lg d-md-none border-0" type="button" data-bs-toggle="collapse" data-bs-target="#navpanel" aria-controls="navpanel" aria-expanded="false" aria-label="Toggle navigation">
            <i class="bi bi-three-dots"></i>
          </button>
          <div class="collapse navbar-collapse" id="navpanel">
            <div id="navbar">
              <form class="search" role="search" id="search">
                <i class="bi bi-search"></i>
                <input class="form-control" id="search-query" type="search" disabled placeholder="Search" autocomplete="off" aria-label="Search">
              </form>
            </div>
          </div>
        </div>
      </nav>
    </header>

    <main class="container-xxl">
      <div class="toc-offcanvas">
        <div class="offcanvas-md offcanvas-start" tabindex="-1" id="tocOffcanvas" aria-labelledby="tocOffcanvasLabel">
          <div class="offcanvas-header">
            <h5 class="offcanvas-title" id="tocOffcanvasLabel">Table of Contents</h5>
            <button type="button" class="btn-close" data-bs-dismiss="offcanvas" data-bs-target="#tocOffcanvas" aria-label="Close"></button>
          </div>
          <div class="offcanvas-body">
            <nav class="toc" id="toc"></nav>
          </div>
        </div>
      </div>

      <div class="content">
        <div class="actionbar">
          <button class="btn btn-lg border-0 d-md-none" type="button" data-bs-toggle="offcanvas" data-bs-target="#tocOffcanvas" aria-controls="tocOffcanvas" aria-expanded="false" aria-label="Show table of contents">
            <i class="bi bi-list"></i>
          </button>

          <nav id="breadcrumb"></nav>
        </div>

        <article data-uid="AiDotNet.NeuralNetworks.Layers.SelfAttentionLayer`1">



  <h1 id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1" data-uid="AiDotNet.NeuralNetworks.Layers.SelfAttentionLayer`1" class="text-break">
Class SelfAttentionLayer&lt;T&gt;  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/NeuralNetworks/Layers/SelfAttentionLayer.cs/#L38"><i class="bi bi-code-slash"></i></a>
  </h1>

  <div class="facts text-secondary">
    <dl><dt>Namespace</dt><dd><a class="xref" href="AiDotNet.html">AiDotNet</a>.<a class="xref" href="AiDotNet.NeuralNetworks.html">NeuralNetworks</a>.<a class="xref" href="AiDotNet.NeuralNetworks.Layers.html">Layers</a></dd></dl>
  <dl><dt>Assembly</dt><dd>AiDotNet.dll</dd></dl>
  </div>

  <div class="markdown summary"><p>Represents a self-attention layer that allows a sequence to attend to itself, capturing relationships between elements.</p>
</div>
  <div class="markdown conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public class SelfAttentionLayer&lt;T&gt; : LayerBase&lt;T&gt;, ILayer&lt;T&gt;, IJitCompilable&lt;T&gt;, IWeightLoadable&lt;T&gt;, IDisposable, IAuxiliaryLossLayer&lt;T&gt;, IDiagnosticsProvider</code></pre>
  </div>



  <h4 class="section">Type Parameters</h4>
  <dl class="parameters">
    <dt><code>T</code></dt>
    <dd><p>The numeric type used for calculations, typically float or double.</p>
</dd>
  </dl>

  <dl class="typelist inheritance">
    <dt>Inheritance</dt>
    <dd>
      <div><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object">object</a></div>
      <div><a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html">LayerBase</a>&lt;T&gt;</div>
      <div><span class="xref">SelfAttentionLayer&lt;T&gt;</span></div>
    </dd>
  </dl>

  <dl class="typelist implements">
    <dt>Implements</dt>
    <dd>
      <div><a class="xref" href="AiDotNet.Interfaces.ILayer-1.html">ILayer</a>&lt;T&gt;</div>
      <div><a class="xref" href="AiDotNet.Interfaces.IJitCompilable-1.html">IJitCompilable</a>&lt;T&gt;</div>
      <div><a class="xref" href="AiDotNet.Interfaces.IWeightLoadable-1.html">IWeightLoadable</a>&lt;T&gt;</div>
      <div><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.idisposable">IDisposable</a></div>
      <div><a class="xref" href="AiDotNet.Interfaces.IAuxiliaryLossLayer-1.html">IAuxiliaryLossLayer</a>&lt;T&gt;</div>
      <div><a class="xref" href="AiDotNet.Interfaces.IDiagnosticsProvider.html">IDiagnosticsProvider</a></div>
    </dd>
  </dl>


  <dl class="typelist inheritedMembers">
    <dt>Inherited Members</dt>
    <dd>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_Engine">LayerBase&lt;T&gt;.Engine</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_ScalarActivation">LayerBase&lt;T&gt;.ScalarActivation</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_VectorActivation">LayerBase&lt;T&gt;.VectorActivation</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_UsingVectorActivation">LayerBase&lt;T&gt;.UsingVectorActivation</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_NumOps">LayerBase&lt;T&gt;.NumOps</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_Random">LayerBase&lt;T&gt;.Random</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_Parameters">LayerBase&lt;T&gt;.Parameters</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_ParameterGradients">LayerBase&lt;T&gt;.ParameterGradients</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_InputShape">LayerBase&lt;T&gt;.InputShape</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_InputShapes">LayerBase&lt;T&gt;.InputShapes</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_UpdateInputShape_System_Int32___">LayerBase&lt;T&gt;.UpdateInputShape(int[])</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_OutputShape">LayerBase&lt;T&gt;.OutputShape</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_IsTrainingMode">LayerBase&lt;T&gt;.IsTrainingMode</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_InitializationStrategy">LayerBase&lt;T&gt;.InitializationStrategy</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_IsInitialized">LayerBase&lt;T&gt;.IsInitialized</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_InitializationLock">LayerBase&lt;T&gt;.InitializationLock</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_EnsureInitialized">LayerBase&lt;T&gt;.EnsureInitialized()</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_UseAutodiff">LayerBase&lt;T&gt;.UseAutodiff</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_SetTrainingMode_System_Boolean_">LayerBase&lt;T&gt;.SetTrainingMode(bool)</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_GetParameterGradients">LayerBase&lt;T&gt;.GetParameterGradients()</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_ClearGradients">LayerBase&lt;T&gt;.ClearGradients()</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_GetInputShape">LayerBase&lt;T&gt;.GetInputShape()</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_GetInputShapes">LayerBase&lt;T&gt;.GetInputShapes()</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_GetOutputShape">LayerBase&lt;T&gt;.GetOutputShape()</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_GetWeights">LayerBase&lt;T&gt;.GetWeights()</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_GetBiases">LayerBase&lt;T&gt;.GetBiases()</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_MapActivationToFused">LayerBase&lt;T&gt;.MapActivationToFused()</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_SupportsGpuTraining">LayerBase&lt;T&gt;.SupportsGpuTraining</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_CanExecuteOnGpu">LayerBase&lt;T&gt;.CanExecuteOnGpu</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_CanTrainOnGpu">LayerBase&lt;T&gt;.CanTrainOnGpu</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_UpdateParametersGpu_AiDotNet_Interfaces_IGpuOptimizerConfig_">LayerBase&lt;T&gt;.UpdateParametersGpu(IGpuOptimizerConfig)</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_UploadWeightsToGpu">LayerBase&lt;T&gt;.UploadWeightsToGpu()</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_DownloadWeightsFromGpu">LayerBase&lt;T&gt;.DownloadWeightsFromGpu()</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_ZeroGradientsGpu">LayerBase&lt;T&gt;.ZeroGradientsGpu()</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_GetActivationTypes">LayerBase&lt;T&gt;.GetActivationTypes()</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_Forward_AiDotNet_Tensors_LinearAlgebra_Tensor__0____">LayerBase&lt;T&gt;.Forward(params Tensor&lt;T&gt;[])</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_ApplyActivation_AiDotNet_Tensors_LinearAlgebra_Tensor__0__">LayerBase&lt;T&gt;.ApplyActivation(Tensor&lt;T&gt;)</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_ApplyActivation_AiDotNet_Tensors_LinearAlgebra_Vector__0__">LayerBase&lt;T&gt;.ApplyActivation(Vector&lt;T&gt;)</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_ActivateTensor_AiDotNet_Interfaces_IActivationFunction__0__AiDotNet_Tensors_LinearAlgebra_Tensor__0__">LayerBase&lt;T&gt;.ActivateTensor(IActivationFunction&lt;T&gt;, Tensor&lt;T&gt;)</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_ActivateTensor_AiDotNet_Interfaces_IVectorActivationFunction__0__AiDotNet_Tensors_LinearAlgebra_Tensor__0__">LayerBase&lt;T&gt;.ActivateTensor(IVectorActivationFunction&lt;T&gt;, Tensor&lt;T&gt;)</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_CalculateInputShape_System_Int32_System_Int32_System_Int32_">LayerBase&lt;T&gt;.CalculateInputShape(int, int, int)</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_CalculateOutputShape_System_Int32_System_Int32_System_Int32_">LayerBase&lt;T&gt;.CalculateOutputShape(int, int, int)</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_Clone">LayerBase&lt;T&gt;.Clone()</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_DerivativeTensor_AiDotNet_Interfaces_IActivationFunction__0__AiDotNet_Tensors_LinearAlgebra_Tensor__0__">LayerBase&lt;T&gt;.DerivativeTensor(IActivationFunction&lt;T&gt;, Tensor&lt;T&gt;)</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_ApplyActivationDerivative__0__0_">LayerBase&lt;T&gt;.ApplyActivationDerivative(T, T)</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_ApplyActivationDerivative_AiDotNet_Tensors_LinearAlgebra_Tensor__0__AiDotNet_Tensors_LinearAlgebra_Tensor__0__">LayerBase&lt;T&gt;.ApplyActivationDerivative(Tensor&lt;T&gt;, Tensor&lt;T&gt;)</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_ComputeActivationJacobian_AiDotNet_Tensors_LinearAlgebra_Vector__0__">LayerBase&lt;T&gt;.ComputeActivationJacobian(Vector&lt;T&gt;)</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_ApplyActivationDerivative_AiDotNet_Tensors_LinearAlgebra_Vector__0__AiDotNet_Tensors_LinearAlgebra_Vector__0__">LayerBase&lt;T&gt;.ApplyActivationDerivative(Vector&lt;T&gt;, Vector&lt;T&gt;)</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_UpdateParameters_AiDotNet_Tensors_LinearAlgebra_Vector__0__">LayerBase&lt;T&gt;.UpdateParameters(Vector&lt;T&gt;)</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_Serialize_System_IO_BinaryWriter_">LayerBase&lt;T&gt;.Serialize(BinaryWriter)</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_Deserialize_System_IO_BinaryReader_">LayerBase&lt;T&gt;.Deserialize(BinaryReader)</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_ApplyActivationToGraph_AiDotNet_Autodiff_ComputationNode__0__">LayerBase&lt;T&gt;.ApplyActivationToGraph(ComputationNode&lt;T&gt;)</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_CanActivationBeJitted">LayerBase&lt;T&gt;.CanActivationBeJitted()</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_RegisterTrainableParameter_AiDotNet_Tensors_LinearAlgebra_Tensor__0__AiDotNet_Tensors_Engines_PersistentTensorRole_">LayerBase&lt;T&gt;.RegisterTrainableParameter(Tensor&lt;T&gt;, PersistentTensorRole)</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_InvalidateTrainableParameter_AiDotNet_Tensors_LinearAlgebra_Tensor__0__">LayerBase&lt;T&gt;.InvalidateTrainableParameter(Tensor&lt;T&gt;)</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_HasGpuActivation">LayerBase&lt;T&gt;.HasGpuActivation()</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_ApplyActivationForwardGpu_AiDotNet_Tensors_Engines_DirectGpu_IDirectGpuBackend_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_System_Int32_">LayerBase&lt;T&gt;.ApplyActivationForwardGpu(IDirectGpuBackend, IGpuBuffer, IGpuBuffer, int)</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_ApplyActivationBackwardGpu_AiDotNet_Tensors_Engines_DirectGpu_IDirectGpuBackend_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_System_Int32_">LayerBase&lt;T&gt;.ApplyActivationBackwardGpu(IDirectGpuBackend, IGpuBuffer, IGpuBuffer, IGpuBuffer, IGpuBuffer, int)</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_GetFusedActivationType">LayerBase&lt;T&gt;.GetFusedActivationType()</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_ApplyGpuActivation_AiDotNet_Tensors_Engines_DirectGpu_IDirectGpuBackend_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_System_Int32_AiDotNet_Tensors_Engines_FusedActivationType_">LayerBase&lt;T&gt;.ApplyGpuActivation(IDirectGpuBackend, IGpuBuffer, IGpuBuffer, int, FusedActivationType)</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_ApplyGpuActivationBackward_AiDotNet_Tensors_Engines_DirectGpu_IDirectGpuBackend_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_System_Int32_AiDotNet_Tensors_Engines_FusedActivationType_System_Single_">LayerBase&lt;T&gt;.ApplyGpuActivationBackward(IDirectGpuBackend, IGpuBuffer, IGpuBuffer, IGpuBuffer, IGpuBuffer, int, FusedActivationType, float)</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_Dispose">LayerBase&lt;T&gt;.Dispose()</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_Dispose_System_Boolean_">LayerBase&lt;T&gt;.Dispose(bool)</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_WeightParameterName">LayerBase&lt;T&gt;.WeightParameterName</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_BiasParameterName">LayerBase&lt;T&gt;.BiasParameterName</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_SetWeights_AiDotNet_Tensors_LinearAlgebra_Tensor__0__">LayerBase&lt;T&gt;.SetWeights(Tensor&lt;T&gt;)</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_SetBiases_AiDotNet_Tensors_LinearAlgebra_Tensor__0__">LayerBase&lt;T&gt;.SetBiases(Tensor&lt;T&gt;)</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_GetParameterNames">LayerBase&lt;T&gt;.GetParameterNames()</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_TryGetParameter_System_String_AiDotNet_Tensors_LinearAlgebra_Tensor__0___">LayerBase&lt;T&gt;.TryGetParameter(string, out Tensor&lt;T&gt;)</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_SetParameter_System_String_AiDotNet_Tensors_LinearAlgebra_Tensor__0__">LayerBase&lt;T&gt;.SetParameter(string, Tensor&lt;T&gt;)</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_GetParameterShape_System_String_">LayerBase&lt;T&gt;.GetParameterShape(string)</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_NamedParameterCount">LayerBase&lt;T&gt;.NamedParameterCount</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_ValidateWeights_System_Collections_Generic_IEnumerable_System_String__System_Func_System_String_System_String__">LayerBase&lt;T&gt;.ValidateWeights(IEnumerable&lt;string&gt;, Func&lt;string, string&gt;)</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_LoadWeights_System_Collections_Generic_Dictionary_System_String_AiDotNet_Tensors_LinearAlgebra_Tensor__0___System_Func_System_String_System_String__System_Boolean_">LayerBase&lt;T&gt;.LoadWeights(Dictionary&lt;string, Tensor&lt;T&gt;&gt;, Func&lt;string, string&gt;, bool)</a>
    </div>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.equals#system-object-equals(system-object)">object.Equals(object)</a>
    </div>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.equals#system-object-equals(system-object-system-object)">object.Equals(object, object)</a>
    </div>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.gethashcode">object.GetHashCode()</a>
    </div>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.gettype">object.GetType()</a>
    </div>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.memberwiseclone">object.MemberwiseClone()</a>
    </div>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.referenceequals">object.ReferenceEquals(object, object)</a>
    </div>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.tostring">object.ToString()</a>
    </div>
  </dd></dl>




  <h2 id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_remarks">Remarks</h2>
  <div class="markdown level0 remarks"><p>
The SelfAttentionLayer implements the self-attention mechanism, a key component of transformer architectures.
It allows each position in a sequence to attend to all positions within the same sequence, enabling the model
to capture long-range dependencies and relationships. The layer uses the scaled dot-product attention mechanism
with multiple attention heads, which allows it to focus on different aspects of the input simultaneously.
</p>
<p><b>For Beginners:</b> This layer helps a neural network understand relationships between different parts of a sequence.
<p>Think of the SelfAttentionLayer like a group of spotlights at a theater performance:</p>
<ul>
<li>Each spotlight (attention head) can focus on different actors on stage</li>
<li>For each actor, the spotlights decide which other actors are most relevant to them</li>
<li>The spotlights assign importance scores to these relationships</li>
<li>This helps the network understand who is interacting with whom, and how</li>
</ul>
<p>For example, in a sentence like &quot;The cat sat on the mat because it was tired&quot;:</p>
<ul>
<li>Traditional networks might struggle to figure out what &quot;it&quot; refers to</li>
<li>Self-attention can learn that &quot;it&quot; has a strong relationship with &quot;cat&quot;</li>
<li>This helps the network understand that the cat was tired, not the mat</li>
</ul>
<p>Multi-head attention (using multiple &quot;spotlights&quot;) allows the layer to focus on different types
of relationships simultaneously, such as grammatical structure, semantic meaning, and contextual clues.</p>
<p>Self-attention is a cornerstone of modern natural language processing and has revolutionized
how neural networks handle sequential data like text, time series, and even images.</p>

</div>


  <h2 class="section" id="constructors">Constructors
</h2>


  <a id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1__ctor_" data-uid="AiDotNet.NeuralNetworks.Layers.SelfAttentionLayer`1.#ctor*"></a>

  <h3 id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1__ctor_System_Int32_System_Int32_System_Int32_AiDotNet_Interfaces_IActivationFunction__0__" data-uid="AiDotNet.NeuralNetworks.Layers.SelfAttentionLayer`1.#ctor(System.Int32,System.Int32,System.Int32,AiDotNet.Interfaces.IActivationFunction{`0})">
  SelfAttentionLayer(int, int, int, IActivationFunction&lt;T&gt;?)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/NeuralNetworks/Layers/SelfAttentionLayer.cs/#L349"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Initializes a new instance of the <a class="xref" href="AiDotNet.NeuralNetworks.Layers.SelfAttentionLayer-1.html">SelfAttentionLayer&lt;T&gt;</a> class with a scalar activation function.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public SelfAttentionLayer(int sequenceLength, int embeddingDimension, int headCount = 8, IActivationFunction&lt;T&gt;? activationFunction = null)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>sequenceLength</code> <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.int32">int</a></dt>
    <dd><p>The length of the input sequence.</p>
</dd>
    <dt><code>embeddingDimension</code> <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.int32">int</a></dt>
    <dd><p>The dimension of the input and output embeddings.</p>
</dd>
    <dt><code>headCount</code> <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.int32">int</a></dt>
    <dd><p>The number of attention heads. Defaults to 8.</p>
</dd>
    <dt><code>activationFunction</code> <a class="xref" href="AiDotNet.Interfaces.IActivationFunction-1.html">IActivationFunction</a>&lt;T&gt;</dt>
    <dd><p>The activation function to apply to the output. Defaults to Identity if not specified.</p>
</dd>
  </dl>








  <h4 class="section" id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1__ctor_System_Int32_System_Int32_System_Int32_AiDotNet_Interfaces_IActivationFunction__0___remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
This constructor creates a new SelfAttentionLayer with the specified dimensions and a scalar activation function.
It validates that the embedding dimension is divisible by the number of heads and initializes the weight matrices
and bias vector with appropriate values. A scalar activation function is applied element-wise to each output
embedding independently.
</p>
<p><b>For Beginners:</b> This creates a new self-attention layer for your neural network using a simple activation function.
<p>When you create this layer, you specify:</p>
<ul>
<li>sequenceLength: How many items (like words) are in your sequence</li>
<li>embeddingDimension: How many features each item has</li>
<li>headCount: How many different &quot;spotlights&quot; the attention mechanism uses (default: 8)</li>
<li>activationFunction: How to transform the output (defaults to Identity, which makes no changes)</li>
</ul>
<p>For example, in a language model:</p>
<ul>
<li>sequenceLength might be 512 (the maximum number of words/tokens in a text)</li>
<li>embeddingDimension might be 768 (the number of features per word/token)</li>
<li>Using 8 attention heads lets the model focus on 8 different types of relationships</li>
</ul>
<p>The embedding dimension must be divisible by the number of heads (e.g., 768 ÷ 8 = 96),
so each head has the same dimension.</p>

</div>

  <h4 class="section">Exceptions</h4>
  <dl class="parameters">
    <dt><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.argumentexception">ArgumentException</a></dt>
    <dd><p>Thrown when the embedding dimension is not divisible by the number of heads.</p>
</dd>
  </dl>



  <a id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1__ctor_" data-uid="AiDotNet.NeuralNetworks.Layers.SelfAttentionLayer`1.#ctor*"></a>

  <h3 id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1__ctor_System_Int32_System_Int32_System_Int32_AiDotNet_Interfaces_IVectorActivationFunction__0__" data-uid="AiDotNet.NeuralNetworks.Layers.SelfAttentionLayer`1.#ctor(System.Int32,System.Int32,System.Int32,AiDotNet.Interfaces.IVectorActivationFunction{`0})">
  SelfAttentionLayer(int, int, int, IVectorActivationFunction&lt;T&gt;?)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/NeuralNetworks/Layers/SelfAttentionLayer.cs/#L409"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Initializes a new instance of the <a class="xref" href="AiDotNet.NeuralNetworks.Layers.SelfAttentionLayer-1.html">SelfAttentionLayer&lt;T&gt;</a> class with a vector activation function.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public SelfAttentionLayer(int sequenceLength, int embeddingDimension, int headCount = 8, IVectorActivationFunction&lt;T&gt;? vectorActivationFunction = null)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>sequenceLength</code> <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.int32">int</a></dt>
    <dd><p>The length of the input sequence.</p>
</dd>
    <dt><code>embeddingDimension</code> <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.int32">int</a></dt>
    <dd><p>The dimension of the input and output embeddings.</p>
</dd>
    <dt><code>headCount</code> <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.int32">int</a></dt>
    <dd><p>The number of attention heads. Defaults to 8.</p>
</dd>
    <dt><code>vectorActivationFunction</code> <a class="xref" href="AiDotNet.Interfaces.IVectorActivationFunction-1.html">IVectorActivationFunction</a>&lt;T&gt;</dt>
    <dd><p>The vector activation function to apply to the output. Defaults to Identity if not specified.</p>
</dd>
  </dl>








  <h4 class="section" id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1__ctor_System_Int32_System_Int32_System_Int32_AiDotNet_Interfaces_IVectorActivationFunction__0___remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
This constructor creates a new SelfAttentionLayer with the specified dimensions and a vector activation function.
It validates that the embedding dimension is divisible by the number of heads and initializes the weight tensors
and bias tensor with appropriate values. A vector activation function is applied to the entire output vector at once,
which allows for interactions between different output elements.
</p>
<p><b>For Beginners:</b> This creates a new self-attention layer for your neural network using an advanced activation function.
<p>When you create this layer, you specify the same parameters as in the scalar version, but with a vector activation:</p>
<ul>
<li>sequenceLength: How many items are in your sequence</li>
<li>embeddingDimension: How many features each item has</li>
<li>headCount: How many different &quot;spotlights&quot; the attention mechanism uses</li>
<li>vectorActivationFunction: How to transform the entire output as a group</li>
</ul>
<p>A vector activation can consider relationships between different positions in the output,
which might be useful for certain advanced applications.</p>
<p>This constructor works the same as the scalar version, but allows for more sophisticated
activation patterns across the output sequence.</p>

</div>

  <h4 class="section">Exceptions</h4>
  <dl class="parameters">
    <dt><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.argumentexception">ArgumentException</a></dt>
    <dd><p>Thrown when the embedding dimension is not divisible by the number of heads.</p>
</dd>
  </dl>



  <h2 class="section" id="properties">Properties
</h2>


  <a id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_AuxiliaryLossWeight_" data-uid="AiDotNet.NeuralNetworks.Layers.SelfAttentionLayer`1.AuxiliaryLossWeight*"></a>

  <h3 id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_AuxiliaryLossWeight" data-uid="AiDotNet.NeuralNetworks.Layers.SelfAttentionLayer`1.AuxiliaryLossWeight">
  AuxiliaryLossWeight
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/NeuralNetworks/Layers/SelfAttentionLayer.cs/#L84"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Gets or sets the weight for the attention sparsity auxiliary loss.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public T AuxiliaryLossWeight { get; set; }</code></pre>
  </div>





  <h4 class="section">Property Value</h4>
  <dl class="parameters">
    <dt><span class="xref">T</span></dt>
    <dd></dd>
  </dl>




  <h4 class="section" id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_AuxiliaryLossWeight_remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
This weight controls how much attention sparsity regularization contributes to the total loss.
Typical values range from 0.001 to 0.01.
</p>
<p><b>For Beginners:</b> This controls how much we encourage focused attention.
<p>Common values:</p>
<ul>
<li>0.005 (default): Balanced sparsity regularization</li>
<li>0.001-0.003: Light sparsity enforcement</li>
<li>0.008-0.01: Strong sparsity enforcement</li>
</ul>
<p>Higher values encourage sharper, more focused attention patterns.</p>

</div>




  <a id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_ParameterCount_" data-uid="AiDotNet.NeuralNetworks.Layers.SelfAttentionLayer`1.ParameterCount*"></a>

  <h3 id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_ParameterCount" data-uid="AiDotNet.NeuralNetworks.Layers.SelfAttentionLayer`1.ParameterCount">
  ParameterCount
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/NeuralNetworks/Layers/SelfAttentionLayer.cs/#L314"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Gets the total number of trainable parameters in this layer.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public override int ParameterCount { get; }</code></pre>
  </div>





  <h4 class="section">Property Value</h4>
  <dl class="parameters">
    <dt><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.int32">int</a></dt>
    <dd><p>The total number of parameters: 3 weight matrices (Q, K, V) each of size [embeddingDimension × embeddingDimension],
plus an output bias of size [embeddingDimension].
Total = 3 × E² + E = E × (3E + 1) where E is the embedding dimension.</p>
</dd>
  </dl>








  <a id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_SupportsGpuExecution_" data-uid="AiDotNet.NeuralNetworks.Layers.SelfAttentionLayer`1.SupportsGpuExecution*"></a>

  <h3 id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_SupportsGpuExecution" data-uid="AiDotNet.NeuralNetworks.Layers.SelfAttentionLayer`1.SupportsGpuExecution">
  SupportsGpuExecution
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/NeuralNetworks/Layers/SelfAttentionLayer.cs/#L304"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Gets a value indicating whether this layer supports GPU execution.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">protected override bool SupportsGpuExecution { get; }</code></pre>
  </div>





  <h4 class="section">Property Value</h4>
  <dl class="parameters">
    <dt><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.boolean">bool</a></dt>
    <dd></dd>
  </dl>








  <a id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_SupportsJitCompilation_" data-uid="AiDotNet.NeuralNetworks.Layers.SelfAttentionLayer`1.SupportsJitCompilation*"></a>

  <h3 id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_SupportsJitCompilation" data-uid="AiDotNet.NeuralNetworks.Layers.SelfAttentionLayer`1.SupportsJitCompilation">
  SupportsJitCompilation
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/NeuralNetworks/Layers/SelfAttentionLayer.cs/#L1798"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Gets whether this self-attention layer supports JIT compilation.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public override bool SupportsJitCompilation { get; }</code></pre>
  </div>





  <h4 class="section">Property Value</h4>
  <dl class="parameters">
    <dt><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.boolean">bool</a></dt>
    <dd><p>True if the layer parameters are initialized.</p>
</dd>
  </dl>




  <h4 class="section" id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_SupportsJitCompilation_remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
This property indicates whether the layer can be JIT compiled. The layer supports JIT if:
- Query, Key, Value projection weights are initialized
- The layer has been properly configured with sequence length and embedding dimensions
</p>
<p><b>For Beginners:</b> This tells you if this layer can use JIT compilation for faster inference.
<p>The layer can be JIT compiled if:</p>
<ul>
<li>The layer has been initialized with projection weight matrices (query, key, value weights)</li>
<li>The multi-head structure has been configured</li>
</ul>
<p>Self-attention layers are computationally expensive because each position attends to all
other positions in the sequence (O(n²) complexity). JIT compilation can provide significant
speedup (5-10x) by optimizing:</p>
<ul>
<li>Parallel matrix multiplications for projections</li>
<li>Multi-head attention score computation across heads</li>
<li>Softmax operations for attention weights</li>
<li>Weighted sums of values across all heads</li>
</ul>
<p>This is especially critical for Transformers where self-attention is the bottleneck:</p>
<ul>
<li>BERT has 12-24 self-attention layers</li>
<li>GPT-3 has 96 self-attention layers</li>
<li>Vision Transformers process image patches as sequences</li>
</ul>
<p>JIT compilation makes these models practical for production use.</p>

</div>




  <a id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_SupportsTraining_" data-uid="AiDotNet.NeuralNetworks.Layers.SelfAttentionLayer`1.SupportsTraining*"></a>

  <h3 id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_SupportsTraining" data-uid="AiDotNet.NeuralNetworks.Layers.SelfAttentionLayer`1.SupportsTraining">
  SupportsTraining
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/NeuralNetworks/Layers/SelfAttentionLayer.cs/#L299"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Gets a value indicating whether this layer supports training.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public override bool SupportsTraining { get; }</code></pre>
  </div>





  <h4 class="section">Property Value</h4>
  <dl class="parameters">
    <dt><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.boolean">bool</a></dt>
    <dd><p>Always <code>true</code> for SelfAttentionLayer, indicating that the layer can be trained through backpropagation.</p>
</dd>
  </dl>




  <h4 class="section" id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_SupportsTraining_remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
This property indicates that the SelfAttentionLayer has trainable parameters (query, key, and value weights,
as well as output biases) that can be optimized during the training process using backpropagation. The gradients
of these parameters are calculated during the backward pass and used to update the parameters.
</p>
<p><b>For Beginners:</b> This property tells you if the layer can learn from data.
<p>A value of true means:</p>
<ul>
<li>The layer has values (weights and biases) that can be adjusted during training</li>
<li>It will improve its performance as it sees more data</li>
<li>It participates in the learning process of the neural network</li>
</ul>
<p>When you train a neural network containing this layer, it will automatically learn
which relationships between sequence positions are important for your specific task.</p>

</div>




  <a id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_UseAuxiliaryLoss_" data-uid="AiDotNet.NeuralNetworks.Layers.SelfAttentionLayer`1.UseAuxiliaryLoss*"></a>

  <h3 id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_UseAuxiliaryLoss" data-uid="AiDotNet.NeuralNetworks.Layers.SelfAttentionLayer`1.UseAuxiliaryLoss">
  UseAuxiliaryLoss
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/NeuralNetworks/Layers/SelfAttentionLayer.cs/#L64"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Gets or sets whether auxiliary loss (attention sparsity regularization) should be used during training.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public bool UseAuxiliaryLoss { get; set; }</code></pre>
  </div>





  <h4 class="section">Property Value</h4>
  <dl class="parameters">
    <dt><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.boolean">bool</a></dt>
    <dd></dd>
  </dl>




  <h4 class="section" id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_UseAuxiliaryLoss_remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
Attention sparsity regularization encourages the attention mechanism to focus on relevant positions
while ignoring irrelevant ones. This prevents attention from being too diffuse and improves interpretability.
</p>
<p><b>For Beginners:</b> This helps self-attention focus on what matters.
<p>Self-attention works best when it's selective:</p>
<ul>
<li>Without regularization: Attention might spread too thin across all positions</li>
<li>With regularization: Attention focuses on truly relevant relationships</li>
</ul>
<p>This includes:</p>
<ol>
<li>Entropy regularization: Prevents overly uniform attention</li>
<li>Sparsity penalties: Encourages sharp, focused attention patterns</li>
</ol>
<p>This helps the model:</p>
<ul>
<li>Learn clearer, more interpretable attention patterns</li>
<li>Focus computational resources on relevant relationships</li>
<li>Improve robustness and generalization</li>
</ul>

</div>




  <h2 class="section" id="methods">Methods
</h2>


  <a id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_Backward_" data-uid="AiDotNet.NeuralNetworks.Layers.SelfAttentionLayer`1.Backward*"></a>

  <h3 id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_Backward_AiDotNet_Tensors_LinearAlgebra_Tensor__0__" data-uid="AiDotNet.NeuralNetworks.Layers.SelfAttentionLayer`1.Backward(AiDotNet.Tensors.LinearAlgebra.Tensor{`0})">
  Backward(Tensor&lt;T&gt;)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/NeuralNetworks/Layers/SelfAttentionLayer.cs/#L778"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Performs the backward pass of the self-attention layer.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public override Tensor&lt;T&gt; Backward(Tensor&lt;T&gt; outputGradient)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>outputGradient</code> <a class="xref" href="AiDotNet.Tensors.LinearAlgebra.Tensor-1.html">Tensor</a>&lt;T&gt;</dt>
    <dd><p>The gradient of the loss with respect to the layer's output.</p>
</dd>
  </dl>

  <h4 class="section">Returns</h4>
  <dl class="parameters">
    <dt><a class="xref" href="AiDotNet.Tensors.LinearAlgebra.Tensor-1.html">Tensor</a>&lt;T&gt;</dt>
    <dd><p>The gradient of the loss with respect to the layer's input.</p>
</dd>
  </dl>







  <h4 class="section" id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_Backward_AiDotNet_Tensors_LinearAlgebra_Tensor__0___remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
This method implements the backward pass of the self-attention layer, which is used during training
to propagate error gradients back through the network. It calculates the gradients of the loss
with respect to the layer's parameters (query, key, and value weights, as well as output biases)
and with respect to the layer's input. The calculation involves complex tensor operations that
essentially reverse the computations done in the forward pass.
</p>
<p><b>For Beginners:</b> This method calculates how the layer's parameters should change to reduce errors.
<p>During the backward pass:</p>
<ol>
<li>The layer receives error gradients indicating how the output should change</li>
<li>It calculates how each of its internal components contributed to the error:
<ul>
<li>How the query weights should change</li>
<li>How the key weights should change</li>
<li>How the value weights should change</li>
<li>How the output biases should change</li>
</ul>
</li>
<li>It also calculates how the error should propagate back to the previous layer</li>
</ol>
<p>This involves complex matrix mathematics, but the basic idea is:</p>
<ul>
<li>Finding which attention patterns led to errors</li>
<li>Adjusting the weights to improve these patterns</li>
<li>Sending appropriate feedback to the previous layer</li>
</ul>
<p>The backward pass is what allows the self-attention mechanism to learn which relationships
in the sequence are important for the specific task.</p>

</div>

  <h4 class="section">Exceptions</h4>
  <dl class="parameters">
    <dt><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.invalidoperationexception">InvalidOperationException</a></dt>
    <dd><p>Thrown when backward is called before forward.</p>
</dd>
  </dl>



  <a id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_BackwardGpu_" data-uid="AiDotNet.NeuralNetworks.Layers.SelfAttentionLayer`1.BackwardGpu*"></a>

  <h3 id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_BackwardGpu_AiDotNet_Tensors_Engines_Gpu_IGpuTensor__0__" data-uid="AiDotNet.NeuralNetworks.Layers.SelfAttentionLayer`1.BackwardGpu(AiDotNet.Tensors.Engines.Gpu.IGpuTensor{`0})">
  BackwardGpu(IGpuTensor&lt;T&gt;)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/NeuralNetworks/Layers/SelfAttentionLayer.cs/#L873"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Performs the backward pass using GPU-resident tensors.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public override IGpuTensor&lt;T&gt; BackwardGpu(IGpuTensor&lt;T&gt; outputGradient)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>outputGradient</code> <a class="xref" href="AiDotNet.Tensors.Engines.Gpu.IGpuTensor-1.html">IGpuTensor</a>&lt;T&gt;</dt>
    <dd><p>GPU-resident gradient of the loss w.r.t. output.</p>
</dd>
  </dl>

  <h4 class="section">Returns</h4>
  <dl class="parameters">
    <dt><a class="xref" href="AiDotNet.Tensors.Engines.Gpu.IGpuTensor-1.html">IGpuTensor</a>&lt;T&gt;</dt>
    <dd><p>GPU-resident gradient of the loss w.r.t. input.</p>
</dd>
  </dl>











  <a id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_ComputeAuxiliaryLoss_" data-uid="AiDotNet.NeuralNetworks.Layers.SelfAttentionLayer`1.ComputeAuxiliaryLoss*"></a>

  <h3 id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_ComputeAuxiliaryLoss" data-uid="AiDotNet.NeuralNetworks.Layers.SelfAttentionLayer`1.ComputeAuxiliaryLoss">
  ComputeAuxiliaryLoss()
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/NeuralNetworks/Layers/SelfAttentionLayer.cs/#L1456"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Initializes the layer's internal parameters based on the sequence length, embedding dimension, and head count.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public T ComputeAuxiliaryLoss()</code></pre>
  </div>


  <h4 class="section">Returns</h4>
  <dl class="parameters">
    <dt><span class="xref">T</span></dt>
    <dd><p>The computed attention sparsity auxiliary loss.</p>
</dd>
  </dl>







  <h4 class="section" id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_ComputeAuxiliaryLoss_remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
This private method initializes the internal parameters of the self-attention layer based on the specified
dimensions. It validates that the embedding dimension is divisible by the number of heads, calculates the
dimension of each head, and then calls InitializeParameters to set up the weight matrices and bias vector.
This method is called by both constructors.
</p>
<p><b>For Beginners:</b> This method sets up the internal structure of the self-attention layer.
<p>During initialization:</p>
<ul>
<li>The method saves the basic dimensions (sequence length, embedding size, head count)</li>
<li>It calculates how large each attention head should be</li>
<li>It verifies that the embedding dimension can be evenly divided by the head count</li>
<li>It triggers the creation of all the weight matrices with proper initial values</li>
</ul>
<p>The head dimension calculation is important - if you have an embedding size of 512 and
8 attention heads, each head will have a dimension of 64 (512 ÷ 8). This allows each
head to specialize in different aspects of the input sequence.</p>
<p>This method throws an error if the embedding dimension isn't divisible by the head count
because the attention mechanism requires equal-sized heads.</p>

</div>

  <h4 class="section">Exceptions</h4>
  <dl class="parameters">
    <dt><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.argumentexception">ArgumentException</a></dt>
    <dd><p>Thrown when the embedding dimension is not divisible by the number of heads.</p>
</dd>
  </dl>



  <a id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_ExportComputationGraph_" data-uid="AiDotNet.NeuralNetworks.Layers.SelfAttentionLayer`1.ExportComputationGraph*"></a>

  <h3 id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_ExportComputationGraph_System_Collections_Generic_List_AiDotNet_Autodiff_ComputationNode__0___" data-uid="AiDotNet.NeuralNetworks.Layers.SelfAttentionLayer`1.ExportComputationGraph(System.Collections.Generic.List{AiDotNet.Autodiff.ComputationNode{`0}})">
  ExportComputationGraph(List&lt;ComputationNode&lt;T&gt;&gt;)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/NeuralNetworks/Layers/SelfAttentionLayer.cs/#L1718"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Exports the self-attention layer as a computation graph for JIT compilation.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public override ComputationNode&lt;T&gt; ExportComputationGraph(List&lt;ComputationNode&lt;T&gt;&gt; inputNodes)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>inputNodes</code> <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.collections.generic.list-1">List</a>&lt;<a class="xref" href="AiDotNet.Autodiff.ComputationNode-1.html">ComputationNode</a>&lt;T&gt;&gt;</dt>
    <dd><p>List to which the input node will be added.</p>
</dd>
  </dl>

  <h4 class="section">Returns</h4>
  <dl class="parameters">
    <dt><a class="xref" href="AiDotNet.Autodiff.ComputationNode-1.html">ComputationNode</a>&lt;T&gt;</dt>
    <dd><p>The output computation node representing the self-attention operation.</p>
</dd>
  </dl>







  <h4 class="section" id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_ExportComputationGraph_System_Collections_Generic_List_AiDotNet_Autodiff_ComputationNode__0____remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
This method creates a symbolic computation graph for JIT compilation:
1. Creates a symbolic input node with shape [batch=1, sequenceLength, embeddingDimension]
2. Creates constant nodes for Query, Key, Value projection weights
3. Projects input to Q, K, V using matrix multiplication (self-attention: all from same input)
4. Applies multi-head scaled dot-product attention mechanism
5. Returns the attention output with residual connection and bias
</p>
<p><b>For Beginners:</b> This method builds a symbolic representation of self-attention for JIT.
<p>JIT compilation converts multi-head self-attention into optimized native code.
Self-attention allows each position in a sequence to attend to all positions, enabling
the model to capture long-range dependencies and relationships within the sequence.</p>
<p>Multi-head attention uses multiple parallel attention mechanisms (&quot;heads&quot;) that:</p>
<ul>
<li>Focus on different aspects of the input simultaneously</li>
<li>Allow the model to capture diverse relationships (syntax, semantics, context)</li>
<li>Improve the model's ability to understand complex patterns</li>
</ul>
<p>The symbolic graph allows the JIT compiler to:</p>
<ul>
<li>Optimize parallel matrix multiplications across heads</li>
<li>Fuse attention score computation and softmax</li>
<li>Generate efficient memory layouts for multi-head processing</li>
<li>Optimize the split and concatenation operations for heads</li>
</ul>
<p>Self-attention is the core of Transformer architectures (BERT, GPT, Vision Transformers).
JIT compilation provides 5-10x speedup by optimizing these complex operations.</p>

</div>

  <h4 class="section">Exceptions</h4>
  <dl class="parameters">
    <dt><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.argumentnullexception">ArgumentNullException</a></dt>
    <dd><p>Thrown when inputNodes is null.</p>
</dd>
    <dt><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.invalidoperationexception">InvalidOperationException</a></dt>
    <dd><p>Thrown when layer parameters are not initialized.</p>
</dd>
  </dl>



  <a id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_Forward_" data-uid="AiDotNet.NeuralNetworks.Layers.SelfAttentionLayer`1.Forward*"></a>

  <h3 id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_Forward_AiDotNet_Tensors_LinearAlgebra_Tensor__0__" data-uid="AiDotNet.NeuralNetworks.Layers.SelfAttentionLayer`1.Forward(AiDotNet.Tensors.LinearAlgebra.Tensor{`0})">
  Forward(Tensor&lt;T&gt;)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/NeuralNetworks/Layers/SelfAttentionLayer.cs/#L471"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Performs the forward pass of the self-attention layer.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public override Tensor&lt;T&gt; Forward(Tensor&lt;T&gt; input)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>input</code> <a class="xref" href="AiDotNet.Tensors.LinearAlgebra.Tensor-1.html">Tensor</a>&lt;T&gt;</dt>
    <dd><p>The input tensor to process, with shape [batchSize, sequenceLength, embeddingDimension].</p>
</dd>
  </dl>

  <h4 class="section">Returns</h4>
  <dl class="parameters">
    <dt><a class="xref" href="AiDotNet.Tensors.LinearAlgebra.Tensor-1.html">Tensor</a>&lt;T&gt;</dt>
    <dd><p>The output tensor after self-attention, with the same shape as the input.</p>
</dd>
  </dl>







  <h4 class="section" id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_Forward_AiDotNet_Tensors_LinearAlgebra_Tensor__0___remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
This method implements the forward pass of the self-attention layer. It transforms the input into queries,
keys, and values, then computes attention scores between each position and all other positions. These scores
are normalized using the softmax function and used to compute a weighted sum of the values. The result is
transformed back to the original embedding dimension and passed through an activation function.
</p>
<p><b>For Beginners:</b> This method processes your sequence data through the self-attention mechanism.
<p>During the forward pass:</p>
<ol>
<li>The input sequence is transformed into three different representations:
<ul>
<li>Queries: What each position is looking for</li>
<li>Keys: What each position has to offer</li>
<li>Values: The actual content at each position</li>
</ul>
</li>
<li>For each position, attention scores are computed by comparing its query with all keys</li>
<li>These scores are scaled and normalized to create attention weights</li>
<li>Each position's output is a weighted sum of all values, based on the attention weights</li>
<li>The result is transformed and passed through an activation function</li>
</ol>
<p>Imagine a classroom where each student (position) asks a question (query) to the entire class.
Other students offer answers (keys) and knowledge (values). Each student pays more attention
to the most relevant answers and combines that knowledge to form their own understanding.</p>
<p>The multi-head mechanism allows this process to happen in parallel with different &quot;perspectives&quot;
or types of questions.</p>

</div>




  <a id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_ForwardGpu_" data-uid="AiDotNet.NeuralNetworks.Layers.SelfAttentionLayer`1.ForwardGpu*"></a>

  <h3 id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_ForwardGpu_AiDotNet_Tensors_Engines_Gpu_IGpuTensor__0____" data-uid="AiDotNet.NeuralNetworks.Layers.SelfAttentionLayer`1.ForwardGpu(AiDotNet.Tensors.Engines.Gpu.IGpuTensor{`0}[])">
  ForwardGpu(params IGpuTensor&lt;T&gt;[])
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/NeuralNetworks/Layers/SelfAttentionLayer.cs/#L598"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Performs the forward pass using GPU-resident tensors.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public override IGpuTensor&lt;T&gt; ForwardGpu(params IGpuTensor&lt;T&gt;[] inputs)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>inputs</code> <a class="xref" href="AiDotNet.Tensors.Engines.Gpu.IGpuTensor-1.html">IGpuTensor</a>&lt;T&gt;[]</dt>
    <dd></dd>
  </dl>

  <h4 class="section">Returns</h4>
  <dl class="parameters">
    <dt><a class="xref" href="AiDotNet.Tensors.Engines.Gpu.IGpuTensor-1.html">IGpuTensor</a>&lt;T&gt;</dt>
    <dd><p>A GPU-resident output tensor.</p>
</dd>
  </dl>







  <h4 class="section" id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_ForwardGpu_AiDotNet_Tensors_Engines_Gpu_IGpuTensor__0_____remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
This method performs the entire self-attention forward pass on the GPU without downloading
intermediate results to CPU. All projections, attention computation, and bias addition
remain GPU-resident for maximum performance.
</p>
</div>




  <a id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_GetAuxiliaryLossDiagnostics_" data-uid="AiDotNet.NeuralNetworks.Layers.SelfAttentionLayer`1.GetAuxiliaryLossDiagnostics*"></a>

  <h3 id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_GetAuxiliaryLossDiagnostics" data-uid="AiDotNet.NeuralNetworks.Layers.SelfAttentionLayer`1.GetAuxiliaryLossDiagnostics">
  GetAuxiliaryLossDiagnostics()
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/NeuralNetworks/Layers/SelfAttentionLayer.cs/#L1544"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Gets diagnostic information about the attention sparsity auxiliary loss.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public Dictionary&lt;string, string&gt; GetAuxiliaryLossDiagnostics()</code></pre>
  </div>


  <h4 class="section">Returns</h4>
  <dl class="parameters">
    <dt><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.collections.generic.dictionary-2">Dictionary</a>&lt;<a class="xref" href="https://learn.microsoft.com/dotnet/api/system.string">string</a>, <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.string">string</a>&gt;</dt>
    <dd><p>A dictionary containing diagnostic information about attention regularization.</p>
</dd>
  </dl>







  <h4 class="section" id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_GetAuxiliaryLossDiagnostics_remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
This method returns detailed diagnostics about attention sparsity regularization, including
entropy loss, sparsity penalty, and configuration parameters.
This information is useful for monitoring training progress and debugging attention patterns.
</p>
<p><b>For Beginners:</b> This provides information about how attention regularization is working.
<p>The diagnostics include:</p>
<ul>
<li>Total entropy loss (how focused attention patterns are)</li>
<li>Total sparsity loss (L1 penalty on attention weights)</li>
<li>Weight applied to the regularization</li>
<li>Whether regularization is enabled</li>
<li>Number of attention heads</li>
</ul>
<p>This helps you:</p>
<ul>
<li>Monitor if attention is becoming too diffuse or too sharp</li>
<li>Debug issues with attention patterns</li>
<li>Understand the impact of regularization on learning</li>
</ul>
<p>You can use this information to adjust regularization weights for better results.</p>

</div>




  <a id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_GetDiagnostics_" data-uid="AiDotNet.NeuralNetworks.Layers.SelfAttentionLayer`1.GetDiagnostics*"></a>

  <h3 id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_GetDiagnostics" data-uid="AiDotNet.NeuralNetworks.Layers.SelfAttentionLayer`1.GetDiagnostics">
  GetDiagnostics()
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/NeuralNetworks/Layers/SelfAttentionLayer.cs/#L1566"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Gets diagnostic information about this component's state and behavior.
Overrides <a class="xref" href="AiDotNet.NeuralNetworks.Layers.LayerBase-1.html#AiDotNet_NeuralNetworks_Layers_LayerBase_1_GetDiagnostics">GetDiagnostics()</a> to include auxiliary loss diagnostics.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public override Dictionary&lt;string, string&gt; GetDiagnostics()</code></pre>
  </div>


  <h4 class="section">Returns</h4>
  <dl class="parameters">
    <dt><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.collections.generic.dictionary-2">Dictionary</a>&lt;<a class="xref" href="https://learn.microsoft.com/dotnet/api/system.string">string</a>, <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.string">string</a>&gt;</dt>
    <dd><p>A dictionary containing diagnostic metrics including both base layer diagnostics and
auxiliary loss diagnostics from <a class="xref" href="AiDotNet.NeuralNetworks.Layers.SelfAttentionLayer-1.html#AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_GetAuxiliaryLossDiagnostics">GetAuxiliaryLossDiagnostics()</a>.</p>
</dd>
  </dl>











  <a id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_GetParameters_" data-uid="AiDotNet.NeuralNetworks.Layers.SelfAttentionLayer`1.GetParameters*"></a>

  <h3 id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_GetParameters" data-uid="AiDotNet.NeuralNetworks.Layers.SelfAttentionLayer`1.GetParameters">
  GetParameters()
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/NeuralNetworks/Layers/SelfAttentionLayer.cs/#L1218"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Gets all trainable parameters of the self-attention layer as a single vector.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public override Vector&lt;T&gt; GetParameters()</code></pre>
  </div>


  <h4 class="section">Returns</h4>
  <dl class="parameters">
    <dt><a class="xref" href="AiDotNet.Tensors.LinearAlgebra.Vector-1.html">Vector</a>&lt;T&gt;</dt>
    <dd><p>A vector containing all trainable parameters (query weights, key weights, value weights, and output biases).</p>
</dd>
  </dl>







  <h4 class="section" id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_GetParameters_remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
This method retrieves all trainable parameters of the self-attention layer as a single vector. The query weights
are stored first, followed by the key weights, value weights, and finally the output biases. This is useful for
optimization algorithms that operate on all parameters at once, or for saving and loading model weights.
</p>
<p><b>For Beginners:</b> This method collects all the learnable values from the self-attention layer.
<p>The parameters:</p>
<ul>
<li>Are the weights and biases that the self-attention layer learns during training</li>
<li>Control how the layer processes sequence information</li>
<li>Are returned as a single list (vector)</li>
</ul>
<p>This is useful for:</p>
<ul>
<li>Saving the model to disk</li>
<li>Loading parameters from a previously trained model</li>
<li>Advanced optimization techniques that need access to all parameters</li>
</ul>
<p>The query weights are stored first in the vector, followed by the key weights, value weights,
and finally the output biases.</p>

</div>




  <a id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_ResetState_" data-uid="AiDotNet.NeuralNetworks.Layers.SelfAttentionLayer`1.ResetState*"></a>

  <h3 id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_ResetState" data-uid="AiDotNet.NeuralNetworks.Layers.SelfAttentionLayer`1.ResetState">
  ResetState()
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/NeuralNetworks/Layers/SelfAttentionLayer.cs/#L1379"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Resets the internal state of the self-attention layer.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public override void ResetState()</code></pre>
  </div>









  <h4 class="section" id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_ResetState_remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
This method resets the internal state of the self-attention layer, including the cached inputs, outputs,
attention scores from the forward pass, and the gradients from the backward pass. This is useful when
starting to process a new batch of data.
</p>
<p><b>For Beginners:</b> This method clears the layer's memory to start fresh.
<p>When resetting the state:</p>
<ul>
<li>Stored inputs, outputs, and attention scores from previous calculations are cleared</li>
<li>Calculated gradients for all weights and biases are cleared</li>
<li>The layer forgets any information from previous batches</li>
</ul>
<p>This is important for:</p>
<ul>
<li>Processing a new, unrelated batch of data</li>
<li>Preventing information from one batch affecting another</li>
<li>Managing memory usage efficiently</li>
</ul>
<p>Since the self-attention layer caches quite a bit of information during the forward
and backward passes, resetting the state helps prevent memory leaks and ensures
each new sequence is processed independently.</p>

</div>




  <a id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_SetParameters_" data-uid="AiDotNet.NeuralNetworks.Layers.SelfAttentionLayer`1.SetParameters*"></a>

  <h3 id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_SetParameters_AiDotNet_Tensors_LinearAlgebra_Vector__0__" data-uid="AiDotNet.NeuralNetworks.Layers.SelfAttentionLayer`1.SetParameters(AiDotNet.Tensors.LinearAlgebra.Vector{`0})">
  SetParameters(Vector&lt;T&gt;)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/NeuralNetworks/Layers/SelfAttentionLayer.cs/#L1296"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Sets the trainable parameters of the self-attention layer.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public override void SetParameters(Vector&lt;T&gt; parameters)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>parameters</code> <a class="xref" href="AiDotNet.Tensors.LinearAlgebra.Vector-1.html">Vector</a>&lt;T&gt;</dt>
    <dd><p>A vector containing all parameters (query weights, key weights, value weights, and output biases) to set.</p>
</dd>
  </dl>








  <h4 class="section" id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_SetParameters_AiDotNet_Tensors_LinearAlgebra_Vector__0___remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
This method sets the trainable parameters of the self-attention layer from a single vector. The vector should
contain the query weight values first, followed by the key weight values, value weight values, and finally
the output bias values. This is useful for loading saved model weights or for implementing optimization
algorithms that operate on all parameters at once.
</p>
<p><b>For Beginners:</b> This method updates all the weights and biases in the self-attention layer.
<p>When setting parameters:</p>
<ul>
<li>The input must be a vector with the correct total length</li>
<li>The first part of the vector is used for the query weights</li>
<li>The second part of the vector is used for the key weights</li>
<li>The third part of the vector is used for the value weights</li>
<li>The last part of the vector is used for the output biases</li>
</ul>
<p>This is useful for:</p>
<ul>
<li>Loading a previously saved model</li>
<li>Transferring parameters from another model</li>
<li>Testing different parameter values</li>
</ul>
<p>An error is thrown if the input vector doesn't have the expected number of parameters.</p>

</div>

  <h4 class="section">Exceptions</h4>
  <dl class="parameters">
    <dt><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.argumentexception">ArgumentException</a></dt>
    <dd><p>Thrown when the parameters vector has incorrect length.</p>
</dd>
  </dl>



  <a id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_UpdateParameters_" data-uid="AiDotNet.NeuralNetworks.Layers.SelfAttentionLayer`1.UpdateParameters*"></a>

  <h3 id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_UpdateParameters__0_" data-uid="AiDotNet.NeuralNetworks.Layers.SelfAttentionLayer`1.UpdateParameters(`0)">
  UpdateParameters(T)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/NeuralNetworks/Layers/SelfAttentionLayer.cs/#L1144"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Updates the parameters of the self-attention layer using the calculated gradients.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public override void UpdateParameters(T learningRate)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>learningRate</code> <span class="xref">T</span></dt>
    <dd><p>The learning rate to use for the parameter updates.</p>
</dd>
  </dl>








  <h4 class="section" id="AiDotNet_NeuralNetworks_Layers_SelfAttentionLayer_1_UpdateParameters__0__remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
This method updates the query weights, key weights, value weights, and output biases of the self-attention
layer based on the gradients calculated during the backward pass. The learning rate controls the size of the
parameter updates. This method should be called after the backward pass to apply the calculated updates.
</p>
<p><b>For Beginners:</b> This method updates the layer's internal values during training.
<p>When updating parameters:</p>
<ol>
<li>The query weight values are adjusted based on their gradients</li>
<li>The key weight values are adjusted based on their gradients</li>
<li>The value weight values are adjusted based on their gradients</li>
<li>The output bias values are adjusted based on their gradients</li>
<li>The learning rate controls how big each update step is</li>
</ol>
<p>These updates help the self-attention mechanism:</p>
<ul>
<li>Focus on more relevant relationships between positions</li>
<li>Ignore irrelevant relationships</li>
<li>Better understand the structure of your sequences</li>
</ul>
<p>Smaller learning rates mean slower but more stable learning, while larger learning rates
mean faster but potentially unstable learning.</p>

</div>

  <h4 class="section">Exceptions</h4>
  <dl class="parameters">
    <dt><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.invalidoperationexception">InvalidOperationException</a></dt>
    <dd><p>Thrown when UpdateParameters is called before Backward.</p>
</dd>
  </dl>




</article>

        <div class="contribution d-print-none">
          <a href="https://github.com/ooples/AiDotNet/blob/master/src/NeuralNetworks/Layers/SelfAttentionLayer.cs/#L38" class="edit-link">Edit this page</a>
        </div>


      </div>

      <div class="affix">
        <nav id="affix"></nav>
      </div>
    </main>

    <div class="container-xxl search-results" id="search-results"></div>

    <footer class="border-top text-secondary">
      <div class="container-xxl">
        <div class="flex-fill">
          AiDotNet - Enterprise AI/ML Library for .NET
        </div>
      </div>
    </footer>
  </body>
</html>
