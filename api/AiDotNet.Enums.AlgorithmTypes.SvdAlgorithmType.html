<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
      <title>Enum SvdAlgorithmType | AiDotNet Documentation </title>
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <meta name="title" content="Enum SvdAlgorithmType | AiDotNet Documentation ">
      
      <meta name="description" content="Represents different algorithm types for Singular Value Decomposition (SVD).">
      <link rel="icon" href="../favicon.ico">
      <link rel="stylesheet" href="../public/docfx.min.css">
      <link rel="stylesheet" href="../public/main.css">
      <meta name="docfx:navrel" content="../toc.html">
      <meta name="docfx:tocrel" content="toc.html">
      
      <meta name="docfx:rel" content="../">
      
      
      <meta name="docfx:docurl" content="https://github.com/ooples/AiDotNet/new/master/apiSpec/new?filename=AiDotNet_Enums_AlgorithmTypes_SvdAlgorithmType.md&amp;value=---%0Auid%3A%20AiDotNet.Enums.AlgorithmTypes.SvdAlgorithmType%0Asummary%3A%20&#39;*You%20can%20override%20summary%20for%20the%20API%20here%20using%20*MARKDOWN*%20syntax&#39;%0A---%0A%0A*Please%20type%20below%20more%20information%20about%20this%20API%3A*%0A%0A">
      <meta name="loc:inThisArticle" content="In this article">
      <meta name="loc:searchResultsCount" content="{count} results for &quot;{query}&quot;">
      <meta name="loc:searchNoResults" content="No results for &quot;{query}&quot;">
      <meta name="loc:tocFilter" content="Filter by title">
      <meta name="loc:nextArticle" content="Next">
      <meta name="loc:prevArticle" content="Previous">
      <meta name="loc:themeLight" content="Light">
      <meta name="loc:themeDark" content="Dark">
      <meta name="loc:themeAuto" content="Auto">
      <meta name="loc:changeTheme" content="Change theme">
      <meta name="loc:copy" content="Copy">
      <meta name="loc:downloadPdf" content="Download PDF">

      <script type="module" src="./../public/docfx.min.js"></script>

      <script>
        const theme = localStorage.getItem('theme') || 'auto'
        document.documentElement.setAttribute('data-bs-theme', theme === 'auto' ? (window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light') : theme)
      </script>

  </head>

  <body class="tex2jax_ignore" data-layout="" data-yaml-mime="ManagedReference">
    <header class="bg-body border-bottom">
      <nav id="autocollapse" class="navbar navbar-expand-md" role="navigation">
        <div class="container-xxl flex-nowrap">
          <a class="navbar-brand" href="../index.html">
            <img id="logo" class="svg" src="../logo.svg" alt="AiDotNet">
            AiDotNet
          </a>
          <button class="btn btn-lg d-md-none border-0" type="button" data-bs-toggle="collapse" data-bs-target="#navpanel" aria-controls="navpanel" aria-expanded="false" aria-label="Toggle navigation">
            <i class="bi bi-three-dots"></i>
          </button>
          <div class="collapse navbar-collapse" id="navpanel">
            <div id="navbar">
              <form class="search" role="search" id="search">
                <i class="bi bi-search"></i>
                <input class="form-control" id="search-query" type="search" disabled placeholder="Search" autocomplete="off" aria-label="Search">
              </form>
            </div>
          </div>
        </div>
      </nav>
    </header>

    <main class="container-xxl">
      <div class="toc-offcanvas">
        <div class="offcanvas-md offcanvas-start" tabindex="-1" id="tocOffcanvas" aria-labelledby="tocOffcanvasLabel">
          <div class="offcanvas-header">
            <h5 class="offcanvas-title" id="tocOffcanvasLabel">Table of Contents</h5>
            <button type="button" class="btn-close" data-bs-dismiss="offcanvas" data-bs-target="#tocOffcanvas" aria-label="Close"></button>
          </div>
          <div class="offcanvas-body">
            <nav class="toc" id="toc"></nav>
          </div>
        </div>
      </div>

      <div class="content">
        <div class="actionbar">
          <button class="btn btn-lg border-0 d-md-none" type="button" data-bs-toggle="offcanvas" data-bs-target="#tocOffcanvas" aria-controls="tocOffcanvas" aria-expanded="false" aria-label="Show table of contents">
            <i class="bi bi-list"></i>
          </button>

          <nav id="breadcrumb"></nav>
        </div>

        <article data-uid="AiDotNet.Enums.AlgorithmTypes.SvdAlgorithmType">




  <h1 id="AiDotNet_Enums_AlgorithmTypes_SvdAlgorithmType" data-uid="AiDotNet.Enums.AlgorithmTypes.SvdAlgorithmType" class="text-break">
Enum SvdAlgorithmType  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/Enums/AlgorithmTypes/SvdAlgorithmType.cs/#L40"><i class="bi bi-code-slash"></i></a>
  </h1>

  <div class="facts text-secondary">
    <dl><dt>Namespace</dt><dd><a class="xref" href="AiDotNet.html">AiDotNet</a>.<a class="xref" href="AiDotNet.Enums.html">Enums</a>.<a class="xref" href="AiDotNet.Enums.AlgorithmTypes.html">AlgorithmTypes</a></dd></dl>
  <dl><dt>Assembly</dt><dd>AiDotNet.dll</dd></dl>
  </div>

  <div class="markdown summary"><p>Represents different algorithm types for Singular Value Decomposition (SVD).</p>
</div>
  <div class="markdown conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public enum SvdAlgorithmType</code></pre>
  </div>









  <h2 id="fields">Fields
</h2>
  <dl class="parameters">
    <dt id="AiDotNet_Enums_AlgorithmTypes_SvdAlgorithmType_DividedAndConquer"><code>DividedAndConquer = 5</code></dt>
  <dd><p>Uses the Divide and Conquer algorithm for SVD computation, which is efficient for large matrices.</p>
<p>
<b>For Beginners:</b> The Divide and Conquer approach breaks down a large problem into smaller, more manageable 
sub-problems, solves them separately, and then combines their solutions.
<p>Think of it like a team working on a big project:</p>
<ol>
<li><p>First, the matrix is divided into smaller sub-matrices</p>
</li>
<li><p>SVD is computed for each of these smaller matrices (which is faster and easier)</p>
</li>
<li><p>These partial results are cleverly combined to form the SVD of the original matrix</p>
</li>
</ol>
<p>The Divide and Conquer approach:</p>
<ol>
<li><p>Is significantly faster than classical methods for large matrices</p>
</li>
<li><p>Has excellent numerical stability</p>
</li>
<li><p>Can compute the full SVD efficiently</p>
</li>
<li><p>Takes advantage of modern computer architectures</p>
</li>
<li><p>Works well in parallel computing environments</p>
</li>
</ol>
<p>This method is particularly valuable when:</p>
<ol>
<li><p>You're working with large matrices</p>
</li>
<li><p>You need the complete SVD (all singular values and vectors)</p>
</li>
<li><p>You want good performance without sacrificing accuracy</p>
</li>
<li><p>You have multiple processors or cores available</p>
</li>
</ol>
<p>In machine learning applications, the Divide and Conquer approach enables efficient processing of large
datasets while maintaining high accuracy, making it suitable for applications like image processing,
natural language processing, and large-scale data analysis where both performance and precision are important.</p>

</dd>
  
    <dt id="AiDotNet_Enums_AlgorithmTypes_SvdAlgorithmType_GolubReinsch"><code>GolubReinsch = 0</code></dt>
  <dd><p>Uses the Golub-Reinsch algorithm for SVD computation, which is the classical approach.</p>
<p>
<b>For Beginners:</b> The Golub-Reinsch algorithm is the "classic" method for computing SVD. It's like the 
standard recipe that has been trusted for decades.
<p>This algorithm works in two main steps:</p>
<ol>
<li><p>First, it reduces the original matrix to a bidiagonal form (a simpler matrix with non-zero elements
only on the main diagonal and the diagonal just above it)</p>
</li>
<li><p>Then, it iteratively computes the SVD of this bidiagonal matrix</p>
</li>
</ol>
<p>The Golub-Reinsch approach:</p>
<ol>
<li><p>Is numerically stable (gives accurate results even with challenging matrices)</p>
</li>
<li><p>Works well for small to medium-sized dense matrices</p>
</li>
<li><p>Has predictable performance across different types of matrices</p>
</li>
<li><p>Is well-studied and understood</p>
</li>
<li><p>Computes the full SVD (all singular values and vectors)</p>
</li>
</ol>
<p>This method is particularly useful when:</p>
<ol>
<li><p>You need high accuracy</p>
</li>
<li><p>Your matrix is dense and not too large</p>
</li>
<li><p>You need all singular values and vectors</p>
</li>
<li><p>You want a reliable, well-tested approach</p>
</li>
</ol>
<p>In machine learning applications, the Golub-Reinsch algorithm provides a solid foundation for techniques
like Principal Component Analysis (PCA), where accuracy in computing the decomposition is important.</p>

</dd>
  
    <dt id="AiDotNet_Enums_AlgorithmTypes_SvdAlgorithmType_Jacobi"><code>Jacobi = 1</code></dt>
  <dd><p>Uses the Jacobi algorithm for SVD computation, which is particularly accurate for small matrices.</p>
<p>
<b>For Beginners:</b> The Jacobi algorithm takes a different approach to computing SVD by using a series of 
rotations to gradually transform the matrix.
<p>Imagine you're trying to align a crooked picture frame. The Jacobi method is like making a series of small
adjustments, rotating it bit by bit until it's perfectly straight:</p>
<ol>
<li><p>It looks for the largest off-diagonal element in the matrix</p>
</li>
<li><p>It applies a rotation to make that element zero</p>
</li>
<li><p>It repeats this process many times until all off-diagonal elements are very close to zero</p>
</li>
</ol>
<p>The Jacobi approach:</p>
<ol>
<li><p>Is extremely accurate, often more precise than other methods</p>
</li>
<li><p>Works particularly well for small matrices</p>
</li>
<li><p>Is easy to parallelize (can use multiple processors efficiently)</p>
</li>
<li><p>Converges more slowly for large matrices</p>
</li>
<li><p>Is simpler to understand and implement than some other methods</p>
</li>
</ol>
<p>This method is particularly valuable when:</p>
<ol>
<li><p>You need very high numerical precision</p>
</li>
<li><p>You're working with small matrices</p>
</li>
<li><p>You have parallel computing resources available</p>
</li>
<li><p>The matrix has special properties (like being symmetric)</p>
</li>
</ol>
<p>In machine learning applications, the Jacobi algorithm can be useful for sensitive applications where
numerical precision is critical, such as in certain scientific computing tasks or when working with
ill-conditioned matrices where other methods might be less stable.</p>

</dd>
  
    <dt id="AiDotNet_Enums_AlgorithmTypes_SvdAlgorithmType_PowerIteration"><code>PowerIteration = 3</code></dt>
  <dd><p>Uses the Power Iteration method for SVD computation, which is efficient for finding the largest singular values.</p>
<p>
<b>For Beginners:</b> The Power Iteration method is a simple but powerful approach that's especially good at 
finding the largest singular values and their corresponding vectors.
<p>Imagine you're trying to find the tallest mountain in a range. The Power Iteration method is like starting
at a random point and always walking uphill - eventually, you'll reach the highest peak:</p>
<ol>
<li><p>It starts with a random vector</p>
</li>
<li><p>It repeatedly multiplies this vector by the matrix (and its transpose)</p>
</li>
<li><p>The vector gradually aligns with the direction of the largest singular value</p>
</li>
<li><p>After finding one singular value/vector pair, it can be &quot;deflated&quot; to find the next largest</p>
</li>
</ol>
<p>The Power Iteration approach:</p>
<ol>
<li><p>Is conceptually simple and easy to implement</p>
</li>
<li><p>Requires minimal memory</p>
</li>
<li><p>Is particularly efficient for sparse matrices (matrices with mostly zeros)</p>
</li>
<li><p>Converges quickly to the largest singular values</p>
</li>
<li><p>May converge slowly if the largest singular values are close in magnitude</p>
</li>
</ol>
<p>This method is particularly valuable when:</p>
<ol>
<li><p>You only need the few largest singular values and vectors</p>
</li>
<li><p>You're working with sparse matrices</p>
</li>
<li><p>Memory efficiency is important</p>
</li>
<li><p>You need a simple, robust approach</p>
</li>
</ol>
<p>In machine learning applications, Power Iteration is useful for tasks like PageRank computation (used by
Google's search algorithm), finding the principal components in PCA when only a few components are needed,
or in spectral clustering algorithms.</p>

</dd>
  
    <dt id="AiDotNet_Enums_AlgorithmTypes_SvdAlgorithmType_Randomized"><code>Randomized = 2</code></dt>
  <dd><p>Uses a randomized algorithm for SVD computation, which is faster but provides an approximation.</p>
<p>
<b>For Beginners:</b> Randomized SVD algorithms use probability and random sampling to quickly compute an 
approximate SVD, trading some accuracy for significant speed improvements.
<p>Think of it like taking a survey: instead of asking everyone in a city about their opinion, you might
randomly sample a few hundred people to get a good approximation much more quickly:</p>
<ol>
<li><p>It first creates a smaller matrix by randomly projecting the original large matrix</p>
</li>
<li><p>It then computes the SVD of this much smaller matrix</p>
</li>
<li><p>Finally, it converts this result back to an approximate SVD of the original matrix</p>
</li>
</ol>
<p>The Randomized approach:</p>
<ol>
<li><p>Is much faster than classical methods for large matrices</p>
</li>
<li><p>Requires less memory</p>
</li>
<li><p>Provides an approximation rather than an exact result</p>
</li>
<li><p>Works particularly well when the matrix has rapidly decaying singular values</p>
</li>
<li><p>Can be tuned to balance speed versus accuracy</p>
</li>
</ol>
<p>This method is particularly useful when:</p>
<ol>
<li><p>You're working with very large matrices</p>
</li>
<li><p>You need results quickly</p>
</li>
<li><p>An approximate solution is acceptable</p>
</li>
<li><p>You're doing exploratory data analysis</p>
</li>
<li><p>The matrix has a low effective rank (most of the information is contained in a few components)</p>
</li>
</ol>
<p>In machine learning applications, Randomized SVD enables processing of large datasets that would be
impractical with classical methods, making it valuable for tasks like large-scale topic modeling,
image processing, or analyzing massive recommendation systems.</p>

</dd>
  
    <dt id="AiDotNet_Enums_AlgorithmTypes_SvdAlgorithmType_TruncatedSVD"><code>TruncatedSVD = 4</code></dt>
  <dd><p>Uses the Truncated SVD algorithm, which computes only the k largest singular values and their corresponding vectors.</p>
<p>
<b>For Beginners:</b> Truncated SVD focuses on computing only a specified number (k) of the largest singular values 
and their corresponding vectors, rather than the complete decomposition.
<p>Think of it like summarizing a book by only keeping the most important chapters:</p>
<ol>
<li><p>It specifically targets the k largest singular values</p>
</li>
<li><p>It ignores the smaller singular values that often represent noise or less important information</p>
</li>
<li><p>It produces a lower-rank approximation of the original matrix</p>
</li>
</ol>
<p>The Truncated SVD approach:</p>
<ol>
<li><p>Is much faster than computing the full SVD</p>
</li>
<li><p>Requires significantly less memory</p>
</li>
<li><p>Often captures the most important information in the data</p>
</li>
<li><p>Is directly applicable to dimensionality reduction</p>
</li>
<li><p>Forms the basis of techniques like Latent Semantic Analysis</p>
</li>
</ol>
<p>This method is particularly useful when:</p>
<ol>
<li><p>You only care about the most significant components</p>
</li>
<li><p>You're using SVD for dimensionality reduction</p>
</li>
<li><p>You're working with large matrices</p>
</li>
<li><p>You want to filter out noise by removing small singular values</p>
</li>
</ol>
<p>In machine learning applications, Truncated SVD is widely used for dimensionality reduction in text analysis
(as in Latent Semantic Analysis), collaborative filtering for recommendation systems, and as a preprocessing
step to make large datasets more manageable for other algorithms.</p>

</dd>
  
  </dl>


  <h2 id="AiDotNet_Enums_AlgorithmTypes_SvdAlgorithmType_remarks">Remarks</h2>
  <div class="markdown level0 remarks"><p>
<b>For Beginners:</b> Singular Value Decomposition (SVD) is a powerful mathematical technique that breaks down a matrix 
(which you can think of as a table of numbers) into three simpler component matrices. It's like taking apart a 
complex machine to understand how it works.
<p>Here's what SVD does in simple terms:</p>
<ol>
<li><p>It takes a matrix A and decomposes it into three matrices: U, S (Sigma), and V^T
A = U × S × V^T</p>
</li>
<li><p>Each of these matrices has special properties:</p>
<ul>
<li>U contains the &quot;left singular vectors&quot; (think of these as the basic patterns in the rows of A)</li>
<li>S is a diagonal matrix containing the &quot;singular values&quot; (think of these as importance scores)</li>
<li>V^T contains the &quot;right singular vectors&quot; (think of these as the basic patterns in the columns of A)</li>
</ul>
</li>
</ol>
<p>Why is SVD important in AI and machine learning?</p>
<ol>
<li><p>Dimensionality Reduction: SVD helps compress data by keeping only the most important components</p>
</li>
<li><p>Noise Reduction: By removing components with small singular values, we can filter out noise</p>
</li>
<li><p>Recommendation Systems: SVD powers many recommendation algorithms (like those used by Netflix)</p>
</li>
<li><p>Image Processing: It's used for image compression and facial recognition</p>
</li>
<li><p>Natural Language Processing: SVD is used in techniques like Latent Semantic Analysis</p>
</li>
<li><p>Data Visualization: It can help reduce high-dimensional data to 2D or 3D for visualization</p>
</li>
</ol>
<p>This enum specifies which specific algorithm to use for computing the SVD, as different methods have different
performance characteristics and may be more suitable for certain types of matrices or applications.</p>

</div>

</article>

        <div class="contribution d-print-none">
          <a href="https://github.com/ooples/AiDotNet/blob/master/src/Enums/AlgorithmTypes/SvdAlgorithmType.cs/#L40" class="edit-link">Edit this page</a>
        </div>


      </div>

      <div class="affix">
        <nav id="affix"></nav>
      </div>
    </main>

    <div class="container-xxl search-results" id="search-results"></div>

    <footer class="border-top text-secondary">
      <div class="container-xxl">
        <div class="flex-fill">
          AiDotNet - Enterprise AI/ML Library for .NET
        </div>
      </div>
    </footer>
  </body>
</html>
