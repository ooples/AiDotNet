<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
      <title>Class ELUActivation&lt;T&gt; | AiDotNet Documentation </title>
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <meta name="title" content="Class ELUActivation&lt;T&gt; | AiDotNet Documentation ">
      
      <meta name="description" content="Implements the Exponential Linear Unit (ELU) activation function for neural networks.">
      <link rel="icon" href="../favicon.ico">
      <link rel="stylesheet" href="../public/docfx.min.css">
      <link rel="stylesheet" href="../public/main.css">
      <meta name="docfx:navrel" content="../toc.html">
      <meta name="docfx:tocrel" content="toc.html">
      
      <meta name="docfx:rel" content="../">
      
      
      <meta name="docfx:docurl" content="https://github.com/ooples/AiDotNet/new/master/apiSpec/new?filename=AiDotNet_ActivationFunctions_ELUActivation_1.md&amp;value=---%0Auid%3A%20AiDotNet.ActivationFunctions.ELUActivation%601%0Asummary%3A%20&#39;*You%20can%20override%20summary%20for%20the%20API%20here%20using%20*MARKDOWN*%20syntax&#39;%0A---%0A%0A*Please%20type%20below%20more%20information%20about%20this%20API%3A*%0A%0A">
      <meta name="loc:inThisArticle" content="In this article">
      <meta name="loc:searchResultsCount" content="{count} results for &quot;{query}&quot;">
      <meta name="loc:searchNoResults" content="No results for &quot;{query}&quot;">
      <meta name="loc:tocFilter" content="Filter by title">
      <meta name="loc:nextArticle" content="Next">
      <meta name="loc:prevArticle" content="Previous">
      <meta name="loc:themeLight" content="Light">
      <meta name="loc:themeDark" content="Dark">
      <meta name="loc:themeAuto" content="Auto">
      <meta name="loc:changeTheme" content="Change theme">
      <meta name="loc:copy" content="Copy">
      <meta name="loc:downloadPdf" content="Download PDF">

      <script type="module" src="./../public/docfx.min.js"></script>

      <script>
        const theme = localStorage.getItem('theme') || 'auto'
        document.documentElement.setAttribute('data-bs-theme', theme === 'auto' ? (window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light') : theme)
      </script>

  </head>

  <body class="tex2jax_ignore" data-layout="" data-yaml-mime="ManagedReference">
    <header class="bg-body border-bottom">
      <nav id="autocollapse" class="navbar navbar-expand-md" role="navigation">
        <div class="container-xxl flex-nowrap">
          <a class="navbar-brand" href="../index.html">
            <img id="logo" class="svg" src="../logo.svg" alt="AiDotNet">
            AiDotNet
          </a>
          <button class="btn btn-lg d-md-none border-0" type="button" data-bs-toggle="collapse" data-bs-target="#navpanel" aria-controls="navpanel" aria-expanded="false" aria-label="Toggle navigation">
            <i class="bi bi-three-dots"></i>
          </button>
          <div class="collapse navbar-collapse" id="navpanel">
            <div id="navbar">
              <form class="search" role="search" id="search">
                <i class="bi bi-search"></i>
                <input class="form-control" id="search-query" type="search" disabled placeholder="Search" autocomplete="off" aria-label="Search">
              </form>
            </div>
          </div>
        </div>
      </nav>
    </header>

    <main class="container-xxl">
      <div class="toc-offcanvas">
        <div class="offcanvas-md offcanvas-start" tabindex="-1" id="tocOffcanvas" aria-labelledby="tocOffcanvasLabel">
          <div class="offcanvas-header">
            <h5 class="offcanvas-title" id="tocOffcanvasLabel">Table of Contents</h5>
            <button type="button" class="btn-close" data-bs-dismiss="offcanvas" data-bs-target="#tocOffcanvas" aria-label="Close"></button>
          </div>
          <div class="offcanvas-body">
            <nav class="toc" id="toc"></nav>
          </div>
        </div>
      </div>

      <div class="content">
        <div class="actionbar">
          <button class="btn btn-lg border-0 d-md-none" type="button" data-bs-toggle="offcanvas" data-bs-target="#tocOffcanvas" aria-controls="tocOffcanvas" aria-expanded="false" aria-label="Show table of contents">
            <i class="bi bi-list"></i>
          </button>

          <nav id="breadcrumb"></nav>
        </div>

        <article data-uid="AiDotNet.ActivationFunctions.ELUActivation`1">



  <h1 id="AiDotNet_ActivationFunctions_ELUActivation_1" data-uid="AiDotNet.ActivationFunctions.ELUActivation`1" class="text-break">
Class ELUActivation&lt;T&gt;  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/ActivationFunctions/ELUActivation.cs/#L24"><i class="bi bi-code-slash"></i></a>
  </h1>

  <div class="facts text-secondary">
    <dl><dt>Namespace</dt><dd><a class="xref" href="AiDotNet.html">AiDotNet</a>.<a class="xref" href="AiDotNet.ActivationFunctions.html">ActivationFunctions</a></dd></dl>
  <dl><dt>Assembly</dt><dd>AiDotNet.dll</dd></dl>
  </div>

  <div class="markdown summary"><p>Implements the Exponential Linear Unit (ELU) activation function for neural networks.</p>
</div>
  <div class="markdown conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public class ELUActivation&lt;T&gt; : ActivationFunctionBase&lt;T&gt;, IActivationFunction&lt;T&gt;, IVectorActivationFunction&lt;T&gt;</code></pre>
  </div>



  <h4 class="section">Type Parameters</h4>
  <dl class="parameters">
    <dt><code>T</code></dt>
    <dd><p>The numeric type used for calculations (e.g., float, double).</p>
</dd>
  </dl>

  <dl class="typelist inheritance">
    <dt>Inheritance</dt>
    <dd>
      <div><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object">object</a></div>
      <div><a class="xref" href="AiDotNet.ActivationFunctions.ActivationFunctionBase-1.html">ActivationFunctionBase</a>&lt;T&gt;</div>
      <div><span class="xref">ELUActivation&lt;T&gt;</span></div>
    </dd>
  </dl>

  <dl class="typelist implements">
    <dt>Implements</dt>
    <dd>
      <div><a class="xref" href="AiDotNet.Interfaces.IActivationFunction-1.html">IActivationFunction</a>&lt;T&gt;</div>
      <div><a class="xref" href="AiDotNet.Interfaces.IVectorActivationFunction-1.html">IVectorActivationFunction</a>&lt;T&gt;</div>
    </dd>
  </dl>


  <dl class="typelist inheritedMembers">
    <dt>Inherited Members</dt>
    <dd>
    <div>
      <a class="xref" href="AiDotNet.ActivationFunctions.ActivationFunctionBase-1.html#AiDotNet_ActivationFunctions_ActivationFunctionBase_1_NumOps">ActivationFunctionBase&lt;T&gt;.NumOps</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.ActivationFunctions.ActivationFunctionBase-1.html#AiDotNet_ActivationFunctions_ActivationFunctionBase_1_Engine">ActivationFunctionBase&lt;T&gt;.Engine</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.ActivationFunctions.ActivationFunctionBase-1.html#AiDotNet_ActivationFunctions_ActivationFunctionBase_1_Activate_AiDotNet_Tensors_LinearAlgebra_Tensor__0__">ActivationFunctionBase&lt;T&gt;.Activate(Tensor&lt;T&gt;)</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.ActivationFunctions.ActivationFunctionBase-1.html#AiDotNet_ActivationFunctions_ActivationFunctionBase_1_Derivative_AiDotNet_Tensors_LinearAlgebra_Tensor__0__">ActivationFunctionBase&lt;T&gt;.Derivative(Tensor&lt;T&gt;)</a>
    </div>
    <div>
      <a class="xref" href="AiDotNet.ActivationFunctions.ActivationFunctionBase-1.html#AiDotNet_ActivationFunctions_ActivationFunctionBase_1_Backward_AiDotNet_Tensors_LinearAlgebra_Tensor__0__AiDotNet_Tensors_LinearAlgebra_Tensor__0__">ActivationFunctionBase&lt;T&gt;.Backward(Tensor&lt;T&gt;, Tensor&lt;T&gt;)</a>
    </div>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.equals#system-object-equals(system-object)">object.Equals(object)</a>
    </div>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.equals#system-object-equals(system-object-system-object)">object.Equals(object, object)</a>
    </div>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.gethashcode">object.GetHashCode()</a>
    </div>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.gettype">object.GetType()</a>
    </div>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.memberwiseclone">object.MemberwiseClone()</a>
    </div>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.referenceequals">object.ReferenceEquals(object, object)</a>
    </div>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.tostring">object.ToString()</a>
    </div>
  </dd></dl>




  <h2 id="AiDotNet_ActivationFunctions_ELUActivation_1_remarks">Remarks</h2>
  <div class="markdown level0 remarks"><p>
<b>For Beginners:</b> ELU is an activation function that, like ReLU, returns the input directly for positive values.
For negative inputs, it returns a smooth curve that approaches -alpha as the input becomes more negative.
<p>Key advantages of ELU include:</p>
<ul>
<li>It helps prevent &quot;dying neurons&quot; (a problem with ReLU) by allowing negative values</li>
<li>It has a smooth curve for negative inputs, which can help with gradient-based learning</li>
<li>The parameter alpha controls how negative the curve can go</li>
<li>It centers the activations closer to zero, which can speed up learning</li>
</ul>
<p>ELU is often used in deep neural networks where ReLU might cause training issues.</p>

</div>


  <h2 class="section" id="constructors">Constructors
</h2>


  <a id="AiDotNet_ActivationFunctions_ELUActivation_1__ctor_" data-uid="AiDotNet.ActivationFunctions.ELUActivation`1.#ctor*"></a>

  <h3 id="AiDotNet_ActivationFunctions_ELUActivation_1__ctor_System_Double_" data-uid="AiDotNet.ActivationFunctions.ELUActivation`1.#ctor(System.Double)">
  ELUActivation(double)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/ActivationFunctions/ELUActivation.cs/#L47"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Initializes a new instance of the ELUActivation class.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public ELUActivation(double alpha = 1)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>alpha</code> <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.double">double</a></dt>
    <dd><p>The alpha parameter that controls the negative saturation value. Default is 1.0.</p>
</dd>
  </dl>








  <h4 class="section" id="AiDotNet_ActivationFunctions_ELUActivation_1__ctor_System_Double__remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
<b>For Beginners:</b> The alpha parameter controls how negative the function can get.
A larger alpha means that the function can reach more negative values for very negative inputs.
The default value of 1.0 works well for most applications.
</p>
</div>




  <h2 class="section" id="properties">Properties
</h2>


  <a id="AiDotNet_ActivationFunctions_ELUActivation_1_Alpha_" data-uid="AiDotNet.ActivationFunctions.ELUActivation`1.Alpha*"></a>

  <h3 id="AiDotNet_ActivationFunctions_ELUActivation_1_Alpha" data-uid="AiDotNet.ActivationFunctions.ELUActivation`1.Alpha">
  Alpha
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/ActivationFunctions/ELUActivation.cs/#L34"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Gets the alpha parameter that controls the saturation value for negative inputs.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public T Alpha { get; }</code></pre>
  </div>





  <h4 class="section">Property Value</h4>
  <dl class="parameters">
    <dt><span class="xref">T</span></dt>
    <dd></dd>
  </dl>








  <a id="AiDotNet_ActivationFunctions_ELUActivation_1_SupportsGpuTraining_" data-uid="AiDotNet.ActivationFunctions.ELUActivation`1.SupportsGpuTraining*"></a>

  <h3 id="AiDotNet_ActivationFunctions_ELUActivation_1_SupportsGpuTraining" data-uid="AiDotNet.ActivationFunctions.ELUActivation`1.SupportsGpuTraining">
  SupportsGpuTraining
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/ActivationFunctions/ELUActivation.cs/#L199"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Gets whether ELU supports GPU-resident training.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public override bool SupportsGpuTraining { get; }</code></pre>
  </div>





  <h4 class="section">Property Value</h4>
  <dl class="parameters">
    <dt><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.boolean">bool</a></dt>
    <dd><p>True because ELU has GPU kernels for both forward and backward passes.</p>
</dd>
  </dl>








  <a id="AiDotNet_ActivationFunctions_ELUActivation_1_SupportsJitCompilation_" data-uid="AiDotNet.ActivationFunctions.ELUActivation`1.SupportsJitCompilation*"></a>

  <h3 id="AiDotNet_ActivationFunctions_ELUActivation_1_SupportsJitCompilation" data-uid="AiDotNet.ActivationFunctions.ELUActivation`1.SupportsJitCompilation">
  SupportsJitCompilation
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/ActivationFunctions/ELUActivation.cs/#L169"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Gets whether this activation function supports JIT compilation.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public override bool SupportsJitCompilation { get; }</code></pre>
  </div>





  <h4 class="section">Property Value</h4>
  <dl class="parameters">
    <dt><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.boolean">bool</a></dt>
    <dd><p>True because gradient computation is fully implemented in TensorOperations.ELU.</p>
</dd>
  </dl>




  <h4 class="section" id="AiDotNet_ActivationFunctions_ELUActivation_1_SupportsJitCompilation_remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
ELU supports JIT compilation because:
- The gradient computation (backward pass) is fully implemented in TensorOperations
- The operation uses IEngine for GPU acceleration
- It can be represented as a static computation graph node
</p>
</div>




  <h2 class="section" id="methods">Methods
</h2>


  <a id="AiDotNet_ActivationFunctions_ELUActivation_1_Activate_" data-uid="AiDotNet.ActivationFunctions.ELUActivation`1.Activate*"></a>

  <h3 id="AiDotNet_ActivationFunctions_ELUActivation_1_Activate_AiDotNet_Tensors_LinearAlgebra_Vector__0__" data-uid="AiDotNet.ActivationFunctions.ELUActivation`1.Activate(AiDotNet.Tensors.LinearAlgebra.Vector{`0})">
  Activate(Vector&lt;T&gt;)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/ActivationFunctions/ELUActivation.cs/#L95"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Applies the ELU activation function to each element of an input vector.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public override Vector&lt;T&gt; Activate(Vector&lt;T&gt; input)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>input</code> <a class="xref" href="AiDotNet.Tensors.LinearAlgebra.Vector-1.html">Vector</a>&lt;T&gt;</dt>
    <dd><p>The input vector.</p>
</dd>
  </dl>

  <h4 class="section">Returns</h4>
  <dl class="parameters">
    <dt><a class="xref" href="AiDotNet.Tensors.LinearAlgebra.Vector-1.html">Vector</a>&lt;T&gt;</dt>
    <dd><p>A vector with the ELU activation applied to each element.</p>
</dd>
  </dl>











  <a id="AiDotNet_ActivationFunctions_ELUActivation_1_Activate_" data-uid="AiDotNet.ActivationFunctions.ELUActivation`1.Activate*"></a>

  <h3 id="AiDotNet_ActivationFunctions_ELUActivation_1_Activate__0_" data-uid="AiDotNet.ActivationFunctions.ELUActivation`1.Activate(`0)">
  Activate(T)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/ActivationFunctions/ELUActivation.cs/#L78"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Applies the ELU activation function to a scalar input value.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public override T Activate(T x)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>x</code> <span class="xref">T</span></dt>
    <dd><p>The input value.</p>
</dd>
  </dl>

  <h4 class="section">Returns</h4>
  <dl class="parameters">
    <dt><span class="xref">T</span></dt>
    <dd><p>x if x &gt; 0, otherwise alpha * (e^x - 1).</p>
</dd>
  </dl>







  <h4 class="section" id="AiDotNet_ActivationFunctions_ELUActivation_1_Activate__0__remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
<b>For Beginners:</b> When given a positive number, ELU simply returns that number unchanged.
When given a negative number, it returns a value on a smooth curve that approaches -alpha as 
the input becomes more negative. This helps the neural network learn from negative signals
while avoiding the "dying neuron" problem that can happen with ReLU.
</p>
</div>




  <a id="AiDotNet_ActivationFunctions_ELUActivation_1_ApplyToGraph_" data-uid="AiDotNet.ActivationFunctions.ELUActivation`1.ApplyToGraph*"></a>

  <h3 id="AiDotNet_ActivationFunctions_ELUActivation_1_ApplyToGraph_AiDotNet_Autodiff_ComputationNode__0__" data-uid="AiDotNet.ActivationFunctions.ELUActivation`1.ApplyToGraph(AiDotNet.Autodiff.ComputationNode{`0})">
  ApplyToGraph(ComputationNode&lt;T&gt;)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/ActivationFunctions/ELUActivation.cs/#L183"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Applies this activation function to a computation graph node.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public override ComputationNode&lt;T&gt; ApplyToGraph(ComputationNode&lt;T&gt; input)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>input</code> <a class="xref" href="AiDotNet.Autodiff.ComputationNode-1.html">ComputationNode</a>&lt;T&gt;</dt>
    <dd><p>The computation node to apply the activation to.</p>
</dd>
  </dl>

  <h4 class="section">Returns</h4>
  <dl class="parameters">
    <dt><a class="xref" href="AiDotNet.Autodiff.ComputationNode-1.html">ComputationNode</a>&lt;T&gt;</dt>
    <dd><p>A new computation node with ELU activation applied.</p>
</dd>
  </dl>







  <h4 class="section" id="AiDotNet_ActivationFunctions_ELUActivation_1_ApplyToGraph_AiDotNet_Autodiff_ComputationNode__0___remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
This method maps the ELU activation to TensorOperations&lt;T&gt;.ELU(input, alpha),
which handles both forward and backward passes for JIT compilation.
</p>
</div>

  <h4 class="section">Exceptions</h4>
  <dl class="parameters">
    <dt><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.argumentnullexception">ArgumentNullException</a></dt>
    <dd><p>Thrown if input is null.</p>
</dd>
  </dl>



  <a id="AiDotNet_ActivationFunctions_ELUActivation_1_BackwardGpu_" data-uid="AiDotNet.ActivationFunctions.ELUActivation`1.BackwardGpu*"></a>

  <h3 id="AiDotNet_ActivationFunctions_ELUActivation_1_BackwardGpu_AiDotNet_Tensors_Engines_DirectGpu_IDirectGpuBackend_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_System_Int32_" data-uid="AiDotNet.ActivationFunctions.ELUActivation`1.BackwardGpu(AiDotNet.Tensors.Engines.DirectGpu.IDirectGpuBackend,AiDotNet.Tensors.Engines.DirectGpu.IGpuBuffer,AiDotNet.Tensors.Engines.DirectGpu.IGpuBuffer,AiDotNet.Tensors.Engines.DirectGpu.IGpuBuffer,AiDotNet.Tensors.Engines.DirectGpu.IGpuBuffer,System.Int32)">
  BackwardGpu(IDirectGpuBackend, IGpuBuffer, IGpuBuffer?, IGpuBuffer?, IGpuBuffer, int)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/ActivationFunctions/ELUActivation.cs/#L230"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Calculates the ELU backward pass gradient on GPU.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public override void BackwardGpu(IDirectGpuBackend backend, IGpuBuffer gradOutput, IGpuBuffer? input, IGpuBuffer? output, IGpuBuffer gradInput, int size)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>backend</code> <a class="xref" href="AiDotNet.Tensors.Engines.DirectGpu.IDirectGpuBackend.html">IDirectGpuBackend</a></dt>
    <dd><p>The GPU backend to use for execution.</p>
</dd>
    <dt><code>gradOutput</code> <a class="xref" href="AiDotNet.Tensors.Engines.DirectGpu.IGpuBuffer.html">IGpuBuffer</a></dt>
    <dd><p>The gradient flowing back from the next layer.</p>
</dd>
    <dt><code>input</code> <a class="xref" href="AiDotNet.Tensors.Engines.DirectGpu.IGpuBuffer.html">IGpuBuffer</a></dt>
    <dd><p>The input buffer from the forward pass.</p>
</dd>
    <dt><code>output</code> <a class="xref" href="AiDotNet.Tensors.Engines.DirectGpu.IGpuBuffer.html">IGpuBuffer</a></dt>
    <dd><p>The output buffer from the forward pass.</p>
</dd>
    <dt><code>gradInput</code> <a class="xref" href="AiDotNet.Tensors.Engines.DirectGpu.IGpuBuffer.html">IGpuBuffer</a></dt>
    <dd><p>The output buffer to store the input gradient.</p>
</dd>
    <dt><code>size</code> <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.int32">int</a></dt>
    <dd><p>The number of elements to process.</p>
</dd>
  </dl>








  <h4 class="section" id="AiDotNet_ActivationFunctions_ELUActivation_1_BackwardGpu_AiDotNet_Tensors_Engines_DirectGpu_IDirectGpuBackend_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_System_Int32__remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>ELU backward on GPU: gradInput[i] = gradOutput[i] * (input[i] &gt; 0 ? 1 : output[i] + alpha)
Note: ELU backward needs both input and output from forward pass.</p>
</div>




  <a id="AiDotNet_ActivationFunctions_ELUActivation_1_Derivative_" data-uid="AiDotNet.ActivationFunctions.ELUActivation`1.Derivative*"></a>

  <h3 id="AiDotNet_ActivationFunctions_ELUActivation_1_Derivative_AiDotNet_Tensors_LinearAlgebra_Vector__0__" data-uid="AiDotNet.ActivationFunctions.ELUActivation`1.Derivative(AiDotNet.Tensors.LinearAlgebra.Vector{`0})">
  Derivative(Vector&lt;T&gt;)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/ActivationFunctions/ELUActivation.cs/#L133"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Calculates the derivative of the ELU activation function for each element of an input vector.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public override Matrix&lt;T&gt; Derivative(Vector&lt;T&gt; input)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>input</code> <a class="xref" href="AiDotNet.Tensors.LinearAlgebra.Vector-1.html">Vector</a>&lt;T&gt;</dt>
    <dd><p>The input vector at which to calculate the derivative.</p>
</dd>
  </dl>

  <h4 class="section">Returns</h4>
  <dl class="parameters">
    <dt><a class="xref" href="AiDotNet.Tensors.LinearAlgebra.Matrix-1.html">Matrix</a>&lt;T&gt;</dt>
    <dd><p>A diagonal matrix containing the derivatives for each input element.</p>
</dd>
  </dl>











  <a id="AiDotNet_ActivationFunctions_ELUActivation_1_Derivative_" data-uid="AiDotNet.ActivationFunctions.ELUActivation`1.Derivative*"></a>

  <h3 id="AiDotNet_ActivationFunctions_ELUActivation_1_Derivative__0_" data-uid="AiDotNet.ActivationFunctions.ELUActivation`1.Derivative(`0)">
  Derivative(T)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/ActivationFunctions/ELUActivation.cs/#L115"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Calculates the derivative of the ELU activation function for a scalar input value.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public override T Derivative(T x)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>x</code> <span class="xref">T</span></dt>
    <dd><p>The input value at which to calculate the derivative.</p>
</dd>
  </dl>

  <h4 class="section">Returns</h4>
  <dl class="parameters">
    <dt><span class="xref">T</span></dt>
    <dd><p>1 if x &gt; 0, otherwise ELU(x, alpha) + alpha.</p>
</dd>
  </dl>







  <h4 class="section" id="AiDotNet_ActivationFunctions_ELUActivation_1_Derivative__0__remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
<b>For Beginners:</b> The derivative tells us how much the ELU function changes when its input changes slightly.
For positive inputs, the derivative is 1, meaning the function increases at the same rate as the input.
For negative inputs, the derivative is related to the function's value at that point.
<p>This information is essential for backpropagation during neural network training,
as it helps determine how to adjust the weights to improve the network's performance.</p>

</div>




  <a id="AiDotNet_ActivationFunctions_ELUActivation_1_ForwardGpu_" data-uid="AiDotNet.ActivationFunctions.ELUActivation`1.ForwardGpu*"></a>

  <h3 id="AiDotNet_ActivationFunctions_ELUActivation_1_ForwardGpu_AiDotNet_Tensors_Engines_DirectGpu_IDirectGpuBackend_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_System_Int32_" data-uid="AiDotNet.ActivationFunctions.ELUActivation`1.ForwardGpu(AiDotNet.Tensors.Engines.DirectGpu.IDirectGpuBackend,AiDotNet.Tensors.Engines.DirectGpu.IGpuBuffer,AiDotNet.Tensors.Engines.DirectGpu.IGpuBuffer,System.Int32)">
  ForwardGpu(IDirectGpuBackend, IGpuBuffer, IGpuBuffer, int)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/ActivationFunctions/ELUActivation.cs/#L211"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Applies the ELU activation function on GPU.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public override void ForwardGpu(IDirectGpuBackend backend, IGpuBuffer input, IGpuBuffer output, int size)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>backend</code> <a class="xref" href="AiDotNet.Tensors.Engines.DirectGpu.IDirectGpuBackend.html">IDirectGpuBackend</a></dt>
    <dd><p>The GPU backend to use for execution.</p>
</dd>
    <dt><code>input</code> <a class="xref" href="AiDotNet.Tensors.Engines.DirectGpu.IGpuBuffer.html">IGpuBuffer</a></dt>
    <dd><p>The input GPU buffer.</p>
</dd>
    <dt><code>output</code> <a class="xref" href="AiDotNet.Tensors.Engines.DirectGpu.IGpuBuffer.html">IGpuBuffer</a></dt>
    <dd><p>The output GPU buffer to store the activated values.</p>
</dd>
    <dt><code>size</code> <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.int32">int</a></dt>
    <dd><p>The number of elements to process.</p>
</dd>
  </dl>








  <h4 class="section" id="AiDotNet_ActivationFunctions_ELUActivation_1_ForwardGpu_AiDotNet_Tensors_Engines_DirectGpu_IDirectGpuBackend_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_AiDotNet_Tensors_Engines_DirectGpu_IGpuBuffer_System_Int32__remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>ELU on GPU: output[i] = input[i] &gt; 0 ? input[i] : alpha * (exp(input[i]) - 1)</p>
</div>




  <a id="AiDotNet_ActivationFunctions_ELUActivation_1_SupportsScalarOperations_" data-uid="AiDotNet.ActivationFunctions.ELUActivation`1.SupportsScalarOperations*"></a>

  <h3 id="AiDotNet_ActivationFunctions_ELUActivation_1_SupportsScalarOperations" data-uid="AiDotNet.ActivationFunctions.ELUActivation`1.SupportsScalarOperations">
  SupportsScalarOperations()
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/ActivationFunctions/ELUActivation.cs/#L63"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Determines if the activation function supports operations on individual scalar values.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">protected override bool SupportsScalarOperations()</code></pre>
  </div>


  <h4 class="section">Returns</h4>
  <dl class="parameters">
    <dt><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.boolean">bool</a></dt>
    <dd><p>True, as ELU can be applied to individual scalar values.</p>
</dd>
  </dl>







  <h4 class="section" id="AiDotNet_ActivationFunctions_ELUActivation_1_SupportsScalarOperations_remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
<b>For Beginners:</b> This returns true because ELU can process one number at a time.
Unlike functions like Softmax that need to look at all values together,
ELU transforms each value independently.
</p>
</div>





</article>

        <div class="contribution d-print-none">
          <a href="https://github.com/ooples/AiDotNet/blob/master/src/ActivationFunctions/ELUActivation.cs/#L24" class="edit-link">Edit this page</a>
        </div>


      </div>

      <div class="affix">
        <nav id="affix"></nav>
      </div>
    </main>

    <div class="container-xxl search-results" id="search-results"></div>

    <footer class="border-top text-secondary">
      <div class="container-xxl">
        <div class="flex-fill">
          AiDotNet - Enterprise AI/ML Library for .NET
        </div>
      </div>
    </footer>
  </body>
</html>
