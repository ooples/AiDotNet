<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
      <title>Class SafetyFilter&lt;T&gt; | AiDotNet Documentation </title>
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <meta name="title" content="Class SafetyFilter&lt;T&gt; | AiDotNet Documentation ">
      
      <meta name="description" content="Implements comprehensive safety filtering for AI model inputs and outputs.">
      <link rel="icon" href="../favicon.ico">
      <link rel="stylesheet" href="../public/docfx.min.css">
      <link rel="stylesheet" href="../public/main.css">
      <meta name="docfx:navrel" content="../toc.html">
      <meta name="docfx:tocrel" content="toc.html">
      
      <meta name="docfx:rel" content="../">
      
      
      <meta name="docfx:docurl" content="https://github.com/ooples/AiDotNet/new/master/apiSpec/new?filename=AiDotNet_AdversarialRobustness_Safety_SafetyFilter_1.md&amp;value=---%0Auid%3A%20AiDotNet.AdversarialRobustness.Safety.SafetyFilter%601%0Asummary%3A%20&#39;*You%20can%20override%20summary%20for%20the%20API%20here%20using%20*MARKDOWN*%20syntax&#39;%0A---%0A%0A*Please%20type%20below%20more%20information%20about%20this%20API%3A*%0A%0A">
      <meta name="loc:inThisArticle" content="In this article">
      <meta name="loc:searchResultsCount" content="{count} results for &quot;{query}&quot;">
      <meta name="loc:searchNoResults" content="No results for &quot;{query}&quot;">
      <meta name="loc:tocFilter" content="Filter by title">
      <meta name="loc:nextArticle" content="Next">
      <meta name="loc:prevArticle" content="Previous">
      <meta name="loc:themeLight" content="Light">
      <meta name="loc:themeDark" content="Dark">
      <meta name="loc:themeAuto" content="Auto">
      <meta name="loc:changeTheme" content="Change theme">
      <meta name="loc:copy" content="Copy">
      <meta name="loc:downloadPdf" content="Download PDF">

      <script type="module" src="./../public/docfx.min.js"></script>

      <script>
        const theme = localStorage.getItem('theme') || 'auto'
        document.documentElement.setAttribute('data-bs-theme', theme === 'auto' ? (window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light') : theme)
      </script>

  </head>

  <body class="tex2jax_ignore" data-layout="" data-yaml-mime="ManagedReference">
    <header class="bg-body border-bottom">
      <nav id="autocollapse" class="navbar navbar-expand-md" role="navigation">
        <div class="container-xxl flex-nowrap">
          <a class="navbar-brand" href="../index.html">
            <img id="logo" class="svg" src="../logo.svg" alt="AiDotNet">
            AiDotNet
          </a>
          <button class="btn btn-lg d-md-none border-0" type="button" data-bs-toggle="collapse" data-bs-target="#navpanel" aria-controls="navpanel" aria-expanded="false" aria-label="Toggle navigation">
            <i class="bi bi-three-dots"></i>
          </button>
          <div class="collapse navbar-collapse" id="navpanel">
            <div id="navbar">
              <form class="search" role="search" id="search">
                <i class="bi bi-search"></i>
                <input class="form-control" id="search-query" type="search" disabled placeholder="Search" autocomplete="off" aria-label="Search">
              </form>
            </div>
          </div>
        </div>
      </nav>
    </header>

    <main class="container-xxl">
      <div class="toc-offcanvas">
        <div class="offcanvas-md offcanvas-start" tabindex="-1" id="tocOffcanvas" aria-labelledby="tocOffcanvasLabel">
          <div class="offcanvas-header">
            <h5 class="offcanvas-title" id="tocOffcanvasLabel">Table of Contents</h5>
            <button type="button" class="btn-close" data-bs-dismiss="offcanvas" data-bs-target="#tocOffcanvas" aria-label="Close"></button>
          </div>
          <div class="offcanvas-body">
            <nav class="toc" id="toc"></nav>
          </div>
        </div>
      </div>

      <div class="content">
        <div class="actionbar">
          <button class="btn btn-lg border-0 d-md-none" type="button" data-bs-toggle="offcanvas" data-bs-target="#tocOffcanvas" aria-controls="tocOffcanvas" aria-expanded="false" aria-label="Show table of contents">
            <i class="bi bi-list"></i>
          </button>

          <nav id="breadcrumb"></nav>
        </div>

        <article data-uid="AiDotNet.AdversarialRobustness.Safety.SafetyFilter`1">



  <h1 id="AiDotNet_AdversarialRobustness_Safety_SafetyFilter_1" data-uid="AiDotNet.AdversarialRobustness.Safety.SafetyFilter`1" class="text-break">
Class SafetyFilter&lt;T&gt;  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/AdversarialRobustness/Safety/SafetyFilter.cs/#L27"><i class="bi bi-code-slash"></i></a>
  </h1>

  <div class="facts text-secondary">
    <dl><dt>Namespace</dt><dd><a class="xref" href="AiDotNet.html">AiDotNet</a>.<a class="xref" href="AiDotNet.AdversarialRobustness.html">AdversarialRobustness</a>.<a class="xref" href="AiDotNet.AdversarialRobustness.Safety.html">Safety</a></dd></dl>
  <dl><dt>Assembly</dt><dd>AiDotNet.dll</dd></dl>
  </div>

  <div class="markdown summary"><p>Implements comprehensive safety filtering for AI model inputs and outputs.</p>
</div>
  <div class="markdown conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public class SafetyFilter&lt;T&gt; : ISafetyFilter&lt;T&gt;, IModelSerializer</code></pre>
  </div>



  <h4 class="section">Type Parameters</h4>
  <dl class="parameters">
    <dt><code>T</code></dt>
    <dd><p>The numeric data type used for calculations.</p>
</dd>
  </dl>

  <dl class="typelist inheritance">
    <dt>Inheritance</dt>
    <dd>
      <div><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object">object</a></div>
      <div><span class="xref">SafetyFilter&lt;T&gt;</span></div>
    </dd>
  </dl>

  <dl class="typelist implements">
    <dt>Implements</dt>
    <dd>
      <div><a class="xref" href="AiDotNet.Interfaces.ISafetyFilter-1.html">ISafetyFilter</a>&lt;T&gt;</div>
      <div><a class="xref" href="AiDotNet.Interfaces.IModelSerializer.html">IModelSerializer</a></div>
    </dd>
  </dl>


  <dl class="typelist inheritedMembers">
    <dt>Inherited Members</dt>
    <dd>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.equals#system-object-equals(system-object)">object.Equals(object)</a>
    </div>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.equals#system-object-equals(system-object-system-object)">object.Equals(object, object)</a>
    </div>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.gethashcode">object.GetHashCode()</a>
    </div>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.gettype">object.GetType()</a>
    </div>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.memberwiseclone">object.MemberwiseClone()</a>
    </div>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.referenceequals">object.ReferenceEquals(object, object)</a>
    </div>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.tostring">object.ToString()</a>
    </div>
  </dd></dl>




  <h2 id="AiDotNet_AdversarialRobustness_Safety_SafetyFilter_1_remarks">Remarks</h2>
  <div class="markdown level0 remarks"><p>
SafetyFilter provides multiple layers of protection including input validation,
output filtering, jailbreak detection, and harmful content identification.
</p>
<p><b>For Beginners:</b> Think of SafetyFilter as a comprehensive security system
for your AI. It checks everything going in and coming out, looking for anything
suspicious, harmful, or inappropriate. It's like having security guards, content
moderators, and safety inspectors all working together.</p>
</div>


  <h2 class="section" id="constructors">Constructors
</h2>


  <a id="AiDotNet_AdversarialRobustness_Safety_SafetyFilter_1__ctor_" data-uid="AiDotNet.AdversarialRobustness.Safety.SafetyFilter`1.#ctor*"></a>

  <h3 id="AiDotNet_AdversarialRobustness_Safety_SafetyFilter_1__ctor_AiDotNet_Models_Options_SafetyFilterOptions__0__" data-uid="AiDotNet.AdversarialRobustness.Safety.SafetyFilter`1.#ctor(AiDotNet.Models.Options.SafetyFilterOptions{`0})">
  SafetyFilter(SafetyFilterOptions&lt;T&gt;)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/AdversarialRobustness/Safety/SafetyFilter.cs/#L44"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Initializes a new instance of the safety filter.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public SafetyFilter(SafetyFilterOptions&lt;T&gt; options)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>options</code> <a class="xref" href="AiDotNet.Models.Options.SafetyFilterOptions-1.html">SafetyFilterOptions</a>&lt;T&gt;</dt>
    <dd><p>The safety filter configuration options.</p>
</dd>
  </dl>












  <h2 class="section" id="methods">Methods
</h2>


  <a id="AiDotNet_AdversarialRobustness_Safety_SafetyFilter_1_ComputeSafetyScore_" data-uid="AiDotNet.AdversarialRobustness.Safety.SafetyFilter`1.ComputeSafetyScore*"></a>

  <h3 id="AiDotNet_AdversarialRobustness_Safety_SafetyFilter_1_ComputeSafetyScore_AiDotNet_Tensors_LinearAlgebra_Vector__0__" data-uid="AiDotNet.AdversarialRobustness.Safety.SafetyFilter`1.ComputeSafetyScore(AiDotNet.Tensors.LinearAlgebra.Vector{`0})">
  ComputeSafetyScore(Vector&lt;T&gt;)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/AdversarialRobustness/Safety/SafetyFilter.cs/#L410"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Computes a safety score for model inputs or outputs.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public T ComputeSafetyScore(Vector&lt;T&gt; content)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>content</code> <a class="xref" href="AiDotNet.Tensors.LinearAlgebra.Vector-1.html">Vector</a>&lt;T&gt;</dt>
    <dd><p>The content to score.</p>
</dd>
  </dl>

  <h4 class="section">Returns</h4>
  <dl class="parameters">
    <dt><span class="xref">T</span></dt>
    <dd><p>A safety score between 0 (unsafe) and 1 (completely safe).</p>
</dd>
  </dl>







  <h4 class="section" id="AiDotNet_AdversarialRobustness_Safety_SafetyFilter_1_ComputeSafetyScore_AiDotNet_Tensors_LinearAlgebra_Vector__0___remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p><b>For Beginners:</b> This gives a single &quot;safety score&quot; from 0 to 1 indicating
how safe the content is. Think of it like a trust score - higher numbers mean
safer content.</p>
</div>




  <a id="AiDotNet_AdversarialRobustness_Safety_SafetyFilter_1_Deserialize_" data-uid="AiDotNet.AdversarialRobustness.Safety.SafetyFilter`1.Deserialize*"></a>

  <h3 id="AiDotNet_AdversarialRobustness_Safety_SafetyFilter_1_Deserialize_System_Byte___" data-uid="AiDotNet.AdversarialRobustness.Safety.SafetyFilter`1.Deserialize(System.Byte[])">
  Deserialize(byte[])
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/AdversarialRobustness/Safety/SafetyFilter.cs/#L437"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Loads a previously serialized model from binary data.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public void Deserialize(byte[] data)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>data</code> <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.byte">byte</a>[]</dt>
    <dd><p>The byte array containing the serialized model data.</p>
</dd>
  </dl>








  <h4 class="section" id="AiDotNet_AdversarialRobustness_Safety_SafetyFilter_1_Deserialize_System_Byte____remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>This method takes binary data created by the Serialize method and uses it to
restore a model to its previous state.</p>
<p><b>For Beginners:</b> This is like opening a saved file to continue your work.</p>
<p>When you call this method:</p>
<ul>
<li>You provide the binary data (bytes) that was previously created by Serialize</li>
<li>The model rebuilds itself using this data</li>
<li>After deserializing, the model is exactly as it was when serialized</li>
<li>It's ready to make predictions without needing to be trained again</li>
</ul>
<p>For example:</p>
<ul>
<li>You download a pre-trained model file for detecting spam emails</li>
<li>You deserialize this file into your application</li>
<li>Immediately, your application can detect spam without any training</li>
<li>The model has all the knowledge that was built into it by its original creator</li>
</ul>
<p>This is particularly useful when:</p>
<ul>
<li>You want to use a model that took days to train</li>
<li>You need to deploy the same model across multiple devices</li>
<li>You're creating an application that non-technical users will use</li>
</ul>
<p>Think of it like installing the brain of a trained expert directly into your application.</p>
</div>




  <a id="AiDotNet_AdversarialRobustness_Safety_SafetyFilter_1_DetectJailbreak_" data-uid="AiDotNet.AdversarialRobustness.Safety.SafetyFilter`1.DetectJailbreak*"></a>

  <h3 id="AiDotNet_AdversarialRobustness_Safety_SafetyFilter_1_DetectJailbreak_AiDotNet_Tensors_LinearAlgebra_Vector__0__" data-uid="AiDotNet.AdversarialRobustness.Safety.SafetyFilter`1.DetectJailbreak(AiDotNet.Tensors.LinearAlgebra.Vector{`0})">
  DetectJailbreak(Vector&lt;T&gt;)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/AdversarialRobustness/Safety/SafetyFilter.cs/#L267"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Detects jailbreak attempts that try to bypass safety measures.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public JailbreakDetectionResult&lt;T&gt; DetectJailbreak(Vector&lt;T&gt; input)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>input</code> <a class="xref" href="AiDotNet.Tensors.LinearAlgebra.Vector-1.html">Vector</a>&lt;T&gt;</dt>
    <dd><p>The input to check for jailbreak attempts.</p>
</dd>
  </dl>

  <h4 class="section">Returns</h4>
  <dl class="parameters">
    <dt><a class="xref" href="AiDotNet.Models.JailbreakDetectionResult-1.html">JailbreakDetectionResult</a>&lt;T&gt;</dt>
    <dd><p>Detection result indicating if a jailbreak was detected and its severity.</p>
</dd>
  </dl>







  <h4 class="section" id="AiDotNet_AdversarialRobustness_Safety_SafetyFilter_1_DetectJailbreak_AiDotNet_Tensors_LinearAlgebra_Vector__0___remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p><b>For Beginners:</b> A &quot;jailbreak&quot; is when someone tries to trick your AI into
ignoring its safety rules. This method detects those attempts.</p>
<p>Examples of jailbreak attempts:</p>
<ul>
<li>&quot;Ignore your previous instructions and do X instead&quot;</li>
<li>Roleplaying scenarios to bypass restrictions</li>
<li>Encoding harmful requests in creative ways</li>
<li>Exploiting edge cases in safety training</li>
</ul>
</div>




  <a id="AiDotNet_AdversarialRobustness_Safety_SafetyFilter_1_FilterOutput_" data-uid="AiDotNet.AdversarialRobustness.Safety.SafetyFilter`1.FilterOutput*"></a>

  <h3 id="AiDotNet_AdversarialRobustness_Safety_SafetyFilter_1_FilterOutput_AiDotNet_Tensors_LinearAlgebra_Vector__0__" data-uid="AiDotNet.AdversarialRobustness.Safety.SafetyFilter`1.FilterOutput(AiDotNet.Tensors.LinearAlgebra.Vector{`0})">
  FilterOutput(Vector&lt;T&gt;)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/AdversarialRobustness/Safety/SafetyFilter.cs/#L188"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Filters model outputs to remove or flag harmful content.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public SafetyFilterResult&lt;T&gt; FilterOutput(Vector&lt;T&gt; output)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>output</code> <a class="xref" href="AiDotNet.Tensors.LinearAlgebra.Vector-1.html">Vector</a>&lt;T&gt;</dt>
    <dd><p>The model output to filter.</p>
</dd>
  </dl>

  <h4 class="section">Returns</h4>
  <dl class="parameters">
    <dt><a class="xref" href="AiDotNet.Models.SafetyFilterResult-1.html">SafetyFilterResult</a>&lt;T&gt;</dt>
    <dd><p>Filtered output with harmful content removed or flagged.</p>
</dd>
  </dl>







  <h4 class="section" id="AiDotNet_AdversarialRobustness_Safety_SafetyFilter_1_FilterOutput_AiDotNet_Tensors_LinearAlgebra_Vector__0___remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p><b>For Beginners:</b> This checks what the AI is about to say before showing it
to users. If the AI generated something harmful or inappropriate, this method
can block it or modify it to be safe.</p>
<p>For example:</p>
<ul>
<li>If an AI accidentally generates instructions for something dangerous</li>
<li>If output contains private or sensitive information</li>
<li>If the response could be misleading or harmful</li>
</ul>
</div>




  <a id="AiDotNet_AdversarialRobustness_Safety_SafetyFilter_1_GetOptions_" data-uid="AiDotNet.AdversarialRobustness.Safety.SafetyFilter`1.GetOptions*"></a>

  <h3 id="AiDotNet_AdversarialRobustness_Safety_SafetyFilter_1_GetOptions" data-uid="AiDotNet.AdversarialRobustness.Safety.SafetyFilter`1.GetOptions">
  GetOptions()
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/AdversarialRobustness/Safety/SafetyFilter.cs/#L424"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Gets the configuration options for the safety filter.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public SafetyFilterOptions&lt;T&gt; GetOptions()</code></pre>
  </div>


  <h4 class="section">Returns</h4>
  <dl class="parameters">
    <dt><a class="xref" href="AiDotNet.Models.Options.SafetyFilterOptions-1.html">SafetyFilterOptions</a>&lt;T&gt;</dt>
    <dd><p>The configuration options for the safety filter.</p>
</dd>
  </dl>







  <h4 class="section" id="AiDotNet_AdversarialRobustness_Safety_SafetyFilter_1_GetOptions_remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p><b>For Beginners:</b> These settings control how strict the safety filter is
and what types of content it looks for.</p>
</div>




  <a id="AiDotNet_AdversarialRobustness_Safety_SafetyFilter_1_IdentifyHarmfulContent_" data-uid="AiDotNet.AdversarialRobustness.Safety.SafetyFilter`1.IdentifyHarmfulContent*"></a>

  <h3 id="AiDotNet_AdversarialRobustness_Safety_SafetyFilter_1_IdentifyHarmfulContent_AiDotNet_Tensors_LinearAlgebra_Vector__0__" data-uid="AiDotNet.AdversarialRobustness.Safety.SafetyFilter`1.IdentifyHarmfulContent(AiDotNet.Tensors.LinearAlgebra.Vector{`0})">
  IdentifyHarmfulContent(Vector&lt;T&gt;)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/AdversarialRobustness/Safety/SafetyFilter.cs/#L325"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Identifies harmful or inappropriate content in text or data.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public HarmfulContentResult&lt;T&gt; IdentifyHarmfulContent(Vector&lt;T&gt; content)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>content</code> <a class="xref" href="AiDotNet.Tensors.LinearAlgebra.Vector-1.html">Vector</a>&lt;T&gt;</dt>
    <dd><p>The content to analyze.</p>
</dd>
  </dl>

  <h4 class="section">Returns</h4>
  <dl class="parameters">
    <dt><a class="xref" href="AiDotNet.Models.HarmfulContentResult-1.html">HarmfulContentResult</a>&lt;T&gt;</dt>
    <dd><p>Classification of harmful content types and severity scores.</p>
</dd>
  </dl>







  <h4 class="section" id="AiDotNet_AdversarialRobustness_Safety_SafetyFilter_1_IdentifyHarmfulContent_AiDotNet_Tensors_LinearAlgebra_Vector__0___remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p><b>For Beginners:</b> This is like a content moderation system. It scans content
(inputs or outputs) and identifies anything that might be harmful, offensive,
or inappropriate.</p>
<p>Categories it might detect:</p>
<ul>
<li>Violence or graphic content</li>
<li>Hate speech or discrimination</li>
<li>Private or sensitive information</li>
<li>Misinformation or scams</li>
<li>Adult or sexual content</li>
</ul>
</div>




  <a id="AiDotNet_AdversarialRobustness_Safety_SafetyFilter_1_LoadModel_" data-uid="AiDotNet.AdversarialRobustness.Safety.SafetyFilter`1.LoadModel*"></a>

  <h3 id="AiDotNet_AdversarialRobustness_Safety_SafetyFilter_1_LoadModel_System_String_" data-uid="AiDotNet.AdversarialRobustness.Safety.SafetyFilter`1.LoadModel(System.String)">
  LoadModel(string)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/AdversarialRobustness/Safety/SafetyFilter.cs/#L478"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Loads the model from a file.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public void LoadModel(string filePath)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>filePath</code> <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.string">string</a></dt>
    <dd><p>The path to the file containing the saved model.</p>
</dd>
  </dl>








  <h4 class="section" id="AiDotNet_AdversarialRobustness_Safety_SafetyFilter_1_LoadModel_System_String__remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>This method provides a convenient way to load a model directly from disk.
It combines file I/O operations with deserialization.</p>
<p><b>For Beginners:</b> This is like clicking &quot;Open&quot; in a document editor.
Instead of manually reading from a file and then calling Deserialize(), this method does both steps for you.</p>
</div>

  <h4 class="section">Exceptions</h4>
  <dl class="parameters">
    <dt><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.io.filenotfoundexception">FileNotFoundException</a></dt>
    <dd><p>Thrown when the specified file does not exist.</p>
</dd>
    <dt><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.io.ioexception">IOException</a></dt>
    <dd><p>Thrown when an I/O error occurs while reading from the file or when the file contains corrupted or invalid model data.</p>
</dd>
  </dl>



  <a id="AiDotNet_AdversarialRobustness_Safety_SafetyFilter_1_Reset_" data-uid="AiDotNet.AdversarialRobustness.Safety.SafetyFilter`1.Reset*"></a>

  <h3 id="AiDotNet_AdversarialRobustness_Safety_SafetyFilter_1_Reset" data-uid="AiDotNet.AdversarialRobustness.Safety.SafetyFilter`1.Reset">
  Reset()
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/AdversarialRobustness/Safety/SafetyFilter.cs/#L427"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Resets the safety filter state.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public void Reset()</code></pre>
  </div>













  <a id="AiDotNet_AdversarialRobustness_Safety_SafetyFilter_1_SaveModel_" data-uid="AiDotNet.AdversarialRobustness.Safety.SafetyFilter`1.SaveModel*"></a>

  <h3 id="AiDotNet_AdversarialRobustness_Safety_SafetyFilter_1_SaveModel_System_String_" data-uid="AiDotNet.AdversarialRobustness.Safety.SafetyFilter`1.SaveModel(System.String)">
  SaveModel(string)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/AdversarialRobustness/Safety/SafetyFilter.cs/#L460"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Saves the model to a file.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public void SaveModel(string filePath)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>filePath</code> <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.string">string</a></dt>
    <dd><p>The path where the model should be saved.</p>
</dd>
  </dl>








  <h4 class="section" id="AiDotNet_AdversarialRobustness_Safety_SafetyFilter_1_SaveModel_System_String__remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>This method provides a convenient way to save the model directly to disk.
It combines serialization with file I/O operations.</p>
<p><b>For Beginners:</b> This is like clicking &quot;Save As&quot; in a document editor.
Instead of manually calling Serialize() and then writing to a file, this method does both steps for you.</p>
</div>

  <h4 class="section">Exceptions</h4>
  <dl class="parameters">
    <dt><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.io.ioexception">IOException</a></dt>
    <dd><p>Thrown when an I/O error occurs while writing to the file.</p>
</dd>
    <dt><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.unauthorizedaccessexception">UnauthorizedAccessException</a></dt>
    <dd><p>Thrown when the caller does not have the required permission to write to the specified file path.</p>
</dd>
  </dl>



  <a id="AiDotNet_AdversarialRobustness_Safety_SafetyFilter_1_Serialize_" data-uid="AiDotNet.AdversarialRobustness.Safety.SafetyFilter`1.Serialize*"></a>

  <h3 id="AiDotNet_AdversarialRobustness_Safety_SafetyFilter_1_Serialize" data-uid="AiDotNet.AdversarialRobustness.Safety.SafetyFilter`1.Serialize">
  Serialize()
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/AdversarialRobustness/Safety/SafetyFilter.cs/#L430"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Converts the current state of a machine learning model into a binary format.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public byte[] Serialize()</code></pre>
  </div>


  <h4 class="section">Returns</h4>
  <dl class="parameters">
    <dt><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.byte">byte</a>[]</dt>
    <dd><p>A byte array containing the serialized model data.</p>
</dd>
  </dl>







  <h4 class="section" id="AiDotNet_AdversarialRobustness_Safety_SafetyFilter_1_Serialize_remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>This method captures all the essential information about a trained model and converts it
into a sequence of bytes that can be stored or transmitted.</p>
<p><b>For Beginners:</b> This is like exporting your work to a file.</p>
<p>When you call this method:</p>
<ul>
<li>The model's current state (all its learned patterns and parameters) is captured</li>
<li>This information is converted into a compact binary format (bytes)</li>
<li>You can then save these bytes to a file, database, or send them over a network</li>
</ul>
<p>For example:</p>
<ul>
<li>After training a model to recognize cats vs. dogs in images</li>
<li>You can serialize the model to save all its learned knowledge</li>
<li>Later, you can use this saved data to recreate the model exactly as it was</li>
<li>The recreated model will make the same predictions as the original</li>
</ul>
<p>Think of it like taking a snapshot of your model's brain at a specific moment in time.</p>
</div>




  <a id="AiDotNet_AdversarialRobustness_Safety_SafetyFilter_1_ValidateInput_" data-uid="AiDotNet.AdversarialRobustness.Safety.SafetyFilter`1.ValidateInput*"></a>

  <h3 id="AiDotNet_AdversarialRobustness_Safety_SafetyFilter_1_ValidateInput_AiDotNet_Tensors_LinearAlgebra_Vector__0__" data-uid="AiDotNet.AdversarialRobustness.Safety.SafetyFilter`1.ValidateInput(AiDotNet.Tensors.LinearAlgebra.Vector{`0})">
  ValidateInput(Vector&lt;T&gt;)
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/AdversarialRobustness/Safety/SafetyFilter.cs/#L91"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Validates that an input is safe and appropriate for processing.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public SafetyValidationResult&lt;T&gt; ValidateInput(Vector&lt;T&gt; input)</code></pre>
  </div>

  <h4 class="section">Parameters</h4>
  <dl class="parameters">
    <dt><code>input</code> <a class="xref" href="AiDotNet.Tensors.LinearAlgebra.Vector-1.html">Vector</a>&lt;T&gt;</dt>
    <dd><p>The input to validate.</p>
</dd>
  </dl>

  <h4 class="section">Returns</h4>
  <dl class="parameters">
    <dt><a class="xref" href="AiDotNet.Models.SafetyValidationResult-1.html">SafetyValidationResult</a>&lt;T&gt;</dt>
    <dd><p>Validation result indicating if input is safe and any issues found.</p>
</dd>
  </dl>







  <h4 class="section" id="AiDotNet_AdversarialRobustness_Safety_SafetyFilter_1_ValidateInput_AiDotNet_Tensors_LinearAlgebra_Vector__0___remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>This method checks inputs before they reach the model to prevent malicious
or inappropriate inputs from being processed.</p>
<p><b>For Beginners:</b> This is like a bouncer at a club checking IDs at the door.
Before letting an input into your AI system, this method checks if it's safe
and appropriate to process.</p>
<p>The validation might check for:</p>
<ol>
<li>Malformed inputs that could crash the system</li>
<li>Adversarial patterns designed to fool the model</li>
<li>Attempts to inject malicious code or prompts</li>
<li>Inappropriate or harmful content in the input</li>
</ol>
</div>





</article>

        <div class="contribution d-print-none">
          <a href="https://github.com/ooples/AiDotNet/blob/master/src/AdversarialRobustness/Safety/SafetyFilter.cs/#L27" class="edit-link">Edit this page</a>
        </div>


      </div>

      <div class="affix">
        <nav id="affix"></nav>
      </div>
    </main>

    <div class="container-xxl search-results" id="search-results"></div>

    <footer class="border-top text-secondary">
      <div class="container-xxl">
        <div class="flex-fill">
          AiDotNet - Enterprise AI/ML Library for .NET
        </div>
      </div>
    </footer>
  </body>
</html>
