<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
      <title>Class DiscretePolicyOptions&lt;T&gt; | AiDotNet Documentation </title>
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <meta name="title" content="Class DiscretePolicyOptions&lt;T&gt; | AiDotNet Documentation ">
      
      <meta name="description" content="Configuration options for discrete action space policies in reinforcement learning. Discrete policies select from a finite set of actions using categorical (softmax) distributions.">
      <link rel="icon" href="../favicon.ico">
      <link rel="stylesheet" href="../public/docfx.min.css">
      <link rel="stylesheet" href="../public/main.css">
      <meta name="docfx:navrel" content="../toc.html">
      <meta name="docfx:tocrel" content="toc.html">
      
      <meta name="docfx:rel" content="../">
      
      
      <meta name="docfx:docurl" content="https://github.com/ooples/AiDotNet/new/master/apiSpec/new?filename=AiDotNet_ReinforcementLearning_Policies_DiscretePolicyOptions_1.md&amp;value=---%0Auid%3A%20AiDotNet.ReinforcementLearning.Policies.DiscretePolicyOptions%601%0Asummary%3A%20&#39;*You%20can%20override%20summary%20for%20the%20API%20here%20using%20*MARKDOWN*%20syntax&#39;%0A---%0A%0A*Please%20type%20below%20more%20information%20about%20this%20API%3A*%0A%0A">
      <meta name="loc:inThisArticle" content="In this article">
      <meta name="loc:searchResultsCount" content="{count} results for &quot;{query}&quot;">
      <meta name="loc:searchNoResults" content="No results for &quot;{query}&quot;">
      <meta name="loc:tocFilter" content="Filter by title">
      <meta name="loc:nextArticle" content="Next">
      <meta name="loc:prevArticle" content="Previous">
      <meta name="loc:themeLight" content="Light">
      <meta name="loc:themeDark" content="Dark">
      <meta name="loc:themeAuto" content="Auto">
      <meta name="loc:changeTheme" content="Change theme">
      <meta name="loc:copy" content="Copy">
      <meta name="loc:downloadPdf" content="Download PDF">

      <script type="module" src="./../public/docfx.min.js"></script>

      <script>
        const theme = localStorage.getItem('theme') || 'auto'
        document.documentElement.setAttribute('data-bs-theme', theme === 'auto' ? (window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light') : theme)
      </script>

  </head>

  <body class="tex2jax_ignore" data-layout="" data-yaml-mime="ManagedReference">
    <header class="bg-body border-bottom">
      <nav id="autocollapse" class="navbar navbar-expand-md" role="navigation">
        <div class="container-xxl flex-nowrap">
          <a class="navbar-brand" href="../index.html">
            <img id="logo" class="svg" src="../logo.svg" alt="AiDotNet">
            AiDotNet
          </a>
          <button class="btn btn-lg d-md-none border-0" type="button" data-bs-toggle="collapse" data-bs-target="#navpanel" aria-controls="navpanel" aria-expanded="false" aria-label="Toggle navigation">
            <i class="bi bi-three-dots"></i>
          </button>
          <div class="collapse navbar-collapse" id="navpanel">
            <div id="navbar">
              <form class="search" role="search" id="search">
                <i class="bi bi-search"></i>
                <input class="form-control" id="search-query" type="search" disabled placeholder="Search" autocomplete="off" aria-label="Search">
              </form>
            </div>
          </div>
        </div>
      </nav>
    </header>

    <main class="container-xxl">
      <div class="toc-offcanvas">
        <div class="offcanvas-md offcanvas-start" tabindex="-1" id="tocOffcanvas" aria-labelledby="tocOffcanvasLabel">
          <div class="offcanvas-header">
            <h5 class="offcanvas-title" id="tocOffcanvasLabel">Table of Contents</h5>
            <button type="button" class="btn-close" data-bs-dismiss="offcanvas" data-bs-target="#tocOffcanvas" aria-label="Close"></button>
          </div>
          <div class="offcanvas-body">
            <nav class="toc" id="toc"></nav>
          </div>
        </div>
      </div>

      <div class="content">
        <div class="actionbar">
          <button class="btn btn-lg border-0 d-md-none" type="button" data-bs-toggle="offcanvas" data-bs-target="#tocOffcanvas" aria-controls="tocOffcanvas" aria-expanded="false" aria-label="Show table of contents">
            <i class="bi bi-list"></i>
          </button>

          <nav id="breadcrumb"></nav>
        </div>

        <article data-uid="AiDotNet.ReinforcementLearning.Policies.DiscretePolicyOptions`1">



  <h1 id="AiDotNet_ReinforcementLearning_Policies_DiscretePolicyOptions_1" data-uid="AiDotNet.ReinforcementLearning.Policies.DiscretePolicyOptions`1" class="text-break">
Class DiscretePolicyOptions&lt;T&gt;  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/ReinforcementLearning/Policies/DiscretePolicyOptions.cs/#L48"><i class="bi bi-code-slash"></i></a>
  </h1>

  <div class="facts text-secondary">
    <dl><dt>Namespace</dt><dd><a class="xref" href="AiDotNet.html">AiDotNet</a>.<a class="xref" href="AiDotNet.ReinforcementLearning.html">ReinforcementLearning</a>.<a class="xref" href="AiDotNet.ReinforcementLearning.Policies.html">Policies</a></dd></dl>
  <dl><dt>Assembly</dt><dd>AiDotNet.dll</dd></dl>
  </div>

  <div class="markdown summary"><p>Configuration options for discrete action space policies in reinforcement learning.
Discrete policies select from a finite set of actions using categorical (softmax) distributions.</p>
</div>
  <div class="markdown conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public class DiscretePolicyOptions&lt;T&gt;</code></pre>
  </div>



  <h4 class="section">Type Parameters</h4>
  <dl class="parameters">
    <dt><code>T</code></dt>
    <dd><p>The numeric type used for calculations (float, double, etc.).</p>
</dd>
  </dl>

  <dl class="typelist inheritance">
    <dt>Inheritance</dt>
    <dd>
      <div><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object">object</a></div>
      <div><span class="xref">DiscretePolicyOptions&lt;T&gt;</span></div>
    </dd>
  </dl>



  <dl class="typelist inheritedMembers">
    <dt>Inherited Members</dt>
    <dd>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.equals#system-object-equals(system-object)">object.Equals(object)</a>
    </div>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.equals#system-object-equals(system-object-system-object)">object.Equals(object, object)</a>
    </div>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.gethashcode">object.GetHashCode()</a>
    </div>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.gettype">object.GetType()</a>
    </div>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.memberwiseclone">object.MemberwiseClone()</a>
    </div>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.referenceequals">object.ReferenceEquals(object, object)</a>
    </div>
    <div>
      <a class="xref" href="https://learn.microsoft.com/dotnet/api/system.object.tostring">object.ToString()</a>
    </div>
  </dd></dl>




  <h2 id="AiDotNet_ReinforcementLearning_Policies_DiscretePolicyOptions_1_remarks">Remarks</h2>
  <div class="markdown level0 remarks"><p>
Discrete policies are fundamental to reinforcement learning in environments with finite action spaces,
such as game playing (left/right/jump), robot arm control with discrete positions, or trading decisions
(buy/sell/hold). The policy network outputs logits (unnormalized log probabilities) for each action,
which are then converted to a probability distribution via softmax. Actions are sampled from this
distribution during training to enable exploration, while the most probable action is typically
selected during evaluation.
</p>
<p>
This configuration class provides sensible defaults aligned with modern deep reinforcement learning
best practices from libraries like Stable Baselines3 and RLlib. The default epsilon-greedy exploration
strategy balances exploration (trying random actions) with exploitation (using learned policy).
</p>
<p><b>For Beginners:</b> Discrete policies are for situations where your AI agent must choose
between specific, separate options rather than continuous values.
<p>Think of it like a video game character deciding between actions:</p>
<ul>
<li>Move Left</li>
<li>Move Right</li>
<li>Jump</li>
<li>Duck</li>
</ul>
<p>The policy learns which action is best in each situation by:</p>
<ol>
<li>Looking at the current state (what's on screen)</li>
<li>Calculating probabilities for each action (40% jump, 35% left, 20% right, 5% duck)</li>
<li>Choosing an action based on these probabilities</li>
</ol>
<p>During training, it sometimes picks random actions (exploration) to discover new strategies.
During evaluation/playing, it picks the best action it has learned.</p>
<p>This options class lets you configure:</p>
<ul>
<li>How many different actions are available (ActionSize)</li>
<li>How complex the neural network should be (HiddenLayers)</li>
<li>How much random exploration to use (ExplorationStrategy)</li>
</ul>

</div>


  <h2 class="section" id="properties">Properties
</h2>


  <a id="AiDotNet_ReinforcementLearning_Policies_DiscretePolicyOptions_1_ActionSize_" data-uid="AiDotNet.ReinforcementLearning.Policies.DiscretePolicyOptions`1.ActionSize*"></a>

  <h3 id="AiDotNet_ReinforcementLearning_Policies_DiscretePolicyOptions_1_ActionSize" data-uid="AiDotNet.ReinforcementLearning.Policies.DiscretePolicyOptions`1.ActionSize">
  ActionSize
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/ReinforcementLearning/Policies/DiscretePolicyOptions.cs/#L95"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Gets or sets the number of discrete actions available to the agent.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public int ActionSize { get; set; }</code></pre>
  </div>





  <h4 class="section">Property Value</h4>
  <dl class="parameters">
    <dt><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.int32">int</a></dt>
    <dd><p>The number of distinct actions the agent can choose from. Must be greater than 0.</p>
</dd>
  </dl>




  <h4 class="section" id="AiDotNet_ReinforcementLearning_Policies_DiscretePolicyOptions_1_ActionSize_remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
This defines the output size of the policy network and the dimensionality of the action probability
distribution. Common values range from 2 (binary decisions) to hundreds (complex action spaces like
language models). The network outputs logits for each action, which are converted to probabilities
via softmax.
</p>
<p><b>For Beginners:</b> How many different actions can your agent choose from?
<p>Examples:</p>
<ul>
<li>Trading bot: 3 actions (buy, sell, hold)</li>
<li>Pac-Man: 4 actions (up, down, left, right)</li>
<li>Fighting game: 12 actions (punch, kick, block, move in 4 directions, etc.)</li>
</ul>
<p>More actions make learning harder because the agent has more to explore.
Start simple with fewer actions when possible.</p>

</div>




  <a id="AiDotNet_ReinforcementLearning_Policies_DiscretePolicyOptions_1_ExplorationStrategy_" data-uid="AiDotNet.ReinforcementLearning.Policies.DiscretePolicyOptions`1.ExplorationStrategy*"></a>

  <h3 id="AiDotNet_ReinforcementLearning_Policies_DiscretePolicyOptions_1_ExplorationStrategy" data-uid="AiDotNet.ReinforcementLearning.Policies.DiscretePolicyOptions`1.ExplorationStrategy">
  ExplorationStrategy
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/ReinforcementLearning/Policies/DiscretePolicyOptions.cs/#L200"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Gets or sets the exploration strategy for balancing exploration vs exploitation during training.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public IExplorationStrategy&lt;T&gt; ExplorationStrategy { get; set; }</code></pre>
  </div>





  <h4 class="section">Property Value</h4>
  <dl class="parameters">
    <dt><a class="xref" href="AiDotNet.ReinforcementLearning.Policies.Exploration.IExplorationStrategy-1.html">IExplorationStrategy</a>&lt;T&gt;</dt>
    <dd><p>The exploration strategy. Defaults to epsilon-greedy with decaying epsilon from 1.0 to 0.01.</p>
</dd>
  </dl>




  <h4 class="section" id="AiDotNet_ReinforcementLearning_Policies_DiscretePolicyOptions_1_ExplorationStrategy_remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
Exploration is critical in reinforcement learning because the agent must try different actions
to discover which ones lead to high rewards. Epsilon-greedy exploration randomly selects actions
with probability ε (epsilon), and follows the learned policy with probability 1-ε. The epsilon
typically starts high (e.g., 1.0 for 100% random) and gradually decreases (to 0.01 for 1% random)
as the agent gains experience. Alternative strategies include Boltzmann (softmax) exploration,
or no exploration for pure exploitation.
</p>
<p><b>For Beginners:</b> Exploration means trying new things instead of always doing what
you think is best.
<p>The default epsilon-greedy strategy works like this:</p>
<ul>
<li>Start of training: 100% random actions (explore everything!)</li>
<li>Middle of training: Mix of random and learned actions</li>
<li>End of training: 99% learned actions, 1% random (mostly exploit what you know)</li>
</ul>
<p>Think of learning to play a new video game:</p>
<ul>
<li>First hour: Press random buttons to see what they do (high exploration)</li>
<li>After some practice: Mostly use moves you know work, occasionally try something new</li>
<li>Expert level: Almost always use best strategies, rarely experiment</li>
</ul>
<p>You might want different exploration if:</p>
<ul>
<li>Your environment is very random → Keep higher exploration longer</li>
<li>Your environment is very predictable → Reduce exploration faster</li>
<li>You're fine-tuning a pre-trained model → Start with low exploration</li>
</ul>
<p>Available strategies:</p>
<ul>
<li>EpsilonGreedyExploration (default): Simple, effective for discrete actions</li>
<li>BoltzmannExploration: Temperature-based, good for multi-armed bandits</li>
<li>NoExploration: For evaluation or when using off-policy algorithms</li>
</ul>

</div>




  <a id="AiDotNet_ReinforcementLearning_Policies_DiscretePolicyOptions_1_HiddenLayers_" data-uid="AiDotNet.ReinforcementLearning.Policies.DiscretePolicyOptions`1.HiddenLayers*"></a>

  <h3 id="AiDotNet_ReinforcementLearning_Policies_DiscretePolicyOptions_1_HiddenLayers" data-uid="AiDotNet.ReinforcementLearning.Policies.DiscretePolicyOptions`1.HiddenLayers">
  HiddenLayers
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/ReinforcementLearning/Policies/DiscretePolicyOptions.cs/#L135"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Gets or sets the architecture of hidden layers in the policy network.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public int[] HiddenLayers { get; set; }</code></pre>
  </div>





  <h4 class="section">Property Value</h4>
  <dl class="parameters">
    <dt><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.int32">int</a>[]</dt>
    <dd><p>An array where each element specifies the number of neurons in that hidden layer.
Defaults to [128, 128] for a two-layer network with 128 neurons each.</p>
</dd>
  </dl>




  <h4 class="section" id="AiDotNet_ReinforcementLearning_Policies_DiscretePolicyOptions_1_HiddenLayers_remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
The hidden layer configuration determines the network's capacity to learn complex policies.
Deeper networks (more layers) can learn more complex relationships but are harder to train
and slower to execute. Wider networks (more neurons per layer) increase capacity without
adding depth. The default [128, 128] works well for many problems including Atari games
and robotic control tasks. For simple problems (like CartPole), [64] may suffice. For
complex problems (like Go or high-dimensional robotics), consider [256, 256, 256] or larger.
</p>
<p><b>For Beginners:</b> This controls how "smart" your neural network can be.
<p>The default [128, 128] means:</p>
<ul>
<li>Your network has 2 hidden layers</li>
<li>Each layer has 128 artificial neurons</li>
<li>This creates a network like: Input → [128 neurons] → [128 neurons] → Output</li>
</ul>
<p>Think of layers like levels of thinking:</p>
<ul>
<li>First layer: Recognizes basic patterns (&quot;is enemy close?&quot;)</li>
<li>Second layer: Combines patterns into strategies (&quot;enemy close + have weapon = attack&quot;)</li>
</ul>
<p>You might want more layers/neurons [256, 256, 256] if:</p>
<ul>
<li>Your problem is very complex (chess, robot navigation)</li>
<li>Simple networks aren't learning well</li>
<li>You have lots of training data and computing power</li>
</ul>
<p>You might want fewer [64] or [64, 64] if:</p>
<ul>
<li>Your problem is simple (tic-tac-toe, balancing a pole)</li>
<li>Training is too slow</li>
<li>You're just experimenting</li>
</ul>
<p>Good rule of thumb: Start with the default and adjust based on results.</p>

</div>




  <a id="AiDotNet_ReinforcementLearning_Policies_DiscretePolicyOptions_1_LossFunction_" data-uid="AiDotNet.ReinforcementLearning.Policies.DiscretePolicyOptions`1.LossFunction*"></a>

  <h3 id="AiDotNet_ReinforcementLearning_Policies_DiscretePolicyOptions_1_LossFunction" data-uid="AiDotNet.ReinforcementLearning.Policies.DiscretePolicyOptions`1.LossFunction">
  LossFunction
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/ReinforcementLearning/Policies/DiscretePolicyOptions.cs/#L161"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Gets or sets the loss function used to train the policy network.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public ILossFunction&lt;T&gt; LossFunction { get; set; }</code></pre>
  </div>





  <h4 class="section">Property Value</h4>
  <dl class="parameters">
    <dt><a class="xref" href="AiDotNet.Interfaces.ILossFunction-1.html">ILossFunction</a>&lt;T&gt;</dt>
    <dd><p>The loss function for computing training error. Defaults to Mean Squared Error.</p>
</dd>
  </dl>




  <h4 class="section" id="AiDotNet_ReinforcementLearning_Policies_DiscretePolicyOptions_1_LossFunction_remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
The loss function quantifies how well the policy's predictions match the target values during
training. For policy gradient methods (PPO, A2C), this is typically used for value function
approximation or advantage estimation. Mean Squared Error is the standard choice as it provides
stable gradients and works well with continuous value predictions. Some advanced algorithms may
benefit from Huber loss for robustness to outliers.
</p>
<p><b>For Beginners:</b> The loss function measures "how wrong" the policy is during learning.
<p>The default Mean Squared Error (MSE) works by:</p>
<ul>
<li>Taking the difference between predicted and actual values</li>
<li>Squaring it (so negatives don't cancel positives)</li>
<li>Averaging across all examples</li>
</ul>
<p>You almost never need to change this from the default. MSE is the industry standard
and works well for reinforcement learning. Only consider alternatives if you're implementing
advanced research algorithms or experiencing specific training instabilities.</p>

</div>




  <a id="AiDotNet_ReinforcementLearning_Policies_DiscretePolicyOptions_1_Seed_" data-uid="AiDotNet.ReinforcementLearning.Policies.DiscretePolicyOptions`1.Seed*"></a>

  <h3 id="AiDotNet_ReinforcementLearning_Policies_DiscretePolicyOptions_1_Seed" data-uid="AiDotNet.ReinforcementLearning.Policies.DiscretePolicyOptions`1.Seed">
  Seed
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/ReinforcementLearning/Policies/DiscretePolicyOptions.cs/#L232"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Gets or sets the random seed for reproducible training runs.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public int? Seed { get; set; }</code></pre>
  </div>





  <h4 class="section">Property Value</h4>
  <dl class="parameters">
    <dt><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.int32">int</a>?</dt>
    <dd><p>Optional random seed. When null, uses a random seed. When set to a value, ensures deterministic behavior.</p>
</dd>
  </dl>




  <h4 class="section" id="AiDotNet_ReinforcementLearning_Policies_DiscretePolicyOptions_1_Seed_remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
Setting a specific seed value ensures that training runs are reproducible, which is essential
for debugging, comparing algorithms, and scientific research. However, in production or when
seeking diverse solutions, using null (random seed) allows for variation across runs that
might discover better policies. Note that reproducibility also requires deterministic environment
implementations and consistent hardware/software configurations.
</p>
<p><b>For Beginners:</b> Random seed controls whether your training is the same every time.
<ul>
<li>Set to a number (e.g., 42): Training will be identical each time you run it</li>
<li>Set to null (default): Each training run will be different</li>
</ul>
<p>Use a fixed seed when:</p>
<ul>
<li>Debugging (you want to see the exact same behavior)</li>
<li>Comparing algorithms (fair comparison requires same randomness)</li>
<li>Publishing research (others should be able to reproduce your results)</li>
</ul>
<p>Use null (random) when:</p>
<ul>
<li>Training multiple models to pick the best one</li>
<li>You want variation in learned behaviors</li>
<li>Running in production where diversity is valuable</li>
</ul>
<p>Common practice: Use seed=42 during development, null in production.</p>

</div>




  <a id="AiDotNet_ReinforcementLearning_Policies_DiscretePolicyOptions_1_StateSize_" data-uid="AiDotNet.ReinforcementLearning.Policies.DiscretePolicyOptions`1.StateSize*"></a>

  <h3 id="AiDotNet_ReinforcementLearning_Policies_DiscretePolicyOptions_1_StateSize" data-uid="AiDotNet.ReinforcementLearning.Policies.DiscretePolicyOptions`1.StateSize">
  StateSize
  <a class="header-action link-secondary" title="View source" href="https://github.com/ooples/AiDotNet/blob/master/src/ReinforcementLearning/Policies/DiscretePolicyOptions.cs/#L71"><i class="bi bi-code-slash"></i></a>
  </h3>

  <div class="markdown level1 summary"><p>Gets or sets the size of the observation/state space.</p>
</div>
  <div class="markdown level1 conceptual"></div>

  <div class="codewrapper">
    <pre><code class="lang-csharp hljs">public int StateSize { get; set; }</code></pre>
  </div>





  <h4 class="section">Property Value</h4>
  <dl class="parameters">
    <dt><a class="xref" href="https://learn.microsoft.com/dotnet/api/system.int32">int</a></dt>
    <dd><p>The number of input features that describe the environment state. Must be greater than 0.</p>
</dd>
  </dl>




  <h4 class="section" id="AiDotNet_ReinforcementLearning_Policies_DiscretePolicyOptions_1_StateSize_remarks">Remarks</h4>
  <div class="markdown level1 remarks"><p>
The state size defines the dimensionality of observations from the environment. For example,
in a CartPole environment this might be 4 (cart position, cart velocity, pole angle, pole velocity).
In an Atari game using pixel inputs, this would be the flattened image size or number of features
extracted from preprocessing.
</p>
<p><b>For Beginners:</b> This is how many numbers describe "what's happening" in your environment.
<p>Examples:</p>
<ul>
<li>Simple game: 4 numbers (player X, player Y, enemy X, enemy Y)</li>
<li>Chess board: 64 squares × types of pieces = hundreds of features</li>
<li>Robot arm: 6 numbers (one for each joint angle)</li>
</ul>
<p>Set this to match your environment's observation space size.</p>

</div>





</article>

        <div class="contribution d-print-none">
          <a href="https://github.com/ooples/AiDotNet/blob/master/src/ReinforcementLearning/Policies/DiscretePolicyOptions.cs/#L48" class="edit-link">Edit this page</a>
        </div>


      </div>

      <div class="affix">
        <nav id="affix"></nav>
      </div>
    </main>

    <div class="container-xxl search-results" id="search-results"></div>

    <footer class="border-top text-secondary">
      <div class="container-xxl">
        <div class="flex-fill">
          AiDotNet - Enterprise AI/ML Library for .NET
        </div>
      </div>
    </footer>
  </body>
</html>
