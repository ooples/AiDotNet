{"data":{"repository":{"pullRequest":{"reviewThreads":{"nodes":[{"id":"PRRT_kwDOKSXUF85s9aDy","isResolved":true,"path":"src/NeuralNetworks/Tabular/AutoIntBase.cs","line":436,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Interacting layers & embeddings never learn.**  \n`BackwardBackbone` only backprops through the MLP, and `InteractingLayer.UpdateParameters` is empty. As a result, attention weights and feature embeddings remain at initialization, which breaks AutoInt‚Äôs core learning mechanism. Please implement gradient propagation/updates for interacting layers and embeddings (or explicitly mark the backbone as inference‚Äëonly to avoid silent failure).  \n\n\n\nAlso applies to: 493-497\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/NeuralNetworks/Tabular/AutoIntBase.cs` around lines 243 - 280,\nBackwardBackbone currently only backprops through _mlpLayers and\nInteractingLayer.UpdateParameters is empty, so interacting layers and embeddings\nnever receive gradients or update; modify BackwardBackbone to propagate\ngradients through the interacting backbone as well (call each interacting\nlayer's Backward in the correct reverse order relative to the MLP so\nattention/embedding grads flow) and implement InteractingLayer.UpdateParameters\nto apply its parameter updates (attention weights and any embedding parameters)\nusing the gradients computed in Backward; keep the existing UpdateParameters\nloop over _interactingLayers/_mlpLayers but ensure each InteractingLayer has\nworking Backward and UpdateParameters (or, if intended to be inference-only, add\nan explicit flag on the backbone and skip training with a clear guard to avoid\nsilent failures).\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:eagle -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85s9aD2","isResolved":true,"path":"src/NeuralNetworks/Tabular/ContrastivePretraining.cs","line":307,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n<details>\n<summary>üß© Analysis chain</summary>\n\nüåê Web query:\n\n`InfoNCE loss positive pairs contrastive learning original corrupted`\n\nüí° Result:\n\n**InfoNCE (the loss) uses ‚Äúpositive pairs‚Äù that are defined by your pretext task; ‚Äúoriginal vs. corrupted‚Äù is one common way to create them.**\n\n### 1) In the *original* InfoNCE / CPC formulation (van den Oord et al., 2018)\n- You build a **context** vector \\(c_t\\) from part of a sequence (e.g., past latents).\n- The **positive** is the *true* related sample from the same underlying data-generating context‚Äîe.g., the **future latent** \\(z_{t+k}\\) from the **same sequence** that the model is supposed to predict.\n- **Negatives** are other samples \\(z\\) that are **not** the correct future for that context (often other positions and/or other sequences in the batch).  \nThis is presented as identifying the correct (positive) sample among many (mostly unrelated) candidates via a softmax/CE objective. [2], [3]\n\n### 2) In modern ‚Äúinstance discrimination‚Äù contrastive learning (e.g., SimCLR)\n- You take one **original** datum \\(x\\) (e.g., an image) and create two **stochastic augmentations** (often thought of as **corruptions**) \\(t_1(x), t_2(x)\\).\n- The **positive pair** is: **(view 1 of the same original, view 2 of the same original)**.\n- All other views from other originals in the minibatch are treated as **negatives**. [1]\n\n### 3) So what about **‚Äúoriginal vs. corrupted‚Äù** specifically?\n- InfoNCE itself doesn‚Äôt require ‚Äúclean vs. corrupted‚Äù; it requires *two views that should match*.\n- Many pipelines **do not** use the pristine original as one side; they use **two corrupted/augmented views** of the same original (SimCLR-style). [1]\n- In *denoising / noisy-view* variants, positives are sometimes explicitly **(noisy, clean)** or **(two independent noisy renderings of the same underlying content)**‚Äîstill the same InfoNCE idea: positives share underlying information; negatives don‚Äôt. [4]\n\n**Sources:** SimCLR (positive = two augmented views of same instance) [1]; CPC / InfoNCE origin and ‚Äúpick the positive among negatives‚Äù framing [2], [3]; example of noisy/noisy positives sharing content with different noise realizations [4].  \n\n[1] https://arxiv.org/abs/2002.05709  \n[2] https://arxiv.org/abs/1807.03748  \n[3] https://lilianweng.github.io/posts/2021-05-31-contrastive/  \n[4] https://www.mdpi.com/1424-8220/26/1/249\n\n---\n\nüåê Web query:\n\n`SAINT tabular contrastive pretraining positive pair definition`\n\nüí° Result:\n\nIn SAINT‚Äôs tabular contrastive pretraining, a **positive pair** is formed by **two ‚Äúviews‚Äù of the same underlying row** \\(x\\):\n\n- **View 1 (clean):** the embedding of the original row, \\(e\\).\n- **View 2 (augmented):** an augmented version of that same row, \\(\\tilde e\\), produced by **CutMix in raw feature space** (mixing some columns of \\(x\\) with a randomly chosen batch partner) followed by **mixup in embedding space** (mixing embeddings with another batch partner). ([ar5iv.org](https://ar5iv.org/pdf/2106.01342))\n\nSAINT then applies the contrastive loss to **pull together the latent representations of these two views** \\((z, \\tilde z)\\) as the positive pair, while treating representations of **other rows in the batch** as negatives. ([ar5iv.org](https://ar5iv.org/pdf/2106.01342))\n\nCitations:\n\n- 1: https://ar5iv.org/pdf/2106.01342\n- 2: https://ar5iv.org/pdf/2106.01342\n\n---\n\nüåê Web query:\n\n`contrastive learning framework NT-Xent loss positive example definition`\n\nüí° Result:\n\nIn SimCLR-style contrastive learning with the **NT‚ÄëXent loss**, a **positive example (positive pair)** is defined as **two differently augmented (‚Äútwo correlated views‚Äù) versions of the *same* underlying data instance** (e.g., the same image augmented twice), denoted \\(\\tilde{x}_i\\) and \\(\\tilde{x}_j\\). The NT‚ÄëXent objective then trains the model to identify \\(\\tilde{x}_j\\) as the match for \\(\\tilde{x}_i\\) among all other augmented samples in the minibatch, treating the remaining \\(2(N-1)\\) views as negatives. [1]\n\n**Source**  \n[1] Chen et al., *A Simple Framework for Contrastive Learning of Visual Representations* (SimCLR), ICML 2020 (PMLR) ‚Äì PDF.\n\n---\n\n\n\n</details>\n\n**InfoNCE positive pair should be original‚Äìcorrupted, not original‚Äìoriginal.**\n\nIn SAINT's tabular contrastive pretraining, the positive pair is defined as two views of the same row: the original embedding and its augmented (corrupted) counterpart. The current implementation incorrectly uses the original‚Äìoriginal diagonal as the positive, which is not a valid positive pair in contrastive learning. This should be corrected to match original sample `i` with its augmented version.\n\n<details>\n<summary>‚úÖ Adjust positive-pair selection</summary>\n\n```diff\n-            var posScore = NumOps.Subtract(similarities[i, i], maxSim); // Diagonal is positive pair\n+            // Positive pair is original vs. its corrupted counterpart\n+            var posScore = NumOps.Subtract(similarities[i, batchSize + i], maxSim);\n```\n</details>\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    private T ComputeInfoNCELoss(Tensor<T> original, Tensor<T> corrupted, int batchSize)\n    {\n        // Compute similarity matrix\n        var similarities = new T[batchSize, batchSize * 2];\n        var tempScale = NumOps.FromDouble(1.0 / _temperature);\n\n        for (int i = 0; i < batchSize; i++)\n        {\n            // Similarities with original samples\n            for (int j = 0; j < batchSize; j++)\n            {\n                var sim = NumOps.Zero;\n                for (int d = 0; d < _projectionDim; d++)\n                {\n                    sim = NumOps.Add(sim, NumOps.Multiply(\n                        original[i * _projectionDim + d],\n                        original[j * _projectionDim + d]));\n                }\n                similarities[i, j] = NumOps.Multiply(sim, tempScale);\n            }\n\n            // Similarities with corrupted samples\n            for (int j = 0; j < batchSize; j++)\n            {\n                var sim = NumOps.Zero;\n                for (int d = 0; d < _projectionDim; d++)\n                {\n                    sim = NumOps.Add(sim, NumOps.Multiply(\n                        original[i * _projectionDim + d],\n                        corrupted[j * _projectionDim + d]));\n                }\n                similarities[i, batchSize + j] = NumOps.Multiply(sim, tempScale);\n            }\n        }\n\n        // Compute cross-entropy loss (positive pairs are diagonal original-original)\n        var totalLoss = NumOps.Zero;\n        for (int i = 0; i < batchSize; i++)\n        {\n            // Find max for numerical stability\n            var maxSim = similarities[i, 0];\n            for (int j = 1; j < batchSize * 2; j++)\n            {\n                if (NumOps.Compare(similarities[i, j], maxSim) > 0)\n                    maxSim = similarities[i, j];\n            }\n\n            // Compute log-sum-exp\n            var sumExp = NumOps.Zero;\n            for (int j = 0; j < batchSize * 2; j++)\n            {\n                sumExp = NumOps.Add(sumExp, NumOps.Exp(NumOps.Subtract(similarities[i, j], maxSim)));\n            }\n\n            // Loss = -log(exp(pos) / sum(exp)) = -pos + log(sum(exp))\n            // Positive pair is original vs. its corrupted counterpart\n            var posScore = NumOps.Subtract(similarities[i, batchSize + i], maxSim);\n            var loss = NumOps.Subtract(\n                NumOps.Add(NumOps.Log(sumExp), maxSim),\n                NumOps.Add(posScore, maxSim));\n            totalLoss = NumOps.Add(totalLoss, loss);\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/NeuralNetworks/Tabular/ContrastivePretraining.cs` around lines 211 - 270,\nComputeInfoNCELoss currently treats the positive pair as original-original\n(using similarities[i, i]) but InfoNCE should match original sample i with its\ncorrupted/augmented counterpart; change the positive score selection to use the\ncorrupted column for the same index (i.e., use similarities[i, batchSize + i]\ninstead of similarities[i, i]) so the posScore and subsequent loss computation\nreflect original‚Äìcorrupted pairs; update any comments referencing the diagonal\npositive pair to avoid confusion.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:eagle -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85s9lPF","isResolved":true,"path":"src/Classification/Boosting/DARTClassifier.cs","line":181,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**Same performance issue: Redundant predictions when subtracting dropped contributions.**\n\nSimilar to the update phase, this loop calls `_trees[dropIdx].Predict(x)` for each dropped tree, computing full-matrix predictions that could be cached once.\n\n\n<details>\n<summary>‚ö° Proposed fix: Cache predictions for reuse in both subtract and re-add phases</summary>\n\n```diff\n                 if (numToDrop > 0)\n                 {\n                     var allIndices = Enumerable.Range(0, _trees.Count).ToList();\n                     for (int d = 0; d < numToDrop; d++)\n                     {\n                         int randomIdx = _random.Next(allIndices.Count);\n                         droppedIndices.Add(allIndices[randomIdx]);\n                         allIndices.RemoveAt(randomIdx);\n                     }\n\n+                    // Cache dropped tree predictions for reuse\n+                    var droppedTreePreds = new Dictionary<int, Vector<T>>();\n+                    foreach (int dropIdx in droppedIndices)\n+                    {\n+                        droppedTreePreds[dropIdx] = _trees[dropIdx].Predict(x);\n+                    }\n+\n                     // Subtract dropped tree contributions\n                     foreach (int dropIdx in droppedIndices)\n                     {\n-                        var treePred = _trees[dropIdx].Predict(x);\n                         double weight = _treeWeights[dropIdx] * _options.LearningRate;\n                         for (int i = 0; i < n; i++)\n                         {\n-                            keptPredictions[i] -= weight * NumOps.ToDouble(treePred[i]);\n+                            keptPredictions[i] -= weight * NumOps.ToDouble(droppedTreePreds[dropIdx][i]);\n                         }\n                     }\n```\n</details>\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/Classification/Boosting/DARTClassifier.cs` around lines 166 - 176, The\nloop calling _trees[dropIdx].Predict(x) for each dropIdx recomputes full\npredictions redundantly; change the code to compute and cache the prediction\narray once per dropped index (e.g., store in a local variable like droppedPred),\nthen reuse that cached array when subtracting contributions from keptPredictions\n(and later when re-adding), using the existing _treeWeights[dropIdx] and\n_options.LearningRate and converting elements with NumOps.ToDouble as before;\nupdate references in the subtract and re-add phases to use the cached\ndroppedPred to avoid repeated Predict calls.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:ocelot -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85s9lPL","isResolved":true,"path":"src/Classification/Boosting/DARTClassifier.cs","line":233,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**Performance issue: Redundant full-matrix predictions inside sample loop.**\n\nLines 224-228 call `_trees[dropIdx].Predict(x)` inside a nested loop (for each sample `i` and each dropped tree). Since `Predict(x)` computes predictions for all samples, this results in `O(n * droppedTrees)` redundant full-matrix predictions when only `O(droppedTrees)` are needed.\n\n\n<details>\n<summary>‚ö° Proposed fix: Cache tree predictions outside the sample loop</summary>\n\n```diff\n             // Update predictions\n             var newTreePred = newTree.Predict(x);\n+\n+            // Cache dropped tree predictions to avoid redundant computation\n+            var droppedTreePreds = new Dictionary<int, Vector<T>>();\n+            foreach (int dropIdx in droppedIndices)\n+            {\n+                droppedTreePreds[dropIdx] = _trees[dropIdx].Predict(x);\n+            }\n+\n             for (int i = 0; i < n; i++)\n             {\n                 predictions[i] = keptPredictions[i] + newWeight * _options.LearningRate * NumOps.ToDouble(newTreePred[i]);\n\n                 // Re-add dropped tree contributions with updated weights\n                 foreach (int dropIdx in droppedIndices)\n                 {\n-                    var treePred = _trees[dropIdx].Predict(x);\n-                    predictions[i] += _treeWeights[dropIdx] * _options.LearningRate * NumOps.ToDouble(treePred[i]);\n+                    predictions[i] += _treeWeights[dropIdx] * _options.LearningRate * NumOps.ToDouble(droppedTreePreds[dropIdx][i]);\n                 }\n             }\n```\n</details>\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/Classification/Boosting/DARTClassifier.cs` around lines 217 - 229, The\nnested loop recomputes full-vector predictions by calling\n_trees[dropIdx].Predict(x) inside the per-sample loop, causing redundant work;\nto fix, compute and cache each dropped tree's prediction vector once before the\nfor (int i = 0; i < n; i++) loop (use a local List<double[]> or similar for\ndroppedPreds indexed to droppedIndices), then inside the sample loop use\ndroppedPreds[dropIdxIndex][i] when accumulating contributions to predictions[i];\nupdate references to newTreePred, keptPredictions, droppedIndices, _trees,\n_treeWeights, _options.LearningRate and NumOps.ToDouble accordingly so all\ndropped-tree predictions are reused instead of recomputed.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:ocelot -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85s9lPR","isResolved":true,"path":"src/Classification/Boosting/ExplainableBoostingClassifier.cs","line":283,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü° Minor_\n\n**Inconsistent condition for enabling interaction detection.**\n\nThe condition checks `MaxInteractionBins > 0` but semantically it should check `MaxInteractions > 0`. With the current defaults (`MaxInteractions=10`, `MaxInteractionBins=32`), interactions are always enabled, but if a user sets `MaxInteractions=0` expecting to disable interactions, they will still run.\n\n\n<details>\n<summary>üí° Proposed fix: Check MaxInteractions instead</summary>\n\n```diff\n         // Detect and add interaction terms if enabled\n-        if (_options.MaxInteractionBins > 0)\n+        if (_options.MaxInteractions > 0)\n         {\n             DetectInteractions(x, sampleBins, yBinary);\n         }\n```\n</details>\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        // Detect and add interaction terms if enabled\n        if (_options.MaxInteractions > 0)\n        {\n            DetectInteractions(x, sampleBins, yBinary);\n        }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/Classification/Boosting/ExplainableBoostingClassifier.cs` around lines\n259 - 263, The interaction detection guard is using the wrong option; replace\nthe check \"if (_options.MaxInteractionBins > 0)\" with \"if\n(_options.MaxInteractions > 0)\" so DetectInteractions(x, sampleBins, yBinary) is\nonly invoked when MaxInteractions is enabled; update the condition in\nExplainableBoostingClassifier (around the DetectInteractions call) to reference\n_options.MaxInteractions instead of _options.MaxInteractionBins.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:ocelot -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85s9lPa","isResolved":true,"path":"src/Classification/Boosting/ExplainableBoostingClassifier.cs","line":null,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü° Minor_\n\n**Hardcoded limit of 10 features for interaction detection.**\n\nThe interaction detection only considers the first 10 features (`Math.Min(_numFeatures, 10)`). This is a significant limitation that should either be configurable or documented. For datasets with many important features beyond the first 10, interactions between those features will never be detected.\n\n\n<details>\n<summary>üí° Proposed fix: Make the feature limit configurable or use feature importances</summary>\n\nConsider adding a `MaxInteractionFeatures` option or selecting features based on their importance scores rather than position:\n\n```diff\n-        for (int f1 = 0; f1 < Math.Min(_numFeatures, 10); f1++)\n+        int maxFeaturesToConsider = Math.Min(_numFeatures, _options.MaxInteractionFeatures ?? 10);\n+        for (int f1 = 0; f1 < maxFeaturesToConsider; f1++)\n         {\n-            for (int f2 = f1 + 1; f2 < Math.Min(_numFeatures, 10); f2++)\n+            for (int f2 = f1 + 1; f2 < maxFeaturesToConsider; f2++)\n```\n</details>\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/Classification/Boosting/ExplainableBoostingClassifier.cs` around lines\n478 - 486, The loop that builds interaction candidates is limited to the first\n10 features (Math.Min(_numFeatures, 10)), which hides interactions beyond index\n9; change this to use a configurable limit or select top-K important features\ninstead. Add a MaxInteractionFeatures option/property (or accept it via the\nclassifier constructor/options) with a sensible default, or compute a list of\ntop feature indices from your feature importance array (e.g.,\n_featureImportances) and use that list; then replace Math.Min(_numFeatures, 10)\nwith Math.Min(_numFeatures, MaxInteractionFeatures) or iterate over the\nselectedIndices when calling ComputeInteractionScore(sampleBins[f1],\nsampleBins[f2], residuals) and adding to candidates. Ensure the new option is\nused consistently wherever interaction candidates are generated so interactions\nare discovered beyond the first 10 features.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:ocelot -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85s9lPd","isResolved":true,"path":"src/Models/Options/DARTClassifierOptions.cs","line":114,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Missing `Seed` property causes runtime failure.**\n\nThe `DARTClassifier` references `_options.Seed` (lines 107-109 in the classifier), but this options class doesn't define a `Seed` property. This will cause a compilation error.\n\n\n<details>\n<summary>üêõ Proposed fix: Add the missing Seed property</summary>\n\n```diff\n     /// <summary>\n     /// Gets or sets how often to print progress.\n     /// </summary>\n     public int VerboseEval { get; set; } = 10;\n+\n+    /// <summary>\n+    /// Gets or sets the random seed for reproducibility.\n+    /// </summary>\n+    /// <value>Default is null (non-deterministic).</value>\n+    public int? Seed { get; set; }\n }\n```\n</details>\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/Models/Options/DARTClassifierOptions.cs` around lines 24 - 114, The\nDARTClassifierOptions<T> class is missing a Seed property but DARTClassifier\naccesses _options.Seed, causing a compile error; add a public int? Seed { get;\nset; } (or int Seed with a sensible default) to DARTClassifierOptions<T> so the\n_options.Seed reference resolves, e.g., declare Seed in the\nDARTClassifierOptions<T> class alongside NumberOfIterations, and ensure the type\n(nullable vs non-nullable) matches how DARTClassifier uses it.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:ocelot -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85s9lPg","isResolved":true,"path":"src/Models/Options/ExplainableBoostingClassifierOptions.cs","line":108,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Missing `Seed` property causes runtime failure.**\n\nThe `ExplainableBoostingClassifier` references `_options.Seed` (lines 141-143 in the classifier), but this options class doesn't define a `Seed` property. This will cause a compilation error.\n\n\n<details>\n<summary>üêõ Proposed fix: Add the missing Seed property</summary>\n\n```diff\n     /// <summary>\n     /// Gets or sets how often to print progress.\n     /// </summary>\n     public int VerboseEval { get; set; } = 10;\n+\n+    /// <summary>\n+    /// Gets or sets the random seed for reproducibility.\n+    /// </summary>\n+    /// <value>Default is null (non-deterministic).</value>\n+    public int? Seed { get; set; }\n }\n```\n</details>\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/Models/Options/ExplainableBoostingClassifierOptions.cs` around lines 24 -\n98, The ExplainableBoostingClassifierOptions<T> class is missing a Seed property\nthat ExplainableBoostingClassifier expects via _options.Seed; add a public int?\nSeed { get; set; } (nullable to preserve default behavior) to\nExplainableBoostingClassifierOptions<T> so the classifier can read the seed\nwithout compile errors, ensuring the property name exactly matches \"Seed\" used\nby ExplainableBoostingClassifier.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:ocelot -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85s9lPh","isResolved":true,"path":"src/Models/Options/ExplainableBoostingClassifierOptions.cs","line":91,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü° Minor_\n\n**Inconsistent interaction control: `MaxInteractions` vs `MaxInteractionBins`.**\n\nThe documentation for `MaxInteractions` says \"Set to 0 to disable interaction detection\", but the classifier actually checks `MaxInteractionBins > 0` to determine whether to detect interactions. This creates inconsistent behavior where setting `MaxInteractions = 0` won't actually disable interactions.\n\nConsider aligning the condition in the classifier to check `MaxInteractions > 0` instead, or update the documentation here.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/Models/Options/ExplainableBoostingClassifierOptions.cs` around lines 65 -\n81, The docs say disabling interactions is controlled by MaxInteractions (set to\n0), but the classifier actually uses MaxInteractionBins to decide whether to\ndetect interactions; update the classifier logic to use MaxInteractions > 0 when\ndeciding to enable interaction detection (instead of checking MaxInteractionBins\n> 0), e.g. in the interaction-detection branch inside the\nExplainableBoostingClassifier (or the method that constructs/determines\ninteractions), and keep MaxInteractionBins solely as the bin-size setting;\nalternatively, if you prefer the current code, update the XML docs for\nMaxInteractions to reflect that MaxInteractionBins controls enabling\ninteractions‚Äîapply one of these two fixes so behavior and documentation are\nconsistent.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:ocelot -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85s9lPj","isResolved":true,"path":"src/Models/Options/NGBoostClassifierOptions.cs","line":157,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Missing `Seed` property causes runtime failure.**\n\nThe `NGBoostClassifier` references `_options.Seed` (lines 107-109 in the classifier), but this options class doesn't define a `Seed` property. This will cause a compilation error.\n\n\n<details>\n<summary>üêõ Proposed fix: Add the missing Seed property</summary>\n\n```diff\n     /// <summary>\n     /// Gets or sets how often to print progress (every N iterations).\n     /// </summary>\n     /// <value>Default is 100.</value>\n     public int VerboseEval { get; set; } = 100;\n+\n+    /// <summary>\n+    /// Gets or sets the random seed for reproducibility.\n+    /// </summary>\n+    /// <value>Default is null (non-deterministic).</value>\n+    public int? Seed { get; set; }\n }\n```\n</details>\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\npublic class NGBoostClassifierOptions<T> : ClassifierOptions<T>\n{\n    /// <summary>\n    /// Gets or sets the number of boosting iterations (trees per class).\n    /// </summary>\n    /// <value>Default is 500.</value>\n    /// <remarks>\n    /// <para>\n    /// More iterations generally improve performance but increase training time\n    /// and can lead to overfitting. Use early stopping to find the optimal number.\n    /// </para>\n    /// <para><b>For Beginners:</b> This is how many rounds of boosting to perform.\n    /// More rounds usually means better predictions, but takes longer to train.\n    /// Start with the default and use validation data to determine if you need more.\n    /// </para>\n    /// </remarks>\n    public int NumberOfIterations { get; set; } = 500;\n\n    /// <summary>\n    /// Gets or sets the learning rate (shrinkage factor).\n    /// </summary>\n    /// <value>Default is 0.01.</value>\n    /// <remarks>\n    /// <para>\n    /// Lower learning rates require more iterations but often achieve better\n    /// generalization. NGBoost typically works well with lower learning rates\n    /// than traditional gradient boosting.\n    /// </para>\n    /// <para><b>For Beginners:</b> This controls how much each tree contributes.\n    /// Smaller values mean each tree has less influence, making learning more gradual\n    /// but more stable. The default of 0.01 is recommended for NGBoost.\n    /// </para>\n    /// </remarks>\n    public double LearningRate { get; set; } = 0.01;\n\n    /// <summary>\n    /// Gets or sets the maximum depth of trees.\n    /// </summary>\n    /// <value>Default is 3.</value>\n    public int MaxDepth { get; set; } = 3;\n\n    /// <summary>\n    /// Gets or sets the minimum number of samples required to split an internal node.\n    /// </summary>\n    /// <value>Default is 2.</value>\n    public int MinSamplesSplit { get; set; } = 2;\n\n    /// <summary>\n    /// Gets or sets the minimum number of samples required to be at a leaf node.\n    /// </summary>\n    /// <value>Default is 1.</value>\n    public int MinSamplesLeaf { get; set; } = 1;\n\n    /// <summary>\n    /// Gets or sets the fraction of features to consider for splits.\n    /// </summary>\n    /// <value>Default is 1.0 (use all features).</value>\n    /// <remarks>\n    /// Value should be between 0 and 1. Set to 1.0 to use all features.\n    /// </remarks>\n    public double MaxFeatures { get; set; } = 1.0;\n\n    /// <summary>\n    /// Gets or sets the split criterion for the base trees.\n    /// </summary>\n    /// <value>Default is VarianceReduction.</value>\n    public SplitCriterion SplitCriterion { get; set; } = SplitCriterion.VarianceReduction;\n\n    /// <summary>\n    /// Gets or sets the subsampling ratio for each iteration.\n    /// </summary>\n    /// <value>Default is 0.5.</value>\n    /// <remarks>\n    /// <para>\n    /// Values less than 1.0 enable stochastic gradient boosting where each tree\n    /// is trained on a random subset of the data. This can improve generalization.\n    /// </para>\n    /// <para><b>For Beginners:</b> This determines what fraction of your data\n    /// is used to train each tree. Using 0.5 means each tree only sees half the\n    /// data, which often leads to better generalization and faster training.\n    /// </para>\n    /// </remarks>\n    public double SubsampleRatio { get; set; } = 0.5;\n\n    /// <summary>\n    /// Gets or sets whether to use natural gradients.\n    /// </summary>\n    /// <value>Default is true.</value>\n    /// <remarks>\n    /// <para>\n    /// Natural gradients precondition the gradient by the Fisher Information Matrix,\n    /// which typically leads to faster convergence and more stable optimization.\n    /// For classification, this helps produce better-calibrated probabilities.\n    /// </para>\n    /// <para><b>For Beginners:</b> Natural gradients make the learning process smarter\n    /// by accounting for the geometry of the probability distribution. This usually\n    /// results in faster training and better probability estimates. Keep this enabled.\n    /// </para>\n    /// </remarks>\n    public bool UseNaturalGradient { get; set; } = true;\n\n    /// <summary>\n    /// Gets or sets whether to apply column subsampling.\n    /// </summary>\n    /// <value>Default is 1.0 (use all columns).</value>\n    /// <remarks>\n    /// Values less than 1.0 randomly select a subset of features for each tree,\n    /// similar to Random Forest. This can prevent overfitting.\n    /// </remarks>\n    public double ColumnSubsampleRatio { get; set; } = 1.0;\n\n    /// <summary>\n    /// Gets or sets the number of early stopping rounds.\n    /// </summary>\n    /// <value>Default is null (no early stopping).</value>\n    /// <remarks>\n    /// If set, training stops when the validation score doesn't improve for\n    /// this many consecutive rounds. Requires validation data to be provided.\n    /// </remarks>\n    public int? EarlyStoppingRounds { get; set; }\n\n    /// <summary>\n    /// Gets or sets whether to verbose output during training.\n    /// </summary>\n    /// <value>Default is false.</value>\n    public bool Verbose { get; set; }\n\n    /// <summary>\n    /// Gets or sets how often to print progress (every N iterations).\n    /// </summary>\n    /// <value>Default is 100.</value>\n    public int VerboseEval { get; set; } = 100;\n\n    /// <summary>\n    /// Gets or sets the random seed for reproducibility.\n    /// </summary>\n    /// <value>Default is null (non-deterministic).</value>\n    public int? Seed { get; set; }\n}\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/Models/Options/NGBoostClassifierOptions.cs` around lines 25 - 157, The\noptions class NGBoostClassifierOptions<T> is missing the Seed property\nreferenced by NGBoostClassifier via _options.Seed, causing a compile error; add\na Seed property to NGBoostClassifierOptions<T> with the same type expected by\nthe classifier (preferably nullable int, e.g. int? Seed) and a sensible default\n(null or a fixed int) so NGBoostClassifier can read _options.Seed without\nerrors.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:ocelot -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85s9m0i","isResolved":true,"path":"src/Models/Options/TabTransformerOptions.cs","line":34,"comments":{"nodes":[{"body":"Corrected spelling of 'recieve' to 'receive'."}]}},{"id":"PRRT_kwDOKSXUF85s9rGr","isResolved":true,"path":"src/Models/Options/MixedEffectsOptions.cs","line":null,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü° Minor_\n\n**Add guardrails for invalid iteration/tolerance/confidence inputs.**  \nMaxIterations ‚â§ 0, Tolerance ‚â§ 0, or ConfidenceLevel outside (0,1) will cause non‚Äëconvergence or invalid CI math. Consider validating in setters (or a shared validation hook) to fail fast.  \n\n\n<details>\n<summary>üí° Proposed guardrails</summary>\n\n```diff\n-    public int MaxIterations { get; set; } = 100;\n+    private int _maxIterations = 100;\n+    public int MaxIterations\n+    {\n+        get => _maxIterations;\n+        set\n+        {\n+            if (value <= 0) throw new ArgumentOutOfRangeException(nameof(MaxIterations), \"MaxIterations must be > 0.\");\n+            _maxIterations = value;\n+        }\n+    }\n\n-    public double Tolerance { get; set; } = 1e-6;\n+    private double _tolerance = 1e-6;\n+    public double Tolerance\n+    {\n+        get => _tolerance;\n+        set\n+        {\n+            if (value <= 0) throw new ArgumentOutOfRangeException(nameof(Tolerance), \"Tolerance must be > 0.\");\n+            _tolerance = value;\n+        }\n+    }\n\n-    public double ConfidenceLevel { get; set; } = 0.95;\n+    private double _confidenceLevel = 0.95;\n+    public double ConfidenceLevel\n+    {\n+        get => _confidenceLevel;\n+        set\n+        {\n+            if (value <= 0 || value >= 1) throw new ArgumentOutOfRangeException(nameof(ConfidenceLevel), \"ConfidenceLevel must be between 0 and 1.\");\n+            _confidenceLevel = value;\n+        }\n+    }\n```\n</details>\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    /// <summary>\n    /// Gets or sets the maximum number of iterations for the optimization.\n    /// </summary>\n    /// <value>Default is 100.</value>\n    private int _maxIterations = 100;\n    public int MaxIterations\n    {\n        get => _maxIterations;\n        set\n        {\n            if (value <= 0) throw new ArgumentOutOfRangeException(nameof(MaxIterations), \"MaxIterations must be > 0.\");\n            _maxIterations = value;\n        }\n    }\n\n    /// <summary>\n    /// Gets or sets the convergence tolerance.\n    /// </summary>\n    /// <value>Default is 1e-6.</value>\n    private double _tolerance = 1e-6;\n    public double Tolerance\n    {\n        get => _tolerance;\n        set\n        {\n            if (value <= 0) throw new ArgumentOutOfRangeException(nameof(Tolerance), \"Tolerance must be > 0.\");\n            _tolerance = value;\n        }\n    }\n\n    /// <summary>\n    /// Gets or sets whether to compute confidence intervals for variance components.\n    /// </summary>\n    /// <value>Default is true.</value>\n    public bool ComputeVarianceCI { get; set; } = true;\n\n    /// <summary>\n    /// Gets or sets the confidence level for intervals.\n    /// </summary>\n    /// <value>Default is 0.95 (95% confidence).</value>\n    private double _confidenceLevel = 0.95;\n    public double ConfidenceLevel\n    {\n        get => _confidenceLevel;\n        set\n        {\n            if (value <= 0 || value >= 1) throw new ArgumentOutOfRangeException(nameof(ConfidenceLevel), \"ConfidenceLevel must be between 0 and 1.\");\n            _confidenceLevel = value;\n        }\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/Models/Options/MixedEffectsOptions.cs` around lines 47 - 69,\nMixedEffectsOptions currently exposes MaxIterations, Tolerance, and\nConfidenceLevel with unsafe defaults but no validation; add guardrails so\nMaxIterations > 0, Tolerance > 0, and 0 < ConfidenceLevel < 1 by validating\ninputs (either in the property setters of MaxIterations, Tolerance,\nConfidenceLevel or by adding a Validate() method called from the constructor or\na factory) and throw ArgumentOutOfRangeException with clear messages when values\nare invalid; keep ComputeVarianceCI unchanged.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:eagle -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85s9xRL","isResolved":true,"path":"src/Models/Options/GLMMOptions.cs","line":240,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü° Minor_\n\n**Align Gamma default-link documentation with factory defaults.**  \nThe enum doc says Inverse is the default for Gamma, but `ForPositiveContinuous()` configures Gamma + Log. Please align either the docs or the factory choice to avoid confusion.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/Models/Options/GLMMOptions.cs` around lines 237 - 240, The XML doc for\nthe enum member Inverse claims \"Default for Gamma\" but the factory method\nForPositiveContinuous() configures Gamma with the Log link; update the\ndocumentation to match the factory by changing the Inverse member's comment to\nnot claim it's the default for Gamma (e.g., state that Log is the\ntypical/default link used by ForPositiveContinuous or remove the \"Default for\nGamma\" phrase), or alternatively change ForPositiveContinuous() to select\nInverse if you prefer the enum comment to be authoritative‚Äîupdate whichever you\nchoose so the enum member Inverse and the ForPositiveContinuous() factory are\nconsistent.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:eagle -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85s9_uE","isResolved":true,"path":"src/Classification/Calibration/CalibratedClassifier.cs","line":149,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**Skip CV/holdout when calibration is disabled.**\n\nLine 129-138: `CalibrationMethod.None` (and currently `Auto`) still forces CV/holdout, which can throw on small datasets and on non-clonable base classifiers. If calibration is disabled, train the base model on all data and return early.  \n\n<details>\n<summary>Proposed fix</summary>\n\n```diff\n public override void Train(Matrix<T> x, Vector<T> y)\n {\n     if (x.Rows != y.Length)\n     {\n         throw new ArgumentException(\"Number of samples in X must match length of y.\");\n     }\n@@\n     ClassLabels = ExtractClassLabels(y);\n     NumClasses = ClassLabels.Length;\n     TaskType = InferTaskType(y);\n \n+    if (_options.CalibrationMethod == ProbabilityCalibrationMethod.None\n+        || _options.CalibrationMethod == ProbabilityCalibrationMethod.Auto)\n+    {\n+        _baseClassifier.Train(x, y);\n+        _isTrained = true;\n+        return;\n+    }\n+\n     int n = x.Rows;\n \n     if (_options.CrossValidationFolds > 1)\n```\n</details>\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/Classification/Calibration/CalibratedClassifier.cs` around lines 115 -\n141, CalibratedClassifier.Train currently always does cross-validation or\nholdout even when calibration is disabled; update Train to check\n_options.CalibrationMethod and if it is CalibrationMethod.None (and treat Auto\nas disabled when appropriate) then train the underlying base classifier on all\ndata and return early instead of calling\nTrainWithCrossValidation/TrainWithHoldout. Specifically: after setting\nNumFeatures, ClassLabels, NumClasses and TaskType, if calibration is disabled\ncall the base trainer (e.g. the field/method that trains the base model) on x,y,\nset _isTrained = true and exit; otherwise proceed to TrainWithCrossValidation or\nTrainWithHoldout as now. Ensure you reference the same symbols:\nCalibratedClassifier.Train, _options.CalibrationMethod,\nTrainWithCrossValidation, TrainWithHoldout and the base classifier training\nentry point.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:eagle -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85s9_uH","isResolved":true,"path":"src/Classification/Calibration/CalibratedClassifier.cs","line":323,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü° Minor_\n\n**Validate `CalibrationSetFraction` range.**\n\nLine 255-263: out-of-range fractions (‚â§0 or ‚â•1) can yield negative/zero train sizes or calibration sizes that bypass the current check. Add explicit bounds validation for clearer failures.  \n\n<details>\n<summary>Proposed fix</summary>\n\n```diff\n private void TrainWithHoldout(Matrix<T> x, Vector<T> y)\n {\n     int n = x.Rows;\n+    if (_options.CalibrationSetFraction <= 0 || _options.CalibrationSetFraction >= 1)\n+    {\n+        throw new ArgumentOutOfRangeException(nameof(_options.CalibrationSetFraction),\n+            \"CalibrationSetFraction must be between 0 and 1 (exclusive).\");\n+    }\n     int calibSize = (int)(n * _options.CalibrationSetFraction);\n     int trainSize = n - calibSize;\n+\n+    if (trainSize < 1)\n+    {\n+        throw new ArgumentException(\"Training set too small after calibration split.\");\n+    }\n```\n</details>\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    private void TrainWithHoldout(Matrix<T> x, Vector<T> y)\n    {\n        int n = x.Rows;\n        if (_options.CalibrationSetFraction <= 0 || _options.CalibrationSetFraction >= 1)\n        {\n            throw new ArgumentOutOfRangeException(nameof(_options.CalibrationSetFraction),\n                \"CalibrationSetFraction must be between 0 and 1 (exclusive).\");\n        }\n        int calibSize = (int)(n * _options.CalibrationSetFraction);\n        int trainSize = n - calibSize;\n\n        if (trainSize < 1)\n        {\n            throw new ArgumentException(\"Training set too small after calibration split.\");\n        }\n\n        if (calibSize < 10)\n        {\n            throw new ArgumentException(\n                $\"Calibration set too small ({calibSize} samples). \" +\n                $\"Use more data or increase CalibrationSetFraction.\");\n        }\n\n        // Create shuffled indices\n        var indices = Enumerable.Range(0, n).ToArray();\n        for (int i = n - 1; i > 0; i--)\n        {\n            int j = _random.Next(i + 1);\n            (indices[i], indices[j]) = (indices[j], indices[i]);\n        }\n\n        // Split into train and calibration sets\n        var trainX = new Matrix<T>(trainSize, NumFeatures);\n        var trainY = new Vector<T>(trainSize);\n        var calibX = new Matrix<T>(calibSize, NumFeatures);\n        var calibY = new Vector<T>(calibSize);\n\n        for (int i = 0; i < trainSize; i++)\n        {\n            for (int j = 0; j < NumFeatures; j++)\n            {\n                trainX[i, j] = x[indices[i], j];\n            }\n            trainY[i] = y[indices[i]];\n        }\n\n        for (int i = 0; i < calibSize; i++)\n        {\n            for (int j = 0; j < NumFeatures; j++)\n            {\n                calibX[i, j] = x[indices[trainSize + i], j];\n            }\n            calibY[i] = y[indices[trainSize + i]];\n        }\n\n        // Train base classifier on training set\n        _baseClassifier.Train(trainX, trainY);\n\n        // Get uncalibrated predictions on calibration set\n        var uncalibrated = _baseClassifier.PredictProbabilities(calibX);\n\n        // Fit calibration model\n        FitCalibration(uncalibrated, calibY);\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/Classification/Calibration/CalibratedClassifier.cs` around lines 252 -\n305, The TrainWithHoldout method currently computes calibSize and trainSize from\n_options.CalibrationSetFraction without validating bounds; add an explicit check\nthat _options.CalibrationSetFraction is > 0.0 and < 1.0 at the start of\nTrainWithHoldout (before computing calibSize/trainSize) and throw an\nArgumentException with a clear message referencing invalid\nCalibrationSetFraction when the value is out of range, so\nnegative/zero/one-or-greater fractions fail fast and avoid producing invalid\ncalibSize/trainSize.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:eagle -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85s9_uP","isResolved":true,"path":"src/Classification/Calibration/CalibratedClassifier.cs","line":372,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**Binary target labeling and multiclass calibration are incorrect.**\n\nLine 319-326: `targets[i]` uses `>= 0.5`, which mislabels any binary dataset with labels not in {0,1} and collapses all non-zero classes for multi-class. Either (a) implement per-class calibration parameters or (b) block non-binary usage; at minimum, derive binary targets by comparing to the chosen positive class label.  \n\n<details>\n<summary>Proposed minimal fix (binary-only guard + proper labeling)</summary>\n\n```diff\n private void FitCalibration(Matrix<T> uncalibrated, Vector<T> actuals)\n {\n     int n = uncalibrated.Rows;\n \n+    if (NumClasses != 2)\n+    {\n+        throw new NotSupportedException(\n+            \"Per-class calibration is not implemented. Use binary classification or add per-class parameters.\");\n+    }\n+\n     // For binary classification, use probability of positive class (last column)\n     // For multi-class, we calibrate each class independently\n     var probs = new double[n];\n     var targets = new double[n];\n@@\n-    int positiveClassIdx = NumClasses - 1;\n+    int positiveClassIdx = NumClasses - 1;\n+    double positiveLabel = NumOps.ToDouble(ClassLabels[positiveClassIdx]);\n \n     for (int i = 0; i < n; i++)\n     {\n         probs[i] = NumOps.ToDouble(uncalibrated[i, positiveClassIdx]);\n-        targets[i] = NumOps.ToDouble(actuals[i]) >= 0.5 ? 1.0 : 0.0;\n+        targets[i] = NumOps.ToDouble(actuals[i]) == positiveLabel ? 1.0 : 0.0;\n     }\n```\n</details>\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/Classification/Calibration/CalibratedClassifier.cs` around lines 310 -\n347, FitCalibration currently assumes binary labels via >= 0.5 and collapses\nmulti-class; update it to only allow binary calibration and derive binary\ntargets by comparing to the chosen positive class index: add an early guard that\nthrows or skips calibration if NumClasses != 2 (or if _options.CalibrationMethod\nrequires binary), and change the target construction to targets[i] =\n(NumOps.ToInt32(actuals[i]) == positiveClassIdx ? 1.0 : 0.0) (or equivalent\nequality check on the label representation) instead of >= 0.5; keep the rest of\nthe switch and per-method calls (FitPlattScaling, FitIsotonicRegression,\nFitBetaCalibration, FitTemperatureScaling) unchanged.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:eagle -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85tASuB","isResolved":true,"path":"src/Classification/Boosting/DARTClassifier.cs","line":325,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü° Minor_\n\n**Binary-only `PredictProbabilities` ‚Äî same concern as EBM classifier.**\n\nColumns 0 and 1 are the only ones populated (lines 314‚Äì315). A `NumClasses != 2` guard would prevent silent misuse.\n\n\n\n<details>\n<summary>üõ°Ô∏è Proposed guard</summary>\n\n```diff\n     public override Matrix<T> PredictProbabilities(Matrix<T> input)\n     {\n+        if (NumClasses != 2)\n+        {\n+            throw new NotSupportedException(\"DARTClassifier currently supports binary classification only.\");\n+        }\n+\n         int n = input.Rows;\n```\n</details>\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/Classification/Boosting/DARTClassifier.cs` around lines 288 - 319,\nPredictProbabilities currently only computes two-class probabilities (fills cols\n0 and 1) and will silently produce incorrect output for multi-class models; in\nthe DARTClassifier.PredictProbabilities method add an explicit guard that checks\nNumClasses == 2 and throws a clear exception (e.g., InvalidOperationException)\nif not, or alternatively implement a multi-class path; reference the\nPredictProbabilities method and NumClasses (and existing logic using _trees and\n_treeWeights) when adding the check so callers cannot silently misuse this\nbinary-only implementation.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:phoenix -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85tASuF","isResolved":true,"path":"src/Classification/Boosting/DARTClassifier.cs","line":343,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**`SelectNumDropout` always drops at least 1 tree, even when `DropoutRate` is 0.**\n\n`Math.Max(1, (int)(numTrees * 0.0))` ‚Üí `Math.Max(1, 0)` ‚Üí `1`. A user setting `DropoutRate = 0` would expect no dropout (i.e., standard gradient boosting), but this forces at least one tree to be dropped every iteration.\n\nThe same issue applies to the `Binomial` path: `CountBinomialDrops` can return 0, but the `Uniform` and default branches cannot.\n\n\n\n<details>\n<summary>üêõ Proposed fix</summary>\n\n```diff\n     private int SelectNumDropout(int numTrees)\n     {\n+        if (_options.DropoutRate <= 0) return 0;\n+\n         return _options.DropoutType switch\n         {\n-            DARTDropoutType.Uniform => Math.Max(1, (int)(numTrees * _options.DropoutRate)),\n+            DARTDropoutType.Uniform => Math.Max(1, (int)Math.Round(numTrees * _options.DropoutRate)),\n             DARTDropoutType.Binomial => CountBinomialDrops(numTrees, _options.DropoutRate),\n-            _ => Math.Max(1, (int)(numTrees * _options.DropoutRate))\n+            _ => Math.Max(1, (int)Math.Round(numTrees * _options.DropoutRate))\n         };\n     }\n```\n</details>\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    private int SelectNumDropout(int numTrees)\n    {\n        if (_options.DropoutRate <= 0) return 0;\n\n        return _options.DropoutType switch\n        {\n            DARTDropoutType.Uniform => Math.Max(1, (int)Math.Round(numTrees * _options.DropoutRate)),\n            DARTDropoutType.Binomial => CountBinomialDrops(numTrees, _options.DropoutRate),\n            _ => Math.Max(1, (int)Math.Round(numTrees * _options.DropoutRate))\n        };\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/Classification/Boosting/DARTClassifier.cs` around lines 324 - 332,\nSelectNumDropout currently always returns at least 1 drop (via Math.Max(1,...)),\nwhich forces a dropout even when _options.DropoutRate == 0; change the logic in\nSelectNumDropout so that if _options.DropoutRate <= 0 it returns 0 immediately,\notherwise compute the dropout count (for Uniform use (int)(numTrees *\n_options.DropoutRate) but ensure at least 1 when rate > 0, for Binomial keep\nCountBinomialDrops as-is); apply this to the default branch as well so only\npositive rates produce a minimum of 1 drop while a zero rate yields zero drops.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:phoenix -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85tASuI","isResolved":true,"path":"src/Classification/Boosting/ExplainableBoostingClassifier.cs","line":249,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**Severe performance issue: `ComputeLogOdds` is recomputed from scratch inside the innermost loop.**\n\n`ComputeLogOdds` (line 199) iterates over all samples √ó all features, and it's called once per feature per inner-bag per outer-bag ‚Äî giving O(OuterBags √ó InnerBags √ó Features¬≤ √ó Samples) time complexity. With defaults (50 √ó 50 √ó F¬≤ √ó N), this makes training impractical for even moderate datasets.\n\nThe standard EBM approach maintains a running prediction array updated incrementally after each feature's shape function update, avoiding the full recomputation.\n\n\n\n<details>\n<summary>‚ö° Sketch of incremental approach</summary>\n\n```diff\n+        // Maintain running log-odds predictions\n+        var logOdds = new double[n];\n+        for (int i = 0; i < n; i++)\n+        {\n+            logOdds[i] = NumOps.ToDouble(_intercept);\n+        }\n+\n         for (int outer = 0; outer < _options.OuterBags; outer++)\n         {\n             for (int inner = 0; inner < _options.InnerBags; inner++)\n             {\n                 for (int f = 0; f < _numFeatures; f++)\n                 {\n-                    var logOdds = ComputeLogOdds(x, sampleBins);\n-\n                     var binGradients = new double[_binEdges[f].Length + 1];\n                     var binHessians = new double[_binEdges[f].Length + 1];\n\n                     for (int i = 0; i < n; i++)\n                     {\n                         double prob = Sigmoid(logOdds[i]);\n                         double grad = yBinary[i] - prob;\n                         double hess = prob * (1 - prob);\n\n                         int bin = sampleBins[f][i];\n                         binGradients[bin] += grad;\n                         binHessians[bin] += hess;\n                     }\n\n                     for (int b = 0; b <= _binEdges[f].Length; b++)\n                     {\n                         if (binHessians[b] > 1e-10)\n                         {\n                             double update = (binGradients[b] / (binHessians[b] + _options.L2Regularization))\n                                            * _options.LearningRate;\n+                            // Incrementally update running logOdds for affected samples\n+                            _shapeFunctions[f][b] = NumOps.Add(_shapeFunctions[f][b], NumOps.FromDouble(update));\n                         }\n                     }\n+\n+                    // Update running log-odds for this feature's changes\n+                    for (int i = 0; i < n; i++)\n+                    {\n+                        int bin = sampleBins[f][i];\n+                        // Re-derive logOdds[i] contribution from the update just applied\n+                        // (or track deltas per bin and apply)\n+                    }\n                 }\n             }\n```\n\nThe exact incremental bookkeeping depends on design preference, but the key is to avoid the O(N √ó F) recomputation on every feature step.\n</details>\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/Classification/Boosting/ExplainableBoostingClassifier.cs` around lines\n191 - 227, ComputeLogOdds is being called inside the innermost feature loop,\ncausing massive repeated O(N√óF) work; instead maintain and incrementally update\na running log-odds array so you only recompute per-sample changes for the bins\nthat actually changed when a feature's shape function (_shapeFunctions[f]) is\nupdated. Concretely: compute logOdds once before the outer/inner loops (or at\nstart of each bag), store it in a mutable array, then when applying the Newton\nupdate to bins for feature f use sampleBins[f] to compute per-bin delta\ncontributions and apply those deltas to logOdds for affected samples (use\nSigmoid only when computing gradients/hessians from current logOdds), so\nsubsequent features reuse the updated logOdds without calling ComputeLogOdds\nrepeatedly; update any code paths that rely on ComputeLogOdds to use the\nmaintained logOdds and ensure numerical stability by applying the same\nL2Regularization and LearningRate (_options.L2Regularization,\n_options.LearningRate) when computing per-bin deltas.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:phoenix -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85tASuJ","isResolved":true,"path":"src/Classification/Boosting/ExplainableBoostingClassifier.cs","line":360,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü° Minor_\n\n**`PredictProbabilities` is hard-coded for binary classification (columns 0 and 1).**\n\nLines 329‚Äì330 only populate two columns regardless of `NumClasses`. If `NumClasses > 2` (or is ever used in a multi-class context), the remaining columns will be default/zero, producing incorrect probabilities. Consider adding a guard or an explicit check that `NumClasses == 2`.\n\n\n\n<details>\n<summary>üõ°Ô∏è Proposed guard</summary>\n\n```diff\n     public override Matrix<T> PredictProbabilities(Matrix<T> input)\n     {\n+        if (NumClasses != 2)\n+        {\n+            throw new NotSupportedException(\"ExplainableBoostingClassifier currently supports binary classification only.\");\n+        }\n+\n         int n = input.Rows;\n         var probs = new Matrix<T>(n, NumClasses);\n```\n</details>\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/Classification/Boosting/ExplainableBoostingClassifier.cs` around lines\n295 - 334, PredictProbabilities currently assumes binary classification by only\npopulating probs[:,0] and probs[:,1]; add an explicit guard at the start of\nPredictProbabilities to ensure NumClasses == 2 (or throw a clear exception) so\nmulti-class usage is rejected rather than producing invalid zero columns;\nreference the method PredictProbabilities and the probs matrix and the later\nassignments to probs[i,0] and probs[i,1] when adding this check, leaving the\nexisting logOdds/Sigmoid/_intercept/_shapeFunctions/_interactionTerms logic\nunchanged.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:phoenix -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85tASuL","isResolved":true,"path":"src/Classification/Boosting/ExplainableBoostingClassifier.cs","line":null,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü° Minor_\n\n**`ExplainPrediction` omits interaction term contributions.**\n\nThe method only returns main-effect contributions. For a model trained with interactions, the explanation will not sum to the actual prediction, which is misleading for interpretability ‚Äî the primary selling point of EBM.\n\nConsider adding interaction contributions (keyed e.g. as negative or tuple-encoded indices) or at minimum documenting this limitation.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/Classification/Boosting/ExplainableBoostingClassifier.cs` around lines\n341 - 352, ExplainPrediction currently only sums main-effect contributions from\n_shapeFunctions using GetBinIndex and therefore omits any interaction\ncontributions; modify ExplainPrediction to also iterate over the model's\ninteraction storage (e.g., fields like _interactions, _interactionFunctions or\nwhatever holds pairwise terms), compute the bin(s) for each interacting feature\nusing GetBinIndex, lookup the interaction contribution from the corresponding\ninteraction shape/function table, and add those contributions into the returned\nDictionary (use a consistent composite key strategy such as Tuple<int,int> keys\nor a negative/encoded int key format and document that choice). If adding\ninteraction entries is infeasible, update the method to clearly document that\ninteractions are omitted in its returned contributions. Ensure you reference\nExplainPrediction, _shapeFunctions, GetBinIndex and the interaction storage name\nyou find in the file when making the change.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:phoenix -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85tASuM","isResolved":true,"path":"src/Classification/Boosting/ExplainableBoostingClassifier.cs","line":432,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü° Minor_\n\n**Quantile binning can produce off-by-one or empty bin-edge arrays.**\n\nLine 373 computes `numBins = Math.Min(MaxBins, values.Count - 1)`. The loop on line 381 then produces `numBins - 1` edges (since `b` runs from 1 to `numBins - 1`). After `Distinct()` on line 387, the actual number of edges may be much smaller than expected. Meanwhile, the shape function is sized to `_binEdges[f].Length + 1` (line 166). This is internally consistent, but when `values.Count` is 2 (two unique values), `numBins = 1`, the loop body never executes (`b` starts at 1, `b < 1` is false), so `edges` is empty and the feature gets a single bin with no discriminating power. A feature with two distinct values should ideally have one edge and two bins.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/Classification/Boosting/ExplainableBoostingClassifier.cs` around lines\n357 - 389, CreateBins can produce zero edges for features with two unique values\nbecause numBins becomes 1 and the loop never adds edges; update CreateBins to\nguarantee at least one bin edge when values.Count >= 2: compute numBins =\nMath.Min(_options.MaxBins, values.Count - 1) as before but if values.Count >= 2\nand numBins == 1, explicitly add a single edge (for example the midpoint between\nthe two unique values using NumOps.FromDouble((values[0]+values[1])/2) or\nsimilar) so _binEdges[f] is non-empty; ensure you still call\nDistinct().ToArray() on edges and keep references to _binEdges,\n_options.MaxBins, NumOps.ToDouble and NumOps.FromDouble consistent.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:phoenix -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85tASuN","isResolved":true,"path":"src/Classification/Calibration/CalibratedClassifier.cs","line":null,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**PAVA implementation has incorrect weight tracking, producing wrong isotonic regression results.**\n\nWhen adjacent elements are merged, both indices receive the combined weight. Subsequent merges involving either index then double-count the original weight, skewing averages. For example, values `[0.6, 0.3, 0.7, 0.1]` should yield `[0.425, 0.425, 0.425, 0.425]`, but the inflated weights cause cascading inaccuracies.\n\nA correct PAVA tracks block boundaries so that a merged pool is treated as a single entity. The standard single-pass algorithm is also O(n) instead of the current O(n¬≤+) multi-pass approach.\n\n\n\n<details>\n<summary>Proposed fix ‚Äî standard single-pass PAVA with block tracking</summary>\n\n```diff\n-        // Pool Adjacent Violators Algorithm (PAVA)\n-        var calibrated = new double[n];\n-        var weights = new int[n];\n-        for (int i = 0; i < n; i++)\n-        {\n-            calibrated[i] = indexed[i].target;\n-            weights[i] = 1;\n-        }\n-\n-        bool changed = true;\n-        while (changed)\n-        {\n-            changed = false;\n-            for (int i = 0; i < n - 1; i++)\n-            {\n-                if (calibrated[i] > calibrated[i + 1])\n-                {\n-                    // Merge pools\n-                    double newValue = (calibrated[i] * weights[i] + calibrated[i + 1] * weights[i + 1])\n-                                     / (weights[i] + weights[i + 1]);\n-                    int newWeight = weights[i] + weights[i + 1];\n-\n-                    calibrated[i] = newValue;\n-                    calibrated[i + 1] = newValue;\n-                    weights[i] = newWeight;\n-                    weights[i + 1] = newWeight;\n-                    changed = true;\n-                }\n-            }\n-        }\n+        // Pool Adjacent Violators Algorithm (PAVA) ‚Äî standard O(n) single-pass\n+        var blockValues = new double[n];\n+        var blockWeights = new double[n];\n+        var blockSizes = new int[n]; // number of original elements in each block\n+        int numBlocks = 0;\n+\n+        for (int i = 0; i < n; i++)\n+        {\n+            // Add new block for element i\n+            blockValues[numBlocks] = indexed[i].target;\n+            blockWeights[numBlocks] = 1.0;\n+            blockSizes[numBlocks] = 1;\n+            numBlocks++;\n+\n+            // Merge backwards while monotonicity is violated\n+            while (numBlocks > 1 && blockValues[numBlocks - 2] > blockValues[numBlocks - 1])\n+            {\n+                double totalWeight = blockWeights[numBlocks - 2] + blockWeights[numBlocks - 1];\n+                blockValues[numBlocks - 2] = (blockValues[numBlocks - 2] * blockWeights[numBlocks - 2]\n+                                             + blockValues[numBlocks - 1] * blockWeights[numBlocks - 1])\n+                                             / totalWeight;\n+                blockWeights[numBlocks - 2] = totalWeight;\n+                blockSizes[numBlocks - 2] += blockSizes[numBlocks - 1];\n+                numBlocks--;\n+            }\n+        }\n+\n+        // Expand blocks back to per-element calibrated values\n+        var calibrated = new double[n];\n+        int pos = 0;\n+        for (int b = 0; b < numBlocks; b++)\n+        {\n+            for (int j = 0; j < blockSizes[b]; j++)\n+            {\n+                calibrated[pos++] = blockValues[b];\n+            }\n+        }\n```\n</details>\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/Classification/Calibration/CalibratedClassifier.cs` around lines 440 -\n469, The current multi-pass PAVA in CalibratedClassifier.cs incorrectly updates\nboth entries' weights (weights[] and calibrated[]) on a merge causing\ndouble-counting; replace it with the standard single-pass PAVA that tracks\nblocks (e.g., maintain a stack or parallel arrays of blockValue and blockWeight\nor blockStart/blockEnd) and merges whole blocks when blockValue[i] >\nblockValue[i+1], updating the merged block's weight and value once and only\ntreating the merged block as a single entity for future comparisons; finally,\nwrite the blockValue back to calibrated[] for the entire block range (using\nindexed[] for targets) so the output is correct and O(n) instead of the current\nmulti-pass loop over calibrated[], weights[], indexed, and n.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:phoenix -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85tASuP","isResolved":true,"path":"src/Classification/Calibration/CalibratedClassifier.cs","line":null,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**Incorrect gradients in beta calibration: extra sigmoid derivative factor.**\n\nThe cross-entropy loss gradient through a sigmoid is the well-known identity `dL/dz = œÉ(z) - t`, which equals `error` directly ‚Äî the sigmoid derivative cancels. `FitPlattScaling` (lines 401‚Äì402) correctly uses `error * logit` and `error`, but here the gradients are multiplied by an additional `dCalib_dZ = œÉ(z)(1-œÉ(z))` factor.\n\nThis suppresses gradients when calibrated probabilities are near 0 or 1, leading to slow convergence and suboptimal parameters.\n\n\n\n<details>\n<summary>Proposed fix ‚Äî remove the extra sigmoid derivative</summary>\n\n```diff\n-                gradA += error * dCalib_dZ * logP;\n-                gradB += error * dCalib_dZ * (-log1mP);\n-                gradC += error * dCalib_dZ;\n+                gradA += error * logP;\n+                gradB += error * (-log1mP);\n+                gradC += error;\n```\n\nThe `dCalib_dZ` variable can then be removed entirely.\n</details>\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n                double error = calibrated - targets[i];\n                double dCalib_dZ = calibrated * (1 - calibrated);\n\n                gradA += error * logP;\n                gradB += error * (-log1mP);\n                gradC += error;\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/Classification/Calibration/CalibratedClassifier.cs` around lines 511 -\n516, The gradient update in CalibratedClassifier is wrongly multiplying by the\nsigmoid derivative dCalib_dZ; remove that factor so gradA, gradB, gradC use the\ndirect cross-entropy-through-sigmoid gradient (dL/dz = œÉ(z) - t). Concretely, in\nthe block computing gradA/gradB/gradC replace uses of \"error * dCalib_dZ * ...\"\nwith just \"error * ...\", mirroring the pattern used in FitPlattScaling (which\nuses error * logit and error). Delete the dCalib_dZ variable and adjust the\nthree updates (gradA, gradB, gradC) accordingly.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:phoenix -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85tASuR","isResolved":true,"path":"src/Classification/Calibration/CalibratedClassifier.cs","line":null,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü° Minor_\n\n**`PredictProbabilities` short-circuits for `None` but not `Auto`.**\n\n`Train` (line 127‚Äì128) treats both `None` and `Auto` as \"no calibration,\" but here only `None` returns early. When the method is `Auto`, the code allocates a new matrix and loops through `CalibrateProb` (which returns `p` unchanged). Add `Auto` to the early return for consistency and to avoid unnecessary work.\n\n\n\n<details>\n<summary>Proposed fix</summary>\n\n```diff\n-        if (_options.CalibrationMethod == ProbabilityCalibrationMethod.None)\n+        if (_options.CalibrationMethod == ProbabilityCalibrationMethod.None\n+            || _options.CalibrationMethod == ProbabilityCalibrationMethod.Auto)\n```\n</details>\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        if (_options.CalibrationMethod == ProbabilityCalibrationMethod.None\n            || _options.CalibrationMethod == ProbabilityCalibrationMethod.Auto)\n        {\n            // No calibration, return raw probabilities\n            return uncalibrated;\n        }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/Classification/Calibration/CalibratedClassifier.cs` around lines 587 -\n591, PredictProbabilities in CalibratedClassifier currently only short-circuits\nwhen _options.CalibrationMethod == ProbabilityCalibrationMethod.None, but Train\ntreats both None and Auto as \"no calibration\", causing unnecessary allocation\nand looping when Auto is set; update PredictProbabilities to also return the\nuncalibrated matrix early when _options.CalibrationMethod ==\nProbabilityCalibrationMethod.Auto so it matches Train's behavior and avoids\ncalling CalibrateProb and allocating the new matrix unnecessarily.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:phoenix -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85tASuU","isResolved":true,"path":"src/NeuralNetworks/Tabular/AutoIntBase.cs","line":218,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü° Minor_\n\n<!-- [scratchpad_start]\n[task review_file_1/1 review_segment_3/8]\nReview EmbedFeatures (lines 160-205).\n[observations]\n- Numerical embedding: multiplies each scalar feature value by its per-feature embedding vector.\n- Categorical embedding: lookup by index into per-feature embedding table.\n- No bounds check on `catIdx = categoricalIndices[b, f]` against `Options.CategoricalCardinalities[f]`.\n[analysis]\n- If a categorical index exceeds cardinality, `_categoricalEmbeddings[f][embIdx]` silently reads garbage or throws a cryptic IndexOutOfRangeException. A descriptive guard would improve debuggability.\n- Given this is a protected method called from the backbone (and ultimately from subclasses), invalid indices could easily surface during development/data prep.\n- Severity: minor ‚Äî the failure mode is a runtime exception anyway, but the error message would be unclear.\n[done]\n[scratchpad_end] -->\n\n**Missing bounds validation on categorical indices.**\n\n`catIdx` on line 191 is used directly to compute an index into the embedding table without checking that it falls within `[0, cardinality)`. An out-of-range index would produce a cryptic `IndexOutOfRangeException`. A descriptive guard would make data-prep errors much easier to diagnose.\n\n\n\n<details>\n<summary>üõ°Ô∏è Suggested guard</summary>\n\n```diff\n                 for (int f = 0; f < NumCategoricalFeatures; f++)\n                 {\n                     int catIdx = categoricalIndices[b, f];\n+                    int cardinality = Options.CategoricalCardinalities![f];\n+                    if ((uint)catIdx >= (uint)cardinality)\n+                    {\n+                        throw new ArgumentOutOfRangeException(\n+                            nameof(categoricalIndices),\n+                            $\"Categorical index {catIdx} for feature {f} (batch {b}) is out of range [0, {cardinality}).\");\n+                    }\n                     int featureIdx = NumNumericalFeatures + f;\n```\n\n</details>\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/NeuralNetworks/Tabular/AutoIntBase.cs` around lines 160 - 205,\nEmbedFeatures uses catIdx (from categoricalIndices) directly when indexing\n_categoricalEmbeddings[f] without bounds checks; add a guard inside the\ncategorical feature loop (in EmbedFeatures, before computing embIdx/outIdx) that\nverifies 0 <= catIdx < Options.CategoricalCardinalities[f] (and that\nOptions.CategoricalCardinalities and _categoricalEmbeddings[f] are\nnon-null/consistent), and if the check fails throw an informative exception\n(e.g., ArgumentOutOfRangeException) that includes batch index b, feature index\nf, catIdx value and the expected cardinality to make errors debuggable.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:phoenix -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85tASuW","isResolved":true,"path":"src/NeuralNetworks/Tabular/ContrastivePretraining.cs","line":149,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü° Minor_\n\n**`batchSize == 1` silently produces no actual corruption but records indices as corrupted.**\n\nWhen `batchSize == 1`, `_random.Next(1)` always returns 0 (== `b`), and the while-loop guard `batchSize > 1` prevents re-rolling. The feature value is \"swapped\" with itself, so the corrupted tensor is identical to the original ‚Äî yet the index is still added to `_corruptedIndicesCache`. Downstream, `ComputeDenoisingLoss` computes MSE on features that weren't actually changed, and `ComputeContrastiveLoss` receives identical embeddings, making the contrastive signal degenerate.\n\nConsider either throwing for `batchSize < 2` or skipping the swap when `otherSample == b`:\n\n<details>\n<summary>Proposed guard</summary>\n\n```diff\n     public Tensor<T> CorruptSamples(Tensor<T> input)\n     {\n         int batchSize = input.Shape[0];\n+        if (batchSize < 2)\n+        {\n+            throw new ArgumentException(\"Contrastive pretraining requires batchSize >= 2.\", nameof(input));\n+        }\n         int numFeatures = input.Shape[1];\n```\n</details>\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/NeuralNetworks/Tabular/ContrastivePretraining.cs` around lines 112 - 129,\nThe corruption loop in ContrastivePretraining (uses _random, _corruptionRate,\ncorrupted, corruptedIndices/_corruptedIndicesCache) can mark indices as\n\"corrupted\" when batchSize == 1 even though the value is swapped with itself;\nupdate the logic to avoid this: either throw an ArgumentException when batchSize\n< 2 at start of the routine, or when selecting otherSample inside the nested\nloops, detect otherSample == b and skip the corruption (do not assign\ncorrupted[...] or add to corruptedIndices) so you never record a no-op swap;\nensure ComputeDenoisingLoss and ComputeContrastiveLoss therefore only receive\ntruly altered features/embeddings.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:phoenix -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85tASuY","isResolved":true,"path":"src/NeuralNetworks/Tabular/ContrastivePretraining.cs","line":310,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Self-similarity (anchor ¬∑ anchor) must be excluded from the InfoNCE denominator.**\n\nThe denominator loop (Lines 260-263) sums over all `2 * batchSize` entries including `j == i`, which is `sim(original_i, original_i)`. For L2-normalized vectors this equals `1.0 / temperature` (= 10.0 at default temperature 0.1), creating a dominant false negative that suppresses the positive pair's contribution and corrupts gradients.\n\nStandard InfoNCE / NT-Xent implementations mask out the self-similarity term. The positive pair at `similarities[i, batchSize + i]` is correctly included in both numerator and denominator, but the self-pair `similarities[i, i]` should be skipped.\n\n<details>\n<summary>Proposed fix ‚Äî skip self-similarity in the denominator and max computation</summary>\n\n```diff\n             // Find max for numerical stability\n-            var maxSim = similarities[i, 0];\n-            for (int j = 1; j < batchSize * 2; j++)\n+            var maxSim = similarities[i, batchSize]; // start from corrupted block\n+            for (int j = 0; j < batchSize * 2; j++)\n             {\n-                if (NumOps.Compare(similarities[i, j], maxSim) > 0)\n+                if (j == i) continue; // skip self-similarity\n+                if (NumOps.Compare(similarities[i, j], maxSim) > 0)\n                     maxSim = similarities[i, j];\n             }\n \n             // Compute log-sum-exp\n             var sumExp = NumOps.Zero;\n             for (int j = 0; j < batchSize * 2; j++)\n             {\n+                if (j == i) continue; // skip self-similarity\n                 sumExp = NumOps.Add(sumExp, NumOps.Exp(NumOps.Subtract(similarities[i, j], maxSim)));\n             }\n```\n</details>\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/NeuralNetworks/Tabular/ContrastivePretraining.cs` around lines 246 - 273,\nThe denominator and max computation currently include the self-similarity term\nsimilarities[i, i], which must be excluded for InfoNCE/NT-Xent; update the loops\nin the loss computation inside ContrastivePretraining (the for-loops that\ncompute maxSim and sumExp over batchSize * 2) to skip the index j == i (the\nanchor¬∑anchor entry) while still including the positive pair at similarities[i,\nbatchSize + i]; ensure posScore calculation (posScore = similarities[i,\nbatchSize + i]) remains unchanged and the final totalLoss / batchSize logic is\npreserved.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:phoenix -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85tASuZ","isResolved":true,"path":"src/NeuralNetworks/Tabular/ContrastivePretraining.cs","line":373,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü° Minor_\n\n**`batchSize` parameter is not validated against the cached corruption indices.**\n\n`_corruptedIndicesCache` stores flat indices computed from the input tensor passed to `CorruptSamples`. If `GetCorruptionMask` is called with a different `batchSize`, the mask tensor may be smaller than the cached indices, causing an `IndexOutOfRangeException`, or larger, leaving stale positions unmarked.\n\nConsider either caching the batch size from `CorruptSamples` and validating here, or removing the parameter and deriving the shape from the cached state.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/NeuralNetworks/Tabular/ContrastivePretraining.cs` around lines 304 - 317,\nGetCorruptionMask currently uses the incoming batchSize without validating\nagainst previously cached flat indices (_corruptedIndicesCache) which were\nproduced by CorruptSamples; add a cached batch size field (e.g.,\n_cachedBatchSize set inside CorruptSamples) and in GetCorruptionMask verify\nbatchSize equals _cachedBatchSize (or if _corruptedIndicesCache is non-null,\nderive the mask shape from _cachedBatchSize and _numFeatures and ignore the\nparameter), and if there is a mismatch either throw an informative\nArgumentException or rebuild/clear the cache before creating the mask to avoid\nIndexOutOfRangeException or stale markings.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:phoenix -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85tINDq","isResolved":true,"path":"src/NeuralNetworks/Tabular/ColumnEmbedding.cs","line":138,"comments":{"nodes":[{"body":"`embDim` is taken from the input tensor, but the embedding lookup uses `_embeddingDim` (`embIdx = c * _embeddingDim + d`). If `featureEmbeddings.Shape[2] != _embeddingDim`, this will produce incorrect results and can index out of bounds on `_embeddings`. Add an explicit dimension check (and likely require exact match) before the loops, e.g. validate `embDim == _embeddingDim` and throw a clear exception if not."}]}},{"id":"PRRT_kwDOKSXUF85tINED","isResolved":true,"path":"src/NeuralNetworks/Tabular/ColumnEmbedding.cs","line":199,"comments":{"nodes":[{"body":"Similar to `AddColumnEmbeddings`, `embDim` comes from the gradient tensor while `_embeddingGradients` is indexed with `_embeddingDim`. If `embDim != _embeddingDim`, this can cause out-of-range writes into `_embeddingGradients` and corrupt training. Add a dimension check (require `gradient.Shape[2] == _embeddingDim`) before accumulating gradients."}]}},{"id":"PRRT_kwDOKSXUF85tINER","isResolved":true,"path":"src/NeuralNetworks/Tabular/CLSToken.cs","line":97,"comments":{"nodes":[{"body":"`PrependCLS` hardcodes `embDim = EmbeddingDimension` but never verifies that `embeddings.Shape[2] == EmbeddingDimension`. If the caller passes embeddings with a different last dimension, the indexing math will read/write at incorrect offsets. Add a shape check up front and throw an `ArgumentException` with expected vs actual embedding dimension."}]}},{"id":"PRRT_kwDOKSXUF85tINEf","isResolved":true,"path":"src/NeuralNetworks/Tabular/AutoIntRegression.cs","line":171,"comments":{"nodes":[{"body":"`targets[i]` is indexed assuming targets has the same flattened length as `_predictionsCache`, but this method doesn‚Äôt validate `targets.Length` (or shape) before indexing. If a caller passes mismatched targets, this will throw an out-of-range exception (or silently compute wrong gradients if shapes differ but lengths match unexpectedly). Add an explicit check that `targets.Length == _predictionsCache.Length` (and ideally shape compatibility) before the loop."}]}},{"id":"PRRT_kwDOKSXUF85tINEs","isResolved":true,"path":"src/NeuralNetworks/Tabular/AutoIntNetwork.cs","line":null,"comments":{"nodes":[{"body":"Backprop uses `prediction.Subtract(expectedOutput)` as the gradient signal regardless of the configured `_lossFunction`. That‚Äôs only the correct derivative for MSE-like losses; for cross-entropy, Huber, etc., the gradient is different. This will produce incorrect training dynamics whenever `_lossFunction` is not MSE. Use the loss function‚Äôs derivative/gradient API (if available) to compute `dL/dPrediction`, and backpropagate that instead of a raw subtraction.\n```suggestion\n        Tensor<T> error = _lossFunction.CalculateGradient(prediction, expectedOutput);\n```"}]}},{"id":"PRRT_kwDOKSXUF85tINFF","isResolved":true,"path":"src/NeuralNetworks/Tabular/AutoIntNetwork.cs","line":234,"comments":{"nodes":[{"body":"`SerializeNetworkSpecificData` writes multiple `_options` fields, but `DeserializeNetworkSpecificData` is empty. Any model loaded from bytes will lose these option values (and may not rebuild its layers consistently), breaking round-trip serialization. Implement deserialization to read back fields in the same order, populate `_options`, and (if needed) reinitialize layers to match the deserialized configuration.\n```suggestion\n    {\n        // Read values in the exact same order they were written in SerializeNetworkSpecificData\n        var embeddingDimension = reader.ReadInt32();\n        var numLayers = reader.ReadInt32();\n        var numHeads = reader.ReadInt32();\n        var attentionDimension = reader.ReadInt32();\n        var dropoutRate = reader.ReadDouble();\n        var useResidual = reader.ReadBoolean();\n        var useLayerNorm = reader.ReadBoolean();\n        var embeddingInitScale = reader.ReadDouble();\n\n        var mlpLength = reader.ReadInt32();\n        var mlpHiddenDimensions = new int[mlpLength];\n        for (var i = 0; i < mlpLength; i++)\n        {\n            mlpHiddenDimensions[i] = reader.ReadInt32();\n        }\n\n        var categoricalLength = reader.ReadInt32();\n        int[]? categoricalCardinalities = null;\n        if (categoricalLength > 0)\n        {\n            categoricalCardinalities = new int[categoricalLength];\n            for (var i = 0; i < categoricalLength; i++)\n            {\n                categoricalCardinalities[i] = reader.ReadInt32();\n            }\n        }\n\n        // Populate the options object so that the deserialized model configuration\n        // matches the serialized one.\n        _options.EmbeddingDimension = embeddingDimension;\n        _options.NumLayers = numLayers;\n        _options.NumHeads = numHeads;\n        _options.AttentionDimension = attentionDimension;\n        _options.DropoutRate = dropoutRate;\n        _options.UseResidual = useResidual;\n        _options.UseLayerNorm = useLayerNorm;\n        _options.EmbeddingInitScale = embeddingInitScale;\n        _options.MLPHiddenDimensions = mlpHiddenDimensions;\n        _options.CategoricalCardinalities = categoricalCardinalities;\n```"}]}},{"id":"PRRT_kwDOKSXUF85tINFb","isResolved":true,"path":"src/LinkFunctions/LinkFunctionFactory.cs","line":66,"comments":{"nodes":[{"body":"The canonical link for `InverseGaussian` is typically \\( g(\\mu)=1/\\mu^2 \\), but the factory currently returns `LinkFunctionType.Inverse` (i.e., `1/Œº`). This is mathematically incorrect and can lead to wrong GLM behavior when callers request a canonical inverse-Gaussian model. Add a dedicated link implementation for `1/Œº¬≤` (or a power link) and return that for `DistributionFamily.InverseGaussian`. If you intentionally don‚Äôt support it, the enum docs should be updated to avoid claiming a canonical link you don‚Äôt provide."}]}},{"id":"PRRT_kwDOKSXUF85tINFq","isResolved":true,"path":"src/Distributions/WeibullDistribution.cs","line":150,"comments":{"nodes":[{"body":"`LogPdf` returns `-Infinity` for `x == 0`, but `Pdf` has explicit special handling at `x == 0` (including returning `+Infinity` when `shape < 1`, and a finite value when `shape == 1`). These should be consistent: `LogPdf(0)` should match `log(Pdf(0))` for the supported cases. Add a `x == 0` branch mirroring the `Pdf` logic (e.g., `shape < 1 => +Infinity`, `shape == 1 => log(shape/scale)`, `shape > 1 => -Infinity`)."}]}},{"id":"PRRT_kwDOKSXUF85tINGD","isResolved":true,"path":"src/Distributions/PoissonDistribution.cs","line":120,"comments":{"nodes":[{"body":"The comment indicates the Poisson CDF uses the *upper* regularized incomplete gamma `Q(k+1, Œª)`, but the implementation `GammaUpperIncomplete` computes `exp(-x) * Œ£_{i=0}^{a-1} x^i / i!`, which corresponds to the Poisson CDF (i.e., the *lower* regularized gamma relationship). Consider correcting the comment (and ideally renaming `GammaUpperIncomplete` to avoid misleading future maintenance).\n```suggestion\n        // CDF is computed via the Poisson CDF series (equivalently, the lower regularized incomplete gamma P(k+1, Œª)).\n```"}]}},{"id":"PRRT_kwDOKSXUF85tITOm","isResolved":true,"path":"src/Classification/Boosting/DARTClassifier.cs","line":84,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**Double-instantiation of default options when `options` is `null`.**\n\nWhen `options` is `null`, `options ?? new DARTClassifierOptions<T>()` is evaluated twice ‚Äî once for the `base(‚Ä¶)` call and once for `_options`. This creates two independent option objects. The base class receives one instance and `_options` holds another. This divergence becomes a concrete bug in `Deserialize` (lines 482‚Äì485), which mutates `_options` but leaves the base's copy stale.\n\n\n<details>\n<summary>üêõ Proposed fix: resolve the options instance once</summary>\n\n```diff\n public DARTClassifier(DARTClassifierOptions<T>? options = null,\n     IRegularization<T, Matrix<T>, Vector<T>>? regularization = null)\n-    : base(options ?? new DARTClassifierOptions<T>(), regularization, new CrossEntropyLoss<T>())\n+    : base((options ??= new DARTClassifierOptions<T>()), regularization, new CrossEntropyLoss<T>())\n {\n-    _options = options ?? new DARTClassifierOptions<T>();\n+    _options = options;\n     _trees = [];\n     _treeWeights = [];\n     _initPrediction = NumOps.Zero;\n     _random = RandomHelper.CreateSecureRandom();\n }\n```\n\nAlternatively, use a local variable or a static helper to ensure a single instance is shared.\n</details>\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/Classification/Boosting/DARTClassifier.cs` around lines 75 - 84, The\nconstructor double-creates default options when options is null; fix by\nresolving a single options instance upfront (e.g., var resolvedOptions = options\n?? new DARTClassifierOptions<T>()) and then pass resolvedOptions to the\nbase(...) call and assign _options = resolvedOptions so both the base class and\nthis class share the same DARTClassifierOptions<T> instance (affects the\nDARTClassifier constructor and ensures Deserialize mutations to _options reflect\nin the base).\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:phoenix -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85tITOr","isResolved":true,"path":"src/Classification/Boosting/DARTClassifier.cs","line":null,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü° Minor_\n\n**`numToDrop` is not clamped to the tree count ‚Äî can crash if `DropoutRate ‚â• 1`.**\n\n`SelectNumDropout` can return a value exceeding `_trees.Count` (e.g., `Uniform` with `DropoutRate > 1`). The sampling loop on line 162 will then call `_random.Next(0)` once `allIndices` is exhausted, throwing `ArgumentOutOfRangeException`.\n\n\n<details>\n<summary>üõ°Ô∏è Proposed fix: clamp numToDrop</summary>\n\n```diff\n-                int numToDrop = SelectNumDropout(_trees.Count);\n+                int numToDrop = Math.Min(SelectNumDropout(_trees.Count), _trees.Count);\n```\n</details>\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/Classification/Boosting/DARTClassifier.cs` around lines 155 - 165,\nSelectNumDropout can return a value larger than the number of trees which causes\nthe sampling loop in the DARTClassifier to call _random.Next(0) once allIndices\nis exhausted; clamp numToDrop to the current tree count before sampling (e.g.,\nset numToDrop = Math.Min(numToDrop, _trees.Count) and ensure it's non-negative)\nso the for-loop that uses allIndices, droppedIndices, and _random never attempts\nto sample from an empty list; update the logic around SelectNumDropout/numToDrop\nin the method that builds droppedIndices to use the clamped value.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:phoenix -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85tITOx","isResolved":true,"path":"src/Classification/Boosting/ExplainableBoostingClassifier.cs","line":145,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**`_random` is dead code ‚Äî outer/inner \"bags\" perform no random sub-sampling.**\n\n`_random` is initialised (lines 141-143) but never read. The outer/inner bag loops (lines 202-247) iterate over **all** samples every pass ‚Äî no bootstrap sampling, no feature sub-sampling, no data partitioning. In the InterpretML EBM implementation, each outer bag trains on a different bootstrap sample and the results are averaged, which is the key source of variance reduction and regularization.\n\nCurrently `OuterBags √ó InnerBags` just controls the total number of cyclic gradient passes, making the two parameters semantically identical to a single \"number of rounds\" setting, and leaving the model without the intended bagging benefit.\n\n\n\n\n\nAlso applies to: 202-247\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/Classification/Boosting/ExplainableBoostingClassifier.cs` around lines\n141 - 143, The _random field is unused because the outer/inner bag loops in\nExplainableBoostingClassifier never perform bootstrap sampling or feature\nsubsampling; update the training loop that iterates OuterBags and InnerBags to\ncreate a bootstrap sample for each outer bag using the existing\nRandomHelper.CreateSeededRandom(_options.Seed) (or CreateSecureRandom) and draw\nsample indices with replacement, then train the inner passes on that sampled\ndataset (and optionally apply feature sub-sampling per outer bag if you want\nfull EBM parity); ensure _random is referenced where you generate the sampled\nindices so OuterBags produce different bootstrap datasets and the model\naggregates/averages results across outer bags rather than just repeating\nfull-data gradient passes.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:phoenix -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85tITO0","isResolved":true,"path":"src/Classification/Calibration/CalibratedClassifier.cs","line":null,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü° Minor_\n\n**Unreachable multiclass calibration branch.**\n\n`FitCalibration` throws `NotSupportedException` when `NumClasses != 2`, so this `else` branch can never be reached through normal training. The code reads as if multiclass per-class calibration is supported‚Äîconsider adding a defensive `throw` or a `// TODO: not yet reachable` comment to avoid confusing future maintainers.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/Classification/Calibration/CalibratedClassifier.cs` around lines 645 -\n673, The multiclass branch in the calibration loop (inside the method using\nNumClasses, CalibrateProb and writing into calibrated[i, c]) is unreachable\nbecause FitCalibration throws NotSupportedException when NumClasses != 2; update\nthe code to make this explicit: either replace the entire else branch with a\ndefensive throw (e.g., throw new NotSupportedException(...) referencing\nFitCalibration/NumClasses) or add a clear TODO/comment above the block stating\nit is currently unreachable and will be implemented later, ensuring the change\nreferences NumClasses, CalibrateProb and the calibrated array so future readers\ncan locate the logic.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:phoenix -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85tITO3","isResolved":true,"path":"src/Classification/Calibration/CalibratedClassifier.cs","line":851,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü° Minor_\n\n**Metadata omits beta calibration parameters.**\n\n`GetModelMetadata` includes Platt A/B and Temperature but not `_betaA`, `_betaB`, `_betaC`, making diagnostics incomplete when beta calibration is used.\n\n\n<details>\n<summary>‚ôªÔ∏è Proposed fix</summary>\n\n```diff\n     metadata.AdditionalInfo[\"PlattA\"] = _plattA;\n     metadata.AdditionalInfo[\"PlattB\"] = _plattB;\n+    metadata.AdditionalInfo[\"BetaA\"] = _betaA;\n+    metadata.AdditionalInfo[\"BetaB\"] = _betaB;\n+    metadata.AdditionalInfo[\"BetaC\"] = _betaC;\n     metadata.AdditionalInfo[\"Temperature\"] = _temperature;\n```\n</details>\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    public override ModelMetadata<T> GetModelMetadata()\n    {\n        var metadata = base.GetModelMetadata();\n        metadata.AdditionalInfo[\"CalibrationMethod\"] = _options.CalibrationMethod.ToString();\n        metadata.AdditionalInfo[\"CrossValidationFolds\"] = _options.CrossValidationFolds;\n        metadata.AdditionalInfo[\"PlattA\"] = _plattA;\n        metadata.AdditionalInfo[\"PlattB\"] = _plattB;\n        metadata.AdditionalInfo[\"BetaA\"] = _betaA;\n        metadata.AdditionalInfo[\"BetaB\"] = _betaB;\n        metadata.AdditionalInfo[\"BetaC\"] = _betaC;\n        metadata.AdditionalInfo[\"Temperature\"] = _temperature;\n        return metadata;\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/Classification/Calibration/CalibratedClassifier.cs` around lines 860 -\n869, GetModelMetadata currently records Platt (_plattA/_plattB) and Temperature\nbut omits the beta calibration parameters, so add entries for _betaA, _betaB and\n_betaC to metadata.AdditionalInfo inside the GetModelMetadata() override (use\nclear keys such as \"BetaA\", \"BetaB\", \"BetaC\") so beta calibration diagnostics\nare preserved; update the GetModelMetadata method to include these three fields\nalongside the existing Platt/Temperature entries.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:phoenix -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85tITPD","isResolved":true,"path":"src/NeuralNetworks/Tabular/AutoIntBase.cs","line":107,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü° Minor_\n\n**Constructor allows `numNumericalFeatures < 0` without validation.**\n\nA negative `numNumericalFeatures` would pass the `TotalFeatures == 0` check (when combined with categorical features) but produce nonsensical behavior. A simple guard would prevent misuse.\n\n\n\n<details>\n<summary>üõ°Ô∏è Suggested guard</summary>\n\n```diff\n     protected AutoIntBase(int numNumericalFeatures, AutoIntOptions<T>? options = null)\n     {\n         Options = options ?? new AutoIntOptions<T>();\n+        if (numNumericalFeatures < 0)\n+        {\n+            throw new ArgumentOutOfRangeException(nameof(numNumericalFeatures), \"Must be non-negative.\");\n+        }\n         NumNumericalFeatures = numNumericalFeatures;\n```\n\n</details>\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    protected AutoIntBase(int numNumericalFeatures, AutoIntOptions<T>? options = null)\n    {\n        Options = options ?? new AutoIntOptions<T>();\n        if (numNumericalFeatures < 0)\n        {\n            throw new ArgumentOutOfRangeException(nameof(numNumericalFeatures), \"Must be non-negative.\");\n        }\n        NumNumericalFeatures = numNumericalFeatures;\n        NumCategoricalFeatures = Options.CategoricalCardinalities?.Length ?? 0;\n        TotalFeatures = NumNumericalFeatures + NumCategoricalFeatures;\n\n        if (TotalFeatures == 0)\n        {\n            throw new ArgumentException(\"Model must have at least one feature\");\n        }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/NeuralNetworks/Tabular/AutoIntBase.cs` around lines 91 - 101, The\nconstructor AutoIntBase(int numNumericalFeatures, AutoIntOptions<T>? options =\nnull) must validate that numNumericalFeatures is non‚Äënegative; add an early\nguard that checks numNumericalFeatures >= 0 and throws an\nArgumentOutOfRangeException (include nameof(numNumericalFeatures) and a clear\nmessage) before assigning NumNumericalFeatures/TotalFeatures so negative values\ncannot produce invalid state (adjust any callers/tests if needed).\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:phoenix -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85tITPI","isResolved":true,"path":"src/NeuralNetworks/Tabular/AutoIntBase.cs","line":436,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü° Minor_\n\n**Embedding and layer updates use vanilla SGD with no gradient scaling.**\n\n`UpdateParameters` applies raw `grad * lr` without dividing by batch size. If `AccumulateEmbeddingGradients` sums gradients across the batch (which it does, lines 338‚Äì352), the effective step size scales linearly with batch size. Typically, averaged gradients (`sum / batchSize`) are used to keep the learning rate independent of batch size. Please verify this is the intended convention ‚Äî if callers are expected to pre-scale `learningRate`, document that contract.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/NeuralNetworks/Tabular/AutoIntBase.cs` around lines 388 - 424, The\nUpdateParameters method in AutoIntBase.UpdateParameters applies raw\ngrad*learningRate (vanilla SGD) but AccumulateEmbeddingGradients sums\nper-example gradients, causing step size to scale with batch size; modify\nUpdateParameters to average gradients before applying updates (divide numerical\nand categorical embedding grads and any layer grads by the batch size used in\nAccumulateEmbeddingGradients) or alternatively document an explicit contract\nthat callers must pass a learningRate already scaled by 1/batchSize; update\nAutoIntBase.UpdateParameters (and interacting/MLP layer UpdateParameters calls\nif they accept raw grads) to accept a batchSize parameter or perform the\ndivision using a stored batch size so all members (_numericalEmbeddingsGrad,\n_categoricalEmbeddingsGrad, and layer gradients via layer.UpdateParameters) use\naveraged gradients.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:phoenix -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85tITPL","isResolved":true,"path":"src/NeuralNetworks/Tabular/ContrastivePretraining.cs","line":85,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**Missing parameter validation ‚Äî `temperature = 0` causes division by zero.**\n\n`ComputeInfoNCELoss` computes `1.0 / _temperature` (line 219). If the caller passes `temperature: 0`, this silently produces `Infinity`, corrupting the entire loss computation. `corruptionRate` outside `[0, 1]` and non-positive dimensions are also worth guarding.\n\n<details>\n<summary>Proposed guard</summary>\n\n```diff\n     public ContrastivePretraining(\n         int embeddingDim,\n         int numFeatures,\n         int projectionDim = 128,\n         double corruptionRate = 0.3,\n         double temperature = 0.1)\n     {\n+        ArgumentOutOfRangeException.ThrowIfNegativeOrZero(embeddingDim);\n+        ArgumentOutOfRangeException.ThrowIfNegativeOrZero(numFeatures);\n+        ArgumentOutOfRangeException.ThrowIfNegativeOrZero(projectionDim);\n+        ArgumentOutOfRangeException.ThrowIfNegativeOrZero(temperature);\n+        if (corruptionRate < 0 || corruptionRate > 1)\n+            throw new ArgumentOutOfRangeException(nameof(corruptionRate), \"Must be in [0, 1].\");\n+\n         _numFeatures = numFeatures;\n```\n</details>\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/NeuralNetworks/Tabular/ContrastivePretraining.cs` around lines 57 - 75,\nAdd input validation to the ContrastivePretraining constructor: validate\nembeddingDim and projectionDim are positive integers, ensure corruptionRate is\nbetween 0.0 and 1.0 (inclusive) and temperature is > 0.0 (non-zero), and throw\nappropriate ArgumentOutOfRangeException/ArgumentException with clear messages if\nany check fails; this protects ComputeInfoNCELoss (which uses _temperature) from\ndivision-by-zero and ensures _projectionDim, _numFeatures and related tensors\nare created only with valid dimensions.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:phoenix -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85tITPP","isResolved":true,"path":"src/NeuralNetworks/Tabular/ContrastivePretraining.cs","line":153,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**`input.Shape[1]` vs `_numFeatures` mismatch silently corrupts downstream masks and denoising loss.**\n\n`CorruptSamples` computes flat indices using the local `numFeatures` from `input.Shape[1]` (line 100), but `GetCorruptionMask` builds the mask using `_numFeatures` (line 313). If the input tensor's feature dimension differs from the constructor's `numFeatures`, the cached flat indices are computed with a different stride, so the mask and denoising loss will reference wrong positions. Add a shape assertion or use `_numFeatures` consistently.\n\n<details>\n<summary>Proposed fix</summary>\n\n```diff\n     public Tensor<T> CorruptSamples(Tensor<T> input)\n     {\n         int batchSize = input.Shape[0];\n         int numFeatures = input.Shape[1];\n+        if (numFeatures != _numFeatures)\n+        {\n+            throw new ArgumentException(\n+                $\"Input feature dimension {numFeatures} does not match expected {_numFeatures}.\",\n+                nameof(input));\n+        }\n```\n</details>\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/NeuralNetworks/Tabular/ContrastivePretraining.cs` around lines 97 - 137,\nCorruptSamples computes flat indices using input.Shape[1] while\nGetCorruptionMask expects strides based on the instance field _numFeatures,\ncausing mismatched indices; update CorruptSamples (and any places that construct\nthe corrupted tensor) to either assert input.Shape[1] == _numFeatures at the top\nor simply use _numFeatures consistently for numFeatures/stride calculation,\nensure the created Tensor uses the correct shape, and set _corruptedIndicesCache\nto an empty int array via Array.Empty<int>() when batchSize <= 1; reference\nCorruptSamples, GetCorruptionMask, _numFeatures, input.Shape, and\n_corruptedIndicesCache when making the change.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:phoenix -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85tJLzJ","isResolved":true,"path":"src/Classification/Boosting/ExplainableBoostingClassifier.cs","line":119,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü° Minor_\n\n**Double-instantiation of default options when `options` is `null` ‚Äî same bug that was fixed in DARTClassifier.**\n\nLines 108 and 110 both evaluate `options ?? new ExplainableBoostingClassifierOptions<T>()` independently, creating two separate instances when `options` is null. The base class receives one, and `_options` holds another. Apply the same `??=` pattern used in `DARTClassifier` (line 77 of that file).\n\n\n\n<details>\n<summary>Proposed fix: use compound assignment to resolve once</summary>\n\n```diff\n public ExplainableBoostingClassifier(\n     ExplainableBoostingClassifierOptions<T>? options = null,\n     IRegularization<T, Matrix<T>, Vector<T>>? regularization = null)\n-    : base(options ?? new ExplainableBoostingClassifierOptions<T>(), regularization, new CrossEntropyLoss<T>())\n+    : base(options ??= new ExplainableBoostingClassifierOptions<T>(), regularization, new CrossEntropyLoss<T>())\n {\n-    _options = options ?? new ExplainableBoostingClassifierOptions<T>();\n+    _options = options;\n     _shapeFunctions = [];\n```\n</details>\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/Classification/Boosting/ExplainableBoostingClassifier.cs` around lines\n105 - 119, The constructor double-creates default options when options is null;\nchange the base call to use the compound assignment so the parameter is set once\n(e.g. use options ??= new ExplainableBoostingClassifierOptions<T>() in the\nconstructor initializer) and then assign _options = options (or options!) in the\nbody so both base and _options reference the same instance; update the\nExplainableBoostingClassifier constructor signature/initializer accordingly and\nremove the duplicate new ExplainableBoostingClassifierOptions<T>() usage.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:phoenix -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85tJLzR","isResolved":true,"path":"src/Classification/Calibration/CalibratedClassifier.cs","line":631,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü° Minor_\n\n**Wasteful matrix allocation before early return.**\n\n`calibrated` is allocated on line 625 but immediately discarded if the method is `None`/`Auto` (lines 627‚Äì632). Move the allocation after the early-return check.\n\n\n<details>\n<summary>Proposed fix</summary>\n\n```diff\n     var uncalibrated = _baseClassifier.PredictProbabilities(input);\n-    var calibrated = new Matrix<T>(input.Rows, NumClasses);\n \n     if (_options.CalibrationMethod == ProbabilityCalibrationMethod.None\n         || _options.CalibrationMethod == ProbabilityCalibrationMethod.Auto)\n     {\n         // No calibration, return raw probabilities\n         return uncalibrated;\n     }\n \n     if (NumClasses != 2)\n     {\n         throw new NotSupportedException(\n             \"Calibration currently supports binary classification only. \" +\n             \"For multiclass, apply one-vs-rest calibration externally.\");\n     }\n \n+    var calibrated = new Matrix<T>(input.Rows, NumClasses);\n+\n     for (int i = 0; i < input.Rows; i++)\n```\n</details>\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        var uncalibrated = _baseClassifier.PredictProbabilities(input);\n\n        if (_options.CalibrationMethod == ProbabilityCalibrationMethod.None\n            || _options.CalibrationMethod == ProbabilityCalibrationMethod.Auto)\n        {\n            // No calibration, return raw probabilities\n            return uncalibrated;\n        }\n\n        if (NumClasses != 2)\n        {\n            throw new NotSupportedException(\n                \"Calibration currently supports binary classification only. \" +\n                \"For multiclass, apply one-vs-rest calibration externally.\");\n        }\n\n        var calibrated = new Matrix<T>(input.Rows, NumClasses);\n\n        for (int i = 0; i < input.Rows; i++)\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/Classification/Calibration/CalibratedClassifier.cs` around lines 624 -\n632, The code currently allocates a Matrix<T> named calibrated before checking\n_options.CalibrationMethod, which wastes memory when returning early; update the\nPredictProbabilities method so you call\n_baseClassifier.PredictProbabilities(input) and perform the early-return check\non _options.CalibrationMethod (compare to ProbabilityCalibrationMethod.None and\nProbabilityCalibrationMethod.Auto) before creating the calibrated Matrix<T>\ninstance, and only allocate calibrated after you know calibration will be\napplied.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:phoenix -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85tJLzd","isResolved":true,"path":"src/Classification/Calibration/CalibratedClassifier.cs","line":810,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü° Minor_\n\n**`SetParameters` silently ignores invalid input.**\n\nIf `parameters.Length < 6`, the method is a no-op with no error signal. Since `GetParameters` always returns exactly 6 elements, receiving fewer is a caller bug that should surface early.\n\n\n<details>\n<summary>Proposed fix</summary>\n\n```diff\n public override void SetParameters(Vector<T> parameters)\n {\n-    if (parameters.Length >= 6)\n+    if (parameters.Length < 6)\n     {\n-        _plattA = NumOps.ToDouble(parameters[0]);\n-        _plattB = NumOps.ToDouble(parameters[1]);\n-        _betaA = NumOps.ToDouble(parameters[2]);\n-        _betaB = NumOps.ToDouble(parameters[3]);\n-        _betaC = NumOps.ToDouble(parameters[4]);\n-        _temperature = NumOps.ToDouble(parameters[5]);\n+        throw new ArgumentException(\n+            $\"Expected at least 6 calibration parameters, but got {parameters.Length}.\",\n+            nameof(parameters));\n     }\n+\n+    _plattA = NumOps.ToDouble(parameters[0]);\n+    _plattB = NumOps.ToDouble(parameters[1]);\n+    _betaA = NumOps.ToDouble(parameters[2]);\n+    _betaB = NumOps.ToDouble(parameters[3]);\n+    _betaC = NumOps.ToDouble(parameters[4]);\n+    _temperature = NumOps.ToDouble(parameters[5]);\n }\n```\n</details>\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    public override void SetParameters(Vector<T> parameters)\n    {\n        if (parameters.Length < 6)\n        {\n            throw new ArgumentException(\n                $\"Expected at least 6 calibration parameters, but got {parameters.Length}.\",\n                nameof(parameters));\n        }\n\n        _plattA = NumOps.ToDouble(parameters[0]);\n        _plattB = NumOps.ToDouble(parameters[1]);\n        _betaA = NumOps.ToDouble(parameters[2]);\n        _betaB = NumOps.ToDouble(parameters[3]);\n        _betaC = NumOps.ToDouble(parameters[4]);\n        _temperature = NumOps.ToDouble(parameters[5]);\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/Classification/Calibration/CalibratedClassifier.cs` around lines 795 -\n806, SetParameters currently ignores calls with parameters.Length < 6; change it\nto validate input and throw an ArgumentException (or ArgumentNullException if\nparameters is null) when parameters.Length != 6 to surface caller bugs. Update\nthe SetParameters(Vector<T> parameters) method to check for null and ensure\nparameters.Length == 6, then proceed to assign _plattA, _plattB, _betaA, _betaB,\n_betaC, _temperature; mention GetParameters (which returns 6 elements) in the\nvalidation rationale in a short comment if desired.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:phoenix -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85tJLzh","isResolved":true,"path":"src/Distributions/PoissonDistribution.cs","line":165,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü° Minor_\n\n**`GradLogPdf` returns a finite gradient for negative inputs where log-PMF is ‚àí‚àû.**\n\nWhen `x` is negative, `Math.Round` yields a negative `k`, producing `k/Œª ‚àí 1` ‚Äî a finite value. This is mathematically inconsistent since `LogPmf` correctly returns `‚àí‚àû` for `k < 0`. Consider guarding:\n\n<details>\n<summary>Proposed fix</summary>\n\n```diff\n public override T[] GradLogPdf(T x)\n {\n     int k = (int)Math.Round(NumOps.ToDouble(x));\n+    if (k < 0)\n+        return [NumOps.FromDouble(double.NaN)];\n+\n     double lambda = NumOps.ToDouble(_lambda);\n```\n\n</details>\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/Distributions/PoissonDistribution.cs` around lines 153 - 162, GradLogPdf\ncurrently computes a finite gradient for negative inputs (k < 0) even though\nLogPmf returns ‚àí‚àû; modify GradLogPdf so it checks k =\n(int)Math.Round(NumOps.ToDouble(x)) and if k < 0 returns an array containing\nNumOps.FromDouble(double.NegativeInfinity) (or the library's representation of\n‚àí‚àû) instead of computing k/_lambda - 1; otherwise keep the existing gradLambda\ncalculation using _lambda and return NumOps.FromDouble(gradLambda).\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:phoenix -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85tJLzl","isResolved":true,"path":"src/Distributions/PoissonDistribution.cs","line":252,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**Numerical underflow in `RegularizedUpperGamma` silently breaks CDF for large Œª.**\n\nThe initial term `Math.Exp(-x)` underflows to exactly `0.0` when `x ‚â≥ 745`, causing the entire summation to return `0` regardless of `a`. This means `Cdf` returns `0` for all inputs when Œª is large, and `InverseCdf` (which depends on `Cdf` via this helper) will also produce wrong results.\n\nThis is a realistic range for Poisson (e.g., high-volume event counts). Consider computing in log-space and using a numerically stable summation (e.g., log-sum-exp), or use the regularized incomplete gamma from a numerics library.\n\n\n\n<details>\n<summary>Sketch: log-space stable approach</summary>\n\n```diff\n private static double RegularizedUpperGamma(int a, double x)\n {\n-    double sum = 0;\n-    double term = Math.Exp(-x);\n-\n-    for (int i = 0; i < a; i++)\n-    {\n-        sum += term;\n-        term *= x / (i + 1);\n-    }\n-\n-    return sum;\n+    // Compute log of each term: log(e^{-x} * x^i / i!) = -x + i*log(x) - log(i!)\n+    // then use log-sum-exp for stable summation.\n+    if (a <= 0) return 0;\n+\n+    double[] logTerms = new double[a];\n+    double logFact = 0;\n+    for (int i = 0; i < a; i++)\n+    {\n+        logTerms[i] = -x + i * Math.Log(x) - logFact;\n+        logFact += Math.Log(i + 1);\n+    }\n+\n+    double maxLog = logTerms[0];\n+    for (int i = 1; i < a; i++)\n+        if (logTerms[i] > maxLog) maxLog = logTerms[i];\n+\n+    double sum = 0;\n+    for (int i = 0; i < a; i++)\n+        sum += Math.Exp(logTerms[i] - maxLog);\n+\n+    return Math.Exp(Math.Log(sum) + maxLog);\n }\n```\n\n</details>\n\nNote: the log-space approach allocates an array of size `a`, which could be large. For production use, consider a two-pass streaming log-sum-exp or leveraging the existing `LogGamma` to compute terms more efficiently.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/Distributions/PoissonDistribution.cs` around lines 220 - 232,\nRegularizedUpperGamma currently computes terms with Math.Exp(-x) which\nunderflows for x ‚â≥ 745; update RegularizedUpperGamma to compute the summation in\nlog-space (use log-term = -x + k*log(x) - log(k!) or use existing LogGamma to\nget log(k!)) and perform a numerically stable log-sum-exp accumulation (either\nby collecting log-terms for 0..a-1 then applying log-sum-exp, or implement a\ntwo-pass streaming log-sum-exp to avoid large allocations), or replace the\nimplementation by calling a vetted regularized incomplete gamma implementation\nfrom a numerics library; ensure the method still returns the non-log regularized\nsum (exp of the log-sum) so callers (Cdf/InverseCdf) get correct nonzero values\nfor large x.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:phoenix -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85tJLzo","isResolved":true,"path":"src/Distributions/WeibullDistribution.cs","line":202,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü° Minor_\n\n**`GradLogPdf` has no domain guard for x ‚â§ 0.**\n\nWhen `x ‚â§ 0`, `LogPdf` returns `‚àí‚àû`, but `GradLogPdf` computes `Math.Log(xOverLambda)` which yields `NaN` (for x < 0) or `‚àí‚àû` (for x = 0), producing inconsistent or silently incorrect gradient arrays. Consider returning `NaN` or throwing for out-of-support inputs, consistent with how `Pdf`/`LogPdf` handle this domain.\n\n<details>\n<summary>Proposed guard</summary>\n\n```diff\n public override T[] GradLogPdf(T x)\n {\n+    if (NumOps.Compare(x, Zero) <= 0)\n+        return [NumOps.FromDouble(double.NaN), NumOps.FromDouble(double.NaN)];\n+\n     double xVal = NumOps.ToDouble(x);\n```\n\n</details>\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    /// <inheritdoc/>\n    public override T[] GradLogPdf(T x)\n    {\n        if (NumOps.Compare(x, Zero) <= 0)\n            return [NumOps.FromDouble(double.NaN), NumOps.FromDouble(double.NaN)];\n\n        double xVal = NumOps.ToDouble(x);\n        double k = NumOps.ToDouble(_shape);\n        double lambda = NumOps.ToDouble(_scale);\n\n        double xOverLambda = xVal / lambda;\n        double xOverLambdaK = Math.Pow(xOverLambda, k);\n\n        // d/d(shape) log(pdf)\n        double gradShape = 1 / k + Math.Log(xOverLambda) * (1 - xOverLambdaK);\n\n        // d/d(scale) log(pdf)\n        double gradScale = -k / lambda + k * Math.Pow(xVal, k) / Math.Pow(lambda, k + 1);\n\n        return [NumOps.FromDouble(gradShape), NumOps.FromDouble(gradScale)];\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/Distributions/WeibullDistribution.cs` around lines 182 - 199, GradLogPdf\ncurrently computes Math.Log(xOverLambda) for all x which yields NaN/‚àí‚àû when x ‚â§\n0; update WeibullDistribution.GradLogPdf to first guard the support (x ‚â§ 0) in\nthe same spirit as LogPdf/Pdf and return a consistent result (e.g., an array\ncontaining NaN for both gradient components) instead of performing the math;\nimplement the guard at the start of GradLogPdf (using xVal, _shape, _scale) and\nwhen triggered return new T[] { NumOps.FromDouble(double.NaN),\nNumOps.FromDouble(double.NaN) } (or alternatively throw to match project policy)\nso gradShape and gradScale are not computed for out-of-support x.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:phoenix -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85tJLzv","isResolved":true,"path":"src/Distributions/WeibullDistribution.cs","line":222,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Fisher information `I_{kk}` entry has an extra `+1` ‚Äî incorrect formula.**\n\nThe (1,1) entry is computed as `(1 + (1‚àíŒ≥)¬≤ + œÄ¬≤/6) / k¬≤`, but the correct Fisher information for the shape parameter of Weibull(k, Œª) is:\n\n**I_{kk} = ((1‚àíŒ≥)¬≤ + œÄ¬≤/6) / k¬≤**\n\nDerivation: `E[‚àí‚àÇ¬≤logf/‚àÇk¬≤] = 1/k¬≤ + (1/k¬≤)¬∑E[Y(logY)¬≤]` where `Y ~ Exp(1)`, and `E[Y(logY)¬≤] = œà'(2) + œà(2)¬≤ = (œÄ¬≤/6 ‚àí 1) + (1‚àíŒ≥)¬≤`. Combining: `(1 + œÄ¬≤/6 ‚àí 1 + (1‚àíŒ≥)¬≤)/k¬≤ = (œÄ¬≤/6 + (1‚àíŒ≥)¬≤)/k¬≤`. The other entries (`I_{ŒªŒª}` and `I_{k,Œª}`) are correct.\n\n\n\n<details>\n<summary>Proposed fix</summary>\n\n```diff\n-        double iShape = (1 + (1 - gamma) * (1 - gamma) + Math.PI * Math.PI / 6) / (k * k);\n+        double iShape = ((1 - gamma) * (1 - gamma) + Math.PI * Math.PI / 6) / (k * k);\n```\n\n</details>\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    /// <inheritdoc/>\n    public override T[,] FisherInformation()\n    {\n        double k = NumOps.ToDouble(_shape);\n        double lambda = NumOps.ToDouble(_scale);\n\n        // Euler-Mascheroni constant\n        const double gamma = 0.5772156649;\n\n        double iShape = ((1 - gamma) * (1 - gamma) + Math.PI * Math.PI / 6) / (k * k);\n        double iScale = k * k / (lambda * lambda);\n        double iShapeScale = -(1 - gamma) / lambda;\n\n        return new T[,]\n        {\n            { NumOps.FromDouble(iShape), NumOps.FromDouble(iShapeScale) },\n            { NumOps.FromDouble(iShapeScale), NumOps.FromDouble(iScale) }\n        };\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/Distributions/WeibullDistribution.cs` around lines 201 - 219, In\nFisherInformation(), the computed iShape currently includes an extra leading +1;\nupdate the iShape calculation in the FisherInformation() method of\nWeibullDistribution to use iShape = ((1 - gamma)*(1 - gamma) +\nMath.PI*Math.PI/6) / (k*k) (replace the existing expression assigned to iShape),\nkeeping the rest of the method (iScale, iShapeScale and the return) unchanged so\nthe (1,1) entry matches I_{kk} = ((1‚àíŒ≥)¬≤ + œÄ¬≤/6)/k¬≤.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:phoenix -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85tJLz0","isResolved":true,"path":"src/LinkFunctions/InverseSquaredLink.cs","line":76,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü° Minor_\n\n**`Variance` lacks the near-zero / non-positive guard applied in the other methods.**\n\nFor the Inverse Gaussian family, `mu` must be positive; a non-positive `mu` here yields a zero or negative variance, which would silently corrupt downstream IRLS weight calculations. Every other method in this class defensively clamps, but `Variance` does not.\n\n<details>\n<summary>üõ°Ô∏è Suggested fix</summary>\n\n```diff\n     public T Variance(T mu)\n     {\n         // For Inverse Gaussian: V(mu) = mu^3\n         double m = NumOps.ToDouble(mu);\n+        if (m <= 0) m = 1e-10;\n         return NumOps.FromDouble(m * m * m);\n     }\n```\n</details>\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/LinkFunctions/InverseSquaredLink.cs` around lines 70 - 75, The Variance\nmethod must mirror the defensive clamping used elsewhere: convert mu to double\nvia NumOps.ToDouble(mu), clamp m to a small positive epsilon (e.g., if m <= 0 or\nm < epsilon then m = epsilon) to ensure positivity, compute m*m*m, and return\nNumOps.FromDouble of that value; update the Variance method to use the same\nepsilon/clamping logic as the other methods in this class (keep using the\nVariance, NumOps.ToDouble, and NumOps.FromDouble symbols).\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:phoenix -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85tJLz5","isResolved":true,"path":"src/LinkFunctions/LinkFunctionFactory.cs","line":66,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**`GetCanonicalLink` silently falls back to `IdentityLink` for unknown families.**\n\n`Create` correctly throws on an unknown value, but `GetCanonicalLink` returns `IdentityLink<T>()` by default. If a new `DistributionFamily` member is added without updating this switch, the caller silently gets a wrong link function ‚Äî a correctness bug that would be hard to diagnose. Consider throwing instead, consistent with `Create`.\n\nAlso, the Tweedie branch returns `LogLink`, but the `DistributionFamily.Tweedie` XML doc (line 169) states \"Canonical link: Power.\" Either the doc or the code should be updated for consistency ‚Äî the true canonical link for Tweedie is a power link (Œ∏^(1‚àíp)), and `Log` is only canonical for the special case p = 1 (Poisson).\n\n<details>\n<summary>üêõ Proposed fix</summary>\n\n```diff\n-            _ => new IdentityLink<T>()\n+            _ => throw new ArgumentException($\"Unknown distribution family: {family}\", nameof(family))\n```\n\nAnd update the Tweedie doc to match the code if `Log` is the intended practical default:\n```diff\n     /// <summary>\n-    /// Tweedie distribution. Canonical link: Power.\n+    /// Tweedie distribution. Common default link: Log. (True canonical link is power with index 1‚àíp.)\n     /// </summary>\n```\n</details>\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/LinkFunctions/LinkFunctionFactory.cs` around lines 52 - 65,\nGetCanonicalLink currently returns IdentityLink<T>() for unknown\nDistributionFamily values which can silently hide errors; change the default arm\nof the switch in GetCanonicalLink to throw an ArgumentOutOfRangeException (or\nsimilar) referencing the family parameter instead of returning IdentityLink<T>()\nso it matches Create‚Äôs behavior; also reconcile the Tweedie branch by replacing\nDistributionFamily.Tweedie => new LogLink<T>() with the correct canonical link\n(e.g., DistributionFamily.Tweedie => new PowerLink<T>() if you intend the\ncanonical power link), or if Log is the intended practical default, update the\nDistributionFamily.Tweedie XML doc to state Log is the chosen default instead of\n‚ÄúPower.‚Äù Ensure references to IdentityLink<T>(), LogLink<T>(), PowerLink<T>(),\nGetCanonicalLink and Create are used to locate the changes.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:phoenix -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85tJLz-","isResolved":true,"path":"src/NeuralNetworks/Tabular/AutoIntNetwork.cs","line":267,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Deserialized options are discarded ‚Äî `_options` is never updated.**\n\n`DeserializeNetworkSpecificData` reads all fields into a local `options` variable (line 235) but never assigns it back to `_options`. Since `_options` is `readonly`, it *cannot* be assigned outside the constructor. After deserialization, the network retains whatever options it was originally constructed with, silently ignoring the serialized state.\n\nThis means a serialized model with, say, `EmbeddingDimension=32` would deserialize and report `EmbeddingDimension=16` (or whatever the default is) ‚Äî a data-corruption-class bug.\n\n\n\n<details>\n<summary>Proposed fix: remove `readonly` from `_options` and assign the deserialized value</summary>\n\nIn the field declaration (line 43):\n```diff\n-    private readonly AutoIntOptions<T> _options;\n+    private AutoIntOptions<T> _options;\n```\n\nAt the end of `DeserializeNetworkSpecificData` (after line 264):\n```diff\n         options.CategoricalCardinalities = cardinalities;\n         }\n+\n+        _options = options;\n     }\n```\n\nAlso consider re-initializing layers after updating options so the network topology matches.\n</details>\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/NeuralNetworks/Tabular/AutoIntNetwork.cs` around lines 232 - 265,\nDeserializeNetworkSpecificData builds a new AutoIntOptions<T> but never assigns\nit to the network's _options, so the deserialized values are discarded; remove\nthe readonly modifier from the _options field declaration and at the end of\nDeserializeNetworkSpecificData assign the deserialized options object to\n_options, then reinitialize any dependent structures (embedding layers,\nattention layers, MLPs, etc.) so the network topology matches the new options\n(e.g., call the same initialization routine used by the constructor or a\ndedicated RebuildNetworkFromOptions method).\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:phoenix -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85tJL0D","isResolved":true,"path":"src/NeuralNetworks/Tabular/AutoIntNetwork.cs","line":277,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü° Minor_\n\n**`CreateNewInstance` shares mutable optimizer and loss-function references.**\n\n`_optimizer` and `_lossFunction` are passed by reference to the new instance. If the optimizer carries internal state (momentum, adaptive learning rates), sharing it between two network instances will cause interference during training. This may be intentional for cloning during cross-validation (where only one instance trains at a time), but it's worth noting.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/NeuralNetworks/Tabular/AutoIntNetwork.cs` around lines 267 - 275,\nCreateNewInstance currently reuses the mutable _optimizer and _lossFunction\nreferences when constructing a new AutoIntNetwork<T>, which can cause\nshared-state interference; modify CreateNewInstance so it supplies\nfresh/independent instances to the new AutoIntNetwork<T> by cloning or creating\nnew optimizer and loss-function objects (e.g., call a Clone() or factory method\non _optimizer and _lossFunction or instantiate new ones from _options) while\nstill passing Architecture and _options as appropriate; ensure the copy is a\ndeep copy of internal optimizer state (momentum/accumulators) so training on the\nnew instance cannot affect the original.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:phoenix -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85tJL0G","isResolved":true,"path":"src/NeuralNetworks/Tabular/CLSToken.cs","line":60,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü° Minor_\n\n**Missing validation for `embeddingDimension`.**\n\nIf a caller passes `embeddingDimension <= 0`, the constructor will create degenerate tensors and downstream methods (`PrependCLS`, `Backward`, etc.) will silently produce empty results or throw confusing indexing errors. A guard clause here would surface the problem immediately.\n\n\n<details>\n<summary>üõ°Ô∏è Proposed fix</summary>\n\n```diff\n public CLSToken(int embeddingDimension, double initScale = 0.02)\n {\n+    ArgumentOutOfRangeException.ThrowIfNegativeOrZero(embeddingDimension);\n     EmbeddingDimension = embeddingDimension;\n     _random = RandomHelper.CreateSecureRandom();\n```\n</details>\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    public CLSToken(int embeddingDimension, double initScale = 0.02)\n    {\n        ArgumentOutOfRangeException.ThrowIfNegativeOrZero(embeddingDimension);\n        EmbeddingDimension = embeddingDimension;\n        _random = RandomHelper.CreateSecureRandom();\n\n        _clsEmbedding = new Tensor<T>([1, embeddingDimension]);\n        _clsGradient = new Tensor<T>([1, embeddingDimension]);\n\n        InitializeEmbedding(initScale);\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/NeuralNetworks/Tabular/CLSToken.cs` around lines 49 - 58, The CLSToken\nconstructor lacks validation for embeddingDimension, which can create degenerate\ntensors (_clsEmbedding, _clsGradient) and cause downstream failures in methods\nlike PrependCLS and Backward; add a guard at the start of the CLSToken(int\nembeddingDimension, double initScale = 0.02) constructor that checks\nembeddingDimension > 0 and throws an ArgumentOutOfRangeException (or\nArgumentException) with a clear message if violated, ensuring invalid values are\nrejected before creating the tensors or calling InitializeEmbedding.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:phoenix -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85tJL0M","isResolved":true,"path":"src/NeuralNetworks/Tabular/CLSToken.cs","line":144,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**`ExtractCLS` does not validate the embedding dimension of the input tensor.**\n\n`PrependCLS` validates that the input embedding dimension matches `EmbeddingDimension`, but `ExtractCLS` does not. If the transformer output has a different last dimension (e.g., due to a projection layer or misconfiguration), the flat-index arithmetic on Line 129 will silently read from wrong offsets, producing corrupted results instead of a clear error.\n\n\n<details>\n<summary>üõ°Ô∏è Proposed fix</summary>\n\n```diff\n public Tensor<T> ExtractCLS(Tensor<T> transformerOutput)\n {\n     int batchSize = transformerOutput.Shape[0];\n     int embDim = EmbeddingDimension;\n+\n+    if (transformerOutput.Shape[2] != embDim)\n+    {\n+        throw new ArgumentException(\n+            $\"Transformer output embedding dimension ({transformerOutput.Shape[2]}) does not match CLS token dimension ({embDim}).\");\n+    }\n \n     var clsOutput = new Tensor<T>([batchSize, embDim]);\n```\n</details>\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    public Tensor<T> ExtractCLS(Tensor<T> transformerOutput)\n    {\n        int batchSize = transformerOutput.Shape[0];\n        int embDim = EmbeddingDimension;\n\n        if (transformerOutput.Shape[2] != embDim)\n        {\n            throw new ArgumentException(\n                $\"Transformer output embedding dimension ({transformerOutput.Shape[2]}) does not match CLS token dimension ({embDim}).\");\n        }\n\n        var clsOutput = new Tensor<T>([batchSize, embDim]);\n\n        for (int b = 0; b < batchSize; b++)\n        {\n            for (int d = 0; d < embDim; d++)\n            {\n                // CLS is at position 0\n                int srcIdx = b * transformerOutput.Shape[1] * embDim + d;\n                clsOutput[b * embDim + d] = transformerOutput[srcIdx];\n            }\n        }\n\n        return clsOutput;\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/NeuralNetworks/Tabular/CLSToken.cs` around lines 117 - 135, ExtractCLS\nmust validate that transformerOutput's embedding dimension matches\nEmbeddingDimension to avoid silent mis-indexing; add a guard at the start of\nExtractCLS that checks transformerOutput.Shape has the expected rank/length and\nthat transformerOutput.Shape[last] == EmbeddingDimension (same check pattern\nused in PrependCLS) and throw a descriptive ArgumentException/InvalidOperation\nif it does not, then proceed with the existing indexing logic (or adjust\nindexing to use tensor accessor methods if available) so malformed inputs fail\nfast and clearly.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:phoenix -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85tJL0Q","isResolved":true,"path":"src/NeuralNetworks/Tabular/CLSToken.cs","line":165,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**`Backward` overwrites gradients instead of accumulating them.**\n\nLine 154 assigns `_clsGradient[d] = gradSum`, which replaces any previously stored gradient. The existence of a separate `ResetGradients()` method implies the design supports the standard pattern of accumulating gradients across multiple `Backward` calls (e.g., micro-batching) before a single `UpdateParameters` + `ResetGradients` cycle. With the current assignment, calling `Backward` twice before an update silently discards the first call's gradients.\n\n\n<details>\n<summary>üêõ Proposed fix ‚Äî accumulate instead of overwrite</summary>\n\n```diff\n-            _clsGradient[d] = gradSum;\n+            _clsGradient[d] = NumOps.Add(_clsGradient[d], gradSum);\n```\n</details>\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    public void Backward(Tensor<T> gradient)\n    {\n        int batchSize = gradient.Shape[0];\n        int embDim = EmbeddingDimension;\n\n        // Accumulate gradients from all batch samples\n        for (int d = 0; d < embDim; d++)\n        {\n            var gradSum = NumOps.Zero;\n            for (int b = 0; b < batchSize; b++)\n            {\n                gradSum = NumOps.Add(gradSum, gradient[b * embDim + d]);\n            }\n            _clsGradient[d] = NumOps.Add(_clsGradient[d], gradSum);\n        }\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/NeuralNetworks/Tabular/CLSToken.cs` around lines 141 - 156, Backward\ncurrently overwrites stored gradients by assigning `_clsGradient[d] = gradSum`;\nchange it to accumulate the new grad into the existing stored gradient so\nmultiple Backward calls sum (not replace) gradients (e.g., use NumOps.Add on the\nexisting `_clsGradient[d]` and `gradSum`), ensuring the accumulation semantics\nexpected by `ResetGradients()`/`UpdateParameters()` are preserved; update the\nloop in the `Backward` method to add `gradSum` to `_clsGradient[d]` instead of\nassigning.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:phoenix -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85tJL0X","isResolved":true,"path":"src/NeuralNetworks/Tabular/ColumnEmbedding.cs","line":176,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü° Minor_\n\n**`Backward` does not validate `numCols` against `_numColumns`.**\n\n`AddColumnEmbeddings` guards against `numCols > _numColumns` (Line 111), but `Backward` does not. If a gradient tensor with more columns than `_numColumns` is passed, the loop at Line 179 will index into `_embeddingGradients` out of bounds via `c * _embeddingDim + d` when `c >= _numColumns`.\n\n\n<details>\n<summary>üõ°Ô∏è Proposed fix</summary>\n\n```diff\n     if (embDim != _embeddingDim)\n     {\n         throw new ArgumentException(\n             $\"Gradient embedding dimension ({embDim}) does not match column embedding dimension ({_embeddingDim}).\");\n     }\n \n+    if (numCols > _numColumns)\n+    {\n+        throw new ArgumentException(\n+            $\"Gradient has {numCols} columns but only {_numColumns} column embeddings exist.\");\n+    }\n+\n     // Accumulate gradients from all batch samples\n```\n</details>\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    public void Backward(Tensor<T> gradient)\n    {\n        if (!_learnable) return;\n\n        int batchSize = gradient.Shape[0];\n        int numCols = gradient.Shape[1];\n        int embDim = gradient.Shape[2];\n\n        if (embDim != _embeddingDim)\n        {\n            throw new ArgumentException(\n                $\"Gradient embedding dimension ({embDim}) does not match column embedding dimension ({_embeddingDim}).\");\n        }\n\n        if (numCols > _numColumns)\n        {\n            throw new ArgumentException(\n                $\"Gradient has {numCols} columns but only {_numColumns} column embeddings exist.\");\n        }\n\n        // Accumulate gradients from all batch samples\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/NeuralNetworks/Tabular/ColumnEmbedding.cs` around lines 164 - 176, The\nBackward method is missing a guard that numCols from gradient.Shape[1] does not\nexceed the class _numColumns, which can cause out-of-bounds indexing into\n_embeddingGradients; add a validation in Backward (same place you check embDim)\nthat throws an ArgumentException if numCols > _numColumns (and optionally if\nnumCols <= 0), mirroring the check in AddColumnEmbeddings, so subsequent loops\nthat index using c * _embeddingDim + d are safe.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:phoenix -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85tJL0e","isResolved":true,"path":"src/NeuralNetworks/Tabular/ColumnEmbedding.cs","line":198,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**`Backward` overwrites gradients instead of accumulating them.**\n\nLine 189 assigns `gradSum` to `_embeddingGradients` instead of adding to it. Within a single call this correctly sums across the batch, but if `Backward` is called multiple times before `UpdateParameters` (e.g., gradient accumulation over micro-batches), previous gradients are silently lost. The existence of `ResetGradients()` as a separate method implies the accumulate-then-reset pattern is intended.\n\n\n<details>\n<summary>üêõ Proposed fix: accumulate instead of overwrite</summary>\n\n```diff\n-                _embeddingGradients[c * _embeddingDim + d] = gradSum;\n+                _embeddingGradients[c * _embeddingDim + d] = NumOps.Add(\n+                    _embeddingGradients[c * _embeddingDim + d], gradSum);\n```\n</details>\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        // Accumulate gradients from all batch samples\n        for (int c = 0; c < numCols; c++)\n        {\n            for (int d = 0; d < embDim; d++)\n            {\n                var gradSum = NumOps.Zero;\n                for (int b = 0; b < batchSize; b++)\n                {\n                    int gradIdx = b * numCols * embDim + c * embDim + d;\n                    gradSum = NumOps.Add(gradSum, gradient[gradIdx]);\n                }\n                _embeddingGradients[c * _embeddingDim + d] = NumOps.Add(\n                    _embeddingGradients[c * _embeddingDim + d], gradSum);\n            }\n        }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/NeuralNetworks/Tabular/ColumnEmbedding.cs` around lines 178 - 191, In\nBackward, you currently set per-column embedding gradients with\n_embeddingGradients[c * _embeddingDim + d] = gradSum which overwrites prior\naccumulated gradients; change this to accumulate by adding gradSum to the\nexisting value (i.e., _embeddingGradients[c * _embeddingDim + d] += gradSum) so\nmultiple Backward calls (micro-batch accumulation) accumulate correctly and only\nResetGradients() clears them before UpdateParameters; ensure you reference the\nsame index calculation and use NumOps.Add if required by the numeric type.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:phoenix -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85tJL0o","isResolved":true,"path":"src/NeuralNetworks/Tabular/ContrastivePretraining.cs","line":195,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**Missing shape validation on embedding inputs.**\n\nThere is no check that `originalEmbeddings` and `corruptedEmbeddings` have matching shapes, nor that `embDim` (from `Shape[1]`) matches the constructor's `embeddingDim` used to initialize `_projectionWeights`. A mismatch would silently corrupt the matrix multiply in `ProjectEmbeddings` or produce out-of-bounds accesses.\n\n<details>\n<summary>Proposed fix</summary>\n\n```diff\n     public T ComputeContrastiveLoss(Tensor<T> originalEmbeddings, Tensor<T> corruptedEmbeddings)\n     {\n+        if (originalEmbeddings.Shape[0] != corruptedEmbeddings.Shape[0] ||\n+            originalEmbeddings.Shape[1] != corruptedEmbeddings.Shape[1])\n+        {\n+            throw new ArgumentException(\"Original and corrupted embeddings must have the same shape.\");\n+        }\n+\n+        if (originalEmbeddings.Shape[1] != _projectionWeights.Shape[0])\n+        {\n+            throw new ArgumentException(\n+                $\"Embedding dimension {originalEmbeddings.Shape[1]} does not match projection input dimension {_projectionWeights.Shape[0]}.\");\n+        }\n+\n         _originalEmbeddingsCache = originalEmbeddings;\n         _corruptedEmbeddingsCache = corruptedEmbeddings;\n```\n</details>\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    public T ComputeContrastiveLoss(Tensor<T> originalEmbeddings, Tensor<T> corruptedEmbeddings)\n    {\n        if (originalEmbeddings.Shape[0] != corruptedEmbeddings.Shape[0] ||\n            originalEmbeddings.Shape[1] != corruptedEmbeddings.Shape[1])\n        {\n            throw new ArgumentException(\"Original and corrupted embeddings must have the same shape.\");\n        }\n\n        if (originalEmbeddings.Shape[1] != _projectionWeights.Shape[0])\n        {\n            throw new ArgumentException(\n                $\"Embedding dimension {originalEmbeddings.Shape[1]} does not match projection input dimension {_projectionWeights.Shape[0]}.\");\n        }\n\n        _originalEmbeddingsCache = originalEmbeddings;\n        _corruptedEmbeddingsCache = corruptedEmbeddings;\n\n        int batchSize = originalEmbeddings.Shape[0];\n        int embDim = originalEmbeddings.Shape[1];\n\n        // Project embeddings\n        var originalProjected = ProjectEmbeddings(originalEmbeddings, batchSize, embDim);\n        var corruptedProjected = ProjectEmbeddings(corruptedEmbeddings, batchSize, embDim);\n\n        // L2 normalize projections\n        originalProjected = L2Normalize(originalProjected, batchSize, _projectionDim);\n        corruptedProjected = L2Normalize(corruptedProjected, batchSize, _projectionDim);\n\n        // Compute InfoNCE loss\n        var loss = ComputeInfoNCELoss(originalProjected, corruptedProjected, batchSize);\n\n        return loss;\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/NeuralNetworks/Tabular/ContrastivePretraining.cs` around lines 161 - 181,\nComputeContrastiveLoss must validate input shapes before using\nProjectEmbeddings: check that originalEmbeddings and corruptedEmbeddings have\nthe same Rank and Shape[0] (batchSize) and Shape[1] (embDim), and ensure embDim\nequals the class constructor's embeddingDim used to size _projectionWeights; if\nany check fails, throw an ArgumentException with a clear message. Move\nassignment to _originalEmbeddingsCache and _corruptedEmbeddingsCache after\nvalidation so caches only store validated tensors. Keep the rest of the pipeline\n(calling ProjectEmbeddings, L2Normalize, ComputeInfoNCELoss) unchanged but rely\non these preconditions.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:phoenix -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85tJL0s","isResolved":true,"path":"src/NeuralNetworks/Tabular/ContrastivePretraining.cs","line":349,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü° Minor_\n\n**`ComputeDenoisingLoss` does not validate that `reconstructed`/`original` shapes match the cached corruption indices.**\n\nThe cached flat indices in `_corruptedIndicesCache` were computed from the tensor passed to `CorruptSamples`. If `reconstructed` or `original` have a different total length (e.g., different batch size), accessing `reconstructed[idx]` or `original[idx]` will throw an `IndexOutOfRangeException`. Unlike `GetCorruptionMask` (which bounds-checks at line 336), this method uses the indices directly.\n\n<details>\n<summary>Proposed fix</summary>\n\n```diff\n     public T ComputeDenoisingLoss(Tensor<T> reconstructed, Tensor<T> original)\n     {\n         if (_corruptedIndicesCache == null || _corruptedIndicesCache.Length == 0)\n         {\n             return NumOps.Zero;\n         }\n \n+        int expectedLength = reconstructed.Length;\n+        if (original.Length != expectedLength)\n+        {\n+            throw new ArgumentException(\"Reconstructed and original tensors must have the same shape.\");\n+        }\n+\n         var loss = NumOps.Zero;\n         foreach (var idx in _corruptedIndicesCache)\n         {\n+            if (idx < 0 || idx >= expectedLength)\n+            {\n+                throw new InvalidOperationException(\n+                    $\"Cached corruption index {idx} is out of range for tensor of length {expectedLength}. \" +\n+                    \"Ensure ComputeDenoisingLoss is called with the same batch size used in CorruptSamples.\");\n+            }\n             var diff = NumOps.Subtract(reconstructed[idx], original[idx]);\n             loss = NumOps.Add(loss, NumOps.Multiply(diff, diff));\n         }\n```\n</details>\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    public T ComputeDenoisingLoss(Tensor<T> reconstructed, Tensor<T> original)\n    {\n        if (_corruptedIndicesCache == null || _corruptedIndicesCache.Length == 0)\n        {\n            return NumOps.Zero;\n        }\n\n        int expectedLength = reconstructed.Length;\n        if (original.Length != expectedLength)\n        {\n            throw new ArgumentException(\"Reconstructed and original tensors must have the same shape.\");\n        }\n\n        var loss = NumOps.Zero;\n        foreach (var idx in _corruptedIndicesCache)\n        {\n            if (idx < 0 || idx >= expectedLength)\n            {\n                throw new InvalidOperationException(\n                    $\"Cached corruption index {idx} is out of range for tensor of length {expectedLength}. \" +\n                    \"Ensure ComputeDenoisingLoss is called with the same batch size used in CorruptSamples.\");\n            }\n            var diff = NumOps.Subtract(reconstructed[idx], original[idx]);\n            loss = NumOps.Add(loss, NumOps.Multiply(diff, diff));\n        }\n\n        return NumOps.Divide(loss, NumOps.FromDouble(_corruptedIndicesCache.Length));\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/NeuralNetworks/Tabular/ContrastivePretraining.cs` around lines 305 - 320,\nThe ComputeDenoisingLoss method must validate that the cached flat indices in\n_corruptedIndicesCache are valid for the given reconstructed and original\ntensors to avoid IndexOutOfRangeException; update ComputeDenoisingLoss to check\nreconstructed.Length and original.Length (or compute a single totalLength)\nagainst the maximum value in _corruptedIndicesCache (or ensure every idx <\nlength) and if they don't match either throw a clear ArgumentException\nmentioning ComputeDenoisingLoss/_corruptedIndicesCache or return NumOps.Zero\nafter clearing/ignoring the cache‚Äîmirror the bounds-checking approach used by\nGetCorruptionMask so the method never indexes out of range.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:phoenix -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85tJL0y","isResolved":true,"path":"src/NeuralNetworks/Tabular/ContrastivePretraining.cs","line":403,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü° Minor_\n\n**No shape validation on gradient tensors.**\n\nIf `gradWeights` or `gradBias` have a different length than `_projectionWeights` / `_projectionBias`, the loop will throw a raw `IndexOutOfRangeException`. A quick guard would provide a clearer diagnostic.\n\n<details>\n<summary>Proposed fix</summary>\n\n```diff\n     public void UpdateParameters(T learningRate, Tensor<T> gradWeights, Tensor<T> gradBias)\n     {\n+        if (gradWeights.Length != _projectionWeights.Length)\n+            throw new ArgumentException($\"gradWeights length {gradWeights.Length} != expected {_projectionWeights.Length}.\");\n+        if (gradBias.Length != _projectionBias.Length)\n+            throw new ArgumentException($\"gradBias length {gradBias.Length} != expected {_projectionBias.Length}.\");\n+\n         for (int i = 0; i < _projectionWeights.Length; i++)\n```\n</details>\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    /// <summary>\n    /// Updates parameters.\n    /// </summary>\n    public void UpdateParameters(T learningRate, Tensor<T> gradWeights, Tensor<T> gradBias)\n    {\n        if (gradWeights.Length != _projectionWeights.Length)\n            throw new ArgumentException($\"gradWeights length {gradWeights.Length} != expected {_projectionWeights.Length}.\");\n        if (gradBias.Length != _projectionBias.Length)\n            throw new ArgumentException($\"gradBias length {gradBias.Length} != expected {_projectionBias.Length}.\");\n\n        for (int i = 0; i < _projectionWeights.Length; i++)\n        {\n            _projectionWeights[i] = NumOps.Subtract(_projectionWeights[i],\n                NumOps.Multiply(learningRate, gradWeights[i]));\n        }\n\n        for (int i = 0; i < _projectionBias.Length; i++)\n        {\n            _projectionBias[i] = NumOps.Subtract(_projectionBias[i],\n                NumOps.Multiply(learningRate, gradBias[i]));\n        }\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/NeuralNetworks/Tabular/ContrastivePretraining.cs` around lines 346 - 362,\nThe UpdateParameters method lacks validation of gradient tensor lengths and will\nthrow an IndexOutOfRangeException if gradWeights or gradBias lengths differ from\n_projectionWeights/_projectionBias; add explicit guards at the start of\nUpdateParameters to validate that gradWeights and gradBias are not null and that\ngradWeights.Length equals _projectionWeights.Length and gradBias.Length equals\n_projectionBias.Length (or the appropriate shape/Count property used in this\ncodebase), and if not, throw an ArgumentException/ArgumentNullException with a\nclear message mentioning UpdateParameters, gradWeights/gradBias and the expected\nlengths so callers get a descriptive diagnostic instead of a raw\nIndexOutOfRangeException.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:phoenix -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85tKdcQ","isResolved":false,"path":"src/NeuralNetworks/Tabular/FTTransformerNetwork.cs","line":134,"comments":{"nodes":[{"body":"Train() backpropagates `prediction - expectedOutput` regardless of the configured loss function. This is only correct for a specific setup (e.g., MSE with identity output); for classification losses (cross-entropy/softmax) or other losses it produces an incorrect gradient. Consider computing the loss gradient via the configured `ILossFunction<T>` (e.g., a `CalculateGradient(...)`/`Backward(...)` API) or delegating to the base class‚Äô loss/backprop pipeline so the gradient matches the selected loss.\n```suggestion\n\n        // Compute loss using the configured loss function\n        var predictionVector = prediction.ToVector();\n        var expectedVector = expectedOutput.ToVector();\n        LastLoss = _lossFunction.CalculateLoss(predictionVector, expectedVector);\n\n        // Compute the loss gradient w.r.t. the network outputs using the loss function,\n        // rather than assuming a specific loss (e.g., MSE) where gradient = prediction - target.\n        var lossGradientVector = _lossFunction.CalculateGradient(predictionVector, expectedVector);\n\n        // Convert the loss gradient back into a tensor with the same shape as the prediction.\n        Tensor<T> error = Tensor<T>.FromVector(lossGradientVector, prediction.Shape);\n```"}]}},{"id":"PRRT_kwDOKSXUF85tKdc9","isResolved":false,"path":"src/NeuralNetworks/Tabular/FTTransformerNetwork.cs","line":244,"comments":{"nodes":[{"body":"SerializeNetworkSpecificData writes FT-Transformer options, but DeserializeNetworkSpecificData is empty. This makes deserialization silently lose all serialized configuration (and may also desync the stream if the base expects these fields to be consumed). Implement the symmetric read logic to rebuild `_options` (including categorical cardinalities) from the BinaryReader."}]}},{"id":"PRRT_kwDOKSXUF85tKddK","isResolved":false,"path":"src/NeuralNetworks/Tabular/FTTransformerNetwork.cs","line":252,"comments":{"nodes":[{"body":"CreateNewInstance passes the existing `_optimizer` into the new network instance. Optimizers typically hold internal state (e.g., Adam moments) and may also be bound to the original network/parameter set. Reusing the same optimizer instance across model instances can corrupt training dynamics or reference the wrong parameters. Prefer constructing a new optimizer for the new instance (or cloning optimizer state in a model-safe way), and avoid passing an optimizer that was created with `this`.\n```suggestion\n            null,\n```"}]}},{"id":"PRRT_kwDOKSXUF85tKddb","isResolved":false,"path":"src/Models/Options/ProbabilityCalibratorOptions.cs","line":36,"comments":{"nodes":[{"body":"This introduces a new `ProbabilityCalibratorMethod` enum, while the repo already has `AiDotNet.Enums.ProbabilityCalibrationMethod` (updated in this PR to include `BetaCalibration`). Having two similar enums for the same concept increases API confusion and conversion boilerplate. Consider reusing the existing enum in ProbabilityCalibratorOptions (or centralizing to a single enum) to keep calibration method selection consistent across calibrated classifiers and calibrator infrastructure."}]}},{"id":"PRRT_kwDOKSXUF85tKddt","isResolved":false,"path":"src/LinkFunctions/LinkFunctionFactory.cs","line":66,"comments":{"nodes":[{"body":"The `DistributionFamily.Tweedie` enum documentation (later in this file) states the canonical link is 'Power', but GetCanonicalLink returns LogLink for Tweedie. Either update the Tweedie enum doc to match the implemented behavior (log link), or implement/return the intended power link to keep canonical-link semantics consistent."}]}},{"id":"PRRT_kwDOKSXUF85tKdd4","isResolved":false,"path":"src/Evaluation/Metrics/Probabilistic/PinballLossMetric.cs","line":122,"comments":{"nodes":[{"body":"For bootstrap resampling, using a cryptographically secure RNG (`CreateSecureRandom()`) can be significantly slower and provides no measurable benefit here. Consider using a standard non-crypto PRNG when no seed is provided (e.g., `new Random()` or a dedicated fast RNG helper), and reserve secure RNG for security-sensitive contexts. (Same pattern appears in other probabilistic metrics in this PR.)\n```suggestion\n        var random = seed.HasValue ? RandomHelper.CreateSeededRandom(seed.Value) : new Random();\n```"}]}},{"id":"PRRT_kwDOKSXUF85tKdeG","isResolved":false,"path":"src/NeuralNetworks/Tabular/ColumnEmbedding.cs","line":64,"comments":{"nodes":[{"body":"Model parameter initialization uses a cryptographically secure RNG, which is typically much slower and also hurts reproducibility unless callers can provide a seed. Consider switching to a fast seeded PRNG by default (or allowing injection of `Random` / seed via options) and reserving secure RNG for security-critical randomness.\n```suggestion\n    public ColumnEmbedding(int numColumns, int embeddingDim, bool learnable = true, double initScale = 0.02, int? seed = null)\n    {\n        _numColumns = numColumns;\n        _embeddingDim = embeddingDim;\n        _learnable = learnable;\n        _random = seed.HasValue ? new Random(seed.Value) : new Random();\n```"}]}},{"id":"PRRT_kwDOKSXUF85tKdeP","isResolved":false,"path":"src/Models/Options/TabTransformerOptions.cs","line":176,"comments":{"nodes":[{"body":"Behavior change: previously `NumCategoricalFeatures` defaulted to 10, but with the new backing field approach it now defaults to 0 unless explicitly set or `CategoricalCardinalities` is provided. That can be a breaking change for existing callers that relied on the old default. If backward compatibility is a goal, consider initializing `_numCategoricalFeatures` to the prior default (10) and only overriding it when `CategoricalCardinalities` is set."}]}},{"id":"PRRT_kwDOKSXUF85tKdeb","isResolved":false,"path":"src/Evaluation/Metrics/Probabilistic/LogScoreMetric.cs","line":84,"comments":{"nodes":[{"body":"This `LogScore` metric returns mean squared error when only point predictions are provided, even though the metric is named/logically defined as negative log-likelihood and `RequiresProbabilities` is `true`. This can lead to silent misuse and incomparable results across models. Consider either (1) throwing/returning an error when distributional inputs are not provided, or (2) clearly separating the point-prediction proxy into a different metric name/category and setting `RequiresProbabilities` accordingly.\n```suggestion\n\n        // This metric is defined for probabilistic predictions only.\n        // Use ComputeFromDistributions for valid LogScore calculations,\n        // or choose a point-based regression metric (e.g., MSE) for point predictions.\n        throw new NotSupportedException(\n            \"LogScoreMetric requires probabilistic predictions (distributions). \" +\n            \"Point predictions are not supported; use ComputeFromDistributions or a point-based metric instead.\");\n```"}]}},{"id":"PRRT_kwDOKSXUF85tKjn3","isResolved":false,"path":"src/Classification/Boosting/ExplainableBoostingClassifier.cs","line":720,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü° Minor_\n\n**`Convert.ToDouble` in `Serialize` is inconsistent with `NumOps.ToDouble` used everywhere else.**\n\nThe rest of the class converts `T ‚Üí double` via `NumOps.ToDouble`, but `Serialize` uses `Convert.ToDouble`. If `T` doesn't implement `IConvertible` (but does satisfy the `NumOps` contract), serialization will throw at runtime while everything else works. The same pattern appears at lines 700, 708, 718, and 734.\n\n\n<details>\n<summary>üîß Proposed fix ‚Äî use NumOps.ToDouble consistently</summary>\n\n```diff\n         writer.Write(_numFeatures);\n-        writer.Write(Convert.ToDouble(_intercept));\n+        writer.Write(NumOps.ToDouble(_intercept));\n\n         // Shape functions\n         for (int f = 0; f < _numFeatures; f++)\n         {\n             writer.Write(_shapeFunctions[f].Length);\n             foreach (var val in _shapeFunctions[f])\n             {\n-                writer.Write(Convert.ToDouble(val));\n+                writer.Write(NumOps.ToDouble(val));\n             }\n         }\n\n         // Bin edges\n         for (int f = 0; f < _numFeatures; f++)\n         {\n             writer.Write(_binEdges[f].Length);\n             foreach (var edge in _binEdges[f])\n             {\n-                writer.Write(Convert.ToDouble(edge));\n+                writer.Write(NumOps.ToDouble(edge));\n             }\n         }\n```\n\nAnd similarly at line 734:\n```diff\n-                    writer.Write(Convert.ToDouble(kvp.Value[i, j]));\n+                    writer.Write(NumOps.ToDouble(kvp.Value[i, j]));\n```\n</details>\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/Classification/Boosting/ExplainableBoostingClassifier.cs` around lines\n698 - 720, In the Serialize method replace all uses of Convert.ToDouble with the\nclass's numeric abstraction NumOps.ToDouble so serialization won't require\nIConvertible; specifically convert _intercept, each value in _shapeFunctions[f],\nand each edge in _binEdges[f] using NumOps.ToDouble (refer to Serialize,\n_numFeatures, _intercept, _shapeFunctions, _binEdges and NumOps.ToDouble) to\nmatch the rest of the class.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:phoenix -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85tKjn6","isResolved":false,"path":"src/Distributions/PoissonDistribution.cs","line":151,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü° Minor_\n\n**`InverseCdf` binary search has a fixed upper bound ‚Äî may return incorrect results for extreme tail probabilities.**\n\nThe upper bound `Œª + 10‚àöŒª + 10` covers ~10 standard deviations, which is sufficient for virtually all practical scenarios. However, if `p` is close enough to 1 that the true quantile exceeds this bound, the binary search will return the upper bound ‚Äî a silently wrong answer.\n\nA simple while-loop to double `high` until `CDF(high) >= p` would make this robust:\n\n<details>\n<summary>Proposed fix</summary>\n\n```diff\n         double lambda = NumOps.ToDouble(_lambda);\n         int low = 0;\n         int high = (int)(lambda + 10 * Math.Sqrt(lambda) + 10);\n \n+        // Expand upper bound if needed\n+        while (RegularizedUpperGamma(high + 1, lambda) < pVal)\n+        {\n+            high *= 2;\n+        }\n+\n         while (low < high)\n```\n\n</details>\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/Distributions/PoissonDistribution.cs` around lines 124 - 151, The\nInverseCdf in PoissonDistribution.cs uses a fixed high = (int)(lambda +\n10*Math.Sqrt(lambda) + 10) which can be too small for extreme tail p; change\nInverseCdf (method name) to first compute lambda and initial high as currently\ndone, then add a loop that doubles high (e.g., high *= 2) and recomputes cdf via\nRegularizedUpperGamma(high + 1, lambda) until cdf >= pVal (or until high exceeds\na very large guard to avoid infinite loops), then proceed with the binary search\nbetween low and high; keep using NumOps.ToDouble/FromDouble for conversions and\npreserve checks for p==0 and p==1.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:phoenix -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85tKjn8","isResolved":false,"path":"src/Distributions/PoissonDistribution.cs","line":226,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü° Minor_\n\n**Duplicate `<summary>` XML doc blocks on `RegularizedUpperGamma`.**\n\nThere are two consecutive `<summary>` blocks (lines 218‚Äì222 and 223‚Äì226). Only one should remain; the second one (which mentions the log-space approach) is the more accurate description.\n\n<details>\n<summary>Proposed fix</summary>\n\n```diff\n-    /// <summary>\n-    /// Computes the regularized upper incomplete gamma function Q(a, x) for integer a.\n-    /// Q(a, x) = e^{-x} * sum_{i=0}^{a-1} x^i / i!\n-    /// For Poisson CDF: P(X &lt;= k) = Q(k+1, lambda)\n-    /// </summary>\n     /// <summary>\n     /// Computes the regularized upper incomplete gamma function Q(a, x) = e^{-x} * sum_{i=0}^{a-1} x^i / i!\n     /// Uses log-space computation to avoid underflow for large x.\n+    /// For Poisson CDF: P(X &lt;= k) = Q(k+1, lambda)\n     /// </summary>\n```\n\n</details>\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    /// <summary>\n    /// Computes the regularized upper incomplete gamma function Q(a, x) = e^{-x} * sum_{i=0}^{a-1} x^i / i!\n    /// Uses log-space computation to avoid underflow for large x.\n    /// For Poisson CDF: P(X &lt;= k) = Q(k+1, lambda)\n    /// </summary>\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/Distributions/PoissonDistribution.cs` around lines 218 - 226, Remove the\nduplicate XML <summary> block above the RegularizedUpperGamma method and keep\nonly the second, more accurate summary that mentions the log-space computation;\nupdate the method's XML to contain a single <summary> describing \"Computes the\nregularized upper incomplete gamma function Q(a, x) = e^{-x} * sum_{i=0}^{a-1}\nx^i / i! Uses log-space computation to avoid underflow for large x.\" and ensure\nthere are no leftover duplicated tags so the XML is well-formed for\nRegularizedUpperGamma.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:phoenix -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85tKjoF","isResolved":false,"path":"src/NeuralNetworks/Tabular/CLSToken.cs","line":153,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü° Minor_\n\n**`Backward` should validate gradient shape to prevent silent data corruption.**\n\n`PrependCLS` and `ExtractCLS` both validate their input dimensions, but `Backward` does not. If `gradient` has the wrong shape, the flat-index arithmetic `b * embDim + d` will silently read from incorrect offsets, producing corrupt gradient accumulations with no error.\n\n<details>\n<summary>üõ°Ô∏è Proposed fix</summary>\n\n```diff\n public void Backward(Tensor<T> gradient)\n {\n+    if (gradient.Shape.Length != 2 || gradient.Shape[1] != EmbeddingDimension)\n+    {\n+        throw new ArgumentException(\n+            $\"Expected gradient shape [batchSize, {EmbeddingDimension}], but got [{string.Join(\", \", gradient.Shape)}].\");\n+    }\n+\n     int batchSize = gradient.Shape[0];\n     int embDim = EmbeddingDimension;\n```\n</details>\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    public void Backward(Tensor<T> gradient)\n    {\n        if (gradient.Shape.Length != 2 || gradient.Shape[1] != EmbeddingDimension)\n        {\n            throw new ArgumentException(\n                $\"Expected gradient shape [batchSize, {EmbeddingDimension}], but got [{string.Join(\", \", gradient.Shape)}].\");\n        }\n\n        int batchSize = gradient.Shape[0];\n        int embDim = EmbeddingDimension;\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/NeuralNetworks/Tabular/CLSToken.cs` around lines 150 - 153, Add the same\ninput-shape validation to Backward that PrependCLS/ExtractCLS perform: verify\ngradient.Rank (or Length of Shape) is 2, gradient.Shape[1] equals\nEmbeddingDimension, and gradient.Shape[0] equals the expected batch count for\nthis CLS operation; if any check fails throw an ArgumentException (or\nArgumentOutOfRangeException) with a clear message. Update the Backward method\n(symbol: Backward in class CLSToken) to perform these checks before using the b\n* embDim + d indexing to prevent silent corruption.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:phoenix -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85tKjoQ","isResolved":false,"path":"src/NeuralNetworks/Tabular/ColumnEmbedding.cs","line":67,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü° Minor_\n\n**Constructor does not validate `numColumns` or `embeddingDim`.**\n\nIf either argument is ‚â§ 0, the `Tensor<T>` constructor may throw a confusing error (or silently create a degenerate tensor). Add guards at the top of the constructor for clearer fail-fast behavior.\n\n\n<details>\n<summary>üõ°Ô∏è Proposed fix</summary>\n\n```diff\n public ColumnEmbedding(int numColumns, int embeddingDim, bool learnable = true, double initScale = 0.02)\n {\n+    ArgumentOutOfRangeException.ThrowIfNegativeOrZero(numColumns);\n+    ArgumentOutOfRangeException.ThrowIfNegativeOrZero(embeddingDim);\n+\n     _numColumns = numColumns;\n     _embeddingDim = embeddingDim;\n```\n</details>\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    public ColumnEmbedding(int numColumns, int embeddingDim, bool learnable = true, double initScale = 0.02)\n    {\n        ArgumentOutOfRangeException.ThrowIfNegativeOrZero(numColumns);\n        ArgumentOutOfRangeException.ThrowIfNegativeOrZero(embeddingDim);\n\n        _numColumns = numColumns;\n        _embeddingDim = embeddingDim;\n        _learnable = learnable;\n        _random = RandomHelper.CreateSecureRandom();\n\n        _embeddings = new Tensor<T>([numColumns, embeddingDim]);\n        _embeddingGradients = new Tensor<T>([numColumns, embeddingDim]);\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/NeuralNetworks/Tabular/ColumnEmbedding.cs` around lines 59 - 67, The\nColumnEmbedding constructor currently accepts invalid sizes and then constructs\nTensor<T> with them; add guard checks at the top of the ColumnEmbedding(int\nnumColumns, int embeddingDim, ...) constructor to validate that numColumns > 0\nand embeddingDim > 0 and throw ArgumentOutOfRangeException (or\nArgumentException) naming the offending parameter if not; do this before\nassigning _numColumns/_embeddingDim or creating _embeddings/_embeddingGradients\nso the failure is fast and clear.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:phoenix -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85tKjoX","isResolved":false,"path":"src/NeuralNetworks/Tabular/ColumnEmbedding.cs","line":109,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üü° Minor_\n\n**No tensor rank validation before accessing `Shape[0..2]`.**\n\nBoth `AddColumnEmbeddings` and `Backward` destructure three dimensions from the input tensor's `Shape` without first verifying the tensor is 3D. Passing a 2D tensor would produce a raw `IndexOutOfRangeException` with no indication of the expected `[batchSize, numColumns, embeddingDim]` shape.\n\n\n<details>\n<summary>üõ°Ô∏è Proposed fix (apply same pattern in Backward)</summary>\n\n```diff\n public Tensor<T> AddColumnEmbeddings(Tensor<T> featureEmbeddings)\n {\n+    if (featureEmbeddings.Shape.Length != 3)\n+    {\n+        throw new ArgumentException(\n+            $\"Expected a 3D tensor [batchSize, numColumns, embeddingDim] but got {featureEmbeddings.Shape.Length}D.\");\n+    }\n+\n     int batchSize = featureEmbeddings.Shape[0];\n     int numCols = featureEmbeddings.Shape[1];\n     int embDim = featureEmbeddings.Shape[2];\n```\n</details>\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        if (featureEmbeddings.Shape.Length != 3)\n        {\n            throw new ArgumentException(\n                $\"Expected a 3D tensor [batchSize, numColumns, embeddingDim] but got {featureEmbeddings.Shape.Length}D.\");\n        }\n\n        int batchSize = featureEmbeddings.Shape[0];\n        int numCols = featureEmbeddings.Shape[1];\n        int embDim = featureEmbeddings.Shape[2];\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/NeuralNetworks/Tabular/ColumnEmbedding.cs` around lines 107 - 109, Add a\nrank/shape validation before destructuring featureEmbeddings in\nAddColumnEmbeddings (and apply the same pattern in Backward): verify the tensor\nhas exactly 3 dimensions (e.g., check tensor.Rank == 3 or tensor.Shape.Length ==\n3) and if not throw a clear ArgumentException/InvalidOperationException that\nstates the expected shape is [batchSize, numColumns, embeddingDim]; perform this\ncheck at the top of AddColumnEmbeddings and Backward so subsequent accesses to\nShape[0], Shape[1], Shape[2] are safe and errors are informative.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:phoenix -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}},{"id":"PRRT_kwDOKSXUF85tKjoc","isResolved":false,"path":"src/NeuralNetworks/Tabular/ContrastivePretraining.cs","line":176,"comments":{"nodes":[{"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**`embDim` is validated against the wrong field ‚Äî should check `_projectionWeights.Shape[0]`, not `_numFeatures`.**\n\n`_numFeatures` is the number of raw input features (passed to `CorruptSamples`), whereas `embDim` here is the embedding dimension of the projected representations. The projection weight matrix is shaped `[embeddingDim, projectionDim]`, so the correct check is `embDim != _projectionWeights.Shape[0]`. In the common case where `embeddingDim ‚â† numFeatures`, this guard will reject every valid call.\n\n<details>\n<summary>Proposed fix</summary>\n\n```diff\n         int embDim = originalEmbeddings.Shape[1];\n-        if (embDim != _numFeatures)\n+        if (embDim != _projectionWeights.Shape[0])\n         {\n             throw new ArgumentException(\n-                $\"Embedding dimension ({embDim}) does not match expected ({_numFeatures}).\");\n+                $\"Embedding dimension ({embDim}) does not match expected ({_projectionWeights.Shape[0]}).\");\n         }\n```\n</details>\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn `@src/NeuralNetworks/Tabular/ContrastivePretraining.cs` around lines 171 - 176,\nThe current guard compares embDim (from originalEmbeddings.Shape[1]) to\n_numFeatures but should validate against the projection matrix row count; change\nthe condition to compare embDim != _projectionWeights.Shape[0] (and update the\nexception message to reference the projection embedding dimension rather than\n_numFeatures) in the method around the embDim check so valid embeddings that\ndiffer from raw feature count are accepted.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:phoenix -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->"}]}}],"pageInfo":{"hasNextPage":false,"endCursor":"Y3Vyc29yOnYyOpK0MjAyNi0wMi0wNlQxODoxMToyOFrObSo6HA=="}}}}}}