"{\"data\":{\"repository\":{\"pullRequest\":{\"reviewThreads\":{\"nodes\":[{\"id\":\"PRRT_kwDOKSXUF85gZFFg\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTWx1\",\"body\":\"The `_numOps` field is declared but never used in the Document class. Since `RelevanceScore` is a public property that can be set directly, and all numeric operations on scores happen in retriever/reranker classes, this field should be removed to avoid unnecessary memory allocation for every document instance.\",\"path\":\"src/RetrievalAugmentedGeneration/Models/Document.cs\",\"position\":1,\"diffHunk\":\"@@ -89,35 +92,44 @@ public class Document\\n     /// <para>\\n     /// The relevance score indicates how well this document matches a query.\\n     /// Higher scores indicate stronger relevance. The score scale and interpretation\\n-    /// depend on the retrieval or reranking algorithm used. This value is null for\\n-    /// documents that haven't been scored.\\n+    /// depend on the retrieval or reranking algorithm used. Use HasRelevanceScore to check\\n+    /// if a score has been assigned before accessing this value.\\n     /// </para>\\n     /// <para><b>For Beginners:</b> This is like a match percentage showing how relevant this document is.\\n     /// \\n     /// Think of it like search results:\\n     /// - Score 0.95: Almost perfect match, highly relevant\\n     /// - Score 0.50: Somewhat relevant\\n     /// - Score 0.10: Barely relevant\\n-    /// - null: Not scored yet\\n+    /// - Check HasRelevanceScore first to see if scored\\n     /// \\n     /// Documents with higher scores are more likely to contain the answer to your question.\\n     /// </para>\\n     /// </remarks>\\n-    public double? RelevanceScore { get; set; }\\n+    public T RelevanceScore { get; set; }\\n+\\n+    /// <summary>\\n+    /// Gets or sets whether this document has a relevance score assigned.\\n+    /// </summary>\\n+    public bool HasRelevanceScore { get; set; }\\n+\\n+    private readonly INumericOperations<T> _numOps;\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFFk\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTWx7\",\"body\":\"The null checks for `minScore` and `maxScore` are incorrect. Since `scores` is a `List<T>` and `Min()`/`Max()` return `T` (not `T?`), these values cannot be null for value types like `float` or `double`. For reference types, `Min()`/`Max()` could return null if the list contains nulls, but this would throw an exception earlier when adding to `docsWithScores`. These null checks should be removed as they're unreachable and misleading.\",\"path\":\"src/RetrievalAugmentedGeneration/Rerankers/RerankerBase.cs\",\"position\":1,\"diffHunk\":\"@@ -142,31 +150,41 @@ protected virtual void ValidateTopK(int topK)\\n     /// - Makes scores consistent with other retrievers/rerankers\\n     /// </para>\\n     /// </remarks>\\n-    protected IList<Document> NormalizeScores(IList<Document> documents)\\n+    protected IList<Document<T>> NormalizeScores(IList<Document<T>> documents)\\n     {\\n-        var docsWithScores = documents.Where(d => d.RelevanceScore.HasValue).ToList();\\n+        var docsWithScores = documents.Where(d => d.HasRelevanceScore).ToList();\\n         if (docsWithScores.Count == 0)\\n             return documents;\\n \\n-        var scores = docsWithScores.Select(d => d.RelevanceScore!.Value).ToList();\\n+        var scores = docsWithScores.Select(d => d.RelevanceScore).ToList();\\n+        if (scores.Count == 0)\\n+            return documents;\\n+\\n         var minScore = scores.Min();\\n         var maxScore = scores.Max();\\n-        var range = maxScore - minScore;\\n+        \\n+        if (minScore == null || maxScore == null)\\n+            return documents;\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFFr\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTWyF\",\"body\":\"The `Vector<T>` type used in the return type of `GenerateHypotheticalEmbedding` method (line 56) is not imported. Add `using AiDotNet.LinearAlgebra;` to resolve this missing namespace.\",\"path\":\"src/RetrievalAugmentedGeneration/QueryExpansion/HyDEQueryExpansion.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,89 @@\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+using AiDotNet.Helpers;\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFFv\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTWyK\",\"body\":\"The constructor accepts an `INumericOperations<T>` parameter but doesn't use it - the class uses the static `NumOps` field from the base class instead. Either remove the `numOps` parameter to match other rerankers (IdentityReranker, MaximalMarginalRelevanceReranker), or use the injected instance instead of the static field for better testability.\",\"path\":\"src/RetrievalAugmentedGeneration/Rerankers/DiversityReranker.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,220 @@\\n+using AiDotNet.Helpers;\\n+using AiDotNet.Interfaces;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.Rerankers;\\n+\\n+/// <summary>\\n+/// Reranks documents to maximize diversity while maintaining relevance.\\n+/// </summary>\\n+/// <typeparam name=\\\"T\\\">The numeric data type used for scoring.</typeparam>\\n+/// <remarks>\\n+/// <para>\\n+/// This reranker addresses the problem of redundant results by explicitly promoting diversity.\\n+/// It uses a greedy algorithm to select documents that are both relevant to the query and\\n+/// dissimilar from already-selected documents. This is similar to Maximal Marginal Relevance (MMR)\\n+/// but uses a simpler diversity metric based on text overlap.\\n+/// </para>\\n+/// <para><b>For Beginners:</b> This prevents showing the same information multiple times.\\n+/// \\n+/// The Problem:\\n+/// Imagine searching for \\\"Python programming\\\" and getting 10 results:\\n+/// - Result 1: \\\"Python is a programming language...\\\"\\n+/// - Result 2: \\\"Python is a programming language used for...\\\"\\n+/// - Result 3: \\\"Python programming language allows...\\\"\\n+/// - Result 4-10: More variations of the same thing\\n+/// \\n+/// That's redundant! You want variety:\\n+/// - Result 1: Python basics\\n+/// - Result 2: Python web development\\n+/// - Result 3: Python data science\\n+/// - Result 4: Python machine learning\\n+/// - Result 5: Python performance tips\\n+/// \\n+/// How it works:\\n+/// 1. Pick the most relevant document first\\n+/// 2. For remaining docs, balance two factors:\\n+///    a) Relevance to the query (should be useful)\\n+///    b) Difference from already-picked docs (should be unique)\\n+/// 3. Keep picking until you have enough results\\n+/// \\n+/// Diversity calculation:\\n+/// - Compares text overlap (how many words are shared)\\n+/// - Higher overlap = less diverse = lower score\\n+/// - Lower overlap = more diverse = higher score\\n+/// \\n+/// Lambda parameter (0 to 1):\\n+/// - lambda=1.0: Only care about relevance (might get duplicates)\\n+/// - lambda=0.0: Only care about diversity (might get irrelevant docs)\\n+/// - lambda=0.5: Balance both (recommended default)\\n+/// \\n+/// Real example with lambda=0.5:\\n+/// Query: \\\"climate change effects\\\"\\n+/// \\n+/// Step 1: Pick most relevant ΓåÆ \\\"Climate change causes rising temperatures\\\" (relevance: 0.9)\\n+/// Step 2: Next candidates:\\n+///   - \\\"Climate change leads to warmer weather\\\" (relevance: 0.85, similarity to picked: 0.7)\\n+///     ΓåÆ Score: 0.5 * 0.85 - 0.5 * 0.7 = 0.075\\n+///   - \\\"Ocean acidification from CO2\\\" (relevance: 0.7, similarity: 0.2)\\n+///     ΓåÆ Score: 0.5 * 0.7 - 0.5 * 0.2 = 0.25 Γ£ô Pick this!\\n+/// \\n+/// Result: You get coverage of temperature AND ocean effects, not just temperature twice!\\n+/// \\n+/// When to use this:\\n+/// - Search results where redundancy is common\\n+/// - Document recommendation systems\\n+/// - Exploratory searches where breadth matters\\n+/// - After initial retrieval that returns many similar docs\\n+/// </para>\\n+/// </remarks>\\n+public class DiversityReranker<T> : RerankerBase<T>\\n+{\\n+    private readonly T _lambda;\\n+\\n+    /// <summary>\\n+    /// Gets a value indicating whether this reranker modifies relevance scores.\\n+    /// </summary>\\n+    public override bool ModifiesScores => true;\\n+\\n+    /// <summary>\\n+    /// Initializes a new instance of the DiversityReranker class.\\n+    /// </summary>\\n+    /// <param name=\\\"numOps\\\">Numeric operations for type T.</param>\\n+    /// <param name=\\\"lambda\\\">Trade-off parameter between relevance and diversity (0 to 1). Default: 0.5.\\n+    /// Higher values favor relevance, lower values favor diversity.</param>\\n+    public DiversityReranker(INumericOperations<T> numOps, T? lambda = default) : base()\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFF1\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTWyT\",\"body\":\"The `_similarityThreshold` field is assigned in the constructor but never used in the `ChunkCore` method. The semantic chunking implementation doesn't actually use similarity calculations - it just splits by size. Either implement actual semantic similarity-based chunking using this threshold, or remove the unused field and parameter.\\n```suggestion\\n\\n\\n        /// <summary>\\n        /// Initializes a new instance of the <see cref=\\\"SemanticChunkingStrategy{T}\\\"/> class.\\n        /// </summary>\\n        /// <param name=\\\"numericOperations\\\">The numeric operations for type T.</param>\\n        /// <param name=\\\"maxChunkSize\\\">The maximum chunk size in characters.</param>\\n        /// <param name=\\\"chunkOverlap\\\">The chunk overlap in characters.</param>\\n        public SemanticChunkingStrategy(\\n            INumericOperations<T> numericOperations,\\n            int maxChunkSize = 1000,\\n            int chunkOverlap = 200)\\n            : base(maxChunkSize, chunkOverlap)\\n        {\\n            _numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\\n```\",\"path\":\"src/RetrievalAugmentedGeneration/ChunkingStrategies/SemanticChunkingStrategy.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,99 @@\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+using AiDotNet.Helpers;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies\\n+{\\n+    /// <summary>\\n+    /// Semantic-based text chunking that uses embeddings to group related content.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class SemanticChunkingStrategy<T> : ChunkingStrategyBase\\n+    {\\n+        private readonly INumericOperations<T> _numOps;\\n+        private readonly T _similarityThreshold;\\n+\\n+        /// <summary>\\n+        /// Initializes a new instance of the <see cref=\\\"SemanticChunkingStrategy{T}\\\"/> class.\\n+        /// </summary>\\n+        /// <param name=\\\"numericOperations\\\">The numeric operations for type T.</param>\\n+        /// <param name=\\\"similarityThreshold\\\">The similarity threshold for grouping sentences.</param>\\n+        /// <param name=\\\"maxChunkSize\\\">The maximum chunk size in characters.</param>\\n+        public SemanticChunkingStrategy(\\n+            INumericOperations<T> numericOperations,\\n+            T similarityThreshold,\\n+            int maxChunkSize = 1000,\\n+            int chunkOverlap = 200)\\n+            : base(maxChunkSize, chunkOverlap)\\n+        {\\n+            _numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\\n+            _similarityThreshold = similarityThreshold;\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFF8\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTWyb\",\"body\":\"This foreach loop [implicitly filters its target sequence](1) - consider filtering the sequence explicitly using '.Where(...)'.\\n```suggestion\\n                if (sentenceEndings.Any(ending => currentSentence.EndsWith(ending)))\\n                {\\n                    sentences.Add(currentSentence.Trim());\\n                    currentSentence = string.Empty;\\n```\",\"path\":\"src/RetrievalAugmentedGeneration/ChunkingStrategies/SemanticChunkingStrategy.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,99 @@\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+using AiDotNet.Helpers;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies\\n+{\\n+    /// <summary>\\n+    /// Semantic-based text chunking that uses embeddings to group related content.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class SemanticChunkingStrategy<T> : ChunkingStrategyBase\\n+    {\\n+        private readonly INumericOperations<T> _numOps;\\n+        private readonly T _similarityThreshold;\\n+\\n+        /// <summary>\\n+        /// Initializes a new instance of the <see cref=\\\"SemanticChunkingStrategy{T}\\\"/> class.\\n+        /// </summary>\\n+        /// <param name=\\\"numericOperations\\\">The numeric operations for type T.</param>\\n+        /// <param name=\\\"similarityThreshold\\\">The similarity threshold for grouping sentences.</param>\\n+        /// <param name=\\\"maxChunkSize\\\">The maximum chunk size in characters.</param>\\n+        public SemanticChunkingStrategy(\\n+            INumericOperations<T> numericOperations,\\n+            T similarityThreshold,\\n+            int maxChunkSize = 1000,\\n+            int chunkOverlap = 200)\\n+            : base(maxChunkSize, chunkOverlap)\\n+        {\\n+            _numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\\n+            _similarityThreshold = similarityThreshold;\\n+        }\\n+\\n+        /// <summary>\\n+        /// Chunks text based on semantic similarity between sentences.\\n+        /// </summary>\\n+        /// <param name=\\\"text\\\">The text to chunk.</param>\\n+        /// <returns>A collection of semantically coherent chunks with positions.</returns>\\n+        protected override IEnumerable<(string Chunk, int StartPosition, int EndPosition)> ChunkCore(string text)\\n+        {\\n+            var sentences = SplitIntoSentences(text);\\n+            var currentChunk = new List<string>();\\n+            var currentSize = 0;\\n+            var position = 0;\\n+\\n+            foreach (var sentence in sentences)\\n+            {\\n+                if (currentSize + sentence.Length > ChunkSize && currentChunk.Count > 0)\\n+                {\\n+                    var chunkText = string.Join(\\\" \\\", currentChunk);\\n+                    var endPos = position + chunkText.Length;\\n+                    yield return (chunkText, position, endPos);\\n+                    \\n+                    position = endPos - ChunkOverlap;\\n+                    currentChunk.Clear();\\n+                    currentSize = 0;\\n+                }\\n+\\n+                currentChunk.Add(sentence);\\n+                currentSize += sentence.Length;\\n+            }\\n+\\n+            if (currentChunk.Count > 0)\\n+            {\\n+                var chunkText = string.Join(\\\" \\\", currentChunk);\\n+                yield return (chunkText, position, position + chunkText.Length);\\n+            }\\n+        }\\n+\\n+        private List<string> SplitIntoSentences(string text)\\n+        {\\n+            var sentences = new List<string>();\\n+            var sentenceEndings = new[] { \\\". \\\", \\\"! \\\", \\\"? \\\", \\\".\\\\n\\\", \\\"!\\\\n\\\", \\\"?\\\\n\\\" };\\n+            var currentSentence = string.Empty;\\n+\\n+            for (int i = 0; i < text.Length; i++)\\n+            {\\n+                currentSentence += text[i];\\n+\\n+                foreach (var ending in sentenceEndings)\\n+                {\\n+                    if (currentSentence.EndsWith(ending))\\n+                    {\\n+                        sentences.Add(currentSentence.Trim());\\n+                        currentSentence = string.Empty;\\n+                        break;\\n+                    }\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFGF\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTWym\",\"body\":\"This foreach loop [implicitly filters its target sequence](1) - consider filtering the sequence explicitly using '.Where(...)'.\\n```suggestion\\n                if (sentenceEndings.Any(ending => currentSentence.EndsWith(ending)))\\n                {\\n                    sentences.Add(currentSentence.Trim());\\n                    currentSentence = string.Empty;\\n```\",\"path\":\"src/RetrievalAugmentedGeneration/ContextCompression/DocumentSummarizer.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,166 @@\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+using AiDotNet.Helpers;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.ContextCompression\\n+{\\n+    /// <summary>\\n+    /// Document summarizer for creating concise summaries of retrieved content.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class DocumentSummarizer<T>\\n+    {\\n+        private readonly INumericOperations<T> _numOps;\\n+        private readonly int _maxSummaryLength;\\n+        private readonly string _llmEndpoint;\\n+        private readonly string _apiKey;\\n+\\n+        /// <summary>\\n+        /// Initializes a new instance of the <see cref=\\\"DocumentSummarizer{T}\\\"/> class.\\n+        /// </summary>\\n+        /// <param name=\\\"numericOperations\\\">The numeric operations for type T.</param>\\n+        /// <param name=\\\"maxSummaryLength\\\">The maximum length of the summary in characters.</param>\\n+        /// <param name=\\\"llmEndpoint\\\">The LLM API endpoint.</param>\\n+        /// <param name=\\\"apiKey\\\">The API key for the LLM service.</param>\\n+        public DocumentSummarizer(\\n+            INumericOperations<T> numericOperations,\\n+            int maxSummaryLength = 500,\\n+            string llmEndpoint = \\\"\\\",\\n+            string apiKey = \\\"\\\")\\n+        {\\n+            _numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\\n+            _maxSummaryLength = maxSummaryLength > 0\\n+                ? maxSummaryLength\\n+                : throw new ArgumentOutOfRangeException(nameof(maxSummaryLength));\\n+            _llmEndpoint = llmEndpoint;\\n+            _apiKey = apiKey;\\n+        }\\n+\\n+        /// <summary>\\n+        /// Summarizes a list of documents.\\n+        /// </summary>\\n+        /// <param name=\\\"documents\\\">The documents to summarize.</param>\\n+        /// <returns>A list of summarized documents.</returns>\\n+        public List<Document<T>> Summarize(List<Document<T>> documents)\\n+        {\\n+            if (documents == null) throw new ArgumentNullException(nameof(documents));\\n+\\n+            var summarized = new List<Document<T>>();\\n+\\n+            foreach (var doc in documents)\\n+            {\\n+                var summary = SummarizeText(doc.Content);\\n+                var summarizedDoc = new Document<T>\\n+                {\\n+                    Id = doc.Id,\\n+                    Content = summary,\\n+                    Metadata = doc.Metadata,\\n+                    RelevanceScore = doc.RelevanceScore\\n+                };\\n+                summarized.Add(summarizedDoc);\\n+            }\\n+\\n+            return summarized;\\n+        }\\n+\\n+        /// <summary>\\n+        /// Summarizes text to a maximum length.\\n+        /// </summary>\\n+        /// <param name=\\\"text\\\">The text to summarize.</param>\\n+        /// <returns>The summarized text.</returns>\\n+        public string SummarizeText(string text)\\n+        {\\n+            if (string.IsNullOrEmpty(text)) return text;\\n+\\n+            if (text.Length <= _maxSummaryLength)\\n+            {\\n+                return text;\\n+            }\\n+\\n+            var sentences = SplitIntoSentences(text);\\n+            var importantSentences = ExtractImportantSentences(sentences);\\n+\\n+            var summary = string.Empty;\\n+            foreach (var sentence in importantSentences)\\n+            {\\n+                if (summary.Length + sentence.Length > _maxSummaryLength)\\n+                {\\n+                    break;\\n+                }\\n+                summary += sentence + \\\" \\\";\\n+            }\\n+\\n+            return summary.Trim();\\n+        }\\n+\\n+        private List<string> ExtractImportantSentences(List<string> sentences)\\n+        {\\n+            var scored = new List<(string sentence, double score)>();\\n+\\n+            foreach (var sentence in sentences)\\n+            {\\n+                var importance = ComputeImportance(sentence, sentences);\\n+                scored.Add((sentence, importance));\\n+            }\\n+\\n+            return scored\\n+                .OrderByDescending(x => x.score)\\n+                .Select(x => x.sentence)\\n+                .ToList();\\n+        }\\n+\\n+        private double ComputeImportance(string sentence, List<string> allSentences)\\n+        {\\n+            var tokens = Tokenize(sentence);\\n+            var uniqueTokens = tokens.Distinct().Count();\\n+            var length = sentence.Length;\\n+\\n+            var positionScore = allSentences.IndexOf(sentence) == 0 ? 1.5 : 1.0;\\n+\\n+            var importance = (uniqueTokens * 0.5) + (Math.Min(length, 200) / 200.0 * 0.5);\\n+            importance *= positionScore;\\n+\\n+            return importance;\\n+        }\\n+\\n+        private List<string> SplitIntoSentences(string text)\\n+        {\\n+            var sentences = new List<string>();\\n+            var sentenceEndings = new[] { \\\". \\\", \\\"! \\\", \\\"? \\\", \\\".\\\\n\\\", \\\"!\\\\n\\\", \\\"?\\\\n\\\" };\\n+            var currentSentence = string.Empty;\\n+\\n+            for (int i = 0; i < text.Length; i++)\\n+            {\\n+                currentSentence += text[i];\\n+\\n+                foreach (var ending in sentenceEndings)\\n+                {\\n+                    if (currentSentence.EndsWith(ending))\\n+                    {\\n+                        sentences.Add(currentSentence.Trim());\\n+                        currentSentence = string.Empty;\\n+                        break;\\n+                    }\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFGM\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTWyv\",\"body\":\"This foreach loop [implicitly filters its target sequence](1) - consider filtering the sequence explicitly using '.Where(...)'.\\n```suggestion\\n                var matchedEnding = sentenceEndings.FirstOrDefault(ending => currentSentence.EndsWith(ending));\\n                if (matchedEnding != null)\\n                {\\n                    sentences.Add(currentSentence.Trim());\\n                    currentSentence = string.Empty;\\n```\",\"path\":\"src/RetrievalAugmentedGeneration/ContextCompression/LLMContextCompressor.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,148 @@\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+using AiDotNet.Helpers;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.ContextCompression\\n+{\\n+    /// <summary>\\n+    /// LLM-based context compression to reduce token usage while preserving key information.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class LLMContextCompressor<T>\\n+    {\\n+        private readonly INumericOperations<T> _numOps;\\n+        private readonly string _llmEndpoint;\\n+        private readonly string _apiKey;\\n+        private readonly double _compressionRatio;\\n+\\n+        /// <summary>\\n+        /// Initializes a new instance of the <see cref=\\\"LLMContextCompressor{T}\\\"/> class.\\n+        /// </summary>\\n+        /// <param name=\\\"numericOperations\\\">The numeric operations for type T.</param>\\n+        /// <param name=\\\"compressionRatio\\\">The target compression ratio (0.0 to 1.0).</param>\\n+        /// <param name=\\\"llmEndpoint\\\">The LLM API endpoint.</param>\\n+        /// <param name=\\\"apiKey\\\">The API key for the LLM service.</param>\\n+        public LLMContextCompressor(\\n+            INumericOperations<T> numericOperations,\\n+            double compressionRatio = 0.5,\\n+            string llmEndpoint = \\\"\\\",\\n+            string apiKey = \\\"\\\")\\n+        {\\n+            _numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\\n+            _compressionRatio = compressionRatio >= 0 && compressionRatio <= 1\\n+                ? compressionRatio\\n+                : throw new ArgumentOutOfRangeException(nameof(compressionRatio), \\\"Compression ratio must be between 0 and 1\\\");\\n+            _llmEndpoint = llmEndpoint;\\n+            _apiKey = apiKey;\\n+        }\\n+\\n+        /// <summary>\\n+        /// Compresses a list of documents while preserving relevance to the query.\\n+        /// </summary>\\n+        /// <param name=\\\"query\\\">The query context.</param>\\n+        /// <param name=\\\"documents\\\">The documents to compress.</param>\\n+        /// <returns>A list of compressed documents.</returns>\\n+        public List<Document<T>> Compress(string query, List<Document<T>> documents)\\n+        {\\n+            if (string.IsNullOrEmpty(query)) throw new ArgumentNullException(nameof(query));\\n+            if (documents == null) throw new ArgumentNullException(nameof(documents));\\n+\\n+            var compressed = new List<Document<T>>();\\n+\\n+            foreach (var doc in documents)\\n+            {\\n+                var compressedContent = CompressText(query, doc.Content);\\n+                var compressedDoc = new Document<T>\\n+                {\\n+                    Id = doc.Id,\\n+                    Content = compressedContent,\\n+                    Metadata = doc.Metadata,\\n+                    RelevanceScore = doc.RelevanceScore\\n+                };\\n+                compressed.Add(compressedDoc);\\n+            }\\n+\\n+            return compressed;\\n+        }\\n+\\n+        /// <summary>\\n+        /// Compresses text based on relevance to the query.\\n+        /// </summary>\\n+        /// <param name=\\\"query\\\">The query context.</param>\\n+        /// <param name=\\\"text\\\">The text to compress.</param>\\n+        /// <returns>The compressed text.</returns>\\n+        public string CompressText(string query, string text)\\n+        {\\n+            if (string.IsNullOrEmpty(text)) return text;\\n+\\n+            var sentences = SplitIntoSentences(text);\\n+            var scoredSentences = new List<(string sentence, double score)>();\\n+\\n+            foreach (var sentence in sentences)\\n+            {\\n+                var relevance = ComputeRelevance(query, sentence);\\n+                scoredSentences.Add((sentence, relevance));\\n+            }\\n+\\n+            var targetCount = Math.Max(1, (int)(sentences.Count * _compressionRatio));\\n+            var topSentences = scoredSentences\\n+                .OrderByDescending(x => x.score)\\n+                .Take(targetCount)\\n+                .OrderBy(x => sentences.IndexOf(x.sentence))\\n+                .Select(x => x.sentence);\\n+\\n+            return string.Join(\\\" \\\", topSentences);\\n+        }\\n+\\n+        private double ComputeRelevance(string query, string sentence)\\n+        {\\n+            var queryTokens = Tokenize(query);\\n+            var sentenceTokens = Tokenize(sentence);\\n+\\n+            var overlap = queryTokens.Intersect(sentenceTokens).Count();\\n+            var total = Math.Max(queryTokens.Count, sentenceTokens.Count);\\n+\\n+            return total > 0 ? (double)overlap / total : 0.0;\\n+        }\\n+\\n+        private List<string> SplitIntoSentences(string text)\\n+        {\\n+            var sentences = new List<string>();\\n+            var sentenceEndings = new[] { \\\". \\\", \\\"! \\\", \\\"? \\\", \\\".\\\\n\\\", \\\"!\\\\n\\\", \\\"?\\\\n\\\" };\\n+            var currentSentence = string.Empty;\\n+\\n+            for (int i = 0; i < text.Length; i++)\\n+            {\\n+                currentSentence += text[i];\\n+\\n+                foreach (var ending in sentenceEndings)\\n+                {\\n+                    if (currentSentence.EndsWith(ending))\\n+                    {\\n+                        sentences.Add(currentSentence.Trim());\\n+                        currentSentence = string.Empty;\\n+                        break;\\n+                    }\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFGO\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTWy0\",\"body\":\"This foreach loop [implicitly filters its target sequence](1) - consider filtering the sequence explicitly using '.Where(...)'.\\n```suggestion\\n            foreach (var doc in vectorResults.Where(d => d.HasRelevanceScore))\\n            {\\n                var score = NumOps.Multiply(_vectorWeight, doc.RelevanceScore);\\n                combinedScores[doc.Id] = score;\\n```\",\"path\":\"src/RetrievalAugmentedGeneration/DocumentStores/HybridDocumentStore.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,124 @@\\n+using AiDotNet.Helpers;\\n+using AiDotNet.LinearAlgebra;\\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores\\n+{\\n+    /// <summary>\\n+    /// Hybrid document store combining vector and keyword search strategies.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class HybridDocumentStore<T> : DocumentStoreBase<T>\\n+    {\\n+        private readonly IDocumentStore<T> _vectorStore;\\n+        private readonly IDocumentStore<T> _keywordStore;\\n+        private readonly T _vectorWeight;\\n+        private readonly T _keywordWeight;\\n+\\n+        public override int DocumentCount => _vectorStore.DocumentCount;\\n+        public override int VectorDimension => _vectorStore.VectorDimension;\\n+\\n+        public HybridDocumentStore(\\n+            IDocumentStore<T> vectorStore,\\n+            IDocumentStore<T> keywordStore,\\n+            T vectorWeight,\\n+            T keywordWeight)\\n+        {\\n+            if (vectorStore == null)\\n+                throw new ArgumentNullException(nameof(vectorStore));\\n+            if (keywordStore == null)\\n+                throw new ArgumentNullException(nameof(keywordStore));\\n+\\n+            _vectorStore = vectorStore;\\n+            _keywordStore = keywordStore;\\n+            _vectorWeight = vectorWeight;\\n+            _keywordWeight = keywordWeight;\\n+        }\\n+\\n+        protected override void AddCore(VectorDocument<T> vectorDocument)\\n+        {\\n+            _vectorStore.Add(vectorDocument);\\n+            _keywordStore.Add(vectorDocument);\\n+        }\\n+\\n+        protected override void AddBatchCore(IList<VectorDocument<T>> vectorDocuments)\\n+        {\\n+            _vectorStore.AddBatch(vectorDocuments);\\n+            _keywordStore.AddBatch(vectorDocuments);\\n+        }\\n+\\n+        protected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)\\n+        {\\n+            var vectorResults = _vectorStore.GetSimilarWithFilters(queryVector, topK * 2, metadataFilters);\\n+            var keywordResults = _keywordStore.GetSimilarWithFilters(queryVector, topK * 2, metadataFilters);\\n+\\n+            var combinedScores = new Dictionary<string, T>();\\n+\\n+            foreach (var doc in vectorResults)\\n+            {\\n+                if (doc.HasRelevanceScore)\\n+                {\\n+                    var score = NumOps.Multiply(_vectorWeight, doc.RelevanceScore);\\n+                    combinedScores[doc.Id] = score;\\n+                }\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFGU\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTWy5\",\"body\":\"This foreach loop [implicitly filters its target sequence](1) - consider filtering the sequence explicitly using '.Where(...)'.\\n```suggestion\\n            foreach (var doc in vectorResults.Where(doc => doc.HasRelevanceScore))\\n            {\\n                var score = NumOps.Multiply(_vectorWeight, doc.RelevanceScore);\\n                combinedScores[doc.Id] = score;\\n            }\\n\\n            foreach (var doc in keywordResults.Where(doc => doc.HasRelevanceScore))\\n            {\\n                var keywordScore = NumOps.Multiply(_keywordWeight, doc.RelevanceScore);\\n                if (combinedScores.ContainsKey(doc.Id))\\n                {\\n                    combinedScores[doc.Id] = NumOps.Add(combinedScores[doc.Id], keywordScore);\\n                }\\n                else\\n                {\\n                    combinedScores[doc.Id] = keywordScore;\\n                }\\n```\",\"path\":\"src/RetrievalAugmentedGeneration/DocumentStores/HybridDocumentStore.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,124 @@\\n+using AiDotNet.Helpers;\\n+using AiDotNet.LinearAlgebra;\\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores\\n+{\\n+    /// <summary>\\n+    /// Hybrid document store combining vector and keyword search strategies.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class HybridDocumentStore<T> : DocumentStoreBase<T>\\n+    {\\n+        private readonly IDocumentStore<T> _vectorStore;\\n+        private readonly IDocumentStore<T> _keywordStore;\\n+        private readonly T _vectorWeight;\\n+        private readonly T _keywordWeight;\\n+\\n+        public override int DocumentCount => _vectorStore.DocumentCount;\\n+        public override int VectorDimension => _vectorStore.VectorDimension;\\n+\\n+        public HybridDocumentStore(\\n+            IDocumentStore<T> vectorStore,\\n+            IDocumentStore<T> keywordStore,\\n+            T vectorWeight,\\n+            T keywordWeight)\\n+        {\\n+            if (vectorStore == null)\\n+                throw new ArgumentNullException(nameof(vectorStore));\\n+            if (keywordStore == null)\\n+                throw new ArgumentNullException(nameof(keywordStore));\\n+\\n+            _vectorStore = vectorStore;\\n+            _keywordStore = keywordStore;\\n+            _vectorWeight = vectorWeight;\\n+            _keywordWeight = keywordWeight;\\n+        }\\n+\\n+        protected override void AddCore(VectorDocument<T> vectorDocument)\\n+        {\\n+            _vectorStore.Add(vectorDocument);\\n+            _keywordStore.Add(vectorDocument);\\n+        }\\n+\\n+        protected override void AddBatchCore(IList<VectorDocument<T>> vectorDocuments)\\n+        {\\n+            _vectorStore.AddBatch(vectorDocuments);\\n+            _keywordStore.AddBatch(vectorDocuments);\\n+        }\\n+\\n+        protected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)\\n+        {\\n+            var vectorResults = _vectorStore.GetSimilarWithFilters(queryVector, topK * 2, metadataFilters);\\n+            var keywordResults = _keywordStore.GetSimilarWithFilters(queryVector, topK * 2, metadataFilters);\\n+\\n+            var combinedScores = new Dictionary<string, T>();\\n+\\n+            foreach (var doc in vectorResults)\\n+            {\\n+                if (doc.HasRelevanceScore)\\n+                {\\n+                    var score = NumOps.Multiply(_vectorWeight, doc.RelevanceScore);\\n+                    combinedScores[doc.Id] = score;\\n+                }\\n+            }\\n+\\n+            foreach (var doc in keywordResults)\\n+            {\\n+                if (doc.HasRelevanceScore)\\n+                {\\n+                    var keywordScore = NumOps.Multiply(_keywordWeight, doc.RelevanceScore);\\n+                    if (combinedScores.ContainsKey(doc.Id))\\n+                    {\\n+                        combinedScores[doc.Id] = NumOps.Add(combinedScores[doc.Id], keywordScore);\\n+                    }\\n+                    else\\n+                    {\\n+                        combinedScores[doc.Id] = keywordScore;\\n+                    }\\n+                }\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFGW\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTWy8\",\"body\":\"This foreach loop [implicitly filters its target sequence](1) - consider filtering the sequence explicitly using '.Where(...)'.\",\"path\":\"src/RetrievalAugmentedGeneration/Generators/GeneratorBase.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,206 @@\\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.Generators;\\n+\\n+/// <summary>\\n+/// Base class for generator implementations providing common functionality and validation.\\n+/// </summary>\\n+/// <typeparam name=\\\"T\\\">The numeric data type used for relevance scoring.</typeparam>\\n+/// <remarks>\\n+/// <para>\\n+/// This base class provides standard validation, prompt construction, and citation handling\\n+/// for generator implementations. It defines the template for implementing custom generation logic.\\n+/// </para>\\n+/// <para><b>For Beginners:</b> This is the foundation for all text generators in RAG systems.\\n+/// \\n+/// It handles common tasks so you don't have to repeat them:\\n+/// - Checking that inputs aren't null or empty\\n+/// - Building prompts that combine the query and retrieved documents\\n+/// - Extracting citations from generated text\\n+/// - Creating properly formatted answers\\n+/// \\n+/// When you create a new generator (like OpenAIGenerator or OnnxGenerator):\\n+/// 1. Inherit from this class\\n+/// 2. Set MaxContextTokens and MaxGenerationTokens in the constructor\\n+/// 3. Implement GenerateCore with your specific generation logic\\n+/// 4. Everything else (validation, prompt formatting, citations) is handled automatically\\n+/// </para>\\n+/// </remarks>\\n+public abstract class GeneratorBase<T> : IGenerator<T>\\n+{\\n+    /// <summary>\\n+    /// Gets the maximum number of tokens this generator can process in a single request.\\n+    /// </summary>\\n+    public int MaxContextTokens { get; protected set; }\\n+\\n+    /// <summary>\\n+    /// Gets the maximum number of tokens this generator can generate in a response.\\n+    /// </summary>\\n+    public int MaxGenerationTokens { get; protected set; }\\n+\\n+    /// <summary>\\n+    /// Initializes a new instance of the GeneratorBase class.\\n+    /// </summary>\\n+    /// <param name=\\\"maxContextTokens\\\">The maximum context window size in tokens.</param>\\n+    /// <param name=\\\"maxGenerationTokens\\\">The maximum number of tokens to generate.</param>\\n+    protected GeneratorBase(int maxContextTokens, int maxGenerationTokens)\\n+    {\\n+        if (maxContextTokens <= 0)\\n+        {\\n+            throw new ArgumentException(\\\"Maximum context tokens must be positive.\\\", nameof(maxContextTokens));\\n+        }\\n+\\n+        if (maxGenerationTokens <= 0)\\n+        {\\n+            throw new ArgumentException(\\\"Maximum generation tokens must be positive.\\\", nameof(maxGenerationTokens));\\n+        }\\n+\\n+        MaxContextTokens = maxContextTokens;\\n+        MaxGenerationTokens = maxGenerationTokens;\\n+    }\\n+\\n+    /// <summary>\\n+    /// Generates a text response based on a prompt with validation.\\n+    /// </summary>\\n+    /// <param name=\\\"prompt\\\">The input prompt or question.</param>\\n+    /// <returns>The generated text response.</returns>\\n+    /// <exception cref=\\\"ArgumentNullException\\\">Thrown when prompt is null.</exception>\\n+    /// <exception cref=\\\"ArgumentException\\\">Thrown when prompt is empty or whitespace.</exception>\\n+    public string Generate(string prompt)\\n+    {\\n+        if (prompt == null)\\n+        {\\n+            throw new ArgumentNullException(nameof(prompt), \\\"Prompt cannot be null.\\\");\\n+        }\\n+\\n+        if (string.IsNullOrWhiteSpace(prompt))\\n+        {\\n+            throw new ArgumentException(\\\"Prompt cannot be empty or whitespace.\\\", nameof(prompt));\\n+        }\\n+\\n+        return GenerateCore(prompt);\\n+    }\\n+\\n+    /// <summary>\\n+    /// Generates a grounded answer using provided context documents.\\n+    /// </summary>\\n+    /// <param name=\\\"query\\\">The user's original query or question.</param>\\n+    /// <param name=\\\"context\\\">The retrieved documents providing context for the answer.</param>\\n+    /// <returns>A grounded answer with the generated text, source documents, and extracted citations.</returns>\\n+    /// <exception cref=\\\"ArgumentNullException\\\">Thrown when query or context is null.</exception>\\n+    /// <exception cref=\\\"ArgumentException\\\">Thrown when query is empty or context has no documents.</exception>\\n+    public GroundedAnswer<T> GenerateGrounded(string query, IEnumerable<Document<T>> context)\\n+    {\\n+        if (query == null)\\n+        {\\n+            throw new ArgumentNullException(nameof(query), \\\"Query cannot be null.\\\");\\n+        }\\n+\\n+        if (string.IsNullOrWhiteSpace(query))\\n+        {\\n+            throw new ArgumentException(\\\"Query cannot be empty or whitespace.\\\", nameof(query));\\n+        }\\n+\\n+        if (context == null)\\n+        {\\n+            throw new ArgumentNullException(nameof(context), \\\"Context cannot be null.\\\");\\n+        }\\n+\\n+        var contextList = context.ToList();\\n+        if (contextList.Count == 0)\\n+        {\\n+            throw new ArgumentException(\\\"Context must contain at least one document.\\\", nameof(context));\\n+        }\\n+\\n+        // Build the prompt with context\\n+        var prompt = BuildPromptWithContext(query, contextList);\\n+\\n+        // Generate the answer\\n+        var generatedText = GenerateCore(prompt);\\n+\\n+        // Extract citations from the generated text\\n+        var citations = ExtractCitations(generatedText, contextList);\\n+\\n+        return new GroundedAnswer<T>\\n+        {\\n+            Answer = generatedText,\\n+            SourceDocuments = contextList.AsReadOnly(),\\n+            Citations = citations.Values.Select(d => d.Id).ToList().AsReadOnly()\\n+        };\\n+    }\\n+\\n+    /// <summary>\\n+    /// Core generation logic to be implemented by derived classes.\\n+    /// </summary>\\n+    /// <param name=\\\"prompt\\\">The validated prompt string.</param>\\n+    /// <returns>The generated text response.</returns>\\n+    protected abstract string GenerateCore(string prompt);\\n+\\n+    /// <summary>\\n+    /// Builds a prompt that incorporates the query and retrieved context documents.\\n+    /// </summary>\\n+    /// <param name=\\\"query\\\">The user's query.</param>\\n+    /// <param name=\\\"context\\\">The retrieved context documents.</param>\\n+    /// <returns>A formatted prompt string.</returns>\\n+    /// <remarks>\\n+    /// <para>\\n+    /// This method can be overridden to customize prompt formatting. The default\\n+    /// implementation creates a structured prompt with numbered context documents\\n+    /// followed by the user's question.\\n+    /// </para>\\n+    /// </remarks>\\n+    protected virtual string BuildPromptWithContext(string query, List<Document<T>> context)\\n+    {\\n+        var promptBuilder = new System.Text.StringBuilder();\\n+        promptBuilder.AppendLine(\\\"Answer the following question based on the provided context. Include citations using [1], [2], etc. to reference the source documents.\\\");\\n+        promptBuilder.AppendLine();\\n+        promptBuilder.AppendLine(\\\"Context:\\\");\\n+\\n+        for (int i = 0; i < context.Count; i++)\\n+        {\\n+            promptBuilder.AppendLine($\\\"[{i + 1}] {context[i].Content}\\\");\\n+            promptBuilder.AppendLine();\\n+        }\\n+\\n+        promptBuilder.AppendLine(\\\"Question:\\\");\\n+        promptBuilder.AppendLine(query);\\n+        promptBuilder.AppendLine();\\n+        promptBuilder.Append(\\\"Answer:\\\");\\n+\\n+        return promptBuilder.ToString();\\n+    }\\n+\\n+    /// <summary>\\n+    /// Extracts citation markers from the generated text and maps them to source documents.\\n+    /// </summary>\\n+    /// <param name=\\\"generatedText\\\">The generated text containing citations.</param>\\n+    /// <param name=\\\"sourceDocuments\\\">The source documents that were used for generation.</param>\\n+    /// <returns>A dictionary mapping citation indices to documents.</returns>\\n+    /// <remarks>\\n+    /// <para>\\n+    /// This method can be overridden to customize citation extraction logic. The default\\n+    /// implementation looks for patterns like [1], [2], etc. in the generated text.\\n+    /// </para>\\n+    /// </remarks>\\n+    protected virtual Dictionary<int, Document<T>> ExtractCitations(string generatedText, List<Document<T>> sourceDocuments)\\n+    {\\n+        var citations = new Dictionary<int, Document<T>>();\\n+        var citationPattern = new System.Text.RegularExpressions.Regex(@\\\"\\\\[(\\\\d+)\\\\]\\\");\\n+        var matches = citationPattern.Matches(generatedText);\\n+\\n+        foreach (System.Text.RegularExpressions.Match match in matches)\\n+        {\\n+            if (int.TryParse(match.Groups[1].Value, out int citationIndex))\\n+            {\\n+                var docIndex = citationIndex - 1; // Convert to 0-based index\\n+                if (docIndex >= 0 && docIndex < sourceDocuments.Count)\\n+                {\\n+                    citations[citationIndex] = sourceDocuments[docIndex];\\n+                }\\n+            }\\n+        }\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFGb\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTWzD\",\"body\":\"This foreach loop [implicitly filters its target sequence](1) - consider filtering the sequence explicitly using '.Where(...)'.\",\"path\":\"src/RetrievalAugmentedGeneration/RerankingStrategies/ReciprocalRankFusion.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,122 @@\\n+using System;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+using AiDotNet.Helpers;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.RerankingStrategies\\n+{\\n+    /// <summary>\\n+    /// Reciprocal Rank Fusion for combining multiple ranking lists.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class ReciprocalRankFusion<T> : RerankingStrategyBase<T>\\n+    {\\n+        private readonly INumericOperations<T> _numOps;\\n+        private readonly int _k;\\n+\\n+        /// <summary>\\n+        /// Initializes a new instance of the <see cref=\\\"ReciprocalRankFusion{T}\\\"/> class.\\n+        /// </summary>\\n+        /// <param name=\\\"numericOperations\\\">The numeric operations for type T.</param>\\n+        /// <param name=\\\"k\\\">The constant k for reciprocal rank formula (default: 60).</param>\\n+        public ReciprocalRankFusion(INumericOperations<T> numericOperations, int k = 60) : base(numericOperations)\\n+        {\\n+            _numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\\n+            _k = k > 0 ? k : throw new ArgumentOutOfRangeException(nameof(k));\\n+        }\\n+\\n+        /// <summary>\\n+        /// Reranks documents using reciprocal rank fusion.\\n+        /// </summary>\\n+        /// <param name=\\\"query\\\">The query string.</param>\\n+        /// <param name=\\\"documents\\\">The documents to rerank.</param>\\n+        /// <param name=\\\"topK\\\">The number of top documents to return.</param>\\n+        /// <returns>A reranked list of documents.</returns>\\n+        public override List<Document<T>> Rerank(string query, List<Document<T>> documents, int topK)\\n+        {\\n+            if (string.IsNullOrEmpty(query)) throw new ArgumentNullException(nameof(query));\\n+            if (documents == null) throw new ArgumentNullException(nameof(documents));\\n+            if (topK <= 0) throw new ArgumentOutOfRangeException(nameof(topK));\\n+\\n+            var scores = new Dictionary<string, T>();\\n+\\n+            for (int rank = 0; rank < documents.Count; rank++)\\n+            {\\n+                var doc = documents[rank];\\n+                var rrfScore = _numOps.FromDouble(1.0 / (_k + rank + 1));\\n+\\n+                if (scores.ContainsKey(doc.Id))\\n+                {\\n+                    scores[doc.Id] = _numOps.Add(scores[doc.Id], rrfScore);\\n+                }\\n+                else\\n+                {\\n+                    scores[doc.Id] = rrfScore;\\n+                }\\n+            }\\n+\\n+            var reranked = documents\\n+                .OrderByDescending(d => Convert.ToDouble(scores.ContainsKey(d.Id) ? scores[d.Id] : _numOps.Zero))\\n+                .Take(topK)\\n+                .ToList();\\n+\\n+            foreach (var doc in reranked)\\n+            {\\n+                if (scores.ContainsKey(doc.Id))\\n+                {\\n+                    doc.RelevanceScore = scores[doc.Id];\\n+                }\\n+            }\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFGd\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTWzH\",\"body\":\"This foreach loop [implicitly filters its target sequence](1) - consider filtering the sequence explicitly using '.Where(...)'.\",\"path\":\"src/RetrievalAugmentedGeneration/Retrievers/BM25Retriever.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,124 @@\\n+using AiDotNet.Helpers;\\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\\n+{\\n+    /// <summary>\\n+    /// BM25 (Best Matching 25) retrieval algorithm for sparse keyword-based search.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class BM25Retriever<T> : RetrieverBase<T>\\n+    {\\n+        private readonly IDocumentStore<T> _documentStore;\\n+        private readonly T _k1;\\n+        private readonly T _b;\\n+        private readonly Dictionary<string, Dictionary<string, T>> _termFrequencies;\\n+        private readonly Dictionary<string, T> _documentLengths;\\n+        private T _avgDocLength;\\n+\\n+        public BM25Retriever(IDocumentStore<T> documentStore, int defaultTopK = 5, double k1 = 1.5, double b = 0.75) \\n+            : base(defaultTopK)\\n+        {\\n+            if (documentStore == null)\\n+                throw new ArgumentNullException(nameof(documentStore));\\n+\\n+            _documentStore = documentStore;\\n+            _k1 = NumOps.FromDouble(k1);\\n+            _b = NumOps.FromDouble(b);\\n+            _termFrequencies = new Dictionary<string, Dictionary<string, T>>();\\n+            _documentLengths = new Dictionary<string, T>();\\n+            _avgDocLength = NumOps.Zero;\\n+        }\\n+\\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\\n+        {\\n+            var queryTerms = Tokenize(query);\\n+            var scores = new Dictionary<string, T>();\\n+\\n+            var candidates = _documentStore.GetSimilar(\\n+                new AiDotNet.LinearAlgebra.Vector<T>(new T[Math.Max(1, _documentStore.VectorDimension)]),\\n+                _documentStore.DocumentCount\\n+            );\\n+\\n+            foreach (var doc in candidates)\\n+            {\\n+                if (!MatchesFilters(doc, metadataFilters))\\n+                    continue;\\n+\\n+                var score = NumOps.Zero;\\n+                \\n+                foreach (var term in queryTerms)\\n+                {\\n+                    var termScore = CalculateBM25Term(doc.Id, term);\\n+                    score = NumOps.Add(score, termScore);\\n+                }\\n+\\n+                scores[doc.Id] = score;\\n+            }\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFGi\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTWzO\",\"body\":\"This foreach loop [implicitly filters its target sequence](1) - consider filtering the sequence explicitly using '.Where(...)'.\",\"path\":\"src/RetrievalAugmentedGeneration/Retrievers/HybridRetriever.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,92 @@\\n+using AiDotNet.Helpers;\\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\\n+{\\n+    /// <summary>\\n+    /// Hybrid retriever combining dense and sparse retrieval strategies.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class HybridRetriever<T> : RetrieverBase<T>\\n+    {\\n+        private readonly IRetriever<T> _denseRetriever;\\n+        private readonly IRetriever<T> _sparseRetriever;\\n+        private readonly T _denseWeight;\\n+        private readonly T _sparseWeight;\\n+\\n+        public HybridRetriever(\\n+            IRetriever<T> denseRetriever, \\n+            IRetriever<T> sparseRetriever, \\n+            double denseWeight = 0.7, \\n+            double sparseWeight = 0.3,\\n+            int defaultTopK = 5) \\n+            : base(defaultTopK)\\n+        {\\n+            if (denseRetriever == null)\\n+                throw new ArgumentNullException(nameof(denseRetriever));\\n+            if (sparseRetriever == null)\\n+                throw new ArgumentNullException(nameof(sparseRetriever));\\n+\\n+            _denseRetriever = denseRetriever;\\n+            _sparseRetriever = sparseRetriever;\\n+            _denseWeight = NumOps.FromDouble(denseWeight);\\n+            _sparseWeight = NumOps.FromDouble(sparseWeight);\\n+        }\\n+\\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\\n+        {\\n+            var denseResults = _denseRetriever.Retrieve(query, topK * 2, metadataFilters);\\n+            var sparseResults = _sparseRetriever.Retrieve(query, topK * 2, metadataFilters);\\n+\\n+            var combinedScores = new Dictionary<string, T>();\\n+\\n+            foreach (var doc in denseResults)\\n+            {\\n+                if (doc.HasRelevanceScore)\\n+                {\\n+                    var score = NumOps.Multiply(_denseWeight, doc.RelevanceScore);\\n+                    combinedScores[doc.Id] = score;\\n+                }\\n+            }\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFGm\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTWzT\",\"body\":\"This foreach loop [implicitly filters its target sequence](1) - consider filtering the sequence explicitly using '.Where(...)'.\",\"path\":\"src/RetrievalAugmentedGeneration/Retrievers/HybridRetriever.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,92 @@\\n+using AiDotNet.Helpers;\\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\\n+{\\n+    /// <summary>\\n+    /// Hybrid retriever combining dense and sparse retrieval strategies.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class HybridRetriever<T> : RetrieverBase<T>\\n+    {\\n+        private readonly IRetriever<T> _denseRetriever;\\n+        private readonly IRetriever<T> _sparseRetriever;\\n+        private readonly T _denseWeight;\\n+        private readonly T _sparseWeight;\\n+\\n+        public HybridRetriever(\\n+            IRetriever<T> denseRetriever, \\n+            IRetriever<T> sparseRetriever, \\n+            double denseWeight = 0.7, \\n+            double sparseWeight = 0.3,\\n+            int defaultTopK = 5) \\n+            : base(defaultTopK)\\n+        {\\n+            if (denseRetriever == null)\\n+                throw new ArgumentNullException(nameof(denseRetriever));\\n+            if (sparseRetriever == null)\\n+                throw new ArgumentNullException(nameof(sparseRetriever));\\n+\\n+            _denseRetriever = denseRetriever;\\n+            _sparseRetriever = sparseRetriever;\\n+            _denseWeight = NumOps.FromDouble(denseWeight);\\n+            _sparseWeight = NumOps.FromDouble(sparseWeight);\\n+        }\\n+\\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\\n+        {\\n+            var denseResults = _denseRetriever.Retrieve(query, topK * 2, metadataFilters);\\n+            var sparseResults = _sparseRetriever.Retrieve(query, topK * 2, metadataFilters);\\n+\\n+            var combinedScores = new Dictionary<string, T>();\\n+\\n+            foreach (var doc in denseResults)\\n+            {\\n+                if (doc.HasRelevanceScore)\\n+                {\\n+                    var score = NumOps.Multiply(_denseWeight, doc.RelevanceScore);\\n+                    combinedScores[doc.Id] = score;\\n+                }\\n+            }\\n+\\n+            foreach (var doc in sparseResults)\\n+            {\\n+                if (doc.HasRelevanceScore)\\n+                {\\n+                    var sparseScore = NumOps.Multiply(_sparseWeight, doc.RelevanceScore);\\n+                    if (combinedScores.ContainsKey(doc.Id))\\n+                    {\\n+                        combinedScores[doc.Id] = NumOps.Add(combinedScores[doc.Id], sparseScore);\\n+                    }\\n+                    else\\n+                    {\\n+                        combinedScores[doc.Id] = sparseScore;\\n+                    }\\n+                }\\n+            }\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFGp\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTWzX\",\"body\":\"This foreach loop [implicitly filters its target sequence](1) - consider filtering the sequence explicitly using '.Where(...)'.\",\"path\":\"src/RetrievalAugmentedGeneration/Retrievers/MultiQueryRetriever.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,83 @@\\n+using AiDotNet.Helpers;\\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\\n+{\\n+    /// <summary>\\n+    /// Multi-query retriever that generates multiple query variations and merges results.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class MultiQueryRetriever<T> : RetrieverBase<T>\\n+    {\\n+        private readonly IRetriever<T> _baseRetriever;\\n+        private readonly int _numQueries;\\n+\\n+        public MultiQueryRetriever(IRetriever<T> baseRetriever, int numQueries = 3, int defaultTopK = 5) : base(defaultTopK)\\n+        {\\n+            if (baseRetriever == null)\\n+                throw new ArgumentNullException(nameof(baseRetriever));\\n+            if (numQueries <= 0)\\n+                throw new ArgumentException(\\\"Number of queries must be positive\\\", nameof(numQueries));\\n+\\n+            _baseRetriever = baseRetriever;\\n+            _numQueries = numQueries;\\n+        }\\n+\\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\\n+        {\\n+            var queries = GenerateQueries(query);\\n+            var allResults = new Dictionary<string, (Document<T> doc, T score)>();\\n+\\n+            foreach (var q in queries)\\n+            {\\n+                var results = _baseRetriever.Retrieve(q, topK, metadataFilters);\\n+\\n+                foreach (var doc in results)\\n+                {\\n+                    if (doc.HasRelevanceScore)\\n+                    {\\n+                        if (allResults.ContainsKey(doc.Id))\\n+                        {\\n+                            var existing = allResults[doc.Id];\\n+                            var newScore = NumOps.Add(existing.score, doc.RelevanceScore);\\n+                            allResults[doc.Id] = (doc, newScore);\\n+                        }\\n+                        else\\n+                        {\\n+                            allResults[doc.Id] = (doc, doc.RelevanceScore);\\n+                        }\\n+                    }\\n+                }\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFGu\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTWzc\",\"body\":\"This foreach loop [implicitly filters its target sequence](1) - consider filtering the sequence explicitly using '.Where(...)'.\",\"path\":\"src/RetrievalAugmentedGeneration/Retrievers/TFIDFRetriever.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,108 @@\\n+using AiDotNet.Helpers;\\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\\n+{\\n+    /// <summary>\\n+    /// TF-IDF (Term Frequency-Inverse Document Frequency) retrieval strategy.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class TFIDFRetriever<T> : RetrieverBase<T>\\n+    {\\n+        private readonly IDocumentStore<T> _documentStore;\\n+        private readonly Dictionary<string, Dictionary<string, T>> _tfidf;\\n+        private readonly Dictionary<string, T> _idf;\\n+\\n+        public TFIDFRetriever(IDocumentStore<T> documentStore, int defaultTopK = 5) : base(defaultTopK)\\n+        {\\n+            if (documentStore == null)\\n+                throw new ArgumentNullException(nameof(documentStore));\\n+                \\n+            _documentStore = documentStore;\\n+            _tfidf = new Dictionary<string, Dictionary<string, T>>();\\n+            _idf = new Dictionary<string, T>();\\n+        }\\n+\\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\\n+        {\\n+            var queryTerms = Tokenize(query);\\n+            var scores = new Dictionary<string, T>();\\n+\\n+            var candidates = _documentStore.GetSimilar(\\n+                new AiDotNet.LinearAlgebra.Vector<T>(new T[_documentStore.VectorDimension]), \\n+                _documentStore.DocumentCount\\n+            );\\n+\\n+            foreach (var doc in candidates)\\n+            {\\n+                if (!MatchesFilters(doc, metadataFilters))\\n+                    continue;\\n+\\n+                var score = NumOps.Zero;\\n+\\n+                if (_tfidf.ContainsKey(doc.Id))\\n+                {\\n+                    var docTfidf = _tfidf[doc.Id];\\n+                    foreach (var term in queryTerms)\\n+                    {\\n+                        if (docTfidf.ContainsKey(term))\\n+                        {\\n+                            score = NumOps.Add(score, docTfidf[term]);\\n+                        }\\n+                    }\\n+                }\\n+\\n+                scores[doc.Id] = score;\\n+            }\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFGx\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTWzj\",\"body\":\"This foreach loop [implicitly filters its target sequence](1) - consider filtering the sequence explicitly using '.Where(...)'.\",\"path\":\"src/RetrievalAugmentedGeneration/Retrievers/TFIDFRetriever.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,108 @@\\n+using AiDotNet.Helpers;\\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\\n+{\\n+    /// <summary>\\n+    /// TF-IDF (Term Frequency-Inverse Document Frequency) retrieval strategy.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class TFIDFRetriever<T> : RetrieverBase<T>\\n+    {\\n+        private readonly IDocumentStore<T> _documentStore;\\n+        private readonly Dictionary<string, Dictionary<string, T>> _tfidf;\\n+        private readonly Dictionary<string, T> _idf;\\n+\\n+        public TFIDFRetriever(IDocumentStore<T> documentStore, int defaultTopK = 5) : base(defaultTopK)\\n+        {\\n+            if (documentStore == null)\\n+                throw new ArgumentNullException(nameof(documentStore));\\n+                \\n+            _documentStore = documentStore;\\n+            _tfidf = new Dictionary<string, Dictionary<string, T>>();\\n+            _idf = new Dictionary<string, T>();\\n+        }\\n+\\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\\n+        {\\n+            var queryTerms = Tokenize(query);\\n+            var scores = new Dictionary<string, T>();\\n+\\n+            var candidates = _documentStore.GetSimilar(\\n+                new AiDotNet.LinearAlgebra.Vector<T>(new T[_documentStore.VectorDimension]), \\n+                _documentStore.DocumentCount\\n+            );\\n+\\n+            foreach (var doc in candidates)\\n+            {\\n+                if (!MatchesFilters(doc, metadataFilters))\\n+                    continue;\\n+\\n+                var score = NumOps.Zero;\\n+\\n+                if (_tfidf.ContainsKey(doc.Id))\\n+                {\\n+                    var docTfidf = _tfidf[doc.Id];\\n+                    foreach (var term in queryTerms)\\n+                    {\\n+                        if (docTfidf.ContainsKey(term))\\n+                        {\\n+                            score = NumOps.Add(score, docTfidf[term]);\\n+                        }\\n+                    }\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFG1\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTWzm\",\"body\":\"This foreach loop immediately [maps its iteration variable to another variable](1) - consider mapping the sequence explicitly using '.Select(...)'.\",\"path\":\"src/RetrievalAugmentedGeneration/Retrievers/BM25Retriever.cs\",\"position\":47,\"diffHunk\":\"@@ -0,0 +1,124 @@\\n+using AiDotNet.Helpers;\\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\\n+{\\n+    /// <summary>\\n+    /// BM25 (Best Matching 25) retrieval algorithm for sparse keyword-based search.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class BM25Retriever<T> : RetrieverBase<T>\\n+    {\\n+        private readonly IDocumentStore<T> _documentStore;\\n+        private readonly T _k1;\\n+        private readonly T _b;\\n+        private readonly Dictionary<string, Dictionary<string, T>> _termFrequencies;\\n+        private readonly Dictionary<string, T> _documentLengths;\\n+        private T _avgDocLength;\\n+\\n+        public BM25Retriever(IDocumentStore<T> documentStore, int defaultTopK = 5, double k1 = 1.5, double b = 0.75) \\n+            : base(defaultTopK)\\n+        {\\n+            if (documentStore == null)\\n+                throw new ArgumentNullException(nameof(documentStore));\\n+\\n+            _documentStore = documentStore;\\n+            _k1 = NumOps.FromDouble(k1);\\n+            _b = NumOps.FromDouble(b);\\n+            _termFrequencies = new Dictionary<string, Dictionary<string, T>>();\\n+            _documentLengths = new Dictionary<string, T>();\\n+            _avgDocLength = NumOps.Zero;\\n+        }\\n+\\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\\n+        {\\n+            var queryTerms = Tokenize(query);\\n+            var scores = new Dictionary<string, T>();\\n+\\n+            var candidates = _documentStore.GetSimilar(\\n+                new AiDotNet.LinearAlgebra.Vector<T>(new T[Math.Max(1, _documentStore.VectorDimension)]),\\n+                _documentStore.DocumentCount\\n+            );\\n+\\n+            foreach (var doc in candidates)\\n+            {\\n+                if (!MatchesFilters(doc, metadataFilters))\\n+                    continue;\\n+\\n+                var score = NumOps.Zero;\\n+                \\n+                foreach (var term in queryTerms)\\n+                {\\n+                    var termScore = CalculateBM25Term(doc.Id, term);\\n+                    score = NumOps.Add(score, termScore);\\n+                }\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFG6\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTWzu\",\"body\":\"This foreach loop immediately [maps its iteration variable to another variable](1) - consider mapping the sequence explicitly using '.Select(...)'.\",\"path\":\"src/RetrievalAugmentedGeneration/Retrievers/MultiQueryRetriever.cs\",\"position\":51,\"diffHunk\":\"@@ -0,0 +1,83 @@\\n+using AiDotNet.Helpers;\\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\\n+{\\n+    /// <summary>\\n+    /// Multi-query retriever that generates multiple query variations and merges results.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class MultiQueryRetriever<T> : RetrieverBase<T>\\n+    {\\n+        private readonly IRetriever<T> _baseRetriever;\\n+        private readonly int _numQueries;\\n+\\n+        public MultiQueryRetriever(IRetriever<T> baseRetriever, int numQueries = 3, int defaultTopK = 5) : base(defaultTopK)\\n+        {\\n+            if (baseRetriever == null)\\n+                throw new ArgumentNullException(nameof(baseRetriever));\\n+            if (numQueries <= 0)\\n+                throw new ArgumentException(\\\"Number of queries must be positive\\\", nameof(numQueries));\\n+\\n+            _baseRetriever = baseRetriever;\\n+            _numQueries = numQueries;\\n+        }\\n+\\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\\n+        {\\n+            var queries = GenerateQueries(query);\\n+            var allResults = new Dictionary<string, (Document<T> doc, T score)>();\\n+\\n+            foreach (var q in queries)\\n+            {\\n+                var results = _baseRetriever.Retrieve(q, topK, metadataFilters);\\n+\\n+                foreach (var doc in results)\\n+                {\\n+                    if (doc.HasRelevanceScore)\\n+                    {\\n+                        if (allResults.ContainsKey(doc.Id))\\n+                        {\\n+                            var existing = allResults[doc.Id];\\n+                            var newScore = NumOps.Add(existing.score, doc.RelevanceScore);\\n+                            allResults[doc.Id] = (doc, newScore);\\n+                        }\\n+                        else\\n+                        {\\n+                            allResults[doc.Id] = (doc, doc.RelevanceScore);\\n+                        }\\n+                    }\\n+                }\\n+            }\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFG_\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTWzy\",\"body\":\"The contents of this container are never accessed.\",\"path\":\"src/RetrievalAugmentedGeneration/Retrievers/TFIDFRetriever.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,108 @@\\n+using AiDotNet.Helpers;\\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\\n+{\\n+    /// <summary>\\n+    /// TF-IDF (Term Frequency-Inverse Document Frequency) retrieval strategy.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class TFIDFRetriever<T> : RetrieverBase<T>\\n+    {\\n+        private readonly IDocumentStore<T> _documentStore;\\n+        private readonly Dictionary<string, Dictionary<string, T>> _tfidf;\\n+        private readonly Dictionary<string, T> _idf;\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFHD\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTWz5\",\"body\":\"The contents of this container are never initialized.\",\"path\":\"src/RetrievalAugmentedGeneration/Retrievers/BM25Retriever.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,124 @@\\n+using AiDotNet.Helpers;\\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\\n+{\\n+    /// <summary>\\n+    /// BM25 (Best Matching 25) retrieval algorithm for sparse keyword-based search.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class BM25Retriever<T> : RetrieverBase<T>\\n+    {\\n+        private readonly IDocumentStore<T> _documentStore;\\n+        private readonly T _k1;\\n+        private readonly T _b;\\n+        private readonly Dictionary<string, Dictionary<string, T>> _termFrequencies;\\n+        private readonly Dictionary<string, T> _documentLengths;\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFHG\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTW0B\",\"body\":\"The contents of this container are never initialized.\",\"path\":\"src/RetrievalAugmentedGeneration/Retrievers/BM25Retriever.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,124 @@\\n+using AiDotNet.Helpers;\\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\\n+{\\n+    /// <summary>\\n+    /// BM25 (Best Matching 25) retrieval algorithm for sparse keyword-based search.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class BM25Retriever<T> : RetrieverBase<T>\\n+    {\\n+        private readonly IDocumentStore<T> _documentStore;\\n+        private readonly T _k1;\\n+        private readonly T _b;\\n+        private readonly Dictionary<string, Dictionary<string, T>> _termFrequencies;\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFHJ\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTW0I\",\"body\":\"The contents of this container are never initialized.\",\"path\":\"src/RetrievalAugmentedGeneration/Retrievers/TFIDFRetriever.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,108 @@\\n+using AiDotNet.Helpers;\\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\\n+{\\n+    /// <summary>\\n+    /// TF-IDF (Term Frequency-Inverse Document Frequency) retrieval strategy.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class TFIDFRetriever<T> : RetrieverBase<T>\\n+    {\\n+        private readonly IDocumentStore<T> _documentStore;\\n+        private readonly Dictionary<string, Dictionary<string, T>> _tfidf;\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFHM\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTW0L\",\"body\":\"Possible overflow: result of integer multiplication cast to double.\",\"path\":\"src/RetrievalAugmentedGeneration/EmbeddingModels/HuggingFaceEmbeddingModel.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,73 @@\\n+using AiDotNet.Helpers;\\n+using AiDotNet.LinearAlgebra;\\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\\n+using System;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels\\n+{\\n+    /// <summary>\\n+    /// HuggingFace-based embedding model for generating embeddings.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class HuggingFaceEmbeddingModel<T> : EmbeddingModelBase<T>\\n+    {\\n+        private readonly string _modelName;\\n+        private readonly string _apiKey;\\n+        private readonly int _dimension;\\n+        private readonly int _maxTokens;\\n+\\n+        public override int EmbeddingDimension => _dimension;\\n+        public override int MaxTokens => _maxTokens;\\n+\\n+        public HuggingFaceEmbeddingModel(string modelName, string apiKey = \\\"\\\", int dimension = 768, int maxTokens = 512)\\n+        {\\n+            if (string.IsNullOrWhiteSpace(modelName))\\n+                throw new ArgumentException(\\\"Model name cannot be empty\\\", nameof(modelName));\\n+            if (dimension <= 0)\\n+                throw new ArgumentException(\\\"Dimension must be positive\\\", nameof(dimension));\\n+            if (maxTokens <= 0)\\n+                throw new ArgumentException(\\\"Max tokens must be positive\\\", nameof(maxTokens));\\n+\\n+            _modelName = modelName;\\n+            _apiKey = apiKey ?? string.Empty;\\n+            _dimension = dimension;\\n+            _maxTokens = maxTokens;\\n+        }\\n+\\n+        protected override Vector<T> EmbedCore(string text)\\n+        {\\n+            var values = new T[_dimension];\\n+            var hash = text.GetHashCode();\\n+            \\n+            for (int i = 0; i < _dimension; i++)\\n+            {\\n+                var val = NumOps.FromDouble(Math.Cos(hash * (i + 1) * 0.002));\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFHR\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTW0S\",\"body\":\"Possible overflow: result of integer multiplication cast to double.\",\"path\":\"src/RetrievalAugmentedGeneration/EmbeddingModels/LocalTransformerEmbedding.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,71 @@\\n+using AiDotNet.Helpers;\\n+using AiDotNet.LinearAlgebra;\\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\\n+using System;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels\\n+{\\n+    /// <summary>\\n+    /// Local transformer embedding model for generating embeddings without external API calls.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class LocalTransformerEmbedding<T> : EmbeddingModelBase<T>\\n+    {\\n+        private readonly string _modelPath;\\n+        private readonly int _dimension;\\n+        private readonly int _maxTokens;\\n+\\n+        public override int EmbeddingDimension => _dimension;\\n+        public override int MaxTokens => _maxTokens;\\n+\\n+        public LocalTransformerEmbedding(string modelPath, int dimension = 384, int maxTokens = 512)\\n+        {\\n+            if (string.IsNullOrWhiteSpace(modelPath))\\n+                throw new ArgumentException(\\\"Model path cannot be empty\\\", nameof(modelPath));\\n+            if (dimension <= 0)\\n+                throw new ArgumentException(\\\"Dimension must be positive\\\", nameof(dimension));\\n+            if (maxTokens <= 0)\\n+                throw new ArgumentException(\\\"Max tokens must be positive\\\", nameof(maxTokens));\\n+\\n+            _modelPath = modelPath;\\n+            _dimension = dimension;\\n+            _maxTokens = maxTokens;\\n+        }\\n+\\n+        protected override Vector<T> EmbedCore(string text)\\n+        {\\n+            var values = new T[_dimension];\\n+            var hash = text.GetHashCode();\\n+            \\n+            for (int i = 0; i < _dimension; i++)\\n+            {\\n+                var val = NumOps.FromDouble(Math.Sin(hash * (i + 1) * 0.003));\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFHU\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTW0X\",\"body\":\"Possible overflow: result of integer multiplication cast to double.\",\"path\":\"src/RetrievalAugmentedGeneration/EmbeddingModels/ONNXSentenceTransformer.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,71 @@\\n+using AiDotNet.Helpers;\\n+using AiDotNet.LinearAlgebra;\\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\\n+using System;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels\\n+{\\n+    /// <summary>\\n+    /// ONNX-based sentence transformer for generating embeddings.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class ONNXSentenceTransformer<T> : EmbeddingModelBase<T>\\n+    {\\n+        private readonly string _modelPath;\\n+        private readonly int _dimension;\\n+        private readonly int _maxTokens;\\n+\\n+        public override int EmbeddingDimension => _dimension;\\n+        public override int MaxTokens => _maxTokens;\\n+\\n+        public ONNXSentenceTransformer(string modelPath, int dimension = 384, int maxTokens = 512)\\n+        {\\n+            if (string.IsNullOrWhiteSpace(modelPath))\\n+                throw new ArgumentException(\\\"Model path cannot be empty\\\", nameof(modelPath));\\n+            if (dimension <= 0)\\n+                throw new ArgumentException(\\\"Dimension must be positive\\\", nameof(dimension));\\n+            if (maxTokens <= 0)\\n+                throw new ArgumentException(\\\"Max tokens must be positive\\\", nameof(maxTokens));\\n+\\n+            _modelPath = modelPath;\\n+            _dimension = dimension;\\n+            _maxTokens = maxTokens;\\n+        }\\n+\\n+        protected override Vector<T> EmbedCore(string text)\\n+        {\\n+            var values = new T[_dimension];\\n+            var hash = text.GetHashCode();\\n+            \\n+            for (int i = 0; i < _dimension; i++)\\n+            {\\n+                var val = NumOps.FromDouble(Math.Cos(hash * (i + 1) * 0.002));\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFHW\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTW0a\",\"body\":\"Possible overflow: result of integer multiplication cast to double.\",\"path\":\"src/RetrievalAugmentedGeneration/EmbeddingModels/OpenAIEmbeddingModel.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,75 @@\\n+using AiDotNet.Helpers;\\n+using AiDotNet.LinearAlgebra;\\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\\n+using System;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels\\n+{\\n+    /// <summary>\\n+    /// OpenAI embedding model for generating embeddings via OpenAI API.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class OpenAIEmbeddingModel<T> : EmbeddingModelBase<T>\\n+    {\\n+        private readonly string _apiKey;\\n+        private readonly string _modelName;\\n+        private readonly int _dimension;\\n+        private readonly int _maxTokens;\\n+\\n+        public override int EmbeddingDimension => _dimension;\\n+        public override int MaxTokens => _maxTokens;\\n+\\n+        public OpenAIEmbeddingModel(string apiKey, string modelName = \\\"text-embedding-ada-002\\\", int dimension = 1536, int maxTokens = 8191)\\n+        {\\n+            if (string.IsNullOrWhiteSpace(apiKey))\\n+                throw new ArgumentException(\\\"API key cannot be empty\\\", nameof(apiKey));\\n+            if (string.IsNullOrWhiteSpace(modelName))\\n+                throw new ArgumentException(\\\"Model name cannot be empty\\\", nameof(modelName));\\n+            if (dimension <= 0)\\n+                throw new ArgumentException(\\\"Dimension must be positive\\\", nameof(dimension));\\n+            if (maxTokens <= 0)\\n+                throw new ArgumentException(\\\"Max tokens must be positive\\\", nameof(maxTokens));\\n+\\n+            _apiKey = apiKey;\\n+            _modelName = modelName;\\n+            _dimension = dimension;\\n+            _maxTokens = maxTokens;\\n+        }\\n+\\n+        protected override Vector<T> EmbedCore(string text)\\n+        {\\n+            var values = new T[_dimension];\\n+            var hash = text.GetHashCode();\\n+            \\n+            for (int i = 0; i < _dimension; i++)\\n+            {\\n+                var val = NumOps.FromDouble(Math.Cos(hash * (i + 1) * 0.001));\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFHa\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTW0h\",\"body\":\"Inefficient use of 'ContainsKey' and [indexer](1).\\n```suggestion\\n                    if (combinedScores.TryGetValue(doc.Id, out var existingScore))\\n                    {\\n                        combinedScores[doc.Id] = NumOps.Add(existingScore, keywordScore);\\n```\",\"path\":\"src/RetrievalAugmentedGeneration/DocumentStores/HybridDocumentStore.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,124 @@\\n+using AiDotNet.Helpers;\\n+using AiDotNet.LinearAlgebra;\\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores\\n+{\\n+    /// <summary>\\n+    /// Hybrid document store combining vector and keyword search strategies.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class HybridDocumentStore<T> : DocumentStoreBase<T>\\n+    {\\n+        private readonly IDocumentStore<T> _vectorStore;\\n+        private readonly IDocumentStore<T> _keywordStore;\\n+        private readonly T _vectorWeight;\\n+        private readonly T _keywordWeight;\\n+\\n+        public override int DocumentCount => _vectorStore.DocumentCount;\\n+        public override int VectorDimension => _vectorStore.VectorDimension;\\n+\\n+        public HybridDocumentStore(\\n+            IDocumentStore<T> vectorStore,\\n+            IDocumentStore<T> keywordStore,\\n+            T vectorWeight,\\n+            T keywordWeight)\\n+        {\\n+            if (vectorStore == null)\\n+                throw new ArgumentNullException(nameof(vectorStore));\\n+            if (keywordStore == null)\\n+                throw new ArgumentNullException(nameof(keywordStore));\\n+\\n+            _vectorStore = vectorStore;\\n+            _keywordStore = keywordStore;\\n+            _vectorWeight = vectorWeight;\\n+            _keywordWeight = keywordWeight;\\n+        }\\n+\\n+        protected override void AddCore(VectorDocument<T> vectorDocument)\\n+        {\\n+            _vectorStore.Add(vectorDocument);\\n+            _keywordStore.Add(vectorDocument);\\n+        }\\n+\\n+        protected override void AddBatchCore(IList<VectorDocument<T>> vectorDocuments)\\n+        {\\n+            _vectorStore.AddBatch(vectorDocuments);\\n+            _keywordStore.AddBatch(vectorDocuments);\\n+        }\\n+\\n+        protected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)\\n+        {\\n+            var vectorResults = _vectorStore.GetSimilarWithFilters(queryVector, topK * 2, metadataFilters);\\n+            var keywordResults = _keywordStore.GetSimilarWithFilters(queryVector, topK * 2, metadataFilters);\\n+\\n+            var combinedScores = new Dictionary<string, T>();\\n+\\n+            foreach (var doc in vectorResults)\\n+            {\\n+                if (doc.HasRelevanceScore)\\n+                {\\n+                    var score = NumOps.Multiply(_vectorWeight, doc.RelevanceScore);\\n+                    combinedScores[doc.Id] = score;\\n+                }\\n+            }\\n+\\n+            foreach (var doc in keywordResults)\\n+            {\\n+                if (doc.HasRelevanceScore)\\n+                {\\n+                    var keywordScore = NumOps.Multiply(_keywordWeight, doc.RelevanceScore);\\n+                    if (combinedScores.ContainsKey(doc.Id))\\n+                    {\\n+                        combinedScores[doc.Id] = NumOps.Add(combinedScores[doc.Id], keywordScore);\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFHg\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTW0q\",\"body\":\"Inefficient use of 'ContainsKey' and [indexer](1).\",\"path\":\"src/RetrievalAugmentedGeneration/RerankingStrategies/ReciprocalRankFusion.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,122 @@\\n+using System;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+using AiDotNet.Helpers;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.RerankingStrategies\\n+{\\n+    /// <summary>\\n+    /// Reciprocal Rank Fusion for combining multiple ranking lists.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class ReciprocalRankFusion<T> : RerankingStrategyBase<T>\\n+    {\\n+        private readonly INumericOperations<T> _numOps;\\n+        private readonly int _k;\\n+\\n+        /// <summary>\\n+        /// Initializes a new instance of the <see cref=\\\"ReciprocalRankFusion{T}\\\"/> class.\\n+        /// </summary>\\n+        /// <param name=\\\"numericOperations\\\">The numeric operations for type T.</param>\\n+        /// <param name=\\\"k\\\">The constant k for reciprocal rank formula (default: 60).</param>\\n+        public ReciprocalRankFusion(INumericOperations<T> numericOperations, int k = 60) : base(numericOperations)\\n+        {\\n+            _numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\\n+            _k = k > 0 ? k : throw new ArgumentOutOfRangeException(nameof(k));\\n+        }\\n+\\n+        /// <summary>\\n+        /// Reranks documents using reciprocal rank fusion.\\n+        /// </summary>\\n+        /// <param name=\\\"query\\\">The query string.</param>\\n+        /// <param name=\\\"documents\\\">The documents to rerank.</param>\\n+        /// <param name=\\\"topK\\\">The number of top documents to return.</param>\\n+        /// <returns>A reranked list of documents.</returns>\\n+        public override List<Document<T>> Rerank(string query, List<Document<T>> documents, int topK)\\n+        {\\n+            if (string.IsNullOrEmpty(query)) throw new ArgumentNullException(nameof(query));\\n+            if (documents == null) throw new ArgumentNullException(nameof(documents));\\n+            if (topK <= 0) throw new ArgumentOutOfRangeException(nameof(topK));\\n+\\n+            var scores = new Dictionary<string, T>();\\n+\\n+            for (int rank = 0; rank < documents.Count; rank++)\\n+            {\\n+                var doc = documents[rank];\\n+                var rrfScore = _numOps.FromDouble(1.0 / (_k + rank + 1));\\n+\\n+                if (scores.ContainsKey(doc.Id))\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFHo\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTW0y\",\"body\":\"Inefficient use of 'ContainsKey' and [indexer](1).\",\"path\":\"src/RetrievalAugmentedGeneration/RerankingStrategies/ReciprocalRankFusion.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,122 @@\\n+using System;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+using AiDotNet.Helpers;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.RerankingStrategies\\n+{\\n+    /// <summary>\\n+    /// Reciprocal Rank Fusion for combining multiple ranking lists.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class ReciprocalRankFusion<T> : RerankingStrategyBase<T>\\n+    {\\n+        private readonly INumericOperations<T> _numOps;\\n+        private readonly int _k;\\n+\\n+        /// <summary>\\n+        /// Initializes a new instance of the <see cref=\\\"ReciprocalRankFusion{T}\\\"/> class.\\n+        /// </summary>\\n+        /// <param name=\\\"numericOperations\\\">The numeric operations for type T.</param>\\n+        /// <param name=\\\"k\\\">The constant k for reciprocal rank formula (default: 60).</param>\\n+        public ReciprocalRankFusion(INumericOperations<T> numericOperations, int k = 60) : base(numericOperations)\\n+        {\\n+            _numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\\n+            _k = k > 0 ? k : throw new ArgumentOutOfRangeException(nameof(k));\\n+        }\\n+\\n+        /// <summary>\\n+        /// Reranks documents using reciprocal rank fusion.\\n+        /// </summary>\\n+        /// <param name=\\\"query\\\">The query string.</param>\\n+        /// <param name=\\\"documents\\\">The documents to rerank.</param>\\n+        /// <param name=\\\"topK\\\">The number of top documents to return.</param>\\n+        /// <returns>A reranked list of documents.</returns>\\n+        public override List<Document<T>> Rerank(string query, List<Document<T>> documents, int topK)\\n+        {\\n+            if (string.IsNullOrEmpty(query)) throw new ArgumentNullException(nameof(query));\\n+            if (documents == null) throw new ArgumentNullException(nameof(documents));\\n+            if (topK <= 0) throw new ArgumentOutOfRangeException(nameof(topK));\\n+\\n+            var scores = new Dictionary<string, T>();\\n+\\n+            for (int rank = 0; rank < documents.Count; rank++)\\n+            {\\n+                var doc = documents[rank];\\n+                var rrfScore = _numOps.FromDouble(1.0 / (_k + rank + 1));\\n+\\n+                if (scores.ContainsKey(doc.Id))\\n+                {\\n+                    scores[doc.Id] = _numOps.Add(scores[doc.Id], rrfScore);\\n+                }\\n+                else\\n+                {\\n+                    scores[doc.Id] = rrfScore;\\n+                }\\n+            }\\n+\\n+            var reranked = documents\\n+                .OrderByDescending(d => Convert.ToDouble(scores.ContainsKey(d.Id) ? scores[d.Id] : _numOps.Zero))\\n+                .Take(topK)\\n+                .ToList();\\n+\\n+            foreach (var doc in reranked)\\n+            {\\n+                if (scores.ContainsKey(doc.Id))\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFHs\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTW03\",\"body\":\"Inefficient use of 'ContainsKey' and [indexer](1).\",\"path\":\"src/RetrievalAugmentedGeneration/RerankingStrategies/ReciprocalRankFusion.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,122 @@\\n+using System;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+using AiDotNet.Helpers;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.RerankingStrategies\\n+{\\n+    /// <summary>\\n+    /// Reciprocal Rank Fusion for combining multiple ranking lists.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class ReciprocalRankFusion<T> : RerankingStrategyBase<T>\\n+    {\\n+        private readonly INumericOperations<T> _numOps;\\n+        private readonly int _k;\\n+\\n+        /// <summary>\\n+        /// Initializes a new instance of the <see cref=\\\"ReciprocalRankFusion{T}\\\"/> class.\\n+        /// </summary>\\n+        /// <param name=\\\"numericOperations\\\">The numeric operations for type T.</param>\\n+        /// <param name=\\\"k\\\">The constant k for reciprocal rank formula (default: 60).</param>\\n+        public ReciprocalRankFusion(INumericOperations<T> numericOperations, int k = 60) : base(numericOperations)\\n+        {\\n+            _numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\\n+            _k = k > 0 ? k : throw new ArgumentOutOfRangeException(nameof(k));\\n+        }\\n+\\n+        /// <summary>\\n+        /// Reranks documents using reciprocal rank fusion.\\n+        /// </summary>\\n+        /// <param name=\\\"query\\\">The query string.</param>\\n+        /// <param name=\\\"documents\\\">The documents to rerank.</param>\\n+        /// <param name=\\\"topK\\\">The number of top documents to return.</param>\\n+        /// <returns>A reranked list of documents.</returns>\\n+        public override List<Document<T>> Rerank(string query, List<Document<T>> documents, int topK)\\n+        {\\n+            if (string.IsNullOrEmpty(query)) throw new ArgumentNullException(nameof(query));\\n+            if (documents == null) throw new ArgumentNullException(nameof(documents));\\n+            if (topK <= 0) throw new ArgumentOutOfRangeException(nameof(topK));\\n+\\n+            var scores = new Dictionary<string, T>();\\n+\\n+            for (int rank = 0; rank < documents.Count; rank++)\\n+            {\\n+                var doc = documents[rank];\\n+                var rrfScore = _numOps.FromDouble(1.0 / (_k + rank + 1));\\n+\\n+                if (scores.ContainsKey(doc.Id))\\n+                {\\n+                    scores[doc.Id] = _numOps.Add(scores[doc.Id], rrfScore);\\n+                }\\n+                else\\n+                {\\n+                    scores[doc.Id] = rrfScore;\\n+                }\\n+            }\\n+\\n+            var reranked = documents\\n+                .OrderByDescending(d => Convert.ToDouble(scores.ContainsKey(d.Id) ? scores[d.Id] : _numOps.Zero))\\n+                .Take(topK)\\n+                .ToList();\\n+\\n+            foreach (var doc in reranked)\\n+            {\\n+                if (scores.ContainsKey(doc.Id))\\n+                {\\n+                    doc.RelevanceScore = scores[doc.Id];\\n+                }\\n+            }\\n+\\n+            return reranked;\\n+        }\\n+\\n+        /// <summary>\\n+        /// Fuses multiple ranking lists using reciprocal rank fusion.\\n+        /// </summary>\\n+        /// <param name=\\\"rankingLists\\\">Multiple lists of ranked documents.</param>\\n+        /// <param name=\\\"topK\\\">The number of top documents to return.</param>\\n+        /// <returns>A fused and reranked list of documents.</returns>\\n+        public List<Document<T>> FuseRankings(List<List<Document<T>>> rankingLists, int topK)\\n+        {\\n+            if (rankingLists == null || rankingLists.Count == 0)\\n+                throw new ArgumentException(\\\"Ranking lists cannot be null or empty\\\", nameof(rankingLists));\\n+            if (topK <= 0) throw new ArgumentOutOfRangeException(nameof(topK));\\n+\\n+            var scores = new Dictionary<string, T>();\\n+            var allDocs = new Dictionary<string, Document<T>>();\\n+\\n+            foreach (var rankingList in rankingLists)\\n+            {\\n+                for (int rank = 0; rank < rankingList.Count; rank++)\\n+                {\\n+                    var doc = rankingList[rank];\\n+                    var rrfScore = _numOps.FromDouble(1.0 / (_k + rank + 1));\\n+\\n+                    if (scores.ContainsKey(doc.Id))\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFHu\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTW05\",\"body\":\"Inefficient use of 'ContainsKey' and [indexer](1).\",\"path\":\"src/RetrievalAugmentedGeneration/RerankingStrategies/ReciprocalRankFusion.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,122 @@\\n+using System;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+using AiDotNet.Helpers;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.RerankingStrategies\\n+{\\n+    /// <summary>\\n+    /// Reciprocal Rank Fusion for combining multiple ranking lists.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class ReciprocalRankFusion<T> : RerankingStrategyBase<T>\\n+    {\\n+        private readonly INumericOperations<T> _numOps;\\n+        private readonly int _k;\\n+\\n+        /// <summary>\\n+        /// Initializes a new instance of the <see cref=\\\"ReciprocalRankFusion{T}\\\"/> class.\\n+        /// </summary>\\n+        /// <param name=\\\"numericOperations\\\">The numeric operations for type T.</param>\\n+        /// <param name=\\\"k\\\">The constant k for reciprocal rank formula (default: 60).</param>\\n+        public ReciprocalRankFusion(INumericOperations<T> numericOperations, int k = 60) : base(numericOperations)\\n+        {\\n+            _numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\\n+            _k = k > 0 ? k : throw new ArgumentOutOfRangeException(nameof(k));\\n+        }\\n+\\n+        /// <summary>\\n+        /// Reranks documents using reciprocal rank fusion.\\n+        /// </summary>\\n+        /// <param name=\\\"query\\\">The query string.</param>\\n+        /// <param name=\\\"documents\\\">The documents to rerank.</param>\\n+        /// <param name=\\\"topK\\\">The number of top documents to return.</param>\\n+        /// <returns>A reranked list of documents.</returns>\\n+        public override List<Document<T>> Rerank(string query, List<Document<T>> documents, int topK)\\n+        {\\n+            if (string.IsNullOrEmpty(query)) throw new ArgumentNullException(nameof(query));\\n+            if (documents == null) throw new ArgumentNullException(nameof(documents));\\n+            if (topK <= 0) throw new ArgumentOutOfRangeException(nameof(topK));\\n+\\n+            var scores = new Dictionary<string, T>();\\n+\\n+            for (int rank = 0; rank < documents.Count; rank++)\\n+            {\\n+                var doc = documents[rank];\\n+                var rrfScore = _numOps.FromDouble(1.0 / (_k + rank + 1));\\n+\\n+                if (scores.ContainsKey(doc.Id))\\n+                {\\n+                    scores[doc.Id] = _numOps.Add(scores[doc.Id], rrfScore);\\n+                }\\n+                else\\n+                {\\n+                    scores[doc.Id] = rrfScore;\\n+                }\\n+            }\\n+\\n+            var reranked = documents\\n+                .OrderByDescending(d => Convert.ToDouble(scores.ContainsKey(d.Id) ? scores[d.Id] : _numOps.Zero))\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFHz\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTW1A\",\"body\":\"Inefficient use of 'ContainsKey' and [indexer](1).\",\"path\":\"src/RetrievalAugmentedGeneration/Retrievers/BM25Retriever.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,124 @@\\n+using AiDotNet.Helpers;\\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\\n+{\\n+    /// <summary>\\n+    /// BM25 (Best Matching 25) retrieval algorithm for sparse keyword-based search.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class BM25Retriever<T> : RetrieverBase<T>\\n+    {\\n+        private readonly IDocumentStore<T> _documentStore;\\n+        private readonly T _k1;\\n+        private readonly T _b;\\n+        private readonly Dictionary<string, Dictionary<string, T>> _termFrequencies;\\n+        private readonly Dictionary<string, T> _documentLengths;\\n+        private T _avgDocLength;\\n+\\n+        public BM25Retriever(IDocumentStore<T> documentStore, int defaultTopK = 5, double k1 = 1.5, double b = 0.75) \\n+            : base(defaultTopK)\\n+        {\\n+            if (documentStore == null)\\n+                throw new ArgumentNullException(nameof(documentStore));\\n+\\n+            _documentStore = documentStore;\\n+            _k1 = NumOps.FromDouble(k1);\\n+            _b = NumOps.FromDouble(b);\\n+            _termFrequencies = new Dictionary<string, Dictionary<string, T>>();\\n+            _documentLengths = new Dictionary<string, T>();\\n+            _avgDocLength = NumOps.Zero;\\n+        }\\n+\\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\\n+        {\\n+            var queryTerms = Tokenize(query);\\n+            var scores = new Dictionary<string, T>();\\n+\\n+            var candidates = _documentStore.GetSimilar(\\n+                new AiDotNet.LinearAlgebra.Vector<T>(new T[Math.Max(1, _documentStore.VectorDimension)]),\\n+                _documentStore.DocumentCount\\n+            );\\n+\\n+            foreach (var doc in candidates)\\n+            {\\n+                if (!MatchesFilters(doc, metadataFilters))\\n+                    continue;\\n+\\n+                var score = NumOps.Zero;\\n+                \\n+                foreach (var term in queryTerms)\\n+                {\\n+                    var termScore = CalculateBM25Term(doc.Id, term);\\n+                    score = NumOps.Add(score, termScore);\\n+                }\\n+\\n+                scores[doc.Id] = score;\\n+            }\\n+\\n+            var results = scores\\n+                .OrderByDescending(kv => kv.Value)\\n+                .Take(topK)\\n+                .Select(kv =>\\n+                {\\n+                    var doc = candidates.FirstOrDefault(d => d.Id == kv.Key);\\n+                    if (doc != null)\\n+                    {\\n+                        doc.RelevanceScore = kv.Value;\\n+                        doc.HasRelevanceScore = true;\\n+                    }\\n+                    return doc;\\n+                })\\n+                .Where(d => d != null)\\n+                .Cast<Document<T>>();\\n+\\n+            return results;\\n+        }\\n+\\n+        private T CalculateBM25Term(string docId, string term)\\n+        {\\n+            if (!_termFrequencies.ContainsKey(docId) || !_termFrequencies[docId].ContainsKey(term))\\n+                return NumOps.Zero;\\n+\\n+            var tf = _termFrequencies[docId][term];\\n+            var docLength = _documentLengths.ContainsKey(docId) ? _documentLengths[docId] : NumOps.Zero;\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFH3\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTW1F\",\"body\":\"Inefficient use of 'ContainsKey' and [indexer](1).\",\"path\":\"src/RetrievalAugmentedGeneration/Retrievers/HybridRetriever.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,92 @@\\n+using AiDotNet.Helpers;\\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\\n+{\\n+    /// <summary>\\n+    /// Hybrid retriever combining dense and sparse retrieval strategies.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class HybridRetriever<T> : RetrieverBase<T>\\n+    {\\n+        private readonly IRetriever<T> _denseRetriever;\\n+        private readonly IRetriever<T> _sparseRetriever;\\n+        private readonly T _denseWeight;\\n+        private readonly T _sparseWeight;\\n+\\n+        public HybridRetriever(\\n+            IRetriever<T> denseRetriever, \\n+            IRetriever<T> sparseRetriever, \\n+            double denseWeight = 0.7, \\n+            double sparseWeight = 0.3,\\n+            int defaultTopK = 5) \\n+            : base(defaultTopK)\\n+        {\\n+            if (denseRetriever == null)\\n+                throw new ArgumentNullException(nameof(denseRetriever));\\n+            if (sparseRetriever == null)\\n+                throw new ArgumentNullException(nameof(sparseRetriever));\\n+\\n+            _denseRetriever = denseRetriever;\\n+            _sparseRetriever = sparseRetriever;\\n+            _denseWeight = NumOps.FromDouble(denseWeight);\\n+            _sparseWeight = NumOps.FromDouble(sparseWeight);\\n+        }\\n+\\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\\n+        {\\n+            var denseResults = _denseRetriever.Retrieve(query, topK * 2, metadataFilters);\\n+            var sparseResults = _sparseRetriever.Retrieve(query, topK * 2, metadataFilters);\\n+\\n+            var combinedScores = new Dictionary<string, T>();\\n+\\n+            foreach (var doc in denseResults)\\n+            {\\n+                if (doc.HasRelevanceScore)\\n+                {\\n+                    var score = NumOps.Multiply(_denseWeight, doc.RelevanceScore);\\n+                    combinedScores[doc.Id] = score;\\n+                }\\n+            }\\n+\\n+            foreach (var doc in sparseResults)\\n+            {\\n+                if (doc.HasRelevanceScore)\\n+                {\\n+                    var sparseScore = NumOps.Multiply(_sparseWeight, doc.RelevanceScore);\\n+                    if (combinedScores.ContainsKey(doc.Id))\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFH8\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTW1L\",\"body\":\"Inefficient use of 'ContainsKey' and [indexer](1).\\nInefficient use of 'ContainsKey' and [indexer](2).\",\"path\":\"src/RetrievalAugmentedGeneration/Retrievers/BM25Retriever.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,124 @@\\n+using AiDotNet.Helpers;\\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\\n+{\\n+    /// <summary>\\n+    /// BM25 (Best Matching 25) retrieval algorithm for sparse keyword-based search.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class BM25Retriever<T> : RetrieverBase<T>\\n+    {\\n+        private readonly IDocumentStore<T> _documentStore;\\n+        private readonly T _k1;\\n+        private readonly T _b;\\n+        private readonly Dictionary<string, Dictionary<string, T>> _termFrequencies;\\n+        private readonly Dictionary<string, T> _documentLengths;\\n+        private T _avgDocLength;\\n+\\n+        public BM25Retriever(IDocumentStore<T> documentStore, int defaultTopK = 5, double k1 = 1.5, double b = 0.75) \\n+            : base(defaultTopK)\\n+        {\\n+            if (documentStore == null)\\n+                throw new ArgumentNullException(nameof(documentStore));\\n+\\n+            _documentStore = documentStore;\\n+            _k1 = NumOps.FromDouble(k1);\\n+            _b = NumOps.FromDouble(b);\\n+            _termFrequencies = new Dictionary<string, Dictionary<string, T>>();\\n+            _documentLengths = new Dictionary<string, T>();\\n+            _avgDocLength = NumOps.Zero;\\n+        }\\n+\\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\\n+        {\\n+            var queryTerms = Tokenize(query);\\n+            var scores = new Dictionary<string, T>();\\n+\\n+            var candidates = _documentStore.GetSimilar(\\n+                new AiDotNet.LinearAlgebra.Vector<T>(new T[Math.Max(1, _documentStore.VectorDimension)]),\\n+                _documentStore.DocumentCount\\n+            );\\n+\\n+            foreach (var doc in candidates)\\n+            {\\n+                if (!MatchesFilters(doc, metadataFilters))\\n+                    continue;\\n+\\n+                var score = NumOps.Zero;\\n+                \\n+                foreach (var term in queryTerms)\\n+                {\\n+                    var termScore = CalculateBM25Term(doc.Id, term);\\n+                    score = NumOps.Add(score, termScore);\\n+                }\\n+\\n+                scores[doc.Id] = score;\\n+            }\\n+\\n+            var results = scores\\n+                .OrderByDescending(kv => kv.Value)\\n+                .Take(topK)\\n+                .Select(kv =>\\n+                {\\n+                    var doc = candidates.FirstOrDefault(d => d.Id == kv.Key);\\n+                    if (doc != null)\\n+                    {\\n+                        doc.RelevanceScore = kv.Value;\\n+                        doc.HasRelevanceScore = true;\\n+                    }\\n+                    return doc;\\n+                })\\n+                .Where(d => d != null)\\n+                .Cast<Document<T>>();\\n+\\n+            return results;\\n+        }\\n+\\n+        private T CalculateBM25Term(string docId, string term)\\n+        {\\n+            if (!_termFrequencies.ContainsKey(docId) || !_termFrequencies[docId].ContainsKey(term))\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFIC\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTW1S\",\"body\":\"Inefficient use of 'ContainsKey' and [indexer](1).\",\"path\":\"src/RetrievalAugmentedGeneration/Retrievers/BM25Retriever.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,124 @@\\n+using AiDotNet.Helpers;\\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\\n+{\\n+    /// <summary>\\n+    /// BM25 (Best Matching 25) retrieval algorithm for sparse keyword-based search.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class BM25Retriever<T> : RetrieverBase<T>\\n+    {\\n+        private readonly IDocumentStore<T> _documentStore;\\n+        private readonly T _k1;\\n+        private readonly T _b;\\n+        private readonly Dictionary<string, Dictionary<string, T>> _termFrequencies;\\n+        private readonly Dictionary<string, T> _documentLengths;\\n+        private T _avgDocLength;\\n+\\n+        public BM25Retriever(IDocumentStore<T> documentStore, int defaultTopK = 5, double k1 = 1.5, double b = 0.75) \\n+            : base(defaultTopK)\\n+        {\\n+            if (documentStore == null)\\n+                throw new ArgumentNullException(nameof(documentStore));\\n+\\n+            _documentStore = documentStore;\\n+            _k1 = NumOps.FromDouble(k1);\\n+            _b = NumOps.FromDouble(b);\\n+            _termFrequencies = new Dictionary<string, Dictionary<string, T>>();\\n+            _documentLengths = new Dictionary<string, T>();\\n+            _avgDocLength = NumOps.Zero;\\n+        }\\n+\\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\\n+        {\\n+            var queryTerms = Tokenize(query);\\n+            var scores = new Dictionary<string, T>();\\n+\\n+            var candidates = _documentStore.GetSimilar(\\n+                new AiDotNet.LinearAlgebra.Vector<T>(new T[Math.Max(1, _documentStore.VectorDimension)]),\\n+                _documentStore.DocumentCount\\n+            );\\n+\\n+            foreach (var doc in candidates)\\n+            {\\n+                if (!MatchesFilters(doc, metadataFilters))\\n+                    continue;\\n+\\n+                var score = NumOps.Zero;\\n+                \\n+                foreach (var term in queryTerms)\\n+                {\\n+                    var termScore = CalculateBM25Term(doc.Id, term);\\n+                    score = NumOps.Add(score, termScore);\\n+                }\\n+\\n+                scores[doc.Id] = score;\\n+            }\\n+\\n+            var results = scores\\n+                .OrderByDescending(kv => kv.Value)\\n+                .Take(topK)\\n+                .Select(kv =>\\n+                {\\n+                    var doc = candidates.FirstOrDefault(d => d.Id == kv.Key);\\n+                    if (doc != null)\\n+                    {\\n+                        doc.RelevanceScore = kv.Value;\\n+                        doc.HasRelevanceScore = true;\\n+                    }\\n+                    return doc;\\n+                })\\n+                .Where(d => d != null)\\n+                .Cast<Document<T>>();\\n+\\n+            return results;\\n+        }\\n+\\n+        private T CalculateBM25Term(string docId, string term)\\n+        {\\n+            if (!_termFrequencies.ContainsKey(docId) || !_termFrequencies[docId].ContainsKey(term))\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFIH\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTW1Y\",\"body\":\"Inefficient use of 'ContainsKey' and [indexer](1).\",\"path\":\"src/RetrievalAugmentedGeneration/Retrievers/MultiQueryRetriever.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,83 @@\\n+using AiDotNet.Helpers;\\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\\n+{\\n+    /// <summary>\\n+    /// Multi-query retriever that generates multiple query variations and merges results.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class MultiQueryRetriever<T> : RetrieverBase<T>\\n+    {\\n+        private readonly IRetriever<T> _baseRetriever;\\n+        private readonly int _numQueries;\\n+\\n+        public MultiQueryRetriever(IRetriever<T> baseRetriever, int numQueries = 3, int defaultTopK = 5) : base(defaultTopK)\\n+        {\\n+            if (baseRetriever == null)\\n+                throw new ArgumentNullException(nameof(baseRetriever));\\n+            if (numQueries <= 0)\\n+                throw new ArgumentException(\\\"Number of queries must be positive\\\", nameof(numQueries));\\n+\\n+            _baseRetriever = baseRetriever;\\n+            _numQueries = numQueries;\\n+        }\\n+\\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\\n+        {\\n+            var queries = GenerateQueries(query);\\n+            var allResults = new Dictionary<string, (Document<T> doc, T score)>();\\n+\\n+            foreach (var q in queries)\\n+            {\\n+                var results = _baseRetriever.Retrieve(q, topK, metadataFilters);\\n+\\n+                foreach (var doc in results)\\n+                {\\n+                    if (doc.HasRelevanceScore)\\n+                    {\\n+                        if (allResults.ContainsKey(doc.Id))\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFII\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTW1b\",\"body\":\"Inefficient use of 'ContainsKey' and [indexer](1).\",\"path\":\"src/RetrievalAugmentedGeneration/Retrievers/TFIDFRetriever.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,108 @@\\n+using AiDotNet.Helpers;\\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\\n+{\\n+    /// <summary>\\n+    /// TF-IDF (Term Frequency-Inverse Document Frequency) retrieval strategy.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class TFIDFRetriever<T> : RetrieverBase<T>\\n+    {\\n+        private readonly IDocumentStore<T> _documentStore;\\n+        private readonly Dictionary<string, Dictionary<string, T>> _tfidf;\\n+        private readonly Dictionary<string, T> _idf;\\n+\\n+        public TFIDFRetriever(IDocumentStore<T> documentStore, int defaultTopK = 5) : base(defaultTopK)\\n+        {\\n+            if (documentStore == null)\\n+                throw new ArgumentNullException(nameof(documentStore));\\n+                \\n+            _documentStore = documentStore;\\n+            _tfidf = new Dictionary<string, Dictionary<string, T>>();\\n+            _idf = new Dictionary<string, T>();\\n+        }\\n+\\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\\n+        {\\n+            var queryTerms = Tokenize(query);\\n+            var scores = new Dictionary<string, T>();\\n+\\n+            var candidates = _documentStore.GetSimilar(\\n+                new AiDotNet.LinearAlgebra.Vector<T>(new T[_documentStore.VectorDimension]), \\n+                _documentStore.DocumentCount\\n+            );\\n+\\n+            foreach (var doc in candidates)\\n+            {\\n+                if (!MatchesFilters(doc, metadataFilters))\\n+                    continue;\\n+\\n+                var score = NumOps.Zero;\\n+\\n+                if (_tfidf.ContainsKey(doc.Id))\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFIM\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTW1g\",\"body\":\"Inefficient use of 'ContainsKey' and [indexer](1).\",\"path\":\"src/RetrievalAugmentedGeneration/Retrievers/TFIDFRetriever.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,108 @@\\n+using AiDotNet.Helpers;\\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\\n+{\\n+    /// <summary>\\n+    /// TF-IDF (Term Frequency-Inverse Document Frequency) retrieval strategy.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class TFIDFRetriever<T> : RetrieverBase<T>\\n+    {\\n+        private readonly IDocumentStore<T> _documentStore;\\n+        private readonly Dictionary<string, Dictionary<string, T>> _tfidf;\\n+        private readonly Dictionary<string, T> _idf;\\n+\\n+        public TFIDFRetriever(IDocumentStore<T> documentStore, int defaultTopK = 5) : base(defaultTopK)\\n+        {\\n+            if (documentStore == null)\\n+                throw new ArgumentNullException(nameof(documentStore));\\n+                \\n+            _documentStore = documentStore;\\n+            _tfidf = new Dictionary<string, Dictionary<string, T>>();\\n+            _idf = new Dictionary<string, T>();\\n+        }\\n+\\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\\n+        {\\n+            var queryTerms = Tokenize(query);\\n+            var scores = new Dictionary<string, T>();\\n+\\n+            var candidates = _documentStore.GetSimilar(\\n+                new AiDotNet.LinearAlgebra.Vector<T>(new T[_documentStore.VectorDimension]), \\n+                _documentStore.DocumentCount\\n+            );\\n+\\n+            foreach (var doc in candidates)\\n+            {\\n+                if (!MatchesFilters(doc, metadataFilters))\\n+                    continue;\\n+\\n+                var score = NumOps.Zero;\\n+\\n+                if (_tfidf.ContainsKey(doc.Id))\\n+                {\\n+                    var docTfidf = _tfidf[doc.Id];\\n+                    foreach (var term in queryTerms)\\n+                    {\\n+                        if (docTfidf.ContainsKey(term))\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFIR\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTW1n\",\"body\":\"String concatenation in loop: use 'StringBuilder'.\",\"path\":\"src/RetrievalAugmentedGeneration/ChunkingStrategies/SemanticChunkingStrategy.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,99 @@\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+using AiDotNet.Helpers;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies\\n+{\\n+    /// <summary>\\n+    /// Semantic-based text chunking that uses embeddings to group related content.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class SemanticChunkingStrategy<T> : ChunkingStrategyBase\\n+    {\\n+        private readonly INumericOperations<T> _numOps;\\n+        private readonly T _similarityThreshold;\\n+\\n+        /// <summary>\\n+        /// Initializes a new instance of the <see cref=\\\"SemanticChunkingStrategy{T}\\\"/> class.\\n+        /// </summary>\\n+        /// <param name=\\\"numericOperations\\\">The numeric operations for type T.</param>\\n+        /// <param name=\\\"similarityThreshold\\\">The similarity threshold for grouping sentences.</param>\\n+        /// <param name=\\\"maxChunkSize\\\">The maximum chunk size in characters.</param>\\n+        public SemanticChunkingStrategy(\\n+            INumericOperations<T> numericOperations,\\n+            T similarityThreshold,\\n+            int maxChunkSize = 1000,\\n+            int chunkOverlap = 200)\\n+            : base(maxChunkSize, chunkOverlap)\\n+        {\\n+            _numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\\n+            _similarityThreshold = similarityThreshold;\\n+        }\\n+\\n+        /// <summary>\\n+        /// Chunks text based on semantic similarity between sentences.\\n+        /// </summary>\\n+        /// <param name=\\\"text\\\">The text to chunk.</param>\\n+        /// <returns>A collection of semantically coherent chunks with positions.</returns>\\n+        protected override IEnumerable<(string Chunk, int StartPosition, int EndPosition)> ChunkCore(string text)\\n+        {\\n+            var sentences = SplitIntoSentences(text);\\n+            var currentChunk = new List<string>();\\n+            var currentSize = 0;\\n+            var position = 0;\\n+\\n+            foreach (var sentence in sentences)\\n+            {\\n+                if (currentSize + sentence.Length > ChunkSize && currentChunk.Count > 0)\\n+                {\\n+                    var chunkText = string.Join(\\\" \\\", currentChunk);\\n+                    var endPos = position + chunkText.Length;\\n+                    yield return (chunkText, position, endPos);\\n+                    \\n+                    position = endPos - ChunkOverlap;\\n+                    currentChunk.Clear();\\n+                    currentSize = 0;\\n+                }\\n+\\n+                currentChunk.Add(sentence);\\n+                currentSize += sentence.Length;\\n+            }\\n+\\n+            if (currentChunk.Count > 0)\\n+            {\\n+                var chunkText = string.Join(\\\" \\\", currentChunk);\\n+                yield return (chunkText, position, position + chunkText.Length);\\n+            }\\n+        }\\n+\\n+        private List<string> SplitIntoSentences(string text)\\n+        {\\n+            var sentences = new List<string>();\\n+            var sentenceEndings = new[] { \\\". \\\", \\\"! \\\", \\\"? \\\", \\\".\\\\n\\\", \\\"!\\\\n\\\", \\\"?\\\\n\\\" };\\n+            var currentSentence = string.Empty;\\n+\\n+            for (int i = 0; i < text.Length; i++)\\n+            {\\n+                currentSentence += text[i];\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFIX\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTW1u\",\"body\":\"String concatenation in loop: use 'StringBuilder'.\",\"path\":\"src/RetrievalAugmentedGeneration/ContextCompression/DocumentSummarizer.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,166 @@\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+using AiDotNet.Helpers;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.ContextCompression\\n+{\\n+    /// <summary>\\n+    /// Document summarizer for creating concise summaries of retrieved content.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class DocumentSummarizer<T>\\n+    {\\n+        private readonly INumericOperations<T> _numOps;\\n+        private readonly int _maxSummaryLength;\\n+        private readonly string _llmEndpoint;\\n+        private readonly string _apiKey;\\n+\\n+        /// <summary>\\n+        /// Initializes a new instance of the <see cref=\\\"DocumentSummarizer{T}\\\"/> class.\\n+        /// </summary>\\n+        /// <param name=\\\"numericOperations\\\">The numeric operations for type T.</param>\\n+        /// <param name=\\\"maxSummaryLength\\\">The maximum length of the summary in characters.</param>\\n+        /// <param name=\\\"llmEndpoint\\\">The LLM API endpoint.</param>\\n+        /// <param name=\\\"apiKey\\\">The API key for the LLM service.</param>\\n+        public DocumentSummarizer(\\n+            INumericOperations<T> numericOperations,\\n+            int maxSummaryLength = 500,\\n+            string llmEndpoint = \\\"\\\",\\n+            string apiKey = \\\"\\\")\\n+        {\\n+            _numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\\n+            _maxSummaryLength = maxSummaryLength > 0\\n+                ? maxSummaryLength\\n+                : throw new ArgumentOutOfRangeException(nameof(maxSummaryLength));\\n+            _llmEndpoint = llmEndpoint;\\n+            _apiKey = apiKey;\\n+        }\\n+\\n+        /// <summary>\\n+        /// Summarizes a list of documents.\\n+        /// </summary>\\n+        /// <param name=\\\"documents\\\">The documents to summarize.</param>\\n+        /// <returns>A list of summarized documents.</returns>\\n+        public List<Document<T>> Summarize(List<Document<T>> documents)\\n+        {\\n+            if (documents == null) throw new ArgumentNullException(nameof(documents));\\n+\\n+            var summarized = new List<Document<T>>();\\n+\\n+            foreach (var doc in documents)\\n+            {\\n+                var summary = SummarizeText(doc.Content);\\n+                var summarizedDoc = new Document<T>\\n+                {\\n+                    Id = doc.Id,\\n+                    Content = summary,\\n+                    Metadata = doc.Metadata,\\n+                    RelevanceScore = doc.RelevanceScore\\n+                };\\n+                summarized.Add(summarizedDoc);\\n+            }\\n+\\n+            return summarized;\\n+        }\\n+\\n+        /// <summary>\\n+        /// Summarizes text to a maximum length.\\n+        /// </summary>\\n+        /// <param name=\\\"text\\\">The text to summarize.</param>\\n+        /// <returns>The summarized text.</returns>\\n+        public string SummarizeText(string text)\\n+        {\\n+            if (string.IsNullOrEmpty(text)) return text;\\n+\\n+            if (text.Length <= _maxSummaryLength)\\n+            {\\n+                return text;\\n+            }\\n+\\n+            var sentences = SplitIntoSentences(text);\\n+            var importantSentences = ExtractImportantSentences(sentences);\\n+\\n+            var summary = string.Empty;\\n+            foreach (var sentence in importantSentences)\\n+            {\\n+                if (summary.Length + sentence.Length > _maxSummaryLength)\\n+                {\\n+                    break;\\n+                }\\n+                summary += sentence + \\\" \\\";\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFIe\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTW11\",\"body\":\"String concatenation in loop: use 'StringBuilder'.\",\"path\":\"src/RetrievalAugmentedGeneration/ContextCompression/DocumentSummarizer.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,166 @@\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+using AiDotNet.Helpers;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.ContextCompression\\n+{\\n+    /// <summary>\\n+    /// Document summarizer for creating concise summaries of retrieved content.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class DocumentSummarizer<T>\\n+    {\\n+        private readonly INumericOperations<T> _numOps;\\n+        private readonly int _maxSummaryLength;\\n+        private readonly string _llmEndpoint;\\n+        private readonly string _apiKey;\\n+\\n+        /// <summary>\\n+        /// Initializes a new instance of the <see cref=\\\"DocumentSummarizer{T}\\\"/> class.\\n+        /// </summary>\\n+        /// <param name=\\\"numericOperations\\\">The numeric operations for type T.</param>\\n+        /// <param name=\\\"maxSummaryLength\\\">The maximum length of the summary in characters.</param>\\n+        /// <param name=\\\"llmEndpoint\\\">The LLM API endpoint.</param>\\n+        /// <param name=\\\"apiKey\\\">The API key for the LLM service.</param>\\n+        public DocumentSummarizer(\\n+            INumericOperations<T> numericOperations,\\n+            int maxSummaryLength = 500,\\n+            string llmEndpoint = \\\"\\\",\\n+            string apiKey = \\\"\\\")\\n+        {\\n+            _numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\\n+            _maxSummaryLength = maxSummaryLength > 0\\n+                ? maxSummaryLength\\n+                : throw new ArgumentOutOfRangeException(nameof(maxSummaryLength));\\n+            _llmEndpoint = llmEndpoint;\\n+            _apiKey = apiKey;\\n+        }\\n+\\n+        /// <summary>\\n+        /// Summarizes a list of documents.\\n+        /// </summary>\\n+        /// <param name=\\\"documents\\\">The documents to summarize.</param>\\n+        /// <returns>A list of summarized documents.</returns>\\n+        public List<Document<T>> Summarize(List<Document<T>> documents)\\n+        {\\n+            if (documents == null) throw new ArgumentNullException(nameof(documents));\\n+\\n+            var summarized = new List<Document<T>>();\\n+\\n+            foreach (var doc in documents)\\n+            {\\n+                var summary = SummarizeText(doc.Content);\\n+                var summarizedDoc = new Document<T>\\n+                {\\n+                    Id = doc.Id,\\n+                    Content = summary,\\n+                    Metadata = doc.Metadata,\\n+                    RelevanceScore = doc.RelevanceScore\\n+                };\\n+                summarized.Add(summarizedDoc);\\n+            }\\n+\\n+            return summarized;\\n+        }\\n+\\n+        /// <summary>\\n+        /// Summarizes text to a maximum length.\\n+        /// </summary>\\n+        /// <param name=\\\"text\\\">The text to summarize.</param>\\n+        /// <returns>The summarized text.</returns>\\n+        public string SummarizeText(string text)\\n+        {\\n+            if (string.IsNullOrEmpty(text)) return text;\\n+\\n+            if (text.Length <= _maxSummaryLength)\\n+            {\\n+                return text;\\n+            }\\n+\\n+            var sentences = SplitIntoSentences(text);\\n+            var importantSentences = ExtractImportantSentences(sentences);\\n+\\n+            var summary = string.Empty;\\n+            foreach (var sentence in importantSentences)\\n+            {\\n+                if (summary.Length + sentence.Length > _maxSummaryLength)\\n+                {\\n+                    break;\\n+                }\\n+                summary += sentence + \\\" \\\";\\n+            }\\n+\\n+            return summary.Trim();\\n+        }\\n+\\n+        private List<string> ExtractImportantSentences(List<string> sentences)\\n+        {\\n+            var scored = new List<(string sentence, double score)>();\\n+\\n+            foreach (var sentence in sentences)\\n+            {\\n+                var importance = ComputeImportance(sentence, sentences);\\n+                scored.Add((sentence, importance));\\n+            }\\n+\\n+            return scored\\n+                .OrderByDescending(x => x.score)\\n+                .Select(x => x.sentence)\\n+                .ToList();\\n+        }\\n+\\n+        private double ComputeImportance(string sentence, List<string> allSentences)\\n+        {\\n+            var tokens = Tokenize(sentence);\\n+            var uniqueTokens = tokens.Distinct().Count();\\n+            var length = sentence.Length;\\n+\\n+            var positionScore = allSentences.IndexOf(sentence) == 0 ? 1.5 : 1.0;\\n+\\n+            var importance = (uniqueTokens * 0.5) + (Math.Min(length, 200) / 200.0 * 0.5);\\n+            importance *= positionScore;\\n+\\n+            return importance;\\n+        }\\n+\\n+        private List<string> SplitIntoSentences(string text)\\n+        {\\n+            var sentences = new List<string>();\\n+            var sentenceEndings = new[] { \\\". \\\", \\\"! \\\", \\\"? \\\", \\\".\\\\n\\\", \\\"!\\\\n\\\", \\\"?\\\\n\\\" };\\n+            var currentSentence = string.Empty;\\n+\\n+            for (int i = 0; i < text.Length; i++)\\n+            {\\n+                currentSentence += text[i];\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFIh\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTW15\",\"body\":\"String concatenation in loop: use 'StringBuilder'.\",\"path\":\"src/RetrievalAugmentedGeneration/ContextCompression/LLMContextCompressor.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,148 @@\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+using AiDotNet.Helpers;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.ContextCompression\\n+{\\n+    /// <summary>\\n+    /// LLM-based context compression to reduce token usage while preserving key information.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class LLMContextCompressor<T>\\n+    {\\n+        private readonly INumericOperations<T> _numOps;\\n+        private readonly string _llmEndpoint;\\n+        private readonly string _apiKey;\\n+        private readonly double _compressionRatio;\\n+\\n+        /// <summary>\\n+        /// Initializes a new instance of the <see cref=\\\"LLMContextCompressor{T}\\\"/> class.\\n+        /// </summary>\\n+        /// <param name=\\\"numericOperations\\\">The numeric operations for type T.</param>\\n+        /// <param name=\\\"compressionRatio\\\">The target compression ratio (0.0 to 1.0).</param>\\n+        /// <param name=\\\"llmEndpoint\\\">The LLM API endpoint.</param>\\n+        /// <param name=\\\"apiKey\\\">The API key for the LLM service.</param>\\n+        public LLMContextCompressor(\\n+            INumericOperations<T> numericOperations,\\n+            double compressionRatio = 0.5,\\n+            string llmEndpoint = \\\"\\\",\\n+            string apiKey = \\\"\\\")\\n+        {\\n+            _numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\\n+            _compressionRatio = compressionRatio >= 0 && compressionRatio <= 1\\n+                ? compressionRatio\\n+                : throw new ArgumentOutOfRangeException(nameof(compressionRatio), \\\"Compression ratio must be between 0 and 1\\\");\\n+            _llmEndpoint = llmEndpoint;\\n+            _apiKey = apiKey;\\n+        }\\n+\\n+        /// <summary>\\n+        /// Compresses a list of documents while preserving relevance to the query.\\n+        /// </summary>\\n+        /// <param name=\\\"query\\\">The query context.</param>\\n+        /// <param name=\\\"documents\\\">The documents to compress.</param>\\n+        /// <returns>A list of compressed documents.</returns>\\n+        public List<Document<T>> Compress(string query, List<Document<T>> documents)\\n+        {\\n+            if (string.IsNullOrEmpty(query)) throw new ArgumentNullException(nameof(query));\\n+            if (documents == null) throw new ArgumentNullException(nameof(documents));\\n+\\n+            var compressed = new List<Document<T>>();\\n+\\n+            foreach (var doc in documents)\\n+            {\\n+                var compressedContent = CompressText(query, doc.Content);\\n+                var compressedDoc = new Document<T>\\n+                {\\n+                    Id = doc.Id,\\n+                    Content = compressedContent,\\n+                    Metadata = doc.Metadata,\\n+                    RelevanceScore = doc.RelevanceScore\\n+                };\\n+                compressed.Add(compressedDoc);\\n+            }\\n+\\n+            return compressed;\\n+        }\\n+\\n+        /// <summary>\\n+        /// Compresses text based on relevance to the query.\\n+        /// </summary>\\n+        /// <param name=\\\"query\\\">The query context.</param>\\n+        /// <param name=\\\"text\\\">The text to compress.</param>\\n+        /// <returns>The compressed text.</returns>\\n+        public string CompressText(string query, string text)\\n+        {\\n+            if (string.IsNullOrEmpty(text)) return text;\\n+\\n+            var sentences = SplitIntoSentences(text);\\n+            var scoredSentences = new List<(string sentence, double score)>();\\n+\\n+            foreach (var sentence in sentences)\\n+            {\\n+                var relevance = ComputeRelevance(query, sentence);\\n+                scoredSentences.Add((sentence, relevance));\\n+            }\\n+\\n+            var targetCount = Math.Max(1, (int)(sentences.Count * _compressionRatio));\\n+            var topSentences = scoredSentences\\n+                .OrderByDescending(x => x.score)\\n+                .Take(targetCount)\\n+                .OrderBy(x => sentences.IndexOf(x.sentence))\\n+                .Select(x => x.sentence);\\n+\\n+            return string.Join(\\\" \\\", topSentences);\\n+        }\\n+\\n+        private double ComputeRelevance(string query, string sentence)\\n+        {\\n+            var queryTokens = Tokenize(query);\\n+            var sentenceTokens = Tokenize(sentence);\\n+\\n+            var overlap = queryTokens.Intersect(sentenceTokens).Count();\\n+            var total = Math.Max(queryTokens.Count, sentenceTokens.Count);\\n+\\n+            return total > 0 ? (double)overlap / total : 0.0;\\n+        }\\n+\\n+        private List<string> SplitIntoSentences(string text)\\n+        {\\n+            var sentences = new List<string>();\\n+            var sentenceEndings = new[] { \\\". \\\", \\\"! \\\", \\\"? \\\", \\\".\\\\n\\\", \\\"!\\\\n\\\", \\\"?\\\\n\\\" };\\n+            var currentSentence = string.Empty;\\n+\\n+            for (int i = 0; i < text.Length; i++)\\n+            {\\n+                currentSentence += text[i];\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFIk\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTW1_\",\"body\":\"Creating a 'StringBuilder' in a loop.\",\"path\":\"src/RetrievalAugmentedGeneration/ChunkingStrategies/CodeAwareTextSplitter.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,110 @@\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Text;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies\\n+{\\n+    /// <summary>\\n+    /// Code-aware text splitter that respects code structure and syntax.\\n+    /// </summary>\\n+    public class CodeAwareTextSplitter : ChunkingStrategyBase\\n+    {\\n+        private readonly string _language;\\n+\\n+        /// <summary>\\n+        /// Initializes a new instance of the <see cref=\\\"CodeAwareTextSplitter\\\"/> class.\\n+        /// </summary>\\n+        /// <param name=\\\"chunkSize\\\">The maximum size of each chunk.</param>\\n+        /// <param name=\\\"chunkOverlap\\\">The overlap between consecutive chunks.</param>\\n+        /// <param name=\\\"language\\\">The programming language (e.g., \\\"csharp\\\", \\\"python\\\", \\\"javascript\\\").</param>\\n+        public CodeAwareTextSplitter(int chunkSize = 1000, int chunkOverlap = 200, string language = \\\"csharp\\\")\\n+            : base(chunkSize, chunkOverlap)\\n+        {\\n+            _language = language ?? throw new ArgumentNullException(nameof(language));\\n+        }\\n+\\n+        /// <summary>\\n+        /// Chunks code while preserving code structure.\\n+        /// </summary>\\n+        /// <param name=\\\"text\\\">The code text to chunk.</param>\\n+        /// <returns>A collection of code chunks with positions.</returns>\\n+        protected override IEnumerable<(string Chunk, int StartPosition, int EndPosition)> ChunkCore(string text)\\n+        {\\n+            var separators = GetLanguageSeparators(_language);\\n+            var chunks = SplitTextRecursive(text, separators);\\n+            \\n+            var position = 0;\\n+            foreach (var chunk in chunks)\\n+            {\\n+                var endPos = position + chunk.Length;\\n+                yield return (chunk, position, endPos);\\n+                position = endPos - ChunkOverlap;\\n+            }\\n+        }\\n+\\n+        private string[] GetLanguageSeparators(string language)\\n+        {\\n+            switch (language.ToLowerInvariant())\\n+            {\\n+                case \\\"csharp\\\":\\n+                case \\\"c#\\\":\\n+                    return new[] { \\\"\\\\n    }\\\\n\\\", \\\"\\\\n}\\\\n\\\", \\\"\\\\n\\\\n\\\", \\\"\\\\n\\\", \\\" \\\" };\\n+                case \\\"python\\\":\\n+                    return new[] { \\\"\\\\ndef \\\", \\\"\\\\nclass \\\", \\\"\\\\n\\\\n\\\", \\\"\\\\n\\\", \\\" \\\" };\\n+                case \\\"javascript\\\":\\n+                case \\\"typescript\\\":\\n+                    return new[] { \\\"\\\\n}\\\\n\\\", \\\"\\\\n\\\\n\\\", \\\"\\\\n\\\", \\\" \\\" };\\n+                default:\\n+                    return new[] { \\\"\\\\n\\\\n\\\", \\\"\\\\n\\\", \\\" \\\" };\\n+            }\\n+        }\\n+\\n+        private List<string> SplitTextRecursive(string text, string[] separators)\\n+        {\\n+            var chunks = new List<string>();\\n+\\n+            if (text.Length <= ChunkSize)\\n+            {\\n+                chunks.Add(text);\\n+                return chunks;\\n+            }\\n+\\n+            foreach (var separator in separators)\\n+            {\\n+                var splits = text.Split(new[] { separator }, StringSplitOptions.None);\\n+                var currentChunk = new StringBuilder();\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFIs\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTW2H\",\"body\":\"Creating a 'StringBuilder' in a loop.\",\"path\":\"src/RetrievalAugmentedGeneration/ChunkingStrategies/MarkdownTextSplitter.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,89 @@\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Text;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies\\n+{\\n+    /// <summary>\\n+    /// Markdown-aware text splitter that respects markdown structure.\\n+    /// </summary>\\n+    public class MarkdownTextSplitter : ChunkingStrategyBase\\n+    {\\n+        /// <summary>\\n+        /// Initializes a new instance of the <see cref=\\\"MarkdownTextSplitter\\\"/> class.\\n+        /// </summary>\\n+        /// <param name=\\\"chunkSize\\\">The maximum size of each chunk.</param>\\n+        /// <param name=\\\"chunkOverlap\\\">The overlap between consecutive chunks.</param>\\n+        public MarkdownTextSplitter(int chunkSize = 1000, int chunkOverlap = 200)\\n+            : base(chunkSize, chunkOverlap)\\n+        {\\n+        }\\n+\\n+        /// <summary>\\n+        /// Chunks markdown text while preserving structure.\\n+        /// </summary>\\n+        /// <param name=\\\"text\\\">The markdown text to chunk.</param>\\n+        /// <returns>A collection of markdown chunks with positions.</returns>\\n+        protected override IEnumerable<(string Chunk, int StartPosition, int EndPosition)> ChunkCore(string text)\\n+        {\\n+            var separators = new[] { \\\"\\\\n## \\\", \\\"\\\\n### \\\", \\\"\\\\n#### \\\", \\\"\\\\n\\\\n\\\", \\\"\\\\n\\\", \\\". \\\", \\\" \\\" };\\n+            var chunks = SplitTextRecursive(text, separators);\\n+            \\n+            var position = 0;\\n+            foreach (var chunk in chunks)\\n+            {\\n+                var endPos = position + chunk.Length;\\n+                yield return (chunk, position, endPos);\\n+                position = endPos - ChunkOverlap;\\n+            }\\n+        }\\n+\\n+        private List<string> SplitTextRecursive(string text, string[] separators)\\n+        {\\n+            var chunks = new List<string>();\\n+\\n+            if (text.Length <= ChunkSize)\\n+            {\\n+                chunks.Add(text);\\n+                return chunks;\\n+            }\\n+\\n+            foreach (var separator in separators)\\n+            {\\n+                var splits = text.Split(new[] { separator }, StringSplitOptions.None);\\n+                var currentChunk = new StringBuilder();\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFIx\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTW2N\",\"body\":\"These 'if' statements can be combined.\\n```suggestion\\n                            currentChunk.Append(separator);\\n                        }\\n```\",\"path\":\"src/RetrievalAugmentedGeneration/ChunkingStrategies/CodeAwareTextSplitter.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,110 @@\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Text;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies\\n+{\\n+    /// <summary>\\n+    /// Code-aware text splitter that respects code structure and syntax.\\n+    /// </summary>\\n+    public class CodeAwareTextSplitter : ChunkingStrategyBase\\n+    {\\n+        private readonly string _language;\\n+\\n+        /// <summary>\\n+        /// Initializes a new instance of the <see cref=\\\"CodeAwareTextSplitter\\\"/> class.\\n+        /// </summary>\\n+        /// <param name=\\\"chunkSize\\\">The maximum size of each chunk.</param>\\n+        /// <param name=\\\"chunkOverlap\\\">The overlap between consecutive chunks.</param>\\n+        /// <param name=\\\"language\\\">The programming language (e.g., \\\"csharp\\\", \\\"python\\\", \\\"javascript\\\").</param>\\n+        public CodeAwareTextSplitter(int chunkSize = 1000, int chunkOverlap = 200, string language = \\\"csharp\\\")\\n+            : base(chunkSize, chunkOverlap)\\n+        {\\n+            _language = language ?? throw new ArgumentNullException(nameof(language));\\n+        }\\n+\\n+        /// <summary>\\n+        /// Chunks code while preserving code structure.\\n+        /// </summary>\\n+        /// <param name=\\\"text\\\">The code text to chunk.</param>\\n+        /// <returns>A collection of code chunks with positions.</returns>\\n+        protected override IEnumerable<(string Chunk, int StartPosition, int EndPosition)> ChunkCore(string text)\\n+        {\\n+            var separators = GetLanguageSeparators(_language);\\n+            var chunks = SplitTextRecursive(text, separators);\\n+            \\n+            var position = 0;\\n+            foreach (var chunk in chunks)\\n+            {\\n+                var endPos = position + chunk.Length;\\n+                yield return (chunk, position, endPos);\\n+                position = endPos - ChunkOverlap;\\n+            }\\n+        }\\n+\\n+        private string[] GetLanguageSeparators(string language)\\n+        {\\n+            switch (language.ToLowerInvariant())\\n+            {\\n+                case \\\"csharp\\\":\\n+                case \\\"c#\\\":\\n+                    return new[] { \\\"\\\\n    }\\\\n\\\", \\\"\\\\n}\\\\n\\\", \\\"\\\\n\\\\n\\\", \\\"\\\\n\\\", \\\" \\\" };\\n+                case \\\"python\\\":\\n+                    return new[] { \\\"\\\\ndef \\\", \\\"\\\\nclass \\\", \\\"\\\\n\\\\n\\\", \\\"\\\\n\\\", \\\" \\\" };\\n+                case \\\"javascript\\\":\\n+                case \\\"typescript\\\":\\n+                    return new[] { \\\"\\\\n}\\\\n\\\", \\\"\\\\n\\\\n\\\", \\\"\\\\n\\\", \\\" \\\" };\\n+                default:\\n+                    return new[] { \\\"\\\\n\\\\n\\\", \\\"\\\\n\\\", \\\" \\\" };\\n+            }\\n+        }\\n+\\n+        private List<string> SplitTextRecursive(string text, string[] separators)\\n+        {\\n+            var chunks = new List<string>();\\n+\\n+            if (text.Length <= ChunkSize)\\n+            {\\n+                chunks.Add(text);\\n+                return chunks;\\n+            }\\n+\\n+            foreach (var separator in separators)\\n+            {\\n+                var splits = text.Split(new[] { separator }, StringSplitOptions.None);\\n+                var currentChunk = new StringBuilder();\\n+\\n+                foreach (var split in splits)\\n+                {\\n+                    if (currentChunk.Length + split.Length + separator.Length > ChunkSize)\\n+                    {\\n+                        if (currentChunk.Length > 0)\\n+                        {\\n+                            chunks.Add(currentChunk.ToString());\\n+                            var overlap = Math.Min(ChunkOverlap, currentChunk.Length);\\n+                            currentChunk = new StringBuilder(currentChunk.ToString(currentChunk.Length - overlap, overlap));\\n+                        }\\n+                    }\\n+\\n+                    if (currentChunk.Length > 0)\\n+                    {\\n+                        currentChunk.Append(separator);\\n+                    }\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFI3\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTW2V\",\"body\":\"These 'if' statements can be combined.\\n```suggestion\\n                    if (currentChunk.Length + split.Length + separator.Length > ChunkSize && currentChunk.Length > 0)\\n                    {\\n                        chunks.Add(currentChunk.ToString());\\n                        var overlap = Math.Min(ChunkOverlap, currentChunk.Length);\\n                        currentChunk = new StringBuilder(currentChunk.ToString(currentChunk.Length - overlap, overlap));\\n```\",\"path\":\"src/RetrievalAugmentedGeneration/ChunkingStrategies/MarkdownTextSplitter.cs\",\"position\":68,\"diffHunk\":\"@@ -0,0 +1,89 @@\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Text;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies\\n+{\\n+    /// <summary>\\n+    /// Markdown-aware text splitter that respects markdown structure.\\n+    /// </summary>\\n+    public class MarkdownTextSplitter : ChunkingStrategyBase\\n+    {\\n+        /// <summary>\\n+        /// Initializes a new instance of the <see cref=\\\"MarkdownTextSplitter\\\"/> class.\\n+        /// </summary>\\n+        /// <param name=\\\"chunkSize\\\">The maximum size of each chunk.</param>\\n+        /// <param name=\\\"chunkOverlap\\\">The overlap between consecutive chunks.</param>\\n+        public MarkdownTextSplitter(int chunkSize = 1000, int chunkOverlap = 200)\\n+            : base(chunkSize, chunkOverlap)\\n+        {\\n+        }\\n+\\n+        /// <summary>\\n+        /// Chunks markdown text while preserving structure.\\n+        /// </summary>\\n+        /// <param name=\\\"text\\\">The markdown text to chunk.</param>\\n+        /// <returns>A collection of markdown chunks with positions.</returns>\\n+        protected override IEnumerable<(string Chunk, int StartPosition, int EndPosition)> ChunkCore(string text)\\n+        {\\n+            var separators = new[] { \\\"\\\\n## \\\", \\\"\\\\n### \\\", \\\"\\\\n#### \\\", \\\"\\\\n\\\\n\\\", \\\"\\\\n\\\", \\\". \\\", \\\" \\\" };\\n+            var chunks = SplitTextRecursive(text, separators);\\n+            \\n+            var position = 0;\\n+            foreach (var chunk in chunks)\\n+            {\\n+                var endPos = position + chunk.Length;\\n+                yield return (chunk, position, endPos);\\n+                position = endPos - ChunkOverlap;\\n+            }\\n+        }\\n+\\n+        private List<string> SplitTextRecursive(string text, string[] separators)\\n+        {\\n+            var chunks = new List<string>();\\n+\\n+            if (text.Length <= ChunkSize)\\n+            {\\n+                chunks.Add(text);\\n+                return chunks;\\n+            }\\n+\\n+            foreach (var separator in separators)\\n+            {\\n+                var splits = text.Split(new[] { separator }, StringSplitOptions.None);\\n+                var currentChunk = new StringBuilder();\\n+\\n+                foreach (var split in splits)\\n+                {\\n+                    if (currentChunk.Length + split.Length + separator.Length > ChunkSize)\\n+                    {\\n+                        if (currentChunk.Length > 0)\\n+                        {\\n+                            chunks.Add(currentChunk.ToString());\\n+                            var overlap = Math.Min(ChunkOverlap, currentChunk.Length);\\n+                            currentChunk = new StringBuilder(currentChunk.ToString(currentChunk.Length - overlap, overlap));\\n+                        }\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFJA\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTW2f\",\"body\":\"These 'if' statements can be combined.\\n```suggestion\\n                    if (currentChunk.Length + split.Length + separator.Length > ChunkSize && currentChunk.Length > 0)\\n                    {\\n                        chunks.Add(currentChunk.ToString());\\n                        var overlap = Math.Min(ChunkOverlap, currentChunk.Length);\\n                        currentChunk = new StringBuilder(currentChunk.ToString(currentChunk.Length - overlap, overlap));\\n```\",\"path\":\"src/RetrievalAugmentedGeneration/ChunkingStrategies/RecursiveCharacterTextSplitter.cs\",\"position\":82,\"diffHunk\":\"@@ -0,0 +1,106 @@\\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Text;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies\\n+{\\n+    /// <summary>\\n+    /// Recursive character-based text splitting that preserves semantic meaning.\\n+    /// </summary>\\n+    public class RecursiveCharacterTextSplitter : ChunkingStrategyBase\\n+    {\\n+        private readonly string[] _separators;\\n+\\n+        /// <summary>\\n+        /// Initializes a new instance of the <see cref=\\\"RecursiveCharacterTextSplitter\\\"/> class.\\n+        /// </summary>\\n+        /// <param name=\\\"chunkSize\\\">The maximum size of each chunk.</param>\\n+        /// <param name=\\\"chunkOverlap\\\">The overlap between consecutive chunks.</param>\\n+        /// <param name=\\\"separators\\\">The separators to use for splitting, in order of preference.</param>\\n+        public RecursiveCharacterTextSplitter(\\n+            int chunkSize = 1000,\\n+            int chunkOverlap = 200,\\n+            string[]? separators = null)\\n+            : base(chunkSize, chunkOverlap)\\n+        {\\n+            _separators = separators ?? new[] { \\\"\\\\n\\\\n\\\", \\\"\\\\n\\\", \\\". \\\", \\\" \\\", \\\"\\\" };\\n+        }\\n+\\n+        /// <summary>\\n+        /// Splits the input text into chunks recursively.\\n+        /// </summary>\\n+        /// <param name=\\\"text\\\">The text to split.</param>\\n+        /// <returns>A collection of text chunks with positions.</returns>\\n+        protected override IEnumerable<(string Chunk, int StartPosition, int EndPosition)> ChunkCore(string text)\\n+        {\\n+            var chunks = SplitTextRecursive(text, _separators);\\n+            \\n+            var position = 0;\\n+            foreach (var chunk in chunks)\\n+            {\\n+                var endPos = position + chunk.Length;\\n+                yield return (chunk, position, endPos);\\n+                position = endPos - ChunkOverlap;\\n+            }\\n+        }\\n+\\n+        private List<string> SplitTextRecursive(string text, string[] separators)\\n+        {\\n+            var chunks = new List<string>();\\n+\\n+            if (text.Length <= ChunkSize)\\n+            {\\n+                chunks.Add(text);\\n+                return chunks;\\n+            }\\n+\\n+            foreach (var separator in separators)\\n+            {\\n+                if (string.IsNullOrEmpty(separator))\\n+                {\\n+                    for (int i = 0; i < text.Length; i += ChunkSize - ChunkOverlap)\\n+                    {\\n+                        var length = Math.Min(ChunkSize, text.Length - i);\\n+                        chunks.Add(text.Substring(i, length));\\n+                    }\\n+                    return chunks;\\n+                }\\n+\\n+                var splits = text.Split(new[] { separator }, StringSplitOptions.None);\\n+                var currentChunk = new StringBuilder();\\n+\\n+                foreach (var split in splits)\\n+                {\\n+                    if (currentChunk.Length + split.Length + separator.Length > ChunkSize)\\n+                    {\\n+                        if (currentChunk.Length > 0)\\n+                        {\\n+                            chunks.Add(currentChunk.ToString());\\n+                            var overlap = Math.Min(ChunkOverlap, currentChunk.Length);\\n+                            currentChunk = new StringBuilder(currentChunk.ToString(currentChunk.Length - overlap, overlap));\\n+                        }\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFJE\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTW2m\",\"body\":\"Field '_avgDocLength' can be 'readonly'.\",\"path\":\"src/RetrievalAugmentedGeneration/Retrievers/BM25Retriever.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,124 @@\\n+using AiDotNet.Helpers;\\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\\n+{\\n+    /// <summary>\\n+    /// BM25 (Best Matching 25) retrieval algorithm for sparse keyword-based search.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class BM25Retriever<T> : RetrieverBase<T>\\n+    {\\n+        private readonly IDocumentStore<T> _documentStore;\\n+        private readonly T _k1;\\n+        private readonly T _b;\\n+        private readonly Dictionary<string, Dictionary<string, T>> _termFrequencies;\\n+        private readonly Dictionary<string, T> _documentLengths;\\n+        private T _avgDocLength;\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFJJ\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTW2s\",\"body\":\"Both branches of this 'if' statement write to the same variable - consider using '?' to express intent better.\\n```suggestion\\n                    combinedScores[doc.Id] = combinedScores.ContainsKey(doc.Id)\\n                        ? NumOps.Add(combinedScores[doc.Id], keywordScore)\\n                        : keywordScore;\\n```\",\"path\":\"src/RetrievalAugmentedGeneration/DocumentStores/HybridDocumentStore.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,124 @@\\n+using AiDotNet.Helpers;\\n+using AiDotNet.LinearAlgebra;\\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores\\n+{\\n+    /// <summary>\\n+    /// Hybrid document store combining vector and keyword search strategies.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class HybridDocumentStore<T> : DocumentStoreBase<T>\\n+    {\\n+        private readonly IDocumentStore<T> _vectorStore;\\n+        private readonly IDocumentStore<T> _keywordStore;\\n+        private readonly T _vectorWeight;\\n+        private readonly T _keywordWeight;\\n+\\n+        public override int DocumentCount => _vectorStore.DocumentCount;\\n+        public override int VectorDimension => _vectorStore.VectorDimension;\\n+\\n+        public HybridDocumentStore(\\n+            IDocumentStore<T> vectorStore,\\n+            IDocumentStore<T> keywordStore,\\n+            T vectorWeight,\\n+            T keywordWeight)\\n+        {\\n+            if (vectorStore == null)\\n+                throw new ArgumentNullException(nameof(vectorStore));\\n+            if (keywordStore == null)\\n+                throw new ArgumentNullException(nameof(keywordStore));\\n+\\n+            _vectorStore = vectorStore;\\n+            _keywordStore = keywordStore;\\n+            _vectorWeight = vectorWeight;\\n+            _keywordWeight = keywordWeight;\\n+        }\\n+\\n+        protected override void AddCore(VectorDocument<T> vectorDocument)\\n+        {\\n+            _vectorStore.Add(vectorDocument);\\n+            _keywordStore.Add(vectorDocument);\\n+        }\\n+\\n+        protected override void AddBatchCore(IList<VectorDocument<T>> vectorDocuments)\\n+        {\\n+            _vectorStore.AddBatch(vectorDocuments);\\n+            _keywordStore.AddBatch(vectorDocuments);\\n+        }\\n+\\n+        protected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)\\n+        {\\n+            var vectorResults = _vectorStore.GetSimilarWithFilters(queryVector, topK * 2, metadataFilters);\\n+            var keywordResults = _keywordStore.GetSimilarWithFilters(queryVector, topK * 2, metadataFilters);\\n+\\n+            var combinedScores = new Dictionary<string, T>();\\n+\\n+            foreach (var doc in vectorResults)\\n+            {\\n+                if (doc.HasRelevanceScore)\\n+                {\\n+                    var score = NumOps.Multiply(_vectorWeight, doc.RelevanceScore);\\n+                    combinedScores[doc.Id] = score;\\n+                }\\n+            }\\n+\\n+            foreach (var doc in keywordResults)\\n+            {\\n+                if (doc.HasRelevanceScore)\\n+                {\\n+                    var keywordScore = NumOps.Multiply(_keywordWeight, doc.RelevanceScore);\\n+                    if (combinedScores.ContainsKey(doc.Id))\\n+                    {\\n+                        combinedScores[doc.Id] = NumOps.Add(combinedScores[doc.Id], keywordScore);\\n+                    }\\n+                    else\\n+                    {\\n+                        combinedScores[doc.Id] = keywordScore;\\n+                    }\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFJP\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTW2z\",\"body\":\"Both branches of this 'if' statement write to the same variable - consider using '?' to express intent better.\",\"path\":\"src/RetrievalAugmentedGeneration/RerankingStrategies/ReciprocalRankFusion.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,122 @@\\n+using System;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+using AiDotNet.Helpers;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.RerankingStrategies\\n+{\\n+    /// <summary>\\n+    /// Reciprocal Rank Fusion for combining multiple ranking lists.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class ReciprocalRankFusion<T> : RerankingStrategyBase<T>\\n+    {\\n+        private readonly INumericOperations<T> _numOps;\\n+        private readonly int _k;\\n+\\n+        /// <summary>\\n+        /// Initializes a new instance of the <see cref=\\\"ReciprocalRankFusion{T}\\\"/> class.\\n+        /// </summary>\\n+        /// <param name=\\\"numericOperations\\\">The numeric operations for type T.</param>\\n+        /// <param name=\\\"k\\\">The constant k for reciprocal rank formula (default: 60).</param>\\n+        public ReciprocalRankFusion(INumericOperations<T> numericOperations, int k = 60) : base(numericOperations)\\n+        {\\n+            _numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\\n+            _k = k > 0 ? k : throw new ArgumentOutOfRangeException(nameof(k));\\n+        }\\n+\\n+        /// <summary>\\n+        /// Reranks documents using reciprocal rank fusion.\\n+        /// </summary>\\n+        /// <param name=\\\"query\\\">The query string.</param>\\n+        /// <param name=\\\"documents\\\">The documents to rerank.</param>\\n+        /// <param name=\\\"topK\\\">The number of top documents to return.</param>\\n+        /// <returns>A reranked list of documents.</returns>\\n+        public override List<Document<T>> Rerank(string query, List<Document<T>> documents, int topK)\\n+        {\\n+            if (string.IsNullOrEmpty(query)) throw new ArgumentNullException(nameof(query));\\n+            if (documents == null) throw new ArgumentNullException(nameof(documents));\\n+            if (topK <= 0) throw new ArgumentOutOfRangeException(nameof(topK));\\n+\\n+            var scores = new Dictionary<string, T>();\\n+\\n+            for (int rank = 0; rank < documents.Count; rank++)\\n+            {\\n+                var doc = documents[rank];\\n+                var rrfScore = _numOps.FromDouble(1.0 / (_k + rank + 1));\\n+\\n+                if (scores.ContainsKey(doc.Id))\\n+                {\\n+                    scores[doc.Id] = _numOps.Add(scores[doc.Id], rrfScore);\\n+                }\\n+                else\\n+                {\\n+                    scores[doc.Id] = rrfScore;\\n+                }\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZFJU\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTW28\",\"body\":\"Both branches of this 'if' statement write to the same variable - consider using '?' to express intent better.\",\"path\":\"src/RetrievalAugmentedGeneration/Retrievers/HybridRetriever.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,92 @@\\n+using AiDotNet.Helpers;\\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\\n+{\\n+    /// <summary>\\n+    /// Hybrid retriever combining dense and sparse retrieval strategies.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class HybridRetriever<T> : RetrieverBase<T>\\n+    {\\n+        private readonly IRetriever<T> _denseRetriever;\\n+        private readonly IRetriever<T> _sparseRetriever;\\n+        private readonly T _denseWeight;\\n+        private readonly T _sparseWeight;\\n+\\n+        public HybridRetriever(\\n+            IRetriever<T> denseRetriever, \\n+            IRetriever<T> sparseRetriever, \\n+            double denseWeight = 0.7, \\n+            double sparseWeight = 0.3,\\n+            int defaultTopK = 5) \\n+            : base(defaultTopK)\\n+        {\\n+            if (denseRetriever == null)\\n+                throw new ArgumentNullException(nameof(denseRetriever));\\n+            if (sparseRetriever == null)\\n+                throw new ArgumentNullException(nameof(sparseRetriever));\\n+\\n+            _denseRetriever = denseRetriever;\\n+            _sparseRetriever = sparseRetriever;\\n+            _denseWeight = NumOps.FromDouble(denseWeight);\\n+            _sparseWeight = NumOps.FromDouble(sparseWeight);\\n+        }\\n+\\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\\n+        {\\n+            var denseResults = _denseRetriever.Retrieve(query, topK * 2, metadataFilters);\\n+            var sparseResults = _sparseRetriever.Retrieve(query, topK * 2, metadataFilters);\\n+\\n+            var combinedScores = new Dictionary<string, T>();\\n+\\n+            foreach (var doc in denseResults)\\n+            {\\n+                if (doc.HasRelevanceScore)\\n+                {\\n+                    var score = NumOps.Multiply(_denseWeight, doc.RelevanceScore);\\n+                    combinedScores[doc.Id] = score;\\n+                }\\n+            }\\n+\\n+            foreach (var doc in sparseResults)\\n+            {\\n+                if (doc.HasRelevanceScore)\\n+                {\\n+                    var sparseScore = NumOps.Multiply(_sparseWeight, doc.RelevanceScore);\\n+                    if (combinedScores.ContainsKey(doc.Id))\\n+                    {\\n+                        combinedScores[doc.Id] = NumOps.Add(combinedScores[doc.Id], sparseScore);\\n+                    }\\n+                    else\\n+                    {\\n+                        combinedScores[doc.Id] = sparseScore;\\n+                    }\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZGL5\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTYQr\",\"body\":\"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\\n\\n**Fix HTML entity encoding in markdown code blocks.**\\n\\nLines 13 and 18 use `&lt;T&gt;` (HTML entities) instead of `<T>`. In raw markdown, generic syntax should use angle brackets directly.\\n\\n\\nApply this diff:\\n\\n```diff\\n 2. **Always include INumericOperations in base classes**:\\n-   `csharp\\n-   protected static readonly INumericOperations&lt;T&gt; NumOps = MathHelper.GetNumericOperations&lt;T&gt;();\\n-   `\\n+   ```csharp\\n+   protected static readonly INumericOperations<T> NumOps = MathHelper.GetNumericOperations<T>();\\n+   ```\\n\\n 4. **Use custom data types** - Prefer Vector&lt;T&gt;, Matrix&lt;T&gt;, Tensor&lt;T&gt; over arrays and collections\\n+4. **Use custom data types** - Prefer Vector<T>, Matrix<T>, Tensor<T> over arrays and collections\\n```\\n\\n\\nAlso applies to: 18-18\\n\\n<details>\\n<summary>≡ƒñû Prompt for AI Agents</summary>\\n\\n```\\nIn .github/AI_PROJECT_RULES.md around lines 11ΓÇô14 (and also line 18), fix\\nHTML-entity encoded generics by replacing &lt;T&gt; and\\nVector&lt;T&gt;/Matrix&lt;T&gt;/Tensor&lt;T&gt; with literal angle-bracket\\ngenerics (<T>, Vector<T>, Matrix<T>, Tensor<T>), and ensure the fenced code\\nblock is properly opened and closed (move or add the closing triple backticks\\nafter the C# code line) so the code renders correctly in Markdown.\\n```\\n\\n</details>\\n\\n<!-- This is an auto-generated comment by CodeRabbit -->\",\"path\":\".github/AI_PROJECT_RULES.md\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,72 @@\\n+# AI Assistant Project Rules\\n+\\n+## Code Architecture Rules\\n+\\n+### Generic Types and INumericOperations\\n+1. **NEVER use default(T)** - Instead use:\\n+   - NumOps.Zero for zero values\\n+   - NumOps.One for one values\\n+   - NumOps.FromDouble(value) to convert from double\\n+   \\n+2. **Always include INumericOperations in base classes**:\\n+   `csharp\\n+   protected static readonly INumericOperations&lt;T&gt; NumOps = MathHelper.GetNumericOperations&lt;T&gt;();\\n+   `\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZGMC\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTYQ4\",\"body\":\"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\\n\\n**Replace hardcoded developer path with relative or generic reference.**\\n\\nThe absolute Windows path `C:\\\\Users\\\\cheat\\\\source\\\\repos\\\\AiDotNet\\\\testconsole\\\\AiDotNetTestConsole.csproj` is not portable across machines, operating systems, or team members.\\n\\n\\nApply this diff:\\n\\n```diff\\n-2. **Examples go in separate test project** - C:\\\\Users\\\\cheat\\\\source\\\\repos\\\\AiDotNet\\\\testconsole\\\\AiDotNetTestConsole.csproj\\n+2. **Examples go in separate test project** - `testconsole/AiDotNetTestConsole.csproj`\\n```\\n\\n<!-- suggestion_start -->\\n\\n<details>\\n<summary>≡ƒô¥ Committable suggestion</summary>\\n\\n> ΓÇ╝∩╕Å **IMPORTANT**\\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\\n\\n```suggestion\\n2. **Examples go in separate test project** - `testconsole/AiDotNetTestConsole.csproj`\\n```\\n\\n</details>\\n\\n<!-- suggestion_end -->\\n\\n<details>\\n<summary>≡ƒñû Prompt for AI Agents</summary>\\n\\n```\\nIn .github/AI_PROJECT_RULES.md around line 56, replace the hardcoded absolute\\nWindows path\\n\\\"C:\\\\Users\\\\cheat\\\\source\\\\repos\\\\AiDotNet\\\\testconsole\\\\AiDotNetTestConsole.csproj\\\"\\nwith a relative or generic reference; update it to a repo-relative path (e.g.,\\n./testconsole/AiDotNetTestConsole.csproj) or a platform-agnostic placeholder\\nlike <path-to-test-project>/AiDotNetTestConsole.csproj so the rule is portable\\nacross machines and OSes.\\n```\\n\\n</details>\\n\\n<!-- This is an auto-generated comment by CodeRabbit -->\",\"path\":\".github/AI_PROJECT_RULES.md\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,72 @@\\n+# AI Assistant Project Rules\\n+\\n+## Code Architecture Rules\\n+\\n+### Generic Types and INumericOperations\\n+1. **NEVER use default(T)** - Instead use:\\n+   - NumOps.Zero for zero values\\n+   - NumOps.One for one values\\n+   - NumOps.FromDouble(value) to convert from double\\n+   \\n+2. **Always include INumericOperations in base classes**:\\n+   `csharp\\n+   protected static readonly INumericOperations&lt;T&gt; NumOps = MathHelper.GetNumericOperations&lt;T&gt;();\\n+   `\\n+\\n+3. **Use generic types everywhere** - Avoid hardcoding double, float, int, etc.\\n+\\n+4. **Use custom data types** - Prefer Vector&lt;T&gt;, Matrix&lt;T&gt;, Tensor&lt;T&gt; over arrays and collections\\n+\\n+5. **NEVER use constraints like where T : struct** - Our architecture doesn't use these\\n+\\n+### Inheritance Pattern\\n+1. **Always create interfaces first**\\n+2. **Then create base classes with common methods**\\n+3. **Concrete implementations MUST inherit from base class, NOT directly from interface**\\n+\\n+### Documentation\\n+1. **Follow existing documentation format exactly**\\n+2. **Include XML documentation for all public members**\\n+3. **Add beginner-friendly explanations in remarks**\\n+\\n+## File Management Rules\\n+\\n+### DO NOT Create Unless Explicitly Requested\\n+1. **NO README files** (AGENTS.md, STATUS.md, REPORT.md, etc.)\\n+2. **NO one-off scripts** unless specifically asked\\n+3. **NO temporary analysis files**\\n+4. **NO status tracking markdown files**\\n+\\n+### What TO Create\\n+1. Source code files (.cs)\\n+2. Configuration files when needed for functionality\\n+3. Documentation only when explicitly requested by name/path\\n+\\n+## Git Workflow\\n+\\n+1. **Always work on feature branches**\\n+2. **Commit early and often**\\n+3. **Use descriptive commit messages**\\n+4. **Create PR when complete**\\n+5. **DO NOT close PRs**, fix them properly instead\\n+\\n+## Code Organization\\n+\\n+1. **Folders match namespaces** - No \\\"Base\\\" folder, use proper namespace hierarchy\\n+2. **Examples go in separate test project** - C:\\\\Users\\\\cheat\\\\source\\\\repos\\\\AiDotNet\\\\testconsole\\\\AiDotNetTestConsole.csproj\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZGMJ\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTYRH\",\"body\":\"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\\n\\n**Enforce the configured chunk size even when no separators match.**\\n\\nRight now, if a `split` exceeds `ChunkSize` (common with long minified code or binary blobs), we append it wholesale. That produces chunks larger than the configured limit and defeats overlap logic, which can explode memory usage. Please add the same fallback the other splitters useΓÇöslice `split` into `ChunkSize`-bounded segments (respecting overlap) before continuing.\\n\\n\\n\\n```diff\\n                     if (currentChunk.Length + split.Length + separator.Length > ChunkSize)\\n                     {\\n                         if (currentChunk.Length > 0)\\n                         {\\n                             chunks.Add(currentChunk.ToString());\\n                             var overlap = Math.Min(ChunkOverlap, currentChunk.Length);\\n                             currentChunk = new StringBuilder(currentChunk.ToString(currentChunk.Length - overlap, overlap));\\n                         }\\n                     }\\n \\n+                    if (currentChunk.Length == 0 && split.Length > ChunkSize)\\n+                    {\\n+                        var step = Math.Max(ChunkSize - ChunkOverlap, 1);\\n+                        for (int i = 0; i < split.Length; i += step)\\n+                        {\\n+                            var length = Math.Min(ChunkSize, split.Length - i);\\n+                            chunks.Add(split.Substring(i, length));\\n+                        }\\n+                        currentChunk.Clear();\\n+                        continue;\\n+                    }\\n+\\n                     if (currentChunk.Length > 0)\\n                     {\\n                         currentChunk.Append(separator);\\n                     }\\n                     currentChunk.Append(split);\\n```\\n\\n<details>\\n<summary>≡ƒñû Prompt for AI Agents</summary>\\n\\n```\\nIn src/RetrievalAugmentedGeneration/ChunkingStrategies/CodeAwareTextSplitter.cs\\naround lines 77 to 99, the loop currently appends a single `split` even when its\\nlength exceeds `ChunkSize`, producing oversized chunks; modify the logic so that\\nbefore appending a too-large `split` you slice it into multiple segments each no\\nlonger than `ChunkSize`, adding each segment to `chunks` and preserving\\n`ChunkOverlap` by starting the next buffer with the last `overlap` chars of the\\nprevious segment; ensure separators are only inserted between segments as\\ncurrently intended and that after processing all full-sized segments you\\ncontinue with the remaining tail (if any) into `currentChunk` so overlap and\\nfinal flush behave the same as for other splitters.\\n```\\n\\n</details>\\n\\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\\n\\n<!-- This is an auto-generated comment by CodeRabbit -->\",\"path\":\"src/RetrievalAugmentedGeneration/ChunkingStrategies/CodeAwareTextSplitter.cs\",\"position\":113,\"diffHunk\":\"@@ -0,0 +1,110 @@\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Text;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies\\n+{\\n+    /// <summary>\\n+    /// Code-aware text splitter that respects code structure and syntax.\\n+    /// </summary>\\n+    public class CodeAwareTextSplitter : ChunkingStrategyBase\\n+    {\\n+        private readonly string _language;\\n+\\n+        /// <summary>\\n+        /// Initializes a new instance of the <see cref=\\\"CodeAwareTextSplitter\\\"/> class.\\n+        /// </summary>\\n+        /// <param name=\\\"chunkSize\\\">The maximum size of each chunk.</param>\\n+        /// <param name=\\\"chunkOverlap\\\">The overlap between consecutive chunks.</param>\\n+        /// <param name=\\\"language\\\">The programming language (e.g., \\\"csharp\\\", \\\"python\\\", \\\"javascript\\\").</param>\\n+        public CodeAwareTextSplitter(int chunkSize = 1000, int chunkOverlap = 200, string language = \\\"csharp\\\")\\n+            : base(chunkSize, chunkOverlap)\\n+        {\\n+            _language = language ?? throw new ArgumentNullException(nameof(language));\\n+        }\\n+\\n+        /// <summary>\\n+        /// Chunks code while preserving code structure.\\n+        /// </summary>\\n+        /// <param name=\\\"text\\\">The code text to chunk.</param>\\n+        /// <returns>A collection of code chunks with positions.</returns>\\n+        protected override IEnumerable<(string Chunk, int StartPosition, int EndPosition)> ChunkCore(string text)\\n+        {\\n+            var separators = GetLanguageSeparators(_language);\\n+            var chunks = SplitTextRecursive(text, separators);\\n+            \\n+            var position = 0;\\n+            foreach (var chunk in chunks)\\n+            {\\n+                var endPos = position + chunk.Length;\\n+                yield return (chunk, position, endPos);\\n+                position = endPos - ChunkOverlap;\\n+            }\\n+        }\\n+\\n+        private string[] GetLanguageSeparators(string language)\\n+        {\\n+            switch (language.ToLowerInvariant())\\n+            {\\n+                case \\\"csharp\\\":\\n+                case \\\"c#\\\":\\n+                    return new[] { \\\"\\\\n    }\\\\n\\\", \\\"\\\\n}\\\\n\\\", \\\"\\\\n\\\\n\\\", \\\"\\\\n\\\", \\\" \\\" };\\n+                case \\\"python\\\":\\n+                    return new[] { \\\"\\\\ndef \\\", \\\"\\\\nclass \\\", \\\"\\\\n\\\\n\\\", \\\"\\\\n\\\", \\\" \\\" };\\n+                case \\\"javascript\\\":\\n+                case \\\"typescript\\\":\\n+                    return new[] { \\\"\\\\n}\\\\n\\\", \\\"\\\\n\\\\n\\\", \\\"\\\\n\\\", \\\" \\\" };\\n+                default:\\n+                    return new[] { \\\"\\\\n\\\\n\\\", \\\"\\\\n\\\", \\\" \\\" };\\n+            }\\n+        }\\n+\\n+        private List<string> SplitTextRecursive(string text, string[] separators)\\n+        {\\n+            var chunks = new List<string>();\\n+\\n+            if (text.Length <= ChunkSize)\\n+            {\\n+                chunks.Add(text);\\n+                return chunks;\\n+            }\\n+\\n+            foreach (var separator in separators)\\n+            {\\n+                var splits = text.Split(new[] { separator }, StringSplitOptions.None);\\n+                var currentChunk = new StringBuilder();\\n+\\n+                foreach (var split in splits)\\n+                {\\n+                    if (currentChunk.Length + split.Length + separator.Length > ChunkSize)\\n+                    {\\n+                        if (currentChunk.Length > 0)\\n+                        {\\n+                            chunks.Add(currentChunk.ToString());\\n+                            var overlap = Math.Min(ChunkOverlap, currentChunk.Length);\\n+                            currentChunk = new StringBuilder(currentChunk.ToString(currentChunk.Length - overlap, overlap));\\n+                        }\\n+                    }\\n+\\n+                    if (currentChunk.Length > 0)\\n+                    {\\n+                        currentChunk.Append(separator);\\n+                    }\\n+                    currentChunk.Append(split);\\n+                }\\n+\\n+                if (currentChunk.Length > 0)\\n+                {\\n+                    chunks.Add(currentChunk.ToString());\\n+                }\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZGMK\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTYRQ\",\"body\":\"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\\n\\n**Semantic chunker never applies semantics or overlap**\\n\\nThis implementation never uses `_similarityThreshold` (or any embeddings), so we just stream sentences until `ChunkSize` is hitΓÇöidentical to a plain size-based splitter despite the class being wired for semantic grouping. Additionally, when you flush a chunk you `Clear()` the buffer without seeding it with the trailing `ChunkOverlap`, so downstream consumers get no overlapping content even though the returned positions imply there is one. Please wire the chunker into actual similarity scoring (or drop the parameter) and keep the configured overlap in the emitted chunks.\\n\\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\\n\\n<!-- This is an auto-generated comment by CodeRabbit -->\\n\\nΓ£à Addressed in commits ec6a7c9 to 77a1bbb\",\"path\":\"src/RetrievalAugmentedGeneration/ChunkingStrategies/SemanticChunkingStrategy.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,99 @@\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+using AiDotNet.Helpers;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies\\n+{\\n+    /// <summary>\\n+    /// Semantic-based text chunking that uses embeddings to group related content.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class SemanticChunkingStrategy<T> : ChunkingStrategyBase\\n+    {\\n+        private readonly INumericOperations<T> _numOps;\\n+        private readonly T _similarityThreshold;\\n+\\n+        /// <summary>\\n+        /// Initializes a new instance of the <see cref=\\\"SemanticChunkingStrategy{T}\\\"/> class.\\n+        /// </summary>\\n+        /// <param name=\\\"numericOperations\\\">The numeric operations for type T.</param>\\n+        /// <param name=\\\"similarityThreshold\\\">The similarity threshold for grouping sentences.</param>\\n+        /// <param name=\\\"maxChunkSize\\\">The maximum chunk size in characters.</param>\\n+        public SemanticChunkingStrategy(\\n+            INumericOperations<T> numericOperations,\\n+            T similarityThreshold,\\n+            int maxChunkSize = 1000,\\n+            int chunkOverlap = 200)\\n+            : base(maxChunkSize, chunkOverlap)\\n+        {\\n+            _numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\\n+            _similarityThreshold = similarityThreshold;\\n+        }\\n+\\n+        /// <summary>\\n+        /// Chunks text based on semantic similarity between sentences.\\n+        /// </summary>\\n+        /// <param name=\\\"text\\\">The text to chunk.</param>\\n+        /// <returns>A collection of semantically coherent chunks with positions.</returns>\\n+        protected override IEnumerable<(string Chunk, int StartPosition, int EndPosition)> ChunkCore(string text)\\n+        {\\n+            var sentences = SplitIntoSentences(text);\\n+            var currentChunk = new List<string>();\\n+            var currentSize = 0;\\n+            var position = 0;\\n+\\n+            foreach (var sentence in sentences)\\n+            {\\n+                if (currentSize + sentence.Length > ChunkSize && currentChunk.Count > 0)\\n+                {\\n+                    var chunkText = string.Join(\\\" \\\", currentChunk);\\n+                    var endPos = position + chunkText.Length;\\n+                    yield return (chunkText, position, endPos);\\n+                    \\n+                    position = endPos - ChunkOverlap;\\n+                    currentChunk.Clear();\\n+                    currentSize = 0;\\n+                }\\n+\\n+                currentChunk.Add(sentence);\\n+                currentSize += sentence.Length;\\n+            }\\n+\\n+            if (currentChunk.Count > 0)\\n+            {\\n+                var chunkText = string.Join(\\\" \\\", currentChunk);\\n+                yield return (chunkText, position, position + chunkText.Length);\\n+            }\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZGMO\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTYRV\",\"body\":\"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\\n\\n**Prevent negative chunk overlap when stride exceeds window size**\\n\\nPassing `windowSize - stride` straight to the base class lets `ChunkOverlap` go negative whenever `stride > windowSize`. Downstream consumers rely on that value being non-negative (e.g., for position math and overlap metadata), so this silently corrupts chunk metadata and can break overlap-sensitive logic. Please clamp or reject oversize strides before calling the base constructor.\\n\\n\\n\\nApply this diff:\\n\\n```diff\\n-        public SlidingWindowChunkingStrategy(int windowSize = 1000, int stride = 500)\\n-            : base(windowSize, windowSize - stride)\\n+        public SlidingWindowChunkingStrategy(int windowSize = 1000, int stride = 500)\\n+            : base(windowSize, Math.Max(0, windowSize - stride))\\n         {\\n-            _stride = stride > 0 ? stride : throw new ArgumentOutOfRangeException(nameof(stride));\\n+            if (stride <= 0) throw new ArgumentOutOfRangeException(nameof(stride));\\n+            if (stride > windowSize)\\n+                throw new ArgumentOutOfRangeException(nameof(stride), \\\"Stride cannot exceed the window size because chunk overlap must stay non-negative.\\\");\\n+            _stride = stride;\\n         }\\n```\\n\\n<!-- suggestion_start -->\\n\\n<details>\\n<summary>≡ƒô¥ Committable suggestion</summary>\\n\\n> ΓÇ╝∩╕Å **IMPORTANT**\\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\\n\\n```suggestion\\n        public SlidingWindowChunkingStrategy(int windowSize = 1000, int stride = 500)\\n            : base(windowSize, Math.Max(0, windowSize - stride))\\n        {\\n            if (stride <= 0) throw new ArgumentOutOfRangeException(nameof(stride));\\n            if (stride > windowSize)\\n                throw new ArgumentOutOfRangeException(nameof(stride), \\\"Stride cannot exceed the window size because chunk overlap must stay non-negative.\\\");\\n            _stride = stride;\\n        }\\n```\\n\\n</details>\\n\\n<!-- suggestion_end -->\\n\\n<details>\\n<summary>≡ƒñû Prompt for AI Agents</summary>\\n\\n```\\nIn\\nsrc/RetrievalAugmentedGeneration/ChunkingStrategies/SlidingWindowChunkingStrategy.cs\\naround lines 18 to 22, the constructor currently passes windowSize - stride\\ndirectly to the base which can produce a negative ChunkOverlap when stride >\\nwindowSize; change the constructor to ensure ChunkOverlap is non-negative by\\ncomputing an overlap value (e.g., overlap = Math.Max(0, windowSize - stride))\\nand pass that overlap to the base constructor, and also validate stride is\\nnon-negative (throw ArgumentOutOfRangeException if stride < 0) before assigning\\n_stride.\\n```\\n\\n</details>\\n\\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\\n\\n<!-- This is an auto-generated comment by CodeRabbit -->\",\"path\":\"src/RetrievalAugmentedGeneration/ChunkingStrategies/SlidingWindowChunkingStrategy.cs\",\"position\":25,\"diffHunk\":\"@@ -0,0 +1,44 @@\\n+using System;\\n+using System.Collections.Generic;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies\\n+{\\n+    /// <summary>\\n+    /// Sliding window chunking strategy with configurable window size and stride.\\n+    /// </summary>\\n+    public class SlidingWindowChunkingStrategy : ChunkingStrategyBase\\n+    {\\n+        private readonly int _stride;\\n+\\n+        /// <summary>\\n+        /// Initializes a new instance of the <see cref=\\\"SlidingWindowChunkingStrategy\\\"/> class.\\n+        /// </summary>\\n+        /// <param name=\\\"windowSize\\\">The size of the sliding window.</param>\\n+        /// <param name=\\\"stride\\\">The stride (step size) of the window.</param>\\n+        public SlidingWindowChunkingStrategy(int windowSize = 1000, int stride = 500)\\n+            : base(windowSize, windowSize - stride)\\n+        {\\n+            _stride = stride > 0 ? stride : throw new ArgumentOutOfRangeException(nameof(stride));\\n+        }\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZGMS\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTYRg\",\"body\":\"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\\n\\n**Consider secure handling guidance for API keys.**\\n\\nStoring API keys as plain strings can lead to inadvertent exposure through logging, serialization, or debugging output. While configuration POCOs typically don't enforce security directly, consider adding XML documentation that warns consumers to protect this value and avoid logging it.\\n\\n\\n\\nExample documentation addition:\\n\\n```diff\\n /// <summary>\\n /// Gets or sets the API key for the embedding service.\\n /// </summary>\\n+/// <remarks>\\n+/// <b>Security:</b> This value should be protected. Do not log, serialize to disk unencrypted, \\n+/// or include in error messages. Consider using secure configuration providers.\\n+/// </remarks>\\n public string ApiKey { get; set; } = string.Empty;\\n```\\n\\n<!-- suggestion_start -->\\n\\n<details>\\n<summary>≡ƒô¥ Committable suggestion</summary>\\n\\n> ΓÇ╝∩╕Å **IMPORTANT**\\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\\n\\n```suggestion\\n        /// <summary>\\n        /// Gets or sets the API key for the embedding service.\\n        /// </summary>\\n        /// <remarks>\\n        /// <b>Security:</b> This value should be protected. Do not log, serialize to disk unencrypted, \\n        /// or include in error messages. Consider using secure configuration providers.\\n        /// </remarks>\\n        public string ApiKey { get; set; } = string.Empty;\\n```\\n\\n</details>\\n\\n<!-- suggestion_end -->\\n\\n<details>\\n<summary>≡ƒñû Prompt for AI Agents</summary>\\n\\n```\\nIn src/RetrievalAugmentedGeneration/Configuration/EmbeddingConfig.cs around\\nlines 20 to 23, the ApiKey property lacks guidance about secure handling; update\\nits XML documentation to warn consumers not to log, serialize, or expose the API\\nkey, and recommend retrieving it from a secure secrets store or protected\\nconfiguration (e.g., environment variables, Azure Key Vault, or IOptions bound\\nto a secrets provider) rather than embedding it in source or plain-text config;\\noptionally mention using secure types or transient in-memory storage and to\\ntreat the property as sensitive when serializing or debugging.\\n```\\n\\n</details>\\n\\n<!-- This is an auto-generated comment by CodeRabbit -->\",\"path\":\"src/RetrievalAugmentedGeneration/Configuration/EmbeddingConfig.cs\",\"position\":28,\"diffHunk\":\"@@ -0,0 +1,35 @@\\n+using System.Collections.Generic;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.Configuration\\n+{\\n+    /// <summary>\\n+    /// Configuration for embedding models.\\n+    /// </summary>\\n+    public class EmbeddingConfig\\n+    {\\n+        /// <summary>\\n+        /// Gets or sets the embedding model type.\\n+        /// </summary>\\n+        public string ModelType { get; set; } = string.Empty;\\n+\\n+        /// <summary>\\n+        /// Gets or sets the path to the embedding model.\\n+        /// </summary>\\n+        public string ModelPath { get; set; } = string.Empty;\\n+\\n+        /// <summary>\\n+        /// Gets or sets the API key for the embedding service.\\n+        /// </summary>\\n+        public string ApiKey { get; set; } = string.Empty;\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZGMb\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTYRs\",\"body\":\"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\\n\\n**Preserve relevance metadata when cloning documents**\\n\\nIncoming documents can arrive with `HasRelevanceScore = true`, but this clone drops that flag, so downstream steps treat the copied `RelevanceScore` as unset. Please propagate the flag.  \\n\\n\\n```diff\\n                 var summarizedDoc = new Document<T>\\n                 {\\n                     Id = doc.Id,\\n                     Content = summary,\\n                     Metadata = doc.Metadata,\\n-                    RelevanceScore = doc.RelevanceScore\\n+                    RelevanceScore = doc.RelevanceScore,\\n+                    HasRelevanceScore = doc.HasRelevanceScore\\n                 };\\n```\\n\\n<!-- suggestion_start -->\\n\\n<details>\\n<summary>≡ƒô¥ Committable suggestion</summary>\\n\\n> ΓÇ╝∩╕Å **IMPORTANT**\\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\\n\\n```suggestion\\n                var summarizedDoc = new Document<T>\\n                {\\n                    Id = doc.Id,\\n                    Content = summary,\\n                    Metadata = doc.Metadata,\\n                    RelevanceScore = doc.RelevanceScore,\\n                    HasRelevanceScore = doc.HasRelevanceScore\\n                };\\n                summarized.Add(summarizedDoc);\\n```\\n\\n</details>\\n\\n<!-- suggestion_end -->\\n\\n<details>\\n<summary>≡ƒñû Prompt for AI Agents</summary>\\n\\n```\\nIn src/RetrievalAugmentedGeneration/ContextCompression/DocumentSummarizer.cs\\naround lines 55 to 62, the cloned Document is copying RelevanceScore but not the\\nHasRelevanceScore flag, so downstream logic treats the score as unset; update\\nthe clone to also set HasRelevanceScore = doc.HasRelevanceScore (or the\\nequivalent flag/property name used on Document<T>) when creating summarizedDoc\\nso the relevance metadata is preserved.\\n```\\n\\n</details>\\n\\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\\n\\n<!-- This is an auto-generated comment by CodeRabbit -->\",\"path\":\"src/RetrievalAugmentedGeneration/ContextCompression/DocumentSummarizer.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,166 @@\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+using AiDotNet.Helpers;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.ContextCompression\\n+{\\n+    /// <summary>\\n+    /// Document summarizer for creating concise summaries of retrieved content.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class DocumentSummarizer<T>\\n+    {\\n+        private readonly INumericOperations<T> _numOps;\\n+        private readonly int _maxSummaryLength;\\n+        private readonly string _llmEndpoint;\\n+        private readonly string _apiKey;\\n+\\n+        /// <summary>\\n+        /// Initializes a new instance of the <see cref=\\\"DocumentSummarizer{T}\\\"/> class.\\n+        /// </summary>\\n+        /// <param name=\\\"numericOperations\\\">The numeric operations for type T.</param>\\n+        /// <param name=\\\"maxSummaryLength\\\">The maximum length of the summary in characters.</param>\\n+        /// <param name=\\\"llmEndpoint\\\">The LLM API endpoint.</param>\\n+        /// <param name=\\\"apiKey\\\">The API key for the LLM service.</param>\\n+        public DocumentSummarizer(\\n+            INumericOperations<T> numericOperations,\\n+            int maxSummaryLength = 500,\\n+            string llmEndpoint = \\\"\\\",\\n+            string apiKey = \\\"\\\")\\n+        {\\n+            _numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\\n+            _maxSummaryLength = maxSummaryLength > 0\\n+                ? maxSummaryLength\\n+                : throw new ArgumentOutOfRangeException(nameof(maxSummaryLength));\\n+            _llmEndpoint = llmEndpoint;\\n+            _apiKey = apiKey;\\n+        }\\n+\\n+        /// <summary>\\n+        /// Summarizes a list of documents.\\n+        /// </summary>\\n+        /// <param name=\\\"documents\\\">The documents to summarize.</param>\\n+        /// <returns>A list of summarized documents.</returns>\\n+        public List<Document<T>> Summarize(List<Document<T>> documents)\\n+        {\\n+            if (documents == null) throw new ArgumentNullException(nameof(documents));\\n+\\n+            var summarized = new List<Document<T>>();\\n+\\n+            foreach (var doc in documents)\\n+            {\\n+                var summary = SummarizeText(doc.Content);\\n+                var summarizedDoc = new Document<T>\\n+                {\\n+                    Id = doc.Id,\\n+                    Content = summary,\\n+                    Metadata = doc.Metadata,\\n+                    RelevanceScore = doc.RelevanceScore\\n+                };\\n+                summarized.Add(summarizedDoc);\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZGMh\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTYR4\",\"body\":\"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\\n\\n**Handle single long sentences without returning an empty summary**\\n\\nWhen the first (or only) sentence exceeds `_maxSummaryLength`, the loop breaks without appending anything and the method returns `string.Empty`. We should still return a truncated slice of the original text.  \\n\\n\\n```diff\\n             foreach (var sentence in importantSentences)\\n             {\\n                 if (summary.Length + sentence.Length > _maxSummaryLength)\\n                 {\\n-                    break;\\n+                    if (summary.Length == 0)\\n+                    {\\n+                        return text.Substring(0, Math.Min(text.Length, _maxSummaryLength));\\n+                    }\\n+                    break;\\n                 }\\n                 summary += sentence + \\\" \\\";\\n             }\\n \\n-            return summary.Trim();\\n+            return summary.Trim();\\n```\\n\\n<details>\\n<summary>≡ƒñû Prompt for AI Agents</summary>\\n\\n```\\nIn src/RetrievalAugmentedGeneration/ContextCompression/DocumentSummarizer.cs\\naround lines 86 to 95, the loop breaks when the first (or only) sentence is\\nlonger than _maxSummaryLength causing an empty return; modify the logic so that\\nif no sentence has been appended and the current sentence length exceeds\\n_maxSummaryLength you append a truncated substring of that sentence up to\\n_maxSummaryLength (then break), otherwise keep the existing append-and-break\\nbehavior, and finally return the trimmed summary.\\n```\\n\\n</details>\\n\\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\\n\\n<!-- This is an auto-generated comment by CodeRabbit -->\",\"path\":\"src/RetrievalAugmentedGeneration/ContextCompression/DocumentSummarizer.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,166 @@\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+using AiDotNet.Helpers;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.ContextCompression\\n+{\\n+    /// <summary>\\n+    /// Document summarizer for creating concise summaries of retrieved content.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class DocumentSummarizer<T>\\n+    {\\n+        private readonly INumericOperations<T> _numOps;\\n+        private readonly int _maxSummaryLength;\\n+        private readonly string _llmEndpoint;\\n+        private readonly string _apiKey;\\n+\\n+        /// <summary>\\n+        /// Initializes a new instance of the <see cref=\\\"DocumentSummarizer{T}\\\"/> class.\\n+        /// </summary>\\n+        /// <param name=\\\"numericOperations\\\">The numeric operations for type T.</param>\\n+        /// <param name=\\\"maxSummaryLength\\\">The maximum length of the summary in characters.</param>\\n+        /// <param name=\\\"llmEndpoint\\\">The LLM API endpoint.</param>\\n+        /// <param name=\\\"apiKey\\\">The API key for the LLM service.</param>\\n+        public DocumentSummarizer(\\n+            INumericOperations<T> numericOperations,\\n+            int maxSummaryLength = 500,\\n+            string llmEndpoint = \\\"\\\",\\n+            string apiKey = \\\"\\\")\\n+        {\\n+            _numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\\n+            _maxSummaryLength = maxSummaryLength > 0\\n+                ? maxSummaryLength\\n+                : throw new ArgumentOutOfRangeException(nameof(maxSummaryLength));\\n+            _llmEndpoint = llmEndpoint;\\n+            _apiKey = apiKey;\\n+        }\\n+\\n+        /// <summary>\\n+        /// Summarizes a list of documents.\\n+        /// </summary>\\n+        /// <param name=\\\"documents\\\">The documents to summarize.</param>\\n+        /// <returns>A list of summarized documents.</returns>\\n+        public List<Document<T>> Summarize(List<Document<T>> documents)\\n+        {\\n+            if (documents == null) throw new ArgumentNullException(nameof(documents));\\n+\\n+            var summarized = new List<Document<T>>();\\n+\\n+            foreach (var doc in documents)\\n+            {\\n+                var summary = SummarizeText(doc.Content);\\n+                var summarizedDoc = new Document<T>\\n+                {\\n+                    Id = doc.Id,\\n+                    Content = summary,\\n+                    Metadata = doc.Metadata,\\n+                    RelevanceScore = doc.RelevanceScore\\n+                };\\n+                summarized.Add(summarizedDoc);\\n+            }\\n+\\n+            return summarized;\\n+        }\\n+\\n+        /// <summary>\\n+        /// Summarizes text to a maximum length.\\n+        /// </summary>\\n+        /// <param name=\\\"text\\\">The text to summarize.</param>\\n+        /// <returns>The summarized text.</returns>\\n+        public string SummarizeText(string text)\\n+        {\\n+            if (string.IsNullOrEmpty(text)) return text;\\n+\\n+            if (text.Length <= _maxSummaryLength)\\n+            {\\n+                return text;\\n+            }\\n+\\n+            var sentences = SplitIntoSentences(text);\\n+            var importantSentences = ExtractImportantSentences(sentences);\\n+\\n+            var summary = string.Empty;\\n+            foreach (var sentence in importantSentences)\\n+            {\\n+                if (summary.Length + sentence.Length > _maxSummaryLength)\\n+                {\\n+                    break;\\n+                }\\n+                summary += sentence + \\\" \\\";\\n+            }\\n+\\n+            return summary.Trim();\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZGMl\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTYR-\",\"body\":\"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\\n\\n**Keep `HasRelevanceScore` in compressed documents**\\n\\nJust like the summarizer, we copy the numeric score but lose the boolean flag, which downstream logic relies on. Please preserve it.  \\n\\n\\n```diff\\n                 var compressedDoc = new Document<T>\\n                 {\\n                     Id = doc.Id,\\n                     Content = compressedContent,\\n                     Metadata = doc.Metadata,\\n-                    RelevanceScore = doc.RelevanceScore\\n+                    RelevanceScore = doc.RelevanceScore,\\n+                    HasRelevanceScore = doc.HasRelevanceScore\\n                 };\\n```\\n\\n<!-- suggestion_start -->\\n\\n<details>\\n<summary>≡ƒô¥ Committable suggestion</summary>\\n\\n> ΓÇ╝∩╕Å **IMPORTANT**\\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\\n\\n```suggestion\\n                var compressedDoc = new Document<T>\\n                {\\n                    Id = doc.Id,\\n                    Content = compressedContent,\\n                    Metadata = doc.Metadata,\\n                    RelevanceScore = doc.RelevanceScore,\\n                    HasRelevanceScore = doc.HasRelevanceScore\\n                };\\n                compressed.Add(compressedDoc);\\n```\\n\\n</details>\\n\\n<!-- suggestion_end -->\\n\\n<details>\\n<summary>≡ƒñû Prompt for AI Agents</summary>\\n\\n```\\nIn src/RetrievalAugmentedGeneration/ContextCompression/LLMContextCompressor.cs\\naround lines 58 to 64, the compressed document initializer copies RelevanceScore\\nbut omits the HasRelevanceScore boolean flag that downstream logic depends on;\\nupdate the compressedDoc creation to set HasRelevanceScore =\\ndoc.HasRelevanceScore (and ensure the compressed document type exposes that\\nproperty) so the boolean is preserved when adding to the compressed list.\\n```\\n\\n</details>\\n\\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\\n\\n<!-- This is an auto-generated comment by CodeRabbit -->\\n\\nΓ£à Addressed in commits ec6a7c9 to 77a1bbb\",\"path\":\"src/RetrievalAugmentedGeneration/ContextCompression/LLMContextCompressor.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,148 @@\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+using AiDotNet.Helpers;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.ContextCompression\\n+{\\n+    /// <summary>\\n+    /// LLM-based context compression to reduce token usage while preserving key information.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class LLMContextCompressor<T>\\n+    {\\n+        private readonly INumericOperations<T> _numOps;\\n+        private readonly string _llmEndpoint;\\n+        private readonly string _apiKey;\\n+        private readonly double _compressionRatio;\\n+\\n+        /// <summary>\\n+        /// Initializes a new instance of the <see cref=\\\"LLMContextCompressor{T}\\\"/> class.\\n+        /// </summary>\\n+        /// <param name=\\\"numericOperations\\\">The numeric operations for type T.</param>\\n+        /// <param name=\\\"compressionRatio\\\">The target compression ratio (0.0 to 1.0).</param>\\n+        /// <param name=\\\"llmEndpoint\\\">The LLM API endpoint.</param>\\n+        /// <param name=\\\"apiKey\\\">The API key for the LLM service.</param>\\n+        public LLMContextCompressor(\\n+            INumericOperations<T> numericOperations,\\n+            double compressionRatio = 0.5,\\n+            string llmEndpoint = \\\"\\\",\\n+            string apiKey = \\\"\\\")\\n+        {\\n+            _numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\\n+            _compressionRatio = compressionRatio >= 0 && compressionRatio <= 1\\n+                ? compressionRatio\\n+                : throw new ArgumentOutOfRangeException(nameof(compressionRatio), \\\"Compression ratio must be between 0 and 1\\\");\\n+            _llmEndpoint = llmEndpoint;\\n+            _apiKey = apiKey;\\n+        }\\n+\\n+        /// <summary>\\n+        /// Compresses a list of documents while preserving relevance to the query.\\n+        /// </summary>\\n+        /// <param name=\\\"query\\\">The query context.</param>\\n+        /// <param name=\\\"documents\\\">The documents to compress.</param>\\n+        /// <returns>A list of compressed documents.</returns>\\n+        public List<Document<T>> Compress(string query, List<Document<T>> documents)\\n+        {\\n+            if (string.IsNullOrEmpty(query)) throw new ArgumentNullException(nameof(query));\\n+            if (documents == null) throw new ArgumentNullException(nameof(documents));\\n+\\n+            var compressed = new List<Document<T>>();\\n+\\n+            foreach (var doc in documents)\\n+            {\\n+                var compressedContent = CompressText(query, doc.Content);\\n+                var compressedDoc = new Document<T>\\n+                {\\n+                    Id = doc.Id,\\n+                    Content = compressedContent,\\n+                    Metadata = doc.Metadata,\\n+                    RelevanceScore = doc.RelevanceScore\\n+                };\\n+                compressed.Add(compressedDoc);\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZGMo\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTYSF\",\"body\":\"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\\n\\n**Replace `GetHashCode` with a deterministic hash**\\n\\n`string.GetHashCode()` is salted per process in modern .NET, so the same text produces different hashes each time the app restarts. That makes every embedding drift between runs, corrupting persisted vectors and breaking any document/query comparison after a restart. Please switch to a stable hash (e.g., SHA256 of the UTF-8 bytes) and derive the synthetic embedding from those bytes instead.\\n\\n```diff\\n-            var values = new T[_dimension];\\n-            var hash = text.GetHashCode();\\n-            \\n-            for (int i = 0; i < _dimension; i++)\\n-            {\\n-                var val = NumOps.FromDouble(Math.Cos(hash * (i + 1) * 0.002));\\n-                values[i] = val;\\n-            }\\n+            using var sha256 = System.Security.Cryptography.SHA256.Create();\\n+            var hashBytes = sha256.ComputeHash(System.Text.Encoding.UTF8.GetBytes(text));\\n+            var values = new T[_dimension];\\n+\\n+            for (int i = 0; i < _dimension; i++)\\n+            {\\n+                var byteValue = hashBytes[i % hashBytes.Length] / 255.0;\\n+                values[i] = NumOps.FromDouble(byteValue * 2 - 1);\\n+            }\\n```\\n\\n<!-- suggestion_start -->\\n\\n<details>\\n<summary>≡ƒô¥ Committable suggestion</summary>\\n\\n> ΓÇ╝∩╕Å **IMPORTANT**\\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\\n\\n```suggestion\\n        protected override Vector<T> EmbedCore(string text)\\n        {\\n            using var sha256 = System.Security.Cryptography.SHA256.Create();\\n            var hashBytes = sha256.ComputeHash(System.Text.Encoding.UTF8.GetBytes(text));\\n            var values = new T[_dimension];\\n\\n            for (int i = 0; i < _dimension; i++)\\n            {\\n                var byteValue = hashBytes[i % hashBytes.Length] / 255.0;\\n                values[i] = NumOps.FromDouble(byteValue * 2 - 1);\\n            }\\n\\n            return NormalizeVector(new Vector<T>(values));\\n        }\\n```\\n\\n</details>\\n\\n<!-- suggestion_end -->\\n\\n<details>\\n<summary>≡ƒñû Prompt for AI Agents</summary>\\n\\n```\\nIn src/RetrievalAugmentedGeneration/EmbeddingModels/HuggingFaceEmbeddingModel.cs\\naround lines 37 to 49, the code currently uses string.GetHashCode() which is\\nnon-deterministic across .NET process restarts; replace it by computing a\\ndeterministic SHA256 hash of the input text's UTF-8 bytes and derive the\\nembedding values from that hash instead. Specifically, compute SHA256(bytes),\\nthen expand those bytes deterministically to produce _dimension numeric seeds\\n(e.g., by reading successive 8-byte chunks, wrapping as needed, or using a\\nKDF/PRF with the hash) and convert each chunk into a double/float input for the\\nexisting NumOps.FromDouble(Math.Cos(...)) expression (or directly map to a\\nstable float between -1 and 1), then fill values[], normalize and return the\\nvector; ensure no use of GetHashCode and that UTF-8 encoding and SHA256 produce\\nconsistent results across runs.\\n```\\n\\n</details>\\n\\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\\n\\n<!-- This is an auto-generated comment by CodeRabbit -->\",\"path\":\"src/RetrievalAugmentedGeneration/Embeddings/HuggingFaceEmbeddingModel.cs\",\"position\":49,\"diffHunk\":\"@@ -0,0 +1,73 @@\\n+using AiDotNet.Helpers;\\n+using AiDotNet.LinearAlgebra;\\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\\n+using System;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels\\n+{\\n+    /// <summary>\\n+    /// HuggingFace-based embedding model for generating embeddings.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class HuggingFaceEmbeddingModel<T> : EmbeddingModelBase<T>\\n+    {\\n+        private readonly string _modelName;\\n+        private readonly string _apiKey;\\n+        private readonly int _dimension;\\n+        private readonly int _maxTokens;\\n+\\n+        public override int EmbeddingDimension => _dimension;\\n+        public override int MaxTokens => _maxTokens;\\n+\\n+        public HuggingFaceEmbeddingModel(string modelName, string apiKey = \\\"\\\", int dimension = 768, int maxTokens = 512)\\n+        {\\n+            if (string.IsNullOrWhiteSpace(modelName))\\n+                throw new ArgumentException(\\\"Model name cannot be empty\\\", nameof(modelName));\\n+            if (dimension <= 0)\\n+                throw new ArgumentException(\\\"Dimension must be positive\\\", nameof(dimension));\\n+            if (maxTokens <= 0)\\n+                throw new ArgumentException(\\\"Max tokens must be positive\\\", nameof(maxTokens));\\n+\\n+            _modelName = modelName;\\n+            _apiKey = apiKey ?? string.Empty;\\n+            _dimension = dimension;\\n+            _maxTokens = maxTokens;\\n+        }\\n+\\n+        protected override Vector<T> EmbedCore(string text)\\n+        {\\n+            var values = new T[_dimension];\\n+            var hash = text.GetHashCode();\\n+            \\n+            for (int i = 0; i < _dimension; i++)\\n+            {\\n+                var val = NumOps.FromDouble(Math.Cos(hash * (i + 1) * 0.002));\\n+                values[i] = val;\\n+            }\\n+\\n+            return NormalizeVector(new Vector<T>(values));\\n+        }\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZGMv\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTYSP\",\"body\":\"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\\n\\n**Use a stable hash for deterministic embeddings.**\\n\\n`string.GetHashCode()` is randomized per-process on modern .NET, so the same text produces different embeddings between runsΓÇöbreaking persistence, retrieval, and cache hits. Switch to a cryptographic hash so embeddings stay consistent.\\n\\n\\n```diff\\n-using System;\\n+using System;\\n+using System.Security.Cryptography;\\n+using System.Text;\\n@@\\n-        protected override Vector<T> EmbedCore(string text)\\n-        {\\n-            var values = new T[_dimension];\\n-            var hash = text.GetHashCode();\\n-            \\n-            for (int i = 0; i < _dimension; i++)\\n-            {\\n-                var val = NumOps.FromDouble(Math.Sin(hash * (i + 1) * 0.003));\\n-                values[i] = val;\\n-            }\\n-\\n-            return NormalizeVector(new Vector<T>(values));\\n-        }\\n+        protected override Vector<T> EmbedCore(string text)\\n+        {\\n+            var values = new T[_dimension];\\n+            using var sha256 = SHA256.Create();\\n+            var hashBytes = sha256.ComputeHash(Encoding.UTF8.GetBytes(text));\\n+\\n+            for (int i = 0; i < _dimension; i++)\\n+            {\\n+                var normalized = ((hashBytes[i % hashBytes.Length] / 255.0) * 2.0) - 1.0;\\n+                values[i] = NumOps.FromDouble(normalized);\\n+            }\\n+\\n+            return NormalizeVector(new Vector<T>(values));\\n+        }\\n```\\n\\n\\nAlso applies to: 35-47\\n\\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\\n\\n<!-- This is an auto-generated comment by CodeRabbit -->\",\"path\":\"src/RetrievalAugmentedGeneration/Embeddings/LocalTransformerEmbedding.cs\",\"position\":4,\"diffHunk\":\"@@ -0,0 +1,71 @@\\n+using AiDotNet.Helpers;\\n+using AiDotNet.LinearAlgebra;\\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\\n+using System;\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZGMy\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTYSU\",\"body\":\"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\\n\\n**Fix nondeterministic embeddings**\\n\\n`string.GetHashCode` is salted per process on modern .NET, so the same text produces different vectors after every restart. That corrupts stored embeddings and breaks similarity search consistency. Please swap to a deterministic digest (e.g., SHA256 over the UTFΓÇæ8 bytes) before projecting into the vector space.\\n\\n```diff\\n using AiDotNet.Helpers;\\n using AiDotNet.LinearAlgebra;\\n using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\\n using System;\\n+using System.Security.Cryptography;\\n+using System.Text;\\n \\n ...\\n-            var hash = text.GetHashCode();\\n-            \\n-            for (int i = 0; i < _dimension; i++)\\n-            {\\n-                var val = NumOps.FromDouble(Math.Cos(hash * (i + 1) * 0.002));\\n-                values[i] = val;\\n-            }\\n+            var hashBytes = SHA256.HashData(Encoding.UTF8.GetBytes(text));\\n+\\n+            for (int i = 0; i < _dimension; i++)\\n+            {\\n+                var byteValue = hashBytes[i % hashBytes.Length] / 255.0;\\n+                values[i] = NumOps.FromDouble((byteValue * 2) - 1);\\n+            }\\n```\\n\\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\\n\\n<!-- This is an auto-generated comment by CodeRabbit -->\\n\\nΓ£à Addressed in commits ec6a7c9 to 77a1bbb\",\"path\":\"src/RetrievalAugmentedGeneration/Embeddings/ONNXSentenceTransformer.cs\",\"position\":63,\"diffHunk\":\"@@ -0,0 +1,71 @@\\n+using AiDotNet.Helpers;\\n+using AiDotNet.LinearAlgebra;\\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\\n+using System;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels\\n+{\\n+    /// <summary>\\n+    /// ONNX-based sentence transformer for generating embeddings.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class ONNXSentenceTransformer<T> : EmbeddingModelBase<T>\\n+    {\\n+        private readonly string _modelPath;\\n+        private readonly int _dimension;\\n+        private readonly int _maxTokens;\\n+\\n+        public override int EmbeddingDimension => _dimension;\\n+        public override int MaxTokens => _maxTokens;\\n+\\n+        public ONNXSentenceTransformer(string modelPath, int dimension = 384, int maxTokens = 512)\\n+        {\\n+            if (string.IsNullOrWhiteSpace(modelPath))\\n+                throw new ArgumentException(\\\"Model path cannot be empty\\\", nameof(modelPath));\\n+            if (dimension <= 0)\\n+                throw new ArgumentException(\\\"Dimension must be positive\\\", nameof(dimension));\\n+            if (maxTokens <= 0)\\n+                throw new ArgumentException(\\\"Max tokens must be positive\\\", nameof(maxTokens));\\n+\\n+            _modelPath = modelPath;\\n+            _dimension = dimension;\\n+            _maxTokens = maxTokens;\\n+        }\\n+\\n+        protected override Vector<T> EmbedCore(string text)\\n+        {\\n+            var values = new T[_dimension];\\n+            var hash = text.GetHashCode();\\n+            \\n+            for (int i = 0; i < _dimension; i++)\\n+            {\\n+                var val = NumOps.FromDouble(Math.Cos(hash * (i + 1) * 0.002));\\n+                values[i] = val;\\n+            }\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZGM2\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTYSd\",\"body\":\"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\\n\\n**GetHashCode-based embeddings are unstable across runs.**\\n\\n`string.GetHashCode()` is randomized per process on modern .NET, so embeddings produced during indexing will differ from those generated later during querying, breaking retrieval reproducibility. Replace this placeholder with a stable implementation (e.g., call the OpenAI embeddings API or derive a deterministic vector via a cryptographic hash fed into `NumOps`) before shipping.\\n\\n<details>\\n<summary>≡ƒñû Prompt for AI Agents</summary>\\n\\n```\\nIn src/RetrievalAugmentedGeneration/EmbeddingModels/OpenAIEmbeddingModel.cs\\naround lines 42 to 48, the code uses string.GetHashCode() which is\\nprocess-randomized and produces unstable embeddings; replace this placeholder\\nwith a deterministic implementation: either call the OpenAI embeddings API and\\nreturn its vector, or compute a cryptographic hash (e.g., SHA256) of the input\\ntext, deterministically derive numeric values from the hash bytes\\n(slice/expand/interpret as little-endian integers or use HMAC-DRBG), map those\\nnumbers into the required dimension using NumOps.FromDouble, and optionally\\nnormalize the vector; ensure the implementation is stable across runs and\\nhandles errors/async calls appropriately.\\n```\\n\\n</details>\\n\\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\\n\\n<!-- This is an auto-generated comment by CodeRabbit -->\\n\\nΓ£à Addressed in commits ec6a7c9 to 77a1bbb\",\"path\":\"src/RetrievalAugmentedGeneration/EmbeddingModels/OpenAIEmbeddingModel.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,75 @@\\n+using AiDotNet.Helpers;\\n+using AiDotNet.LinearAlgebra;\\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\\n+using System;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels\\n+{\\n+    /// <summary>\\n+    /// OpenAI embedding model for generating embeddings via OpenAI API.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class OpenAIEmbeddingModel<T> : EmbeddingModelBase<T>\\n+    {\\n+        private readonly string _apiKey;\\n+        private readonly string _modelName;\\n+        private readonly int _dimension;\\n+        private readonly int _maxTokens;\\n+\\n+        public override int EmbeddingDimension => _dimension;\\n+        public override int MaxTokens => _maxTokens;\\n+\\n+        public OpenAIEmbeddingModel(string apiKey, string modelName = \\\"text-embedding-ada-002\\\", int dimension = 1536, int maxTokens = 8191)\\n+        {\\n+            if (string.IsNullOrWhiteSpace(apiKey))\\n+                throw new ArgumentException(\\\"API key cannot be empty\\\", nameof(apiKey));\\n+            if (string.IsNullOrWhiteSpace(modelName))\\n+                throw new ArgumentException(\\\"Model name cannot be empty\\\", nameof(modelName));\\n+            if (dimension <= 0)\\n+                throw new ArgumentException(\\\"Dimension must be positive\\\", nameof(dimension));\\n+            if (maxTokens <= 0)\\n+                throw new ArgumentException(\\\"Max tokens must be positive\\\", nameof(maxTokens));\\n+\\n+            _apiKey = apiKey;\\n+            _modelName = modelName;\\n+            _dimension = dimension;\\n+            _maxTokens = maxTokens;\\n+        }\\n+\\n+        protected override Vector<T> EmbedCore(string text)\\n+        {\\n+            var values = new T[_dimension];\\n+            var hash = text.GetHashCode();\\n+            \\n+            for (int i = 0; i < _dimension; i++)\\n+            {\\n+                var val = NumOps.FromDouble(Math.Cos(hash * (i + 1) * 0.001));\\n+                values[i] = val;\\n+            }\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZGM5\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTYSi\",\"body\":\"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\\n\\n**Use the numeric operations helper instead of `Convert.ToDouble`.**\\n\\n`Convert.ToDouble(d.RelevanceScore)` will throw for any numeric type that doesnΓÇÖt implement `IConvertible` (e.g., the `Complex<T>` support you just enabled via `MathHelper.GetNumericOperations<T>()`). The stub generator therefore crashes as soon as the pipeline runs with one of those valid `T`s. Please route the conversion through the `INumericOperations<T>` abstraction you already cached.\\n\\n\\n\\n```diff\\n-            .Where(d => d.HasRelevanceScore)\\n-            .Select(d => Convert.ToDouble(d.RelevanceScore))\\n+            .Where(d => d.HasRelevanceScore)\\n+            .Select(d => NumOps.ToDouble(d.RelevanceScore))\\n```\\n\\n<!-- suggestion_start -->\\n\\n<details>\\n<summary>≡ƒô¥ Committable suggestion</summary>\\n\\n> ΓÇ╝∩╕Å **IMPORTANT**\\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\\n\\n```suggestion\\n            .Where(d => d.HasRelevanceScore)\\n            .Select(d => NumOps.ToDouble(d.RelevanceScore))\\n            .DefaultIfEmpty(0.5)\\n```\\n\\n</details>\\n\\n<!-- suggestion_end -->\\n\\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\\n\\n<!-- This is an auto-generated comment by CodeRabbit -->\",\"path\":\"src/RetrievalAugmentedGeneration/Generators/StubGenerator.cs\",\"position\":61,\"diffHunk\":\"@@ -128,15 +132,15 @@ public GroundedAnswer GenerateGrounded(string query, IEnumerable<Document> conte\\n \\n         // Calculate confidence based on retrieval scores (normalized to [0,1])\\n         var avgScore = contextList\\n-            .Where(d => d.RelevanceScore.HasValue)\\n-            .Select(d => d.RelevanceScore!.Value)\\n+            .Where(d => d.HasRelevanceScore)\\n+            .Select(d => Convert.ToDouble(d.RelevanceScore))\\n             .DefaultIfEmpty(0.5)\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZGM9\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTYSq\",\"body\":\"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\\n\\n**Fix `bestScore` initialization overflow**\\n\\n`NumOps.FromDouble(double.MinValue)` will overflow for decimal and integral `T` (e.g., `Convert.ToInt32(double.MinValue)`), crashing the reranker before it scores anything. Initialize with `NumOps.Zero` and use a guard instead of converting an out-of-range sentinel.\\n\\n```diff\\n-            Document<T>? bestDoc = null;\\n-            var bestScore = NumOps.FromDouble(double.MinValue);\\n+            Document<T>? bestDoc = null;\\n+            var bestScore = NumOps.Zero;\\n+            var hasBestScore = false;\\n@@\\n-                if (bestDoc == null || NumOps.GreaterThan(score, bestScore))\\n+                if (!hasBestScore || NumOps.GreaterThan(score, bestScore))\\n                 {\\n                     bestDoc = doc;\\n                     bestScore = score;\\n+                    hasBestScore = true;\\n                 }\\n```\\n\\n<details>\\n<summary>≡ƒñû Prompt for AI Agents</summary>\\n\\n```\\nIn src/RetrievalAugmentedGeneration/Rerankers/DiversityReranker.cs around lines\\n126-156, replace the unsafe sentinel initialization of bestScore\\n(NumOps.FromDouble(double.MinValue)) with NumOps.Zero and rely on the existing\\nbestDoc == null guard when selecting the first document; remove any conversion\\nfrom double.MinValue so no out-of-range conversions occur, and keep the\\nselection condition as \\\"if (bestDoc == null || NumOps.GreaterThan(score,\\nbestScore))\\\" so the first scored doc sets bestScore safely.\\n```\\n\\n</details>\\n\\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\\n\\n<!-- This is an auto-generated comment by CodeRabbit -->\\n\\nΓ£à Addressed in commits ec6a7c9 to 77a1bbb\",\"path\":\"src/RetrievalAugmentedGeneration/Rerankers/DiversityReranker.cs\",\"position\":155,\"diffHunk\":\"@@ -0,0 +1,220 @@\\n+using AiDotNet.Helpers;\\n+using AiDotNet.Interfaces;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.Rerankers;\\n+\\n+/// <summary>\\n+/// Reranks documents to maximize diversity while maintaining relevance.\\n+/// </summary>\\n+/// <typeparam name=\\\"T\\\">The numeric data type used for scoring.</typeparam>\\n+/// <remarks>\\n+/// <para>\\n+/// This reranker addresses the problem of redundant results by explicitly promoting diversity.\\n+/// It uses a greedy algorithm to select documents that are both relevant to the query and\\n+/// dissimilar from already-selected documents. This is similar to Maximal Marginal Relevance (MMR)\\n+/// but uses a simpler diversity metric based on text overlap.\\n+/// </para>\\n+/// <para><b>For Beginners:</b> This prevents showing the same information multiple times.\\n+/// \\n+/// The Problem:\\n+/// Imagine searching for \\\"Python programming\\\" and getting 10 results:\\n+/// - Result 1: \\\"Python is a programming language...\\\"\\n+/// - Result 2: \\\"Python is a programming language used for...\\\"\\n+/// - Result 3: \\\"Python programming language allows...\\\"\\n+/// - Result 4-10: More variations of the same thing\\n+/// \\n+/// That's redundant! You want variety:\\n+/// - Result 1: Python basics\\n+/// - Result 2: Python web development\\n+/// - Result 3: Python data science\\n+/// - Result 4: Python machine learning\\n+/// - Result 5: Python performance tips\\n+/// \\n+/// How it works:\\n+/// 1. Pick the most relevant document first\\n+/// 2. For remaining docs, balance two factors:\\n+///    a) Relevance to the query (should be useful)\\n+///    b) Difference from already-picked docs (should be unique)\\n+/// 3. Keep picking until you have enough results\\n+/// \\n+/// Diversity calculation:\\n+/// - Compares text overlap (how many words are shared)\\n+/// - Higher overlap = less diverse = lower score\\n+/// - Lower overlap = more diverse = higher score\\n+/// \\n+/// Lambda parameter (0 to 1):\\n+/// - lambda=1.0: Only care about relevance (might get duplicates)\\n+/// - lambda=0.0: Only care about diversity (might get irrelevant docs)\\n+/// - lambda=0.5: Balance both (recommended default)\\n+/// \\n+/// Real example with lambda=0.5:\\n+/// Query: \\\"climate change effects\\\"\\n+/// \\n+/// Step 1: Pick most relevant ΓåÆ \\\"Climate change causes rising temperatures\\\" (relevance: 0.9)\\n+/// Step 2: Next candidates:\\n+///   - \\\"Climate change leads to warmer weather\\\" (relevance: 0.85, similarity to picked: 0.7)\\n+///     ΓåÆ Score: 0.5 * 0.85 - 0.5 * 0.7 = 0.075\\n+///   - \\\"Ocean acidification from CO2\\\" (relevance: 0.7, similarity: 0.2)\\n+///     ΓåÆ Score: 0.5 * 0.7 - 0.5 * 0.2 = 0.25 Γ£ô Pick this!\\n+/// \\n+/// Result: You get coverage of temperature AND ocean effects, not just temperature twice!\\n+/// \\n+/// When to use this:\\n+/// - Search results where redundancy is common\\n+/// - Document recommendation systems\\n+/// - Exploratory searches where breadth matters\\n+/// - After initial retrieval that returns many similar docs\\n+/// </para>\\n+/// </remarks>\\n+public class DiversityReranker<T> : RerankerBase<T>\\n+{\\n+    private readonly T _lambda;\\n+\\n+    /// <summary>\\n+    /// Gets a value indicating whether this reranker modifies relevance scores.\\n+    /// </summary>\\n+    public override bool ModifiesScores => true;\\n+\\n+    /// <summary>\\n+    /// Initializes a new instance of the DiversityReranker class.\\n+    /// </summary>\\n+    /// <param name=\\\"numOps\\\">Numeric operations for type T.</param>\\n+    /// <param name=\\\"lambda\\\">Trade-off parameter between relevance and diversity (0 to 1). Default: 0.5.\\n+    /// Higher values favor relevance, lower values favor diversity.</param>\\n+    public DiversityReranker(INumericOperations<T> numOps, T? lambda = default) : base()\\n+    {\\n+        var lambdaValue = lambda ?? NumOps.FromDouble(0.5);\\n+        \\n+        // Validate lambda is in [0, 1]\\n+        if (NumOps.LessThan(lambdaValue, NumOps.Zero) || NumOps.GreaterThan(lambdaValue, NumOps.One))\\n+        {\\n+            throw new ArgumentException(\\\"Lambda must be between 0 and 1.\\\", nameof(lambda));\\n+        }\\n+\\n+        _lambda = lambdaValue;\\n+    }\\n+\\n+    /// <summary>\\n+    /// Core reranking logic that maximizes diversity while maintaining relevance.\\n+    /// </summary>\\n+    /// <param name=\\\"query\\\">The validated query text.</param>\\n+    /// <param name=\\\"documents\\\">The validated list of documents to rerank.</param>\\n+    /// <returns>Documents reranked to balance relevance and diversity.</returns>\\n+    protected override IEnumerable<Document<T>> RerankCore(string query, IList<Document<T>> documents)\\n+    {\\n+        var docList = documents.ToList();\\n+        if (docList.Count <= 1)\\n+        {\\n+            return docList;\\n+        }\\n+\\n+        var rerankedDocs = new List<Document<T>>();\\n+        var remainingDocs = new List<Document<T>>(docList);\\n+\\n+        // Start with the most relevant document\\n+        var firstDoc = remainingDocs.OrderByDescending(d => d.RelevanceScore, \\n+            Comparer<T>.Create((a, b) => {\\n+                if (NumOps.GreaterThan(a, b)) return 1;\\n+                if (NumOps.LessThan(a, b)) return -1;\\n+                return 0;\\n+            })).First();\\n+        rerankedDocs.Add(firstDoc);\\n+        remainingDocs.Remove(firstDoc);\\n+\\n+        // Greedily select documents that maximize: lambda * relevance - (1 - lambda) * max_similarity\\n+        while (remainingDocs.Count > 0)\\n+        {\\n+            Document<T>? bestDoc = null;\\n+            var bestScore = NumOps.FromDouble(double.MinValue);\\n+\\n+            foreach (var doc in remainingDocs)\\n+            {\\n+                // Calculate maximum similarity to already-selected documents\\n+                var maxSimilarity = NumOps.Zero;\\n+                foreach (var selectedDoc in rerankedDocs)\\n+                {\\n+                    var similarity = CalculateTextSimilarity(doc.Content, selectedDoc.Content);\\n+                    if (NumOps.GreaterThan(similarity, maxSimilarity))\\n+                    {\\n+                        maxSimilarity = similarity;\\n+                    }\\n+                }\\n+\\n+                // Calculate diversity-aware score: lambda * relevance - (1 - lambda) * max_similarity\\n+                var relevancePart = NumOps.Multiply(_lambda, doc.RelevanceScore);\\n+                var oneMinusLambda = NumOps.Subtract(NumOps.One, _lambda);\\n+                var diversityPenalty = NumOps.Multiply(oneMinusLambda, maxSimilarity);\\n+                var score = NumOps.Subtract(relevancePart, diversityPenalty);\\n+\\n+                if (bestDoc == null || NumOps.GreaterThan(score, bestScore))\\n+                {\\n+                    bestDoc = doc;\\n+                    bestScore = score;\\n+                }\\n+            }\\n+\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZGNB\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTYSv\",\"body\":\"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\\n\\n**Avoid converting negative infinity for generic numeric types.**\\n\\nLine 156 calls `NumOps.FromDouble(double.NegativeInfinity)`. For many supported numeric types (e.g., `decimal`, `int`), that conversion throws an `OverflowException`, breaking MMR reranking the moment those generics are used. Initialize the sentinel without relying on infinities by tracking whether a candidate has been seen instead.  \\n\\n```diff\\n-            var bestIdx = -1;\\n-            var bestScore = NumOps.FromDouble(double.NegativeInfinity);\\n+            var bestIdx = -1;\\n+            var bestScore = default(T);\\n+            var hasCandidate = false;\\n@@\\n-                if (NumOps.GreaterThan(mmrScore, bestScore))\\n+                if (!hasCandidate || NumOps.GreaterThan(mmrScore, bestScore))\\n                 {\\n                     bestScore = mmrScore;\\n                     bestIdx = i;\\n+                    hasCandidate = true;\\n                 }\\n```\\n\\n<!-- suggestion_start -->\\n\\n<details>\\n<summary>≡ƒô¥ Committable suggestion</summary>\\n\\n> ΓÇ╝∩╕Å **IMPORTANT**\\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\\n\\n```suggestion\\n            var bestIdx = -1;\\n            var bestScore = default(T);\\n            var hasCandidate = false;\\n            \\n            foreach (var i in unselected)\\n            {\\n                var doc = docList[i];\\n                \\n                // Relevance component (original score)\\n                var relevance = doc.HasRelevanceScore ? doc.RelevanceScore : NumOps.Zero;\\n                \\n                // Diversity component (max similarity to selected docs)\\n                var maxSimilarity = NumOps.Zero;\\n                foreach (var j in selected)\\n                {\\n                    var similarity = NumOps.FromDouble(CalculateCosineSimilarity(embeddings[i], embeddings[j]));\\n                    if (NumOps.GreaterThan(similarity, maxSimilarity))\\n                        maxSimilarity = similarity;\\n                }\\n                \\n                // MMR score = ╬╗ * relevance - (1 - ╬╗) * maxSimilarity\\n                var lambdaT = NumOps.FromDouble(_lambda);\\n                var oneMinusLambda = NumOps.FromDouble(1.0 - _lambda);\\n                \\n                var mmrScore = NumOps.Subtract(\\n                    NumOps.Multiply(lambdaT, relevance),\\n                    NumOps.Multiply(oneMinusLambda, maxSimilarity)\\n                );\\n                \\n                if (!hasCandidate || NumOps.GreaterThan(mmrScore, bestScore))\\n                {\\n                    bestScore = mmrScore;\\n                    bestIdx = i;\\n                    hasCandidate = true;\\n                }\\n```\\n\\n</details>\\n\\n<!-- suggestion_end -->\\n\\n<details>\\n<summary>≡ƒñû Prompt for AI Agents</summary>\\n\\n```\\nIn\\nsrc/RetrievalAugmentedGeneration/Rerankers/MaximalMarginalRelevanceReranker.cs\\naround lines 154-186, avoid calling NumOps.FromDouble(double.NegativeInfinity)\\nwhich can throw for some numeric T; instead remove the NegativeInfinity sentinel\\nand track whether a best candidate has been seen (e.g., add a bool bestFound =\\nfalse), then on the first candidate set bestScore = mmrScore and bestIdx = i and\\nset bestFound = true; subsequently compare mmrScore to bestScore as before.\\nEnsure no conversion from double.NegativeInfinity is used anywhere in this\\nblock.\\n```\\n\\n</details>\\n\\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\\n\\n<!-- This is an auto-generated comment by CodeRabbit -->\\n\\nΓ£à Addressed in commits ec6a7c9 to 77a1bbb\",\"path\":\"src/RetrievalAugmentedGeneration/Rerankers/MaximalMarginalRelevanceReranker.cs\",\"position\":186,\"diffHunk\":\"@@ -0,0 +1,230 @@\\n+using AiDotNet.Helpers;\\n+using AiDotNet.Interfaces;\\n+using AiDotNet.LinearAlgebra;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.Rerankers;\\n+\\n+/// <summary>\\n+/// Implements Maximal Marginal Relevance (MMR) reranking to balance relevance and diversity.\\n+/// </summary>\\n+/// <typeparam name=\\\"T\\\">The numeric data type used for calculations.</typeparam>\\n+/// <remarks>\\n+/// <para>\\n+/// MMR reranking ensures that retrieved documents are not only relevant to the query but also\\n+/// diverse from each other. This prevents redundancy where all top results say essentially the\\n+/// same thing, providing users with a broader range of information. MMR is particularly valuable\\n+/// for exploratory search, news aggregation, and research applications.\\n+/// </para>\\n+/// <para><b>For Beginners:</b> MMR prevents search results from being too similar to each other.\\n+/// \\n+/// The problem MMR solves:\\n+/// Imagine searching for \\\"climate change\\\" and getting:\\n+/// 1. \\\"Climate change threatens polar bears\\\"\\n+/// 2. \\\"Polar bears endangered by climate change\\\"\\n+/// 3. \\\"Climate change impact on polar ice affecting bears\\\"\\n+/// 4. \\\"Global warming threatens polar bear habitats\\\"\\n+/// 5. \\\"Arctic ice melting endangers polar bears\\\"\\n+/// \\n+/// All relevant, but they're all saying the same thing! You're getting one narrow aspect\\n+/// repeated 5 times instead of a diverse view of climate change.\\n+/// \\n+/// What MMR does instead:\\n+/// 1. \\\"Climate change threatens polar bears\\\" (relevant: Γ£ô, diverse: Γ£ô first result)\\n+/// 2. \\\"Rising sea levels threaten coastal cities\\\" (relevant: Γ£ô, different topic: Γ£ô)\\n+/// 3. \\\"Carbon emissions reach record highs\\\" (relevant: Γ£ô, different aspect: Γ£ô)\\n+/// 4. \\\"Renewable energy adoption accelerates globally\\\" (relevant: Γ£ô, solutions angle: Γ£ô)\\n+/// 5. \\\"Climate refugees increase in developing nations\\\" (relevant: Γ£ô, human impact: Γ£ô)\\n+/// \\n+/// Now you get a comprehensive view with diverse perspectives!\\n+/// \\n+/// How MMR works:\\n+/// 1. Pick the most relevant document ΓåÆ Add to results\\n+/// 2. For next pick, consider:\\n+///    - Relevance to query (you want relevant docs)\\n+///    - Dissimilarity to already-picked docs (you want diversity)\\n+/// 3. Balance these two goals with a lambda parameter\\n+/// 4. Repeat until you have K documents\\n+/// \\n+/// The lambda parameter (╬╗):\\n+/// - ╬╗ = 1.0: Only care about relevance (normal ranking, no diversity)\\n+/// - ╬╗ = 0.0: Only care about diversity (might get irrelevant but diverse docs)\\n+/// - ╬╗ = 0.7: Balanced (70% relevance, 30% diversity) ΓåÉ Good default\\n+/// \\n+/// When to use MMR:\\n+/// - Research/exploratory queries: Users want comprehensive coverage\\n+/// - News aggregation: Don't show 10 articles about the same event\\n+/// - Product search: Show variety, not just variations of one product\\n+/// - Question answering: Provide multiple perspectives\\n+/// \\n+/// When NOT to use MMR:\\n+/// - User wants very specific info: \\\"iPhone 15 Pro Max price\\\" (diversity not helpful)\\n+/// - Transactional queries: \\\"buy Nike Air Max\\\" (user knows what they want)\\n+/// - Fact lookups: \\\"Paris population\\\" (one correct answer)\\n+/// </para>\\n+/// </remarks>\\n+public class MaximalMarginalRelevanceReranker<T> : RerankerBase<T>\\n+{\\n+    private readonly double _lambda;\\n+    private readonly Func<Document<T>, Vector<T>> _getEmbedding;\\n+\\n+    /// <summary>\\n+    /// Gets a value indicating whether this reranker modifies relevance scores.\\n+    /// </summary>\\n+    public override bool ModifiesScores => true;\\n+\\n+    /// <summary>\\n+    /// Initializes a new instance of the MaximalMarginalRelevanceReranker class.\\n+    /// </summary>\\n+    /// <param name=\\\"getEmbedding\\\">Function to get document embeddings for similarity calculation.</param>\\n+    /// <param name=\\\"lambda\\\">Balance between relevance and diversity (0-1, default: 0.7). Higher values prioritize relevance.</param>\\n+    /// <remarks>\\n+    /// <para><b>For Beginners:</b> Configuration explained.\\n+    /// \\n+    /// **getEmbedding Function:**\\n+    /// This function should return the vector embedding of a document. If you don't have\\n+    /// embeddings, you can't use MMR (it needs to measure similarity between documents).\\n+    /// \\n+    /// Example:\\n+    /// ```csharp\\n+    /// Func&lt;Document&lt;double&gt;, Vector&lt;double&gt;&gt; getEmb = (doc) => {\\n+    ///     // Return cached embedding if available\\n+    ///     if (doc.Metadata.ContainsKey(\\\"embedding\\\"))\\n+    ///         return (Vector&lt;double&gt;)doc.Metadata[\\\"embedding\\\"];\\n+    ///     \\n+    ///     // Otherwise compute it\\n+    ///     return embeddingModel.Encode(doc.Content);\\n+    /// };\\n+    /// ```\\n+    /// \\n+    /// **Lambda Parameter Guidelines:**\\n+    /// - 1.0 = Pure relevance (same as no reranking)\\n+    /// - 0.9 = Slight diversity boost\\n+    /// - 0.7 = Balanced (recommended default)\\n+    /// - 0.5 = Equal weight to relevance and diversity\\n+    /// - 0.3 = Heavy diversity (may sacrifice relevance)\\n+    /// - 0.0 = Pure diversity (probably too extreme)\\n+    /// \\n+    /// Start with 0.7 and adjust based on user feedback!\\n+    /// </para>\\n+    /// </remarks>\\n+    public MaximalMarginalRelevanceReranker(Func<Document<T>, Vector<T>> getEmbedding, double lambda = 0.7)\\n+    {\\n+        if (getEmbedding == null)\\n+            throw new ArgumentNullException(nameof(getEmbedding));\\n+        \\n+        if (lambda < 0 || lambda > 1)\\n+            throw new ArgumentException(\\\"Lambda must be between 0 and 1\\\", nameof(lambda));\\n+\\n+        _getEmbedding = getEmbedding;\\n+        _lambda = lambda;\\n+    }\\n+\\n+    /// <summary>\\n+    /// Reranks documents using Maximal Marginal Relevance.\\n+    /// </summary>\\n+    protected override IEnumerable<Document<T>> RerankCore(string query, IList<Document<T>> documents)\\n+    {\\n+        var docList = documents.ToList();\\n+        \\n+        if (docList.Count == 0)\\n+            return Enumerable.Empty<Document<T>>();\\n+\\n+        if (docList.Count == 1)\\n+            return docList;\\n+\\n+        // Get embeddings for all documents\\n+        var embeddings = docList.Select(doc => _getEmbedding(doc)).ToList();\\n+        \\n+        // Track which documents have been selected\\n+        var selected = new List<int>();\\n+        var unselected = Enumerable.Range(0, docList.Count).ToList();\\n+        \\n+        // Select first document (most relevant)\\n+        var firstIdx = unselected\\n+            .OrderByDescending(i => docList[i].HasRelevanceScore ? Convert.ToDouble(docList[i].RelevanceScore) : 0.0)\\n+            .First();\\n+        \\n+        selected.Add(firstIdx);\\n+        unselected.Remove(firstIdx);\\n+        \\n+        // Iteratively select remaining documents using MMR\\n+        while (unselected.Count > 0)\\n+        {\\n+            var bestIdx = -1;\\n+            var bestScore = NumOps.FromDouble(double.NegativeInfinity);\\n+            \\n+            foreach (var i in unselected)\\n+            {\\n+                var doc = docList[i];\\n+                \\n+                // Relevance component (original score)\\n+                var relevance = doc.HasRelevanceScore ? doc.RelevanceScore : NumOps.Zero;\\n+                \\n+                // Diversity component (max similarity to selected docs)\\n+                var maxSimilarity = NumOps.Zero;\\n+                foreach (var j in selected)\\n+                {\\n+                    var similarity = NumOps.FromDouble(CalculateCosineSimilarity(embeddings[i], embeddings[j]));\\n+                    if (NumOps.GreaterThan(similarity, maxSimilarity))\\n+                        maxSimilarity = similarity;\\n+                }\\n+                \\n+                // MMR score = ╬╗ * relevance - (1 - ╬╗) * maxSimilarity\\n+                var lambdaT = NumOps.FromDouble(_lambda);\\n+                var oneMinusLambda = NumOps.FromDouble(1.0 - _lambda);\\n+                \\n+                var mmrScore = NumOps.Subtract(\\n+                    NumOps.Multiply(lambdaT, relevance),\\n+                    NumOps.Multiply(oneMinusLambda, maxSimilarity)\\n+                );\\n+                \\n+                if (NumOps.GreaterThan(mmrScore, bestScore))\\n+                {\\n+                    bestScore = mmrScore;\\n+                    bestIdx = i;\\n+                }\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZGNE\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTYS1\",\"body\":\"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\\n\\n**Prevent normalization crashes for generic score types**\\n\\n`Enumerable.Min/Max` requires `T` to implement `IComparable`. For supported types like `Complex<T>` that lack an ordering, this throws at runtime. Additionally, `NumOps.FromDouble(1e-8)` becomes zero for integral types, so the zero-range case falls through and `Divide(..., range)` dereferences a zero divisor. Both paths break downstream rerankers. Please compute min/max via `NumOps` and treat zero range explicitly before dividing. \\n\\nApply this diff:\\n\\n```diff\\n-        var minScore = scores.Min();\\n-        var maxScore = scores.Max();\\n-        \\n-        if (minScore == null || maxScore == null)\\n-            return documents;\\n-            \\n-        var range = NumOps.Subtract(maxScore, minScore);\\n-\\n-        var epsilon = NumOps.FromDouble(1e-8);\\n-        if (NumOps.LessThan(NumOps.Abs(range), epsilon))\\n+        var minScore = scores[0];\\n+        var maxScore = scores[0];\\n+        for (var i = 1; i < scores.Count; i++)\\n+        {\\n+            var score = scores[i];\\n+            if (NumOps.LessThan(score, minScore))\\n+            {\\n+                minScore = score;\\n+            }\\n+            if (NumOps.LessThan(maxScore, score))\\n+            {\\n+                maxScore = score;\\n+            }\\n+        }\\n+\\n+        var range = NumOps.Subtract(maxScore, minScore);\\n+        var epsilon = NumOps.FromDouble(1e-8);\\n+        var isZeroRange = EqualityComparer<T>.Default.Equals(range, NumOps.Zero);\\n+        if (isZeroRange || NumOps.LessThan(NumOps.Abs(range), epsilon))\\n         {\\n             // All scores are the same, set them all to 1.0\\n             var one = NumOps.One;\\n             foreach (var doc in docsWithScores)\\n             {\\n                 doc.RelevanceScore = one;\\n                 doc.HasRelevanceScore = true;\\n             }\\n         }\\n         else\\n         {\\n             foreach (var doc in docsWithScores)\\n             {\\n                 doc.RelevanceScore = NumOps.Divide(NumOps.Subtract(doc.RelevanceScore, minScore), range);\\n                 doc.HasRelevanceScore = true;\\n             }\\n         }\\n```\\n\\n<!-- suggestion_start -->\\n\\n<details>\\n<summary>≡ƒô¥ Committable suggestion</summary>\\n\\n> ΓÇ╝∩╕Å **IMPORTANT**\\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\\n\\n```suggestion\\n    protected IList<Document<T>> NormalizeScores(IList<Document<T>> documents)\\n    {\\n        var docsWithScores = documents.Where(d => d.HasRelevanceScore).ToList();\\n        if (docsWithScores.Count == 0)\\n            return documents;\\n\\n        var scores = docsWithScores.Select(d => d.RelevanceScore).ToList();\\n        if (scores.Count == 0)\\n            return documents;\\n\\n        var minScore = scores[0];\\n        var maxScore = scores[0];\\n        for (var i = 1; i < scores.Count; i++)\\n        {\\n            var score = scores[i];\\n            if (NumOps.LessThan(score, minScore))\\n            {\\n                minScore = score;\\n            }\\n            if (NumOps.LessThan(maxScore, score))\\n            {\\n                maxScore = score;\\n            }\\n        }\\n\\n        var range = NumOps.Subtract(maxScore, minScore);\\n        var epsilon = NumOps.FromDouble(1e-8);\\n        var isZeroRange = EqualityComparer<T>.Default.Equals(range, NumOps.Zero);\\n        if (isZeroRange || NumOps.LessThan(NumOps.Abs(range), epsilon))\\n        {\\n            // All scores are the same, set them all to 1.0\\n            var one = NumOps.One;\\n            foreach (var doc in docsWithScores)\\n            {\\n                doc.RelevanceScore = one;\\n                doc.HasRelevanceScore = true;\\n            }\\n        }\\n        else\\n        {\\n            foreach (var doc in docsWithScores)\\n            {\\n                doc.RelevanceScore = NumOps.Divide(NumOps.Subtract(doc.RelevanceScore, minScore), range);\\n                doc.HasRelevanceScore = true;\\n            }\\n        }\\n```\\n\\n</details>\\n\\n<!-- suggestion_end -->\\n\\n<details>\\n<summary>≡ƒñû Prompt for AI Agents</summary>\\n\\n```\\nIn src/RetrievalAugmentedGeneration/Rerankers/RerankerBase.cs around lines\\n153-188, the code uses LINQ Min/Max (which requires IComparable) and a\\ndouble-derived epsilon that can be zero for integral score types; replace the\\nEnumerable Min/Max with an explicit loop that computes min and max using NumOps\\n(e.g., initialize min/max to the first score and update via\\nNumOps.Min/NumOps.Max), compute range = NumOps.Subtract(max, min), then handle\\nzero/near-zero range before dividing by checking NumOps.Equals(range,\\nNumOps.Zero) OR, if NumOps.FromDouble(1e-8) is non-zero,\\nNumOps.LessThan(NumOps.Abs(range), epsilon); if the range is zero/near-zero set\\nall HasRelevanceScore and RelevanceScore to NumOps.One, otherwise normalize each\\nscore with NumOps.Divide(NumOps.Subtract(score, min), range).\\n```\\n\\n</details>\\n\\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\\n\\n<!-- This is an auto-generated comment by CodeRabbit -->\\n\\nΓ£à Addressed in commits ec6a7c9 to 77a1bbb\",\"path\":\"src/RetrievalAugmentedGeneration/Rerankers/RerankerBase.cs\",\"position\":1,\"diffHunk\":\"@@ -142,31 +150,41 @@ protected virtual void ValidateTopK(int topK)\\n     /// - Makes scores consistent with other retrievers/rerankers\\n     /// </para>\\n     /// </remarks>\\n-    protected IList<Document> NormalizeScores(IList<Document> documents)\\n+    protected IList<Document<T>> NormalizeScores(IList<Document<T>> documents)\\n     {\\n-        var docsWithScores = documents.Where(d => d.RelevanceScore.HasValue).ToList();\\n+        var docsWithScores = documents.Where(d => d.HasRelevanceScore).ToList();\\n         if (docsWithScores.Count == 0)\\n             return documents;\\n \\n-        var scores = docsWithScores.Select(d => d.RelevanceScore!.Value).ToList();\\n+        var scores = docsWithScores.Select(d => d.RelevanceScore).ToList();\\n+        if (scores.Count == 0)\\n+            return documents;\\n+\\n         var minScore = scores.Min();\\n         var maxScore = scores.Max();\\n-        var range = maxScore - minScore;\\n+        \\n+        if (minScore == null || maxScore == null)\\n+            return documents;\\n+            \\n+        var range = NumOps.Subtract(maxScore, minScore);\\n \\n-        const double epsilon = 1e-8;\\n-        if (Math.Abs(range) < epsilon)\\n+        var epsilon = NumOps.FromDouble(1e-8);\\n+        if (NumOps.LessThan(NumOps.Abs(range), epsilon))\\n         {\\n             // All scores are the same, set them all to 1.0\\n+            var one = NumOps.One;\\n             foreach (var doc in docsWithScores)\\n             {\\n-                doc.RelevanceScore = 1.0;\\n+                doc.RelevanceScore = one;\\n+                doc.HasRelevanceScore = true;\\n             }\\n         }\\n         else\\n         {\\n             foreach (var doc in docsWithScores)\\n             {\\n-                doc.RelevanceScore = (doc.RelevanceScore!.Value - minScore) / range;\\n+                doc.RelevanceScore = NumOps.Divide(NumOps.Subtract(doc.RelevanceScore, minScore), range);\\n+                doc.HasRelevanceScore = true;\\n             }\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZGNK\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTYS_\",\"body\":\"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\\n\\n**Set `HasRelevanceScore` after assigning scores.**\\n\\nLine 55 assigns a new relevance value but never marks the document as scored. Downstream components (e.g., MMR or diversity rerankers) check `HasRelevanceScore` before using the value; leaving it false causes them to ignore these scores entirely. Please flag the documents as scored when you update the value.  \\n\\n```diff\\n             .Select(x =>\\n             {\\n                 x.doc.RelevanceScore = x.score;\\n+                x.doc.HasRelevanceScore = true;\\n                 return x.doc;\\n             })\\n```\\n\\n<details>\\n<summary>≡ƒñû Prompt for AI Agents</summary>\\n\\n```\\nIn src/RetrievalAugmentedGeneration/RerankingStrategies/CrossEncoderReranker.cs\\naround lines 50 to 58, after assigning x.doc.RelevanceScore = x.score you must\\nalso set x.doc.HasRelevanceScore = true so downstream components recognize the\\ndocument as scored; update the Select block to mark HasRelevanceScore = true\\n(alongside setting RelevanceScore) before returning the doc.\\n```\\n\\n</details>\\n\\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\\n\\n<!-- This is an auto-generated comment by CodeRabbit -->\\n\\nΓ£à Addressed in commits ec6a7c9 to 77a1bbb\",\"path\":\"src/RetrievalAugmentedGeneration/RerankingStrategies/CrossEncoderReranker.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,88 @@\\n+using System;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+using AiDotNet.Helpers;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.RerankingStrategies\\n+{\\n+    /// <summary>\\n+    /// Cross-encoder based reranking for improved relevance scoring.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class CrossEncoderReranker<T> : RerankingStrategyBase<T>\\n+    {\\n+        private readonly INumericOperations<T> _numOps;\\n+        private readonly string _modelPath;\\n+\\n+        /// <summary>\\n+        /// Initializes a new instance of the <see cref=\\\"CrossEncoderReranker{T}\\\"/> class.\\n+        /// </summary>\\n+        /// <param name=\\\"numericOperations\\\">The numeric operations for type T.</param>\\n+        /// <param name=\\\"modelPath\\\">The path to the cross-encoder model.</param>\\n+        public CrossEncoderReranker(INumericOperations<T> numericOperations, string? modelPath = null) : base(numericOperations)\\n+        {\\n+            _numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\\n+            _modelPath = modelPath ?? string.Empty;\\n+        }\\n+\\n+        /// <summary>\\n+        /// Reranks documents using cross-encoder scoring.\\n+        /// </summary>\\n+        /// <param name=\\\"query\\\">The query string.</param>\\n+        /// <param name=\\\"documents\\\">The documents to rerank.</param>\\n+        /// <param name=\\\"topK\\\">The number of top documents to return.</param>\\n+        /// <returns>A reranked list of documents.</returns>\\n+        public override List<Document<T>> Rerank(string query, List<Document<T>> documents, int topK)\\n+        {\\n+            if (string.IsNullOrEmpty(query)) throw new ArgumentNullException(nameof(query));\\n+            if (documents == null) throw new ArgumentNullException(nameof(documents));\\n+            if (topK <= 0) throw new ArgumentOutOfRangeException(nameof(topK));\\n+\\n+            var scoredDocs = new List<(Document<T> doc, T score)>();\\n+\\n+            foreach (var doc in documents)\\n+            {\\n+                var score = ComputeCrossEncoderScore(query, doc.Content);\\n+                scoredDocs.Add((doc, score));\\n+            }\\n+\\n+            var reranked = scoredDocs\\n+                .OrderByDescending(x => Convert.ToDouble(x.score))\\n+                .Take(topK)\\n+                .Select(x =>\\n+                {\\n+                    x.doc.RelevanceScore = x.score;\\n+                    return x.doc;\\n+                })\\n+                .ToList();\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZGNP\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTYTI\",\"body\":\"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\\n\\n**Prevent zero-token queries from producing bogus scores.**\\n\\nWhitespace-only or punctuation-only queries slip past the current guard, leaving `queryWords.Count` at 0 and turning `totalScore / queryWords.Count` into `Infinity`, so every document gets the same max score. Tighten the validation and short-circuit empty token lists so we return a zero score instead of poisoning the ranking.\\n\\n\\n```diff\\n-            if (string.IsNullOrEmpty(query)) throw new ArgumentNullException(nameof(query));\\n+            if (string.IsNullOrWhiteSpace(query)) throw new ArgumentNullException(nameof(query));\\n@@\\n-            var queryWords = Tokenize(query);\\n+            var queryWords = Tokenize(query);\\n+            if (queryWords.Count == 0)\\n+                return _numOps.Zero;\\n```\\n\\n\\nAlso applies to: 68-77\\n\\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\\n\\n<!-- This is an auto-generated comment by CodeRabbit -->\",\"path\":\"src/RetrievalAugmentedGeneration/RerankingStrategies/LLMBasedReranker.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,124 @@\\n+using System;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+using AiDotNet.Helpers;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.RerankingStrategies\\n+{\\n+    /// <summary>\\n+    /// LLM-based reranking using language model relevance assessment.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class LLMBasedReranker<T> : RerankingStrategyBase<T>\\n+    {\\n+        private readonly INumericOperations<T> _numOps;\\n+        private readonly string _llmEndpoint;\\n+        private readonly string _apiKey;\\n+\\n+        /// <summary>\\n+        /// Initializes a new instance of the <see cref=\\\"LLMBasedReranker{T}\\\"/> class.\\n+        /// </summary>\\n+        /// <param name=\\\"numericOperations\\\">The numeric operations for type T.</param>\\n+        /// <param name=\\\"llmEndpoint\\\">The LLM API endpoint.</param>\\n+        /// <param name=\\\"apiKey\\\">The API key for the LLM service.</param>\\n+        public LLMBasedReranker(INumericOperations<T> numericOperations, string? llmEndpoint = null, string? apiKey = null) : base(numericOperations)\\n+        {\\n+            _numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\\n+            _llmEndpoint = llmEndpoint ?? string.Empty;\\n+            _apiKey = apiKey ?? string.Empty;\\n+        }\\n+\\n+        /// <summary>\\n+        /// Reranks documents using LLM-based relevance scoring.\\n+        /// </summary>\\n+        /// <param name=\\\"query\\\">The query string.</param>\\n+        /// <param name=\\\"documents\\\">The documents to rerank.</param>\\n+        /// <param name=\\\"topK\\\">The number of top documents to return.</param>\\n+        /// <returns>A reranked list of documents.</returns>\\n+        public override List<Document<T>> Rerank(string query, List<Document<T>> documents, int topK)\\n+        {\\n+            if (string.IsNullOrEmpty(query)) throw new ArgumentNullException(nameof(query));\\n+            if (documents == null) throw new ArgumentNullException(nameof(documents));\\n+            if (topK <= 0) throw new ArgumentOutOfRangeException(nameof(topK));\\n+\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZGNT\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTYTQ\",\"body\":\"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\\n\\n**Mark reranked documents as scored.**\\n\\nDownstream components rely on `HasRelevanceScore`. Leaving it `false` after reranking means later stages will think no score was assigned.\\n\\n\\n```diff\\n                     x.doc.RelevanceScore = x.score;\\n+                    x.doc.HasRelevanceScore = true;\\n                     return x.doc;\\n```\\n\\n<details>\\n<summary>≡ƒñû Prompt for AI Agents</summary>\\n\\n```\\nIn src/RetrievalAugmentedGeneration/RerankingStrategies/LLMBasedReranker.cs\\naround lines 58 to 60, the code sets x.doc.RelevanceScore = x.score but does not\\nmark the document as scored; update the reranking mapping to also set\\nx.doc.HasRelevanceScore = true (or the equivalent flag/property) immediately\\nafter assigning RelevanceScore so downstream components see the document as\\nscored.\\n```\\n\\n</details>\\n\\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\\n\\n<!-- This is an auto-generated comment by CodeRabbit -->\\n\\nΓ£à Addressed in commits ec6a7c9 to 77a1bbb\",\"path\":\"src/RetrievalAugmentedGeneration/RerankingStrategies/LLMBasedReranker.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,124 @@\\n+using System;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+using AiDotNet.Helpers;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.RerankingStrategies\\n+{\\n+    /// <summary>\\n+    /// LLM-based reranking using language model relevance assessment.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class LLMBasedReranker<T> : RerankingStrategyBase<T>\\n+    {\\n+        private readonly INumericOperations<T> _numOps;\\n+        private readonly string _llmEndpoint;\\n+        private readonly string _apiKey;\\n+\\n+        /// <summary>\\n+        /// Initializes a new instance of the <see cref=\\\"LLMBasedReranker{T}\\\"/> class.\\n+        /// </summary>\\n+        /// <param name=\\\"numericOperations\\\">The numeric operations for type T.</param>\\n+        /// <param name=\\\"llmEndpoint\\\">The LLM API endpoint.</param>\\n+        /// <param name=\\\"apiKey\\\">The API key for the LLM service.</param>\\n+        public LLMBasedReranker(INumericOperations<T> numericOperations, string? llmEndpoint = null, string? apiKey = null) : base(numericOperations)\\n+        {\\n+            _numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\\n+            _llmEndpoint = llmEndpoint ?? string.Empty;\\n+            _apiKey = apiKey ?? string.Empty;\\n+        }\\n+\\n+        /// <summary>\\n+        /// Reranks documents using LLM-based relevance scoring.\\n+        /// </summary>\\n+        /// <param name=\\\"query\\\">The query string.</param>\\n+        /// <param name=\\\"documents\\\">The documents to rerank.</param>\\n+        /// <param name=\\\"topK\\\">The number of top documents to return.</param>\\n+        /// <returns>A reranked list of documents.</returns>\\n+        public override List<Document<T>> Rerank(string query, List<Document<T>> documents, int topK)\\n+        {\\n+            if (string.IsNullOrEmpty(query)) throw new ArgumentNullException(nameof(query));\\n+            if (documents == null) throw new ArgumentNullException(nameof(documents));\\n+            if (topK <= 0) throw new ArgumentOutOfRangeException(nameof(topK));\\n+\\n+            var scoredDocs = new List<(Document<T> doc, T score)>();\\n+\\n+            foreach (var doc in documents)\\n+            {\\n+                var score = AssessRelevance(query, doc.Content);\\n+                scoredDocs.Add((doc, score));\\n+            }\\n+\\n+            var reranked = scoredDocs\\n+                .OrderByDescending(x => Convert.ToDouble(x.score))\\n+                .Take(topK)\\n+                .Select(x =>\\n+                {\\n+                    x.doc.RelevanceScore = x.score;\\n+                    return x.doc;\\n+                })\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZGNa\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTYTa\",\"body\":\"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\\n\\n**Mark documents as having scores**\\n\\nAfter you assign `RelevanceScore`, you need to set `doc.HasRelevanceScore = true`; otherwise anything that checks the flag (e.g., coverage heuristics) will treat these reranker outputs as unscored.\\n\\n\\n\\nAlso applies to: 114-117\\n\\n<details>\\n<summary>≡ƒñû Prompt for AI Agents</summary>\\n\\n```\\nIn src/RetrievalAugmentedGeneration/RerankingStrategies/ReciprocalRankFusion.cs\\naround lines 64-70 (and similarly at 114-117), after assigning\\ndoc.RelevanceScore = scores[doc.Id] also set doc.HasRelevanceScore = true so\\ndownstream checks treat these documents as scored; update both blocks to set the\\nflag immediately after assigning the score (and ensure the scores lookup is safe\\nas currently implemented).\\n```\\n\\n</details>\\n\\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\\n\\n<!-- This is an auto-generated comment by CodeRabbit -->\\n\\nΓ£à Addressed in commits ec6a7c9 to 77a1bbb\",\"path\":\"src/RetrievalAugmentedGeneration/RerankingStrategies/ReciprocalRankFusion.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,122 @@\\n+using System;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+using AiDotNet.Helpers;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.RerankingStrategies\\n+{\\n+    /// <summary>\\n+    /// Reciprocal Rank Fusion for combining multiple ranking lists.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class ReciprocalRankFusion<T> : RerankingStrategyBase<T>\\n+    {\\n+        private readonly INumericOperations<T> _numOps;\\n+        private readonly int _k;\\n+\\n+        /// <summary>\\n+        /// Initializes a new instance of the <see cref=\\\"ReciprocalRankFusion{T}\\\"/> class.\\n+        /// </summary>\\n+        /// <param name=\\\"numericOperations\\\">The numeric operations for type T.</param>\\n+        /// <param name=\\\"k\\\">The constant k for reciprocal rank formula (default: 60).</param>\\n+        public ReciprocalRankFusion(INumericOperations<T> numericOperations, int k = 60) : base(numericOperations)\\n+        {\\n+            _numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\\n+            _k = k > 0 ? k : throw new ArgumentOutOfRangeException(nameof(k));\\n+        }\\n+\\n+        /// <summary>\\n+        /// Reranks documents using reciprocal rank fusion.\\n+        /// </summary>\\n+        /// <param name=\\\"query\\\">The query string.</param>\\n+        /// <param name=\\\"documents\\\">The documents to rerank.</param>\\n+        /// <param name=\\\"topK\\\">The number of top documents to return.</param>\\n+        /// <returns>A reranked list of documents.</returns>\\n+        public override List<Document<T>> Rerank(string query, List<Document<T>> documents, int topK)\\n+        {\\n+            if (string.IsNullOrEmpty(query)) throw new ArgumentNullException(nameof(query));\\n+            if (documents == null) throw new ArgumentNullException(nameof(documents));\\n+            if (topK <= 0) throw new ArgumentOutOfRangeException(nameof(topK));\\n+\\n+            var scores = new Dictionary<string, T>();\\n+\\n+            for (int rank = 0; rank < documents.Count; rank++)\\n+            {\\n+                var doc = documents[rank];\\n+                var rrfScore = _numOps.FromDouble(1.0 / (_k + rank + 1));\\n+\\n+                if (scores.ContainsKey(doc.Id))\\n+                {\\n+                    scores[doc.Id] = _numOps.Add(scores[doc.Id], rrfScore);\\n+                }\\n+                else\\n+                {\\n+                    scores[doc.Id] = rrfScore;\\n+                }\\n+            }\\n+\\n+            var reranked = documents\\n+                .OrderByDescending(d => Convert.ToDouble(scores.ContainsKey(d.Id) ? scores[d.Id] : _numOps.Zero))\\n+                .Take(topK)\\n+                .ToList();\\n+\\n+            foreach (var doc in reranked)\\n+            {\\n+                if (scores.ContainsKey(doc.Id))\\n+                {\\n+                    doc.RelevanceScore = scores[doc.Id];\\n+                }\\n+            }\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZGNe\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTYTf\",\"body\":\"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\\n\\n**Initialize BM25 corpus statistics before scoring.**\\n\\n`_termFrequencies`, `_documentLengths`, and `_avgDocLength` are never populated, so `CalculateBM25Term` returns zero for every term and then divides by `_avgDocLength == 0`, throwing the first time you evaluate a document with content. Please precompute per-document term frequencies, lengths, and a non-zero average (and guard against an empty corpus) before invoking the scoring loop.\\n\\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\\n\\n<!-- This is an auto-generated comment by CodeRabbit -->\\n\\nΓ£à Addressed in commits ec6a7c9 to 77a1bbb\",\"path\":\"src/RetrievalAugmentedGeneration/Retrievers/BM25Retriever.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,124 @@\\n+using AiDotNet.Helpers;\\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\\n+{\\n+    /// <summary>\\n+    /// BM25 (Best Matching 25) retrieval algorithm for sparse keyword-based search.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class BM25Retriever<T> : RetrieverBase<T>\\n+    {\\n+        private readonly IDocumentStore<T> _documentStore;\\n+        private readonly T _k1;\\n+        private readonly T _b;\\n+        private readonly Dictionary<string, Dictionary<string, T>> _termFrequencies;\\n+        private readonly Dictionary<string, T> _documentLengths;\\n+        private T _avgDocLength;\\n+\\n+        public BM25Retriever(IDocumentStore<T> documentStore, int defaultTopK = 5, double k1 = 1.5, double b = 0.75) \\n+            : base(defaultTopK)\\n+        {\\n+            if (documentStore == null)\\n+                throw new ArgumentNullException(nameof(documentStore));\\n+\\n+            _documentStore = documentStore;\\n+            _k1 = NumOps.FromDouble(k1);\\n+            _b = NumOps.FromDouble(b);\\n+            _termFrequencies = new Dictionary<string, Dictionary<string, T>>();\\n+            _documentLengths = new Dictionary<string, T>();\\n+            _avgDocLength = NumOps.Zero;\\n+        }\\n+\\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\\n+        {\\n+            var queryTerms = Tokenize(query);\\n+            var scores = new Dictionary<string, T>();\\n+\\n+            var candidates = _documentStore.GetSimilar(\\n+                new AiDotNet.LinearAlgebra.Vector<T>(new T[Math.Max(1, _documentStore.VectorDimension)]),\\n+                _documentStore.DocumentCount\\n+            );\\n+\\n+            foreach (var doc in candidates)\\n+            {\\n+                if (!MatchesFilters(doc, metadataFilters))\\n+                    continue;\\n+\\n+                var score = NumOps.Zero;\\n+                \\n+                foreach (var term in queryTerms)\\n+                {\\n+                    var termScore = CalculateBM25Term(doc.Id, term);\\n+                    score = NumOps.Add(score, termScore);\\n+                }\\n+\\n+                scores[doc.Id] = score;\\n+            }\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZGNi\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTYTi\",\"body\":\"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\\n\\n**Include the IDF factor in the BM25 score.**\\n\\nRight now we only accumulate term-frequency contributions. BM25 weights each term by its inverse document frequency; without IDF, ubiquitous terms dominate the ranking and the retriever no longer behaves like BM25. Please compute doc frequencies (N, n(q)) from the corpus stats you gather and fold the IDF multiplier into each term contribution.\\n\\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\\n\\n<!-- This is an auto-generated comment by CodeRabbit -->\\n\\nΓ£à Addressed in commits ec6a7c9 to 77a1bbb\",\"path\":\"src/RetrievalAugmentedGeneration/Retrievers/BM25Retriever.cs\",\"position\":47,\"diffHunk\":\"@@ -0,0 +1,124 @@\\n+using AiDotNet.Helpers;\\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\\n+{\\n+    /// <summary>\\n+    /// BM25 (Best Matching 25) retrieval algorithm for sparse keyword-based search.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class BM25Retriever<T> : RetrieverBase<T>\\n+    {\\n+        private readonly IDocumentStore<T> _documentStore;\\n+        private readonly T _k1;\\n+        private readonly T _b;\\n+        private readonly Dictionary<string, Dictionary<string, T>> _termFrequencies;\\n+        private readonly Dictionary<string, T> _documentLengths;\\n+        private T _avgDocLength;\\n+\\n+        public BM25Retriever(IDocumentStore<T> documentStore, int defaultTopK = 5, double k1 = 1.5, double b = 0.75) \\n+            : base(defaultTopK)\\n+        {\\n+            if (documentStore == null)\\n+                throw new ArgumentNullException(nameof(documentStore));\\n+\\n+            _documentStore = documentStore;\\n+            _k1 = NumOps.FromDouble(k1);\\n+            _b = NumOps.FromDouble(b);\\n+            _termFrequencies = new Dictionary<string, Dictionary<string, T>>();\\n+            _documentLengths = new Dictionary<string, T>();\\n+            _avgDocLength = NumOps.Zero;\\n+        }\\n+\\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\\n+        {\\n+            var queryTerms = Tokenize(query);\\n+            var scores = new Dictionary<string, T>();\\n+\\n+            var candidates = _documentStore.GetSimilar(\\n+                new AiDotNet.LinearAlgebra.Vector<T>(new T[Math.Max(1, _documentStore.VectorDimension)]),\\n+                _documentStore.DocumentCount\\n+            );\\n+\\n+            foreach (var doc in candidates)\\n+            {\\n+                if (!MatchesFilters(doc, metadataFilters))\\n+                    continue;\\n+\\n+                var score = NumOps.Zero;\\n+                \\n+                foreach (var term in queryTerms)\\n+                {\\n+                    var termScore = CalculateBM25Term(doc.Id, term);\\n+                    score = NumOps.Add(score, termScore);\\n+                }\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZGNm\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTYTn\",\"body\":\"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\\n\\n**Fix the BM25 length-normalization formula.**\\n\\nThe denominator currently computes `1 - (b + b * |d| / avgdl)`, but BM25 requires `1 - b + b * |d| / avgdl`. The inverted sign drives the normalization negative once documents exceed the average length, producing incorrect or even negative scores. Update the algebra and reuse the normalization term when building the denominator:\\n\\n```diff\\n-            var denominator = NumOps.Add(tf, NumOps.Multiply(_k1, \\n-                NumOps.Subtract(NumOps.One, NumOps.Add(_b, NumOps.Multiply(_b, NumOps.Divide(docLength, _avgDocLength))))));\\n+            var lengthNorm = NumOps.Add(\\n+                NumOps.Subtract(NumOps.One, _b),\\n+                NumOps.Multiply(_b, NumOps.Divide(docLength, _avgDocLength)));\\n+            var denominator = NumOps.Add(tf, NumOps.Multiply(_k1, lengthNorm));\\n```\\n\\n<!-- suggestion_start -->\\n\\n<details>\\n<summary>≡ƒô¥ Committable suggestion</summary>\\n\\n> ΓÇ╝∩╕Å **IMPORTANT**\\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\\n\\n```suggestion\\n            var numerator = NumOps.Multiply(tf, NumOps.Add(_k1, NumOps.One));\\n            var lengthNorm = NumOps.Add(\\n                NumOps.Subtract(NumOps.One, _b),\\n                NumOps.Multiply(_b, NumOps.Divide(docLength, _avgDocLength)));\\n            var denominator = NumOps.Add(tf, NumOps.Multiply(_k1, lengthNorm));\\n\\n            return NumOps.Divide(numerator, denominator);\\n        }\\n```\\n\\n</details>\\n\\n<!-- suggestion_end -->\\n\\n<details>\\n<summary>≡ƒñû Prompt for AI Agents</summary>\\n\\n```\\nIn src/RetrievalAugmentedGeneration/Retrievers/BM25Retriever.cs around lines 90\\nto 95, the BM25 length-normalization term is built with the wrong sign; replace\\nthe current expression so the normalization term equals (1 - b) + b * (|d| /\\navgdl) and reuse that term when constructing the denominator. Concretely,\\ncompute normalizationTerm = (NumOps.One - _b) + (_b * (docLength /\\n_avgDocLength)) using NumOps, then set denominator = NumOps.Add(tf,\\nNumOps.Multiply(_k1, normalizationTerm)) so the denominator implements tf + k1 *\\n((1 - b) + b * |d|/avgdl).\\n```\\n\\n</details>\\n\\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\\n\\n<!-- This is an auto-generated comment by CodeRabbit -->\\n\\nΓ£à Addressed in commits ec6a7c9 to 77a1bbb\",\"path\":\"src/RetrievalAugmentedGeneration/Retrievers/BM25Retriever.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,124 @@\\n+using AiDotNet.Helpers;\\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\\n+{\\n+    /// <summary>\\n+    /// BM25 (Best Matching 25) retrieval algorithm for sparse keyword-based search.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class BM25Retriever<T> : RetrieverBase<T>\\n+    {\\n+        private readonly IDocumentStore<T> _documentStore;\\n+        private readonly T _k1;\\n+        private readonly T _b;\\n+        private readonly Dictionary<string, Dictionary<string, T>> _termFrequencies;\\n+        private readonly Dictionary<string, T> _documentLengths;\\n+        private T _avgDocLength;\\n+\\n+        public BM25Retriever(IDocumentStore<T> documentStore, int defaultTopK = 5, double k1 = 1.5, double b = 0.75) \\n+            : base(defaultTopK)\\n+        {\\n+            if (documentStore == null)\\n+                throw new ArgumentNullException(nameof(documentStore));\\n+\\n+            _documentStore = documentStore;\\n+            _k1 = NumOps.FromDouble(k1);\\n+            _b = NumOps.FromDouble(b);\\n+            _termFrequencies = new Dictionary<string, Dictionary<string, T>>();\\n+            _documentLengths = new Dictionary<string, T>();\\n+            _avgDocLength = NumOps.Zero;\\n+        }\\n+\\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\\n+        {\\n+            var queryTerms = Tokenize(query);\\n+            var scores = new Dictionary<string, T>();\\n+\\n+            var candidates = _documentStore.GetSimilar(\\n+                new AiDotNet.LinearAlgebra.Vector<T>(new T[Math.Max(1, _documentStore.VectorDimension)]),\\n+                _documentStore.DocumentCount\\n+            );\\n+\\n+            foreach (var doc in candidates)\\n+            {\\n+                if (!MatchesFilters(doc, metadataFilters))\\n+                    continue;\\n+\\n+                var score = NumOps.Zero;\\n+                \\n+                foreach (var term in queryTerms)\\n+                {\\n+                    var termScore = CalculateBM25Term(doc.Id, term);\\n+                    score = NumOps.Add(score, termScore);\\n+                }\\n+\\n+                scores[doc.Id] = score;\\n+            }\\n+\\n+            var results = scores\\n+                .OrderByDescending(kv => kv.Value)\\n+                .Take(topK)\\n+                .Select(kv =>\\n+                {\\n+                    var doc = candidates.FirstOrDefault(d => d.Id == kv.Key);\\n+                    if (doc != null)\\n+                    {\\n+                        doc.RelevanceScore = kv.Value;\\n+                        doc.HasRelevanceScore = true;\\n+                    }\\n+                    return doc;\\n+                })\\n+                .Where(d => d != null)\\n+                .Cast<Document<T>>();\\n+\\n+            return results;\\n+        }\\n+\\n+        private T CalculateBM25Term(string docId, string term)\\n+        {\\n+            if (!_termFrequencies.ContainsKey(docId) || !_termFrequencies[docId].ContainsKey(term))\\n+                return NumOps.Zero;\\n+\\n+            var tf = _termFrequencies[docId][term];\\n+            var docLength = _documentLengths.ContainsKey(docId) ? _documentLengths[docId] : NumOps.Zero;\\n+\\n+            var numerator = NumOps.Multiply(tf, NumOps.Add(_k1, NumOps.One));\\n+            var denominator = NumOps.Add(tf, NumOps.Multiply(_k1, \\n+                NumOps.Subtract(NumOps.One, NumOps.Add(_b, NumOps.Multiply(_b, NumOps.Divide(docLength, _avgDocLength))))));\\n+\\n+            return NumOps.Divide(numerator, denominator);\\n+        }\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZGNs\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTYTv\",\"body\":\"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\\n\\n**TFΓÇæIDF scores never built**\\n\\n`_tfidf` (and `_idf`) stay empty for the lifetime of this retriever, so every document ends up with the default `NumOps.Zero` score and the ranking collapses. Please add the TF/IDF bookkeeping (e.g., during ingestion or lazily before retrieval) so `RetrieveCore` has real weights to sum.\\n\\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\\n\\n<!-- This is an auto-generated comment by CodeRabbit -->\\n\\nΓ£à Addressed in commits ec6a7c9 to 77a1bbb\",\"path\":\"src/RetrievalAugmentedGeneration/Retrievers/TFIDFRetriever.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,108 @@\\n+using AiDotNet.Helpers;\\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\\n+{\\n+    /// <summary>\\n+    /// TF-IDF (Term Frequency-Inverse Document Frequency) retrieval strategy.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class TFIDFRetriever<T> : RetrieverBase<T>\\n+    {\\n+        private readonly IDocumentStore<T> _documentStore;\\n+        private readonly Dictionary<string, Dictionary<string, T>> _tfidf;\\n+        private readonly Dictionary<string, T> _idf;\\n+\\n+        public TFIDFRetriever(IDocumentStore<T> documentStore, int defaultTopK = 5) : base(defaultTopK)\\n+        {\\n+            if (documentStore == null)\\n+                throw new ArgumentNullException(nameof(documentStore));\\n+                \\n+            _documentStore = documentStore;\\n+            _tfidf = new Dictionary<string, Dictionary<string, T>>();\\n+            _idf = new Dictionary<string, T>();\\n+        }\\n+\\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\\n+        {\\n+            var queryTerms = Tokenize(query);\\n+            var scores = new Dictionary<string, T>();\\n+\\n+            var candidates = _documentStore.GetSimilar(\\n+                new AiDotNet.LinearAlgebra.Vector<T>(new T[_documentStore.VectorDimension]), \\n+                _documentStore.DocumentCount\\n+            );\\n+\\n+            foreach (var doc in candidates)\\n+            {\\n+                if (!MatchesFilters(doc, metadataFilters))\\n+                    continue;\\n+\\n+                var score = NumOps.Zero;\\n+\\n+                if (_tfidf.ContainsKey(doc.Id))\\n+                {\\n+                    var docTfidf = _tfidf[doc.Id];\\n+                    foreach (var term in queryTerms)\\n+                    {\\n+                        if (docTfidf.ContainsKey(term))\\n+                        {\\n+                            score = NumOps.Add(score, docTfidf[term]);\\n+                        }\\n+                    }\\n+                }\\n+\\n+                scores[doc.Id] = score;\\n+            }\\n+\\n+            var results = scores\\n+                .OrderByDescending(kv => kv.Value)\\n+                .Take(topK)\\n+                .Select(kv =>\\n+                {\\n+                    var doc = candidates.FirstOrDefault(d => d.Id == kv.Key);\\n+                    if (doc != null)\\n+                    {\\n+                        doc.RelevanceScore = kv.Value;\\n+                        doc.HasRelevanceScore = true;\\n+                    }\\n+                    return doc;\\n+                })\\n+                .Where(d => d != null)\\n+                .Cast<Document<T>>();\\n+\\n+            return results;\\n+        }\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZaUQ\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTz2q\",\"body\":\"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\\n\\n<details>\\n<summary>≡ƒº⌐ Analysis chain</summary>\\n\\n**Verify the INumericOperations<T> constructor pattern is consistently enforced across implementations.**\\n\\nThe rule states that `INumericOperations<T>` should never be requested in public constructors. This is a strong constraint that must be verified across all concrete implementations in the PR to ensure consistency and prevent future violations.\\n\\n\\n\\nRun the following script to verify this pattern is upheld in concrete implementations:\\n\\n---\\n\\n≡ƒÅü Script executed:\\n\\n```shell\\n#!/bin/bash\\n# Description: Check if public constructors in RAG implementations request INumericOperations<T>\\n\\n# Search for public constructors in RAG concrete implementations that have INumericOperations parameter\\n# Expected: Zero matches (indicating the pattern is enforced)\\nrg -n 'public\\\\s+\\\\w+\\\\s*\\\\(' \\\\\\n  --type=cs \\\\\\n  -A 2 'INumericOperations<' \\\\\\n  -g '**/RetrievalAugmentedGeneration/**' \\\\\\n  -g '!**/Interfaces/**' \\\\\\n  | grep -E '(public|INumericOperations)' \\\\\\n  | head -20\\n```\\n\\nLength of output: 59\\n\\n---\\n\\n[/run_scripts]\\n\\n---\\n\\n≡ƒÅü Script executed:\\n\\n```shell\\n#!/bin/bash\\n# Search for public constructors with INumericOperations parameter in RAG implementations\\n# Corrected approach: search for the pattern directly\\n\\nrg -n 'public\\\\s+\\\\w+\\\\s*\\\\([^)]*INumericOperations<' \\\\\\n  --type=cs \\\\\\n  --glob='**/RetrievalAugmentedGeneration/**' \\\\\\n  --glob='!**/Interfaces/**' \\\\\\n  -C 2\\n```\\n\\nLength of output: 3886\\n\\n---\\n\\n\\n\\n</details>\\n\\n**Fix public constructor violations of INumericOperations<T> constraint.**\\n\\nThe verification found 5 concrete implementations that violate the guideline - all have public constructors requesting `INumericOperations<T>`. These must be refactored to handle the dependency internally:\\n\\n- `src/RetrievalAugmentedGeneration/RerankingStrategies/LLMBasedReranker.cs:25`\\n- `src/RetrievalAugmentedGeneration/RerankingStrategies/ReciprocalRankFusion.cs:23`\\n- `src/RetrievalAugmentedGeneration/RerankingStrategies/CrossEncoderReranker.cs:23`\\n- `src/RetrievalAugmentedGeneration/Configuration/RAGConfigurationBuilder.cs:19`\\n- `src/RetrievalAugmentedGeneration/QueryExpansion/HyDEQueryExpansion.cs:24`\\n\\n<details>\\n<summary>≡ƒñû Prompt for AI Agents</summary>\\n\\n```\\nIn the listed files\\n(src/RetrievalAugmentedGeneration/RerankingStrategies/LLMBasedReranker.cs:25,\\nsrc/RetrievalAugmentedGeneration/RerankingStrategies/ReciprocalRankFusion.cs:23,\\nsrc/RetrievalAugmentedGeneration/RerankingStrategies/CrossEncoderReranker.cs:23,\\nsrc/RetrievalAugmentedGeneration/Configuration/RAGConfigurationBuilder.cs:19,\\nsrc/RetrievalAugmentedGeneration/QueryExpansion/HyDEQueryExpansion.cs:24) the\\npublic constructors currently accept an INumericOperations<T> parameter which\\nviolates the guideline; remove INumericOperations<T> from the public constructor\\nsignatures, make the dependency internal/private by resolving or creating the\\nnumeric operations instance inside the class (e.g., use a private factory\\nmethod, internal static provider, or obtain it from an internal service\\nlocator/DI configured only inside the assembly), update constructors to accept\\nonly necessary public dependencies, and adjust any callers/tests to stop passing\\nINumericOperations<T> while preserving behavior by initializing the numeric\\noperations within the class.\\n```\\n\\n</details>\\n\\n<!-- This is an auto-generated comment by CodeRabbit -->\",\"path\":\".github/COPILOT_INSTRUCTIONS.md\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,103 @@\\n+# AiDotNet Copilot Instructions - CRITICAL RULES\\n+\\n+## Architecture Rules (DO NOT VIOLATE)\\n+\\n+### 1. Generic Types and Numeric Operations\\n+- **ALWAYS** use generic types with `INumericOperations<T>` interface\\n+- **NEVER** hardcode `double`, `float`, or specific numeric types\\n+- Use `NumOps.FromDouble()`, `NumOps.Zero`, `NumOps.One` instead of literals or `default(T)`\\n+- **NEVER** request `INumericOperations<T>` in public constructors - this is handled internally\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZaUW\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTz2y\",\"body\":\"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\\n\\n**Placeholder implementation creates critical production risk.**\\n\\nThis class is named `OpenAIEmbeddingModel` and accepts an API key, strongly implying it calls the OpenAI embeddings API. However, `EmbedCore` (lines 39-51) generates synthetic cosine-based vectors instead of calling OpenAI. Developers integrating this code will expect real semantic embeddings but receive synthetic placeholders, breaking retrieval quality and potentially causing silent production failures.\\n\\n\\n\\nEither:\\n1. Implement actual OpenAI API calls using the stored `_apiKey` and `_modelName`, or\\n2. Rename to `SyntheticEmbeddingModel` / `PlaceholderEmbeddingModel` and remove the misleading API parameters.\\n\\n```diff\\n-public class OpenAIEmbeddingModel<T> : EmbeddingModelBase<T>\\n+public class SyntheticCosineLikeEmbeddingModel<T> : EmbeddingModelBase<T>\\n {\\n-    private readonly string _apiKey;\\n-    private readonly string _modelName;\\n     private readonly int _dimension;\\n     private readonly int _maxTokens;\\n \\n-    public OpenAIEmbeddingModel(string apiKey, string modelName = \\\"text-embedding-ada-002\\\", int dimension = 1536, int maxTokens = 8191)\\n+    public SyntheticCosineLikeEmbeddingModel(int dimension = 1536, int maxTokens = 8191)\\n     {\\n-        if (string.IsNullOrWhiteSpace(apiKey))\\n-            throw new ArgumentException(\\\"API key cannot be empty\\\", nameof(apiKey));\\n-        if (string.IsNullOrWhiteSpace(modelName))\\n-            throw new ArgumentException(\\\"Model name cannot be empty\\\", nameof(modelName));\\n         if (dimension <= 0)\\n             throw new ArgumentException(\\\"Dimension must be positive\\\", nameof(dimension));\\n         if (maxTokens <= 0)\\n             throw new ArgumentException(\\\"Max tokens must be positive\\\", nameof(maxTokens));\\n \\n-        _apiKey = apiKey;\\n-        _modelName = modelName;\\n         _dimension = dimension;\\n         _maxTokens = maxTokens;\\n     }\\n```\\n\\n\\n> Committable suggestion skipped: line range outside the PR's diff.\\n\\n<details>\\n<summary>≡ƒñû Prompt for AI Agents</summary>\\n\\n```\\nIn src/RetrievalAugmentedGeneration/EmbeddingModels/OpenAIEmbeddingModel.cs\\naround lines 22-37: the constructor exposes an API key and model name but the\\nclass currently returns synthetic cosine-based vectors (placeholder) which is\\nmisleading and dangerous; either implement real OpenAI embedding calls using the\\nstored _apiKey and _modelName or rename/remove API-specific parameters to make\\nit explicitly a synthetic placeholder. To implement real calls: replace the\\nplaceholder EmbedCore logic to call the OpenAI embeddings endpoint (or official\\nSDK) using _apiKey and _modelName, send the input text, validate response vector\\nlength equals _dimension, handle HTTP errors, rate limits, timeouts, and surface\\nmeaningful exceptions/logging; add unit/integration tests and update any\\ncallers. To instead rename: rename class to SyntheticEmbeddingModel (or\\nPlaceholderEmbeddingModel), remove apiKey/modelName parameters from the\\nconstructor (or mark them optional and ignored), update usages/tests and\\ndocumentation to reflect synthetic behavior.\\n```\\n\\n</details>\\n\\n<!-- This is an auto-generated comment by CodeRabbit -->\",\"path\":\"src/RetrievalAugmentedGeneration/Embeddings/OpenAIEmbeddingModel.cs\",\"position\":37,\"diffHunk\":\"@@ -0,0 +1,91 @@\\n+using AiDotNet.Helpers;\\n+using AiDotNet.LinearAlgebra;\\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\\n+using System;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels\\n+{\\n+    /// <summary>\\n+    /// OpenAI embedding model for generating embeddings via OpenAI API.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class OpenAIEmbeddingModel<T> : EmbeddingModelBase<T>\\n+    {\\n+        private readonly string _apiKey;\\n+        private readonly string _modelName;\\n+        private readonly int _dimension;\\n+        private readonly int _maxTokens;\\n+\\n+        public override int EmbeddingDimension => _dimension;\\n+        public override int MaxTokens => _maxTokens;\\n+\\n+        public OpenAIEmbeddingModel(string apiKey, string modelName = \\\"text-embedding-ada-002\\\", int dimension = 1536, int maxTokens = 8191)\\n+        {\\n+            if (string.IsNullOrWhiteSpace(apiKey))\\n+                throw new ArgumentException(\\\"API key cannot be empty\\\", nameof(apiKey));\\n+            if (string.IsNullOrWhiteSpace(modelName))\\n+                throw new ArgumentException(\\\"Model name cannot be empty\\\", nameof(modelName));\\n+            if (dimension <= 0)\\n+                throw new ArgumentException(\\\"Dimension must be positive\\\", nameof(dimension));\\n+            if (maxTokens <= 0)\\n+                throw new ArgumentException(\\\"Max tokens must be positive\\\", nameof(maxTokens));\\n+\\n+            _apiKey = apiKey;\\n+            _modelName = modelName;\\n+            _dimension = dimension;\\n+            _maxTokens = maxTokens;\\n+        }\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZaUY\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTz22\",\"body\":\"_≡ƒ¢á∩╕Å Refactor suggestion_ | _≡ƒƒá Major_\\n\\n**Extract duplicated helper methods to base class.**\\n\\n`GetDeterministicHash` and `NormalizeVector` are duplicated identically across OpenAIEmbeddingModel.cs, ONNXSentenceTransformer.cs, HuggingFaceEmbeddingModel.cs, and LocalTransformerEmbedding.cs. This violates DRY and complicates maintenance.\\n\\n\\n\\nMove both methods to `EmbeddingModelBase<T>` as protected methods so all derived classes can reuse them:\\n\\nIn `src/RetrievalAugmentedGeneration/Embeddings/EmbeddingModelBase.cs`, add:\\n\\n```csharp\\nprotected int GetDeterministicHash(string text)\\n{\\n    if (string.IsNullOrEmpty(text))\\n        return 0;\\n\\n    unchecked\\n    {\\n        int hash = 23;\\n        foreach (char c in text)\\n        {\\n            hash = hash * 31 + c;\\n        }\\n        return hash;\\n    }\\n}\\n\\nprotected Vector<T> NormalizeVector(Vector<T> vector)\\n{\\n    var magnitude = NumOps.Zero;\\n    for (int i = 0; i < vector.Length; i++)\\n    {\\n        magnitude = NumOps.Add(magnitude, NumOps.Multiply(vector[i], vector[i]));\\n    }\\n    magnitude = NumOps.FromDouble(Math.Sqrt(Convert.ToDouble(magnitude)));\\n\\n    if (NumOps.GreaterThan(magnitude, NumOps.Zero))\\n    {\\n        var normalized = new T[vector.Length];\\n        for (int i = 0; i < vector.Length; i++)\\n        {\\n            normalized[i] = NumOps.Divide(vector[i], magnitude);\\n        }\\n        return new Vector<T>(normalized);\\n    }\\n    \\n    return vector;\\n}\\n```\\n\\nThen remove these methods from all four derived classes.\\n\\n<details>\\n<summary>≡ƒñû Prompt for AI Agents</summary>\\n\\n```\\nIn src/RetrievalAugmentedGeneration/EmbeddingModels/OpenAIEmbeddingModel.cs\\naround lines 53 to 89 the methods GetDeterministicHash and NormalizeVector are\\nduplicated across multiple embedding classes; move both methods into\\nsrc/RetrievalAugmentedGeneration/Embeddings/EmbeddingModelBase.cs as protected\\ngeneric methods (preserving their logic and using NumOps/Vector<T> generics),\\nthen delete the duplicate implementations from OpenAIEmbeddingModel.cs,\\nONNXSentenceTransformer.cs, HuggingFaceEmbeddingModel.cs, and\\nLocalTransformerEmbedding.cs so all derived classes call the shared protected\\nbase implementations.\\n```\\n\\n</details>\\n\\n<!-- This is an auto-generated comment by CodeRabbit -->\",\"path\":\"src/RetrievalAugmentedGeneration/EmbeddingModels/OpenAIEmbeddingModel.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,91 @@\\n+using AiDotNet.Helpers;\\n+using AiDotNet.LinearAlgebra;\\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\\n+using System;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels\\n+{\\n+    /// <summary>\\n+    /// OpenAI embedding model for generating embeddings via OpenAI API.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class OpenAIEmbeddingModel<T> : EmbeddingModelBase<T>\\n+    {\\n+        private readonly string _apiKey;\\n+        private readonly string _modelName;\\n+        private readonly int _dimension;\\n+        private readonly int _maxTokens;\\n+\\n+        public override int EmbeddingDimension => _dimension;\\n+        public override int MaxTokens => _maxTokens;\\n+\\n+        public OpenAIEmbeddingModel(string apiKey, string modelName = \\\"text-embedding-ada-002\\\", int dimension = 1536, int maxTokens = 8191)\\n+        {\\n+            if (string.IsNullOrWhiteSpace(apiKey))\\n+                throw new ArgumentException(\\\"API key cannot be empty\\\", nameof(apiKey));\\n+            if (string.IsNullOrWhiteSpace(modelName))\\n+                throw new ArgumentException(\\\"Model name cannot be empty\\\", nameof(modelName));\\n+            if (dimension <= 0)\\n+                throw new ArgumentException(\\\"Dimension must be positive\\\", nameof(dimension));\\n+            if (maxTokens <= 0)\\n+                throw new ArgumentException(\\\"Max tokens must be positive\\\", nameof(maxTokens));\\n+\\n+            _apiKey = apiKey;\\n+            _modelName = modelName;\\n+            _dimension = dimension;\\n+            _maxTokens = maxTokens;\\n+        }\\n+\\n+        protected override Vector<T> EmbedCore(string text)\\n+        {\\n+            var values = new T[_dimension];\\n+            var hash = GetDeterministicHash(text);\\n+            \\n+            for (int i = 0; i < _dimension; i++)\\n+            {\\n+                var val = NumOps.FromDouble(Math.Cos(hash * (i + 1) * 0.001));\\n+                values[i] = val;\\n+            }\\n+\\n+            return NormalizeVector(new Vector<T>(values));\\n+        }\\n+\\n+        private int GetDeterministicHash(string text)\\n+        {\\n+            if (string.IsNullOrEmpty(text))\\n+                return 0;\\n+\\n+            unchecked\\n+            {\\n+                int hash = 23;\\n+                foreach (char c in text)\\n+                {\\n+                    hash = hash * 31 + c;\\n+                }\\n+                return hash;\\n+            }\\n+        }\\n+\\n+        private Vector<T> NormalizeVector(Vector<T> vector)\\n+        {\\n+            var magnitude = NumOps.Zero;\\n+            for (int i = 0; i < vector.Length; i++)\\n+            {\\n+                magnitude = NumOps.Add(magnitude, NumOps.Multiply(vector[i], vector[i]));\\n+            }\\n+            magnitude = NumOps.FromDouble(Math.Sqrt(Convert.ToDouble(magnitude)));\\n+\\n+            if (NumOps.GreaterThan(magnitude, NumOps.Zero))\\n+            {\\n+                var normalized = new T[vector.Length];\\n+                for (int i = 0; i < vector.Length; i++)\\n+                {\\n+                    normalized[i] = NumOps.Divide(vector[i], magnitude);\\n+                }\\n+                return new Vector<T>(normalized);\\n+            }\\n+            \\n+            return vector;\\n+        }\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZaUe\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTz2-\",\"body\":\"_≡ƒ¢á∩╕Å Refactor suggestion_ | _≡ƒƒá Major_\\n\\n**Remove redundant field.**\\n\\nThe `_numOps` field duplicates the `NumOps` field inherited from `RerankingStrategyBase<T>`. Remove this field and use the inherited `NumOps` directly.\\n\\n\\n```diff\\n-        private readonly INumericOperations<T> _numOps;\\n         private readonly int _k;\\n```\\n\\nThen update all references from `_numOps` to `NumOps` (lines 25, 47, 51, 55, 96, 100, 104).\\n\\n\\n> Committable suggestion skipped: line range outside the PR's diff.\\n\\n<details>\\n<summary>≡ƒñû Prompt for AI Agents</summary>\\n\\n```\\nIn src/RetrievalAugmentedGeneration/RerankingStrategies/ReciprocalRankFusion.cs\\naround line 15, remove the redundant private field declaration \\\"private readonly\\nINumericOperations<T> _numOps;\\\" and replace all usages of \\\"_numOps\\\" with the\\ninherited \\\"NumOps\\\" (occurrences at lines ~25, 47, 51, 55, 96, 100, 104). Ensure\\nconstructors/initializers no longer assign to _numOps and that only the base\\nclass NumOps is referenced throughout the file.\\n```\\n\\n</details>\\n\\n<!-- This is an auto-generated comment by CodeRabbit -->\\n\\nΓ£à Addressed in commits c0784b0 to a13deec\",\"path\":\"src/RetrievalAugmentedGeneration/RerankingStrategies/ReciprocalRankFusion.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,123 @@\\n+using System;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+using AiDotNet.Helpers;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.RerankingStrategies\\n+{\\n+    /// <summary>\\n+    /// Reciprocal Rank Fusion for combining multiple ranking lists.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class ReciprocalRankFusion<T> : RerankingStrategyBase<T>\\n+    {\\n+        private readonly INumericOperations<T> _numOps;\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZaUn\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTz3M\",\"body\":\"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\\n\\n**Fix generic StatisticsHelper usage to unblock the build**\\n\\n`StatisticsHelper` is a generic type; calling `StatisticsHelper.Log(...)` triggers CS0305 (ΓÇ£requires 1 type argumentΓÇ¥) and breaks the CI build. Use the generic form so the compiler can resolve the overload.  \\n\\n\\nApply this diff:\\n\\n```diff\\n-            return StatisticsHelper.Log(ratio, NumOps);\\n+            return StatisticsHelper<T>.Log(ratio, NumOps);\\n```\\n\\n<!-- suggestion_start -->\\n\\n<details>\\n<summary>≡ƒô¥ Committable suggestion</summary>\\n\\n> ΓÇ╝∩╕Å **IMPORTANT**\\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\\n\\n```suggestion\\n            return StatisticsHelper<T>.Log(ratio, NumOps);\\n        }\\n```\\n\\n</details>\\n\\n<!-- suggestion_end -->\\n\\n<details>\\n<summary>≡ƒº░ Tools</summary>\\n\\n<details>\\n<summary>≡ƒ¬¢ GitHub Actions: Build</summary>\\n\\n[error] 121-121: dotnet build --no-restore --configuration Debug failed: CS0305: Using the generic type 'StatisticsHelper<T>' requires 1 type argument.\\n\\n</details>\\n<details>\\n<summary>≡ƒ¬¢ GitHub Check: Build All Frameworks</summary>\\n\\n[failure] 121-121: \\nUsing the generic type 'StatisticsHelper<T>' requires 1 type arguments\\n\\n---\\n\\n[failure] 121-121: \\nUsing the generic type 'StatisticsHelper<T>' requires 1 type arguments\\n\\n</details>\\n<details>\\n<summary>≡ƒ¬¢ GitHub Check: Publish Size Analysis</summary>\\n\\n[failure] 121-121: \\nUsing the generic type 'StatisticsHelper<T>' requires 1 type arguments\\n\\n</details>\\n\\n</details>\\n\\n<details>\\n<summary>≡ƒñû Prompt for AI Agents</summary>\\n\\n```\\nIn src/RetrievalAugmentedGeneration/Retrievers/BM25Retriever.cs around lines\\n121-122, the call to StatisticsHelper.Log(...) fails because StatisticsHelper is\\ngeneric; replace the non-generic call with the generic form using the actual\\nnumeric type of ratio (e.g., StatisticsHelper<double>.Log(ratio, NumOps)). If\\nratio is a different numeric type, use that type argument instead so the\\ncompiler can resolve the overload.\\n```\\n\\n</details>\\n\\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\\n\\n<!-- This is an auto-generated comment by CodeRabbit -->\",\"path\":\"src/RetrievalAugmentedGeneration/Retrievers/BM25Retriever.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,200 @@\\n+using AiDotNet.Helpers;\\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\\n+{\\n+    /// <summary>\\n+    /// BM25 (Best Matching 25) retrieval algorithm for sparse keyword-based search.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class BM25Retriever<T> : RetrieverBase<T>\\n+    {\\n+        private readonly IDocumentStore<T> _documentStore;\\n+        private readonly T _k1;\\n+        private readonly T _b;\\n+        private readonly Dictionary<string, Dictionary<string, T>> _termFrequencies;\\n+        private readonly Dictionary<string, T> _documentLengths;\\n+        private readonly Dictionary<string, int> _documentFrequencies;\\n+        private int _totalDocuments;\\n+        private T _avgDocLength;\\n+\\n+        public BM25Retriever(IDocumentStore<T> documentStore, int defaultTopK = 5, double k1 = 1.5, double b = 0.75) \\n+            : base(defaultTopK)\\n+        {\\n+            if (documentStore == null)\\n+                throw new ArgumentNullException(nameof(documentStore));\\n+\\n+            _documentStore = documentStore;\\n+            _k1 = NumOps.FromDouble(k1);\\n+            _b = NumOps.FromDouble(b);\\n+            _termFrequencies = new Dictionary<string, Dictionary<string, T>>();\\n+            _documentLengths = new Dictionary<string, T>();\\n+            _documentFrequencies = new Dictionary<string, int>();\\n+            _avgDocLength = NumOps.Zero;\\n+            _totalDocuments = 0;\\n+        }\\n+\\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\\n+        {\\n+            var queryTerms = Tokenize(query);\\n+            var scores = new Dictionary<string, T>();\\n+\\n+            var candidates = _documentStore.GetSimilar(\\n+                new AiDotNet.LinearAlgebra.Vector<T>(new T[Math.Max(1, _documentStore.VectorDimension)]),\\n+                _documentStore.DocumentCount\\n+            );\\n+\\n+            var candidatesList = candidates.ToList();\\n+            BuildCorpusStatistics(candidatesList);\\n+\\n+            foreach (var doc in candidatesList)\\n+            {\\n+                if (!MatchesFilters(doc, metadataFilters))\\n+                    continue;\\n+\\n+                var score = NumOps.Zero;\\n+                \\n+                foreach (var term in queryTerms)\\n+                {\\n+                    var termScore = CalculateBM25Term(doc.Id, term);\\n+                    score = NumOps.Add(score, termScore);\\n+                }\\n+\\n+                scores[doc.Id] = score;\\n+            }\\n+\\n+            var results = scores\\n+                .OrderByDescending(kv => kv.Value)\\n+                .Take(topK)\\n+                .Select(kv =>\\n+                {\\n+                    var doc = candidates.FirstOrDefault(d => d.Id == kv.Key);\\n+                    if (doc != null)\\n+                    {\\n+                        doc.RelevanceScore = kv.Value;\\n+                        doc.HasRelevanceScore = true;\\n+                    }\\n+                    return doc;\\n+                })\\n+                .Where(d => d != null)\\n+                .Cast<Document<T>>();\\n+\\n+            return results;\\n+        }\\n+\\n+        private T CalculateBM25Term(string docId, string term)\\n+        {\\n+            if (!_termFrequencies.ContainsKey(docId) || !_termFrequencies[docId].ContainsKey(term))\\n+                return NumOps.Zero;\\n+\\n+            var tf = _termFrequencies[docId][term];\\n+            var docLength = _documentLengths.ContainsKey(docId) ? _documentLengths[docId] : NumOps.Zero;\\n+\\n+            var idf = CalculateIDF(term);\\n+\\n+            var numerator = NumOps.Multiply(tf, NumOps.Add(_k1, NumOps.One));\\n+            var lengthNorm = NumOps.Add(\\n+                NumOps.Subtract(NumOps.One, _b),\\n+                NumOps.Multiply(_b, NumOps.Divide(docLength, _avgDocLength)));\\n+            var denominator = NumOps.Add(tf, NumOps.Multiply(_k1, lengthNorm));\\n+\\n+            return NumOps.Multiply(idf, NumOps.Divide(numerator, denominator));\\n+        }\\n+\\n+        private T CalculateIDF(string term)\\n+        {\\n+            if (!_documentFrequencies.ContainsKey(term) || _totalDocuments == 0)\\n+                return NumOps.Zero;\\n+\\n+            var df = _documentFrequencies[term];\\n+            if (df == 0)\\n+                return NumOps.Zero;\\n+\\n+            var numerator = NumOps.FromDouble(_totalDocuments - df + 0.5);\\n+            var denominator = NumOps.FromDouble(df + 0.5);\\n+            var ratio = NumOps.Divide(numerator, denominator);\\n+            \\n+            return StatisticsHelper.Log(ratio, NumOps);\\n+        }\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZaUr\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTz3T\",\"body\":\"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\\n\\n**Replace unavailable `NumOps.Compare` call**\\n\\n`INumericOperations<T>` has no `Compare` method, so this line fails compilation (CS1061). Use the existing relational helpers to detect a non-positive average length instead of calling the missing member.  \\n\\n\\nApply this diff:\\n\\n```diff\\n-            if (NumOps.Compare(_avgDocLength, NumOps.Zero) <= 0)\\n+            var isNonPositiveAvg = NumOps.LessThan(_avgDocLength, NumOps.Zero) ||\\n+                                   EqualityComparer<T>.Default.Equals(_avgDocLength, NumOps.Zero);\\n+            if (isNonPositiveAvg)\\n```\\n\\n<!-- suggestion_start -->\\n\\n<details>\\n<summary>≡ƒô¥ Committable suggestion</summary>\\n\\n> ΓÇ╝∩╕Å **IMPORTANT**\\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\\n\\n```suggestion\\n            var isNonPositiveAvg = NumOps.LessThan(_avgDocLength, NumOps.Zero) ||\\n                                   EqualityComparer<T>.Default.Equals(_avgDocLength, NumOps.Zero);\\n            if (isNonPositiveAvg)\\n                _avgDocLength = NumOps.One;\\n```\\n\\n</details>\\n\\n<!-- suggestion_end -->\\n\\n<details>\\n<summary>≡ƒº░ Tools</summary>\\n\\n<details>\\n<summary>≡ƒ¬¢ GitHub Check: Build All Frameworks</summary>\\n\\n[failure] 169-169: \\n'INumericOperations<T>' does not contain a definition for 'Compare' and no accessible extension method 'Compare' accepting a first argument of type 'INumericOperations<T>' could be found (are you missing a using directive or an assembly reference?)\\n\\n---\\n\\n[failure] 169-169: \\n'INumericOperations<T>' does not contain a definition for 'Compare' and no accessible extension method 'Compare' accepting a first argument of type 'INumericOperations<T>' could be found (are you missing a using directive or an assembly reference?)\\n\\n</details>\\n<details>\\n<summary>≡ƒ¬¢ GitHub Check: Publish Size Analysis</summary>\\n\\n[failure] 169-169: \\n'INumericOperations<T>' does not contain a definition for 'Compare' and no accessible extension method 'Compare' accepting a first argument of type 'INumericOperations<T>' could be found (are you missing a using directive or an assembly reference?)\\n\\n</details>\\n\\n</details>\\n\\n<details>\\n<summary>≡ƒñû Prompt for AI Agents</summary>\\n\\n```\\nIn src/RetrievalAugmentedGeneration/Retrievers/BM25Retriever.cs around lines\\n169-170, the code calls a non-existent NumOps.Compare method causing a compile\\nerror; replace that call with the numeric relational helper provided by\\nINumericOperations<T> (e.g. use NumOps.LessThanOrEqual or the library's\\nequivalent) to test for non-positive _avgDocLength and set it to NumOps.One when\\n<= zero.\\n```\\n\\n</details>\\n\\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\\n\\n<!-- This is an auto-generated comment by CodeRabbit -->\\n\\nΓ£à Addressed in commits 1d6eceb to fb037f7\",\"path\":\"src/RetrievalAugmentedGeneration/Retrievers/BM25Retriever.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,200 @@\\n+using AiDotNet.Helpers;\\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\\n+{\\n+    /// <summary>\\n+    /// BM25 (Best Matching 25) retrieval algorithm for sparse keyword-based search.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class BM25Retriever<T> : RetrieverBase<T>\\n+    {\\n+        private readonly IDocumentStore<T> _documentStore;\\n+        private readonly T _k1;\\n+        private readonly T _b;\\n+        private readonly Dictionary<string, Dictionary<string, T>> _termFrequencies;\\n+        private readonly Dictionary<string, T> _documentLengths;\\n+        private readonly Dictionary<string, int> _documentFrequencies;\\n+        private int _totalDocuments;\\n+        private T _avgDocLength;\\n+\\n+        public BM25Retriever(IDocumentStore<T> documentStore, int defaultTopK = 5, double k1 = 1.5, double b = 0.75) \\n+            : base(defaultTopK)\\n+        {\\n+            if (documentStore == null)\\n+                throw new ArgumentNullException(nameof(documentStore));\\n+\\n+            _documentStore = documentStore;\\n+            _k1 = NumOps.FromDouble(k1);\\n+            _b = NumOps.FromDouble(b);\\n+            _termFrequencies = new Dictionary<string, Dictionary<string, T>>();\\n+            _documentLengths = new Dictionary<string, T>();\\n+            _documentFrequencies = new Dictionary<string, int>();\\n+            _avgDocLength = NumOps.Zero;\\n+            _totalDocuments = 0;\\n+        }\\n+\\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\\n+        {\\n+            var queryTerms = Tokenize(query);\\n+            var scores = new Dictionary<string, T>();\\n+\\n+            var candidates = _documentStore.GetSimilar(\\n+                new AiDotNet.LinearAlgebra.Vector<T>(new T[Math.Max(1, _documentStore.VectorDimension)]),\\n+                _documentStore.DocumentCount\\n+            );\\n+\\n+            var candidatesList = candidates.ToList();\\n+            BuildCorpusStatistics(candidatesList);\\n+\\n+            foreach (var doc in candidatesList)\\n+            {\\n+                if (!MatchesFilters(doc, metadataFilters))\\n+                    continue;\\n+\\n+                var score = NumOps.Zero;\\n+                \\n+                foreach (var term in queryTerms)\\n+                {\\n+                    var termScore = CalculateBM25Term(doc.Id, term);\\n+                    score = NumOps.Add(score, termScore);\\n+                }\\n+\\n+                scores[doc.Id] = score;\\n+            }\\n+\\n+            var results = scores\\n+                .OrderByDescending(kv => kv.Value)\\n+                .Take(topK)\\n+                .Select(kv =>\\n+                {\\n+                    var doc = candidates.FirstOrDefault(d => d.Id == kv.Key);\\n+                    if (doc != null)\\n+                    {\\n+                        doc.RelevanceScore = kv.Value;\\n+                        doc.HasRelevanceScore = true;\\n+                    }\\n+                    return doc;\\n+                })\\n+                .Where(d => d != null)\\n+                .Cast<Document<T>>();\\n+\\n+            return results;\\n+        }\\n+\\n+        private T CalculateBM25Term(string docId, string term)\\n+        {\\n+            if (!_termFrequencies.ContainsKey(docId) || !_termFrequencies[docId].ContainsKey(term))\\n+                return NumOps.Zero;\\n+\\n+            var tf = _termFrequencies[docId][term];\\n+            var docLength = _documentLengths.ContainsKey(docId) ? _documentLengths[docId] : NumOps.Zero;\\n+\\n+            var idf = CalculateIDF(term);\\n+\\n+            var numerator = NumOps.Multiply(tf, NumOps.Add(_k1, NumOps.One));\\n+            var lengthNorm = NumOps.Add(\\n+                NumOps.Subtract(NumOps.One, _b),\\n+                NumOps.Multiply(_b, NumOps.Divide(docLength, _avgDocLength)));\\n+            var denominator = NumOps.Add(tf, NumOps.Multiply(_k1, lengthNorm));\\n+\\n+            return NumOps.Multiply(idf, NumOps.Divide(numerator, denominator));\\n+        }\\n+\\n+        private T CalculateIDF(string term)\\n+        {\\n+            if (!_documentFrequencies.ContainsKey(term) || _totalDocuments == 0)\\n+                return NumOps.Zero;\\n+\\n+            var df = _documentFrequencies[term];\\n+            if (df == 0)\\n+                return NumOps.Zero;\\n+\\n+            var numerator = NumOps.FromDouble(_totalDocuments - df + 0.5);\\n+            var denominator = NumOps.FromDouble(df + 0.5);\\n+            var ratio = NumOps.Divide(numerator, denominator);\\n+            \\n+            return StatisticsHelper.Log(ratio, NumOps);\\n+        }\\n+\\n+        private void BuildCorpusStatistics(List<Document<T>> documents)\\n+        {\\n+            if (documents == null || documents.Count == 0)\\n+            {\\n+                _avgDocLength = NumOps.One;\\n+                _totalDocuments = 0;\\n+                return;\\n+            }\\n+\\n+            _termFrequencies.Clear();\\n+            _documentLengths.Clear();\\n+            _documentFrequencies.Clear();\\n+            _totalDocuments = documents.Count;\\n+\\n+            var totalLength = NumOps.Zero;\\n+\\n+            foreach (var doc in documents)\\n+            {\\n+                var terms = Tokenize(doc.Content);\\n+                var termCounts = new Dictionary<string, T>();\\n+                \\n+                foreach (var term in terms)\\n+                {\\n+                    if (termCounts.ContainsKey(term))\\n+                        termCounts[term] = NumOps.Add(termCounts[term], NumOps.One);\\n+                    else\\n+                        termCounts[term] = NumOps.One;\\n+\\n+                    if (!_documentFrequencies.ContainsKey(term))\\n+                        _documentFrequencies[term] = 0;\\n+                }\\n+\\n+                foreach (var term in termCounts.Keys.Distinct())\\n+                {\\n+                    _documentFrequencies[term]++;\\n+                }\\n+\\n+                _termFrequencies[doc.Id] = termCounts;\\n+                var docLength = NumOps.FromDouble(terms.Count);\\n+                _documentLengths[doc.Id] = docLength;\\n+                totalLength = NumOps.Add(totalLength, docLength);\\n+            }\\n+\\n+            _avgDocLength = NumOps.Divide(totalLength, NumOps.FromDouble(_totalDocuments));\\n+            \\n+            if (NumOps.Compare(_avgDocLength, NumOps.Zero) <= 0)\\n+                _avgDocLength = NumOps.One;\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZaUu\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UTz3W\",\"body\":\"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\\n\\n**Guard against empty term frequency lists**\\n\\nWhen a documentΓÇÖs content tokenizes to zero terms, `termCounts.Values.Max()` throws `InvalidOperationException`, breaking retrieval at runtime. Handle the empty case before calling `Max()` by skipping the doc or short-circuiting the TF-IDF computation.  \\n\\n\\nApply this diff:\\n\\n```diff\\n-                var termTfidf = new Dictionary<string, T>();\\n-                var termCounts = docTermFreq[doc.Id];\\n-                var maxFreq = termCounts.Values.Max();\\n+                var termTfidf = new Dictionary<string, T>();\\n+                var termCounts = docTermFreq[doc.Id];\\n+                if (termCounts.Count == 0)\\n+                {\\n+                    _tfidf[doc.Id] = termTfidf;\\n+                    continue;\\n+                }\\n+                var maxFreq = termCounts.Values.Max();\\n```\\n\\n\\n> Committable suggestion skipped: line range outside the PR's diff.\\n\\n<details>\\n<summary>≡ƒñû Prompt for AI Agents</summary>\\n\\n```\\nIn src/RetrievalAugmentedGeneration/Retrievers/TFIDFRetriever.cs around lines\\n140 to 147, guard against calling termCounts.Values.Max() when termCounts is\\nempty: check if termCounts is null or termCounts.Count == 0 (or\\n!termCounts.Values.Any()) and short-circuit (e.g., continue to next document or\\nset termTfidf empty) before calling Max(); otherwise compute maxFreq and TF-IDF\\nas before. Ensure the early-return/continue preserves downstream behavior (no\\nentries added for empty-token docs) and avoids the InvalidOperationException.\\n```\\n\\n</details>\\n\\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\\n\\n<!-- This is an auto-generated comment by CodeRabbit -->\",\"path\":\"src/RetrievalAugmentedGeneration/Retrievers/TFIDFRetriever.cs\",\"position\":140,\"diffHunk\":\"@@ -0,0 +1,170 @@\\n+using AiDotNet.Helpers;\\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+using System;\\n+using System.Collections.Generic;\\n+using System.Linq;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\\n+{\\n+    /// <summary>\\n+    /// TF-IDF (Term Frequency-Inverse Document Frequency) retrieval strategy.\\n+    /// </summary>\\n+    /// <typeparam name=\\\"T\\\">The numeric type for vector operations.</typeparam>\\n+    public class TFIDFRetriever<T> : RetrieverBase<T>\\n+    {\\n+        private readonly IDocumentStore<T> _documentStore;\\n+        private readonly Dictionary<string, Dictionary<string, T>> _tfidf;\\n+        private readonly Dictionary<string, T> _idf;\\n+\\n+        public TFIDFRetriever(IDocumentStore<T> documentStore, int defaultTopK = 5) : base(defaultTopK)\\n+        {\\n+            if (documentStore == null)\\n+                throw new ArgumentNullException(nameof(documentStore));\\n+                \\n+            _documentStore = documentStore;\\n+            _tfidf = new Dictionary<string, Dictionary<string, T>>();\\n+            _idf = new Dictionary<string, T>();\\n+        }\\n+\\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\\n+        {\\n+            var queryTerms = Tokenize(query);\\n+            var scores = new Dictionary<string, T>();\\n+\\n+            var candidates = _documentStore.GetSimilar(\\n+                new AiDotNet.LinearAlgebra.Vector<T>(new T[_documentStore.VectorDimension]), \\n+                _documentStore.DocumentCount\\n+            );\\n+\\n+            var candidatesList = candidates.ToList();\\n+            BuildTFIDFStatistics(candidatesList);\\n+\\n+            foreach (var doc in candidatesList)\\n+            {\\n+                if (!MatchesFilters(doc, metadataFilters))\\n+                    continue;\\n+\\n+                var score = NumOps.Zero;\\n+\\n+                if (_tfidf.ContainsKey(doc.Id))\\n+                {\\n+                    var docTfidf = _tfidf[doc.Id];\\n+                    foreach (var term in queryTerms)\\n+                    {\\n+                        if (docTfidf.ContainsKey(term))\\n+                        {\\n+                            score = NumOps.Add(score, docTfidf[term]);\\n+                        }\\n+                    }\\n+                }\\n+\\n+                scores[doc.Id] = score;\\n+            }\\n+\\n+            var results = scores\\n+                .OrderByDescending(kv => kv.Value)\\n+                .Take(topK)\\n+                .Select(kv =>\\n+                {\\n+                    var doc = candidates.FirstOrDefault(d => d.Id == kv.Key);\\n+                    if (doc != null)\\n+                    {\\n+                        doc.RelevanceScore = kv.Value;\\n+                        doc.HasRelevanceScore = true;\\n+                    }\\n+                    return doc;\\n+                })\\n+                .Where(d => d != null)\\n+                .Cast<Document<T>>();\\n+\\n+            return results;\\n+        }\\n+\\n+        private List<string> Tokenize(string text)\\n+        {\\n+            if (string.IsNullOrWhiteSpace(text))\\n+                return new List<string>();\\n+\\n+            return text.ToLowerInvariant()\\n+                .Split(new[] { ' ', '\\\\t', '\\\\n', '\\\\r', ',', '.', '!', '?' }, StringSplitOptions.RemoveEmptyEntries)\\n+                .ToList();\\n+        }\\n+\\n+        private void BuildTFIDFStatistics(List<Document<T>> documents)\\n+        {\\n+            if (documents == null || documents.Count == 0)\\n+                return;\\n+\\n+            _tfidf.Clear();\\n+            _idf.Clear();\\n+\\n+            var termDocFreq = new Dictionary<string, int>();\\n+            var docTermFreq = new Dictionary<string, Dictionary<string, int>>();\\n+\\n+            foreach (var doc in documents)\\n+            {\\n+                var terms = Tokenize(doc.Content);\\n+                var termCounts = new Dictionary<string, int>();\\n+                \\n+                foreach (var term in terms)\\n+                {\\n+                    if (termCounts.ContainsKey(term))\\n+                        termCounts[term]++;\\n+                    else\\n+                        termCounts[term] = 1;\\n+                }\\n+\\n+                docTermFreq[doc.Id] = termCounts;\\n+\\n+                foreach (var term in termCounts.Keys)\\n+                {\\n+                    if (termDocFreq.ContainsKey(term))\\n+                        termDocFreq[term]++;\\n+                    else\\n+                        termDocFreq[term] = 1;\\n+                }\\n+            }\\n+\\n+            foreach (var term in termDocFreq.Keys)\\n+            {\\n+                var df = termDocFreq[term];\\n+                var idf = NumOps.FromDouble(Math.Log((double)documents.Count / (double)df));\\n+                _idf[term] = idf;\\n+            }\\n+\\n+            foreach (var doc in documents)\\n+            {\\n+                var termTfidf = new Dictionary<string, T>();\\n+                var termCounts = docTermFreq[doc.Id];\\n+                var maxFreq = termCounts.Values.Max();\\n+\\n+                foreach (var termCount in termCounts)\\n+                {\\n+                    var tf = NumOps.FromDouble((double)termCount.Value / (double)maxFreq);\\n+                    var tfidf = NumOps.Multiply(tf, _idf[termCount.Key]);\\n+                    termTfidf[termCount.Key] = tfidf;\\n+                }\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZg2_\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UT9qP\",\"body\":\"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\\n\\n**Finish chain-of-thought retrieval logic.**\\n\\n`Retrieve` currently throws `NotImplementedException`, so this ΓÇ£concreteΓÇ¥ retriever cannot be used without crashing. Please deliver the reasoning+retrieval workflow outlined in the TODO (or defer by making the type non-public) before shipping.\\n\\n<details>\\n<summary>≡ƒñû Prompt for AI Agents</summary>\\n\\n```\\nIn src/RetrievalAugmentedGeneration/AdvancedPatterns/ChainOfThoughtRetriever.cs\\naround lines 44 to 59, Retrieve currently throws NotImplementedException and\\nmust implement the chain-of-thought workflow described in the TODO. Replace the\\nthrow with an implementation that: 1) validates inputs (already present), 2)\\ncalls the configured LLM client to prompt for step-by-step reasoning for the\\nquery and parse the response into a list of concepts/sub-questions, 3) for each\\nconcept call the existing base retriever to fetch documents, 4) merge results\\ninto a single list, deduplicate by document ID or content fingerprint,\\nscore/rank them (e.g., by aggregate relevance), and 5) return the topK\\ndocuments; ensure LLM errors are caught and surfaced as meaningful exceptions\\nand make the method unit-testable by extracting LLM and retriever calls into\\ninjectable interfaces if not already.\\n```\\n\\n</details>\\n\\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\\n\\n<!-- This is an auto-generated comment by CodeRabbit -->\",\"path\":\"src/RetrievalAugmentedGeneration/AdvancedPatterns/ChainOfThoughtRetriever.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,60 @@\\n+using AiDotNet.Interfaces;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+using AiDotNet.RetrievalAugmentedGeneration.Retrievers;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.AdvancedPatterns;\\n+\\n+/// <summary>\\n+/// Chain-of-Thought retriever that generates reasoning steps before retrieving documents.\\n+/// </summary>\\n+/// <typeparam name=\\\"T\\\">The numeric data type used for calculations.</typeparam>\\n+/// <remarks>\\n+/// Generates a chain of thought reasoning process based on the query, then uses\\n+/// that reasoning to inform retrieval, leading to more targeted and relevant results.\\n+/// </remarks>\\n+public class ChainOfThoughtRetriever<T>\\n+{\\n+    private readonly INumericOperations<T> _numericOperations;\\n+    private readonly string _llmEndpoint;\\n+    private readonly string _llmApiKey;\\n+    private readonly RetrieverBase<T> _baseRetriever;\\n+\\n+    /// <summary>\\n+    /// Initializes a new instance of the <see cref=\\\"ChainOfThoughtRetriever{T}\\\"/> class.\\n+    /// </summary>\\n+    /// <param name=\\\"llmEndpoint\\\">The LLM API endpoint.</param>\\n+    /// <param name=\\\"llmApiKey\\\">The API key for the LLM service.</param>\\n+    /// <param name=\\\"baseRetriever\\\">The underlying retriever to use.</param>\\n+    /// <param name=\\\"numericOperations\\\">The numeric operations provider.</param>\\n+    public ChainOfThoughtRetriever(\\n+        string llmEndpoint,\\n+        string llmApiKey,\\n+        RetrieverBase<T> baseRetriever,\\n+        INumericOperations<T> numericOperations)\\n+    {\\n+        _llmEndpoint = llmEndpoint ?? throw new ArgumentNullException(nameof(llmEndpoint));\\n+        _llmApiKey = llmApiKey ?? throw new ArgumentNullException(nameof(llmApiKey));\\n+        _baseRetriever = baseRetriever ?? throw new ArgumentNullException(nameof(baseRetriever));\\n+        _numericOperations = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\\n+    }\\n+\\n+    /// <summary>\\n+    /// Retrieves documents using chain-of-thought reasoning.\\n+    /// </summary>\\n+    public IEnumerable<Document<T>> Retrieve(string query, int topK)\\n+    {\\n+        if (string.IsNullOrWhiteSpace(query))\\n+            throw new ArgumentException(\\\"Query cannot be null or whitespace\\\", nameof(query));\\n+\\n+        if (topK <= 0)\\n+            throw new ArgumentOutOfRangeException(nameof(topK), \\\"topK must be positive\\\");\\n+\\n+        // TODO: Implement chain-of-thought retrieval\\n+        // 1. Send query to LLM to generate reasoning steps\\n+        // 2. Extract key concepts and sub-questions from reasoning\\n+        // 3. Use base retriever to fetch documents for each concept\\n+        // 4. Combine and deduplicate results\\n+        // 5. Return top-K documents\\n+        throw new NotImplementedException(\\\"Chain-of-thought retrieval requires LLM integration\\\");\\n+    }\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZg3D\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UT9qX\",\"body\":\"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\\n\\n**Implement FLARE retrieval instead of throwing.**\\n\\n`GenerateWithActiveRetrieval` always throws `NotImplementedException`, so any caller of this supposedly concrete retriever will fail at runtime. Please either ship the actual FLARE control loop (confidence monitoring, dynamic retrieval, final answer) or keep this type internal/abstract until the behavior is complete.\\n\\n<details>\\n<summary>≡ƒñû Prompt for AI Agents</summary>\\n\\n```\\nIn src/RetrievalAugmentedGeneration/AdvancedPatterns/FLARERetriever.cs around\\nlines 48ΓÇô63, the GenerateWithActiveRetrieval method currently throws\\nNotImplementedException; replace this with either a real FLARE control loop or\\nmake the type/method abstract/internal. To implement FLARE: validate query,\\ninitialize or accept an LLM streaming client, start token-by-token generation\\nwhile collecting per-token/confidence scores, detect confidence drop below a\\nconfigurable threshold, then pause generation, call the existing retrieval\\ncomponent to fetch relevant documents, augment the prompt/context with retrieved\\ncontent, resume generation (repeat until EOS or max length), and finally return\\nthe composed answer; remove the NotImplementedException and add configuration\\nfor confidence threshold, max retrievals, and unit/integration tests.\\nAlternatively, if FLARE isnΓÇÖt ready, change the method to abstract or mark the\\nclass internal and update callers to avoid runtime failures.\\n```\\n\\n</details>\\n\\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\\n\\n<!-- This is an auto-generated comment by CodeRabbit -->\\n\\nΓ£à Addressed in commits 71fdd3b to 0990756\",\"path\":\"src/RetrievalAugmentedGeneration/AdvancedPatterns/FLARERetriever.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,64 @@\\n+using AiDotNet.Interfaces;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+using AiDotNet.RetrievalAugmentedGeneration.Retrievers;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.AdvancedPatterns;\\n+\\n+/// <summary>\\n+/// FLARE (Forward-Looking Active REtrieval) pattern that actively decides when and what to retrieve.\\n+/// </summary>\\n+/// <typeparam name=\\\"T\\\">The numeric data type used for calculations.</typeparam>\\n+/// <remarks>\\n+/// During generation, FLARE monitors the model's confidence and actively retrieves additional\\n+/// information when uncertainty is detected, enabling dynamic and adaptive retrieval.\\n+/// </remarks>\\n+public class FLARERetriever<T>\\n+{\\n+    private readonly INumericOperations<T> _numericOperations;\\n+    private readonly string _llmEndpoint;\\n+    private readonly string _llmApiKey;\\n+    private readonly RetrieverBase<T> _baseRetriever;\\n+    private readonly T _uncertaintyThreshold;\\n+\\n+    /// <summary>\\n+    /// Initializes a new instance of the <see cref=\\\"FLARERetriever{T}\\\"/> class.\\n+    /// </summary>\\n+    /// <param name=\\\"llmEndpoint\\\">The LLM API endpoint.</param>\\n+    /// <param name=\\\"llmApiKey\\\">The API key for the LLM service.</param>\\n+    /// <param name=\\\"baseRetriever\\\">The underlying retriever to use.</param>\\n+    /// <param name=\\\"uncertaintyThreshold\\\">Threshold for triggering retrieval.</param>\\n+    /// <param name=\\\"numericOperations\\\">The numeric operations provider.</param>\\n+    public FLARERetriever(\\n+        string llmEndpoint,\\n+        string llmApiKey,\\n+        RetrieverBase<T> baseRetriever,\\n+        T uncertaintyThreshold,\\n+        INumericOperations<T> numericOperations)\\n+    {\\n+        _llmEndpoint = llmEndpoint ?? throw new ArgumentNullException(nameof(llmEndpoint));\\n+        _llmApiKey = llmApiKey ?? throw new ArgumentNullException(nameof(llmApiKey));\\n+        _baseRetriever = baseRetriever ?? throw new ArgumentNullException(nameof(baseRetriever));\\n+        _uncertaintyThreshold = uncertaintyThreshold;\\n+        _numericOperations = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\\n+    }\\n+\\n+    /// <summary>\\n+    /// Generates answer with active retrieval based on uncertainty.\\n+    /// </summary>\\n+    public string GenerateWithActiveRetrieval(string query)\\n+    {\\n+        if (string.IsNullOrWhiteSpace(query))\\n+            throw new ArgumentException(\\\"Query cannot be null or whitespace\\\", nameof(query));\\n+\\n+        // TODO: Implement FLARE\\n+        // 1. Start generating answer\\n+        // 2. Monitor token-level confidence\\n+        // 3. When confidence drops below threshold:\\n+        //    a. Identify what information is needed\\n+        //    b. Retrieve relevant documents\\n+        //    c. Continue generation with new context\\n+        // 4. Repeat until answer is complete\\n+        // 5. Return final answer\\n+        throw new NotImplementedException(\\\"FLARE requires LLM integration with confidence monitoring\\\");\\n+    }\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZg3N\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UT9qh\",\"body\":\"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\\n\\n**Use consistent string validation in constructor.**\\n\\nThe constructor validates `llmEndpoint` and `llmApiKey` with null checks only (lines 38-39), but `RetrieveAndAnswer` uses `IsNullOrWhiteSpace` for the `query` parameter (line 54). Empty or whitespace-only strings would pass constructor validation but likely cause issues when used with the LLM API.\\n\\nApply this diff to use consistent validation:\\n\\n```diff\\n-        _llmEndpoint = llmEndpoint ?? throw new ArgumentNullException(nameof(llmEndpoint));\\n-        _llmApiKey = llmApiKey ?? throw new ArgumentNullException(nameof(llmApiKey));\\n+        if (string.IsNullOrWhiteSpace(llmEndpoint))\\n+            throw new ArgumentException(\\\"LLM endpoint cannot be null or whitespace\\\", nameof(llmEndpoint));\\n+        if (string.IsNullOrWhiteSpace(llmApiKey))\\n+            throw new ArgumentException(\\\"API key cannot be null or whitespace\\\", nameof(llmApiKey));\\n+            \\n+        _llmEndpoint = llmEndpoint;\\n+        _llmApiKey = llmApiKey;\\n         _baseRetriever = baseRetriever ?? throw new ArgumentNullException(nameof(baseRetriever));\\n```\\n\\n<!-- suggestion_start -->\\n\\n<details>\\n<summary>≡ƒô¥ Committable suggestion</summary>\\n\\n> ΓÇ╝∩╕Å **IMPORTANT**\\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\\n\\n```suggestion\\n    public SelfCorrectingRetriever(\\n        string llmEndpoint,\\n        string llmApiKey,\\n        RetrieverBase<T> baseRetriever,\\n        int maxIterations,\\n        INumericOperations<T> numericOperations)\\n    {\\n        if (string.IsNullOrWhiteSpace(llmEndpoint))\\n            throw new ArgumentException(\\\"LLM endpoint cannot be null or whitespace\\\", nameof(llmEndpoint));\\n        if (string.IsNullOrWhiteSpace(llmApiKey))\\n            throw new ArgumentException(\\\"API key cannot be null or whitespace\\\", nameof(llmApiKey));\\n            \\n        _llmEndpoint = llmEndpoint;\\n        _llmApiKey = llmApiKey;\\n        _baseRetriever = baseRetriever ?? throw new ArgumentNullException(nameof(baseRetriever));\\n        \\n        if (maxIterations <= 0)\\n            throw new ArgumentOutOfRangeException(nameof(maxIterations), \\\"Max iterations must be positive\\\");\\n            \\n        _maxIterations = maxIterations;\\n        _numericOperations = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\\n    }\\n```\\n\\n</details>\\n\\n<!-- suggestion_end -->\\n\\n<details>\\n<summary>≡ƒñû Prompt for AI Agents</summary>\\n\\n```\\nIn src/RetrievalAugmentedGeneration/AdvancedPatterns/SelfCorrectingRetriever.cs\\naround lines 31 to 47, the constructor currently only performs null checks for\\nllmEndpoint and llmApiKey but other code uses IsNullOrWhiteSpace for inputs;\\nupdate the constructor to validate llmEndpoint and llmApiKey with\\nstring.IsNullOrWhiteSpace and throw an ArgumentException (or\\nArgumentNullException with a clear message) when they are null/empty/whitespace,\\nleaving the null check for baseRetriever and numericOperations as-is and keeping\\nthe maxIterations range check unchanged.\\n```\\n\\n</details>\\n\\n<!-- This is an auto-generated comment by CodeRabbit -->\",\"path\":\"src/RetrievalAugmentedGeneration/AdvancedPatterns/SelfCorrectingRetriever.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,72 @@\\n+using AiDotNet.Interfaces;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+using AiDotNet.RetrievalAugmentedGeneration.Retrievers;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.AdvancedPatterns;\\n+\\n+/// <summary>\\n+/// Self-correcting retriever that iteratively refines answers through critique and re-retrieval.\\n+/// </summary>\\n+/// <typeparam name=\\\"T\\\">The numeric data type used for calculations.</typeparam>\\n+/// <remarks>\\n+/// Retrieves documents, generates an answer, critiques the answer, and retrieves additional\\n+/// documents to address gaps or errors, repeating until a satisfactory answer is achieved.\\n+/// </remarks>\\n+public class SelfCorrectingRetriever<T>\\n+{\\n+    private readonly INumericOperations<T> _numericOperations;\\n+    private readonly string _llmEndpoint;\\n+    private readonly string _llmApiKey;\\n+    private readonly RetrieverBase<T> _baseRetriever;\\n+    private readonly int _maxIterations;\\n+\\n+    /// <summary>\\n+    /// Initializes a new instance of the <see cref=\\\"SelfCorrectingRetriever{T}\\\"/> class.\\n+    /// </summary>\\n+    /// <param name=\\\"llmEndpoint\\\">The LLM API endpoint.</param>\\n+    /// <param name=\\\"llmApiKey\\\">The API key for the LLM service.</param>\\n+    /// <param name=\\\"baseRetriever\\\">The underlying retriever to use.</param>\\n+    /// <param name=\\\"maxIterations\\\">Maximum number of correction iterations.</param>\\n+    /// <param name=\\\"numericOperations\\\">The numeric operations provider.</param>\\n+    public SelfCorrectingRetriever(\\n+        string llmEndpoint,\\n+        string llmApiKey,\\n+        RetrieverBase<T> baseRetriever,\\n+        int maxIterations,\\n+        INumericOperations<T> numericOperations)\\n+    {\\n+        _llmEndpoint = llmEndpoint ?? throw new ArgumentNullException(nameof(llmEndpoint));\\n+        _llmApiKey = llmApiKey ?? throw new ArgumentNullException(nameof(llmApiKey));\\n+        _baseRetriever = baseRetriever ?? throw new ArgumentNullException(nameof(baseRetriever));\\n+        \\n+        if (maxIterations <= 0)\\n+            throw new ArgumentOutOfRangeException(nameof(maxIterations), \\\"Max iterations must be positive\\\");\\n+            \\n+        _maxIterations = maxIterations;\\n+        _numericOperations = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\\n+    }\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZg3U\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UT9qq\",\"body\":\"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\\n\\n**Security posture gap: API key stored as plain string without secure cleanup.**\\n\\nStoring the API key as a plain string in memory poses a security risk, as it remains readable until garbage collected and cannot be reliably zeroed. Consider implementing `IDisposable` to securely clear sensitive fields, or use a credential management approach that avoids storing secrets directly in application memory.\\n\\n\\n\\nExample secure disposal pattern:\\n\\n```diff\\n-public class AgenticChunker<T> : ChunkingStrategyBase<T>\\n+public class AgenticChunker<T> : ChunkingStrategyBase<T>, IDisposable\\n {\\n     private readonly string _llmEndpoint;\\n-    private readonly string _llmApiKey;\\n+    private string? _llmApiKey;\\n     private readonly int _maxChunkSize;\\n+    private bool _disposed;\\n+\\n+    public void Dispose()\\n+    {\\n+        if (!_disposed)\\n+        {\\n+            _llmApiKey = null; // Allow GC; consider SecureString for true zeroing\\n+            _disposed = true;\\n+        }\\n+    }\\n```\\n\\n\\nAlso applies to: 27-41\\n\\n<!-- This is an auto-generated comment by CodeRabbit -->\",\"path\":\"src/RetrievalAugmentedGeneration/ChunkingStrategies/AgenticChunker.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,57 @@\\n+using AiDotNet.Interfaces;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies;\\n+\\n+/// <summary>\\n+/// LLM-powered agentic chunker that decides where to split text based on content meaning.\\n+/// </summary>\\n+/// <typeparam name=\\\"T\\\">The numeric data type used for calculations.</typeparam>\\n+/// <remarks>\\n+/// Uses an LLM to analyze text and determine optimal split points based on semantic boundaries,\\n+/// topic changes, and natural breaks in the content flow.\\n+/// </remarks>\\n+public class AgenticChunker<T> : ChunkingStrategyBase<T>\\n+{\\n+    private readonly string _llmEndpoint;\\n+    private readonly string _llmApiKey;\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZg3Y\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UT9qv\",\"body\":\"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\\n\\n**Add validation to ensure `minChunkSize` does not exceed `maxChunkSize`.**\\n\\nThe constructor validates each parameter individually but doesn't verify their relationship. If `minChunkSize > maxChunkSize`, it creates an impossible constraint where no chunk can satisfy both conditions, leading to unpredictable behavior.\\n\\n\\n\\nApply this diff to add the validation:\\n\\n```diff\\n     if (minChunkSize < 0)\\n         throw new ArgumentOutOfRangeException(nameof(minChunkSize), \\\"Min chunk size cannot be negative\\\");\\n+        \\n+    if (minChunkSize > maxChunkSize)\\n+        throw new ArgumentException($\\\"Min chunk size ({minChunkSize}) cannot exceed max chunk size ({maxChunkSize})\\\");\\n         \\n     _maxChunkSize = maxChunkSize;\\n```\\n\\n<!-- suggestion_start -->\\n\\n<details>\\n<summary>≡ƒô¥ Committable suggestion</summary>\\n\\n> ΓÇ╝∩╕Å **IMPORTANT**\\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\\n\\n```suggestion\\n    public HeaderBasedTextSplitter(\\n        int maxChunkSize,\\n        int minChunkSize,\\n        bool combineSmallChunks,\\n        INumericOperations<T> numericOperations)\\n        : base(numericOperations)\\n    {\\n        if (maxChunkSize <= 0)\\n            throw new ArgumentOutOfRangeException(nameof(maxChunkSize), \\\"Max chunk size must be positive\\\");\\n            \\n        if (minChunkSize < 0)\\n            throw new ArgumentOutOfRangeException(nameof(minChunkSize), \\\"Min chunk size cannot be negative\\\");\\n            \\n        if (minChunkSize > maxChunkSize)\\n            throw new ArgumentException($\\\"Min chunk size ({minChunkSize}) cannot exceed max chunk size ({maxChunkSize})\\\");\\n            \\n        _maxChunkSize = maxChunkSize;\\n        _minChunkSize = minChunkSize;\\n        _combineSmallChunks = combineSmallChunks;\\n    }\\n```\\n\\n</details>\\n\\n<!-- suggestion_end -->\\n\\n<details>\\n<summary>≡ƒñû Prompt for AI Agents</summary>\\n\\n```\\nIn\\nsrc/RetrievalAugmentedGeneration/ChunkingStrategies/HeaderBasedTextSplitter.cs\\naround lines 27 to 43, the constructor validates maxChunkSize and minChunkSize\\nindependently but doesn't check their relationship; add a check after the\\nindividual validations that throws an ArgumentException (or\\nArgumentOutOfRangeException) when minChunkSize > maxChunkSize with a clear\\nmessage (e.g., \\\"minChunkSize cannot be greater than maxChunkSize\\\") so callers\\nget an explicit error; ensure the new validation runs before assigning fields.\\n```\\n\\n</details>\\n\\n<!-- This is an auto-generated comment by CodeRabbit -->\",\"path\":\"src/RetrievalAugmentedGeneration/ChunkingStrategies/HeaderBasedTextSplitter.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,146 @@\\n+using AiDotNet.Interfaces;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies;\\n+\\n+/// <summary>\\n+/// Splits structured documents based on header tags (H1, H2, H3, etc.).\\n+/// </summary>\\n+/// <typeparam name=\\\"T\\\">The numeric data type used for calculations.</typeparam>\\n+/// <remarks>\\n+/// Ideal for Markdown and HTML documents where headers provide natural semantic boundaries.\\n+/// Preserves document structure and hierarchy.\\n+/// </remarks>\\n+public class HeaderBasedTextSplitter<T> : ChunkingStrategyBase<T>\\n+{\\n+    private readonly int _maxChunkSize;\\n+    private readonly bool _combineSmallChunks;\\n+    private readonly int _minChunkSize;\\n+\\n+    /// <summary>\\n+    /// Initializes a new instance of the <see cref=\\\"HeaderBasedTextSplitter{T}\\\"/> class.\\n+    /// </summary>\\n+    /// <param name=\\\"maxChunkSize\\\">Maximum size of each chunk in characters.</param>\\n+    /// <param name=\\\"minChunkSize\\\">Minimum size for chunk combination.</param>\\n+    /// <param name=\\\"combineSmallChunks\\\">Whether to combine small chunks.</param>\\n+    /// <param name=\\\"numericOperations\\\">The numeric operations provider.</param>\\n+    public HeaderBasedTextSplitter(\\n+        int maxChunkSize,\\n+        int minChunkSize,\\n+        bool combineSmallChunks,\\n+        INumericOperations<T> numericOperations)\\n+        : base(numericOperations)\\n+    {\\n+        if (maxChunkSize <= 0)\\n+            throw new ArgumentOutOfRangeException(nameof(maxChunkSize), \\\"Max chunk size must be positive\\\");\\n+            \\n+        if (minChunkSize < 0)\\n+            throw new ArgumentOutOfRangeException(nameof(minChunkSize), \\\"Min chunk size cannot be negative\\\");\\n+            \\n+        _maxChunkSize = maxChunkSize;\\n+        _minChunkSize = minChunkSize;\\n+        _combineSmallChunks = combineSmallChunks;\\n+    }\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZg3b\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UT9qz\",\"body\":\"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\\n\\n**Critical: Small chunks lose their header context when combined.**\\n\\nWhen `combineSmallChunks` is true and a chunk is smaller than `minChunkSize`, the chunk is neither saved nor cleared (line 67-80 condition is false, so the inner block doesn't execute). The code then updates `currentHeader` to the new header (line 83) and adds the new header line to the existing `currentChunk` (line 84). This means content from the previous section gets merged with the next section under the wrong header, violating semantic boundaries.\\n\\nFor example:\\n```\\n## Section A\\nSmall content here\\n## Section B\\nMore content\\n```\\n\\nIf \\\"Small content here\\\" is below `minChunkSize`, it will be merged with Section B content but labeled with \\\"## Section B\\\" header in metadata, losing its association with Section A.\\n\\n\\n\\nApply this diff to fix the semantic boundary issue:\\n\\n```diff\\n             // Check if line is a header (Markdown ## or HTML <h>)\\n             if (IsHeader(line))\\n             {\\n                 // Save current chunk if not empty\\n                 if (currentChunk.Count > 0)\\n                 {\\n                     var content = string.Join(Environment.NewLine, currentChunk);\\n-                    if (content.Length >= _minChunkSize || !_combineSmallChunks)\\n+                    \\n+                    // Always save chunks at header boundaries to preserve semantic structure\\n+                    // Small chunks can be combined in a post-processing step if needed\\n+                    chunks.Add(new Document<T>\\n                     {\\n-                        chunks.Add(new Document<T>\\n-                        {\\n-                            Id = Guid.NewGuid().ToString(),\\n-                            Content = content,\\n-                            Metadata = new Dictionary<string, object>\\n-                            {\\n-                                [\\\"header\\\"] = currentHeader,\\n-                                [\\\"chunkIndex\\\"] = chunks.Count\\n-                            }\\n-                        });\\n-                        currentChunk.Clear();\\n-                    }\\n+                        Id = Guid.NewGuid().ToString(),\\n+                        Content = content,\\n+                        Metadata = new Dictionary<string, object>\\n+                        {\\n+                            [\\\"header\\\"] = currentHeader,\\n+                            [\\\"chunkIndex\\\"] = chunks.Count,\\n+                            [\\\"isBelowMinSize\\\"] = content.Length < _minChunkSize\\n+                        }\\n+                    });\\n+                    currentChunk.Clear();\\n                 }\\n\\n                 currentHeader = line.Trim();\\n```\\n\\nNote: If chunk combination is still desired, implement it as a separate post-processing step that can merge adjacent chunks while preserving metadata about their original structure.\\n\\n<details>\\n<summary>≡ƒñû Prompt for AI Agents</summary>\\n\\n```\\nIn\\nsrc/RetrievalAugmentedGeneration/ChunkingStrategies/HeaderBasedTextSplitter.cs\\naround lines 64 to 84, when encountering a new header the code currently skips\\ncreating a chunk if it's below _minChunkSize and _combineSmallChunks is true,\\nthen replaces currentHeader and appends the new header to the same currentChunk\\nwhich loses the original header metadata; always flush the existing currentChunk\\nas a chunk tied to currentHeader before updating currentHeader (i.e., create and\\nadd a Document with the currentHeader and content even if itΓÇÖs smaller than\\n_minChunkSize), clear currentChunk, then set currentHeader = line.Trim() and add\\nthe new header line; if you still want to combine small adjacent chunks,\\nimplement that as an independent post-processing pass that merges neighboring\\nchunks while preserving an array/list of original headers/indices in metadata\\nrather than overwriting header information.\\n```\\n\\n</details>\\n\\n<!-- This is an auto-generated comment by CodeRabbit -->\\n\\nΓ£à Addressed in commits 15063ed to 8468360\",\"path\":\"src/RetrievalAugmentedGeneration/ChunkingStrategies/HeaderBasedTextSplitter.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,146 @@\\n+using AiDotNet.Interfaces;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies;\\n+\\n+/// <summary>\\n+/// Splits structured documents based on header tags (H1, H2, H3, etc.).\\n+/// </summary>\\n+/// <typeparam name=\\\"T\\\">The numeric data type used for calculations.</typeparam>\\n+/// <remarks>\\n+/// Ideal for Markdown and HTML documents where headers provide natural semantic boundaries.\\n+/// Preserves document structure and hierarchy.\\n+/// </remarks>\\n+public class HeaderBasedTextSplitter<T> : ChunkingStrategyBase<T>\\n+{\\n+    private readonly int _maxChunkSize;\\n+    private readonly bool _combineSmallChunks;\\n+    private readonly int _minChunkSize;\\n+\\n+    /// <summary>\\n+    /// Initializes a new instance of the <see cref=\\\"HeaderBasedTextSplitter{T}\\\"/> class.\\n+    /// </summary>\\n+    /// <param name=\\\"maxChunkSize\\\">Maximum size of each chunk in characters.</param>\\n+    /// <param name=\\\"minChunkSize\\\">Minimum size for chunk combination.</param>\\n+    /// <param name=\\\"combineSmallChunks\\\">Whether to combine small chunks.</param>\\n+    /// <param name=\\\"numericOperations\\\">The numeric operations provider.</param>\\n+    public HeaderBasedTextSplitter(\\n+        int maxChunkSize,\\n+        int minChunkSize,\\n+        bool combineSmallChunks,\\n+        INumericOperations<T> numericOperations)\\n+        : base(numericOperations)\\n+    {\\n+        if (maxChunkSize <= 0)\\n+            throw new ArgumentOutOfRangeException(nameof(maxChunkSize), \\\"Max chunk size must be positive\\\");\\n+            \\n+        if (minChunkSize < 0)\\n+            throw new ArgumentOutOfRangeException(nameof(minChunkSize), \\\"Min chunk size cannot be negative\\\");\\n+            \\n+        _maxChunkSize = maxChunkSize;\\n+        _minChunkSize = minChunkSize;\\n+        _combineSmallChunks = combineSmallChunks;\\n+    }\\n+\\n+    /// <summary>\\n+    /// Splits text based on header hierarchy.\\n+    /// </summary>\\n+    public override IEnumerable<Document<T>> Chunk(string text)\\n+    {\\n+        if (string.IsNullOrWhiteSpace(text))\\n+            return Enumerable.Empty<Document<T>>();\\n+\\n+        var chunks = new List<Document<T>>();\\n+        var lines = text.Split(new[] { \\\"\\\\r\\\\n\\\", \\\"\\\\r\\\", \\\"\\\\n\\\" }, StringSplitOptions.None);\\n+        var currentChunk = new List<string>();\\n+        var currentHeader = string.Empty;\\n+\\n+        foreach (var line in lines)\\n+        {\\n+            // Check if line is a header (Markdown ## or HTML <h>)\\n+            if (IsHeader(line))\\n+            {\\n+                // Save current chunk if not empty\\n+                if (currentChunk.Count > 0)\\n+                {\\n+                    var content = string.Join(Environment.NewLine, currentChunk);\\n+                    if (content.Length >= _minChunkSize || !_combineSmallChunks)\\n+                    {\\n+                        chunks.Add(new Document<T>\\n+                        {\\n+                            Id = Guid.NewGuid().ToString(),\\n+                            Content = content,\\n+                            Metadata = new Dictionary<string, object>\\n+                            {\\n+                                [\\\"header\\\"] = currentHeader,\\n+                                [\\\"chunkIndex\\\"] = chunks.Count\\n+                            }\\n+                        });\\n+                        currentChunk.Clear();\\n+                    }\\n+                }\\n+\\n+                currentHeader = line.Trim();\\n+                currentChunk.Add(line);\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZg3f\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UT9q5\",\"body\":\"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\\n\\n**Final chunk handling is inconsistent with header boundary logic.**\\n\\nThe final chunk is always saved regardless of size (line 110-122), while chunks at header boundaries are conditionally saved based on `minChunkSize` (line 67). This creates inconsistent behavior where a small final chunk is kept but a small intermediate chunk might be combined or lost.\\n\\nIf the header boundary logic is fixed per the previous comment to always save chunks at boundaries, this inconsistency will be resolved. Otherwise, consider applying the same size logic here.\\n\\n<details>\\n<summary>≡ƒñû Prompt for AI Agents</summary>\\n\\n```\\nIn\\nsrc/RetrievalAugmentedGeneration/ChunkingStrategies/HeaderBasedTextSplitter.cs\\naround lines 110 to 122, the final chunk is always emitted regardless of size,\\ncausing inconsistent behavior compared to header-boundary chunking which\\nconditionally saves based on minChunkSize; update the final-chunk handling to\\napply the same size rule (e.g., only add the final chunk if currentChunk.Count\\n>= minChunkSize) or, if header-boundary behavior is changed to always save, make\\nthe final-chunk logic consistent with that decision; ensure metadata (header and\\nchunkIndex) is set the same way as other chunks when added.\\n```\\n\\n</details>\\n\\n<!-- This is an auto-generated comment by CodeRabbit -->\",\"path\":\"src/RetrievalAugmentedGeneration/ChunkingStrategies/HeaderBasedTextSplitter.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,146 @@\\n+using AiDotNet.Interfaces;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies;\\n+\\n+/// <summary>\\n+/// Splits structured documents based on header tags (H1, H2, H3, etc.).\\n+/// </summary>\\n+/// <typeparam name=\\\"T\\\">The numeric data type used for calculations.</typeparam>\\n+/// <remarks>\\n+/// Ideal for Markdown and HTML documents where headers provide natural semantic boundaries.\\n+/// Preserves document structure and hierarchy.\\n+/// </remarks>\\n+public class HeaderBasedTextSplitter<T> : ChunkingStrategyBase<T>\\n+{\\n+    private readonly int _maxChunkSize;\\n+    private readonly bool _combineSmallChunks;\\n+    private readonly int _minChunkSize;\\n+\\n+    /// <summary>\\n+    /// Initializes a new instance of the <see cref=\\\"HeaderBasedTextSplitter{T}\\\"/> class.\\n+    /// </summary>\\n+    /// <param name=\\\"maxChunkSize\\\">Maximum size of each chunk in characters.</param>\\n+    /// <param name=\\\"minChunkSize\\\">Minimum size for chunk combination.</param>\\n+    /// <param name=\\\"combineSmallChunks\\\">Whether to combine small chunks.</param>\\n+    /// <param name=\\\"numericOperations\\\">The numeric operations provider.</param>\\n+    public HeaderBasedTextSplitter(\\n+        int maxChunkSize,\\n+        int minChunkSize,\\n+        bool combineSmallChunks,\\n+        INumericOperations<T> numericOperations)\\n+        : base(numericOperations)\\n+    {\\n+        if (maxChunkSize <= 0)\\n+            throw new ArgumentOutOfRangeException(nameof(maxChunkSize), \\\"Max chunk size must be positive\\\");\\n+            \\n+        if (minChunkSize < 0)\\n+            throw new ArgumentOutOfRangeException(nameof(minChunkSize), \\\"Min chunk size cannot be negative\\\");\\n+            \\n+        _maxChunkSize = maxChunkSize;\\n+        _minChunkSize = minChunkSize;\\n+        _combineSmallChunks = combineSmallChunks;\\n+    }\\n+\\n+    /// <summary>\\n+    /// Splits text based on header hierarchy.\\n+    /// </summary>\\n+    public override IEnumerable<Document<T>> Chunk(string text)\\n+    {\\n+        if (string.IsNullOrWhiteSpace(text))\\n+            return Enumerable.Empty<Document<T>>();\\n+\\n+        var chunks = new List<Document<T>>();\\n+        var lines = text.Split(new[] { \\\"\\\\r\\\\n\\\", \\\"\\\\r\\\", \\\"\\\\n\\\" }, StringSplitOptions.None);\\n+        var currentChunk = new List<string>();\\n+        var currentHeader = string.Empty;\\n+\\n+        foreach (var line in lines)\\n+        {\\n+            // Check if line is a header (Markdown ## or HTML <h>)\\n+            if (IsHeader(line))\\n+            {\\n+                // Save current chunk if not empty\\n+                if (currentChunk.Count > 0)\\n+                {\\n+                    var content = string.Join(Environment.NewLine, currentChunk);\\n+                    if (content.Length >= _minChunkSize || !_combineSmallChunks)\\n+                    {\\n+                        chunks.Add(new Document<T>\\n+                        {\\n+                            Id = Guid.NewGuid().ToString(),\\n+                            Content = content,\\n+                            Metadata = new Dictionary<string, object>\\n+                            {\\n+                                [\\\"header\\\"] = currentHeader,\\n+                                [\\\"chunkIndex\\\"] = chunks.Count\\n+                            }\\n+                        });\\n+                        currentChunk.Clear();\\n+                    }\\n+                }\\n+\\n+                currentHeader = line.Trim();\\n+                currentChunk.Add(line);\\n+            }\\n+            else\\n+            {\\n+                currentChunk.Add(line);\\n+\\n+                // Split if chunk gets too large\\n+                var currentSize = string.Join(Environment.NewLine, currentChunk).Length;\\n+                if (currentSize >= _maxChunkSize)\\n+                {\\n+                    chunks.Add(new Document<T>\\n+                    {\\n+                        Id = Guid.NewGuid().ToString(),\\n+                        Content = string.Join(Environment.NewLine, currentChunk),\\n+                        Metadata = new Dictionary<string, object>\\n+                        {\\n+                            [\\\"header\\\"] = currentHeader,\\n+                            [\\\"chunkIndex\\\"] = chunks.Count\\n+                        }\\n+                    });\\n+                    currentChunk.Clear();\\n+                }\\n+            }\\n+        }\\n+\\n+        // Add remaining content\\n+        if (currentChunk.Count > 0)\\n+        {\\n+            chunks.Add(new Document<T>\\n+            {\\n+                Id = Guid.NewGuid().ToString(),\\n+                Content = string.Join(Environment.NewLine, currentChunk),\\n+                Metadata = new Dictionary<string, object>\\n+                {\\n+                    [\\\"header\\\"] = currentHeader,\\n+                    [\\\"chunkIndex\\\"] = chunks.Count\\n+                }\\n+            });\\n+        }\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZg3g\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UT9q8\",\"body\":\"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\\n\\n**Prevent infinite loop when context window <ΓÇ»50 chars**  \\nThe logic sets `contextEnd = Math.Min(lines.Length, i + (_contextWindowSize / 50))` and then forces `i = contextEnd - 1`. If `_contextWindowSize` is less than 50 (the constructor even permits 0), the integer division yields `0`, making `contextEnd == i` and `i` becomes `i - 1`. After the loopΓÇÖs `i++`, the index returns to the original value, so the same image line is processed again forever. Any multimodal document with a small context window will hang. Please ensure you only rewind `i` when `contextEnd > i`; otherwise leave it unchanged (or explicitly advance) before continuing.\\n\\n<details>\\n<summary>≡ƒñû Prompt for AI Agents</summary>\\n\\n```\\nIn src/RetrievalAugmentedGeneration/ChunkingStrategies/MultiModalTextSplitter.cs\\naround lines 85 to 93, the calculation contextEnd uses integer division\\n(_contextWindowSize / 50) which can be zero and causes i to be set to contextEnd\\n- 1 producing an infinite loop; change the logic so you only rewind/assign i\\nwhen contextEnd > i (i.e., if contextEnd <= i leave i unchanged or set i = i to\\nexplicitly advance via the loop), or ensure a minimum advance (e.g., compute a\\nnonzero advance = Math.Max(1, _contextWindowSize / 50) before using it) so the\\nloop always progresses when adding context lines.\\n```\\n\\n</details>\\n\\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\\n\\n<!-- This is an auto-generated comment by CodeRabbit -->\",\"path\":\"src/RetrievalAugmentedGeneration/ChunkingStrategies/MultiModalTextSplitter.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,194 @@\\n+using AiDotNet.Interfaces;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies;\\n+\\n+/// <summary>\\n+/// Multi-modal splitter for documents containing both text and images.\\n+/// </summary>\\n+/// <typeparam name=\\\"T\\\">The numeric data type used for calculations.</typeparam>\\n+/// <remarks>\\n+/// Creates chunks that keep text and related images together, preserving the relationship\\n+/// between visual and textual content for better context preservation.\\n+/// </remarks>\\n+public class MultiModalTextSplitter<T> : ChunkingStrategyBase<T>\\n+{\\n+    private readonly int _maxChunkSize;\\n+    private readonly bool _preserveImageContext;\\n+    private readonly int _contextWindowSize;\\n+\\n+    /// <summary>\\n+    /// Initializes a new instance of the <see cref=\\\"MultiModalTextSplitter{T}\\\"/> class.\\n+    /// </summary>\\n+    /// <param name=\\\"maxChunkSize\\\">Maximum size of text portion in each chunk.</param>\\n+    /// <param name=\\\"contextWindowSize\\\">Number of characters before/after image to include.</param>\\n+    /// <param name=\\\"preserveImageContext\\\">Whether to keep surrounding text with images.</param>\\n+    /// <param name=\\\"numericOperations\\\">The numeric operations provider.</param>\\n+    public MultiModalTextSplitter(\\n+        int maxChunkSize,\\n+        int contextWindowSize,\\n+        bool preserveImageContext,\\n+        INumericOperations<T> numericOperations)\\n+        : base(numericOperations)\\n+    {\\n+        if (maxChunkSize <= 0)\\n+            throw new ArgumentOutOfRangeException(nameof(maxChunkSize), \\\"Max chunk size must be positive\\\");\\n+            \\n+        if (contextWindowSize < 0)\\n+            throw new ArgumentOutOfRangeException(nameof(contextWindowSize), \\\"Context window size cannot be negative\\\");\\n+            \\n+        _maxChunkSize = maxChunkSize;\\n+        _contextWindowSize = contextWindowSize;\\n+        _preserveImageContext = preserveImageContext;\\n+    }\\n+\\n+    /// <summary>\\n+    /// Splits text while preserving text-image relationships.\\n+    /// </summary>\\n+    public override IEnumerable<Document<T>> Chunk(string text)\\n+    {\\n+        if (string.IsNullOrWhiteSpace(text))\\n+            return Enumerable.Empty<Document<T>>();\\n+\\n+        var chunks = new List<Document<T>>();\\n+        var lines = text.Split(new[] { \\\"\\\\r\\\\n\\\", \\\"\\\\r\\\", \\\"\\\\n\\\" }, StringSplitOptions.None);\\n+        \\n+        var currentChunk = new List<string>();\\n+        var imageReferences = new List<string>();\\n+\\n+        for (var i = 0; i < lines.Length; i++)\\n+        {\\n+            var line = lines[i];\\n+\\n+            // Detect image references (Markdown ![alt](url) or HTML <img>)\\n+            if (IsImageReference(line))\\n+            {\\n+                if (_preserveImageContext)\\n+                {\\n+                    // Include context before image\\n+                    var contextStart = Math.Max(0, i - (_contextWindowSize / 50)); // Approximate lines\\n+                    for (var j = contextStart; j < i; j++)\\n+                    {\\n+                        if (!currentChunk.Contains(lines[j]))\\n+                        {\\n+                            currentChunk.Add(lines[j]);\\n+                        }\\n+                    }\\n+                }\\n+\\n+                imageReferences.Add(ExtractImageReference(line));\\n+                currentChunk.Add(line);\\n+\\n+                if (_preserveImageContext)\\n+                {\\n+                    // Include context after image\\n+                    var contextEnd = Math.Min(lines.Length, i + (_contextWindowSize / 50));\\n+                    for (var j = i + 1; j < contextEnd; j++)\\n+                    {\\n+                        currentChunk.Add(lines[j]);\\n+                    }\\n+                    i = contextEnd - 1; // Skip ahead\\n+                }\\n+\\n+                // Create chunk with image\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZg3l\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UT9rC\",\"body\":\"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\\n\\n**Minor issues in constructor: unused field and missing validation.**\\n\\nTwo minor concerns:\\n1. The `_compressionRatio` field is stored but never used in the class methods.\\n2. The XML documentation states `compressionRatio` should be a \\\"Target compression ratio (0-1)\\\", but there's no validation to ensure it's within this range.\\n\\nAdd validation for the compression ratio range:\\n\\n```diff\\n public AutoCompressor(\\n     string modelPath,\\n     int maxOutputLength,\\n     T compressionRatio,\\n     INumericOperations<T> numericOperations)\\n {\\n     _modelPath = modelPath ?? throw new ArgumentNullException(nameof(modelPath));\\n     \\n     if (maxOutputLength <= 0)\\n         throw new ArgumentOutOfRangeException(nameof(maxOutputLength), \\\"Max output length must be positive\\\");\\n         \\n+    _numericOperations = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\\n+    \\n+    if (_numericOperations.LessThan(compressionRatio, _numericOperations.Zero) || \\n+        _numericOperations.GreaterThan(compressionRatio, _numericOperations.One))\\n+        throw new ArgumentOutOfRangeException(nameof(compressionRatio), \\\"Compression ratio must be between 0 and 1\\\");\\n+    \\n     _maxOutputLength = maxOutputLength;\\n     _compressionRatio = compressionRatio;\\n-    _numericOperations = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\\n }\\n```\\n\\nNote: The `_compressionRatio` field will likely be used once the compression logic is implemented.\\n\\n<!-- suggestion_start -->\\n\\n<details>\\n<summary>≡ƒô¥ Committable suggestion</summary>\\n\\n> ΓÇ╝∩╕Å **IMPORTANT**\\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\\n\\n```suggestion\\n    public AutoCompressor(\\n        string modelPath,\\n        int maxOutputLength,\\n        T compressionRatio,\\n        INumericOperations<T> numericOperations)\\n    {\\n        _modelPath = modelPath ?? throw new ArgumentNullException(nameof(modelPath));\\n        \\n        if (maxOutputLength <= 0)\\n            throw new ArgumentOutOfRangeException(nameof(maxOutputLength), \\\"Max output length must be positive\\\");\\n            \\n        _numericOperations = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\\n        \\n        if (_numericOperations.LessThan(compressionRatio, _numericOperations.Zero) || \\n            _numericOperations.GreaterThan(compressionRatio, _numericOperations.One))\\n            throw new ArgumentOutOfRangeException(nameof(compressionRatio), \\\"Compression ratio must be between 0 and 1\\\");\\n        \\n        _maxOutputLength = maxOutputLength;\\n        _compressionRatio = compressionRatio;\\n    }\\n```\\n\\n</details>\\n\\n<!-- suggestion_end -->\\n\\n<details>\\n<summary>≡ƒñû Prompt for AI Agents</summary>\\n\\n```\\nIn src/RetrievalAugmentedGeneration/ContextCompression/AutoCompressor.cs around\\nlines 28-42, the constructor currently stores _compressionRatio but doesn't\\nvalidate it; add an argument check that ensures compressionRatio is in a valid\\nrange (e.g., > 0 and <= 1) and throw an ArgumentOutOfRangeException with a clear\\nmessage if it isn't, and keep the existing null checks; you can also suppress or\\nleave as-is the unused-field warning for _compressionRatio until compression\\nlogic is implemented.\\n```\\n\\n</details>\\n\\n<!-- This is an auto-generated comment by CodeRabbit -->\",\"path\":\"src/RetrievalAugmentedGeneration/ContextCompression/AutoCompressor.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,81 @@\\n+using AiDotNet.Interfaces;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.ContextCompression;\\n+\\n+/// <summary>\\n+/// Auto-compressor using a sequence-to-sequence model fine-tuned for document compression.\\n+/// </summary>\\n+/// <typeparam name=\\\"T\\\">The numeric data type used for calculations.</typeparam>\\n+/// <remarks>\\n+/// Uses a trained seq2seq model to compress documents into shorter, more informative summaries\\n+/// while preserving query-relevant information.\\n+/// </remarks>\\n+public class AutoCompressor<T>\\n+{\\n+    private readonly INumericOperations<T> _numericOperations;\\n+    private readonly string _modelPath;\\n+    private readonly int _maxOutputLength;\\n+    private readonly T _compressionRatio;\\n+\\n+    /// <summary>\\n+    /// Initializes a new instance of the <see cref=\\\"AutoCompressor{T}\\\"/> class.\\n+    /// </summary>\\n+    /// <param name=\\\"modelPath\\\">Path to the compression model.</param>\\n+    /// <param name=\\\"maxOutputLength\\\">Maximum length of compressed output.</param>\\n+    /// <param name=\\\"compressionRatio\\\">Target compression ratio (0-1).</param>\\n+    /// <param name=\\\"numericOperations\\\">The numeric operations provider.</param>\\n+    public AutoCompressor(\\n+        string modelPath,\\n+        int maxOutputLength,\\n+        T compressionRatio,\\n+        INumericOperations<T> numericOperations)\\n+    {\\n+        _modelPath = modelPath ?? throw new ArgumentNullException(nameof(modelPath));\\n+        \\n+        if (maxOutputLength <= 0)\\n+            throw new ArgumentOutOfRangeException(nameof(maxOutputLength), \\\"Max output length must be positive\\\");\\n+            \\n+        _maxOutputLength = maxOutputLength;\\n+        _compressionRatio = compressionRatio;\\n+        _numericOperations = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\\n+    }\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZg3o\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UT9rI\",\"body\":\"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\\n\\n**Critical: OrderByDescending requires IComparable constraint on T.**\\n\\nThe `OrderByDescending(s => s.score)` call will fail at runtime if `T` does not implement `IComparable<T>`. Generic type `T` is currently unconstrained, so this will throw an `InvalidOperationException` for non-comparable types.\\n\\nConsider one of these solutions:\\n\\n**Solution 1 (Recommended)**: Add an `IComparable<T>` constraint to the class:\\n\\n```diff\\n-public class SelectiveContextCompressor<T>\\n+public class SelectiveContextCompressor<T> where T : IComparable<T>\\n```\\n\\n**Solution 2**: Use `INumericOperations<T>` to perform comparisons manually and sort the list yourself:\\n\\n```diff\\n-            var selectedSentences = scoredSentences\\n-                .OrderByDescending(s => s.score)\\n-                .Take(_maxSentences)\\n-                .Select(s => s.sentence);\\n+            scoredSentences.Sort((a, b) => _numericOperations.GreaterThan(a.score, b.score) ? -1 : \\n+                                            _numericOperations.LessThan(a.score, b.score) ? 1 : 0);\\n+            var selectedSentences = scoredSentences\\n+                .Take(_maxSentences)\\n+                .Select(s => s.sentence);\\n```\\n\\n\\n> Committable suggestion skipped: line range outside the PR's diff.\\n\\n<details>\\n<summary>≡ƒñû Prompt for AI Agents</summary>\\n\\n```\\nIn\\nsrc/RetrievalAugmentedGeneration/ContextCompression/SelectiveContextCompressor.cs\\naround lines 66ΓÇô69, OrderByDescending(s => s.score) can throw at runtime because\\nT is unconstrained and may not implement IComparable<T>; fix by adding an\\nIComparable<T> constraint to the generic (recommended) so comparisons are valid\\nat compile time and adjust the class/method signature accordingly, or if you\\ncannot change the generic constraint, replace the LINQ ordering with a\\ncomparison that uses Comparer<T>.Default (or an injected IComparer<T>/IComparer)\\nto perform safe comparisons before Take(_maxSentences).\\n```\\n\\n</details>\\n\\n<!-- This is an auto-generated comment by CodeRabbit -->\",\"path\":\"src/RetrievalAugmentedGeneration/ContextCompression/SelectiveContextCompressor.cs\",\"position\":66,\"diffHunk\":\"@@ -0,0 +1,108 @@\\n+using AiDotNet.Interfaces;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.ContextCompression;\\n+\\n+/// <summary>\\n+/// Selective context compressor that picks the most relevant sentences based on the query.\\n+/// </summary>\\n+/// <typeparam name=\\\"T\\\">The numeric data type used for calculations.</typeparam>\\n+/// <remarks>\\n+/// Analyzes retrieved documents and selectively extracts only the sentences most relevant\\n+/// to the query, reducing context length while preserving important information.\\n+/// </remarks>\\n+public class SelectiveContextCompressor<T>\\n+{\\n+    private readonly INumericOperations<T> _numericOperations;\\n+    private readonly int _maxSentences;\\n+    private readonly T _relevanceThreshold;\\n+\\n+    /// <summary>\\n+    /// Initializes a new instance of the <see cref=\\\"SelectiveContextCompressor{T}\\\"/> class.\\n+    /// </summary>\\n+    /// <param name=\\\"maxSentences\\\">Maximum number of sentences to keep.</param>\\n+    /// <param name=\\\"relevanceThreshold\\\">Minimum relevance score to keep a sentence.</param>\\n+    /// <param name=\\\"numericOperations\\\">The numeric operations provider.</param>\\n+    public SelectiveContextCompressor(\\n+        int maxSentences,\\n+        T relevanceThreshold,\\n+        INumericOperations<T> numericOperations)\\n+    {\\n+        if (maxSentences <= 0)\\n+            throw new ArgumentOutOfRangeException(nameof(maxSentences), \\\"Max sentences must be positive\\\");\\n+            \\n+        _maxSentences = maxSentences;\\n+        _relevanceThreshold = relevanceThreshold;\\n+        _numericOperations = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\\n+    }\\n+\\n+    /// <summary>\\n+    /// Compresses documents by selecting relevant sentences.\\n+    /// </summary>\\n+    public IEnumerable<Document<T>> Compress(string query, IEnumerable<Document<T>> documents)\\n+    {\\n+        if (string.IsNullOrWhiteSpace(query))\\n+            throw new ArgumentException(\\\"Query cannot be null or whitespace\\\", nameof(query));\\n+\\n+        if (documents == null)\\n+            throw new ArgumentNullException(nameof(documents));\\n+\\n+        var compressed = new List<Document<T>>();\\n+\\n+        foreach (var doc in documents)\\n+        {\\n+            var sentences = SplitIntoSentences(doc.Content);\\n+            var scoredSentences = new List<(string sentence, T score)>();\\n+\\n+            foreach (var sentence in sentences)\\n+            {\\n+                var score = CalculateRelevance(query, sentence);\\n+                if (_numericOperations.GreaterThanOrEqual(score, _relevanceThreshold))\\n+                {\\n+                    scoredSentences.Add((sentence, score));\\n+                }\\n+            }\\n+\\n+            var selectedSentences = scoredSentences\\n+                .OrderByDescending(s => s.score)\\n+                .Take(_maxSentences)\\n+                .Select(s => s.sentence);\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZg3u\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UT9rQ\",\"body\":\"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\\n\\n<details>\\n<summary>≡ƒº⌐ Analysis chain</summary>\\n\\n**Verify vectorDimension validation in base constructor.**\\n\\nThe constructor validates string parameters but not `vectorDimension`. Ensure the base constructor validates that `vectorDimension > 0`, as negative or zero dimensions would be invalid.\\n\\n\\n\\nRun the following script to verify the base constructor's validation:\\n\\n---\\n\\n≡ƒÅü Script executed:\\n\\n```shell\\n#!/bin/bash\\n# Description: Check if DocumentStoreBase constructor validates vectorDimension\\n\\nast-grep --pattern $'class DocumentStoreBase<$_> {\\n  $$$\\n  DocumentStoreBase($_, $_) {\\n    $$$\\n  }\\n  $$$\\n}'\\n```\\n\\nLength of output: 0\\n\\n---\\n\\n≡ƒÅü Script executed:\\n\\n```shell\\n#!/bin/bash\\n# Find DocumentStoreBase class definition and examine constructor\\n\\n# First, locate the DocumentStoreBase file\\nfd -type f -name \\\"*DocumentStore*\\\" | head -20\\n\\n# Then search for DocumentStoreBase class and constructor with context\\nrg -n \\\"class DocumentStoreBase\\\" -A 30 --type cs\\n```\\n\\nLength of output: 3500\\n\\n---\\n\\n≡ƒÅü Script executed:\\n\\n```shell\\n#!/bin/bash\\n# Find DocumentStoreBase constructor implementation\\n\\ncat src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs | head -100 | tail -70\\n```\\n\\nLength of output: 2945\\n\\n---\\n\\n≡ƒÅü Script executed:\\n\\n```shell\\n#!/bin/bash\\n# Search for constructor in DocumentStoreBase\\n\\nrg -n \\\"protected.*DocumentStoreBase|public.*DocumentStoreBase\\\" src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs -A 5\\n```\\n\\nLength of output: 283\\n\\n---\\n\\n≡ƒÅü Script executed:\\n\\n```shell\\n#!/bin/bash\\n# View entire DocumentStoreBase file to find constructors\\n\\nwc -l src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs\\n\\n# Search for any method that looks like constructor (pattern matching)\\nrg -n \\\"^\\\\s*(protected|public|private)?\\\\s*DocumentStoreBase\\\\s*\\\\(\\\" src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs\\n```\\n\\nLength of output: 72\\n\\n---\\n\\n≡ƒÅü Script executed:\\n\\n```shell\\n#!/bin/bash\\n# View the entire DocumentStoreBase file\\n\\ncat src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs\\n```\\n\\nLength of output: 14172\\n\\n---\\n\\n≡ƒÅü Script executed:\\n\\n```shell\\n#!/bin/bash\\n# View AzureSearchDocumentStore constructor to verify base() call\\n\\ncat src/RetrievalAugmentedGeneration/DocumentStores/AzureSearchDocumentStore.cs | head -50\\n```\\n\\nLength of output: 2097\\n\\n---\\n\\n≡ƒÅü Script executed:\\n\\n```shell\\n#!/bin/bash\\n# Search for DocumentStore* files and look for constructors\\n\\nfd -type f -name \\\"*DocumentStore*\\\" -path \\\"*/DocumentStores/*\\\"\\n\\n# Check if there's any protected constructor in DocumentStoreBase\\nrg -n \\\"protected DocumentStoreBase\\\" src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs\\n```\\n\\nLength of output: 170\\n\\n---\\n\\n≡ƒÅü Script executed:\\n\\n```shell\\n#!/bin/bash\\n# Verify actual AzureSearchDocumentStore constructor\\n\\nrg -n \\\"public AzureSearchDocumentStore\\\" src/RetrievalAugmentedGeneration/DocumentStores/AzureSearchDocumentStore.cs -A 10\\n```\\n\\nLength of output: 536\\n\\n---\\n\\n\\n\\n</details>\\n\\n**Critical compilation error: base constructor call has no matching constructor.**\\n\\nThe `base(vectorDimension, numericOperations)` call on line 35 fails because `DocumentStoreBase<T>` has no explicit constructor. The base class only provides an implicit parameterless constructor and defines `VectorDimension` as an abstract property that derived classes must implement.\\n\\nAdditionally, `AzureSearchDocumentStore<T>` must implement the following abstract members from `DocumentStoreBase<T>`:\\n- Properties: `DocumentCount`, `VectorDimension`\\n- Methods: `AddCore()`, `GetSimilarCore()`, `GetByIdCore()`, `RemoveCore()`, `Clear()`\\n\\n<details>\\n<summary>≡ƒñû Prompt for AI Agents</summary>\\n\\n```\\nIn src/RetrievalAugmentedGeneration/DocumentStores/AzureSearchDocumentStore.cs\\naround lines 29ΓÇô40, the constructor currently calls base(vectorDimension,\\nnumericOperations) but the base class has no matching constructor and exposes\\nabstract members that this class must implement. Remove the base(...) call (use\\nthe implicit base constructor), store the incoming vectorDimension and\\nnumericOperations into private readonly fields or implement VectorDimension to\\nreturn the provided dimension, and implement all abstract members from\\nDocumentStoreBase<T>: the properties DocumentCount and VectorDimension, and the\\nmethods AddCore(...), GetSimilarCore(...), GetByIdCore(...), RemoveCore(...),\\nand Clear() with concrete logic (or throw NotImplementedException if stubbing)\\nso the class compiles and fulfills the base contract.\\n```\\n\\n</details>\\n\\n<!-- This is an auto-generated comment by CodeRabbit -->\",\"path\":\"src/RetrievalAugmentedGeneration/DocumentStores/AzureSearchDocumentStore.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,82 @@\\n+using AiDotNet.Interfaces;\\n+using AiDotNet.LinearAlgebra;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\\n+\\n+/// <summary>\\n+/// Azure Cognitive Search document store providing fully managed search capabilities.\\n+/// </summary>\\n+/// <typeparam name=\\\"T\\\">The numeric data type used for vector operations.</typeparam>\\n+/// <remarks>\\n+/// Azure Cognitive Search combines full-text search, semantic search, and vector search\\n+/// in a fully managed cloud service with enterprise-grade security and compliance.\\n+/// </remarks>\\n+public class AzureSearchDocumentStore<T> : DocumentStoreBase<T>\\n+{\\n+    private readonly string _serviceName;\\n+    private readonly string _indexName;\\n+    private readonly string _apiKey;\\n+\\n+    /// <summary>\\n+    /// Initializes a new instance of the <see cref=\\\"AzureSearchDocumentStore{T}\\\"/> class.\\n+    /// </summary>\\n+    /// <param name=\\\"serviceName\\\">The Azure Search service name.</param>\\n+    /// <param name=\\\"indexName\\\">The name of the index to use.</param>\\n+    /// <param name=\\\"apiKey\\\">The admin API key for authentication.</param>\\n+    /// <param name=\\\"vectorDimension\\\">The dimensionality of document vectors.</param>\\n+    /// <param name=\\\"numericOperations\\\">The numeric operations provider.</param>\\n+    public AzureSearchDocumentStore(\\n+        string serviceName,\\n+        string indexName,\\n+        string apiKey,\\n+        int vectorDimension,\\n+        INumericOperations<T> numericOperations)\\n+        : base(vectorDimension, numericOperations)\\n+    {\\n+        _serviceName = serviceName ?? throw new ArgumentNullException(nameof(serviceName));\\n+        _indexName = indexName ?? throw new ArgumentNullException(nameof(indexName));\\n+        _apiKey = apiKey ?? throw new ArgumentNullException(nameof(apiKey));\\n+    }\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZg3w\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UT9rU\",\"body\":\"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\\n\\n**Implement AddCore instead of overriding AddDocument.**\\n\\nThe base class `DocumentStoreBase<T>` establishes a template method pattern where public methods perform validation and call protected abstract `*Core` methods. According to the base class documentation:\\n\\n- Public method: `Add(VectorDocument<T>)` handles validation\\n- Abstract method to implement: `AddCore(VectorDocument<T>)` \\n\\nThis class appears to override `AddDocument(Document<T>)` directly, which bypasses the base class validation logic and violates the established pattern. Additionally, the parameter type differsΓÇöbase expects `VectorDocument<T>` but this takes `Document<T>`.\\n\\n\\n\\nVerify the correct method to override:\\n\\n```shell\\n#!/bin/bash\\n# Description: Verify which Add methods exist in DocumentStoreBase\\n\\nast-grep --pattern $'class DocumentStoreBase<$_> {\\n  $$$\\n  AddDocument($$$) {\\n    $$$\\n  }\\n  $$$\\n}'\\n\\nast-grep --pattern $'class DocumentStoreBase<$_> {\\n  $$$\\n  AddCore($$$) {\\n    $$$\\n  }\\n  $$$\\n}'\\n```\\n\\n<!-- This is an auto-generated comment by CodeRabbit -->\",\"path\":\"src/RetrievalAugmentedGeneration/DocumentStores/AzureSearchDocumentStore.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,82 @@\\n+using AiDotNet.Interfaces;\\n+using AiDotNet.LinearAlgebra;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\\n+\\n+/// <summary>\\n+/// Azure Cognitive Search document store providing fully managed search capabilities.\\n+/// </summary>\\n+/// <typeparam name=\\\"T\\\">The numeric data type used for vector operations.</typeparam>\\n+/// <remarks>\\n+/// Azure Cognitive Search combines full-text search, semantic search, and vector search\\n+/// in a fully managed cloud service with enterprise-grade security and compliance.\\n+/// </remarks>\\n+public class AzureSearchDocumentStore<T> : DocumentStoreBase<T>\\n+{\\n+    private readonly string _serviceName;\\n+    private readonly string _indexName;\\n+    private readonly string _apiKey;\\n+\\n+    /// <summary>\\n+    /// Initializes a new instance of the <see cref=\\\"AzureSearchDocumentStore{T}\\\"/> class.\\n+    /// </summary>\\n+    /// <param name=\\\"serviceName\\\">The Azure Search service name.</param>\\n+    /// <param name=\\\"indexName\\\">The name of the index to use.</param>\\n+    /// <param name=\\\"apiKey\\\">The admin API key for authentication.</param>\\n+    /// <param name=\\\"vectorDimension\\\">The dimensionality of document vectors.</param>\\n+    /// <param name=\\\"numericOperations\\\">The numeric operations provider.</param>\\n+    public AzureSearchDocumentStore(\\n+        string serviceName,\\n+        string indexName,\\n+        string apiKey,\\n+        int vectorDimension,\\n+        INumericOperations<T> numericOperations)\\n+        : base(vectorDimension, numericOperations)\\n+    {\\n+        _serviceName = serviceName ?? throw new ArgumentNullException(nameof(serviceName));\\n+        _indexName = indexName ?? throw new ArgumentNullException(nameof(indexName));\\n+        _apiKey = apiKey ?? throw new ArgumentNullException(nameof(apiKey));\\n+    }\\n+\\n+    /// <summary>\\n+    /// Adds a document to the Azure Search index.\\n+    /// </summary>\\n+    public override void AddDocument(Document<T> document)\\n+    {\\n+        if (document == null)\\n+            throw new ArgumentNullException(nameof(document));\\n+\\n+        // TODO: Implement Azure Search indexing via REST API\\n+        throw new NotImplementedException(\\\"Azure Search integration requires HTTP client implementation\\\");\\n+    }\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZg3y\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UT9rW\",\"body\":\"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\\n\\n**Implement GetSimilarCore instead of overriding GetSimilar.**\\n\\nThe base class provides a public `GetSimilar(Vector<T>, int)` method that handles validation and calls `GetSimilarWithFilters(Vector<T>, int, Dictionary<string, object>)`, which then invokes the protected abstract `GetSimilarCore`. By overriding `GetSimilar` directly:\\n\\n1. You bypass the base class's validation logic\\n2. You lose metadata filtering support that the base class provides via `GetSimilarWithFilters`\\n3. You duplicate validation that the base already performs\\n\\nAccording to the base class pattern, implement `GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)` instead.\\n\\n\\n\\nVerify the correct method signature:\\n\\n```shell\\n#!/bin/bash\\n# Description: Find GetSimilarCore method signature in base class\\n\\nrg -A 5 'GetSimilarCore' --type cs -g '*DocumentStoreBase.cs'\\n```\\n\\n<details>\\n<summary>≡ƒñû Prompt for AI Agents</summary>\\n\\n```\\nIn src/RetrievalAugmentedGeneration/DocumentStores/AzureSearchDocumentStore.cs\\naround lines 57ΓÇô67, don't override the public GetSimilar method; instead\\nimplement the protected override GetSimilarCore(Vector<T> queryVector, int topK,\\nDictionary<string, object> metadataFilters) that the base class expects. Remove\\nthe current public override, implement GetSimilarCore with that exact signature,\\naccept the provided metadataFilters (do not re-do validation ΓÇö the base handles\\nit), call Azure Search via the REST/HTTP client already available on this class\\n(build the vector search request, include metadataFilters as query/body\\nfilters), translate Azure Search results into IEnumerable<Document<T>> and\\nreturn the mapped documents; if HTTP injection is missing, use the existing\\nHttpClient or add one via constructor DI and use it here.\\n```\\n\\n</details>\\n\\n<!-- This is an auto-generated comment by CodeRabbit -->\",\"path\":\"src/RetrievalAugmentedGeneration/DocumentStores/AzureSearchDocumentStore.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,82 @@\\n+using AiDotNet.Interfaces;\\n+using AiDotNet.LinearAlgebra;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\\n+\\n+/// <summary>\\n+/// Azure Cognitive Search document store providing fully managed search capabilities.\\n+/// </summary>\\n+/// <typeparam name=\\\"T\\\">The numeric data type used for vector operations.</typeparam>\\n+/// <remarks>\\n+/// Azure Cognitive Search combines full-text search, semantic search, and vector search\\n+/// in a fully managed cloud service with enterprise-grade security and compliance.\\n+/// </remarks>\\n+public class AzureSearchDocumentStore<T> : DocumentStoreBase<T>\\n+{\\n+    private readonly string _serviceName;\\n+    private readonly string _indexName;\\n+    private readonly string _apiKey;\\n+\\n+    /// <summary>\\n+    /// Initializes a new instance of the <see cref=\\\"AzureSearchDocumentStore{T}\\\"/> class.\\n+    /// </summary>\\n+    /// <param name=\\\"serviceName\\\">The Azure Search service name.</param>\\n+    /// <param name=\\\"indexName\\\">The name of the index to use.</param>\\n+    /// <param name=\\\"apiKey\\\">The admin API key for authentication.</param>\\n+    /// <param name=\\\"vectorDimension\\\">The dimensionality of document vectors.</param>\\n+    /// <param name=\\\"numericOperations\\\">The numeric operations provider.</param>\\n+    public AzureSearchDocumentStore(\\n+        string serviceName,\\n+        string indexName,\\n+        string apiKey,\\n+        int vectorDimension,\\n+        INumericOperations<T> numericOperations)\\n+        : base(vectorDimension, numericOperations)\\n+    {\\n+        _serviceName = serviceName ?? throw new ArgumentNullException(nameof(serviceName));\\n+        _indexName = indexName ?? throw new ArgumentNullException(nameof(indexName));\\n+        _apiKey = apiKey ?? throw new ArgumentNullException(nameof(apiKey));\\n+    }\\n+\\n+    /// <summary>\\n+    /// Adds a document to the Azure Search index.\\n+    /// </summary>\\n+    public override void AddDocument(Document<T> document)\\n+    {\\n+        if (document == null)\\n+            throw new ArgumentNullException(nameof(document));\\n+\\n+        // TODO: Implement Azure Search indexing via REST API\\n+        throw new NotImplementedException(\\\"Azure Search integration requires HTTP client implementation\\\");\\n+    }\\n+\\n+    /// <summary>\\n+    /// Retrieves documents similar to the query vector.\\n+    /// </summary>\\n+    public override IEnumerable<Document<T>> GetSimilar(Vector<T> queryVector, int topK)\\n+    {\\n+        if (queryVector == null)\\n+            throw new ArgumentNullException(nameof(queryVector));\\n+\\n+        if (topK <= 0)\\n+            throw new ArgumentOutOfRangeException(nameof(topK), \\\"topK must be positive\\\");\\n+\\n+        // TODO: Implement Azure Search vector search via REST API\\n+        throw new NotImplementedException(\\\"Azure Search integration requires HTTP client implementation\\\");\\n+    }\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZg31\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UT9rb\",\"body\":\"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\\n\\n<details>\\n<summary>≡ƒº⌐ Analysis chain</summary>\\n\\n**Verify GetAllDocuments exists in the base class.**\\n\\nThe `GetAllDocuments()` method is not present in the provided base class snippet. Verify that `DocumentStoreBase<T>` defines this as an abstract or virtual method that requires implementation.\\n\\n---\\n\\n≡ƒÅü Script executed:\\n\\n```shell\\n#!/bin/bash\\n# Description: Check if GetAllDocuments is defined in DocumentStoreBase\\n\\nrg -n 'GetAllDocuments' --type cs -g '*DocumentStoreBase.cs'\\n```\\n\\nLength of output: 0\\n\\n---\\n\\n≡ƒÅü Script executed:\\n\\n```shell\\n#!/bin/bash\\n# Description: Find DocumentStoreBase class definition and check for GetAllDocuments method\\n\\n# First, locate all occurrences of DocumentStoreBase\\necho \\\"=== Finding DocumentStoreBase class definition ===\\\"\\nrg -n 'class DocumentStoreBase' --type cs -C 5\\n\\n# Also check if there's a method named GetAllDocuments anywhere in the codebase\\necho -e \\\"\\\\n=== Finding GetAllDocuments anywhere in codebase ===\\\"\\nrg -n 'GetAllDocuments' --type cs -B 2 -A 2\\n\\n# Search for the AzureSearchDocumentStore class to understand its inheritance\\necho -e \\\"\\\\n=== Finding AzureSearchDocumentStore class ===\\\"\\nrg -n 'class AzureSearchDocumentStore' --type cs -C 3\\n```\\n\\nLength of output: 6396\\n\\n---\\n\\n</thinking>\\n\\n---\\n\\n≡ƒÅü Script executed:\\n\\n```shell\\n#!/bin/bash\\n# Description: Find where GetAllDocuments is declared as abstract/virtual\\n\\n# Search for IDocumentStore interface definition\\necho \\\"=== Finding IDocumentStore interface ===\\\"\\nrg -n 'interface IDocumentStore' --type cs -A 20\\n\\n# Search for all method signatures (abstract/virtual) in DocumentStoreBase\\necho -e \\\"\\\\n=== Finding method signatures in DocumentStoreBase ===\\\"\\nrg -n 'abstract|virtual' src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs --type cs -B 1 -A 1\\n```\\n\\nLength of output: 3733\\n\\n---\\n\\n≡ƒÅü Script executed:\\n\\n```shell\\n#!/bin/bash\\n# Description: Find GetAllDocuments declaration\\n\\n# Get full IDocumentStore interface content\\necho \\\"=== Full IDocumentStore interface ===\\\"\\ncat src/Interfaces/IDocumentStore.cs\\n\\n# Also search for GetAllDocuments declaration (method signature without implementation details)\\necho -e \\\"\\\\n=== Searching for GetAllDocuments declaration ===\\\"\\nrg -n 'GetAllDocuments\\\\s*\\\\(' src/Interfaces/IDocumentStore.cs src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs --type cs -B 3 -A 1\\n```\\n\\nLength of output: 8634\\n\\n---\\n\\n≡ƒÅü Script executed:\\n\\n```shell\\n#!/bin/bash\\n# Description: Search explicitly for GetAllDocuments in DocumentStoreBase\\n\\n# Search for GetAllDocuments in DocumentStoreBase file\\necho \\\"=== Searching for GetAllDocuments in DocumentStoreBase.cs ===\\\"\\nrg -n 'GetAllDocuments' src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs --type cs -B 5 -A 5\\n\\n# Also check the full length of DocumentStoreBase to understand its structure\\necho -e \\\"\\\\n=== Line count and end of DocumentStoreBase.cs ===\\\"\\nwc -l src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs\\ntail -20 src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs\\n```\\n\\nLength of output: 723\\n\\n---\\n\\n\\n\\n</details>\\n\\n**Remove `override` keyword or add `GetAllDocuments()` abstract method to base class.**\\n\\nThe `GetAllDocuments()` method is not declared in `DocumentStoreBase<T>` or `IDocumentStore<T>`, making the `override` keyword at line 72 invalid. Either:\\n1. Add `public abstract IEnumerable<Document<T>> GetAllDocuments();` to `DocumentStoreBase<T>`, or\\n2. Change line 72 from `public override` to `public`\\n\\nOther implementations (SQLiteVSSDocumentStore, RedisVLDocumentStore, QdrantDocumentStore, ElasticsearchDocumentStore, ChromaDBDocumentStore) have the same issue.\\n\\n<details>\\n<summary>≡ƒñû Prompt for AI Agents</summary>\\n\\n```\\nIn src/RetrievalAugmentedGeneration/DocumentStores/AzureSearchDocumentStore.cs\\naround lines 72-76, the method is declared with `public override` but no\\n`GetAllDocuments()` exists in DocumentStoreBase<T> or IDocumentStore<T>, causing\\na compile error; add `public abstract IEnumerable<Document<T>>\\nGetAllDocuments();` to DocumentStoreBase<T> (and if you want the interface to\\nrequire it, also add the signature to IDocumentStore<T>), then keep the\\n`override` implementations in AzureSearchDocumentStore and the other stores\\n(SQLiteVSSDocumentStore, RedisVLDocumentStore, QdrantDocumentStore,\\nElasticsearchDocumentStore, ChromaDBDocumentStore); alternatively, if you prefer\\nnot to change the base types, change each storeΓÇÖs declaration from `public\\noverride` to `public` insteadΓÇöpick one approach and apply it consistently across\\nall listed files.\\n```\\n\\n</details>\\n\\n<!-- This is an auto-generated comment by CodeRabbit -->\",\"path\":\"src/RetrievalAugmentedGeneration/DocumentStores/AzureSearchDocumentStore.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,82 @@\\n+using AiDotNet.Interfaces;\\n+using AiDotNet.LinearAlgebra;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\\n+\\n+/// <summary>\\n+/// Azure Cognitive Search document store providing fully managed search capabilities.\\n+/// </summary>\\n+/// <typeparam name=\\\"T\\\">The numeric data type used for vector operations.</typeparam>\\n+/// <remarks>\\n+/// Azure Cognitive Search combines full-text search, semantic search, and vector search\\n+/// in a fully managed cloud service with enterprise-grade security and compliance.\\n+/// </remarks>\\n+public class AzureSearchDocumentStore<T> : DocumentStoreBase<T>\\n+{\\n+    private readonly string _serviceName;\\n+    private readonly string _indexName;\\n+    private readonly string _apiKey;\\n+\\n+    /// <summary>\\n+    /// Initializes a new instance of the <see cref=\\\"AzureSearchDocumentStore{T}\\\"/> class.\\n+    /// </summary>\\n+    /// <param name=\\\"serviceName\\\">The Azure Search service name.</param>\\n+    /// <param name=\\\"indexName\\\">The name of the index to use.</param>\\n+    /// <param name=\\\"apiKey\\\">The admin API key for authentication.</param>\\n+    /// <param name=\\\"vectorDimension\\\">The dimensionality of document vectors.</param>\\n+    /// <param name=\\\"numericOperations\\\">The numeric operations provider.</param>\\n+    public AzureSearchDocumentStore(\\n+        string serviceName,\\n+        string indexName,\\n+        string apiKey,\\n+        int vectorDimension,\\n+        INumericOperations<T> numericOperations)\\n+        : base(vectorDimension, numericOperations)\\n+    {\\n+        _serviceName = serviceName ?? throw new ArgumentNullException(nameof(serviceName));\\n+        _indexName = indexName ?? throw new ArgumentNullException(nameof(indexName));\\n+        _apiKey = apiKey ?? throw new ArgumentNullException(nameof(apiKey));\\n+    }\\n+\\n+    /// <summary>\\n+    /// Adds a document to the Azure Search index.\\n+    /// </summary>\\n+    public override void AddDocument(Document<T> document)\\n+    {\\n+        if (document == null)\\n+            throw new ArgumentNullException(nameof(document));\\n+\\n+        // TODO: Implement Azure Search indexing via REST API\\n+        throw new NotImplementedException(\\\"Azure Search integration requires HTTP client implementation\\\");\\n+    }\\n+\\n+    /// <summary>\\n+    /// Retrieves documents similar to the query vector.\\n+    /// </summary>\\n+    public override IEnumerable<Document<T>> GetSimilar(Vector<T> queryVector, int topK)\\n+    {\\n+        if (queryVector == null)\\n+            throw new ArgumentNullException(nameof(queryVector));\\n+\\n+        if (topK <= 0)\\n+            throw new ArgumentOutOfRangeException(nameof(topK), \\\"topK must be positive\\\");\\n+\\n+        // TODO: Implement Azure Search vector search via REST API\\n+        throw new NotImplementedException(\\\"Azure Search integration requires HTTP client implementation\\\");\\n+    }\\n+\\n+    /// <summary>\\n+    /// Gets all documents from the index.\\n+    /// </summary>\\n+    public override IEnumerable<Document<T>> GetAllDocuments()\\n+    {\\n+        // TODO: Implement Azure Search document retrieval\\n+        throw new NotImplementedException(\\\"Azure Search integration requires HTTP client implementation\\\");\\n+    }\"}]}},{\"id\":\"PRRT_kwDOKSXUF85gZg35\",\"isResolved\":true,\"comments\":{\"nodes\":[{\"id\":\"PRRC_kwDOKSXUF86UT9ri\",\"body\":\"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\\n\\n<details>\\n<summary>≡ƒº⌐ Analysis chain</summary>\\n\\n**Verify all abstract base class methods are implemented.**\\n\\nThe `DocumentCount` placeholder is acceptable for a stub. However, the base class defines several abstract methods that must be implemented:\\n\\n- `protected abstract void AddCore(VectorDocument<T>)`\\n- `protected abstract IEnumerable<Document<T>> GetSimilarCore(Vector<T>, int, Dictionary<string, object>)`\\n- `protected abstract Document<T>? GetByIdCore(string)`\\n- `protected abstract bool RemoveCore(string)`\\n- `public abstract void Clear()`\\n- `public abstract int VectorDimension { get; }`\\n\\nThis file shows none of these implementations. Verify that:\\n1. All required abstract members are implemented (or the code wouldn't compile)\\n2. The correct methods are being overridden per the base class pattern\\n\\n---\\n\\n≡ƒÅü Script executed:\\n\\n```shell\\n#!/bin/bash\\n# Description: List all abstract members in DocumentStoreBase that need implementation\\n\\nrg -n 'abstract' --type cs -g '*DocumentStoreBase.cs' | grep -E '(method|property|void|IEnumerable|bool|int)'\\n```\\n\\nLength of output: 1006\\n\\n---\\n\\nI'll search the codebase directly to verify all abstract methods are implemented in AzureSearchDocumentStore.\\n\\n\\n\\n```\\n\\n---\\n\\n≡ƒÅü Script executed:\\n\\n```shell\\n#!/bin/bash\\n# List all public and protected methods/properties in AzureSearchDocumentStore\\nrg -n 'public\\\\s+(override|abstract)' src/RetrievalAugmentedGeneration/DocumentStores/AzureSearchDocumentStore.cs\\nrg -n 'protected\\\\s+(override|abstract)' src/RetrievalAugmentedGeneration/DocumentStores/AzureSearchDocumentStore.cs\\n```\\n\\nLength of output: 306\\n\\n---\\n\\n```\\n\\n---\\n\\n≡ƒÅü Script executed:\\n\\n```shell\\n#!/bin/bash\\n# Get full file to see all implementations\\ncat src/RetrievalAugmentedGeneration/DocumentStores/AzureSearchDocumentStore.cs\\n```\\n\\nLength of output: 3297\\n\\n---\\n\\n```\\n\\n---\\n\\n≡ƒÅü Script executed:\\n\\n```shell\\n#!/bin/bash\\n# Check if DocumentStoreBase actually defines these members as abstract or virtual\\ncat src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs\\n```\\n\\nLength of output: 14172\\n\\n---\\n\\n\\n\\n</details>\\n\\n**Implement all required abstract base class members to fix compilation errors.**\\n\\nThe code is missing critical implementations. DocumentStoreBase defines 7 abstract members: `DocumentCount`, `VectorDimension`, `Clear()`, `AddCore()`, `GetSimilarCore()`, `GetByIdCore()`, and `RemoveCore()`. AzureSearchDocumentStore currently implements only `DocumentCount`; all other abstract members are missing.\\n\\nRequired implementations in `src/RetrievalAugmentedGeneration/DocumentStores/AzureSearchDocumentStore.cs`:\\n\\n- `public override int VectorDimension { get; }`\\n- `public override void Clear()`\\n- `protected override void AddCore(VectorDocument<T> vectorDocument)`\\n- `protected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)`\\n- `protected override Document<T>? GetByIdCore(string documentId)`\\n- `protected override bool RemoveCore(string documentId)`\\n\\nThese can throw `NotImplementedException` with TODO comments (matching the existing stub pattern), but they must be declared to allow compilation.\\n\\n<details>\\n<summary>≡ƒñû Prompt for AI Agents</summary>\\n\\n```\\nIn src/RetrievalAugmentedGeneration/DocumentStores/AzureSearchDocumentStore.cs\\naround line 81, the class currently only implements DocumentCount and is missing\\nthe other required abstract members from DocumentStoreBase; add the following\\noverrides with TODO NotImplementedException stubs so the class compiles: public\\noverride int VectorDimension { get; }, public override void Clear(), protected\\noverride void AddCore(VectorDocument<T> vectorDocument), protected override\\nIEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK,\\nDictionary<string, object> metadataFilters), protected override Document<T>?\\nGetByIdCore(string documentId), and protected override bool RemoveCore(string\\ndocumentId) ΓÇö each should throw new NotImplementedException() and include a TODO\\ncomment matching the existing stub pattern.\\n```\\n\\n</details>\\n\\n<!-- This is an auto-generated comment by CodeRabbit -->\",\"path\":\"src/RetrievalAugmentedGeneration/DocumentStores/AzureSearchDocumentStore.cs\",\"position\":1,\"diffHunk\":\"@@ -0,0 +1,82 @@\\n+using AiDotNet.Interfaces;\\n+using AiDotNet.LinearAlgebra;\\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\\n+\\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\\n+\\n+/// <summary>\\n+/// Azure Cognitive Search document store providing fully managed search capabilities.\\n+/// </summary>\\n+/// <typeparam name=\\\"T\\\">The numeric data type used for vector operations.</typeparam>\\n+/// <remarks>\\n+/// Azure Cognitive Search combines full-text search, semantic search, and vector search\\n+/// in a fully managed cloud service with enterprise-grade security and compliance.\\n+/// </remarks>\\n+public class AzureSearchDocumentStore<T> : DocumentStoreBase<T>\\n+{\\n+    private readonly string _serviceName;\\n+    private readonly string _indexName;\\n+    private readonly string _apiKey;\\n+\\n+    /// <summary>\\n+    /// Initializes a new instance of the <see cref=\\\"AzureSearchDocumentStore{T}\\\"/> class.\\n+    /// </summary>\\n+    /// <param name=\\\"serviceName\\\">The Azure Search service name.</param>\\n+    /// <param name=\\\"indexName\\\">The name of the index to use.</param>\\n+    /// <param name=\\\"apiKey\\\">The admin API key for authentication.</param>\\n+    /// <param name=\\\"vectorDimension\\\">The dimensionality of document vectors.</param>\\n+    /// <param name=\\\"numericOperations\\\">The numeric operations provider.</param>\\n+    public AzureSearchDocumentStore(\\n+        string serviceName,\\n+        string indexName,\\n+        string apiKey,\\n+        int vectorDimension,\\n+        INumericOperations<T> numericOperations)\\n+        : base(vectorDimension, numericOperations)\\n+    {\\n+        _serviceName = serviceName ?? throw new ArgumentNullException(nameof(serviceName));\\n+        _indexName = indexName ?? throw new ArgumentNullException(nameof(indexName));\\n+        _apiKey = apiKey ?? throw new ArgumentNullException(nameof(apiKey));\\n+    }\\n+\\n+    /// <summary>\\n+    /// Adds a document to the Azure Search index.\\n+    /// </summary>\\n+    public override void AddDocument(Document<T> document)\\n+    {\\n+        if (document == null)\\n+            throw new ArgumentNullException(nameof(document));\\n+\\n+        // TODO: Implement Azure Search indexing via REST API\\n+        throw new NotImplementedException(\\\"Azure Search integration requires HTTP client implementation\\\");\\n+    }\\n+\\n+    /// <summary>\\n+    /// Retrieves documents similar to the query vector.\\n+    /// </summary>\\n+    public override IEnumerable<Document<T>> GetSimilar(Vector<T> queryVector, int topK)\\n+    {\\n+        if (queryVector == null)\\n+            throw new ArgumentNullException(nameof(queryVector));\\n+\\n+        if (topK <= 0)\\n+            throw new ArgumentOutOfRangeException(nameof(topK), \\\"topK must be positive\\\");\\n+\\n+        // TODO: Implement Azure Search vector search via REST API\\n+        throw new NotImplementedException(\\\"Azure Search integration requires HTTP client implementation\\\");\\n+    }\\n+\\n+    /// <summary>\\n+    /// Gets all documents from the index.\\n+    /// </summary>\\n+    public override IEnumerable<Document<T>> GetAllDocuments()\\n+    {\\n+        // TODO: Implement Azure Search document retrieval\\n+        throw new NotImplementedException(\\\"Azure Search integration requires HTTP client implementation\\\");\\n+    }\\n+\\n+    /// <summary>\\n+    /// Gets the total number of documents in the index.\\n+    /// </summary>\\n+    public override int DocumentCount => 0; // TODO: Implement via Azure Search API\"}]}}],\"pageInfo\":{\"hasNextPage\":true,\"endCursor\":\"Y3Vyc29yOnYyOpK0MjAyNS0xMS0wNFQwMDozNDowMVrOYGYN-Q==\"}}}}}}"
