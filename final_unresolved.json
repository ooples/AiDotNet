[
  {
    "id": "PRRT_kwDOKSXUF85gqWj5",
    "isResolved": false,
    "comments": {
      "nodes": [
        {
          "id": "PRRC_kwDOKSXUF86UryNx",
          "body": "_≡ƒ¢á∩╕Å Refactor suggestion_ | _≡ƒƒá Major_\n\n**Extract duplicated text processing methods to shared utility class.**\n\nBoth `SplitIntoSentences` (lines 163-187) and `Tokenize` (lines 189-196) are duplicated verbatim in `SemanticChunkingStrategy.cs` and `LLMContextCompressor.cs`. This violates DRY and creates maintenance burden.\n\n\n\nRefactor by creating a shared utility class:\n\n```csharp\n// In AiDotNet.Helpers or similar namespace\npublic static class TextProcessingHelper\n{\n    public static List<string> SplitIntoSentences(string text)\n    {\n        var sentences = new List<string>();\n        var sentenceEndings = new[] { \". \", \"! \", \"? \", \".\\n\", \"!\\n\", \"?\\n\" };\n        var currentSentence = new System.Text.StringBuilder();\n\n        for (int i = 0; i < text.Length; i++)\n        {\n            currentSentence.Append(text[i]);\n\n            var matchedEnding = sentenceEndings.FirstOrDefault(ending => \n                currentSentence.ToString().EndsWith(ending));\n            if (matchedEnding != null)\n            {\n                sentences.Add(currentSentence.ToString().Trim());\n                currentSentence.Clear();\n            }\n        }\n\n        if (currentSentence.Length > 0 && !string.IsNullOrWhiteSpace(currentSentence.ToString()))\n        {\n            sentences.Add(currentSentence.ToString().Trim());\n        }\n\n        return sentences;\n    }\n\n    public static List<string> Tokenize(string text)\n    {\n        if (string.IsNullOrEmpty(text)) return new List<string>();\n\n        return text.ToLowerInvariant()\n            .Split(new[] { ' ', '\\t', '\\n', '\\r', '.', ',', '!', '?' }, \n                   StringSplitOptions.RemoveEmptyEntries)\n            .ToList();\n    }\n}\n```\n\nThen update this file and the other files to use the shared methods:\n\n```diff\n private List<string> SplitIntoSentences(string text)\n {\n-    var sentences = new List<string>();\n-    var sentenceEndings = new[] { \". \", \"! \", \"? \", \".\\n\", \"!\\n\", \"?\\n\" };\n-    var currentSentence = new System.Text.StringBuilder();\n-\n-    for (int i = 0; i < text.Length; i++)\n-    {\n-        currentSentence.Append(text[i]);\n-\n-        var matchedEnding = sentenceEndings.FirstOrDefault(ending => currentSentence.ToString().EndsWith(ending));\n-        if (matchedEnding != null)\n-        {\n-            sentences.Add(currentSentence.ToString().Trim());\n-            currentSentence.Clear();\n-        }\n-    }\n-\n-    if (currentSentence.Length > 0 && !string.IsNullOrWhiteSpace(currentSentence.ToString()))\n-    {\n-        sentences.Add(currentSentence.ToString().Trim());\n-    }\n-\n-    return sentences;\n+    return TextProcessingHelper.SplitIntoSentences(text);\n }\n\n private List<string> Tokenize(string text)\n {\n-    if (string.IsNullOrEmpty(text)) return new List<string>();\n-\n-    return text.ToLowerInvariant()\n-        .Split(new[] { ' ', '\\t', '\\n', '\\r', '.', ',', '!', '?' }, StringSplitOptions.RemoveEmptyEntries)\n-        .ToList();\n+    return TextProcessingHelper.Tokenize(text);\n }\n```\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
          "path": "src/RetrievalAugmentedGeneration/ContextCompression/DocumentSummarizer.cs",
          "line": 209
        }
      ]
    }
  },
  {
    "id": "PRRT_kwDOKSXUF85gqxEI",
    "isResolved": false,
    "comments": {
      "nodes": [
        {
          "id": "PRRC_kwDOKSXUF86UsWxL",
          "body": "_≡ƒ¢á∩╕Å Refactor suggestion_ | _≡ƒƒá Major_\n\n**Extract duplicated PreserveCase logic to a shared utility.**\n\nThis `PreserveCase` method is duplicated across multiple query processors (SpellCheckQueryProcessor at lines 73-84, and here). The logic is identical and should be extracted to a shared helper class or base class to follow DRY principles.\n\n\n\nConsider creating a shared utility class:\n\n```csharp\nnamespace AiDotNet.RetrievalAugmentedGeneration.QueryProcessors;\n\ninternal static class QueryProcessorHelpers\n{\n    internal static string PreserveCase(string original, string transformed)\n    {\n        if (string.IsNullOrEmpty(original) || string.IsNullOrEmpty(transformed))\n            return transformed;\n\n        if (char.IsUpper(original[0]))\n        {\n            return char.ToUpper(transformed[0]) + transformed.Substring(1);\n        }\n\n        return transformed;\n    }\n}\n```\n\nThen update LemmatizationQueryProcessor, SpellCheckQueryProcessor, and any other processors to use this shared helper.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn\nsrc/RetrievalAugmentedGeneration/QueryProcessors/LemmatizationQueryProcessor.cs\naround lines 85-96, the PreserveCase method is duplicated across processors;\nextract this logic into a shared internal static helper (e.g.,\nAiDotNet.RetrievalAugmentedGeneration.QueryProcessors.QueryProcessorHelpers.PreserveCase)\nthat accepts (string original, string transformed) and implements the same\nnull/empty and uppercase-first-char behavior, then remove the local PreserveCase\nmethod and replace calls with QueryProcessorHelpers.PreserveCase; repeat the\nsame replacement in SpellCheckQueryProcessor (around lines 73-84) and any other\nprocessors, adding the appropriate using or fully-qualified name and making the\nhelper internal static so all processors can access it.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
          "path": "src/RetrievalAugmentedGeneration/QueryProcessors/LemmatizationQueryProcessor.cs",
          "line": 96
        }
      ]
    }
  },
  {
    "id": "PRRT_kwDOKSXUF85grEBb",
    "isResolved": false,
    "comments": {
      "nodes": [
        {
          "id": "PRRC_kwDOKSXUF86Usw3z",
          "body": "_≡ƒ¢á∩╕Å Refactor suggestion_ | _≡ƒƒá Major_\n\n**Query processing logic is duplicated.**\n\nThe query processing loop (lines 889-896) is identical to the one in `RetrieveDocuments` (lines 957-964). Consider extracting this into a private helper method to eliminate duplication:\n\n\n\n```csharp\nprivate string ProcessQueryWithProcessors(string query)\n{\n    if (QueryProcessors == null)\n        return query;\n    \n    var processedQuery = query;\n    foreach (var processor in QueryProcessors)\n    {\n        processedQuery = processor.ProcessQuery(processedQuery);\n    }\n    return processedQuery;\n}\n```\n\nThen use it in both methods:\n```csharp\nvar processedQuery = ProcessQueryWithProcessors(query);\n```\n\nThis also makes the query processing pipeline easier to test and maintain.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/Models/Results/PredictionModelResult.cs around lines 874 to 925, the\nquery processing loop is duplicated; extract it into a private helper (e.g.,\nProcessQueryWithProcessors) that returns the original query if QueryProcessors\nis null and otherwise runs each processor in order, then replace the inline\nloops in GenerateAnswer and RetrieveDocuments to call this helper so query\nprocessing is centralized and testable.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
          "path": "src/Models/Results/PredictionModelResult.cs",
          "line": 925
        }
      ]
    }
  },
  {
    "id": "PRRT_kwDOKSXUF85grZxx",
    "isResolved": false,
    "comments": {
      "nodes": [
        {
          "id": "PRRC_kwDOKSXUF86UtPXr",
          "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Remove duplicate using directive.**\n\nThe `using AiDotNet.Interfaces;` directive appears on both lines 2 and 3.\n\n\nApply this diff:\n\n```diff\n using AiDotNet.Helpers;\n using AiDotNet.Interfaces;\n-using AiDotNet.Interfaces;\n using AiDotNet.RetrievalAugmentedGeneration.Models;\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\nusing AiDotNet.Interfaces;\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Retrievers/RetrieverBase.cs around lines 2\nto 3, there's a duplicate using directive \"using AiDotNet.Interfaces;\" ΓÇö remove\nthe redundant second occurrence so the file contains the directive only once;\nensure no other using statements are affected and save the file.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
          "path": "src/RetrievalAugmentedGeneration/Retrievers/RetrieverBase.cs",
          "line": 3
        }
      ]
    }
  },
  {
    "id": "PRRT_kwDOKSXUF85gsJx5",
    "isResolved": false,
    "comments": {
      "nodes": [
        {
          "id": "PRRC_kwDOKSXUF86UuPgO",
          "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Major: `ChunkOverlap` parameter is accepted but not implemented.**\n\nThe constructor accepts a `chunkOverlap` parameter and passes it to the base class, but `ChunkCore` never applies overlap between chunks. When splitting at header boundaries (lines 60-65) or size boundaries (lines 74-82), the code sets `chunkStart` to the position immediately after the previous chunk with no overlap (line 81). This violates the expected behavior documented in the base class.\n\n\n\nConsider one of these approaches:\n\n**Option 1: Implement overlap (recommended if overlap makes sense for header-based splitting):**\n\n```diff\n                 if (currentSize >= ChunkSize)\n                 {\n                     var content = string.Join(Environment.NewLine, currentChunk);\n                     chunks.Add((content, chunkStart, position + lineLength));\n-                    currentChunk.Clear();\n-                    chunkStart = position + lineLength;\n+                    \n+                    // Keep last ChunkOverlap characters for overlap\n+                    var overlapContent = content.Length > ChunkOverlap \n+                        ? content.Substring(content.Length - ChunkOverlap) \n+                        : content;\n+                    currentChunk.Clear();\n+                    currentChunk.Add(overlapContent);\n+                    chunkStart = position + lineLength - overlapContent.Length;\n                 }\n```\n\n**Option 2: Remove the parameter if overlap isn't applicable:**\n\n```diff\n     public HeaderBasedTextSplitter(\n         int chunkSize,\n-        int chunkOverlap = 0,\n         int minChunkSize = 100,\n         bool combineSmallChunks = true)\n-        : base(chunkSize, chunkOverlap)\n+        : base(chunkSize, 0)\n```\n\nUpdate documentation to clarify that header-based splitting doesn't support overlap since it maintains semantic boundaries.\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
          "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/HeaderBasedTextSplitter.cs",
          "line": 102
        }
      ]
    }
  },
  {
    "id": "PRRT_kwDOKSXUF85gsJx-",
    "isResolved": false,
    "comments": {
      "nodes": [
        {
          "id": "PRRC_kwDOKSXUF86UuPgT",
          "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Major: Position tracking is incorrect for non-native line endings.**\n\nLine 54 increments `position` by `line.Length + Environment.NewLine.Length`, but line 47 splits on the actual separators in the input text (`\\r\\n`, `\\r`, or `\\n`). When the input uses different line endings than the current platform's `Environment.NewLine` (e.g., Unix files on Windows), the position calculations drift and `StartPosition`/`EndPosition` values become inaccurate.\n\nFor example, if processing a Unix file (LF line endings) on Windows:\n- Actual separator length: 1 byte (`\\n`)\n- `Environment.NewLine.Length` on Windows: 2 bytes (`\\r\\n`)\n- Position drift: 1 byte per line\n\n\n\nApply this diff to track positions based on actual input separators:\n\n```diff\n         var chunks = new List<(string, int, int)>();\n         var lines = text.Split(new[] { \"\\r\\n\", \"\\r\", \"\\n\" }, StringSplitOptions.None);\n         var currentChunk = new List<string>();\n         var chunkStart = 0;\n-        var position = 0;\n+        var position = 0;\n+        \n+        // Detect actual line separator used in input\n+        var separatorLength = 1; // Default to LF\n+        if (text.Contains(\"\\r\\n\"))\n+            separatorLength = 2;\n\n         foreach (var line in lines)\n         {\n-            var lineLength = line.Length + Environment.NewLine.Length;\n+            var lineLength = line.Length + separatorLength;\n```\n\n\nAlso applies to: 85-85\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn\nsrc/RetrievalAugmentedGeneration/ChunkingStrategies/HeaderBasedTextSplitter.cs\naround lines 47-54 (and also at line 85), the code increments position using\nEnvironment.NewLine.Length which is wrong when the input uses different line\nendings; instead compute the actual separator length from the original text at\nthe current position: after a line of length L, look at the original input at\nindex position+L to see whether the next character(s) form \"\\r\\n\"\n(separatorLen=2), \"\\r\" or \"\\n\" (separatorLen=1), or nothing (separatorLen=0),\nthen increment position by L + separatorLen; replace uses of\nEnvironment.NewLine.Length with this computed separatorLen logic and apply the\nsame fix at the other occurrence around line 85 so StartPosition/EndPosition are\nbased on real input separators.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
          "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/HeaderBasedTextSplitter.cs",
          "line": 54
        }
      ]
    }
  },
  {
    "id": "PRRT_kwDOKSXUF85gsJyD",
    "isResolved": false,
    "comments": {
      "nodes": [
        {
          "id": "PRRC_kwDOKSXUF86UuPgb",
          "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Inverted index string conversion causes false negatives for typed metadata filters.**\n\nLines 213, 229, and 259 convert all metadata values and filter values to strings for indexing. This breaks filtering on non-string types:\n\n**Scenario:**\n- Document metadata: `{ \"age\": 25 }` (int)\n- Indexed as: `_invertedIndex[\"age\"][\"25\"] = {docId}`\n- Query filter: `{ \"age\": 30 }`\n- GetCandidateIds looks for `_invertedIndex[\"age\"][\"30\"]`\n- Returns empty set (line 276) because \"30\" was never indexed\n- Zero candidates ΓåÆ MatchesFilters never runs ΓåÆ document incorrectly excluded\n\nEven though `MatchesFilters` in the base class can handle `IComparable` range semantics, `GetCandidateIds` performs exact string matching and returns empty before `MatchesFilters` is reached.\n\nFor exact equality on the same value (e.g., filter `{ \"age\": 25 }`), string conversion works. But for any other value or type-aware comparison, the index causes false negatives.\n\n\n\n**Solution:** Preserve types in the inverted index or skip the index for non-string filters.\n\n<details>\n<summary>Option 1: Skip inverted index for non-string filters (simpler, safer)</summary>\n\n```diff\n private HashSet<string>? GetCandidateIds(Dictionary<string, object> metadataFilters)\n {\n     if (metadataFilters.Count == 0)\n         return null;\n+    \n+    // Skip inverted index optimization for non-string filters to avoid false negatives\n+    if (metadataFilters.Values.Any(v => v != null && v.GetType() != typeof(string)))\n+        return null; // Full scan\n \n     HashSet<string>? candidateIds = null;\n \n     foreach (var filter in metadataFilters)\n     {\n         var fieldName = filter.Key;\n         var indexKey = filter.Value?.ToString() ?? string.Empty;\n \n         if (_invertedIndex.TryGetValue(fieldName, out var fieldIndex))\n         {\n```\n\nThis approach returns `null` (triggering a full scan in `GetSimilarCore`) when any filter value is non-string, ensuring `MatchesFilters` evaluates all documents with their original typed values.\n</details>\n\n<details>\n<summary>Option 2: Preserve types in inverted index (more complex)</summary>\n\nChange the index type to handle typed keys and update all three methods accordingly. This requires careful equality comparison for boxed value types.\n</details>\n\nBased on learnings (past review comment flagged this same issue).\n\n\nAlso applies to: 249-286\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/AzureSearchDocumentStore.cs\naround lines 200-222 (and related logic at 249-286), the inverted index\ncurrently converts all metadata values to strings which causes false negatives\nfor non-string typed filters; update the logic so the index is only used for\nstring-valued metadata/filters: (1) in IndexMetadata only add entries for\nmetadata values that are already strings (skip indexing non-string values), and\n(2) in GetCandidateIds detect when any incoming filter value is non-string and\nreturn null (force a full-scan so MatchesFilters can evaluate typed\ncomparisons). Ensure these changes avoid changing stored Document.Metadata and\nonly affect whether the inverted index is consulted.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
          "path": "src/RetrievalAugmentedGeneration/DocumentStores/AzureSearchDocumentStore.cs",
          "line": 222
        }
      ]
    }
  },
  {
    "id": "PRRT_kwDOKSXUF85gsJyK",
    "isResolved": false,
    "comments": {
      "nodes": [
        {
          "id": "PRRC_kwDOKSXUF86UuPgi",
          "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Fix import ordering per pipeline failure.**\n\nThe pipeline reports a formatting issue with import ordering at line 1.\n\n\nEnsure using directives follow the project's style guidelines (typically: System namespaces first, then third-party, then project namespaces, all alphabetically sorted).\n\n<details>\n<summary>≡ƒº░ Tools</summary>\n\n<details>\n<summary>≡ƒ¬¢ GitHub Actions: Build</summary>\n\n[error] 1-1: Import/order formatting issue at line 1.\n\n</details>\n\n</details>\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/ChromaDBDocumentStore.cs\naround lines 1 to 11, the using directives are out of the project's expected\norder; reorder them so System namespaces come first (alphabetically), then\nthirdΓÇæparty namespaces (e.g., Newtonsoft.Json.Linq), then project namespaces\n(AiDotNet.*) alphabetically, and remove any extra blank lines so the block is\nconsistently sorted and formatted per the pipeline style rules.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
          "path": "src/RetrievalAugmentedGeneration/DocumentStores/ChromaDBDocumentStore.cs",
          "line": null
        }
      ]
    }
  },
  {
    "id": "PRRT_kwDOKSXUF85gsJzJ",
    "isResolved": false,
    "comments": {
      "nodes": [
        {
          "id": "PRRC_kwDOKSXUF86UuPh4",
          "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Implement Elasticsearch scroll API as documented.**\n\nThe XML documentation mentions using the scroll API for efficient retrieval, but the implementation only returns cached documents. This means:\n- After app restart, GetAll() returns an empty collection even if Elasticsearch has documents\n- Documents indexed directly in Elasticsearch won't appear\n- The cache can become stale\n\n\n\nConsider implementing the scroll API as documented:\n\n```csharp\nprotected override IEnumerable<Document<T>> GetAllCore()\n{\n    var results = new List<Document<T>>();\n    \n    try\n    {\n        // Initial search with scroll\n        var query = new { size = 1000, query = new { match_all = new { } } };\n        var content = new StringContent(\n            Newtonsoft.Json.JsonConvert.SerializeObject(query),\n            Encoding.UTF8,\n            \"application/json\");\n        \n        var response = _httpClient.PostAsync($\"/{_indexName}/_search?scroll=1m\", content).Result;\n        if (!response.IsSuccessStatusCode)\n            throw new InvalidOperationException($\"Initial scroll failed: {response.StatusCode}\");\n        \n        var responseContent = response.Content.ReadAsStringAsync().Result;\n        var result = JObject.Parse(responseContent);\n        var scrollId = result[\"_scroll_id\"]?.ToString();\n        \n        while (true)\n        {\n            var hits = result[\"hits\"]?[\"hits\"];\n            if (hits == null || !hits.Any())\n                break;\n            \n            foreach (var hit in hits)\n            {\n                var source = hit[\"_source\"];\n                if (source != null)\n                {\n                    var id = source[\"id\"]?.ToString() ?? string.Empty;\n                    var docContent = source[\"content\"]?.ToString() ?? string.Empty;\n                    var metadata = source[\"metadata\"]?.ToObject<Dictionary<string, object>>() \n                        ?? new Dictionary<string, object>();\n                    results.Add(new Document<T>(id, docContent, metadata));\n                }\n            }\n            \n            // Continue scroll\n            var scrollQuery = new { scroll = \"1m\", scroll_id = scrollId };\n            var scrollContent = new StringContent(\n                Newtonsoft.Json.JsonConvert.SerializeObject(scrollQuery),\n                Encoding.UTF8,\n                \"application/json\");\n            response = _httpClient.PostAsync(\"/_search/scroll\", scrollContent).Result;\n            responseContent = response.Content.ReadAsStringAsync().Result;\n            result = JObject.Parse(responseContent);\n        }\n        \n        // Clear scroll\n        if (scrollId != null)\n            _httpClient.DeleteAsync($\"/_search/scroll/{scrollId}\").Wait();\n    }\n    catch (Exception ex)\n    {\n        throw new InvalidOperationException($\"Error retrieving all documents: {ex.Message}\", ex);\n    }\n    \n    return results;\n}\n```\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
          "path": "src/RetrievalAugmentedGeneration/DocumentStores/ElasticsearchDocumentStore.cs",
          "line": 372
        }
      ]
    }
  },
  {
    "id": "PRRT_kwDOKSXUF85gsJzT",
    "isResolved": false,
    "comments": {
      "nodes": [
        {
          "id": "PRRC_kwDOKSXUF86UuPiH",
          "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Class name and documentation are fundamentally misleadingΓÇöimplementation is in-memory, not SQLite-based.**\n\nThe class is named `SQLiteVSSDocumentStore` and all XML documentation describes it as a \"SQLite-based vector store using the SQLite-VSS extension\" with \"file-based database\" storage, \"ACID transactions,\" and suitability for \"edge deployments, mobile apps.\" However, the actual implementation uses a transient in-memory `Dictionary<string, VectorDocument<T>>` (line 42) with no SQLite integration whatsoever.\n\n**Impact:**\n- Consumers expecting persistent, file-based storage will experience silent data loss on process restart\n- The class violates the Liskov Substitution PrincipleΓÇöit cannot substitute for a real document store in production\n- Documentation promises features (ACID, single-file backup, zero-config deployment) that are not delivered\n\n**Required actions:**\n\n1. **Rename the class** to reflect its actual behavior: `InMemoryDocumentStore<T>` or `DictionaryDocumentStore<T>`\n2. **Rewrite all XML documentation** to describe the in-memory, non-persistent nature\n3. **Remove references** to SQLite, file-based storage, edge deployment, database files, transactions, and ACID guarantees\n4. **Update constructor** to remove file-path parameters that suggest persistence (see separate comment)\n\nAlternatively, if SQLite-VSS integration is planned but deferred, mark this class `internal` or `abstract` until the real implementation is complete, and document it as a temporary in-memory stub.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/SQLiteVSSDocumentStore.cs\naround lines 11 to 40, the class is misnamed and documented as a SQLite-VSS\nfile-based store while the implementation is an in-memory Dictionary; rename the\nclass to InMemoryDocumentStore<T> (or DictionaryDocumentStore<T>), rewrite the\nXML summary/remarks to clearly state it is an in-memory, non-persistent store\n(remove any mention of SQLite, VSS, file-based storage, ACID, transactions,\nbackups, edge/mobile persistence), remove or change constructor parameters that\nimply a file path/persistence, and update visibility to internal or abstract if\nyou intend to mark it as a temporary stub; ensure all references/usages are\nupdated to the new name and documentation reflects its transient behavior.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
          "path": "src/RetrievalAugmentedGeneration/DocumentStores/SQLiteVSSDocumentStore.cs",
          "line": 40
        }
      ]
    }
  },
  {
    "id": "PRRT_kwDOKSXUF85gsaCW",
    "isResolved": false,
    "comments": {
      "nodes": [
        {
          "id": "PRRC_kwDOKSXUF86UumAe",
          "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Query term matching can produce false positives.**\n\nThe substring matching at line 166 will incorrectly boost sentences containing partial word matches. For example, query term \"car\" would match \"scar\" or \"cartoon\".\n\n\n\nConsider using word boundary matching:\n\n```diff\n             if (queryTerms != null && queryTerms.Count > 0)\n             {\n                 var sentenceLower = sentence.ToLowerInvariant();\n-                var matchCount = queryTerms.Count(qt => sentenceLower.Contains(qt));\n+                var sentenceTokens = Tokenize(sentenceLower);\n+                var matchCount = queryTerms.Count(qt => sentenceTokens.Contains(qt));\n                 if (matchCount > 0)\n                 {\n                     importance *= (1.0 + (matchCount * 0.5)); // Boost by 50% per matching query term\n                 }\n             }\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/ContextCompression/DocumentSummarizer.cs\naround lines 163 to 171, the current substring-based query term matching causes\nfalse positives (e.g., \"car\" matching \"scar\"); replace the simple .Contains\nchecks with word-boundary matching: build safe, case-insensitive matches by\nescaping each query term (Regex.Escape) and either compile a single alternation\nregex like @\"\\b(term1|term2|...)\\b\" with RegexOptions.CultureInvariant |\nRegexOptions.IgnoreCase | RegexOptions.Compiled or test each term with a\nper-term regex using \\b boundaries; then count matches based on regex.Matches\n(or Match collection per term) instead of substring Contains, ensuring\npunctuation and boundaries are respected and performance is considered by\nreusing the compiled regex.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
          "path": "src/RetrievalAugmentedGeneration/ContextCompression/DocumentSummarizer.cs",
          "line": 171
        }
      ]
    }
  },
  {
    "id": "PRRT_kwDOKSXUF85gsaCo",
    "isResolved": false,
    "comments": {
      "nodes": [
        {
          "id": "PRRC_kwDOKSXUF86UumA4",
          "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Performance bottleneck: TF-IDF statistics recomputed on every query.**\n\n`BuildTFIDFStatistics` is called on every retrieval, scanning the entire document collection and recomputing term frequencies, document frequencies, and TF-IDF vectors from scratch. For a corpus of N documents with average length M, this is O(N*M) on every query.\n\nFor production use with large document collections (10k+ documents), this will cause severe performance degradation. TF-IDF statistics should be computed once when documents are indexed and cached, only rebuilding when the document store changes.\n\n\n\nConsider one of these approaches:\n1. **Pre-compute and cache**: Build statistics in constructor or first query, cache in instance fields, and provide a method to refresh when documents change.\n2. **Event-based invalidation**: Subscribe to document store changes and invalidate cache when documents are added/removed.\n3. **Separate indexing phase**: Require explicit `BuildIndex()` call after documents are loaded, making the cost explicit to users.\n\nExample for approach 1:\n\n```diff\n public class TFIDFRetriever<T> : RetrieverBase<T>\n {\n     private readonly IDocumentStore<T> _documentStore;\n+    private Dictionary<string, Dictionary<string, T>>? _cachedTfidf;\n+    private int _lastDocumentCount;\n\n     // ... constructor ...\n\n     protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\n     {\n         var queryTerms = Tokenize(query);\n         var scores = new Dictionary<string, T>();\n\n         var candidates = _documentStore.GetSimilar(\n             new AiDotNet.LinearAlgebra.Vector<T>(new T[_documentStore.VectorDimension]), \n             _documentStore.DocumentCount\n         );\n\n         var candidatesList = candidates.ToList();\n         var candidatesById = candidatesList.ToDictionary(d => d.Id);\n-        var tfidf = BuildTFIDFStatistics(candidatesList);\n+        \n+        // Rebuild statistics only if document count changed\n+        if (_cachedTfidf == null || _lastDocumentCount != _documentStore.DocumentCount)\n+        {\n+            _cachedTfidf = BuildTFIDFStatistics(candidatesList);\n+            _lastDocumentCount = _documentStore.DocumentCount;\n+        }\n+        var tfidf = _cachedTfidf;\n\n         // ... rest of method ...\n     }\n }\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Retrievers/TFIDFRetriever.cs around lines\n36-38, BuildTFIDFStatistics is being called on every retrieval causing O(N*M)\nwork per query; change to compute and cache TF-IDF statistics once (e.g., in the\nconstructor or lazily on first query) storing term frequencies, document\nfrequencies and TF-IDF vectors in instance fields, and add a public RefreshIndex\nor InvalidateCache method to recompute when documents change; alternatively hook\ninto document-store change events to invalidate the cache so subsequent queries\nreuse the cached statistics instead of rebuilding them each time.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
          "path": "src/RetrievalAugmentedGeneration/Retrievers/TFIDFRetriever.cs",
          "line": 34
        }
      ]
    }
  },
  {
    "id": "PRRT_kwDOKSXUF85gtTzj",
    "isResolved": false,
    "comments": {
      "nodes": [
        {
          "id": "PRRC_kwDOKSXUF86Uv4iy",
          "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Critical: Add missing `using System.Linq;` directive.**\n\nThe code uses `TakeLast` on line 103, which is a LINQ extension method. Without this using directive, the code fails to compile with CS1061.\n\nApply this diff:\n\n```diff\n using AiDotNet.Interfaces;\n+using System.Linq;\n \n namespace AiDotNet.RetrievalAugmentedGeneration.QueryProcessors;\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\nusing AiDotNet.Interfaces;\nusing System.Linq;\n\nnamespace AiDotNet.RetrievalAugmentedGeneration.QueryProcessors;\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/QueryProcessors/QueryRewritingProcessor.cs\naround lines 1 to 3, the file is missing the `using System.Linq;` directive\nrequired for LINQ extension methods (e.g., TakeLast) causing CS1061; add `using\nSystem.Linq;` to the top alongside the other using directives (ensure it's\nplaced before the namespace declaration) so the LINQ extension methods compile\ncorrectly.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
          "path": "src/RetrievalAugmentedGeneration/QueryProcessors/QueryRewritingProcessor.cs",
          "line": 3
        }
      ]
    }
  },
  {
    "id": "PRRT_kwDOKSXUF85gtTzs",
    "isResolved": false,
    "comments": {
      "nodes": [
        {
          "id": "PRRC_kwDOKSXUF86Uv4i9",
          "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Topic extraction logic is too simplistic.**\n\nThe method returns only the last word, which often includes punctuation (\"AI?\") or non-topical words (\"used?\", \"are?\"). For the query \"How are transformers used?\", it extracts \"used?\" instead of \"transformers\". This degrades contextual rewriting quality.\n\n\nConsider a more robust approach:\n\n```diff\n private static string ExtractTopic(string query)\n {\n-    var words = query.Split(new[] { ' ' }, StringSplitOptions.RemoveEmptyEntries);\n+    // Remove punctuation and split\n+    var cleanQuery = new string(query.Where(c => char.IsLetterOrDigit(c) || char.IsWhiteSpace(c)).ToArray());\n+    var words = cleanQuery.Split(new[] { ' ' }, StringSplitOptions.RemoveEmptyEntries);\n     \n+    // Filter out common stop words and find noun-like words\n+    var stopWords = new HashSet<string> { \"what\", \"how\", \"is\", \"are\", \"was\", \"were\", \"the\", \"a\", \"an\", \"about\", \"used\", \"do\", \"does\" };\n+    var contentWords = words.Where(w => !stopWords.Contains(w.ToLowerInvariant())).ToArray();\n+    \n-    if (words.Length > 2)\n+    if (contentWords.Length > 0)\n     {\n-        return words[words.Length - 1];\n+        // Return last content word or join multi-word topics\n+        return contentWords[contentWords.Length - 1];\n     }\n \n     return string.Empty;\n }\n```\n\nNote: This requires `using System.Linq;` which you're already adding for line 103.\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    private static string ExtractTopic(string query)\n    {\n        // Remove punctuation and split\n        var cleanQuery = new string(query.Where(c => char.IsLetterOrDigit(c) || char.IsWhiteSpace(c)).ToArray());\n        var words = cleanQuery.Split(new[] { ' ' }, StringSplitOptions.RemoveEmptyEntries);\n        \n        // Filter out common stop words and find noun-like words\n        var stopWords = new HashSet<string> { \"what\", \"how\", \"is\", \"are\", \"was\", \"were\", \"the\", \"a\", \"an\", \"about\", \"used\", \"do\", \"does\" };\n        var contentWords = words.Where(w => !stopWords.Contains(w.ToLowerInvariant())).ToArray();\n        \n        if (contentWords.Length > 0)\n        {\n            // Return last content word or join multi-word topics\n            return contentWords[contentWords.Length - 1];\n        }\n\n        return string.Empty;\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/QueryProcessors/QueryRewritingProcessor.cs\naround lines 132 to 142, the ExtractTopic method naively returns the last token\n(often with punctuation or a stopword) which yields wrong topics; replace it\nwith logic that splits the query into tokens, trims leading/trailing punctuation\n(use char.IsLetterOrDigit), lowercases tokens, and then iterates tokens in\nreverse using LINQ to return the first token not in a small\nstopword/question-word set (e.g.,\n\"what\",\"how\",\"are\",\"is\",\"used\",\"do\",\"does\",\"the\",\"a\",\"an\",\"in\",\"on\",\"of\",\"for\",\"to\",\"why\",\"when\",\"where\",\"which\");\nif none found return string.Empty. Ensure you add/keep using System.Linq and\nkeep the method deterministic and null-safe.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
          "path": "src/RetrievalAugmentedGeneration/QueryProcessors/QueryRewritingProcessor.cs",
          "line": 143
        }
      ]
    }
  },
  {
    "id": "PRRT_kwDOKSXUF85gtTzy",
    "isResolved": false,
    "comments": {
      "nodes": [
        {
          "id": "PRRC_kwDOKSXUF86Uv4jF",
          "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Critical: Fuzzy matching compares against wrong dictionary side.**\n\nThe fuzzy matching compares the user's word against dictionary **keys** (known misspellings) rather than **values** (correct spellings). This is a design flaw that significantly limits correction capability.\n\n**Problem example:**\n- Dictionary entry: `{ \"retreival\" ΓåÆ \"retrieval\" }`\n- User types: `\"retrievel\"` (1 char from correct \"retrieval\")\n- Current behavior: Compares against key `\"retreival\"`, distance = 2, likely no match\n- Expected behavior: Should compare against value `\"retrieval\"`, distance = 1, corrects to \"retrieval\"\n\n**Secondary issue:** Non-deterministic tie-breaking when multiple corrections have the same edit distance. Dictionary iteration order is not guaranteed in older .NET versions.\n\n\n\nRefactor to compare against correct spellings (values) and add deterministic tie-breaking:\n\n```diff\n private string? FindFuzzyMatch(string word)\n {\n     string? bestMatch = null;\n     var minDistance = int.MaxValue;\n+    string? bestKey = null;\n\n-    foreach (var key in _corrections.Keys)\n+    foreach (var kvp in _corrections)\n     {\n-        var distance = LevenshteinDistance(word, key);\n+        // Compare against the CORRECT spelling (value), not the misspelling (key)\n+        var distance = LevenshteinDistance(word, kvp.Value);\n-        if (distance <= _maxEditDistance && distance < minDistance)\n+        if (distance <= _maxEditDistance && (distance < minDistance || \n+            (distance == minDistance && (bestKey == null || string.Compare(kvp.Key, bestKey, StringComparison.Ordinal) < 0))))\n         {\n             minDistance = distance;\n-            bestMatch = _corrections[key];\n+            bestMatch = kvp.Value;\n+            bestKey = kvp.Key;\n         }\n     }\n\n     return bestMatch;\n }\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/QueryProcessors/SpellCheckQueryProcessor.cs\naround lines 80-96, the fuzzy matcher currently computes edit distance between\nthe input word and dictionary keys (misspellings) instead of the dictionary\nvalues (correct spellings); change the loop to iterate over the set of unique\nvalues (_corrections.Values), compute LevenshteinDistance against each correct\nspelling, track the lowest distance, and set bestMatch to that correct spelling;\nto make tie-breaking deterministic, when distance equals minDistance choose the\nlexicographically smallest candidate (or use a stable sort by value) so behavior\nis consistent across runtimes, and ensure you still respect _maxEditDistance\nbefore returning bestMatch (null if none within limit).\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
          "path": "src/RetrievalAugmentedGeneration/QueryProcessors/SpellCheckQueryProcessor.cs",
          "line": 96
        }
      ]
    }
  },
  {
    "id": "PRRT_kwDOKSXUF85gtfoM",
    "isResolved": false,
    "comments": {
      "nodes": [
        {
          "id": "PRRC_kwDOKSXUF86UwItx",
          "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Add defensive error handling and timeout protection.**\n\nSeveral issues in this block:\n\n1. `execSync` has no timeout, which could cause the script to hang indefinitely.\n2. Line 40 assumes the response structure exists without null checks - will crash if the API returns an error or unexpected format.\n3. The catch block logs the error but allows processing to continue with potentially incomplete data.\n\n\n\nApply this diff to add timeout and null checks:\n\n```diff\n try {\n-    const result = JSON.parse(execSync(`gh api graphql -f query='${query}'`).toString());\n+    const result = JSON.parse(\n+        execSync(`gh api graphql -f query='${query}'`, { \n+            timeout: 30000,\n+            encoding: 'utf8'\n+        })\n+    );\n+    \n+    if (!result?.data?.repository?.pullRequest?.reviewThreads) {\n+        console.error('Unexpected API response structure');\n+        break;\n+    }\n+    \n     const threads = result.data.repository.pullRequest.reviewThreads;\n     \n     allThreads = allThreads.concat(threads.nodes);\n     hasNext = threads.pageInfo.hasNextPage;\n     cursor = threads.pageInfo.endCursor;\n     page++;\n } catch (e) {\n     console.error('Error:', e.message);\n     break;\n }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        try {\n            const result = JSON.parse(\n                execSync(`gh api graphql -f query='${query}'`, { \n                    timeout: 30000,\n                    encoding: 'utf8'\n                })\n            );\n            \n            if (!result?.data?.repository?.pullRequest?.reviewThreads) {\n                console.error('Unexpected API response structure');\n                break;\n            }\n            \n            const threads = result.data.repository.pullRequest.reviewThreads;\n            \n            allThreads = allThreads.concat(threads.nodes);\n            hasNext = threads.pageInfo.hasNextPage;\n            cursor = threads.pageInfo.endCursor;\n            page++;\n        } catch (e) {\n            console.error('Error:', e.message);\n            break;\n        }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn fetch_threads.js around lines 38 to 49, the block calling execSync needs\ntimeout protection and defensive null checks: invoke execSync with a timeout\noption (e.g. 30000 ms) to avoid hanging, verify that result, result.data,\nresult.data.repository, result.data.repository.pullRequest and\nresult.data.repository.pullRequest.reviewThreads exist before accessing\nnodes/pageInfo (and if missing, log a clear error and break/return), and change\nthe catch to stop further processing on error (log full error and exit/throw)\ninstead of continuing with potentially incomplete data.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
          "path": "fetch_threads.js",
          "line": 49
        }
      ]
    }
  },
  {
    "id": "PRRT_kwDOKSXUF85gtfoe",
    "isResolved": false,
    "comments": {
      "nodes": [
        {
          "id": "PRRC_kwDOKSXUF86UwIuG",
          "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Add null safety for comments access.**\n\nLine 59 accesses `t.comments.nodes[0]` without verifying that `nodes` exists and has elements. While the `if (comment)` check at line 60 provides some protection, the access could still fail if `nodes` is undefined.\n\n\n\nApply this diff for safer access:\n\n```diff\n console.log('\\n=== FIRST 10 UNRESOLVED ===');\n unresolved.slice(0, 10).forEach((t, i) => {\n-    const comment = t.comments.nodes[0];\n+    const comment = t.comments?.nodes?.[0];\n     if (comment) {\n         console.log(`\\n[${i+1}] ${t.id}`);\n         console.log(`File: ${comment.path}`);\n         console.log(`Preview: ${comment.body.substring(0, 100)}...`);\n     }\n });\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    // Show first 10 unresolved\n    console.log('\\n=== FIRST 10 UNRESOLVED ===');\n    unresolved.slice(0, 10).forEach((t, i) => {\n        const comment = t.comments?.nodes?.[0];\n        if (comment) {\n            console.log(`\\n[${i+1}] ${t.id}`);\n            console.log(`File: ${comment.path}`);\n            console.log(`Preview: ${comment.body.substring(0, 100)}...`);\n        }\n    });\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn fetch_threads.js around lines 56 to 65, accessing t.comments.nodes[0] can\nthrow if t.comments or t.comments.nodes is undefined; change the assignment to\nsafely access the first comment (e.g. use optional chaining like\nt.comments?.nodes?.[0] or explicitly check t.comments and\nArray.isArray(t.comments.nodes) and length>0) and then keep the existing if\n(comment) guard before logging to ensure null-safety.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
          "path": "fetch_threads.js",
          "line": 65
        }
      ]
    }
  },
  {
    "id": "PRRT_kwDOKSXUF85gtfou",
    "isResolved": false,
    "comments": {
      "nodes": [
        {
          "id": "PRRC_kwDOKSXUF86UwIua",
          "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Add a main guard before invoking `get_all_threads`.**  \nRight now, simply importing this module will hit the GitHub API and write `unresolved_final.json`, which is a surprising side effect anywhere the helper is reused. Gate the call behind the standard `if __name__ == \"__main__\":` guard so that the script only runs when executed directly.\n\n```diff\n-get_all_threads()\n+if __name__ == \"__main__\":\n+    get_all_threads()\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\nif __name__ == \"__main__\":\n    get_all_threads()\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn fetch_threads.py around line 52 the module invokes get_all_threads() at\nimport time causing side effects; wrap that invocation in a standard main guard\nby adding an if __name__ == \"__main__\": block and placing the get_all_threads()\ncall (and any top-level script-only logic like writing unresolved_final.json)\ninside it so the function only runs when the file is executed directly, not when\nimported.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
          "path": "fetch_threads.py",
          "line": 52
        }
      ]
    }
  },
  {
    "id": "PRRT_kwDOKSXUF85gtfo7",
    "isResolved": false,
    "comments": {
      "nodes": [
        {
          "id": "PRRC_kwDOKSXUF86UwIun",
          "body": "_≡ƒ¢á∩╕Å Refactor suggestion_ | _≡ƒƒá Major_\n\n<details>\n<summary>≡ƒº⌐ Analysis chain</summary>\n\n**Verify the necessity of the generic type parameter.**\n\nThe generic type parameter `T` is declared but unused for any computations within this classΓÇöit's only threaded through to `IGenerator<T>`. Query rewriting is inherently a string-based operation. While this may align with the broader RAG framework's generic architecture, consider whether a non-generic version or a different design (e.g., accepting `IGenerator<T>` via an explicit constraint or factory) would better communicate intent.\n\n\n\nRun the following script to check if other query processors in the RAG framework also use generic type parameters:\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Description: Check if other query processors use generic type parameters\n\n# Search for query processor class declarations\nrg -nP --type=cs '^public class \\w+.*QueryProcessor' -A2\n```\n\nLength of output: 2520\n\n---\n\n\n\n</details>\n\n**Remove unnecessary generic type parameter `<T>` from `QueryRewritingProcessor`.**\n\nThe script reveals that `QueryRewritingProcessor<T>` is the only generic query processor in the framework. All six peer processors (`StopWordRemovalQueryProcessor`, `SpellCheckQueryProcessor`, `QueryExpansionProcessor`, `LemmatizationQueryProcessor`, `KeywordExtractionQueryProcessor`, and `IdentityQueryProcessor`) are non-generic. Since `T` is unused for computations within the class and query processing is inherently string-based, the generic parameter breaks architectural consistency and should be removed. If the `IGenerator` needs type parameterization for embeddings or scoring, that concern belongs in the generator's design, not the query processor.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/QueryProcessors/QueryRewritingProcessor.cs\naround line 28, the class is declared as QueryRewritingProcessor<T> but the\ngeneric parameter T is unused and inconsistent with other non-generic query\nprocessors; remove the generic parameter by changing the class declaration to\nQueryRewritingProcessor (and update any internal/constructor references or type\nconstraints that mention T), ensure file-level using/constructor signatures and\nbase-class references compile without T, and run/adjust any callers or\nregistrations to use the non-generic QueryRewritingProcessor type.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
          "path": "src/RetrievalAugmentedGeneration/QueryProcessors/QueryRewritingProcessor.cs",
          "line": 28
        }
      ]
    }
  },
  {
    "id": "PRRT_kwDOKSXUF85gtfpF",
    "isResolved": false,
    "comments": {
      "nodes": [
        {
          "id": "PRRC_kwDOKSXUF86UwIuz",
          "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Consider case-insensitive basic rewrites.**\n\nThe replacements are case-sensitive, so queries starting with capitalized words (e.g., \"How r u\") won't be rewritten. While this is a minor issue given the downstream contextual rewriting, consider using case-insensitive replacements or adding capitalized variants to improve reliability.\n\n\n\nExample improvement using Regex:\n\n```diff\n private string ApplyBasicRewrites(string query)\n {\n+    var rewritten = query;\n-    var rewritten = query;\n-\n-    rewritten = rewritten.Replace(\" r \", \" are \");\n-    rewritten = rewritten.Replace(\" u \", \" you \");\n-    rewritten = rewritten.Replace(\"wht \", \"what \");\n-    rewritten = rewritten.Replace(\"hw \", \"how \");\n-    rewritten = rewritten.Replace(\" w/ \", \" with \");\n-    rewritten = rewritten.Replace(\" w/o \", \" without \");\n+    \n+    rewritten = System.Text.RegularExpressions.Regex.Replace(rewritten, @\"\\br\\b\", \"are\", System.Text.RegularExpressions.RegexOptions.IgnoreCase);\n+    rewritten = System.Text.RegularExpressions.Regex.Replace(rewritten, @\"\\bu\\b\", \"you\", System.Text.RegularExpressions.RegexOptions.IgnoreCase);\n+    rewritten = System.Text.RegularExpressions.Regex.Replace(rewritten, @\"\\bwht\\b\", \"what\", System.Text.RegularExpressions.RegexOptions.IgnoreCase);\n+    rewritten = System.Text.RegularExpressions.Regex.Replace(rewritten, @\"\\bhw\\b\", \"how\", System.Text.RegularExpressions.RegexOptions.IgnoreCase);\n+    rewritten = rewritten.Replace(\" w/ \", \" with \");\n+    rewritten = rewritten.Replace(\" w/o \", \" without \");\n \n     return rewritten.Trim();\n }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    private string ApplyBasicRewrites(string query)\n    {\n        var rewritten = query;\n        \n        rewritten = System.Text.RegularExpressions.Regex.Replace(rewritten, @\"\\br\\b\", \"are\", System.Text.RegularExpressions.RegexOptions.IgnoreCase);\n        rewritten = System.Text.RegularExpressions.Regex.Replace(rewritten, @\"\\bu\\b\", \"you\", System.Text.RegularExpressions.RegexOptions.IgnoreCase);\n        rewritten = System.Text.RegularExpressions.Regex.Replace(rewritten, @\"\\bwht\\b\", \"what\", System.Text.RegularExpressions.RegexOptions.IgnoreCase);\n        rewritten = System.Text.RegularExpressions.Regex.Replace(rewritten, @\"\\bhw\\b\", \"how\", System.Text.RegularExpressions.RegexOptions.IgnoreCase);\n        rewritten = rewritten.Replace(\" w/ \", \" with \");\n        rewritten = rewritten.Replace(\" w/o \", \" without \");\n\n        return rewritten.Trim();\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/QueryProcessors/QueryRewritingProcessor.cs\naround lines 76 to 88, the current string.Replace calls are case-sensitive so\nqueries like \"How r u\" are not rewritten; replace these with Regex.Replace calls\nusing word-boundary patterns (e.g. \\br\\b, \\bu\\b, \\bwht\\b, \\bhw\\b, and patterns\nfor \"w/\" and \"w/o\") and pass RegexOptions.IgnoreCase to make replacements\ncase-insensitive and robust, ensuring you use word boundaries to avoid partial\nmatches and keep the final Trim() call.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
          "path": "src/RetrievalAugmentedGeneration/QueryProcessors/QueryRewritingProcessor.cs",
          "line": 88
        }
      ]
    }
  },
  {
    "id": "PRRT_kwDOKSXUF85gtfpN",
    "isResolved": false,
    "comments": {
      "nodes": [
        {
          "id": "PRRC_kwDOKSXUF86UwIu7",
          "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Anchor the regex pattern to match only at the start.**\n\nLines 119-120 check `StartsWith(\"what about\")` or `StartsWith(\"how about\")`, but lines 124-125 use unanchored regex patterns that match these phrases anywhere in the query. This creates a mismatch:\n\n- Input: `\"I was wondering what about transformers\"`\n- Result: `\"I was wondering what about {topic} and transformers\"` (incorrect)\n\nAdditionally, both replacements run sequentially on the same result, so a query containing both phrases would be double-modified.\n\n\n\nApply this diff to anchor patterns and handle each case separately:\n\n```diff\n if (lowerQuery.StartsWith(\"what about\") || lowerQuery.StartsWith(\"how about\"))\n {\n     var topic = ExtractTopic(lastContext);\n     if (!string.IsNullOrEmpty(topic))\n     {\n-        var result = System.Text.RegularExpressions.Regex.Replace(query, \"what about\", $\"what about {topic} and\", System.Text.RegularExpressions.RegexOptions.IgnoreCase);\n-        result = System.Text.RegularExpressions.Regex.Replace(result, \"how about\", $\"how about {topic} and\", System.Text.RegularExpressions.RegexOptions.IgnoreCase);\n-        return result;\n+        if (lowerQuery.StartsWith(\"what about\"))\n+        {\n+            return System.Text.RegularExpressions.Regex.Replace(query, @\"^what about\\b\", $\"what about {topic} and\", System.Text.RegularExpressions.RegexOptions.IgnoreCase);\n+        }\n+        if (lowerQuery.StartsWith(\"how about\"))\n+        {\n+            return System.Text.RegularExpressions.Regex.Replace(query, @\"^how about\\b\", $\"how about {topic} and\", System.Text.RegularExpressions.RegexOptions.IgnoreCase);\n+        }\n     }\n }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        if (lowerQuery.StartsWith(\"what about\") || lowerQuery.StartsWith(\"how about\"))\n        {\n            var topic = ExtractTopic(lastContext);\n            if (!string.IsNullOrEmpty(topic))\n            {\n                if (lowerQuery.StartsWith(\"what about\"))\n                {\n                    return System.Text.RegularExpressions.Regex.Replace(query, @\"^what about\\b\", $\"what about {topic} and\", System.Text.RegularExpressions.RegexOptions.IgnoreCase);\n                }\n                if (lowerQuery.StartsWith(\"how about\"))\n                {\n                    return System.Text.RegularExpressions.Regex.Replace(query, @\"^how about\\b\", $\"how about {topic} and\", System.Text.RegularExpressions.RegexOptions.IgnoreCase);\n                }\n            }\n        }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/QueryProcessors/QueryRewritingProcessor.cs\naround lines 119-128, the regex replacements are unanchored and can match\nphrases anywhere and are both applied sequentially causing incorrect or double\nchanges; update the regex patterns to anchor to the start of the string (e.g.,\nprefix with ^) and perform each replacement only when the corresponding\nStartsWith check matched (i.e., run the \"what about\" replacement only when\nStartsWith(\"what about\") and similarly for \"how about\") so the replacement is\napplied once and only at the beginning of the query.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
          "path": "src/RetrievalAugmentedGeneration/QueryProcessors/QueryRewritingProcessor.cs",
          "line": 128
        }
      ]
    }
  },
  {
    "id": "PRRT_kwDOKSXUF85gtmjr",
    "isResolved": false,
    "comments": {
      "nodes": [
        {
          "id": "PRRC_kwDOKSXUF86UwSIU",
          "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Remove duplicate import.**\n\n`AiDotNet.Interfaces` is imported twice (lines 2 and 4).\n\n\nApply this diff to remove the duplicate:\n\n```diff\n using AiDotNet.Helpers;\n using AiDotNet.Interfaces;\n using AiDotNet.RetrievalAugmentedGeneration.Generators;\n-using AiDotNet.Interfaces;\n using AiDotNet.RetrievalAugmentedGeneration.Models;\n using AiDotNet.RetrievalAugmentedGeneration.Retrievers;\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\nusing AiDotNet.Helpers;\nusing AiDotNet.Interfaces;\nusing AiDotNet.RetrievalAugmentedGeneration.Generators;\nusing AiDotNet.RetrievalAugmentedGeneration.Models;\nusing AiDotNet.RetrievalAugmentedGeneration.Retrievers;\nusing System;\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Text;\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/AdvancedPatterns/FLARERetriever.cs lines 1\nto 10, there is a duplicate using directive for AiDotNet.Interfaces (appears on\nlines 2 and 4); remove the redundant duplicate import (delete one of the two\nusing AiDotNet.Interfaces lines) so each namespace is imported only once,\nkeeping the file's existing using order intact.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
          "path": "src/RetrievalAugmentedGeneration/AdvancedPatterns/FLARERetriever.cs",
          "line": 10
        }
      ]
    }
  },
  {
    "id": "PRRT_kwDOKSXUF85gtmjy",
    "isResolved": false,
    "comments": {
      "nodes": [
        {
          "id": "PRRC_kwDOKSXUF86UwSIc",
          "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n<details>\n<summary>≡ƒº⌐ Analysis chain</summary>\n\n**Fix the build failure: `ToDouble` method not found.**\n\nThe build is failing because `INumericOperations<T>` doesn't have a `ToDouble` method. The code at line 255 attempts to call `NumOps.ToDouble(d.RelevanceScore)`, but this method doesn't exist on the interface.\n\n\n\nVerify the correct conversion approach by checking the `INumericOperations<T>` interface definition:\n\n\n\n**Recommended solutions:**\n\n1. **If the interface has a different conversion method** (e.g., `ConvertToDouble`, `AsDouble`), use that method name instead.\n\n2. **If no conversion method exists**, add one to the interface and all implementations:\n   ```diff\n   -            .Select(d => NumOps.ToDouble(d.RelevanceScore))\n   +            .Select(d => Convert.ToDouble(NumOps.ToDouble(d.RelevanceScore)))\n   ```\n\n3. **Alternative**: Cast the relevance score directly if T is constrained:\n   ```diff\n   -            .Select(d => NumOps.ToDouble(d.RelevanceScore))\n   +            .Select(d => Convert.ToDouble(d.RelevanceScore))\n   ```\n   (Though past review comments indicate this breaks for `Complex<T>`)\n\nPlease verify which numeric conversion method is available and update line 255 accordingly.\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Description: Find the INumericOperations interface definition and check for conversion methods\n\n# Search for the INumericOperations interface definition\nast-grep --pattern 'interface INumericOperations<$_> {\n  $$$\n}'\n\n# Also search for any ToDouble methods in numeric operations\nrg -n \"ToDouble\" --type cs -C 3\n```\n\nLength of output: 246050\n\n---\n\n\n\n</details>\n\n**Replace `NumOps.ToDouble()` with `Convert.ToDouble()` at line 255.**\n\nThe build is failing because `NumOps.ToDouble()` does not exist. Review the codebase patterns: numeric conversions use `Convert.ToDouble()` throughout (200+ instances across serialization, regressions, neural networks, etc.).\n\nChange line 255 from:\n```csharp\n.Select(d => NumOps.ToDouble(d.RelevanceScore))\n```\n\nTo:\n```csharp\n.Select(d => Convert.ToDouble(d.RelevanceScore))\n```\n\nThis aligns with the established pattern used consistently across the entire codebase.\n\n<details>\n<summary>≡ƒº░ Tools</summary>\n\n<details>\n<summary>≡ƒ¬¢ GitHub Actions: Build</summary>\n\n[error] 255-255: CS1061: 'INumericOperations<T>' does not contain a definition for 'ToDouble' and no accessible extension method 'ToDouble' accepting a first argument of type 'INumericOperations<T>' could be found (are you missing a using directive or an assembly reference?)\n\n</details>\n<details>\n<summary>≡ƒ¬¢ GitHub Actions: Quality Gates (.NET)</summary>\n\n[error] 255-255: CS1061: 'INumericOperations<T>' does not contain a definition for 'ToDouble' and no accessible extension method 'ToDouble' accepting a first argument of type 'INumericOperations<T>' could be found (are you missing a using directive or an assembly reference?)\n\n</details>\n<details>\n<summary>≡ƒ¬¢ GitHub Check: Build All Frameworks</summary>\n\n[failure] 255-255: \n'INumericOperations<T>' does not contain a definition for 'ToDouble' and no accessible extension method 'ToDouble' accepting a first argument of type 'INumericOperations<T>' could be found (are you missing a using directive or an assembly reference?)\n\n---\n\n[failure] 255-255: \n'INumericOperations<T>' does not contain a definition for 'ToDouble' and no accessible extension method 'ToDouble' accepting a first argument of type 'INumericOperations<T>' could be found (are you missing a using directive or an assembly reference?)\n\n---\n\n[failure] 255-255: \n'INumericOperations<T>' does not contain a definition for 'ToDouble' and no accessible extension method 'ToDouble' accepting a first argument of type 'INumericOperations<T>' could be found (are you missing a using directive or an assembly reference?)\n\n</details>\n<details>\n<summary>≡ƒ¬¢ GitHub Check: Publish Size Analysis</summary>\n\n[failure] 255-255: \n'INumericOperations<T>' does not contain a definition for 'ToDouble' and no accessible extension method 'ToDouble' accepting a first argument of type 'INumericOperations<T>' could be found (are you missing a using directive or an assembly reference?)\n\n</details>\n\n</details>\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/AdvancedPatterns/FLARERetriever.cs around\nlines 233 to 261, the code uses NumOps.ToDouble(d.RelevanceScore) which does not\nexist and causes build failures; change this call to\nConvert.ToDouble(d.RelevanceScore) to match the established numeric conversion\npattern used across the codebase, ensuring using System is available or\nfully-qualify Convert if necessary.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
          "path": "src/RetrievalAugmentedGeneration/AdvancedPatterns/FLARERetriever.cs",
          "line": 261
        }
      ]
    }
  },
  {
    "id": "PRRT_kwDOKSXUF85gtmj5",
    "isResolved": false,
    "comments": {
      "nodes": [
        {
          "id": "PRRC_kwDOKSXUF86UwSIl",
          "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Add input validation to prevent invalid graph entries.**\n\nThe method accepts string parameters without validation. Null, empty, or whitespace-only values would pollute the knowledge graph and cause incorrect entity matching downstream.\n\n\n\nApply this diff:\n\n```diff\n public void AddRelation(string entity, string relation, string target)\n {\n+    if (string.IsNullOrWhiteSpace(entity))\n+        throw new ArgumentException(\"Entity cannot be null or whitespace\", nameof(entity));\n+    if (string.IsNullOrWhiteSpace(relation))\n+        throw new ArgumentException(\"Relation cannot be null or whitespace\", nameof(relation));\n+    if (string.IsNullOrWhiteSpace(target))\n+        throw new ArgumentException(\"Target cannot be null or whitespace\", nameof(target));\n+\n     if (!_knowledgeGraph.ContainsKey(entity))\n     {\n         _knowledgeGraph[entity] = new List<(string, string)>();\n     }\n     \n     _knowledgeGraph[entity].Add((relation, target));\n }\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/AdvancedPatterns/GraphRAG.cs around lines\n127 to 135, the AddRelation method accepts raw string inputs which may be null,\nempty, or whitespace and pollute the knowledge graph; validate each parameter\n(entity, relation, target) by checking for null and String.IsNullOrWhiteSpace,\ntrim them, and throw ArgumentNullException or ArgumentException with a clear\nparameter name when invalid; use the trimmed entity as the dictionary key and\nproceed to add the tuple (trimmed relation, trimmed target) to the list,\nensuring no null/empty entries are inserted.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
          "path": "src/RetrievalAugmentedGeneration/AdvancedPatterns/GraphRAG.cs",
          "line": 135
        }
      ]
    }
  },
  {
    "id": "PRRT_kwDOKSXUF85gtmj-",
    "isResolved": false,
    "comments": {
      "nodes": [
        {
          "id": "PRRC_kwDOKSXUF86UwSIs",
          "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Retrieve more than `topK` documents before boosting.**\n\nThe method retrieves only `topK` documents at line 198, then applies score boosts (lines 219-223). Documents initially ranked just outside the top-K might rank higher after boosting, but they're never retrieved. Consider retrieving `topK * 2` or `topK * 3` initially to allow for re-ranking.\n\n\n\nApply this diff:\n\n```diff\n-        // Step 3: Use vector retriever for unstructured text\n-        var vectorResults = _vectorRetriever.Retrieve(query, topK).ToList();\n+        // Step 3: Use vector retriever for unstructured text (retrieve more to allow for re-ranking)\n+        var vectorResults = _vectorRetriever.Retrieve(query, topK * 2).ToList();\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        // Step 3: Use vector retriever for unstructured text (retrieve more to allow for re-ranking)\n        var vectorResults = _vectorRetriever.Retrieve(query, topK * 2).ToList();\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
          "path": "src/RetrievalAugmentedGeneration/AdvancedPatterns/GraphRAG.cs",
          "line": 198
        }
      ]
    }
  },
  {
    "id": "PRRT_kwDOKSXUF85gtmkB",
    "isResolved": false,
    "comments": {
      "nodes": [
        {
          "id": "PRRC_kwDOKSXUF86UwSIx",
          "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Entity matching can produce false positives.**\n\nThe case-insensitive `Contains` check can incorrectly match unrelated terms. For example, searching for \"Einstein\" would match \"Einsteinium\" or \"Stein\" within other words.\n\n\n\nConsider using word-boundary matching:\n\n```diff\n             // Check if document mentions any of our graph entities\n             var mentionedEntities = relatedEntities\n-                .Where(entity => doc.Content.ToLower().Contains(entity.ToLower()))\n+                .Where(entity => Regex.IsMatch(doc.Content, $@\"\\b{Regex.Escape(entity)}\\b\", RegexOptions.IgnoreCase))\n                 .ToList();\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/AdvancedPatterns/GraphRAG.cs around lines\n208 to 210, the current case-insensitive substring check can yield false\npositives (e.g., \"Einstein\" matching \"Einsteinium\"); replace the Contains-based\nfilter with a word-boundary regex match for each entity. For each entity, build\na pattern using Regex.Escape(entity) wrapped with \\b (or appropriate\nUnicode-aware boundaries), use RegexOptions.IgnoreCase |\nRegexOptions.CultureInvariant (and RegexOptions.Compiled if reused) and match\nagainst doc.Content to ensure only whole-word matches are accepted. Ensure\nnull/empty entity strings are skipped and consider normalizing whitespace if\nneeded.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
          "path": "src/RetrievalAugmentedGeneration/AdvancedPatterns/GraphRAG.cs",
          "line": 210
        }
      ]
    }
  },
  {
    "id": "PRRT_kwDOKSXUF85gtmkG",
    "isResolved": false,
    "comments": {
      "nodes": [
        {
          "id": "PRRC_kwDOKSXUF86UwSI6",
          "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Add error handling for LLM entity extraction.**\n\nThe `_generator.Generate()` call at line 266 can throw exceptions or return unexpected formats. The parsing logic (lines 269-275) assumes a specific comma/newline-separated format. If the LLM fails or returns a different format, the method could crash or return poor results.\n\n\n\nApply this diff:\n\n```diff\n         // Use LLM for more sophisticated extraction if needed\n         if (entities.Count == 0)\n         {\n-            var extractionPrompt = $\"Extract the main entities (people, places, concepts) from: '{text}'\\nList them separated by commas.\";\n-            var llmResponse = _generator.Generate(extractionPrompt);\n-            \n-            // Parse comma-separated entities from LLM\n-            var llmEntities = llmResponse\n-                .Split(new[] { ',', '\\n' }, StringSplitOptions.RemoveEmptyEntries)\n-                .Select(e => e.Trim())\n-                .Where(e => e.Length > 2 && !e.StartsWith(\"[\"))\n-                .Take(10);\n-                \n-            entities.AddRange(llmEntities);\n+            try\n+            {\n+                var extractionPrompt = $\"Extract the main entities (people, places, concepts) from: '{text}'\\nList them separated by commas.\";\n+                var llmResponse = _generator.Generate(extractionPrompt);\n+                \n+                if (!string.IsNullOrWhiteSpace(llmResponse))\n+                {\n+                    // Parse comma-separated entities from LLM\n+                    var llmEntities = llmResponse\n+                        .Split(new[] { ',', '\\n' }, StringSplitOptions.RemoveEmptyEntries)\n+                        .Select(e => e.Trim())\n+                        .Where(e => e.Length > 2 && !e.StartsWith(\"[\"))\n+                        .Take(10);\n+                        \n+                    entities.AddRange(llmEntities);\n+                }\n+            }\n+            catch (Exception)\n+            {\n+                // Fall back to empty entity list if LLM extraction fails\n+                // The method will still work with an empty entity list (no graph boost)\n+            }\n         }\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/AdvancedPatterns/GraphRAG.cs around lines\n263 to 276, wrap the _generator.Generate() call in a try/catch and validate its\noutput before parsing: catch exceptions from Generate, log the error (or handle\nit gracefully) and skip extraction on failure; if a response is returned,\nnormalize it (null/empty check), strip surrounding\npunctuation/quotes/Markdown/list numbering, support both comma/newline-separated\nand line-item formats (split on commas, newlines and map/regex to extract\nwords), trim results, filter duplicates and items shorter than 3 chars, limit to\n10 entities, and only then AddRange; ensure any parse fallback yields an empty\nsafe list instead of throwing.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
          "path": "src/RetrievalAugmentedGeneration/AdvancedPatterns/GraphRAG.cs",
          "line": 276
        }
      ]
    }
  },
  {
    "id": "PRRT_kwDOKSXUF85gtmkR",
    "isResolved": false,
    "comments": {
      "nodes": [
        {
          "id": "PRRC_kwDOKSXUF86UwSJH",
          "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Chunks can exceed the configured max size**\n\nIf the text lacks natural boundaries (e.g., plain prose), the fallback path only splits once and then leaves a trailing chunk far larger than `_maxChunkSize`, defeating the size guarantee and breaking downstream window limits. Keep splitting until the remainder fits within `_maxChunkSize`.\n\n\nApply this diff:\n\n```diff\n                 if (shouldSplit)\n                 {\n-                    // Finalize current chunk\n-                    if (currentChunkEnd > currentChunkStart)\n-                    {\n-                        var chunkText = text.Substring(currentChunkStart, currentChunkEnd - currentChunkStart).Trim();\n-                        if (!string.IsNullOrWhiteSpace(chunkText))\n-                        {\n-                            chunks.Add((chunkText, currentChunkStart, currentChunkEnd));\n-                        }\n-\n-                        // Start new chunk with overlap\n-                        currentChunkStart = Math.Max(currentChunkStart, currentChunkEnd - ChunkOverlap);\n-                        currentChunkEnd = boundaryPos;\n-                    }\n-                    else\n-                    {\n-                        // First chunk, split at max size\n-                        var splitPos = Math.Min(currentChunkStart + _maxChunkSize, text.Length);\n-                        var chunkText = text.Substring(currentChunkStart, splitPos - currentChunkStart).Trim();\n-                        if (!string.IsNullOrWhiteSpace(chunkText))\n-                        {\n-                            chunks.Add((chunkText, currentChunkStart, splitPos));\n-                        }\n-                        currentChunkStart = Math.Max(currentChunkStart, splitPos - ChunkOverlap);\n-                        currentChunkEnd = boundaryPos;\n-                    }\n+                    if (currentChunkEnd > currentChunkStart)\n+                    {\n+                        var chunkText = text.Substring(currentChunkStart, currentChunkEnd - currentChunkStart).Trim();\n+                        if (!string.IsNullOrWhiteSpace(chunkText))\n+                        {\n+                            chunks.Add((chunkText, currentChunkStart, currentChunkEnd));\n+                        }\n+\n+                        currentChunkStart = Math.Max(currentChunkStart, currentChunkEnd - ChunkOverlap);\n+                    }\n+\n+                    while (boundaryPos - currentChunkStart > _maxChunkSize)\n+                    {\n+                        var splitPos = Math.Min(currentChunkStart + _maxChunkSize, boundaryPos);\n+                        var chunkText = text.Substring(currentChunkStart, splitPos - currentChunkStart).Trim();\n+                        if (!string.IsNullOrWhiteSpace(chunkText))\n+                        {\n+                            chunks.Add((chunkText, currentChunkStart, splitPos));\n+                        }\n+\n+                        currentChunkStart = Math.Max(currentChunkStart, splitPos - ChunkOverlap);\n+                    }\n+\n+                    currentChunkEnd = boundaryPos;\n                 }\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/ChunkingStrategies/AgenticChunker.cs around\nlines 166 to 236, the fallback branch that splits when no natural boundary\nexists only emits a single max-sized chunk and then leaves a trailing remainder\nthat can exceed _maxChunkSize; change the fallback to iteratively emit max-sized\nchunks (applying ChunkOverlap between successive chunks) until the remaining\nsegment length is <= _maxChunkSize, updating currentChunkStart/currentChunkEnd\neach iteration and adding each trimmed non-empty chunk to chunks, then set\ncurrentChunkEnd to the boundaryPos for the next outer-loop iteration.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
          "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/AgenticChunker.cs",
          "line": 236
        }
      ]
    }
  },
  {
    "id": "PRRT_kwDOKSXUF85gtmkW",
    "isResolved": false,
    "comments": {
      "nodes": [
        {
          "id": "PRRC_kwDOKSXUF86UwSJO",
          "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Build fails on target frameworks without Enumerable.ToHashSet**\n\n`ToHashSet()` isnΓÇÖt available on older frameworks we still target, so the project doesnΓÇÖt compile (see build failure). Construct the `HashSet<T>` explicitly instead.\n\n\nApply this diff:\n\n```diff\n-        var words1 = segment1.Split(new[] { ' ', '\\n', '\\r', '\\t', '.', ',', '!', '?' }, \n-            StringSplitOptions.RemoveEmptyEntries)\n-            .Where(w => w.Length > 2 && !stopWords.Contains(w))\n-            .Select(w => w.ToLowerInvariant())\n-            .ToHashSet();\n-\n-        var words2 = segment2.Split(new[] { ' ', '\\n', '\\r', '\\t', '.', ',', '!', '?' }, \n-            StringSplitOptions.RemoveEmptyEntries)\n-            .Where(w => w.Length > 2 && !stopWords.Contains(w))\n-            .Select(w => w.ToLowerInvariant())\n-            .ToHashSet();\n+        var words1 = new HashSet<string>(\n+            segment1.Split(new[] { ' ', '\\n', '\\r', '\\t', '.', ',', '!', '?' },\n+                StringSplitOptions.RemoveEmptyEntries)\n+                .Where(w => w.Length > 2 && !stopWords.Contains(w))\n+                .Select(w => w.ToLowerInvariant()));\n+\n+        var words2 = new HashSet<string>(\n+            segment2.Split(new[] { ' ', '\\n', '\\r', '\\t', '.', ',', '!', '?' },\n+                StringSplitOptions.RemoveEmptyEntries)\n+                .Where(w => w.Length > 2 && !stopWords.Contains(w))\n+                .Select(w => w.ToLowerInvariant()));\n...\n-        var entities1 = Regex.Matches(segment1, @\"\\b[A-Z][a-z]+(?:\\s[A-Z][a-z]+)*\\b\")\n-            .Cast<Match>()\n-            .Select(m => m.Value)\n-            .ToHashSet();\n-            \n-        var entities2 = Regex.Matches(segment2, @\"\\b[A-Z][a-z]+(?:\\s[A-Z][a-z]+)*\\b\")\n-            .Cast<Match>()\n-            .Select(m => m.Value)\n-            .ToHashSet();\n+        var entities1 = new HashSet<string>(\n+            Regex.Matches(segment1, @\"\\b[A-Z][a-z]+(?:\\s[A-Z][a-z]+)*\\b\")\n+                .Cast<Match>()\n+                .Select(m => m.Value));\n+            \n+        var entities2 = new HashSet<string>(\n+            Regex.Matches(segment2, @\"\\b[A-Z][a-z]+(?:\\s[A-Z][a-z]+)*\\b\")\n+                .Cast<Match>()\n+                .Select(m => m.Value));\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        var words1 = new HashSet<string>(\n            segment1.Split(new[] { ' ', '\\n', '\\r', '\\t', '.', ',', '!', '?' },\n                StringSplitOptions.RemoveEmptyEntries)\n                .Where(w => w.Length > 2 && !stopWords.Contains(w))\n                .Select(w => w.ToLowerInvariant()));\n\n        var words2 = new HashSet<string>(\n            segment2.Split(new[] { ' ', '\\n', '\\r', '\\t', '.', ',', '!', '?' },\n                StringSplitOptions.RemoveEmptyEntries)\n                .Where(w => w.Length > 2 && !stopWords.Contains(w))\n                .Select(w => w.ToLowerInvariant()));\n\n        if (words1.Count == 0 || words2.Count == 0)\n            return 0.0;\n\n        // Calculate Jaccard similarity (intersection over union)\n        var intersection = words1.Intersect(words2).Count();\n        var union = words1.Union(words2).Count();\n        var jaccardScore = union > 0 ? (double)intersection / union : 0.0;\n\n        // Check for discourse markers (transition words) at start of segment2\n        var segment2Trimmed = segment2.TrimStart();\n        var discourseMarkers = new[] \n        { \n            \"however\", \"therefore\", \"thus\", \"furthermore\", \"moreover\", \"additionally\",\n            \"consequently\", \"nevertheless\", \"meanwhile\", \"similarly\", \"likewise\",\n            \"in contrast\", \"on the other hand\", \"as a result\", \"for example\", \"for instance\"\n        };\n        \n        var hasTransition = discourseMarkers.Any(marker => \n            segment2Trimmed.StartsWith(marker, StringComparison.OrdinalIgnoreCase));\n        var transitionBonus = hasTransition ? 0.15 : 0.0;\n\n        // Check for entity continuity (capitalized words that might be names/entities)\n        var entities1 = new HashSet<string>(\n            Regex.Matches(segment1, @\"\\b[A-Z][a-z]+(?:\\s[A-Z][a-z]+)*\\b\")\n                .Cast<Match>()\n                .Select(m => m.Value));\n            \n        var entities2 = new HashSet<string>(\n            Regex.Matches(segment2, @\"\\b[A-Z][a-z]+(?:\\s[A-Z][a-z]+)*\\b\")\n                .Cast<Match>()\n                .Select(m => m.Value));\n\n        var entityOverlap = entities1.Count > 0 && entities2.Count > 0\n            ? (double)entities1.Intersect(entities2).Count() / Math.Max(entities1.Count, entities2.Count)\n            : 0.0;\n\n        // Weighted combination of signals\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒº░ Tools</summary>\n\n<details>\n<summary>≡ƒ¬¢ GitHub Check: Build All Frameworks</summary>\n\n[failure] 313-313: \n'IEnumerable<string>' does not contain a definition for 'ToHashSet' and no accessible extension method 'ToHashSet' accepting a first argument of type 'IEnumerable<string>' could be found (are you missing a using directive or an assembly reference?)\n\n---\n\n[failure] 286-286: \n'IEnumerable<string>' does not contain a definition for 'ToHashSet' and no accessible extension method 'ToHashSet' accepting a first argument of type 'IEnumerable<string>' could be found (are you missing a using directive or an assembly reference?)\n\n---\n\n[failure] 280-280: \n'IEnumerable<string>' does not contain a definition for 'ToHashSet' and no accessible extension method 'ToHashSet' accepting a first argument of type 'IEnumerable<string>' could be found (are you missing a using directive or an assembly reference?)\n\n---\n\n[failure] 318-318: \n'IEnumerable<string>' does not contain a definition for 'ToHashSet' and no accessible extension method 'ToHashSet' accepting a first argument of type 'IEnumerable<string>' could be found (are you missing a using directive or an assembly reference?)\n\n---\n\n[failure] 313-313: \n'IEnumerable<string>' does not contain a definition for 'ToHashSet' and no accessible extension method 'ToHashSet' accepting a first argument of type 'IEnumerable<string>' could be found (are you missing a using directive or an assembly reference?)\n\n---\n\n[failure] 286-286: \n'IEnumerable<string>' does not contain a definition for 'ToHashSet' and no accessible extension method 'ToHashSet' accepting a first argument of type 'IEnumerable<string>' could be found (are you missing a using directive or an assembly reference?)\n\n---\n\n[failure] 280-280: \n'IEnumerable<string>' does not contain a definition for 'ToHashSet' and no accessible extension method 'ToHashSet' accepting a first argument of type 'IEnumerable<string>' could be found (are you missing a using directive or an assembly reference?)\n\n</details>\n\n</details>\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/ChunkingStrategies/AgenticChunker.cs around\nlines 276 to 324, the use of Enumerable.ToHashSet() is breaking builds on older\ntarget frameworks; replace the ToHashSet() calls with explicit HashSet<string>\nconstructions (e.g., new HashSet<string>(...)) for words1, words2, entities1 and\nentities2 so the code compiles across all targeted frameworks, preserving the\nsame filtering/selection pipeline before constructing the HashSet.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
          "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/AgenticChunker.cs",
          "line": 324
        }
      ]
    }
  },
  {
    "id": "PRRT_kwDOKSXUF85gtmkj",
    "isResolved": false,
    "comments": {
      "nodes": [
        {
          "id": "PRRC_kwDOKSXUF86UwSJb",
          "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Position tracking is broken due to whitespace normalization.**\n\nThe position tracking logic has a fundamental flaw: chunks are created using `string.Join(\" \", currentChunk)` which normalizes whitespace to single spaces, but the original text may contain multiple spaces, tabs, newlines, or other whitespace between sentences. This causes `IndexOf` on line 149 to frequently return -1 because the exact joined string doesn't exist in the original text.\n\n**Example:**\n```csharp\n// Original text with varied whitespace:\n\"Sentence one.  Sentence two.\\nSentence three.\"\n\n// Chunk created by Join:\n\"Sentence one. Sentence two.\"  // single spaces\n\n// IndexOf fails ΓåÆ falls back to searchPos ΓåÆ wrong positions\n```\n\nAdditionally, even when `IndexOf` succeeds with overlapping chunks, advancing `searchPos` by only 1 (line 160) can cause it to find the overlapped portion instead of the new chunk's actual start position.\n\n\n\n**Solution:** Track sentence positions during `SplitIntoSentences` and maintain those positions through chunking, or build chunks by tracking character ranges instead of joining strings.\n\n```csharp\n// In SplitIntoSentences, return positions:\nprivate List<(string Sentence, int Start, int End)> SplitIntoSentences(string text)\n{\n    // Track positions while splitting...\n}\n\n// In ChunkCore, use those positions:\nforeach (var sentencesInChunk in /* grouping logic */)\n{\n    var startPos = sentencesInChunk.First().Start;\n    var endPos = sentencesInChunk.Last().End;\n    var chunk = text.Substring(startPos, endPos - startPos);\n    results.Add((chunk, startPos, endPos));\n}\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn\nsrc/RetrievalAugmentedGeneration/ChunkingStrategies/SentenceChunkingStrategy.cs\naround lines 142 to 164, the current approach joins sentences with single spaces\nand then uses IndexOf to find that joined string in the original text, which\nfails when original whitespace differs and leads to wrong positions and\nincorrect searchPos advancement; instead, modify SplitIntoSentences to return\nsentence text with exact start/end character offsets, update chunking to build\nchunks using those character ranges (e.g., chunk start = first sentence start,\nchunk end = last sentence end) and extract chunk with text.Substring(start,\nend-start), and set searchPos to the chunk end (or next sentence start) rather\nthan advancing by 1 so positions are always accurate and robust to whitespace\ndifferences.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
          "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/SentenceChunkingStrategy.cs",
          "line": 164
        }
      ]
    }
  },
  {
    "id": "PRRT_kwDOKSXUF85gtmko",
    "isResolved": false,
    "comments": {
      "nodes": [
        {
          "id": "PRRC_kwDOKSXUF86UwSJh",
          "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Critical: Documentation falsely claims \"production-ready\" LSTM-based generation.**\n\nThe class documentation states this generator \"uses an LSTM-based neural network architecture\" and \"processes through LSTM layers\" (lines 18, 33), but the actual implementation never invokes the LSTM network. Lines 273-305 use simplistic `Random`-based arithmetic instead of neural network forward passes. This misleads users into believing they're using a trained language model when they're getting random token generation.\n\n\n\n\nEither:\n- Implement actual LSTM-based generation using the `_network` field and `_temperature` for sampling, or\n- Update documentation to clearly state this is a simplified stub/prototype implementation, remove \"production-ready\" claims, and note that the LSTM network parameter is currently unused\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Generators/NeuralGenerator.cs around lines\n12 to 51, the XML doc falsely claims a \"production-ready\" LSTM-based generator\nand that the class ΓÇ£processes through LSTM layersΓÇ¥ even though the\nimplementation uses Random-based arithmetic and never calls the _network; update\nthe documentation to clearly state this is a simplified/stub/prototype\ngenerator, remove or change \"production-ready\" and explicit LSTM-layer claims,\nadd a short note that the _network field is currently unused and that true LSTM\nforward-pass sampling (using _network and _temperature) is TODO or to be\nimplemented in a future version, and include guidance for callers (e.g., not for\nproduction use) and where to find/implement the real neural-forward logic.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
          "path": "src/RetrievalAugmentedGeneration/Generators/NeuralGenerator.cs",
          "line": 51
        }
      ]
    }
  },
  {
    "id": "PRRT_kwDOKSXUF85gtmkt",
    "isResolved": false,
    "comments": {
      "nodes": [
        {
          "id": "PRRC_kwDOKSXUF86UwSJr",
          "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Critical: Detokenization produces meaningless output due to random token generation.**\n\nThe detokenization logic correctly maps token IDs to words (lines 256-261), but the tokens being detokenized are generated randomly by `PredictNextToken` (lines 294-305: `(lastToken + random.Next(1, 100)) % _vocabularySize`). This means the output will be:\n\n- Random words from the vocabulary (if random token IDs happen to match vocabulary entries)\n- `\"<unknown>\"` placeholders (if tokens don't exist in vocabulary)\n\nEither way, the result is gibberishΓÇönot coherent language. For example, a query might produce output like: `\"hello <unknown> context world <unknown> answer\"` with no semantic meaning.\n\n\n\nThis contradicts the \"production-ready\" and \"generates fluent, natural responses\" claims (lines 13, 41). The issue stems from `PredictNextToken` not using the LSTM network.\n\n**Fix**: Once `PredictNextToken` is updated to use the neural network (per earlier comment), detokenization will map meaningful predicted tokens back to coherent text.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Generators/NeuralGenerator.cs around lines\n249 to 271, DetokenizeText is fine but produces gibberish because\nPredictNextToken currently returns random token IDs instead of using the LSTM;\nreplace the random token generation with logic that runs the trained LSTM/NN\nforward pass to produce output logits, convert logits to token probabilities and\nselect a token (argmax or sampling with temperature/top-k as desired),\nclamp/normalize the selected token ID to the valid vocabulary range, and return\nthat token ID so DetokenizeText maps meaningful predicted tokens back to\ncoherent text.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
          "path": "src/RetrievalAugmentedGeneration/Generators/NeuralGenerator.cs",
          "line": 271
        }
      ]
    }
  },
  {
    "id": "PRRT_kwDOKSXUF85gtuAZ",
    "isResolved": false,
    "comments": {
      "nodes": [
        {
          "id": "PRRC_kwDOKSXUF86UwchM",
          "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Critical: IEntityExtractor<T> interface not found - build is broken.**\n\nThe build fails because `IEntityExtractor<T>` cannot be resolved (lines 58, 86). This interface must be defined in the `AiDotNet.Interfaces` namespace or explicitly imported. The error blocks compilation across all target frameworks.\n\n\n\nVerify the interface exists and is accessible:\n\n```shell\n#!/bin/bash\n# Search for IEntityExtractor interface definition\nast-grep --pattern $'interface IEntityExtractor<$_> {\n  $$$\n}'\n\n# If not found, search for any EntityExtractor types\nrg -n --type=cs 'interface.*EntityExtractor' -C 3\n```\n\n\nAlso applies to: 58-58, 86-86\n\n<details>\n<summary>≡ƒº░ Tools</summary>\n\n<details>\n<summary>≡ƒ¬¢ GitHub Actions: Build</summary>\n\n[error] 1-1: dotnet build failed due to previous error.\n\n</details>\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
          "path": "src/RetrievalAugmentedGeneration/Retrievers/GraphRetriever.cs",
          "line": 2
        }
      ]
    }
  }
]
