{"data":{"repository":{"pullRequest":{"reviewThreads":{"nodes":[{"id":"PRRT_kwDOKSXUF85hck9Y","isResolved":false,"isOutdated":false,"path":"src/Autodiff/TensorOperations.cs","line":1518,"originalLine":1518,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzD9Y","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement write to the same variable - consider using '?' to express intent better.\n```suggestion\n                    nodes[i].Gradient = nodes[i].Gradient == null\n                        ? gradPart\n                        : nodes[i].Gradient.Add(gradPart);\n```","createdAt":"2025-11-11T07:53:23Z","path":"src/Autodiff/TensorOperations.cs","line":1518,"originalLine":1518,"diffHunk":"@@ -1219,4 +1219,1769 @@ void BackwardFunction(Tensor<T> gradient)\n \n         return node;\n     }\n+\n+    /// <summary>\n+    /// Computes the softmax function for a computation node along a specified axis.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"axis\">The axis along which to compute softmax. Default is -1 (last axis).</param>\n+    /// <returns>A new computation node containing the softmax result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method computes softmax (σ(x_i) = exp(x_i) / Σexp(x_j)) along the specified axis.\n+    /// Uses numerical stability trick: subtract max before exponentiating.\n+    /// The backward function uses: ∂softmax/∂x = softmax(x) * (grad - Σ(grad * softmax(x))).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Softmax converts a vector of numbers into probabilities.\n+    ///\n+    /// For softmax:\n+    /// - The forward pass exponentiates each element, then normalizes so they sum to 1\n+    /// - The result is a probability distribution (all values between 0 and 1, summing to 1)\n+    /// - The backward pass is complex but efficient: uses the Jacobian of softmax\n+    ///\n+    /// Softmax is crucial for:\n+    /// - Multi-class classification (final layer outputs)\n+    /// - Attention mechanisms (computing attention weights)\n+    /// - Anywhere you need to convert scores to probabilities\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Softmax(ComputationNode<T> a, int axis = -1)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        // Normalize axis to positive index\n+        if (axis < 0)\n+            axis = shape.Length + axis;\n+\n+        // For simplicity, handle 2D case (batch, features) with axis=-1\n+        if (shape.Length == 2 && axis == 1)\n+        {\n+            int batchSize = shape[0];\n+            int features = shape[1];\n+            var result = new Tensor<T>(shape);\n+\n+            // Compute softmax for each row\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                // Find max for numerical stability\n+                var maxVal = a.Value[b, 0];\n+                for (int f = 1; f < features; f++)\n+                {\n+                    if (numOps.GreaterThan(a.Value[b, f], maxVal))\n+                        maxVal = a.Value[b, f];\n+                }\n+\n+                // Compute exp(x - max) and sum\n+                var expSum = numOps.Zero;\n+                var expValues = new T[features];\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var shifted = numOps.Subtract(a.Value[b, f], maxVal);\n+                    expValues[f] = numOps.Exp(shifted);\n+                    expSum = numOps.Add(expSum, expValues[f]);\n+                }\n+\n+                // Normalize\n+                for (int f = 0; f < features; f++)\n+                {\n+                    result[b, f] = numOps.Divide(expValues[f], expSum);\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (a.RequiresGradient)\n+                {\n+                    // ∂softmax/∂x_i = softmax_i * (∂L/∂y_i - Σ_j(∂L/∂y_j * softmax_j))\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        // Compute sum of (gradient * softmax)\n+                        var dotProduct = numOps.Zero;\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            dotProduct = numOps.Add(dotProduct,\n+                                numOps.Multiply(gradient[b, f], result[b, f]));\n+                        }\n+\n+                        // Compute gradient for each element\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            var gradMinusDot = numOps.Subtract(gradient[b, f], dotProduct);\n+                            gradA[b, f] = numOps.Multiply(result[b, f], gradMinusDot);\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient,\n+                parents: new List<ComputationNode<T>> { a },\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Softmax is currently only implemented for 2D tensors along axis=-1. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}], axis={axis}\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Concatenates multiple computation nodes along a specified axis.\n+    /// </summary>\n+    /// <param name=\"nodes\">The list of nodes to concatenate.</param>\n+    /// <param name=\"axis\">The axis along which to concatenate. Default is 0.</param>\n+    /// <returns>A new computation node containing the concatenated result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method concatenates tensors along the specified axis.\n+    /// All tensors must have the same shape except along the concatenation axis.\n+    /// The backward function splits the gradient and sends each portion to the corresponding input.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Concat stacks tensors together along a dimension.\n+    ///\n+    /// For concatenation:\n+    /// - The forward pass combines multiple tensors into one larger tensor\n+    /// - The backward pass splits the gradient back to each input\n+    /// - Think of it like gluing arrays together end-to-end\n+    ///\n+    /// Used in:\n+    /// - Skip connections (concatenating features from different layers)\n+    /// - Multi-input architectures\n+    /// - Feature fusion in neural networks\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Concat(List<ComputationNode<T>> nodes, int axis = 0)\n+    {\n+        if (nodes.Count == 0)\n+            throw new ArgumentException(\"Cannot concatenate empty list of nodes\");\n+\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var firstShape = nodes[0].Value.Shape;\n+\n+        // Normalize axis\n+        if (axis < 0)\n+            axis = firstShape.Length + axis;\n+\n+        // Validate shapes match except on concat axis\n+        for (int i = 1; i < nodes.Count; i++)\n+        {\n+            var shape = nodes[i].Value.Shape;\n+            if (shape.Length != firstShape.Length)\n+                throw new ArgumentException(\"All tensors must have the same rank\");\n+\n+            for (int d = 0; d < firstShape.Length; d++)\n+            {\n+                if (d != axis && shape[d] != firstShape[d])\n+                    throw new ArgumentException(\n+                        $\"Shape mismatch at dimension {d}: {shape[d]} vs {firstShape[d]}\");\n+            }\n+        }\n+\n+        // Compute output shape\n+        int[] outputShape = (int[])firstShape.Clone();\n+        for (int i = 1; i < nodes.Count; i++)\n+        {\n+            outputShape[axis] += nodes[i].Value.Shape[axis];\n+        }\n+\n+        // Perform concatenation (handle 2D case for simplicity)\n+        Tensor<T> result;\n+        if (firstShape.Length == 2 && axis == 1)\n+        {\n+            // Concatenate along columns (features)\n+            int rows = firstShape[0];\n+            int totalCols = outputShape[1];\n+            result = new Tensor<T>(new int[] { rows, totalCols });\n+\n+            int colOffset = 0;\n+            foreach (var node in nodes)\n+            {\n+                int cols = node.Value.Shape[1];\n+                for (int r = 0; r < rows; r++)\n+                {\n+                    for (int c = 0; c < cols; c++)\n+                    {\n+                        result[r, colOffset + c] = node.Value[r, c];\n+                    }\n+                }\n+                colOffset += cols;\n+            }\n+        }\n+        else if (firstShape.Length == 2 && axis == 0)\n+        {\n+            // Concatenate along rows (batch)\n+            int cols = firstShape[1];\n+            int totalRows = outputShape[0];\n+            result = new Tensor<T>(new int[] { totalRows, cols });\n+\n+            int rowOffset = 0;\n+            foreach (var node in nodes)\n+            {\n+                int rows = node.Value.Shape[0];\n+                for (int r = 0; r < rows; r++)\n+                {\n+                    for (int c = 0; c < cols; c++)\n+                    {\n+                        result[rowOffset + r, c] = node.Value[r, c];\n+                    }\n+                }\n+                rowOffset += rows;\n+            }\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Concat is currently only implemented for 2D tensors. \" +\n+                $\"Got shape=[{string.Join(\", \", firstShape)}]\");\n+        }\n+\n+        // Store sizes for gradient splitting\n+        var sizes = nodes.Select(n => n.Value.Shape[axis]).ToList();\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            // Split gradient along concat axis and distribute to inputs\n+            if (firstShape.Length == 2 && axis == 1)\n+            {\n+                int rows = firstShape[0];\n+                int colOffset = 0;\n+                for (int i = 0; i < nodes.Count; i++)\n+                {\n+                    if (!nodes[i].RequiresGradient)\n+                    {\n+                        colOffset += sizes[i];\n+                        continue;\n+                    }\n+\n+                    int cols = sizes[i];\n+                    var gradPart = new Tensor<T>(new int[] { rows, cols });\n+\n+                    for (int r = 0; r < rows; r++)\n+                    {\n+                        for (int c = 0; c < cols; c++)\n+                        {\n+                            gradPart[r, c] = gradient[r, colOffset + c];\n+                        }\n+                    }\n+\n+                    if (nodes[i].Gradient == null)\n+                        nodes[i].Gradient = gradPart;\n+                    else\n+                        nodes[i].Gradient = nodes[i].Gradient.Add(gradPart);\n+\n+                    colOffset += cols;\n+                }\n+            }\n+            else if (firstShape.Length == 2 && axis == 0)\n+            {\n+                int cols = firstShape[1];\n+                int rowOffset = 0;\n+                for (int i = 0; i < nodes.Count; i++)\n+                {\n+                    if (!nodes[i].RequiresGradient)\n+                    {\n+                        rowOffset += sizes[i];\n+                        continue;\n+                    }\n+\n+                    int rows = sizes[i];\n+                    var gradPart = new Tensor<T>(new int[] { rows, cols });\n+\n+                    for (int r = 0; r < rows; r++)\n+                    {\n+                        for (int c = 0; c < cols; c++)\n+                        {\n+                            gradPart[r, c] = gradient[rowOffset + r, c];\n+                        }\n+                    }\n+\n+                    if (nodes[i].Gradient == null)\n+                        nodes[i].Gradient = gradPart;\n+                    else\n+                        nodes[i].Gradient = nodes[i].Gradient.Add(gradPart);"}]}},{"id":"PRRT_kwDOKSXUF85hck9k","isResolved":false,"isOutdated":false,"path":"src/Autodiff/TensorOperations.cs","line":1626,"originalLine":1626,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzD9o","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement write to the same variable - consider using '?' to express intent better.\n```suggestion\n                    a.Gradient = (a.Gradient == null) ? gradA : a.Gradient.Add(gradA);\n```","createdAt":"2025-11-11T07:53:23Z","path":"src/Autodiff/TensorOperations.cs","line":1626,"originalLine":1626,"diffHunk":"@@ -1219,4 +1219,1769 @@ void BackwardFunction(Tensor<T> gradient)\n \n         return node;\n     }\n+\n+    /// <summary>\n+    /// Computes the softmax function for a computation node along a specified axis.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"axis\">The axis along which to compute softmax. Default is -1 (last axis).</param>\n+    /// <returns>A new computation node containing the softmax result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method computes softmax (σ(x_i) = exp(x_i) / Σexp(x_j)) along the specified axis.\n+    /// Uses numerical stability trick: subtract max before exponentiating.\n+    /// The backward function uses: ∂softmax/∂x = softmax(x) * (grad - Σ(grad * softmax(x))).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Softmax converts a vector of numbers into probabilities.\n+    ///\n+    /// For softmax:\n+    /// - The forward pass exponentiates each element, then normalizes so they sum to 1\n+    /// - The result is a probability distribution (all values between 0 and 1, summing to 1)\n+    /// - The backward pass is complex but efficient: uses the Jacobian of softmax\n+    ///\n+    /// Softmax is crucial for:\n+    /// - Multi-class classification (final layer outputs)\n+    /// - Attention mechanisms (computing attention weights)\n+    /// - Anywhere you need to convert scores to probabilities\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Softmax(ComputationNode<T> a, int axis = -1)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        // Normalize axis to positive index\n+        if (axis < 0)\n+            axis = shape.Length + axis;\n+\n+        // For simplicity, handle 2D case (batch, features) with axis=-1\n+        if (shape.Length == 2 && axis == 1)\n+        {\n+            int batchSize = shape[0];\n+            int features = shape[1];\n+            var result = new Tensor<T>(shape);\n+\n+            // Compute softmax for each row\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                // Find max for numerical stability\n+                var maxVal = a.Value[b, 0];\n+                for (int f = 1; f < features; f++)\n+                {\n+                    if (numOps.GreaterThan(a.Value[b, f], maxVal))\n+                        maxVal = a.Value[b, f];\n+                }\n+\n+                // Compute exp(x - max) and sum\n+                var expSum = numOps.Zero;\n+                var expValues = new T[features];\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var shifted = numOps.Subtract(a.Value[b, f], maxVal);\n+                    expValues[f] = numOps.Exp(shifted);\n+                    expSum = numOps.Add(expSum, expValues[f]);\n+                }\n+\n+                // Normalize\n+                for (int f = 0; f < features; f++)\n+                {\n+                    result[b, f] = numOps.Divide(expValues[f], expSum);\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (a.RequiresGradient)\n+                {\n+                    // ∂softmax/∂x_i = softmax_i * (∂L/∂y_i - Σ_j(∂L/∂y_j * softmax_j))\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        // Compute sum of (gradient * softmax)\n+                        var dotProduct = numOps.Zero;\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            dotProduct = numOps.Add(dotProduct,\n+                                numOps.Multiply(gradient[b, f], result[b, f]));\n+                        }\n+\n+                        // Compute gradient for each element\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            var gradMinusDot = numOps.Subtract(gradient[b, f], dotProduct);\n+                            gradA[b, f] = numOps.Multiply(result[b, f], gradMinusDot);\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient,\n+                parents: new List<ComputationNode<T>> { a },\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Softmax is currently only implemented for 2D tensors along axis=-1. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}], axis={axis}\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Concatenates multiple computation nodes along a specified axis.\n+    /// </summary>\n+    /// <param name=\"nodes\">The list of nodes to concatenate.</param>\n+    /// <param name=\"axis\">The axis along which to concatenate. Default is 0.</param>\n+    /// <returns>A new computation node containing the concatenated result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method concatenates tensors along the specified axis.\n+    /// All tensors must have the same shape except along the concatenation axis.\n+    /// The backward function splits the gradient and sends each portion to the corresponding input.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Concat stacks tensors together along a dimension.\n+    ///\n+    /// For concatenation:\n+    /// - The forward pass combines multiple tensors into one larger tensor\n+    /// - The backward pass splits the gradient back to each input\n+    /// - Think of it like gluing arrays together end-to-end\n+    ///\n+    /// Used in:\n+    /// - Skip connections (concatenating features from different layers)\n+    /// - Multi-input architectures\n+    /// - Feature fusion in neural networks\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Concat(List<ComputationNode<T>> nodes, int axis = 0)\n+    {\n+        if (nodes.Count == 0)\n+            throw new ArgumentException(\"Cannot concatenate empty list of nodes\");\n+\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var firstShape = nodes[0].Value.Shape;\n+\n+        // Normalize axis\n+        if (axis < 0)\n+            axis = firstShape.Length + axis;\n+\n+        // Validate shapes match except on concat axis\n+        for (int i = 1; i < nodes.Count; i++)\n+        {\n+            var shape = nodes[i].Value.Shape;\n+            if (shape.Length != firstShape.Length)\n+                throw new ArgumentException(\"All tensors must have the same rank\");\n+\n+            for (int d = 0; d < firstShape.Length; d++)\n+            {\n+                if (d != axis && shape[d] != firstShape[d])\n+                    throw new ArgumentException(\n+                        $\"Shape mismatch at dimension {d}: {shape[d]} vs {firstShape[d]}\");\n+            }\n+        }\n+\n+        // Compute output shape\n+        int[] outputShape = (int[])firstShape.Clone();\n+        for (int i = 1; i < nodes.Count; i++)\n+        {\n+            outputShape[axis] += nodes[i].Value.Shape[axis];\n+        }\n+\n+        // Perform concatenation (handle 2D case for simplicity)\n+        Tensor<T> result;\n+        if (firstShape.Length == 2 && axis == 1)\n+        {\n+            // Concatenate along columns (features)\n+            int rows = firstShape[0];\n+            int totalCols = outputShape[1];\n+            result = new Tensor<T>(new int[] { rows, totalCols });\n+\n+            int colOffset = 0;\n+            foreach (var node in nodes)\n+            {\n+                int cols = node.Value.Shape[1];\n+                for (int r = 0; r < rows; r++)\n+                {\n+                    for (int c = 0; c < cols; c++)\n+                    {\n+                        result[r, colOffset + c] = node.Value[r, c];\n+                    }\n+                }\n+                colOffset += cols;\n+            }\n+        }\n+        else if (firstShape.Length == 2 && axis == 0)\n+        {\n+            // Concatenate along rows (batch)\n+            int cols = firstShape[1];\n+            int totalRows = outputShape[0];\n+            result = new Tensor<T>(new int[] { totalRows, cols });\n+\n+            int rowOffset = 0;\n+            foreach (var node in nodes)\n+            {\n+                int rows = node.Value.Shape[0];\n+                for (int r = 0; r < rows; r++)\n+                {\n+                    for (int c = 0; c < cols; c++)\n+                    {\n+                        result[rowOffset + r, c] = node.Value[r, c];\n+                    }\n+                }\n+                rowOffset += rows;\n+            }\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Concat is currently only implemented for 2D tensors. \" +\n+                $\"Got shape=[{string.Join(\", \", firstShape)}]\");\n+        }\n+\n+        // Store sizes for gradient splitting\n+        var sizes = nodes.Select(n => n.Value.Shape[axis]).ToList();\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            // Split gradient along concat axis and distribute to inputs\n+            if (firstShape.Length == 2 && axis == 1)\n+            {\n+                int rows = firstShape[0];\n+                int colOffset = 0;\n+                for (int i = 0; i < nodes.Count; i++)\n+                {\n+                    if (!nodes[i].RequiresGradient)\n+                    {\n+                        colOffset += sizes[i];\n+                        continue;\n+                    }\n+\n+                    int cols = sizes[i];\n+                    var gradPart = new Tensor<T>(new int[] { rows, cols });\n+\n+                    for (int r = 0; r < rows; r++)\n+                    {\n+                        for (int c = 0; c < cols; c++)\n+                        {\n+                            gradPart[r, c] = gradient[r, colOffset + c];\n+                        }\n+                    }\n+\n+                    if (nodes[i].Gradient == null)\n+                        nodes[i].Gradient = gradPart;\n+                    else\n+                        nodes[i].Gradient = nodes[i].Gradient.Add(gradPart);\n+\n+                    colOffset += cols;\n+                }\n+            }\n+            else if (firstShape.Length == 2 && axis == 0)\n+            {\n+                int cols = firstShape[1];\n+                int rowOffset = 0;\n+                for (int i = 0; i < nodes.Count; i++)\n+                {\n+                    if (!nodes[i].RequiresGradient)\n+                    {\n+                        rowOffset += sizes[i];\n+                        continue;\n+                    }\n+\n+                    int rows = sizes[i];\n+                    var gradPart = new Tensor<T>(new int[] { rows, cols });\n+\n+                    for (int r = 0; r < rows; r++)\n+                    {\n+                        for (int c = 0; c < cols; c++)\n+                        {\n+                            gradPart[r, c] = gradient[rowOffset + r, c];\n+                        }\n+                    }\n+\n+                    if (nodes[i].Gradient == null)\n+                        nodes[i].Gradient = gradPart;\n+                    else\n+                        nodes[i].Gradient = nodes[i].Gradient.Add(gradPart);\n+\n+                    rowOffset += rows;\n+                }\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: nodes.Any(n => n.RequiresGradient),\n+            parents: nodes,\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Pads a tensor with a constant value along specified dimensions.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"padWidth\">Padding width for each dimension as (before, after) pairs.</param>\n+    /// <param name=\"value\">The value to use for padding. Default is zero.</param>\n+    /// <returns>A new computation node containing the padded result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method adds padding around the tensor.\n+    /// The backward function simply crops the gradient back to the original size (gradients for padding are zero).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Pad adds extra elements around a tensor.\n+    ///\n+    /// For padding:\n+    /// - The forward pass adds border elements with a constant value\n+    /// - The backward pass removes those border gradients (they don't affect the original tensor)\n+    /// - Think of it like adding margins to an image\n+    ///\n+    /// Used in:\n+    /// - Convolutional layers (to maintain spatial dimensions)\n+    /// - Handling variable-length sequences\n+    /// - Data augmentation\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Pad(ComputationNode<T> a, int[,] padWidth, T? value = default)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var padValue = value ?? numOps.Zero;\n+        var shape = a.Value.Shape;\n+\n+        // Validate padWidth dimensions\n+        if (padWidth.GetLength(0) != shape.Length)\n+            throw new ArgumentException(\"padWidth must have same number of dimensions as input tensor\");\n+\n+        // Compute output shape\n+        var outputShape = new int[shape.Length];\n+        for (int d = 0; d < shape.Length; d++)\n+        {\n+            outputShape[d] = shape[d] + padWidth[d, 0] + padWidth[d, 1];\n+        }\n+\n+        // Handle 2D case\n+        if (shape.Length == 2)\n+        {\n+            int inputRows = shape[0];\n+            int inputCols = shape[1];\n+            int padTop = padWidth[0, 0];\n+            int padBottom = padWidth[0, 1];\n+            int padLeft = padWidth[1, 0];\n+            int padRight = padWidth[1, 1];\n+\n+            var result = new Tensor<T>(outputShape);\n+\n+            // Initialize with pad value\n+            for (int i = 0; i < result.Length; i++)\n+            {\n+                result[i] = padValue;\n+            }\n+\n+            // Copy input data to center\n+            for (int r = 0; r < inputRows; r++)\n+            {\n+                for (int c = 0; c < inputCols; c++)\n+                {\n+                    result[padTop + r, padLeft + c] = a.Value[r, c];\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (a.RequiresGradient)\n+                {\n+                    // Extract gradient for original (unpadded) region\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int r = 0; r < inputRows; r++)\n+                    {\n+                        for (int c = 0; c < inputCols; c++)\n+                        {\n+                            gradA[r, c] = gradient[padTop + r, padLeft + c];\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);"}]}},{"id":"PRRT_kwDOKSXUF85hck9w","isResolved":false,"isOutdated":false,"path":"src/Autodiff/TensorOperations.cs","line":1793,"originalLine":1793,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzD92","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement write to the same variable - consider using '?' to express intent better.\n```suggestion\n                a.Gradient = a.Gradient == null ? gradA : a.Gradient.Add(gradA);\n```","createdAt":"2025-11-11T07:53:23Z","path":"src/Autodiff/TensorOperations.cs","line":1793,"originalLine":1793,"diffHunk":"@@ -1219,4 +1219,1769 @@ void BackwardFunction(Tensor<T> gradient)\n \n         return node;\n     }\n+\n+    /// <summary>\n+    /// Computes the softmax function for a computation node along a specified axis.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"axis\">The axis along which to compute softmax. Default is -1 (last axis).</param>\n+    /// <returns>A new computation node containing the softmax result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method computes softmax (σ(x_i) = exp(x_i) / Σexp(x_j)) along the specified axis.\n+    /// Uses numerical stability trick: subtract max before exponentiating.\n+    /// The backward function uses: ∂softmax/∂x = softmax(x) * (grad - Σ(grad * softmax(x))).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Softmax converts a vector of numbers into probabilities.\n+    ///\n+    /// For softmax:\n+    /// - The forward pass exponentiates each element, then normalizes so they sum to 1\n+    /// - The result is a probability distribution (all values between 0 and 1, summing to 1)\n+    /// - The backward pass is complex but efficient: uses the Jacobian of softmax\n+    ///\n+    /// Softmax is crucial for:\n+    /// - Multi-class classification (final layer outputs)\n+    /// - Attention mechanisms (computing attention weights)\n+    /// - Anywhere you need to convert scores to probabilities\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Softmax(ComputationNode<T> a, int axis = -1)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        // Normalize axis to positive index\n+        if (axis < 0)\n+            axis = shape.Length + axis;\n+\n+        // For simplicity, handle 2D case (batch, features) with axis=-1\n+        if (shape.Length == 2 && axis == 1)\n+        {\n+            int batchSize = shape[0];\n+            int features = shape[1];\n+            var result = new Tensor<T>(shape);\n+\n+            // Compute softmax for each row\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                // Find max for numerical stability\n+                var maxVal = a.Value[b, 0];\n+                for (int f = 1; f < features; f++)\n+                {\n+                    if (numOps.GreaterThan(a.Value[b, f], maxVal))\n+                        maxVal = a.Value[b, f];\n+                }\n+\n+                // Compute exp(x - max) and sum\n+                var expSum = numOps.Zero;\n+                var expValues = new T[features];\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var shifted = numOps.Subtract(a.Value[b, f], maxVal);\n+                    expValues[f] = numOps.Exp(shifted);\n+                    expSum = numOps.Add(expSum, expValues[f]);\n+                }\n+\n+                // Normalize\n+                for (int f = 0; f < features; f++)\n+                {\n+                    result[b, f] = numOps.Divide(expValues[f], expSum);\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (a.RequiresGradient)\n+                {\n+                    // ∂softmax/∂x_i = softmax_i * (∂L/∂y_i - Σ_j(∂L/∂y_j * softmax_j))\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        // Compute sum of (gradient * softmax)\n+                        var dotProduct = numOps.Zero;\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            dotProduct = numOps.Add(dotProduct,\n+                                numOps.Multiply(gradient[b, f], result[b, f]));\n+                        }\n+\n+                        // Compute gradient for each element\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            var gradMinusDot = numOps.Subtract(gradient[b, f], dotProduct);\n+                            gradA[b, f] = numOps.Multiply(result[b, f], gradMinusDot);\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient,\n+                parents: new List<ComputationNode<T>> { a },\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Softmax is currently only implemented for 2D tensors along axis=-1. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}], axis={axis}\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Concatenates multiple computation nodes along a specified axis.\n+    /// </summary>\n+    /// <param name=\"nodes\">The list of nodes to concatenate.</param>\n+    /// <param name=\"axis\">The axis along which to concatenate. Default is 0.</param>\n+    /// <returns>A new computation node containing the concatenated result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method concatenates tensors along the specified axis.\n+    /// All tensors must have the same shape except along the concatenation axis.\n+    /// The backward function splits the gradient and sends each portion to the corresponding input.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Concat stacks tensors together along a dimension.\n+    ///\n+    /// For concatenation:\n+    /// - The forward pass combines multiple tensors into one larger tensor\n+    /// - The backward pass splits the gradient back to each input\n+    /// - Think of it like gluing arrays together end-to-end\n+    ///\n+    /// Used in:\n+    /// - Skip connections (concatenating features from different layers)\n+    /// - Multi-input architectures\n+    /// - Feature fusion in neural networks\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Concat(List<ComputationNode<T>> nodes, int axis = 0)\n+    {\n+        if (nodes.Count == 0)\n+            throw new ArgumentException(\"Cannot concatenate empty list of nodes\");\n+\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var firstShape = nodes[0].Value.Shape;\n+\n+        // Normalize axis\n+        if (axis < 0)\n+            axis = firstShape.Length + axis;\n+\n+        // Validate shapes match except on concat axis\n+        for (int i = 1; i < nodes.Count; i++)\n+        {\n+            var shape = nodes[i].Value.Shape;\n+            if (shape.Length != firstShape.Length)\n+                throw new ArgumentException(\"All tensors must have the same rank\");\n+\n+            for (int d = 0; d < firstShape.Length; d++)\n+            {\n+                if (d != axis && shape[d] != firstShape[d])\n+                    throw new ArgumentException(\n+                        $\"Shape mismatch at dimension {d}: {shape[d]} vs {firstShape[d]}\");\n+            }\n+        }\n+\n+        // Compute output shape\n+        int[] outputShape = (int[])firstShape.Clone();\n+        for (int i = 1; i < nodes.Count; i++)\n+        {\n+            outputShape[axis] += nodes[i].Value.Shape[axis];\n+        }\n+\n+        // Perform concatenation (handle 2D case for simplicity)\n+        Tensor<T> result;\n+        if (firstShape.Length == 2 && axis == 1)\n+        {\n+            // Concatenate along columns (features)\n+            int rows = firstShape[0];\n+            int totalCols = outputShape[1];\n+            result = new Tensor<T>(new int[] { rows, totalCols });\n+\n+            int colOffset = 0;\n+            foreach (var node in nodes)\n+            {\n+                int cols = node.Value.Shape[1];\n+                for (int r = 0; r < rows; r++)\n+                {\n+                    for (int c = 0; c < cols; c++)\n+                    {\n+                        result[r, colOffset + c] = node.Value[r, c];\n+                    }\n+                }\n+                colOffset += cols;\n+            }\n+        }\n+        else if (firstShape.Length == 2 && axis == 0)\n+        {\n+            // Concatenate along rows (batch)\n+            int cols = firstShape[1];\n+            int totalRows = outputShape[0];\n+            result = new Tensor<T>(new int[] { totalRows, cols });\n+\n+            int rowOffset = 0;\n+            foreach (var node in nodes)\n+            {\n+                int rows = node.Value.Shape[0];\n+                for (int r = 0; r < rows; r++)\n+                {\n+                    for (int c = 0; c < cols; c++)\n+                    {\n+                        result[rowOffset + r, c] = node.Value[r, c];\n+                    }\n+                }\n+                rowOffset += rows;\n+            }\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Concat is currently only implemented for 2D tensors. \" +\n+                $\"Got shape=[{string.Join(\", \", firstShape)}]\");\n+        }\n+\n+        // Store sizes for gradient splitting\n+        var sizes = nodes.Select(n => n.Value.Shape[axis]).ToList();\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            // Split gradient along concat axis and distribute to inputs\n+            if (firstShape.Length == 2 && axis == 1)\n+            {\n+                int rows = firstShape[0];\n+                int colOffset = 0;\n+                for (int i = 0; i < nodes.Count; i++)\n+                {\n+                    if (!nodes[i].RequiresGradient)\n+                    {\n+                        colOffset += sizes[i];\n+                        continue;\n+                    }\n+\n+                    int cols = sizes[i];\n+                    var gradPart = new Tensor<T>(new int[] { rows, cols });\n+\n+                    for (int r = 0; r < rows; r++)\n+                    {\n+                        for (int c = 0; c < cols; c++)\n+                        {\n+                            gradPart[r, c] = gradient[r, colOffset + c];\n+                        }\n+                    }\n+\n+                    if (nodes[i].Gradient == null)\n+                        nodes[i].Gradient = gradPart;\n+                    else\n+                        nodes[i].Gradient = nodes[i].Gradient.Add(gradPart);\n+\n+                    colOffset += cols;\n+                }\n+            }\n+            else if (firstShape.Length == 2 && axis == 0)\n+            {\n+                int cols = firstShape[1];\n+                int rowOffset = 0;\n+                for (int i = 0; i < nodes.Count; i++)\n+                {\n+                    if (!nodes[i].RequiresGradient)\n+                    {\n+                        rowOffset += sizes[i];\n+                        continue;\n+                    }\n+\n+                    int rows = sizes[i];\n+                    var gradPart = new Tensor<T>(new int[] { rows, cols });\n+\n+                    for (int r = 0; r < rows; r++)\n+                    {\n+                        for (int c = 0; c < cols; c++)\n+                        {\n+                            gradPart[r, c] = gradient[rowOffset + r, c];\n+                        }\n+                    }\n+\n+                    if (nodes[i].Gradient == null)\n+                        nodes[i].Gradient = gradPart;\n+                    else\n+                        nodes[i].Gradient = nodes[i].Gradient.Add(gradPart);\n+\n+                    rowOffset += rows;\n+                }\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: nodes.Any(n => n.RequiresGradient),\n+            parents: nodes,\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Pads a tensor with a constant value along specified dimensions.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"padWidth\">Padding width for each dimension as (before, after) pairs.</param>\n+    /// <param name=\"value\">The value to use for padding. Default is zero.</param>\n+    /// <returns>A new computation node containing the padded result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method adds padding around the tensor.\n+    /// The backward function simply crops the gradient back to the original size (gradients for padding are zero).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Pad adds extra elements around a tensor.\n+    ///\n+    /// For padding:\n+    /// - The forward pass adds border elements with a constant value\n+    /// - The backward pass removes those border gradients (they don't affect the original tensor)\n+    /// - Think of it like adding margins to an image\n+    ///\n+    /// Used in:\n+    /// - Convolutional layers (to maintain spatial dimensions)\n+    /// - Handling variable-length sequences\n+    /// - Data augmentation\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Pad(ComputationNode<T> a, int[,] padWidth, T? value = default)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var padValue = value ?? numOps.Zero;\n+        var shape = a.Value.Shape;\n+\n+        // Validate padWidth dimensions\n+        if (padWidth.GetLength(0) != shape.Length)\n+            throw new ArgumentException(\"padWidth must have same number of dimensions as input tensor\");\n+\n+        // Compute output shape\n+        var outputShape = new int[shape.Length];\n+        for (int d = 0; d < shape.Length; d++)\n+        {\n+            outputShape[d] = shape[d] + padWidth[d, 0] + padWidth[d, 1];\n+        }\n+\n+        // Handle 2D case\n+        if (shape.Length == 2)\n+        {\n+            int inputRows = shape[0];\n+            int inputCols = shape[1];\n+            int padTop = padWidth[0, 0];\n+            int padBottom = padWidth[0, 1];\n+            int padLeft = padWidth[1, 0];\n+            int padRight = padWidth[1, 1];\n+\n+            var result = new Tensor<T>(outputShape);\n+\n+            // Initialize with pad value\n+            for (int i = 0; i < result.Length; i++)\n+            {\n+                result[i] = padValue;\n+            }\n+\n+            // Copy input data to center\n+            for (int r = 0; r < inputRows; r++)\n+            {\n+                for (int c = 0; c < inputCols; c++)\n+                {\n+                    result[padTop + r, padLeft + c] = a.Value[r, c];\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (a.RequiresGradient)\n+                {\n+                    // Extract gradient for original (unpadded) region\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int r = 0; r < inputRows; r++)\n+                    {\n+                        for (int c = 0; c < inputCols; c++)\n+                        {\n+                            gradA[r, c] = gradient[padTop + r, padLeft + c];\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient,\n+                parents: new List<ComputationNode<T>> { a },\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Pad is currently only implemented for 2D tensors. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}]\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs 2D max pooling on a 4D tensor (batch, channels, height, width).\n+    /// </summary>\n+    /// <param name=\"a\">The input node with shape [batch, channels, height, width].</param>\n+    /// <param name=\"poolSize\">The size of the pooling window [poolH, poolW].</param>\n+    /// <param name=\"strides\">The stride for the pooling operation [strideH, strideW]. If null, uses poolSize.</param>\n+    /// <returns>A new computation node containing the max pooled result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method performs max pooling over 2D spatial dimensions.\n+    /// During forward pass, it tracks which element was the max for routing gradients during backward pass.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> MaxPool downsamples by taking the maximum value in each window.\n+    ///\n+    /// For max pooling:\n+    /// - The forward pass slides a window and takes the max value in each position\n+    /// - This reduces spatial dimensions (downsampling)\n+    /// - The backward pass routes gradients only to the positions that were max\n+    /// - Other positions get zero gradient (they didn't contribute to the output)\n+    ///\n+    /// Used in:\n+    /// - CNNs for translation invariance\n+    /// - Reducing spatial resolution\n+    /// - Building hierarchical features\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> MaxPool2D(\n+        ComputationNode<T> a,\n+        int[] poolSize,\n+        int[]? strides = null)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        if (shape.Length != 4)\n+            throw new ArgumentException(\"MaxPool2D requires 4D input [batch, channels, height, width]\");\n+\n+        strides ??= poolSize;\n+\n+        int batch = shape[0];\n+        int channels = shape[1];\n+        int inH = shape[2];\n+        int inW = shape[3];\n+        int poolH = poolSize[0];\n+        int poolW = poolSize[1];\n+        int strideH = strides[0];\n+        int strideW = strides[1];\n+\n+        int outH = (inH - poolH) / strideH + 1;\n+        int outW = (inW - poolW) / strideW + 1;\n+\n+        var result = new Tensor<T>(new int[] { batch, channels, outH, outW });\n+        // Store max positions for backprop\n+        var maxPositions = new int[batch, channels, outH, outW, 2]; // [h_offset, w_offset]\n+\n+        // Forward pass: compute max pooling and track positions\n+        for (int b = 0; b < batch; b++)\n+        {\n+            for (int c = 0; c < channels; c++)\n+            {\n+                for (int oh = 0; oh < outH; oh++)\n+                {\n+                    for (int ow = 0; ow < outW; ow++)\n+                    {\n+                        int hStart = oh * strideH;\n+                        int wStart = ow * strideW;\n+\n+                        var maxVal = a.Value[b * channels * inH * inW +\n+                                           c * inH * inW +\n+                                           hStart * inW +\n+                                           wStart];\n+                        int maxHOffset = 0;\n+                        int maxWOffset = 0;\n+\n+                        // Find max in pooling window\n+                        for (int ph = 0; ph < poolH; ph++)\n+                        {\n+                            for (int pw = 0; pw < poolW; pw++)\n+                            {\n+                                int h = hStart + ph;\n+                                int w = wStart + pw;\n+                                if (h < inH && w < inW)\n+                                {\n+                                    var val = a.Value[b * channels * inH * inW +\n+                                                     c * inH * inW +\n+                                                     h * inW +\n+                                                     w];\n+                                    if (numOps.GreaterThan(val, maxVal))\n+                                    {\n+                                        maxVal = val;\n+                                        maxHOffset = ph;\n+                                        maxWOffset = pw;\n+                                    }\n+                                }\n+                            }\n+                        }\n+\n+                        result[b, c, oh, ow] = maxVal;\n+                        maxPositions[b, c, oh, ow, 0] = maxHOffset;\n+                        maxPositions[b, c, oh, ow, 1] = maxWOffset;\n+                    }\n+                }\n+            }\n+        }\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            if (a.RequiresGradient)\n+            {\n+                var gradA = new Tensor<T>(shape);\n+\n+                // Route gradients to max positions\n+                for (int b = 0; b < batch; b++)\n+                {\n+                    for (int c = 0; c < channels; c++)\n+                    {\n+                        for (int oh = 0; oh < outH; oh++)\n+                        {\n+                            for (int ow = 0; ow < outW; ow++)\n+                            {\n+                                int hStart = oh * strideH;\n+                                int wStart = ow * strideW;\n+                                int maxHOffset = maxPositions[b, c, oh, ow, 0];\n+                                int maxWOffset = maxPositions[b, c, oh, ow, 1];\n+\n+                                int maxH = hStart + maxHOffset;\n+                                int maxW = wStart + maxWOffset;\n+\n+                                int gradIdx = b * channels * inH * inW +\n+                                             c * inH * inW +\n+                                            maxH * inW +\n+                                             maxW;\n+\n+                                gradA[gradIdx] = numOps.Add(gradA[gradIdx], gradient[b, c, oh, ow]);\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (a.Gradient == null)\n+                    a.Gradient = gradA;\n+                else\n+                    a.Gradient = a.Gradient.Add(gradA);"}]}},{"id":"PRRT_kwDOKSXUF85hck99","isResolved":false,"isOutdated":false,"path":"src/Autodiff/TensorOperations.cs","line":1940,"originalLine":1940,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzD-G","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement write to the same variable - consider using '?' to express intent better.\n```suggestion\n                a.Gradient = a.Gradient?.Add(gradA) ?? gradA;\n```","createdAt":"2025-11-11T07:53:24Z","path":"src/Autodiff/TensorOperations.cs","line":1940,"originalLine":1940,"diffHunk":"@@ -1219,4 +1219,1769 @@ void BackwardFunction(Tensor<T> gradient)\n \n         return node;\n     }\n+\n+    /// <summary>\n+    /// Computes the softmax function for a computation node along a specified axis.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"axis\">The axis along which to compute softmax. Default is -1 (last axis).</param>\n+    /// <returns>A new computation node containing the softmax result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method computes softmax (σ(x_i) = exp(x_i) / Σexp(x_j)) along the specified axis.\n+    /// Uses numerical stability trick: subtract max before exponentiating.\n+    /// The backward function uses: ∂softmax/∂x = softmax(x) * (grad - Σ(grad * softmax(x))).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Softmax converts a vector of numbers into probabilities.\n+    ///\n+    /// For softmax:\n+    /// - The forward pass exponentiates each element, then normalizes so they sum to 1\n+    /// - The result is a probability distribution (all values between 0 and 1, summing to 1)\n+    /// - The backward pass is complex but efficient: uses the Jacobian of softmax\n+    ///\n+    /// Softmax is crucial for:\n+    /// - Multi-class classification (final layer outputs)\n+    /// - Attention mechanisms (computing attention weights)\n+    /// - Anywhere you need to convert scores to probabilities\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Softmax(ComputationNode<T> a, int axis = -1)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        // Normalize axis to positive index\n+        if (axis < 0)\n+            axis = shape.Length + axis;\n+\n+        // For simplicity, handle 2D case (batch, features) with axis=-1\n+        if (shape.Length == 2 && axis == 1)\n+        {\n+            int batchSize = shape[0];\n+            int features = shape[1];\n+            var result = new Tensor<T>(shape);\n+\n+            // Compute softmax for each row\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                // Find max for numerical stability\n+                var maxVal = a.Value[b, 0];\n+                for (int f = 1; f < features; f++)\n+                {\n+                    if (numOps.GreaterThan(a.Value[b, f], maxVal))\n+                        maxVal = a.Value[b, f];\n+                }\n+\n+                // Compute exp(x - max) and sum\n+                var expSum = numOps.Zero;\n+                var expValues = new T[features];\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var shifted = numOps.Subtract(a.Value[b, f], maxVal);\n+                    expValues[f] = numOps.Exp(shifted);\n+                    expSum = numOps.Add(expSum, expValues[f]);\n+                }\n+\n+                // Normalize\n+                for (int f = 0; f < features; f++)\n+                {\n+                    result[b, f] = numOps.Divide(expValues[f], expSum);\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (a.RequiresGradient)\n+                {\n+                    // ∂softmax/∂x_i = softmax_i * (∂L/∂y_i - Σ_j(∂L/∂y_j * softmax_j))\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        // Compute sum of (gradient * softmax)\n+                        var dotProduct = numOps.Zero;\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            dotProduct = numOps.Add(dotProduct,\n+                                numOps.Multiply(gradient[b, f], result[b, f]));\n+                        }\n+\n+                        // Compute gradient for each element\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            var gradMinusDot = numOps.Subtract(gradient[b, f], dotProduct);\n+                            gradA[b, f] = numOps.Multiply(result[b, f], gradMinusDot);\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient,\n+                parents: new List<ComputationNode<T>> { a },\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Softmax is currently only implemented for 2D tensors along axis=-1. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}], axis={axis}\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Concatenates multiple computation nodes along a specified axis.\n+    /// </summary>\n+    /// <param name=\"nodes\">The list of nodes to concatenate.</param>\n+    /// <param name=\"axis\">The axis along which to concatenate. Default is 0.</param>\n+    /// <returns>A new computation node containing the concatenated result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method concatenates tensors along the specified axis.\n+    /// All tensors must have the same shape except along the concatenation axis.\n+    /// The backward function splits the gradient and sends each portion to the corresponding input.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Concat stacks tensors together along a dimension.\n+    ///\n+    /// For concatenation:\n+    /// - The forward pass combines multiple tensors into one larger tensor\n+    /// - The backward pass splits the gradient back to each input\n+    /// - Think of it like gluing arrays together end-to-end\n+    ///\n+    /// Used in:\n+    /// - Skip connections (concatenating features from different layers)\n+    /// - Multi-input architectures\n+    /// - Feature fusion in neural networks\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Concat(List<ComputationNode<T>> nodes, int axis = 0)\n+    {\n+        if (nodes.Count == 0)\n+            throw new ArgumentException(\"Cannot concatenate empty list of nodes\");\n+\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var firstShape = nodes[0].Value.Shape;\n+\n+        // Normalize axis\n+        if (axis < 0)\n+            axis = firstShape.Length + axis;\n+\n+        // Validate shapes match except on concat axis\n+        for (int i = 1; i < nodes.Count; i++)\n+        {\n+            var shape = nodes[i].Value.Shape;\n+            if (shape.Length != firstShape.Length)\n+                throw new ArgumentException(\"All tensors must have the same rank\");\n+\n+            for (int d = 0; d < firstShape.Length; d++)\n+            {\n+                if (d != axis && shape[d] != firstShape[d])\n+                    throw new ArgumentException(\n+                        $\"Shape mismatch at dimension {d}: {shape[d]} vs {firstShape[d]}\");\n+            }\n+        }\n+\n+        // Compute output shape\n+        int[] outputShape = (int[])firstShape.Clone();\n+        for (int i = 1; i < nodes.Count; i++)\n+        {\n+            outputShape[axis] += nodes[i].Value.Shape[axis];\n+        }\n+\n+        // Perform concatenation (handle 2D case for simplicity)\n+        Tensor<T> result;\n+        if (firstShape.Length == 2 && axis == 1)\n+        {\n+            // Concatenate along columns (features)\n+            int rows = firstShape[0];\n+            int totalCols = outputShape[1];\n+            result = new Tensor<T>(new int[] { rows, totalCols });\n+\n+            int colOffset = 0;\n+            foreach (var node in nodes)\n+            {\n+                int cols = node.Value.Shape[1];\n+                for (int r = 0; r < rows; r++)\n+                {\n+                    for (int c = 0; c < cols; c++)\n+                    {\n+                        result[r, colOffset + c] = node.Value[r, c];\n+                    }\n+                }\n+                colOffset += cols;\n+            }\n+        }\n+        else if (firstShape.Length == 2 && axis == 0)\n+        {\n+            // Concatenate along rows (batch)\n+            int cols = firstShape[1];\n+            int totalRows = outputShape[0];\n+            result = new Tensor<T>(new int[] { totalRows, cols });\n+\n+            int rowOffset = 0;\n+            foreach (var node in nodes)\n+            {\n+                int rows = node.Value.Shape[0];\n+                for (int r = 0; r < rows; r++)\n+                {\n+                    for (int c = 0; c < cols; c++)\n+                    {\n+                        result[rowOffset + r, c] = node.Value[r, c];\n+                    }\n+                }\n+                rowOffset += rows;\n+            }\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Concat is currently only implemented for 2D tensors. \" +\n+                $\"Got shape=[{string.Join(\", \", firstShape)}]\");\n+        }\n+\n+        // Store sizes for gradient splitting\n+        var sizes = nodes.Select(n => n.Value.Shape[axis]).ToList();\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            // Split gradient along concat axis and distribute to inputs\n+            if (firstShape.Length == 2 && axis == 1)\n+            {\n+                int rows = firstShape[0];\n+                int colOffset = 0;\n+                for (int i = 0; i < nodes.Count; i++)\n+                {\n+                    if (!nodes[i].RequiresGradient)\n+                    {\n+                        colOffset += sizes[i];\n+                        continue;\n+                    }\n+\n+                    int cols = sizes[i];\n+                    var gradPart = new Tensor<T>(new int[] { rows, cols });\n+\n+                    for (int r = 0; r < rows; r++)\n+                    {\n+                        for (int c = 0; c < cols; c++)\n+                        {\n+                            gradPart[r, c] = gradient[r, colOffset + c];\n+                        }\n+                    }\n+\n+                    if (nodes[i].Gradient == null)\n+                        nodes[i].Gradient = gradPart;\n+                    else\n+                        nodes[i].Gradient = nodes[i].Gradient.Add(gradPart);\n+\n+                    colOffset += cols;\n+                }\n+            }\n+            else if (firstShape.Length == 2 && axis == 0)\n+            {\n+                int cols = firstShape[1];\n+                int rowOffset = 0;\n+                for (int i = 0; i < nodes.Count; i++)\n+                {\n+                    if (!nodes[i].RequiresGradient)\n+                    {\n+                        rowOffset += sizes[i];\n+                        continue;\n+                    }\n+\n+                    int rows = sizes[i];\n+                    var gradPart = new Tensor<T>(new int[] { rows, cols });\n+\n+                    for (int r = 0; r < rows; r++)\n+                    {\n+                        for (int c = 0; c < cols; c++)\n+                        {\n+                            gradPart[r, c] = gradient[rowOffset + r, c];\n+                        }\n+                    }\n+\n+                    if (nodes[i].Gradient == null)\n+                        nodes[i].Gradient = gradPart;\n+                    else\n+                        nodes[i].Gradient = nodes[i].Gradient.Add(gradPart);\n+\n+                    rowOffset += rows;\n+                }\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: nodes.Any(n => n.RequiresGradient),\n+            parents: nodes,\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Pads a tensor with a constant value along specified dimensions.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"padWidth\">Padding width for each dimension as (before, after) pairs.</param>\n+    /// <param name=\"value\">The value to use for padding. Default is zero.</param>\n+    /// <returns>A new computation node containing the padded result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method adds padding around the tensor.\n+    /// The backward function simply crops the gradient back to the original size (gradients for padding are zero).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Pad adds extra elements around a tensor.\n+    ///\n+    /// For padding:\n+    /// - The forward pass adds border elements with a constant value\n+    /// - The backward pass removes those border gradients (they don't affect the original tensor)\n+    /// - Think of it like adding margins to an image\n+    ///\n+    /// Used in:\n+    /// - Convolutional layers (to maintain spatial dimensions)\n+    /// - Handling variable-length sequences\n+    /// - Data augmentation\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Pad(ComputationNode<T> a, int[,] padWidth, T? value = default)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var padValue = value ?? numOps.Zero;\n+        var shape = a.Value.Shape;\n+\n+        // Validate padWidth dimensions\n+        if (padWidth.GetLength(0) != shape.Length)\n+            throw new ArgumentException(\"padWidth must have same number of dimensions as input tensor\");\n+\n+        // Compute output shape\n+        var outputShape = new int[shape.Length];\n+        for (int d = 0; d < shape.Length; d++)\n+        {\n+            outputShape[d] = shape[d] + padWidth[d, 0] + padWidth[d, 1];\n+        }\n+\n+        // Handle 2D case\n+        if (shape.Length == 2)\n+        {\n+            int inputRows = shape[0];\n+            int inputCols = shape[1];\n+            int padTop = padWidth[0, 0];\n+            int padBottom = padWidth[0, 1];\n+            int padLeft = padWidth[1, 0];\n+            int padRight = padWidth[1, 1];\n+\n+            var result = new Tensor<T>(outputShape);\n+\n+            // Initialize with pad value\n+            for (int i = 0; i < result.Length; i++)\n+            {\n+                result[i] = padValue;\n+            }\n+\n+            // Copy input data to center\n+            for (int r = 0; r < inputRows; r++)\n+            {\n+                for (int c = 0; c < inputCols; c++)\n+                {\n+                    result[padTop + r, padLeft + c] = a.Value[r, c];\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (a.RequiresGradient)\n+                {\n+                    // Extract gradient for original (unpadded) region\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int r = 0; r < inputRows; r++)\n+                    {\n+                        for (int c = 0; c < inputCols; c++)\n+                        {\n+                            gradA[r, c] = gradient[padTop + r, padLeft + c];\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient,\n+                parents: new List<ComputationNode<T>> { a },\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Pad is currently only implemented for 2D tensors. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}]\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs 2D max pooling on a 4D tensor (batch, channels, height, width).\n+    /// </summary>\n+    /// <param name=\"a\">The input node with shape [batch, channels, height, width].</param>\n+    /// <param name=\"poolSize\">The size of the pooling window [poolH, poolW].</param>\n+    /// <param name=\"strides\">The stride for the pooling operation [strideH, strideW]. If null, uses poolSize.</param>\n+    /// <returns>A new computation node containing the max pooled result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method performs max pooling over 2D spatial dimensions.\n+    /// During forward pass, it tracks which element was the max for routing gradients during backward pass.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> MaxPool downsamples by taking the maximum value in each window.\n+    ///\n+    /// For max pooling:\n+    /// - The forward pass slides a window and takes the max value in each position\n+    /// - This reduces spatial dimensions (downsampling)\n+    /// - The backward pass routes gradients only to the positions that were max\n+    /// - Other positions get zero gradient (they didn't contribute to the output)\n+    ///\n+    /// Used in:\n+    /// - CNNs for translation invariance\n+    /// - Reducing spatial resolution\n+    /// - Building hierarchical features\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> MaxPool2D(\n+        ComputationNode<T> a,\n+        int[] poolSize,\n+        int[]? strides = null)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        if (shape.Length != 4)\n+            throw new ArgumentException(\"MaxPool2D requires 4D input [batch, channels, height, width]\");\n+\n+        strides ??= poolSize;\n+\n+        int batch = shape[0];\n+        int channels = shape[1];\n+        int inH = shape[2];\n+        int inW = shape[3];\n+        int poolH = poolSize[0];\n+        int poolW = poolSize[1];\n+        int strideH = strides[0];\n+        int strideW = strides[1];\n+\n+        int outH = (inH - poolH) / strideH + 1;\n+        int outW = (inW - poolW) / strideW + 1;\n+\n+        var result = new Tensor<T>(new int[] { batch, channels, outH, outW });\n+        // Store max positions for backprop\n+        var maxPositions = new int[batch, channels, outH, outW, 2]; // [h_offset, w_offset]\n+\n+        // Forward pass: compute max pooling and track positions\n+        for (int b = 0; b < batch; b++)\n+        {\n+            for (int c = 0; c < channels; c++)\n+            {\n+                for (int oh = 0; oh < outH; oh++)\n+                {\n+                    for (int ow = 0; ow < outW; ow++)\n+                    {\n+                        int hStart = oh * strideH;\n+                        int wStart = ow * strideW;\n+\n+                        var maxVal = a.Value[b * channels * inH * inW +\n+                                           c * inH * inW +\n+                                           hStart * inW +\n+                                           wStart];\n+                        int maxHOffset = 0;\n+                        int maxWOffset = 0;\n+\n+                        // Find max in pooling window\n+                        for (int ph = 0; ph < poolH; ph++)\n+                        {\n+                            for (int pw = 0; pw < poolW; pw++)\n+                            {\n+                                int h = hStart + ph;\n+                                int w = wStart + pw;\n+                                if (h < inH && w < inW)\n+                                {\n+                                    var val = a.Value[b * channels * inH * inW +\n+                                                     c * inH * inW +\n+                                                     h * inW +\n+                                                     w];\n+                                    if (numOps.GreaterThan(val, maxVal))\n+                                    {\n+                                        maxVal = val;\n+                                        maxHOffset = ph;\n+                                        maxWOffset = pw;\n+                                    }\n+                                }\n+                            }\n+                        }\n+\n+                        result[b, c, oh, ow] = maxVal;\n+                        maxPositions[b, c, oh, ow, 0] = maxHOffset;\n+                        maxPositions[b, c, oh, ow, 1] = maxWOffset;\n+                    }\n+                }\n+            }\n+        }\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            if (a.RequiresGradient)\n+            {\n+                var gradA = new Tensor<T>(shape);\n+\n+                // Route gradients to max positions\n+                for (int b = 0; b < batch; b++)\n+                {\n+                    for (int c = 0; c < channels; c++)\n+                    {\n+                        for (int oh = 0; oh < outH; oh++)\n+                        {\n+                            for (int ow = 0; ow < outW; ow++)\n+                            {\n+                                int hStart = oh * strideH;\n+                                int wStart = ow * strideW;\n+                                int maxHOffset = maxPositions[b, c, oh, ow, 0];\n+                                int maxWOffset = maxPositions[b, c, oh, ow, 1];\n+\n+                                int maxH = hStart + maxHOffset;\n+                                int maxW = wStart + maxWOffset;\n+\n+                                int gradIdx = b * channels * inH * inW +\n+                                             c * inH * inW +\n+                                            maxH * inW +\n+                                             maxW;\n+\n+                                gradA[gradIdx] = numOps.Add(gradA[gradIdx], gradient[b, c, oh, ow]);\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (a.Gradient == null)\n+                    a.Gradient = gradA;\n+                else\n+                    a.Gradient = a.Gradient.Add(gradA);\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: a.RequiresGradient,\n+            parents: new List<ComputationNode<T>> { a },\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Performs 2D average pooling on a 4D tensor (batch, channels, height, width).\n+    /// </summary>\n+    /// <param name=\"a\">The input node with shape [batch, channels, height, width].</param>\n+    /// <param name=\"poolSize\">The size of the pooling window [poolH, poolW].</param>\n+    /// <param name=\"strides\">The stride for the pooling operation [strideH, strideW]. If null, uses poolSize.</param>\n+    /// <returns>A new computation node containing the average pooled result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method performs average pooling over 2D spatial dimensions.\n+    /// The backward function distributes gradients equally across the pooling window.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> AvgPool downsamples by taking the average value in each window.\n+    ///\n+    /// For average pooling:\n+    /// - The forward pass slides a window and computes the average\n+    /// - This smoothly reduces spatial dimensions\n+    /// - The backward pass distributes gradients equally to all elements in the window\n+    /// - Each element gets gradient / pool_area\n+    ///\n+    /// Used in:\n+    /// - CNNs for smoother downsampling than max pooling\n+    /// - Global average pooling (replacing fully connected layers)\n+    /// - Reducing overfitting\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> AvgPool2D(\n+        ComputationNode<T> a,\n+        int[] poolSize,\n+        int[]? strides = null)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        if (shape.Length != 4)\n+            throw new ArgumentException(\"AvgPool2D requires 4D input [batch, channels, height, width]\");\n+\n+        strides ??= poolSize;\n+\n+        int batch = shape[0];\n+        int channels = shape[1];\n+        int inH = shape[2];\n+        int inW = shape[3];\n+        int poolH = poolSize[0];\n+        int poolW = poolSize[1];\n+        int strideH = strides[0];\n+        int strideW = strides[1];\n+\n+        int outH = (inH - poolH) / strideH + 1;\n+        int outW = (inW - poolW) / strideW + 1;\n+\n+        var result = new Tensor<T>(new int[] { batch, channels, outH, outW });\n+        var poolArea = numOps.FromDouble(poolH * poolW);\n+\n+        // Forward pass: compute average pooling\n+        for (int b = 0; b < batch; b++)\n+        {\n+            for (int c = 0; c < channels; c++)\n+            {\n+                for (int oh = 0; oh < outH; oh++)\n+                {\n+                    for (int ow = 0; ow < outW; ow++)\n+                    {\n+                        int hStart = oh * strideH;\n+                        int wStart = ow * strideW;\n+\n+                        var sum = numOps.Zero;\n+\n+                        // Sum values in pooling window\n+                        for (int ph = 0; ph < poolH; ph++)\n+                        {\n+                            for (int pw = 0; pw < poolW; pw++)\n+                            {\n+                                int h = hStart + ph;\n+                                int w = wStart + pw;\n+                                if (h < inH && w < inW)\n+                                {\n+                                    sum = numOps.Add(sum, a.Value[b, c, h, w]);\n+                                }\n+                            }\n+                        }\n+\n+                        result[b, c, oh, ow] = numOps.Divide(sum, poolArea);\n+                    }\n+                }\n+            }\n+        }\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            if (a.RequiresGradient)\n+            {\n+                var gradA = new Tensor<T>(shape);\n+\n+                // Distribute gradients equally across pooling windows\n+                for (int b = 0; b < batch; b++)\n+                {\n+                    for (int c = 0; c < channels; c++)\n+                    {\n+                        for (int oh = 0; oh < outH; oh++)\n+                        {\n+                            for (int ow = 0; ow < outW; ow++)\n+                            {\n+                                int hStart = oh * strideH;\n+                                int wStart = ow * strideW;\n+\n+                                var gradValue = numOps.Divide(gradient[b, c, oh, ow], poolArea);\n+\n+                                // Distribute to all elements in window\n+                                for (int ph = 0; ph < poolH; ph++)\n+                                {\n+                                    for (int pw = 0; pw < poolW; pw++)\n+                                    {\n+                                        int h = hStart + ph;\n+                                        int w = wStart + pw;\n+                                        if (h < inH && w < inW)\n+                                        {\n+                                            gradA[b, c, h, w] = numOps.Add(gradA[b, c, h, w], gradValue);\n+                                        }\n+                                    }\n+                                }\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (a.Gradient == null)\n+                    a.Gradient = gradA;\n+                else\n+                    a.Gradient = a.Gradient.Add(gradA);"}]}},{"id":"PRRT_kwDOKSXUF85hck-J","isResolved":false,"isOutdated":false,"path":"src/Autodiff/TensorOperations.cs","line":2080,"originalLine":2080,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzD-U","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement write to the same variable - consider using '?' to express intent better.","createdAt":"2025-11-11T07:53:24Z","path":"src/Autodiff/TensorOperations.cs","line":2080,"originalLine":2080,"diffHunk":"@@ -1219,4 +1219,1769 @@ void BackwardFunction(Tensor<T> gradient)\n \n         return node;\n     }\n+\n+    /// <summary>\n+    /// Computes the softmax function for a computation node along a specified axis.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"axis\">The axis along which to compute softmax. Default is -1 (last axis).</param>\n+    /// <returns>A new computation node containing the softmax result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method computes softmax (σ(x_i) = exp(x_i) / Σexp(x_j)) along the specified axis.\n+    /// Uses numerical stability trick: subtract max before exponentiating.\n+    /// The backward function uses: ∂softmax/∂x = softmax(x) * (grad - Σ(grad * softmax(x))).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Softmax converts a vector of numbers into probabilities.\n+    ///\n+    /// For softmax:\n+    /// - The forward pass exponentiates each element, then normalizes so they sum to 1\n+    /// - The result is a probability distribution (all values between 0 and 1, summing to 1)\n+    /// - The backward pass is complex but efficient: uses the Jacobian of softmax\n+    ///\n+    /// Softmax is crucial for:\n+    /// - Multi-class classification (final layer outputs)\n+    /// - Attention mechanisms (computing attention weights)\n+    /// - Anywhere you need to convert scores to probabilities\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Softmax(ComputationNode<T> a, int axis = -1)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        // Normalize axis to positive index\n+        if (axis < 0)\n+            axis = shape.Length + axis;\n+\n+        // For simplicity, handle 2D case (batch, features) with axis=-1\n+        if (shape.Length == 2 && axis == 1)\n+        {\n+            int batchSize = shape[0];\n+            int features = shape[1];\n+            var result = new Tensor<T>(shape);\n+\n+            // Compute softmax for each row\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                // Find max for numerical stability\n+                var maxVal = a.Value[b, 0];\n+                for (int f = 1; f < features; f++)\n+                {\n+                    if (numOps.GreaterThan(a.Value[b, f], maxVal))\n+                        maxVal = a.Value[b, f];\n+                }\n+\n+                // Compute exp(x - max) and sum\n+                var expSum = numOps.Zero;\n+                var expValues = new T[features];\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var shifted = numOps.Subtract(a.Value[b, f], maxVal);\n+                    expValues[f] = numOps.Exp(shifted);\n+                    expSum = numOps.Add(expSum, expValues[f]);\n+                }\n+\n+                // Normalize\n+                for (int f = 0; f < features; f++)\n+                {\n+                    result[b, f] = numOps.Divide(expValues[f], expSum);\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (a.RequiresGradient)\n+                {\n+                    // ∂softmax/∂x_i = softmax_i * (∂L/∂y_i - Σ_j(∂L/∂y_j * softmax_j))\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        // Compute sum of (gradient * softmax)\n+                        var dotProduct = numOps.Zero;\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            dotProduct = numOps.Add(dotProduct,\n+                                numOps.Multiply(gradient[b, f], result[b, f]));\n+                        }\n+\n+                        // Compute gradient for each element\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            var gradMinusDot = numOps.Subtract(gradient[b, f], dotProduct);\n+                            gradA[b, f] = numOps.Multiply(result[b, f], gradMinusDot);\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient,\n+                parents: new List<ComputationNode<T>> { a },\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Softmax is currently only implemented for 2D tensors along axis=-1. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}], axis={axis}\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Concatenates multiple computation nodes along a specified axis.\n+    /// </summary>\n+    /// <param name=\"nodes\">The list of nodes to concatenate.</param>\n+    /// <param name=\"axis\">The axis along which to concatenate. Default is 0.</param>\n+    /// <returns>A new computation node containing the concatenated result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method concatenates tensors along the specified axis.\n+    /// All tensors must have the same shape except along the concatenation axis.\n+    /// The backward function splits the gradient and sends each portion to the corresponding input.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Concat stacks tensors together along a dimension.\n+    ///\n+    /// For concatenation:\n+    /// - The forward pass combines multiple tensors into one larger tensor\n+    /// - The backward pass splits the gradient back to each input\n+    /// - Think of it like gluing arrays together end-to-end\n+    ///\n+    /// Used in:\n+    /// - Skip connections (concatenating features from different layers)\n+    /// - Multi-input architectures\n+    /// - Feature fusion in neural networks\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Concat(List<ComputationNode<T>> nodes, int axis = 0)\n+    {\n+        if (nodes.Count == 0)\n+            throw new ArgumentException(\"Cannot concatenate empty list of nodes\");\n+\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var firstShape = nodes[0].Value.Shape;\n+\n+        // Normalize axis\n+        if (axis < 0)\n+            axis = firstShape.Length + axis;\n+\n+        // Validate shapes match except on concat axis\n+        for (int i = 1; i < nodes.Count; i++)\n+        {\n+            var shape = nodes[i].Value.Shape;\n+            if (shape.Length != firstShape.Length)\n+                throw new ArgumentException(\"All tensors must have the same rank\");\n+\n+            for (int d = 0; d < firstShape.Length; d++)\n+            {\n+                if (d != axis && shape[d] != firstShape[d])\n+                    throw new ArgumentException(\n+                        $\"Shape mismatch at dimension {d}: {shape[d]} vs {firstShape[d]}\");\n+            }\n+        }\n+\n+        // Compute output shape\n+        int[] outputShape = (int[])firstShape.Clone();\n+        for (int i = 1; i < nodes.Count; i++)\n+        {\n+            outputShape[axis] += nodes[i].Value.Shape[axis];\n+        }\n+\n+        // Perform concatenation (handle 2D case for simplicity)\n+        Tensor<T> result;\n+        if (firstShape.Length == 2 && axis == 1)\n+        {\n+            // Concatenate along columns (features)\n+            int rows = firstShape[0];\n+            int totalCols = outputShape[1];\n+            result = new Tensor<T>(new int[] { rows, totalCols });\n+\n+            int colOffset = 0;\n+            foreach (var node in nodes)\n+            {\n+                int cols = node.Value.Shape[1];\n+                for (int r = 0; r < rows; r++)\n+                {\n+                    for (int c = 0; c < cols; c++)\n+                    {\n+                        result[r, colOffset + c] = node.Value[r, c];\n+                    }\n+                }\n+                colOffset += cols;\n+            }\n+        }\n+        else if (firstShape.Length == 2 && axis == 0)\n+        {\n+            // Concatenate along rows (batch)\n+            int cols = firstShape[1];\n+            int totalRows = outputShape[0];\n+            result = new Tensor<T>(new int[] { totalRows, cols });\n+\n+            int rowOffset = 0;\n+            foreach (var node in nodes)\n+            {\n+                int rows = node.Value.Shape[0];\n+                for (int r = 0; r < rows; r++)\n+                {\n+                    for (int c = 0; c < cols; c++)\n+                    {\n+                        result[rowOffset + r, c] = node.Value[r, c];\n+                    }\n+                }\n+                rowOffset += rows;\n+            }\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Concat is currently only implemented for 2D tensors. \" +\n+                $\"Got shape=[{string.Join(\", \", firstShape)}]\");\n+        }\n+\n+        // Store sizes for gradient splitting\n+        var sizes = nodes.Select(n => n.Value.Shape[axis]).ToList();\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            // Split gradient along concat axis and distribute to inputs\n+            if (firstShape.Length == 2 && axis == 1)\n+            {\n+                int rows = firstShape[0];\n+                int colOffset = 0;\n+                for (int i = 0; i < nodes.Count; i++)\n+                {\n+                    if (!nodes[i].RequiresGradient)\n+                    {\n+                        colOffset += sizes[i];\n+                        continue;\n+                    }\n+\n+                    int cols = sizes[i];\n+                    var gradPart = new Tensor<T>(new int[] { rows, cols });\n+\n+                    for (int r = 0; r < rows; r++)\n+                    {\n+                        for (int c = 0; c < cols; c++)\n+                        {\n+                            gradPart[r, c] = gradient[r, colOffset + c];\n+                        }\n+                    }\n+\n+                    if (nodes[i].Gradient == null)\n+                        nodes[i].Gradient = gradPart;\n+                    else\n+                        nodes[i].Gradient = nodes[i].Gradient.Add(gradPart);\n+\n+                    colOffset += cols;\n+                }\n+            }\n+            else if (firstShape.Length == 2 && axis == 0)\n+            {\n+                int cols = firstShape[1];\n+                int rowOffset = 0;\n+                for (int i = 0; i < nodes.Count; i++)\n+                {\n+                    if (!nodes[i].RequiresGradient)\n+                    {\n+                        rowOffset += sizes[i];\n+                        continue;\n+                    }\n+\n+                    int rows = sizes[i];\n+                    var gradPart = new Tensor<T>(new int[] { rows, cols });\n+\n+                    for (int r = 0; r < rows; r++)\n+                    {\n+                        for (int c = 0; c < cols; c++)\n+                        {\n+                            gradPart[r, c] = gradient[rowOffset + r, c];\n+                        }\n+                    }\n+\n+                    if (nodes[i].Gradient == null)\n+                        nodes[i].Gradient = gradPart;\n+                    else\n+                        nodes[i].Gradient = nodes[i].Gradient.Add(gradPart);\n+\n+                    rowOffset += rows;\n+                }\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: nodes.Any(n => n.RequiresGradient),\n+            parents: nodes,\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Pads a tensor with a constant value along specified dimensions.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"padWidth\">Padding width for each dimension as (before, after) pairs.</param>\n+    /// <param name=\"value\">The value to use for padding. Default is zero.</param>\n+    /// <returns>A new computation node containing the padded result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method adds padding around the tensor.\n+    /// The backward function simply crops the gradient back to the original size (gradients for padding are zero).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Pad adds extra elements around a tensor.\n+    ///\n+    /// For padding:\n+    /// - The forward pass adds border elements with a constant value\n+    /// - The backward pass removes those border gradients (they don't affect the original tensor)\n+    /// - Think of it like adding margins to an image\n+    ///\n+    /// Used in:\n+    /// - Convolutional layers (to maintain spatial dimensions)\n+    /// - Handling variable-length sequences\n+    /// - Data augmentation\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Pad(ComputationNode<T> a, int[,] padWidth, T? value = default)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var padValue = value ?? numOps.Zero;\n+        var shape = a.Value.Shape;\n+\n+        // Validate padWidth dimensions\n+        if (padWidth.GetLength(0) != shape.Length)\n+            throw new ArgumentException(\"padWidth must have same number of dimensions as input tensor\");\n+\n+        // Compute output shape\n+        var outputShape = new int[shape.Length];\n+        for (int d = 0; d < shape.Length; d++)\n+        {\n+            outputShape[d] = shape[d] + padWidth[d, 0] + padWidth[d, 1];\n+        }\n+\n+        // Handle 2D case\n+        if (shape.Length == 2)\n+        {\n+            int inputRows = shape[0];\n+            int inputCols = shape[1];\n+            int padTop = padWidth[0, 0];\n+            int padBottom = padWidth[0, 1];\n+            int padLeft = padWidth[1, 0];\n+            int padRight = padWidth[1, 1];\n+\n+            var result = new Tensor<T>(outputShape);\n+\n+            // Initialize with pad value\n+            for (int i = 0; i < result.Length; i++)\n+            {\n+                result[i] = padValue;\n+            }\n+\n+            // Copy input data to center\n+            for (int r = 0; r < inputRows; r++)\n+            {\n+                for (int c = 0; c < inputCols; c++)\n+                {\n+                    result[padTop + r, padLeft + c] = a.Value[r, c];\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (a.RequiresGradient)\n+                {\n+                    // Extract gradient for original (unpadded) region\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int r = 0; r < inputRows; r++)\n+                    {\n+                        for (int c = 0; c < inputCols; c++)\n+                        {\n+                            gradA[r, c] = gradient[padTop + r, padLeft + c];\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient,\n+                parents: new List<ComputationNode<T>> { a },\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Pad is currently only implemented for 2D tensors. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}]\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs 2D max pooling on a 4D tensor (batch, channels, height, width).\n+    /// </summary>\n+    /// <param name=\"a\">The input node with shape [batch, channels, height, width].</param>\n+    /// <param name=\"poolSize\">The size of the pooling window [poolH, poolW].</param>\n+    /// <param name=\"strides\">The stride for the pooling operation [strideH, strideW]. If null, uses poolSize.</param>\n+    /// <returns>A new computation node containing the max pooled result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method performs max pooling over 2D spatial dimensions.\n+    /// During forward pass, it tracks which element was the max for routing gradients during backward pass.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> MaxPool downsamples by taking the maximum value in each window.\n+    ///\n+    /// For max pooling:\n+    /// - The forward pass slides a window and takes the max value in each position\n+    /// - This reduces spatial dimensions (downsampling)\n+    /// - The backward pass routes gradients only to the positions that were max\n+    /// - Other positions get zero gradient (they didn't contribute to the output)\n+    ///\n+    /// Used in:\n+    /// - CNNs for translation invariance\n+    /// - Reducing spatial resolution\n+    /// - Building hierarchical features\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> MaxPool2D(\n+        ComputationNode<T> a,\n+        int[] poolSize,\n+        int[]? strides = null)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        if (shape.Length != 4)\n+            throw new ArgumentException(\"MaxPool2D requires 4D input [batch, channels, height, width]\");\n+\n+        strides ??= poolSize;\n+\n+        int batch = shape[0];\n+        int channels = shape[1];\n+        int inH = shape[2];\n+        int inW = shape[3];\n+        int poolH = poolSize[0];\n+        int poolW = poolSize[1];\n+        int strideH = strides[0];\n+        int strideW = strides[1];\n+\n+        int outH = (inH - poolH) / strideH + 1;\n+        int outW = (inW - poolW) / strideW + 1;\n+\n+        var result = new Tensor<T>(new int[] { batch, channels, outH, outW });\n+        // Store max positions for backprop\n+        var maxPositions = new int[batch, channels, outH, outW, 2]; // [h_offset, w_offset]\n+\n+        // Forward pass: compute max pooling and track positions\n+        for (int b = 0; b < batch; b++)\n+        {\n+            for (int c = 0; c < channels; c++)\n+            {\n+                for (int oh = 0; oh < outH; oh++)\n+                {\n+                    for (int ow = 0; ow < outW; ow++)\n+                    {\n+                        int hStart = oh * strideH;\n+                        int wStart = ow * strideW;\n+\n+                        var maxVal = a.Value[b * channels * inH * inW +\n+                                           c * inH * inW +\n+                                           hStart * inW +\n+                                           wStart];\n+                        int maxHOffset = 0;\n+                        int maxWOffset = 0;\n+\n+                        // Find max in pooling window\n+                        for (int ph = 0; ph < poolH; ph++)\n+                        {\n+                            for (int pw = 0; pw < poolW; pw++)\n+                            {\n+                                int h = hStart + ph;\n+                                int w = wStart + pw;\n+                                if (h < inH && w < inW)\n+                                {\n+                                    var val = a.Value[b * channels * inH * inW +\n+                                                     c * inH * inW +\n+                                                     h * inW +\n+                                                     w];\n+                                    if (numOps.GreaterThan(val, maxVal))\n+                                    {\n+                                        maxVal = val;\n+                                        maxHOffset = ph;\n+                                        maxWOffset = pw;\n+                                    }\n+                                }\n+                            }\n+                        }\n+\n+                        result[b, c, oh, ow] = maxVal;\n+                        maxPositions[b, c, oh, ow, 0] = maxHOffset;\n+                        maxPositions[b, c, oh, ow, 1] = maxWOffset;\n+                    }\n+                }\n+            }\n+        }\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            if (a.RequiresGradient)\n+            {\n+                var gradA = new Tensor<T>(shape);\n+\n+                // Route gradients to max positions\n+                for (int b = 0; b < batch; b++)\n+                {\n+                    for (int c = 0; c < channels; c++)\n+                    {\n+                        for (int oh = 0; oh < outH; oh++)\n+                        {\n+                            for (int ow = 0; ow < outW; ow++)\n+                            {\n+                                int hStart = oh * strideH;\n+                                int wStart = ow * strideW;\n+                                int maxHOffset = maxPositions[b, c, oh, ow, 0];\n+                                int maxWOffset = maxPositions[b, c, oh, ow, 1];\n+\n+                                int maxH = hStart + maxHOffset;\n+                                int maxW = wStart + maxWOffset;\n+\n+                                int gradIdx = b * channels * inH * inW +\n+                                             c * inH * inW +\n+                                            maxH * inW +\n+                                             maxW;\n+\n+                                gradA[gradIdx] = numOps.Add(gradA[gradIdx], gradient[b, c, oh, ow]);\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (a.Gradient == null)\n+                    a.Gradient = gradA;\n+                else\n+                    a.Gradient = a.Gradient.Add(gradA);\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: a.RequiresGradient,\n+            parents: new List<ComputationNode<T>> { a },\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Performs 2D average pooling on a 4D tensor (batch, channels, height, width).\n+    /// </summary>\n+    /// <param name=\"a\">The input node with shape [batch, channels, height, width].</param>\n+    /// <param name=\"poolSize\">The size of the pooling window [poolH, poolW].</param>\n+    /// <param name=\"strides\">The stride for the pooling operation [strideH, strideW]. If null, uses poolSize.</param>\n+    /// <returns>A new computation node containing the average pooled result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method performs average pooling over 2D spatial dimensions.\n+    /// The backward function distributes gradients equally across the pooling window.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> AvgPool downsamples by taking the average value in each window.\n+    ///\n+    /// For average pooling:\n+    /// - The forward pass slides a window and computes the average\n+    /// - This smoothly reduces spatial dimensions\n+    /// - The backward pass distributes gradients equally to all elements in the window\n+    /// - Each element gets gradient / pool_area\n+    ///\n+    /// Used in:\n+    /// - CNNs for smoother downsampling than max pooling\n+    /// - Global average pooling (replacing fully connected layers)\n+    /// - Reducing overfitting\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> AvgPool2D(\n+        ComputationNode<T> a,\n+        int[] poolSize,\n+        int[]? strides = null)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        if (shape.Length != 4)\n+            throw new ArgumentException(\"AvgPool2D requires 4D input [batch, channels, height, width]\");\n+\n+        strides ??= poolSize;\n+\n+        int batch = shape[0];\n+        int channels = shape[1];\n+        int inH = shape[2];\n+        int inW = shape[3];\n+        int poolH = poolSize[0];\n+        int poolW = poolSize[1];\n+        int strideH = strides[0];\n+        int strideW = strides[1];\n+\n+        int outH = (inH - poolH) / strideH + 1;\n+        int outW = (inW - poolW) / strideW + 1;\n+\n+        var result = new Tensor<T>(new int[] { batch, channels, outH, outW });\n+        var poolArea = numOps.FromDouble(poolH * poolW);\n+\n+        // Forward pass: compute average pooling\n+        for (int b = 0; b < batch; b++)\n+        {\n+            for (int c = 0; c < channels; c++)\n+            {\n+                for (int oh = 0; oh < outH; oh++)\n+                {\n+                    for (int ow = 0; ow < outW; ow++)\n+                    {\n+                        int hStart = oh * strideH;\n+                        int wStart = ow * strideW;\n+\n+                        var sum = numOps.Zero;\n+\n+                        // Sum values in pooling window\n+                        for (int ph = 0; ph < poolH; ph++)\n+                        {\n+                            for (int pw = 0; pw < poolW; pw++)\n+                            {\n+                                int h = hStart + ph;\n+                                int w = wStart + pw;\n+                                if (h < inH && w < inW)\n+                                {\n+                                    sum = numOps.Add(sum, a.Value[b, c, h, w]);\n+                                }\n+                            }\n+                        }\n+\n+                        result[b, c, oh, ow] = numOps.Divide(sum, poolArea);\n+                    }\n+                }\n+            }\n+        }\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            if (a.RequiresGradient)\n+            {\n+                var gradA = new Tensor<T>(shape);\n+\n+                // Distribute gradients equally across pooling windows\n+                for (int b = 0; b < batch; b++)\n+                {\n+                    for (int c = 0; c < channels; c++)\n+                    {\n+                        for (int oh = 0; oh < outH; oh++)\n+                        {\n+                            for (int ow = 0; ow < outW; ow++)\n+                            {\n+                                int hStart = oh * strideH;\n+                                int wStart = ow * strideW;\n+\n+                                var gradValue = numOps.Divide(gradient[b, c, oh, ow], poolArea);\n+\n+                                // Distribute to all elements in window\n+                                for (int ph = 0; ph < poolH; ph++)\n+                                {\n+                                    for (int pw = 0; pw < poolW; pw++)\n+                                    {\n+                                        int h = hStart + ph;\n+                                        int w = wStart + pw;\n+                                        if (h < inH && w < inW)\n+                                        {\n+                                            gradA[b, c, h, w] = numOps.Add(gradA[b, c, h, w], gradValue);\n+                                        }\n+                                    }\n+                                }\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (a.Gradient == null)\n+                    a.Gradient = gradA;\n+                else\n+                    a.Gradient = a.Gradient.Add(gradA);\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: a.RequiresGradient,\n+            parents: new List<ComputationNode<T>> { a },\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Applies layer normalization to a computation node.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"normalizedShape\">The shape over which to normalize (typically the feature dimensions).</param>\n+    /// <param name=\"gamma\">Optional scale parameter (learnable). If null, uses ones.</param>\n+    /// <param name=\"beta\">Optional shift parameter (learnable). If null, uses zeros.</param>\n+    /// <param name=\"epsilon\">Small constant for numerical stability. Default is 1e-5.</param>\n+    /// <returns>A new computation node containing the layer normalized result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Layer normalization normalizes inputs across the feature dimension for each sample independently.\n+    /// Formula: y = gamma * (x - mean) / sqrt(variance + epsilon) + beta\n+    /// Unlike batch normalization, this doesn't depend on batch statistics.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> LayerNorm standardizes features for each sample independently.\n+    ///\n+    /// For layer normalization:\n+    /// - Computes mean and variance for each sample's features\n+    /// - Normalizes: (x - mean) / sqrt(variance)\n+    /// - Scales and shifts: result * gamma + beta\n+    /// - Works the same during training and inference (no batch dependency)\n+    ///\n+    /// Used in:\n+    /// - Transformers (critical component)\n+    /// - RNNs (stabilizes training)\n+    /// - Any architecture needing sample-independent normalization\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> LayerNorm(\n+        ComputationNode<T> a,\n+        int[] normalizedShape,\n+        ComputationNode<T>? gamma = null,\n+        ComputationNode<T>? beta = null,\n+        double epsilon = 1e-5)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+        var eps = numOps.FromDouble(epsilon);\n+\n+        // For 2D input [batch, features], normalize over features\n+        if (shape.Length == 2 && normalizedShape.Length == 1 && normalizedShape[0] == shape[1])\n+        {\n+            int batchSize = shape[0];\n+            int features = shape[1];\n+\n+            // Create default gamma (ones) and beta (zeros) if not provided\n+            if (gamma == null)\n+            {\n+                var gammaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    gammaTensor[i] = numOps.One;\n+                gamma = Variable(gammaTensor, requiresGradient: false);\n+            }\n+\n+            if (beta == null)\n+            {\n+                var betaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    betaTensor[i] = numOps.Zero;\n+                beta = Variable(betaTensor, requiresGradient: false);\n+            }\n+\n+            var result = new Tensor<T>(shape);\n+            var means = new T[batchSize];\n+            var variances = new T[batchSize];\n+            var normalized = new Tensor<T>(shape);\n+\n+            // Forward pass: compute mean and variance per sample\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                // Compute mean\n+                var sum = numOps.Zero;\n+                for (int f = 0; f < features; f++)\n+                {\n+                    sum = numOps.Add(sum, a.Value[b, f]);\n+                }\n+                means[b] = numOps.Divide(sum, numOps.FromDouble(features));\n+\n+                // Compute variance\n+                var varSum = numOps.Zero;\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var diff = numOps.Subtract(a.Value[b, f], means[b]);\n+                    varSum = numOps.Add(varSum, numOps.Multiply(diff, diff));\n+                }\n+                variances[b] = numOps.Divide(varSum, numOps.FromDouble(features));\n+\n+                // Normalize and scale\n+                var std = numOps.Sqrt(numOps.Add(variances[b], eps));\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var norm = numOps.Divide(\n+                        numOps.Subtract(a.Value[b, f], means[b]),\n+                        std);\n+                    normalized[b, f] = norm;\n+                    result[b, f] = numOps.Add(\n+                        numOps.Multiply(norm, gamma.Value[f]),\n+                        beta.Value[f]);\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                // Gradients for gamma and beta\n+                if (gamma.RequiresGradient)\n+                {\n+                    var gradGamma = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum,\n+                                numOps.Multiply(gradient[b, f], normalized[b, f]));\n+                        }\n+                        gradGamma[f] = sum;\n+                    }\n+\n+                    if (gamma.Gradient == null)\n+                        gamma.Gradient = gradGamma;\n+                    else\n+                        gamma.Gradient = gamma.Gradient.Add(gradGamma);"}]}},{"id":"PRRT_kwDOKSXUF85hck-Q","isResolved":false,"isOutdated":false,"path":"src/Autodiff/TensorOperations.cs","line":2099,"originalLine":2099,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzD-g","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement write to the same variable - consider using '?' to express intent better.\n```suggestion\n                    beta.Gradient = beta.Gradient?.Add(gradBeta) ?? gradBeta;\n```","createdAt":"2025-11-11T07:53:24Z","path":"src/Autodiff/TensorOperations.cs","line":2099,"originalLine":2099,"diffHunk":"@@ -1219,4 +1219,1769 @@ void BackwardFunction(Tensor<T> gradient)\n \n         return node;\n     }\n+\n+    /// <summary>\n+    /// Computes the softmax function for a computation node along a specified axis.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"axis\">The axis along which to compute softmax. Default is -1 (last axis).</param>\n+    /// <returns>A new computation node containing the softmax result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method computes softmax (σ(x_i) = exp(x_i) / Σexp(x_j)) along the specified axis.\n+    /// Uses numerical stability trick: subtract max before exponentiating.\n+    /// The backward function uses: ∂softmax/∂x = softmax(x) * (grad - Σ(grad * softmax(x))).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Softmax converts a vector of numbers into probabilities.\n+    ///\n+    /// For softmax:\n+    /// - The forward pass exponentiates each element, then normalizes so they sum to 1\n+    /// - The result is a probability distribution (all values between 0 and 1, summing to 1)\n+    /// - The backward pass is complex but efficient: uses the Jacobian of softmax\n+    ///\n+    /// Softmax is crucial for:\n+    /// - Multi-class classification (final layer outputs)\n+    /// - Attention mechanisms (computing attention weights)\n+    /// - Anywhere you need to convert scores to probabilities\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Softmax(ComputationNode<T> a, int axis = -1)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        // Normalize axis to positive index\n+        if (axis < 0)\n+            axis = shape.Length + axis;\n+\n+        // For simplicity, handle 2D case (batch, features) with axis=-1\n+        if (shape.Length == 2 && axis == 1)\n+        {\n+            int batchSize = shape[0];\n+            int features = shape[1];\n+            var result = new Tensor<T>(shape);\n+\n+            // Compute softmax for each row\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                // Find max for numerical stability\n+                var maxVal = a.Value[b, 0];\n+                for (int f = 1; f < features; f++)\n+                {\n+                    if (numOps.GreaterThan(a.Value[b, f], maxVal))\n+                        maxVal = a.Value[b, f];\n+                }\n+\n+                // Compute exp(x - max) and sum\n+                var expSum = numOps.Zero;\n+                var expValues = new T[features];\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var shifted = numOps.Subtract(a.Value[b, f], maxVal);\n+                    expValues[f] = numOps.Exp(shifted);\n+                    expSum = numOps.Add(expSum, expValues[f]);\n+                }\n+\n+                // Normalize\n+                for (int f = 0; f < features; f++)\n+                {\n+                    result[b, f] = numOps.Divide(expValues[f], expSum);\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (a.RequiresGradient)\n+                {\n+                    // ∂softmax/∂x_i = softmax_i * (∂L/∂y_i - Σ_j(∂L/∂y_j * softmax_j))\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        // Compute sum of (gradient * softmax)\n+                        var dotProduct = numOps.Zero;\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            dotProduct = numOps.Add(dotProduct,\n+                                numOps.Multiply(gradient[b, f], result[b, f]));\n+                        }\n+\n+                        // Compute gradient for each element\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            var gradMinusDot = numOps.Subtract(gradient[b, f], dotProduct);\n+                            gradA[b, f] = numOps.Multiply(result[b, f], gradMinusDot);\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient,\n+                parents: new List<ComputationNode<T>> { a },\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Softmax is currently only implemented for 2D tensors along axis=-1. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}], axis={axis}\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Concatenates multiple computation nodes along a specified axis.\n+    /// </summary>\n+    /// <param name=\"nodes\">The list of nodes to concatenate.</param>\n+    /// <param name=\"axis\">The axis along which to concatenate. Default is 0.</param>\n+    /// <returns>A new computation node containing the concatenated result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method concatenates tensors along the specified axis.\n+    /// All tensors must have the same shape except along the concatenation axis.\n+    /// The backward function splits the gradient and sends each portion to the corresponding input.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Concat stacks tensors together along a dimension.\n+    ///\n+    /// For concatenation:\n+    /// - The forward pass combines multiple tensors into one larger tensor\n+    /// - The backward pass splits the gradient back to each input\n+    /// - Think of it like gluing arrays together end-to-end\n+    ///\n+    /// Used in:\n+    /// - Skip connections (concatenating features from different layers)\n+    /// - Multi-input architectures\n+    /// - Feature fusion in neural networks\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Concat(List<ComputationNode<T>> nodes, int axis = 0)\n+    {\n+        if (nodes.Count == 0)\n+            throw new ArgumentException(\"Cannot concatenate empty list of nodes\");\n+\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var firstShape = nodes[0].Value.Shape;\n+\n+        // Normalize axis\n+        if (axis < 0)\n+            axis = firstShape.Length + axis;\n+\n+        // Validate shapes match except on concat axis\n+        for (int i = 1; i < nodes.Count; i++)\n+        {\n+            var shape = nodes[i].Value.Shape;\n+            if (shape.Length != firstShape.Length)\n+                throw new ArgumentException(\"All tensors must have the same rank\");\n+\n+            for (int d = 0; d < firstShape.Length; d++)\n+            {\n+                if (d != axis && shape[d] != firstShape[d])\n+                    throw new ArgumentException(\n+                        $\"Shape mismatch at dimension {d}: {shape[d]} vs {firstShape[d]}\");\n+            }\n+        }\n+\n+        // Compute output shape\n+        int[] outputShape = (int[])firstShape.Clone();\n+        for (int i = 1; i < nodes.Count; i++)\n+        {\n+            outputShape[axis] += nodes[i].Value.Shape[axis];\n+        }\n+\n+        // Perform concatenation (handle 2D case for simplicity)\n+        Tensor<T> result;\n+        if (firstShape.Length == 2 && axis == 1)\n+        {\n+            // Concatenate along columns (features)\n+            int rows = firstShape[0];\n+            int totalCols = outputShape[1];\n+            result = new Tensor<T>(new int[] { rows, totalCols });\n+\n+            int colOffset = 0;\n+            foreach (var node in nodes)\n+            {\n+                int cols = node.Value.Shape[1];\n+                for (int r = 0; r < rows; r++)\n+                {\n+                    for (int c = 0; c < cols; c++)\n+                    {\n+                        result[r, colOffset + c] = node.Value[r, c];\n+                    }\n+                }\n+                colOffset += cols;\n+            }\n+        }\n+        else if (firstShape.Length == 2 && axis == 0)\n+        {\n+            // Concatenate along rows (batch)\n+            int cols = firstShape[1];\n+            int totalRows = outputShape[0];\n+            result = new Tensor<T>(new int[] { totalRows, cols });\n+\n+            int rowOffset = 0;\n+            foreach (var node in nodes)\n+            {\n+                int rows = node.Value.Shape[0];\n+                for (int r = 0; r < rows; r++)\n+                {\n+                    for (int c = 0; c < cols; c++)\n+                    {\n+                        result[rowOffset + r, c] = node.Value[r, c];\n+                    }\n+                }\n+                rowOffset += rows;\n+            }\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Concat is currently only implemented for 2D tensors. \" +\n+                $\"Got shape=[{string.Join(\", \", firstShape)}]\");\n+        }\n+\n+        // Store sizes for gradient splitting\n+        var sizes = nodes.Select(n => n.Value.Shape[axis]).ToList();\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            // Split gradient along concat axis and distribute to inputs\n+            if (firstShape.Length == 2 && axis == 1)\n+            {\n+                int rows = firstShape[0];\n+                int colOffset = 0;\n+                for (int i = 0; i < nodes.Count; i++)\n+                {\n+                    if (!nodes[i].RequiresGradient)\n+                    {\n+                        colOffset += sizes[i];\n+                        continue;\n+                    }\n+\n+                    int cols = sizes[i];\n+                    var gradPart = new Tensor<T>(new int[] { rows, cols });\n+\n+                    for (int r = 0; r < rows; r++)\n+                    {\n+                        for (int c = 0; c < cols; c++)\n+                        {\n+                            gradPart[r, c] = gradient[r, colOffset + c];\n+                        }\n+                    }\n+\n+                    if (nodes[i].Gradient == null)\n+                        nodes[i].Gradient = gradPart;\n+                    else\n+                        nodes[i].Gradient = nodes[i].Gradient.Add(gradPart);\n+\n+                    colOffset += cols;\n+                }\n+            }\n+            else if (firstShape.Length == 2 && axis == 0)\n+            {\n+                int cols = firstShape[1];\n+                int rowOffset = 0;\n+                for (int i = 0; i < nodes.Count; i++)\n+                {\n+                    if (!nodes[i].RequiresGradient)\n+                    {\n+                        rowOffset += sizes[i];\n+                        continue;\n+                    }\n+\n+                    int rows = sizes[i];\n+                    var gradPart = new Tensor<T>(new int[] { rows, cols });\n+\n+                    for (int r = 0; r < rows; r++)\n+                    {\n+                        for (int c = 0; c < cols; c++)\n+                        {\n+                            gradPart[r, c] = gradient[rowOffset + r, c];\n+                        }\n+                    }\n+\n+                    if (nodes[i].Gradient == null)\n+                        nodes[i].Gradient = gradPart;\n+                    else\n+                        nodes[i].Gradient = nodes[i].Gradient.Add(gradPart);\n+\n+                    rowOffset += rows;\n+                }\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: nodes.Any(n => n.RequiresGradient),\n+            parents: nodes,\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Pads a tensor with a constant value along specified dimensions.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"padWidth\">Padding width for each dimension as (before, after) pairs.</param>\n+    /// <param name=\"value\">The value to use for padding. Default is zero.</param>\n+    /// <returns>A new computation node containing the padded result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method adds padding around the tensor.\n+    /// The backward function simply crops the gradient back to the original size (gradients for padding are zero).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Pad adds extra elements around a tensor.\n+    ///\n+    /// For padding:\n+    /// - The forward pass adds border elements with a constant value\n+    /// - The backward pass removes those border gradients (they don't affect the original tensor)\n+    /// - Think of it like adding margins to an image\n+    ///\n+    /// Used in:\n+    /// - Convolutional layers (to maintain spatial dimensions)\n+    /// - Handling variable-length sequences\n+    /// - Data augmentation\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Pad(ComputationNode<T> a, int[,] padWidth, T? value = default)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var padValue = value ?? numOps.Zero;\n+        var shape = a.Value.Shape;\n+\n+        // Validate padWidth dimensions\n+        if (padWidth.GetLength(0) != shape.Length)\n+            throw new ArgumentException(\"padWidth must have same number of dimensions as input tensor\");\n+\n+        // Compute output shape\n+        var outputShape = new int[shape.Length];\n+        for (int d = 0; d < shape.Length; d++)\n+        {\n+            outputShape[d] = shape[d] + padWidth[d, 0] + padWidth[d, 1];\n+        }\n+\n+        // Handle 2D case\n+        if (shape.Length == 2)\n+        {\n+            int inputRows = shape[0];\n+            int inputCols = shape[1];\n+            int padTop = padWidth[0, 0];\n+            int padBottom = padWidth[0, 1];\n+            int padLeft = padWidth[1, 0];\n+            int padRight = padWidth[1, 1];\n+\n+            var result = new Tensor<T>(outputShape);\n+\n+            // Initialize with pad value\n+            for (int i = 0; i < result.Length; i++)\n+            {\n+                result[i] = padValue;\n+            }\n+\n+            // Copy input data to center\n+            for (int r = 0; r < inputRows; r++)\n+            {\n+                for (int c = 0; c < inputCols; c++)\n+                {\n+                    result[padTop + r, padLeft + c] = a.Value[r, c];\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (a.RequiresGradient)\n+                {\n+                    // Extract gradient for original (unpadded) region\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int r = 0; r < inputRows; r++)\n+                    {\n+                        for (int c = 0; c < inputCols; c++)\n+                        {\n+                            gradA[r, c] = gradient[padTop + r, padLeft + c];\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient,\n+                parents: new List<ComputationNode<T>> { a },\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Pad is currently only implemented for 2D tensors. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}]\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs 2D max pooling on a 4D tensor (batch, channels, height, width).\n+    /// </summary>\n+    /// <param name=\"a\">The input node with shape [batch, channels, height, width].</param>\n+    /// <param name=\"poolSize\">The size of the pooling window [poolH, poolW].</param>\n+    /// <param name=\"strides\">The stride for the pooling operation [strideH, strideW]. If null, uses poolSize.</param>\n+    /// <returns>A new computation node containing the max pooled result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method performs max pooling over 2D spatial dimensions.\n+    /// During forward pass, it tracks which element was the max for routing gradients during backward pass.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> MaxPool downsamples by taking the maximum value in each window.\n+    ///\n+    /// For max pooling:\n+    /// - The forward pass slides a window and takes the max value in each position\n+    /// - This reduces spatial dimensions (downsampling)\n+    /// - The backward pass routes gradients only to the positions that were max\n+    /// - Other positions get zero gradient (they didn't contribute to the output)\n+    ///\n+    /// Used in:\n+    /// - CNNs for translation invariance\n+    /// - Reducing spatial resolution\n+    /// - Building hierarchical features\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> MaxPool2D(\n+        ComputationNode<T> a,\n+        int[] poolSize,\n+        int[]? strides = null)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        if (shape.Length != 4)\n+            throw new ArgumentException(\"MaxPool2D requires 4D input [batch, channels, height, width]\");\n+\n+        strides ??= poolSize;\n+\n+        int batch = shape[0];\n+        int channels = shape[1];\n+        int inH = shape[2];\n+        int inW = shape[3];\n+        int poolH = poolSize[0];\n+        int poolW = poolSize[1];\n+        int strideH = strides[0];\n+        int strideW = strides[1];\n+\n+        int outH = (inH - poolH) / strideH + 1;\n+        int outW = (inW - poolW) / strideW + 1;\n+\n+        var result = new Tensor<T>(new int[] { batch, channels, outH, outW });\n+        // Store max positions for backprop\n+        var maxPositions = new int[batch, channels, outH, outW, 2]; // [h_offset, w_offset]\n+\n+        // Forward pass: compute max pooling and track positions\n+        for (int b = 0; b < batch; b++)\n+        {\n+            for (int c = 0; c < channels; c++)\n+            {\n+                for (int oh = 0; oh < outH; oh++)\n+                {\n+                    for (int ow = 0; ow < outW; ow++)\n+                    {\n+                        int hStart = oh * strideH;\n+                        int wStart = ow * strideW;\n+\n+                        var maxVal = a.Value[b * channels * inH * inW +\n+                                           c * inH * inW +\n+                                           hStart * inW +\n+                                           wStart];\n+                        int maxHOffset = 0;\n+                        int maxWOffset = 0;\n+\n+                        // Find max in pooling window\n+                        for (int ph = 0; ph < poolH; ph++)\n+                        {\n+                            for (int pw = 0; pw < poolW; pw++)\n+                            {\n+                                int h = hStart + ph;\n+                                int w = wStart + pw;\n+                                if (h < inH && w < inW)\n+                                {\n+                                    var val = a.Value[b * channels * inH * inW +\n+                                                     c * inH * inW +\n+                                                     h * inW +\n+                                                     w];\n+                                    if (numOps.GreaterThan(val, maxVal))\n+                                    {\n+                                        maxVal = val;\n+                                        maxHOffset = ph;\n+                                        maxWOffset = pw;\n+                                    }\n+                                }\n+                            }\n+                        }\n+\n+                        result[b, c, oh, ow] = maxVal;\n+                        maxPositions[b, c, oh, ow, 0] = maxHOffset;\n+                        maxPositions[b, c, oh, ow, 1] = maxWOffset;\n+                    }\n+                }\n+            }\n+        }\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            if (a.RequiresGradient)\n+            {\n+                var gradA = new Tensor<T>(shape);\n+\n+                // Route gradients to max positions\n+                for (int b = 0; b < batch; b++)\n+                {\n+                    for (int c = 0; c < channels; c++)\n+                    {\n+                        for (int oh = 0; oh < outH; oh++)\n+                        {\n+                            for (int ow = 0; ow < outW; ow++)\n+                            {\n+                                int hStart = oh * strideH;\n+                                int wStart = ow * strideW;\n+                                int maxHOffset = maxPositions[b, c, oh, ow, 0];\n+                                int maxWOffset = maxPositions[b, c, oh, ow, 1];\n+\n+                                int maxH = hStart + maxHOffset;\n+                                int maxW = wStart + maxWOffset;\n+\n+                                int gradIdx = b * channels * inH * inW +\n+                                             c * inH * inW +\n+                                            maxH * inW +\n+                                             maxW;\n+\n+                                gradA[gradIdx] = numOps.Add(gradA[gradIdx], gradient[b, c, oh, ow]);\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (a.Gradient == null)\n+                    a.Gradient = gradA;\n+                else\n+                    a.Gradient = a.Gradient.Add(gradA);\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: a.RequiresGradient,\n+            parents: new List<ComputationNode<T>> { a },\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Performs 2D average pooling on a 4D tensor (batch, channels, height, width).\n+    /// </summary>\n+    /// <param name=\"a\">The input node with shape [batch, channels, height, width].</param>\n+    /// <param name=\"poolSize\">The size of the pooling window [poolH, poolW].</param>\n+    /// <param name=\"strides\">The stride for the pooling operation [strideH, strideW]. If null, uses poolSize.</param>\n+    /// <returns>A new computation node containing the average pooled result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method performs average pooling over 2D spatial dimensions.\n+    /// The backward function distributes gradients equally across the pooling window.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> AvgPool downsamples by taking the average value in each window.\n+    ///\n+    /// For average pooling:\n+    /// - The forward pass slides a window and computes the average\n+    /// - This smoothly reduces spatial dimensions\n+    /// - The backward pass distributes gradients equally to all elements in the window\n+    /// - Each element gets gradient / pool_area\n+    ///\n+    /// Used in:\n+    /// - CNNs for smoother downsampling than max pooling\n+    /// - Global average pooling (replacing fully connected layers)\n+    /// - Reducing overfitting\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> AvgPool2D(\n+        ComputationNode<T> a,\n+        int[] poolSize,\n+        int[]? strides = null)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        if (shape.Length != 4)\n+            throw new ArgumentException(\"AvgPool2D requires 4D input [batch, channels, height, width]\");\n+\n+        strides ??= poolSize;\n+\n+        int batch = shape[0];\n+        int channels = shape[1];\n+        int inH = shape[2];\n+        int inW = shape[3];\n+        int poolH = poolSize[0];\n+        int poolW = poolSize[1];\n+        int strideH = strides[0];\n+        int strideW = strides[1];\n+\n+        int outH = (inH - poolH) / strideH + 1;\n+        int outW = (inW - poolW) / strideW + 1;\n+\n+        var result = new Tensor<T>(new int[] { batch, channels, outH, outW });\n+        var poolArea = numOps.FromDouble(poolH * poolW);\n+\n+        // Forward pass: compute average pooling\n+        for (int b = 0; b < batch; b++)\n+        {\n+            for (int c = 0; c < channels; c++)\n+            {\n+                for (int oh = 0; oh < outH; oh++)\n+                {\n+                    for (int ow = 0; ow < outW; ow++)\n+                    {\n+                        int hStart = oh * strideH;\n+                        int wStart = ow * strideW;\n+\n+                        var sum = numOps.Zero;\n+\n+                        // Sum values in pooling window\n+                        for (int ph = 0; ph < poolH; ph++)\n+                        {\n+                            for (int pw = 0; pw < poolW; pw++)\n+                            {\n+                                int h = hStart + ph;\n+                                int w = wStart + pw;\n+                                if (h < inH && w < inW)\n+                                {\n+                                    sum = numOps.Add(sum, a.Value[b, c, h, w]);\n+                                }\n+                            }\n+                        }\n+\n+                        result[b, c, oh, ow] = numOps.Divide(sum, poolArea);\n+                    }\n+                }\n+            }\n+        }\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            if (a.RequiresGradient)\n+            {\n+                var gradA = new Tensor<T>(shape);\n+\n+                // Distribute gradients equally across pooling windows\n+                for (int b = 0; b < batch; b++)\n+                {\n+                    for (int c = 0; c < channels; c++)\n+                    {\n+                        for (int oh = 0; oh < outH; oh++)\n+                        {\n+                            for (int ow = 0; ow < outW; ow++)\n+                            {\n+                                int hStart = oh * strideH;\n+                                int wStart = ow * strideW;\n+\n+                                var gradValue = numOps.Divide(gradient[b, c, oh, ow], poolArea);\n+\n+                                // Distribute to all elements in window\n+                                for (int ph = 0; ph < poolH; ph++)\n+                                {\n+                                    for (int pw = 0; pw < poolW; pw++)\n+                                    {\n+                                        int h = hStart + ph;\n+                                        int w = wStart + pw;\n+                                        if (h < inH && w < inW)\n+                                        {\n+                                            gradA[b, c, h, w] = numOps.Add(gradA[b, c, h, w], gradValue);\n+                                        }\n+                                    }\n+                                }\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (a.Gradient == null)\n+                    a.Gradient = gradA;\n+                else\n+                    a.Gradient = a.Gradient.Add(gradA);\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: a.RequiresGradient,\n+            parents: new List<ComputationNode<T>> { a },\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Applies layer normalization to a computation node.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"normalizedShape\">The shape over which to normalize (typically the feature dimensions).</param>\n+    /// <param name=\"gamma\">Optional scale parameter (learnable). If null, uses ones.</param>\n+    /// <param name=\"beta\">Optional shift parameter (learnable). If null, uses zeros.</param>\n+    /// <param name=\"epsilon\">Small constant for numerical stability. Default is 1e-5.</param>\n+    /// <returns>A new computation node containing the layer normalized result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Layer normalization normalizes inputs across the feature dimension for each sample independently.\n+    /// Formula: y = gamma * (x - mean) / sqrt(variance + epsilon) + beta\n+    /// Unlike batch normalization, this doesn't depend on batch statistics.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> LayerNorm standardizes features for each sample independently.\n+    ///\n+    /// For layer normalization:\n+    /// - Computes mean and variance for each sample's features\n+    /// - Normalizes: (x - mean) / sqrt(variance)\n+    /// - Scales and shifts: result * gamma + beta\n+    /// - Works the same during training and inference (no batch dependency)\n+    ///\n+    /// Used in:\n+    /// - Transformers (critical component)\n+    /// - RNNs (stabilizes training)\n+    /// - Any architecture needing sample-independent normalization\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> LayerNorm(\n+        ComputationNode<T> a,\n+        int[] normalizedShape,\n+        ComputationNode<T>? gamma = null,\n+        ComputationNode<T>? beta = null,\n+        double epsilon = 1e-5)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+        var eps = numOps.FromDouble(epsilon);\n+\n+        // For 2D input [batch, features], normalize over features\n+        if (shape.Length == 2 && normalizedShape.Length == 1 && normalizedShape[0] == shape[1])\n+        {\n+            int batchSize = shape[0];\n+            int features = shape[1];\n+\n+            // Create default gamma (ones) and beta (zeros) if not provided\n+            if (gamma == null)\n+            {\n+                var gammaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    gammaTensor[i] = numOps.One;\n+                gamma = Variable(gammaTensor, requiresGradient: false);\n+            }\n+\n+            if (beta == null)\n+            {\n+                var betaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    betaTensor[i] = numOps.Zero;\n+                beta = Variable(betaTensor, requiresGradient: false);\n+            }\n+\n+            var result = new Tensor<T>(shape);\n+            var means = new T[batchSize];\n+            var variances = new T[batchSize];\n+            var normalized = new Tensor<T>(shape);\n+\n+            // Forward pass: compute mean and variance per sample\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                // Compute mean\n+                var sum = numOps.Zero;\n+                for (int f = 0; f < features; f++)\n+                {\n+                    sum = numOps.Add(sum, a.Value[b, f]);\n+                }\n+                means[b] = numOps.Divide(sum, numOps.FromDouble(features));\n+\n+                // Compute variance\n+                var varSum = numOps.Zero;\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var diff = numOps.Subtract(a.Value[b, f], means[b]);\n+                    varSum = numOps.Add(varSum, numOps.Multiply(diff, diff));\n+                }\n+                variances[b] = numOps.Divide(varSum, numOps.FromDouble(features));\n+\n+                // Normalize and scale\n+                var std = numOps.Sqrt(numOps.Add(variances[b], eps));\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var norm = numOps.Divide(\n+                        numOps.Subtract(a.Value[b, f], means[b]),\n+                        std);\n+                    normalized[b, f] = norm;\n+                    result[b, f] = numOps.Add(\n+                        numOps.Multiply(norm, gamma.Value[f]),\n+                        beta.Value[f]);\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                // Gradients for gamma and beta\n+                if (gamma.RequiresGradient)\n+                {\n+                    var gradGamma = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum,\n+                                numOps.Multiply(gradient[b, f], normalized[b, f]));\n+                        }\n+                        gradGamma[f] = sum;\n+                    }\n+\n+                    if (gamma.Gradient == null)\n+                        gamma.Gradient = gradGamma;\n+                    else\n+                        gamma.Gradient = gamma.Gradient.Add(gradGamma);\n+                }\n+\n+                if (beta.RequiresGradient)\n+                {\n+                    var gradBeta = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum, gradient[b, f]);\n+                        }\n+                        gradBeta[f] = sum;\n+                    }\n+\n+                    if (beta.Gradient == null)\n+                        beta.Gradient = gradBeta;\n+                    else\n+                        beta.Gradient = beta.Gradient.Add(gradBeta);"}]}},{"id":"PRRT_kwDOKSXUF85hck-W","isResolved":false,"isOutdated":false,"path":"src/Autodiff/TensorOperations.cs","line":2147,"originalLine":2147,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzD-n","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement write to the same variable - consider using '?' to express intent better.\n```suggestion\n                    a.Gradient = a.Gradient == null ? gradA : a.Gradient.Add(gradA);\n```","createdAt":"2025-11-11T07:53:25Z","path":"src/Autodiff/TensorOperations.cs","line":2147,"originalLine":2147,"diffHunk":"@@ -1219,4 +1219,1769 @@ void BackwardFunction(Tensor<T> gradient)\n \n         return node;\n     }\n+\n+    /// <summary>\n+    /// Computes the softmax function for a computation node along a specified axis.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"axis\">The axis along which to compute softmax. Default is -1 (last axis).</param>\n+    /// <returns>A new computation node containing the softmax result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method computes softmax (σ(x_i) = exp(x_i) / Σexp(x_j)) along the specified axis.\n+    /// Uses numerical stability trick: subtract max before exponentiating.\n+    /// The backward function uses: ∂softmax/∂x = softmax(x) * (grad - Σ(grad * softmax(x))).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Softmax converts a vector of numbers into probabilities.\n+    ///\n+    /// For softmax:\n+    /// - The forward pass exponentiates each element, then normalizes so they sum to 1\n+    /// - The result is a probability distribution (all values between 0 and 1, summing to 1)\n+    /// - The backward pass is complex but efficient: uses the Jacobian of softmax\n+    ///\n+    /// Softmax is crucial for:\n+    /// - Multi-class classification (final layer outputs)\n+    /// - Attention mechanisms (computing attention weights)\n+    /// - Anywhere you need to convert scores to probabilities\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Softmax(ComputationNode<T> a, int axis = -1)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        // Normalize axis to positive index\n+        if (axis < 0)\n+            axis = shape.Length + axis;\n+\n+        // For simplicity, handle 2D case (batch, features) with axis=-1\n+        if (shape.Length == 2 && axis == 1)\n+        {\n+            int batchSize = shape[0];\n+            int features = shape[1];\n+            var result = new Tensor<T>(shape);\n+\n+            // Compute softmax for each row\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                // Find max for numerical stability\n+                var maxVal = a.Value[b, 0];\n+                for (int f = 1; f < features; f++)\n+                {\n+                    if (numOps.GreaterThan(a.Value[b, f], maxVal))\n+                        maxVal = a.Value[b, f];\n+                }\n+\n+                // Compute exp(x - max) and sum\n+                var expSum = numOps.Zero;\n+                var expValues = new T[features];\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var shifted = numOps.Subtract(a.Value[b, f], maxVal);\n+                    expValues[f] = numOps.Exp(shifted);\n+                    expSum = numOps.Add(expSum, expValues[f]);\n+                }\n+\n+                // Normalize\n+                for (int f = 0; f < features; f++)\n+                {\n+                    result[b, f] = numOps.Divide(expValues[f], expSum);\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (a.RequiresGradient)\n+                {\n+                    // ∂softmax/∂x_i = softmax_i * (∂L/∂y_i - Σ_j(∂L/∂y_j * softmax_j))\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        // Compute sum of (gradient * softmax)\n+                        var dotProduct = numOps.Zero;\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            dotProduct = numOps.Add(dotProduct,\n+                                numOps.Multiply(gradient[b, f], result[b, f]));\n+                        }\n+\n+                        // Compute gradient for each element\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            var gradMinusDot = numOps.Subtract(gradient[b, f], dotProduct);\n+                            gradA[b, f] = numOps.Multiply(result[b, f], gradMinusDot);\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient,\n+                parents: new List<ComputationNode<T>> { a },\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Softmax is currently only implemented for 2D tensors along axis=-1. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}], axis={axis}\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Concatenates multiple computation nodes along a specified axis.\n+    /// </summary>\n+    /// <param name=\"nodes\">The list of nodes to concatenate.</param>\n+    /// <param name=\"axis\">The axis along which to concatenate. Default is 0.</param>\n+    /// <returns>A new computation node containing the concatenated result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method concatenates tensors along the specified axis.\n+    /// All tensors must have the same shape except along the concatenation axis.\n+    /// The backward function splits the gradient and sends each portion to the corresponding input.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Concat stacks tensors together along a dimension.\n+    ///\n+    /// For concatenation:\n+    /// - The forward pass combines multiple tensors into one larger tensor\n+    /// - The backward pass splits the gradient back to each input\n+    /// - Think of it like gluing arrays together end-to-end\n+    ///\n+    /// Used in:\n+    /// - Skip connections (concatenating features from different layers)\n+    /// - Multi-input architectures\n+    /// - Feature fusion in neural networks\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Concat(List<ComputationNode<T>> nodes, int axis = 0)\n+    {\n+        if (nodes.Count == 0)\n+            throw new ArgumentException(\"Cannot concatenate empty list of nodes\");\n+\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var firstShape = nodes[0].Value.Shape;\n+\n+        // Normalize axis\n+        if (axis < 0)\n+            axis = firstShape.Length + axis;\n+\n+        // Validate shapes match except on concat axis\n+        for (int i = 1; i < nodes.Count; i++)\n+        {\n+            var shape = nodes[i].Value.Shape;\n+            if (shape.Length != firstShape.Length)\n+                throw new ArgumentException(\"All tensors must have the same rank\");\n+\n+            for (int d = 0; d < firstShape.Length; d++)\n+            {\n+                if (d != axis && shape[d] != firstShape[d])\n+                    throw new ArgumentException(\n+                        $\"Shape mismatch at dimension {d}: {shape[d]} vs {firstShape[d]}\");\n+            }\n+        }\n+\n+        // Compute output shape\n+        int[] outputShape = (int[])firstShape.Clone();\n+        for (int i = 1; i < nodes.Count; i++)\n+        {\n+            outputShape[axis] += nodes[i].Value.Shape[axis];\n+        }\n+\n+        // Perform concatenation (handle 2D case for simplicity)\n+        Tensor<T> result;\n+        if (firstShape.Length == 2 && axis == 1)\n+        {\n+            // Concatenate along columns (features)\n+            int rows = firstShape[0];\n+            int totalCols = outputShape[1];\n+            result = new Tensor<T>(new int[] { rows, totalCols });\n+\n+            int colOffset = 0;\n+            foreach (var node in nodes)\n+            {\n+                int cols = node.Value.Shape[1];\n+                for (int r = 0; r < rows; r++)\n+                {\n+                    for (int c = 0; c < cols; c++)\n+                    {\n+                        result[r, colOffset + c] = node.Value[r, c];\n+                    }\n+                }\n+                colOffset += cols;\n+            }\n+        }\n+        else if (firstShape.Length == 2 && axis == 0)\n+        {\n+            // Concatenate along rows (batch)\n+            int cols = firstShape[1];\n+            int totalRows = outputShape[0];\n+            result = new Tensor<T>(new int[] { totalRows, cols });\n+\n+            int rowOffset = 0;\n+            foreach (var node in nodes)\n+            {\n+                int rows = node.Value.Shape[0];\n+                for (int r = 0; r < rows; r++)\n+                {\n+                    for (int c = 0; c < cols; c++)\n+                    {\n+                        result[rowOffset + r, c] = node.Value[r, c];\n+                    }\n+                }\n+                rowOffset += rows;\n+            }\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Concat is currently only implemented for 2D tensors. \" +\n+                $\"Got shape=[{string.Join(\", \", firstShape)}]\");\n+        }\n+\n+        // Store sizes for gradient splitting\n+        var sizes = nodes.Select(n => n.Value.Shape[axis]).ToList();\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            // Split gradient along concat axis and distribute to inputs\n+            if (firstShape.Length == 2 && axis == 1)\n+            {\n+                int rows = firstShape[0];\n+                int colOffset = 0;\n+                for (int i = 0; i < nodes.Count; i++)\n+                {\n+                    if (!nodes[i].RequiresGradient)\n+                    {\n+                        colOffset += sizes[i];\n+                        continue;\n+                    }\n+\n+                    int cols = sizes[i];\n+                    var gradPart = new Tensor<T>(new int[] { rows, cols });\n+\n+                    for (int r = 0; r < rows; r++)\n+                    {\n+                        for (int c = 0; c < cols; c++)\n+                        {\n+                            gradPart[r, c] = gradient[r, colOffset + c];\n+                        }\n+                    }\n+\n+                    if (nodes[i].Gradient == null)\n+                        nodes[i].Gradient = gradPart;\n+                    else\n+                        nodes[i].Gradient = nodes[i].Gradient.Add(gradPart);\n+\n+                    colOffset += cols;\n+                }\n+            }\n+            else if (firstShape.Length == 2 && axis == 0)\n+            {\n+                int cols = firstShape[1];\n+                int rowOffset = 0;\n+                for (int i = 0; i < nodes.Count; i++)\n+                {\n+                    if (!nodes[i].RequiresGradient)\n+                    {\n+                        rowOffset += sizes[i];\n+                        continue;\n+                    }\n+\n+                    int rows = sizes[i];\n+                    var gradPart = new Tensor<T>(new int[] { rows, cols });\n+\n+                    for (int r = 0; r < rows; r++)\n+                    {\n+                        for (int c = 0; c < cols; c++)\n+                        {\n+                            gradPart[r, c] = gradient[rowOffset + r, c];\n+                        }\n+                    }\n+\n+                    if (nodes[i].Gradient == null)\n+                        nodes[i].Gradient = gradPart;\n+                    else\n+                        nodes[i].Gradient = nodes[i].Gradient.Add(gradPart);\n+\n+                    rowOffset += rows;\n+                }\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: nodes.Any(n => n.RequiresGradient),\n+            parents: nodes,\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Pads a tensor with a constant value along specified dimensions.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"padWidth\">Padding width for each dimension as (before, after) pairs.</param>\n+    /// <param name=\"value\">The value to use for padding. Default is zero.</param>\n+    /// <returns>A new computation node containing the padded result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method adds padding around the tensor.\n+    /// The backward function simply crops the gradient back to the original size (gradients for padding are zero).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Pad adds extra elements around a tensor.\n+    ///\n+    /// For padding:\n+    /// - The forward pass adds border elements with a constant value\n+    /// - The backward pass removes those border gradients (they don't affect the original tensor)\n+    /// - Think of it like adding margins to an image\n+    ///\n+    /// Used in:\n+    /// - Convolutional layers (to maintain spatial dimensions)\n+    /// - Handling variable-length sequences\n+    /// - Data augmentation\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Pad(ComputationNode<T> a, int[,] padWidth, T? value = default)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var padValue = value ?? numOps.Zero;\n+        var shape = a.Value.Shape;\n+\n+        // Validate padWidth dimensions\n+        if (padWidth.GetLength(0) != shape.Length)\n+            throw new ArgumentException(\"padWidth must have same number of dimensions as input tensor\");\n+\n+        // Compute output shape\n+        var outputShape = new int[shape.Length];\n+        for (int d = 0; d < shape.Length; d++)\n+        {\n+            outputShape[d] = shape[d] + padWidth[d, 0] + padWidth[d, 1];\n+        }\n+\n+        // Handle 2D case\n+        if (shape.Length == 2)\n+        {\n+            int inputRows = shape[0];\n+            int inputCols = shape[1];\n+            int padTop = padWidth[0, 0];\n+            int padBottom = padWidth[0, 1];\n+            int padLeft = padWidth[1, 0];\n+            int padRight = padWidth[1, 1];\n+\n+            var result = new Tensor<T>(outputShape);\n+\n+            // Initialize with pad value\n+            for (int i = 0; i < result.Length; i++)\n+            {\n+                result[i] = padValue;\n+            }\n+\n+            // Copy input data to center\n+            for (int r = 0; r < inputRows; r++)\n+            {\n+                for (int c = 0; c < inputCols; c++)\n+                {\n+                    result[padTop + r, padLeft + c] = a.Value[r, c];\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (a.RequiresGradient)\n+                {\n+                    // Extract gradient for original (unpadded) region\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int r = 0; r < inputRows; r++)\n+                    {\n+                        for (int c = 0; c < inputCols; c++)\n+                        {\n+                            gradA[r, c] = gradient[padTop + r, padLeft + c];\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient,\n+                parents: new List<ComputationNode<T>> { a },\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Pad is currently only implemented for 2D tensors. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}]\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs 2D max pooling on a 4D tensor (batch, channels, height, width).\n+    /// </summary>\n+    /// <param name=\"a\">The input node with shape [batch, channels, height, width].</param>\n+    /// <param name=\"poolSize\">The size of the pooling window [poolH, poolW].</param>\n+    /// <param name=\"strides\">The stride for the pooling operation [strideH, strideW]. If null, uses poolSize.</param>\n+    /// <returns>A new computation node containing the max pooled result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method performs max pooling over 2D spatial dimensions.\n+    /// During forward pass, it tracks which element was the max for routing gradients during backward pass.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> MaxPool downsamples by taking the maximum value in each window.\n+    ///\n+    /// For max pooling:\n+    /// - The forward pass slides a window and takes the max value in each position\n+    /// - This reduces spatial dimensions (downsampling)\n+    /// - The backward pass routes gradients only to the positions that were max\n+    /// - Other positions get zero gradient (they didn't contribute to the output)\n+    ///\n+    /// Used in:\n+    /// - CNNs for translation invariance\n+    /// - Reducing spatial resolution\n+    /// - Building hierarchical features\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> MaxPool2D(\n+        ComputationNode<T> a,\n+        int[] poolSize,\n+        int[]? strides = null)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        if (shape.Length != 4)\n+            throw new ArgumentException(\"MaxPool2D requires 4D input [batch, channels, height, width]\");\n+\n+        strides ??= poolSize;\n+\n+        int batch = shape[0];\n+        int channels = shape[1];\n+        int inH = shape[2];\n+        int inW = shape[3];\n+        int poolH = poolSize[0];\n+        int poolW = poolSize[1];\n+        int strideH = strides[0];\n+        int strideW = strides[1];\n+\n+        int outH = (inH - poolH) / strideH + 1;\n+        int outW = (inW - poolW) / strideW + 1;\n+\n+        var result = new Tensor<T>(new int[] { batch, channels, outH, outW });\n+        // Store max positions for backprop\n+        var maxPositions = new int[batch, channels, outH, outW, 2]; // [h_offset, w_offset]\n+\n+        // Forward pass: compute max pooling and track positions\n+        for (int b = 0; b < batch; b++)\n+        {\n+            for (int c = 0; c < channels; c++)\n+            {\n+                for (int oh = 0; oh < outH; oh++)\n+                {\n+                    for (int ow = 0; ow < outW; ow++)\n+                    {\n+                        int hStart = oh * strideH;\n+                        int wStart = ow * strideW;\n+\n+                        var maxVal = a.Value[b * channels * inH * inW +\n+                                           c * inH * inW +\n+                                           hStart * inW +\n+                                           wStart];\n+                        int maxHOffset = 0;\n+                        int maxWOffset = 0;\n+\n+                        // Find max in pooling window\n+                        for (int ph = 0; ph < poolH; ph++)\n+                        {\n+                            for (int pw = 0; pw < poolW; pw++)\n+                            {\n+                                int h = hStart + ph;\n+                                int w = wStart + pw;\n+                                if (h < inH && w < inW)\n+                                {\n+                                    var val = a.Value[b * channels * inH * inW +\n+                                                     c * inH * inW +\n+                                                     h * inW +\n+                                                     w];\n+                                    if (numOps.GreaterThan(val, maxVal))\n+                                    {\n+                                        maxVal = val;\n+                                        maxHOffset = ph;\n+                                        maxWOffset = pw;\n+                                    }\n+                                }\n+                            }\n+                        }\n+\n+                        result[b, c, oh, ow] = maxVal;\n+                        maxPositions[b, c, oh, ow, 0] = maxHOffset;\n+                        maxPositions[b, c, oh, ow, 1] = maxWOffset;\n+                    }\n+                }\n+            }\n+        }\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            if (a.RequiresGradient)\n+            {\n+                var gradA = new Tensor<T>(shape);\n+\n+                // Route gradients to max positions\n+                for (int b = 0; b < batch; b++)\n+                {\n+                    for (int c = 0; c < channels; c++)\n+                    {\n+                        for (int oh = 0; oh < outH; oh++)\n+                        {\n+                            for (int ow = 0; ow < outW; ow++)\n+                            {\n+                                int hStart = oh * strideH;\n+                                int wStart = ow * strideW;\n+                                int maxHOffset = maxPositions[b, c, oh, ow, 0];\n+                                int maxWOffset = maxPositions[b, c, oh, ow, 1];\n+\n+                                int maxH = hStart + maxHOffset;\n+                                int maxW = wStart + maxWOffset;\n+\n+                                int gradIdx = b * channels * inH * inW +\n+                                             c * inH * inW +\n+                                            maxH * inW +\n+                                             maxW;\n+\n+                                gradA[gradIdx] = numOps.Add(gradA[gradIdx], gradient[b, c, oh, ow]);\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (a.Gradient == null)\n+                    a.Gradient = gradA;\n+                else\n+                    a.Gradient = a.Gradient.Add(gradA);\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: a.RequiresGradient,\n+            parents: new List<ComputationNode<T>> { a },\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Performs 2D average pooling on a 4D tensor (batch, channels, height, width).\n+    /// </summary>\n+    /// <param name=\"a\">The input node with shape [batch, channels, height, width].</param>\n+    /// <param name=\"poolSize\">The size of the pooling window [poolH, poolW].</param>\n+    /// <param name=\"strides\">The stride for the pooling operation [strideH, strideW]. If null, uses poolSize.</param>\n+    /// <returns>A new computation node containing the average pooled result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method performs average pooling over 2D spatial dimensions.\n+    /// The backward function distributes gradients equally across the pooling window.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> AvgPool downsamples by taking the average value in each window.\n+    ///\n+    /// For average pooling:\n+    /// - The forward pass slides a window and computes the average\n+    /// - This smoothly reduces spatial dimensions\n+    /// - The backward pass distributes gradients equally to all elements in the window\n+    /// - Each element gets gradient / pool_area\n+    ///\n+    /// Used in:\n+    /// - CNNs for smoother downsampling than max pooling\n+    /// - Global average pooling (replacing fully connected layers)\n+    /// - Reducing overfitting\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> AvgPool2D(\n+        ComputationNode<T> a,\n+        int[] poolSize,\n+        int[]? strides = null)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        if (shape.Length != 4)\n+            throw new ArgumentException(\"AvgPool2D requires 4D input [batch, channels, height, width]\");\n+\n+        strides ??= poolSize;\n+\n+        int batch = shape[0];\n+        int channels = shape[1];\n+        int inH = shape[2];\n+        int inW = shape[3];\n+        int poolH = poolSize[0];\n+        int poolW = poolSize[1];\n+        int strideH = strides[0];\n+        int strideW = strides[1];\n+\n+        int outH = (inH - poolH) / strideH + 1;\n+        int outW = (inW - poolW) / strideW + 1;\n+\n+        var result = new Tensor<T>(new int[] { batch, channels, outH, outW });\n+        var poolArea = numOps.FromDouble(poolH * poolW);\n+\n+        // Forward pass: compute average pooling\n+        for (int b = 0; b < batch; b++)\n+        {\n+            for (int c = 0; c < channels; c++)\n+            {\n+                for (int oh = 0; oh < outH; oh++)\n+                {\n+                    for (int ow = 0; ow < outW; ow++)\n+                    {\n+                        int hStart = oh * strideH;\n+                        int wStart = ow * strideW;\n+\n+                        var sum = numOps.Zero;\n+\n+                        // Sum values in pooling window\n+                        for (int ph = 0; ph < poolH; ph++)\n+                        {\n+                            for (int pw = 0; pw < poolW; pw++)\n+                            {\n+                                int h = hStart + ph;\n+                                int w = wStart + pw;\n+                                if (h < inH && w < inW)\n+                                {\n+                                    sum = numOps.Add(sum, a.Value[b, c, h, w]);\n+                                }\n+                            }\n+                        }\n+\n+                        result[b, c, oh, ow] = numOps.Divide(sum, poolArea);\n+                    }\n+                }\n+            }\n+        }\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            if (a.RequiresGradient)\n+            {\n+                var gradA = new Tensor<T>(shape);\n+\n+                // Distribute gradients equally across pooling windows\n+                for (int b = 0; b < batch; b++)\n+                {\n+                    for (int c = 0; c < channels; c++)\n+                    {\n+                        for (int oh = 0; oh < outH; oh++)\n+                        {\n+                            for (int ow = 0; ow < outW; ow++)\n+                            {\n+                                int hStart = oh * strideH;\n+                                int wStart = ow * strideW;\n+\n+                                var gradValue = numOps.Divide(gradient[b, c, oh, ow], poolArea);\n+\n+                                // Distribute to all elements in window\n+                                for (int ph = 0; ph < poolH; ph++)\n+                                {\n+                                    for (int pw = 0; pw < poolW; pw++)\n+                                    {\n+                                        int h = hStart + ph;\n+                                        int w = wStart + pw;\n+                                        if (h < inH && w < inW)\n+                                        {\n+                                            gradA[b, c, h, w] = numOps.Add(gradA[b, c, h, w], gradValue);\n+                                        }\n+                                    }\n+                                }\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (a.Gradient == null)\n+                    a.Gradient = gradA;\n+                else\n+                    a.Gradient = a.Gradient.Add(gradA);\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: a.RequiresGradient,\n+            parents: new List<ComputationNode<T>> { a },\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Applies layer normalization to a computation node.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"normalizedShape\">The shape over which to normalize (typically the feature dimensions).</param>\n+    /// <param name=\"gamma\">Optional scale parameter (learnable). If null, uses ones.</param>\n+    /// <param name=\"beta\">Optional shift parameter (learnable). If null, uses zeros.</param>\n+    /// <param name=\"epsilon\">Small constant for numerical stability. Default is 1e-5.</param>\n+    /// <returns>A new computation node containing the layer normalized result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Layer normalization normalizes inputs across the feature dimension for each sample independently.\n+    /// Formula: y = gamma * (x - mean) / sqrt(variance + epsilon) + beta\n+    /// Unlike batch normalization, this doesn't depend on batch statistics.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> LayerNorm standardizes features for each sample independently.\n+    ///\n+    /// For layer normalization:\n+    /// - Computes mean and variance for each sample's features\n+    /// - Normalizes: (x - mean) / sqrt(variance)\n+    /// - Scales and shifts: result * gamma + beta\n+    /// - Works the same during training and inference (no batch dependency)\n+    ///\n+    /// Used in:\n+    /// - Transformers (critical component)\n+    /// - RNNs (stabilizes training)\n+    /// - Any architecture needing sample-independent normalization\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> LayerNorm(\n+        ComputationNode<T> a,\n+        int[] normalizedShape,\n+        ComputationNode<T>? gamma = null,\n+        ComputationNode<T>? beta = null,\n+        double epsilon = 1e-5)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+        var eps = numOps.FromDouble(epsilon);\n+\n+        // For 2D input [batch, features], normalize over features\n+        if (shape.Length == 2 && normalizedShape.Length == 1 && normalizedShape[0] == shape[1])\n+        {\n+            int batchSize = shape[0];\n+            int features = shape[1];\n+\n+            // Create default gamma (ones) and beta (zeros) if not provided\n+            if (gamma == null)\n+            {\n+                var gammaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    gammaTensor[i] = numOps.One;\n+                gamma = Variable(gammaTensor, requiresGradient: false);\n+            }\n+\n+            if (beta == null)\n+            {\n+                var betaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    betaTensor[i] = numOps.Zero;\n+                beta = Variable(betaTensor, requiresGradient: false);\n+            }\n+\n+            var result = new Tensor<T>(shape);\n+            var means = new T[batchSize];\n+            var variances = new T[batchSize];\n+            var normalized = new Tensor<T>(shape);\n+\n+            // Forward pass: compute mean and variance per sample\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                // Compute mean\n+                var sum = numOps.Zero;\n+                for (int f = 0; f < features; f++)\n+                {\n+                    sum = numOps.Add(sum, a.Value[b, f]);\n+                }\n+                means[b] = numOps.Divide(sum, numOps.FromDouble(features));\n+\n+                // Compute variance\n+                var varSum = numOps.Zero;\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var diff = numOps.Subtract(a.Value[b, f], means[b]);\n+                    varSum = numOps.Add(varSum, numOps.Multiply(diff, diff));\n+                }\n+                variances[b] = numOps.Divide(varSum, numOps.FromDouble(features));\n+\n+                // Normalize and scale\n+                var std = numOps.Sqrt(numOps.Add(variances[b], eps));\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var norm = numOps.Divide(\n+                        numOps.Subtract(a.Value[b, f], means[b]),\n+                        std);\n+                    normalized[b, f] = norm;\n+                    result[b, f] = numOps.Add(\n+                        numOps.Multiply(norm, gamma.Value[f]),\n+                        beta.Value[f]);\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                // Gradients for gamma and beta\n+                if (gamma.RequiresGradient)\n+                {\n+                    var gradGamma = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum,\n+                                numOps.Multiply(gradient[b, f], normalized[b, f]));\n+                        }\n+                        gradGamma[f] = sum;\n+                    }\n+\n+                    if (gamma.Gradient == null)\n+                        gamma.Gradient = gradGamma;\n+                    else\n+                        gamma.Gradient = gamma.Gradient.Add(gradGamma);\n+                }\n+\n+                if (beta.RequiresGradient)\n+                {\n+                    var gradBeta = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum, gradient[b, f]);\n+                        }\n+                        gradBeta[f] = sum;\n+                    }\n+\n+                    if (beta.Gradient == null)\n+                        beta.Gradient = gradBeta;\n+                    else\n+                        beta.Gradient = beta.Gradient.Add(gradBeta);\n+                }\n+\n+                // Gradient for input\n+                if (a.RequiresGradient)\n+                {\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        var std = numOps.Sqrt(numOps.Add(variances[b], eps));\n+                        var invStd = numOps.Divide(numOps.One, std);\n+\n+                        // Compute gradient components\n+                        var gradNormSum = numOps.Zero;\n+                        var gradNormDotNorm = numOps.Zero;\n+\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            var gradNorm = numOps.Multiply(gradient[b, f], gamma.Value[f]);\n+                            gradNormSum = numOps.Add(gradNormSum, gradNorm);\n+                            gradNormDotNorm = numOps.Add(gradNormDotNorm,\n+                                numOps.Multiply(gradNorm, normalized[b, f]));\n+                        }\n+\n+                        // Apply gradient formula\n+                        var featuresT = numOps.FromDouble(features);\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            var gradNorm = numOps.Multiply(gradient[b, f], gamma.Value[f]);\n+\n+                            var term1 = gradNorm;\n+                            var term2 = numOps.Divide(gradNormSum, featuresT);\n+                            var term3 = numOps.Divide(\n+                                numOps.Multiply(normalized[b, f], gradNormDotNorm),\n+                                featuresT);\n+\n+                            var grad = numOps.Multiply(\n+                                numOps.Subtract(numOps.Subtract(term1, term2), term3),\n+                                invStd);\n+\n+                            gradA[b, f] = grad;\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);"}]}},{"id":"PRRT_kwDOKSXUF85hck-a","isResolved":false,"isOutdated":false,"path":"src/Autodiff/TensorOperations.cs","line":2336,"originalLine":2336,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzD-t","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement write to the same variable - consider using '?' to express intent better.\n```suggestion\n                        a.Gradient = a.Gradient == null ? gradA : a.Gradient.Add(gradA);\n```","createdAt":"2025-11-11T07:53:25Z","path":"src/Autodiff/TensorOperations.cs","line":2336,"originalLine":2336,"diffHunk":"@@ -1219,4 +1219,1769 @@ void BackwardFunction(Tensor<T> gradient)\n \n         return node;\n     }\n+\n+    /// <summary>\n+    /// Computes the softmax function for a computation node along a specified axis.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"axis\">The axis along which to compute softmax. Default is -1 (last axis).</param>\n+    /// <returns>A new computation node containing the softmax result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method computes softmax (σ(x_i) = exp(x_i) / Σexp(x_j)) along the specified axis.\n+    /// Uses numerical stability trick: subtract max before exponentiating.\n+    /// The backward function uses: ∂softmax/∂x = softmax(x) * (grad - Σ(grad * softmax(x))).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Softmax converts a vector of numbers into probabilities.\n+    ///\n+    /// For softmax:\n+    /// - The forward pass exponentiates each element, then normalizes so they sum to 1\n+    /// - The result is a probability distribution (all values between 0 and 1, summing to 1)\n+    /// - The backward pass is complex but efficient: uses the Jacobian of softmax\n+    ///\n+    /// Softmax is crucial for:\n+    /// - Multi-class classification (final layer outputs)\n+    /// - Attention mechanisms (computing attention weights)\n+    /// - Anywhere you need to convert scores to probabilities\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Softmax(ComputationNode<T> a, int axis = -1)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        // Normalize axis to positive index\n+        if (axis < 0)\n+            axis = shape.Length + axis;\n+\n+        // For simplicity, handle 2D case (batch, features) with axis=-1\n+        if (shape.Length == 2 && axis == 1)\n+        {\n+            int batchSize = shape[0];\n+            int features = shape[1];\n+            var result = new Tensor<T>(shape);\n+\n+            // Compute softmax for each row\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                // Find max for numerical stability\n+                var maxVal = a.Value[b, 0];\n+                for (int f = 1; f < features; f++)\n+                {\n+                    if (numOps.GreaterThan(a.Value[b, f], maxVal))\n+                        maxVal = a.Value[b, f];\n+                }\n+\n+                // Compute exp(x - max) and sum\n+                var expSum = numOps.Zero;\n+                var expValues = new T[features];\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var shifted = numOps.Subtract(a.Value[b, f], maxVal);\n+                    expValues[f] = numOps.Exp(shifted);\n+                    expSum = numOps.Add(expSum, expValues[f]);\n+                }\n+\n+                // Normalize\n+                for (int f = 0; f < features; f++)\n+                {\n+                    result[b, f] = numOps.Divide(expValues[f], expSum);\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (a.RequiresGradient)\n+                {\n+                    // ∂softmax/∂x_i = softmax_i * (∂L/∂y_i - Σ_j(∂L/∂y_j * softmax_j))\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        // Compute sum of (gradient * softmax)\n+                        var dotProduct = numOps.Zero;\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            dotProduct = numOps.Add(dotProduct,\n+                                numOps.Multiply(gradient[b, f], result[b, f]));\n+                        }\n+\n+                        // Compute gradient for each element\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            var gradMinusDot = numOps.Subtract(gradient[b, f], dotProduct);\n+                            gradA[b, f] = numOps.Multiply(result[b, f], gradMinusDot);\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient,\n+                parents: new List<ComputationNode<T>> { a },\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Softmax is currently only implemented for 2D tensors along axis=-1. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}], axis={axis}\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Concatenates multiple computation nodes along a specified axis.\n+    /// </summary>\n+    /// <param name=\"nodes\">The list of nodes to concatenate.</param>\n+    /// <param name=\"axis\">The axis along which to concatenate. Default is 0.</param>\n+    /// <returns>A new computation node containing the concatenated result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method concatenates tensors along the specified axis.\n+    /// All tensors must have the same shape except along the concatenation axis.\n+    /// The backward function splits the gradient and sends each portion to the corresponding input.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Concat stacks tensors together along a dimension.\n+    ///\n+    /// For concatenation:\n+    /// - The forward pass combines multiple tensors into one larger tensor\n+    /// - The backward pass splits the gradient back to each input\n+    /// - Think of it like gluing arrays together end-to-end\n+    ///\n+    /// Used in:\n+    /// - Skip connections (concatenating features from different layers)\n+    /// - Multi-input architectures\n+    /// - Feature fusion in neural networks\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Concat(List<ComputationNode<T>> nodes, int axis = 0)\n+    {\n+        if (nodes.Count == 0)\n+            throw new ArgumentException(\"Cannot concatenate empty list of nodes\");\n+\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var firstShape = nodes[0].Value.Shape;\n+\n+        // Normalize axis\n+        if (axis < 0)\n+            axis = firstShape.Length + axis;\n+\n+        // Validate shapes match except on concat axis\n+        for (int i = 1; i < nodes.Count; i++)\n+        {\n+            var shape = nodes[i].Value.Shape;\n+            if (shape.Length != firstShape.Length)\n+                throw new ArgumentException(\"All tensors must have the same rank\");\n+\n+            for (int d = 0; d < firstShape.Length; d++)\n+            {\n+                if (d != axis && shape[d] != firstShape[d])\n+                    throw new ArgumentException(\n+                        $\"Shape mismatch at dimension {d}: {shape[d]} vs {firstShape[d]}\");\n+            }\n+        }\n+\n+        // Compute output shape\n+        int[] outputShape = (int[])firstShape.Clone();\n+        for (int i = 1; i < nodes.Count; i++)\n+        {\n+            outputShape[axis] += nodes[i].Value.Shape[axis];\n+        }\n+\n+        // Perform concatenation (handle 2D case for simplicity)\n+        Tensor<T> result;\n+        if (firstShape.Length == 2 && axis == 1)\n+        {\n+            // Concatenate along columns (features)\n+            int rows = firstShape[0];\n+            int totalCols = outputShape[1];\n+            result = new Tensor<T>(new int[] { rows, totalCols });\n+\n+            int colOffset = 0;\n+            foreach (var node in nodes)\n+            {\n+                int cols = node.Value.Shape[1];\n+                for (int r = 0; r < rows; r++)\n+                {\n+                    for (int c = 0; c < cols; c++)\n+                    {\n+                        result[r, colOffset + c] = node.Value[r, c];\n+                    }\n+                }\n+                colOffset += cols;\n+            }\n+        }\n+        else if (firstShape.Length == 2 && axis == 0)\n+        {\n+            // Concatenate along rows (batch)\n+            int cols = firstShape[1];\n+            int totalRows = outputShape[0];\n+            result = new Tensor<T>(new int[] { totalRows, cols });\n+\n+            int rowOffset = 0;\n+            foreach (var node in nodes)\n+            {\n+                int rows = node.Value.Shape[0];\n+                for (int r = 0; r < rows; r++)\n+                {\n+                    for (int c = 0; c < cols; c++)\n+                    {\n+                        result[rowOffset + r, c] = node.Value[r, c];\n+                    }\n+                }\n+                rowOffset += rows;\n+            }\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Concat is currently only implemented for 2D tensors. \" +\n+                $\"Got shape=[{string.Join(\", \", firstShape)}]\");\n+        }\n+\n+        // Store sizes for gradient splitting\n+        var sizes = nodes.Select(n => n.Value.Shape[axis]).ToList();\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            // Split gradient along concat axis and distribute to inputs\n+            if (firstShape.Length == 2 && axis == 1)\n+            {\n+                int rows = firstShape[0];\n+                int colOffset = 0;\n+                for (int i = 0; i < nodes.Count; i++)\n+                {\n+                    if (!nodes[i].RequiresGradient)\n+                    {\n+                        colOffset += sizes[i];\n+                        continue;\n+                    }\n+\n+                    int cols = sizes[i];\n+                    var gradPart = new Tensor<T>(new int[] { rows, cols });\n+\n+                    for (int r = 0; r < rows; r++)\n+                    {\n+                        for (int c = 0; c < cols; c++)\n+                        {\n+                            gradPart[r, c] = gradient[r, colOffset + c];\n+                        }\n+                    }\n+\n+                    if (nodes[i].Gradient == null)\n+                        nodes[i].Gradient = gradPart;\n+                    else\n+                        nodes[i].Gradient = nodes[i].Gradient.Add(gradPart);\n+\n+                    colOffset += cols;\n+                }\n+            }\n+            else if (firstShape.Length == 2 && axis == 0)\n+            {\n+                int cols = firstShape[1];\n+                int rowOffset = 0;\n+                for (int i = 0; i < nodes.Count; i++)\n+                {\n+                    if (!nodes[i].RequiresGradient)\n+                    {\n+                        rowOffset += sizes[i];\n+                        continue;\n+                    }\n+\n+                    int rows = sizes[i];\n+                    var gradPart = new Tensor<T>(new int[] { rows, cols });\n+\n+                    for (int r = 0; r < rows; r++)\n+                    {\n+                        for (int c = 0; c < cols; c++)\n+                        {\n+                            gradPart[r, c] = gradient[rowOffset + r, c];\n+                        }\n+                    }\n+\n+                    if (nodes[i].Gradient == null)\n+                        nodes[i].Gradient = gradPart;\n+                    else\n+                        nodes[i].Gradient = nodes[i].Gradient.Add(gradPart);\n+\n+                    rowOffset += rows;\n+                }\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: nodes.Any(n => n.RequiresGradient),\n+            parents: nodes,\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Pads a tensor with a constant value along specified dimensions.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"padWidth\">Padding width for each dimension as (before, after) pairs.</param>\n+    /// <param name=\"value\">The value to use for padding. Default is zero.</param>\n+    /// <returns>A new computation node containing the padded result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method adds padding around the tensor.\n+    /// The backward function simply crops the gradient back to the original size (gradients for padding are zero).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Pad adds extra elements around a tensor.\n+    ///\n+    /// For padding:\n+    /// - The forward pass adds border elements with a constant value\n+    /// - The backward pass removes those border gradients (they don't affect the original tensor)\n+    /// - Think of it like adding margins to an image\n+    ///\n+    /// Used in:\n+    /// - Convolutional layers (to maintain spatial dimensions)\n+    /// - Handling variable-length sequences\n+    /// - Data augmentation\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Pad(ComputationNode<T> a, int[,] padWidth, T? value = default)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var padValue = value ?? numOps.Zero;\n+        var shape = a.Value.Shape;\n+\n+        // Validate padWidth dimensions\n+        if (padWidth.GetLength(0) != shape.Length)\n+            throw new ArgumentException(\"padWidth must have same number of dimensions as input tensor\");\n+\n+        // Compute output shape\n+        var outputShape = new int[shape.Length];\n+        for (int d = 0; d < shape.Length; d++)\n+        {\n+            outputShape[d] = shape[d] + padWidth[d, 0] + padWidth[d, 1];\n+        }\n+\n+        // Handle 2D case\n+        if (shape.Length == 2)\n+        {\n+            int inputRows = shape[0];\n+            int inputCols = shape[1];\n+            int padTop = padWidth[0, 0];\n+            int padBottom = padWidth[0, 1];\n+            int padLeft = padWidth[1, 0];\n+            int padRight = padWidth[1, 1];\n+\n+            var result = new Tensor<T>(outputShape);\n+\n+            // Initialize with pad value\n+            for (int i = 0; i < result.Length; i++)\n+            {\n+                result[i] = padValue;\n+            }\n+\n+            // Copy input data to center\n+            for (int r = 0; r < inputRows; r++)\n+            {\n+                for (int c = 0; c < inputCols; c++)\n+                {\n+                    result[padTop + r, padLeft + c] = a.Value[r, c];\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (a.RequiresGradient)\n+                {\n+                    // Extract gradient for original (unpadded) region\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int r = 0; r < inputRows; r++)\n+                    {\n+                        for (int c = 0; c < inputCols; c++)\n+                        {\n+                            gradA[r, c] = gradient[padTop + r, padLeft + c];\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient,\n+                parents: new List<ComputationNode<T>> { a },\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Pad is currently only implemented for 2D tensors. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}]\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs 2D max pooling on a 4D tensor (batch, channels, height, width).\n+    /// </summary>\n+    /// <param name=\"a\">The input node with shape [batch, channels, height, width].</param>\n+    /// <param name=\"poolSize\">The size of the pooling window [poolH, poolW].</param>\n+    /// <param name=\"strides\">The stride for the pooling operation [strideH, strideW]. If null, uses poolSize.</param>\n+    /// <returns>A new computation node containing the max pooled result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method performs max pooling over 2D spatial dimensions.\n+    /// During forward pass, it tracks which element was the max for routing gradients during backward pass.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> MaxPool downsamples by taking the maximum value in each window.\n+    ///\n+    /// For max pooling:\n+    /// - The forward pass slides a window and takes the max value in each position\n+    /// - This reduces spatial dimensions (downsampling)\n+    /// - The backward pass routes gradients only to the positions that were max\n+    /// - Other positions get zero gradient (they didn't contribute to the output)\n+    ///\n+    /// Used in:\n+    /// - CNNs for translation invariance\n+    /// - Reducing spatial resolution\n+    /// - Building hierarchical features\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> MaxPool2D(\n+        ComputationNode<T> a,\n+        int[] poolSize,\n+        int[]? strides = null)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        if (shape.Length != 4)\n+            throw new ArgumentException(\"MaxPool2D requires 4D input [batch, channels, height, width]\");\n+\n+        strides ??= poolSize;\n+\n+        int batch = shape[0];\n+        int channels = shape[1];\n+        int inH = shape[2];\n+        int inW = shape[3];\n+        int poolH = poolSize[0];\n+        int poolW = poolSize[1];\n+        int strideH = strides[0];\n+        int strideW = strides[1];\n+\n+        int outH = (inH - poolH) / strideH + 1;\n+        int outW = (inW - poolW) / strideW + 1;\n+\n+        var result = new Tensor<T>(new int[] { batch, channels, outH, outW });\n+        // Store max positions for backprop\n+        var maxPositions = new int[batch, channels, outH, outW, 2]; // [h_offset, w_offset]\n+\n+        // Forward pass: compute max pooling and track positions\n+        for (int b = 0; b < batch; b++)\n+        {\n+            for (int c = 0; c < channels; c++)\n+            {\n+                for (int oh = 0; oh < outH; oh++)\n+                {\n+                    for (int ow = 0; ow < outW; ow++)\n+                    {\n+                        int hStart = oh * strideH;\n+                        int wStart = ow * strideW;\n+\n+                        var maxVal = a.Value[b * channels * inH * inW +\n+                                           c * inH * inW +\n+                                           hStart * inW +\n+                                           wStart];\n+                        int maxHOffset = 0;\n+                        int maxWOffset = 0;\n+\n+                        // Find max in pooling window\n+                        for (int ph = 0; ph < poolH; ph++)\n+                        {\n+                            for (int pw = 0; pw < poolW; pw++)\n+                            {\n+                                int h = hStart + ph;\n+                                int w = wStart + pw;\n+                                if (h < inH && w < inW)\n+                                {\n+                                    var val = a.Value[b * channels * inH * inW +\n+                                                     c * inH * inW +\n+                                                     h * inW +\n+                                                     w];\n+                                    if (numOps.GreaterThan(val, maxVal))\n+                                    {\n+                                        maxVal = val;\n+                                        maxHOffset = ph;\n+                                        maxWOffset = pw;\n+                                    }\n+                                }\n+                            }\n+                        }\n+\n+                        result[b, c, oh, ow] = maxVal;\n+                        maxPositions[b, c, oh, ow, 0] = maxHOffset;\n+                        maxPositions[b, c, oh, ow, 1] = maxWOffset;\n+                    }\n+                }\n+            }\n+        }\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            if (a.RequiresGradient)\n+            {\n+                var gradA = new Tensor<T>(shape);\n+\n+                // Route gradients to max positions\n+                for (int b = 0; b < batch; b++)\n+                {\n+                    for (int c = 0; c < channels; c++)\n+                    {\n+                        for (int oh = 0; oh < outH; oh++)\n+                        {\n+                            for (int ow = 0; ow < outW; ow++)\n+                            {\n+                                int hStart = oh * strideH;\n+                                int wStart = ow * strideW;\n+                                int maxHOffset = maxPositions[b, c, oh, ow, 0];\n+                                int maxWOffset = maxPositions[b, c, oh, ow, 1];\n+\n+                                int maxH = hStart + maxHOffset;\n+                                int maxW = wStart + maxWOffset;\n+\n+                                int gradIdx = b * channels * inH * inW +\n+                                             c * inH * inW +\n+                                            maxH * inW +\n+                                             maxW;\n+\n+                                gradA[gradIdx] = numOps.Add(gradA[gradIdx], gradient[b, c, oh, ow]);\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (a.Gradient == null)\n+                    a.Gradient = gradA;\n+                else\n+                    a.Gradient = a.Gradient.Add(gradA);\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: a.RequiresGradient,\n+            parents: new List<ComputationNode<T>> { a },\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Performs 2D average pooling on a 4D tensor (batch, channels, height, width).\n+    /// </summary>\n+    /// <param name=\"a\">The input node with shape [batch, channels, height, width].</param>\n+    /// <param name=\"poolSize\">The size of the pooling window [poolH, poolW].</param>\n+    /// <param name=\"strides\">The stride for the pooling operation [strideH, strideW]. If null, uses poolSize.</param>\n+    /// <returns>A new computation node containing the average pooled result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method performs average pooling over 2D spatial dimensions.\n+    /// The backward function distributes gradients equally across the pooling window.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> AvgPool downsamples by taking the average value in each window.\n+    ///\n+    /// For average pooling:\n+    /// - The forward pass slides a window and computes the average\n+    /// - This smoothly reduces spatial dimensions\n+    /// - The backward pass distributes gradients equally to all elements in the window\n+    /// - Each element gets gradient / pool_area\n+    ///\n+    /// Used in:\n+    /// - CNNs for smoother downsampling than max pooling\n+    /// - Global average pooling (replacing fully connected layers)\n+    /// - Reducing overfitting\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> AvgPool2D(\n+        ComputationNode<T> a,\n+        int[] poolSize,\n+        int[]? strides = null)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        if (shape.Length != 4)\n+            throw new ArgumentException(\"AvgPool2D requires 4D input [batch, channels, height, width]\");\n+\n+        strides ??= poolSize;\n+\n+        int batch = shape[0];\n+        int channels = shape[1];\n+        int inH = shape[2];\n+        int inW = shape[3];\n+        int poolH = poolSize[0];\n+        int poolW = poolSize[1];\n+        int strideH = strides[0];\n+        int strideW = strides[1];\n+\n+        int outH = (inH - poolH) / strideH + 1;\n+        int outW = (inW - poolW) / strideW + 1;\n+\n+        var result = new Tensor<T>(new int[] { batch, channels, outH, outW });\n+        var poolArea = numOps.FromDouble(poolH * poolW);\n+\n+        // Forward pass: compute average pooling\n+        for (int b = 0; b < batch; b++)\n+        {\n+            for (int c = 0; c < channels; c++)\n+            {\n+                for (int oh = 0; oh < outH; oh++)\n+                {\n+                    for (int ow = 0; ow < outW; ow++)\n+                    {\n+                        int hStart = oh * strideH;\n+                        int wStart = ow * strideW;\n+\n+                        var sum = numOps.Zero;\n+\n+                        // Sum values in pooling window\n+                        for (int ph = 0; ph < poolH; ph++)\n+                        {\n+                            for (int pw = 0; pw < poolW; pw++)\n+                            {\n+                                int h = hStart + ph;\n+                                int w = wStart + pw;\n+                                if (h < inH && w < inW)\n+                                {\n+                                    sum = numOps.Add(sum, a.Value[b, c, h, w]);\n+                                }\n+                            }\n+                        }\n+\n+                        result[b, c, oh, ow] = numOps.Divide(sum, poolArea);\n+                    }\n+                }\n+            }\n+        }\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            if (a.RequiresGradient)\n+            {\n+                var gradA = new Tensor<T>(shape);\n+\n+                // Distribute gradients equally across pooling windows\n+                for (int b = 0; b < batch; b++)\n+                {\n+                    for (int c = 0; c < channels; c++)\n+                    {\n+                        for (int oh = 0; oh < outH; oh++)\n+                        {\n+                            for (int ow = 0; ow < outW; ow++)\n+                            {\n+                                int hStart = oh * strideH;\n+                                int wStart = ow * strideW;\n+\n+                                var gradValue = numOps.Divide(gradient[b, c, oh, ow], poolArea);\n+\n+                                // Distribute to all elements in window\n+                                for (int ph = 0; ph < poolH; ph++)\n+                                {\n+                                    for (int pw = 0; pw < poolW; pw++)\n+                                    {\n+                                        int h = hStart + ph;\n+                                        int w = wStart + pw;\n+                                        if (h < inH && w < inW)\n+                                        {\n+                                            gradA[b, c, h, w] = numOps.Add(gradA[b, c, h, w], gradValue);\n+                                        }\n+                                    }\n+                                }\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (a.Gradient == null)\n+                    a.Gradient = gradA;\n+                else\n+                    a.Gradient = a.Gradient.Add(gradA);\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: a.RequiresGradient,\n+            parents: new List<ComputationNode<T>> { a },\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Applies layer normalization to a computation node.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"normalizedShape\">The shape over which to normalize (typically the feature dimensions).</param>\n+    /// <param name=\"gamma\">Optional scale parameter (learnable). If null, uses ones.</param>\n+    /// <param name=\"beta\">Optional shift parameter (learnable). If null, uses zeros.</param>\n+    /// <param name=\"epsilon\">Small constant for numerical stability. Default is 1e-5.</param>\n+    /// <returns>A new computation node containing the layer normalized result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Layer normalization normalizes inputs across the feature dimension for each sample independently.\n+    /// Formula: y = gamma * (x - mean) / sqrt(variance + epsilon) + beta\n+    /// Unlike batch normalization, this doesn't depend on batch statistics.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> LayerNorm standardizes features for each sample independently.\n+    ///\n+    /// For layer normalization:\n+    /// - Computes mean and variance for each sample's features\n+    /// - Normalizes: (x - mean) / sqrt(variance)\n+    /// - Scales and shifts: result * gamma + beta\n+    /// - Works the same during training and inference (no batch dependency)\n+    ///\n+    /// Used in:\n+    /// - Transformers (critical component)\n+    /// - RNNs (stabilizes training)\n+    /// - Any architecture needing sample-independent normalization\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> LayerNorm(\n+        ComputationNode<T> a,\n+        int[] normalizedShape,\n+        ComputationNode<T>? gamma = null,\n+        ComputationNode<T>? beta = null,\n+        double epsilon = 1e-5)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+        var eps = numOps.FromDouble(epsilon);\n+\n+        // For 2D input [batch, features], normalize over features\n+        if (shape.Length == 2 && normalizedShape.Length == 1 && normalizedShape[0] == shape[1])\n+        {\n+            int batchSize = shape[0];\n+            int features = shape[1];\n+\n+            // Create default gamma (ones) and beta (zeros) if not provided\n+            if (gamma == null)\n+            {\n+                var gammaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    gammaTensor[i] = numOps.One;\n+                gamma = Variable(gammaTensor, requiresGradient: false);\n+            }\n+\n+            if (beta == null)\n+            {\n+                var betaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    betaTensor[i] = numOps.Zero;\n+                beta = Variable(betaTensor, requiresGradient: false);\n+            }\n+\n+            var result = new Tensor<T>(shape);\n+            var means = new T[batchSize];\n+            var variances = new T[batchSize];\n+            var normalized = new Tensor<T>(shape);\n+\n+            // Forward pass: compute mean and variance per sample\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                // Compute mean\n+                var sum = numOps.Zero;\n+                for (int f = 0; f < features; f++)\n+                {\n+                    sum = numOps.Add(sum, a.Value[b, f]);\n+                }\n+                means[b] = numOps.Divide(sum, numOps.FromDouble(features));\n+\n+                // Compute variance\n+                var varSum = numOps.Zero;\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var diff = numOps.Subtract(a.Value[b, f], means[b]);\n+                    varSum = numOps.Add(varSum, numOps.Multiply(diff, diff));\n+                }\n+                variances[b] = numOps.Divide(varSum, numOps.FromDouble(features));\n+\n+                // Normalize and scale\n+                var std = numOps.Sqrt(numOps.Add(variances[b], eps));\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var norm = numOps.Divide(\n+                        numOps.Subtract(a.Value[b, f], means[b]),\n+                        std);\n+                    normalized[b, f] = norm;\n+                    result[b, f] = numOps.Add(\n+                        numOps.Multiply(norm, gamma.Value[f]),\n+                        beta.Value[f]);\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                // Gradients for gamma and beta\n+                if (gamma.RequiresGradient)\n+                {\n+                    var gradGamma = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum,\n+                                numOps.Multiply(gradient[b, f], normalized[b, f]));\n+                        }\n+                        gradGamma[f] = sum;\n+                    }\n+\n+                    if (gamma.Gradient == null)\n+                        gamma.Gradient = gradGamma;\n+                    else\n+                        gamma.Gradient = gamma.Gradient.Add(gradGamma);\n+                }\n+\n+                if (beta.RequiresGradient)\n+                {\n+                    var gradBeta = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum, gradient[b, f]);\n+                        }\n+                        gradBeta[f] = sum;\n+                    }\n+\n+                    if (beta.Gradient == null)\n+                        beta.Gradient = gradBeta;\n+                    else\n+                        beta.Gradient = beta.Gradient.Add(gradBeta);\n+                }\n+\n+                // Gradient for input\n+                if (a.RequiresGradient)\n+                {\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        var std = numOps.Sqrt(numOps.Add(variances[b], eps));\n+                        var invStd = numOps.Divide(numOps.One, std);\n+\n+                        // Compute gradient components\n+                        var gradNormSum = numOps.Zero;\n+                        var gradNormDotNorm = numOps.Zero;\n+\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            var gradNorm = numOps.Multiply(gradient[b, f], gamma.Value[f]);\n+                            gradNormSum = numOps.Add(gradNormSum, gradNorm);\n+                            gradNormDotNorm = numOps.Add(gradNormDotNorm,\n+                                numOps.Multiply(gradNorm, normalized[b, f]));\n+                        }\n+\n+                        // Apply gradient formula\n+                        var featuresT = numOps.FromDouble(features);\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            var gradNorm = numOps.Multiply(gradient[b, f], gamma.Value[f]);\n+\n+                            var term1 = gradNorm;\n+                            var term2 = numOps.Divide(gradNormSum, featuresT);\n+                            var term3 = numOps.Divide(\n+                                numOps.Multiply(normalized[b, f], gradNormDotNorm),\n+                                featuresT);\n+\n+                            var grad = numOps.Multiply(\n+                                numOps.Subtract(numOps.Subtract(term1, term2), term3),\n+                                invStd);\n+\n+                            gradA[b, f] = grad;\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var parents = new List<ComputationNode<T>> { a };\n+            if (gamma != null) parents.Add(gamma);\n+            if (beta != null) parents.Add(beta);\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient || (gamma?.RequiresGradient ?? false) || (beta?.RequiresGradient ?? false),\n+                parents: parents,\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"LayerNorm is currently only implemented for 2D tensors normalizing over last dimension. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}], normalizedShape=[{string.Join(\", \", normalizedShape)}]\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Applies batch normalization to a computation node.\n+    /// </summary>\n+    /// <param name=\"a\">The input node with shape [batch, features].</param>\n+    /// <param name=\"gamma\">Optional scale parameter (learnable). If null, uses ones.</param>\n+    /// <param name=\"beta\">Optional shift parameter (learnable). If null, uses zeros.</param>\n+    /// <param name=\"runningMean\">Running mean for inference (not updated during this operation).</param>\n+    /// <param name=\"runningVar\">Running variance for inference (not updated during this operation).</param>\n+    /// <param name=\"training\">Whether in training mode (uses batch statistics) or inference mode (uses running statistics).</param>\n+    /// <param name=\"epsilon\">Small constant for numerical stability. Default is 1e-5.</param>\n+    /// <returns>A new computation node containing the batch normalized result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Batch normalization normalizes inputs across the batch dimension.\n+    /// During training: Uses batch statistics (mean and variance computed from current batch).\n+    /// During inference: Uses running statistics (accumulated during training).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> BatchNorm standardizes features across the batch.\n+    ///\n+    /// For batch normalization:\n+    /// - Training mode: Uses current batch's mean and variance\n+    /// - Inference mode: Uses running mean/variance from training\n+    /// - Normalizes: (x - mean) / sqrt(variance)\n+    /// - Scales and shifts: result * gamma + beta\n+    ///\n+    /// Benefits:\n+    /// - Stabilizes training (reduces internal covariate shift)\n+    /// - Allows higher learning rates\n+    /// - Acts as regularization\n+    ///\n+    /// Used in:\n+    /// - CNNs (after convolutional layers)\n+    /// - Deep feedforward networks\n+    /// - GANs and many other architectures\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> BatchNorm(\n+        ComputationNode<T> a,\n+        ComputationNode<T>? gamma = null,\n+        ComputationNode<T>? beta = null,\n+        Tensor<T>? runningMean = null,\n+        Tensor<T>? runningVar = null,\n+        bool training = true,\n+        double epsilon = 1e-5)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+        var eps = numOps.FromDouble(epsilon);\n+\n+        // Handle 2D case [batch, features]\n+        if (shape.Length == 2)\n+        {\n+            int batchSize = shape[0];\n+            int features = shape[1];\n+\n+            // Create default gamma and beta if not provided\n+            if (gamma == null)\n+            {\n+                var gammaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    gammaTensor[i] = numOps.One;\n+                gamma = Variable(gammaTensor, requiresGradient: false);\n+            }\n+\n+            if (beta == null)\n+            {\n+                var betaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    betaTensor[i] = numOps.Zero;\n+                beta = Variable(betaTensor, requiresGradient: false);\n+            }\n+\n+            var result = new Tensor<T>(shape);\n+            T[] batchMean;\n+            T[] batchVar;\n+            var normalized = new Tensor<T>(shape);\n+\n+            if (training)\n+            {\n+                // Compute batch statistics\n+                batchMean = new T[features];\n+                batchVar = new T[features];\n+\n+                // Compute mean per feature\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var sum = numOps.Zero;\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        sum = numOps.Add(sum, a.Value[b, f]);\n+                    }\n+                    batchMean[f] = numOps.Divide(sum, numOps.FromDouble(batchSize));\n+                }\n+\n+                // Compute variance per feature\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var varSum = numOps.Zero;\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        var diff = numOps.Subtract(a.Value[b, f], batchMean[f]);\n+                        varSum = numOps.Add(varSum, numOps.Multiply(diff, diff));\n+                    }\n+                    batchVar[f] = numOps.Divide(varSum, numOps.FromDouble(batchSize));\n+                }\n+            }\n+            else\n+            {\n+                // Use running statistics for inference\n+                if (runningMean == null || runningVar == null)\n+                    throw new ArgumentException(\"Running statistics required for inference mode\");\n+\n+                batchMean = new T[features];\n+                batchVar = new T[features];\n+                for (int f = 0; f < features; f++)\n+                {\n+                    batchMean[f] = runningMean[f];\n+                    batchVar[f] = runningVar[f];\n+                }\n+            }\n+\n+            // Normalize and scale\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var std = numOps.Sqrt(numOps.Add(batchVar[f], eps));\n+                    var norm = numOps.Divide(\n+                        numOps.Subtract(a.Value[b, f], batchMean[f]),\n+                        std);\n+                    normalized[b, f] = norm;\n+                    result[b, f] = numOps.Add(\n+                        numOps.Multiply(norm, gamma.Value[f]),\n+                        beta.Value[f]);\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (!training)\n+                {\n+                    // Inference mode: simpler gradient (no batch statistics gradient)\n+                    if (a.RequiresGradient)\n+                    {\n+                        var gradA = new Tensor<T>(shape);\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            for (int f = 0; f < features; f++)\n+                            {\n+                                var std = numOps.Sqrt(numOps.Add(batchVar[f], eps));\n+                                var invStd = numOps.Divide(numOps.One, std);\n+                                gradA[b, f] = numOps.Multiply(\n+                                    numOps.Multiply(gradient[b, f], gamma.Value[f]),\n+                                    invStd);\n+                            }\n+                        }\n+\n+                        if (a.Gradient == null)\n+                            a.Gradient = gradA;\n+                        else\n+                            a.Gradient = a.Gradient.Add(gradA);"}]}},{"id":"PRRT_kwDOKSXUF85hck-h","isResolved":false,"isOutdated":false,"path":"src/Autodiff/TensorOperations.cs","line":2360,"originalLine":2360,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzD-2","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement write to the same variable - consider using '?' to express intent better.\n```suggestion\n                    gamma.Gradient = gamma.Gradient == null ? gradGamma : gamma.Gradient.Add(gradGamma);\n```","createdAt":"2025-11-11T07:53:25Z","path":"src/Autodiff/TensorOperations.cs","line":2360,"originalLine":2360,"diffHunk":"@@ -1219,4 +1219,1769 @@ void BackwardFunction(Tensor<T> gradient)\n \n         return node;\n     }\n+\n+    /// <summary>\n+    /// Computes the softmax function for a computation node along a specified axis.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"axis\">The axis along which to compute softmax. Default is -1 (last axis).</param>\n+    /// <returns>A new computation node containing the softmax result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method computes softmax (σ(x_i) = exp(x_i) / Σexp(x_j)) along the specified axis.\n+    /// Uses numerical stability trick: subtract max before exponentiating.\n+    /// The backward function uses: ∂softmax/∂x = softmax(x) * (grad - Σ(grad * softmax(x))).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Softmax converts a vector of numbers into probabilities.\n+    ///\n+    /// For softmax:\n+    /// - The forward pass exponentiates each element, then normalizes so they sum to 1\n+    /// - The result is a probability distribution (all values between 0 and 1, summing to 1)\n+    /// - The backward pass is complex but efficient: uses the Jacobian of softmax\n+    ///\n+    /// Softmax is crucial for:\n+    /// - Multi-class classification (final layer outputs)\n+    /// - Attention mechanisms (computing attention weights)\n+    /// - Anywhere you need to convert scores to probabilities\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Softmax(ComputationNode<T> a, int axis = -1)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        // Normalize axis to positive index\n+        if (axis < 0)\n+            axis = shape.Length + axis;\n+\n+        // For simplicity, handle 2D case (batch, features) with axis=-1\n+        if (shape.Length == 2 && axis == 1)\n+        {\n+            int batchSize = shape[0];\n+            int features = shape[1];\n+            var result = new Tensor<T>(shape);\n+\n+            // Compute softmax for each row\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                // Find max for numerical stability\n+                var maxVal = a.Value[b, 0];\n+                for (int f = 1; f < features; f++)\n+                {\n+                    if (numOps.GreaterThan(a.Value[b, f], maxVal))\n+                        maxVal = a.Value[b, f];\n+                }\n+\n+                // Compute exp(x - max) and sum\n+                var expSum = numOps.Zero;\n+                var expValues = new T[features];\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var shifted = numOps.Subtract(a.Value[b, f], maxVal);\n+                    expValues[f] = numOps.Exp(shifted);\n+                    expSum = numOps.Add(expSum, expValues[f]);\n+                }\n+\n+                // Normalize\n+                for (int f = 0; f < features; f++)\n+                {\n+                    result[b, f] = numOps.Divide(expValues[f], expSum);\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (a.RequiresGradient)\n+                {\n+                    // ∂softmax/∂x_i = softmax_i * (∂L/∂y_i - Σ_j(∂L/∂y_j * softmax_j))\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        // Compute sum of (gradient * softmax)\n+                        var dotProduct = numOps.Zero;\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            dotProduct = numOps.Add(dotProduct,\n+                                numOps.Multiply(gradient[b, f], result[b, f]));\n+                        }\n+\n+                        // Compute gradient for each element\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            var gradMinusDot = numOps.Subtract(gradient[b, f], dotProduct);\n+                            gradA[b, f] = numOps.Multiply(result[b, f], gradMinusDot);\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient,\n+                parents: new List<ComputationNode<T>> { a },\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Softmax is currently only implemented for 2D tensors along axis=-1. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}], axis={axis}\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Concatenates multiple computation nodes along a specified axis.\n+    /// </summary>\n+    /// <param name=\"nodes\">The list of nodes to concatenate.</param>\n+    /// <param name=\"axis\">The axis along which to concatenate. Default is 0.</param>\n+    /// <returns>A new computation node containing the concatenated result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method concatenates tensors along the specified axis.\n+    /// All tensors must have the same shape except along the concatenation axis.\n+    /// The backward function splits the gradient and sends each portion to the corresponding input.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Concat stacks tensors together along a dimension.\n+    ///\n+    /// For concatenation:\n+    /// - The forward pass combines multiple tensors into one larger tensor\n+    /// - The backward pass splits the gradient back to each input\n+    /// - Think of it like gluing arrays together end-to-end\n+    ///\n+    /// Used in:\n+    /// - Skip connections (concatenating features from different layers)\n+    /// - Multi-input architectures\n+    /// - Feature fusion in neural networks\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Concat(List<ComputationNode<T>> nodes, int axis = 0)\n+    {\n+        if (nodes.Count == 0)\n+            throw new ArgumentException(\"Cannot concatenate empty list of nodes\");\n+\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var firstShape = nodes[0].Value.Shape;\n+\n+        // Normalize axis\n+        if (axis < 0)\n+            axis = firstShape.Length + axis;\n+\n+        // Validate shapes match except on concat axis\n+        for (int i = 1; i < nodes.Count; i++)\n+        {\n+            var shape = nodes[i].Value.Shape;\n+            if (shape.Length != firstShape.Length)\n+                throw new ArgumentException(\"All tensors must have the same rank\");\n+\n+            for (int d = 0; d < firstShape.Length; d++)\n+            {\n+                if (d != axis && shape[d] != firstShape[d])\n+                    throw new ArgumentException(\n+                        $\"Shape mismatch at dimension {d}: {shape[d]} vs {firstShape[d]}\");\n+            }\n+        }\n+\n+        // Compute output shape\n+        int[] outputShape = (int[])firstShape.Clone();\n+        for (int i = 1; i < nodes.Count; i++)\n+        {\n+            outputShape[axis] += nodes[i].Value.Shape[axis];\n+        }\n+\n+        // Perform concatenation (handle 2D case for simplicity)\n+        Tensor<T> result;\n+        if (firstShape.Length == 2 && axis == 1)\n+        {\n+            // Concatenate along columns (features)\n+            int rows = firstShape[0];\n+            int totalCols = outputShape[1];\n+            result = new Tensor<T>(new int[] { rows, totalCols });\n+\n+            int colOffset = 0;\n+            foreach (var node in nodes)\n+            {\n+                int cols = node.Value.Shape[1];\n+                for (int r = 0; r < rows; r++)\n+                {\n+                    for (int c = 0; c < cols; c++)\n+                    {\n+                        result[r, colOffset + c] = node.Value[r, c];\n+                    }\n+                }\n+                colOffset += cols;\n+            }\n+        }\n+        else if (firstShape.Length == 2 && axis == 0)\n+        {\n+            // Concatenate along rows (batch)\n+            int cols = firstShape[1];\n+            int totalRows = outputShape[0];\n+            result = new Tensor<T>(new int[] { totalRows, cols });\n+\n+            int rowOffset = 0;\n+            foreach (var node in nodes)\n+            {\n+                int rows = node.Value.Shape[0];\n+                for (int r = 0; r < rows; r++)\n+                {\n+                    for (int c = 0; c < cols; c++)\n+                    {\n+                        result[rowOffset + r, c] = node.Value[r, c];\n+                    }\n+                }\n+                rowOffset += rows;\n+            }\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Concat is currently only implemented for 2D tensors. \" +\n+                $\"Got shape=[{string.Join(\", \", firstShape)}]\");\n+        }\n+\n+        // Store sizes for gradient splitting\n+        var sizes = nodes.Select(n => n.Value.Shape[axis]).ToList();\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            // Split gradient along concat axis and distribute to inputs\n+            if (firstShape.Length == 2 && axis == 1)\n+            {\n+                int rows = firstShape[0];\n+                int colOffset = 0;\n+                for (int i = 0; i < nodes.Count; i++)\n+                {\n+                    if (!nodes[i].RequiresGradient)\n+                    {\n+                        colOffset += sizes[i];\n+                        continue;\n+                    }\n+\n+                    int cols = sizes[i];\n+                    var gradPart = new Tensor<T>(new int[] { rows, cols });\n+\n+                    for (int r = 0; r < rows; r++)\n+                    {\n+                        for (int c = 0; c < cols; c++)\n+                        {\n+                            gradPart[r, c] = gradient[r, colOffset + c];\n+                        }\n+                    }\n+\n+                    if (nodes[i].Gradient == null)\n+                        nodes[i].Gradient = gradPart;\n+                    else\n+                        nodes[i].Gradient = nodes[i].Gradient.Add(gradPart);\n+\n+                    colOffset += cols;\n+                }\n+            }\n+            else if (firstShape.Length == 2 && axis == 0)\n+            {\n+                int cols = firstShape[1];\n+                int rowOffset = 0;\n+                for (int i = 0; i < nodes.Count; i++)\n+                {\n+                    if (!nodes[i].RequiresGradient)\n+                    {\n+                        rowOffset += sizes[i];\n+                        continue;\n+                    }\n+\n+                    int rows = sizes[i];\n+                    var gradPart = new Tensor<T>(new int[] { rows, cols });\n+\n+                    for (int r = 0; r < rows; r++)\n+                    {\n+                        for (int c = 0; c < cols; c++)\n+                        {\n+                            gradPart[r, c] = gradient[rowOffset + r, c];\n+                        }\n+                    }\n+\n+                    if (nodes[i].Gradient == null)\n+                        nodes[i].Gradient = gradPart;\n+                    else\n+                        nodes[i].Gradient = nodes[i].Gradient.Add(gradPart);\n+\n+                    rowOffset += rows;\n+                }\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: nodes.Any(n => n.RequiresGradient),\n+            parents: nodes,\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Pads a tensor with a constant value along specified dimensions.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"padWidth\">Padding width for each dimension as (before, after) pairs.</param>\n+    /// <param name=\"value\">The value to use for padding. Default is zero.</param>\n+    /// <returns>A new computation node containing the padded result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method adds padding around the tensor.\n+    /// The backward function simply crops the gradient back to the original size (gradients for padding are zero).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Pad adds extra elements around a tensor.\n+    ///\n+    /// For padding:\n+    /// - The forward pass adds border elements with a constant value\n+    /// - The backward pass removes those border gradients (they don't affect the original tensor)\n+    /// - Think of it like adding margins to an image\n+    ///\n+    /// Used in:\n+    /// - Convolutional layers (to maintain spatial dimensions)\n+    /// - Handling variable-length sequences\n+    /// - Data augmentation\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Pad(ComputationNode<T> a, int[,] padWidth, T? value = default)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var padValue = value ?? numOps.Zero;\n+        var shape = a.Value.Shape;\n+\n+        // Validate padWidth dimensions\n+        if (padWidth.GetLength(0) != shape.Length)\n+            throw new ArgumentException(\"padWidth must have same number of dimensions as input tensor\");\n+\n+        // Compute output shape\n+        var outputShape = new int[shape.Length];\n+        for (int d = 0; d < shape.Length; d++)\n+        {\n+            outputShape[d] = shape[d] + padWidth[d, 0] + padWidth[d, 1];\n+        }\n+\n+        // Handle 2D case\n+        if (shape.Length == 2)\n+        {\n+            int inputRows = shape[0];\n+            int inputCols = shape[1];\n+            int padTop = padWidth[0, 0];\n+            int padBottom = padWidth[0, 1];\n+            int padLeft = padWidth[1, 0];\n+            int padRight = padWidth[1, 1];\n+\n+            var result = new Tensor<T>(outputShape);\n+\n+            // Initialize with pad value\n+            for (int i = 0; i < result.Length; i++)\n+            {\n+                result[i] = padValue;\n+            }\n+\n+            // Copy input data to center\n+            for (int r = 0; r < inputRows; r++)\n+            {\n+                for (int c = 0; c < inputCols; c++)\n+                {\n+                    result[padTop + r, padLeft + c] = a.Value[r, c];\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (a.RequiresGradient)\n+                {\n+                    // Extract gradient for original (unpadded) region\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int r = 0; r < inputRows; r++)\n+                    {\n+                        for (int c = 0; c < inputCols; c++)\n+                        {\n+                            gradA[r, c] = gradient[padTop + r, padLeft + c];\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient,\n+                parents: new List<ComputationNode<T>> { a },\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Pad is currently only implemented for 2D tensors. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}]\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs 2D max pooling on a 4D tensor (batch, channels, height, width).\n+    /// </summary>\n+    /// <param name=\"a\">The input node with shape [batch, channels, height, width].</param>\n+    /// <param name=\"poolSize\">The size of the pooling window [poolH, poolW].</param>\n+    /// <param name=\"strides\">The stride for the pooling operation [strideH, strideW]. If null, uses poolSize.</param>\n+    /// <returns>A new computation node containing the max pooled result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method performs max pooling over 2D spatial dimensions.\n+    /// During forward pass, it tracks which element was the max for routing gradients during backward pass.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> MaxPool downsamples by taking the maximum value in each window.\n+    ///\n+    /// For max pooling:\n+    /// - The forward pass slides a window and takes the max value in each position\n+    /// - This reduces spatial dimensions (downsampling)\n+    /// - The backward pass routes gradients only to the positions that were max\n+    /// - Other positions get zero gradient (they didn't contribute to the output)\n+    ///\n+    /// Used in:\n+    /// - CNNs for translation invariance\n+    /// - Reducing spatial resolution\n+    /// - Building hierarchical features\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> MaxPool2D(\n+        ComputationNode<T> a,\n+        int[] poolSize,\n+        int[]? strides = null)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        if (shape.Length != 4)\n+            throw new ArgumentException(\"MaxPool2D requires 4D input [batch, channels, height, width]\");\n+\n+        strides ??= poolSize;\n+\n+        int batch = shape[0];\n+        int channels = shape[1];\n+        int inH = shape[2];\n+        int inW = shape[3];\n+        int poolH = poolSize[0];\n+        int poolW = poolSize[1];\n+        int strideH = strides[0];\n+        int strideW = strides[1];\n+\n+        int outH = (inH - poolH) / strideH + 1;\n+        int outW = (inW - poolW) / strideW + 1;\n+\n+        var result = new Tensor<T>(new int[] { batch, channels, outH, outW });\n+        // Store max positions for backprop\n+        var maxPositions = new int[batch, channels, outH, outW, 2]; // [h_offset, w_offset]\n+\n+        // Forward pass: compute max pooling and track positions\n+        for (int b = 0; b < batch; b++)\n+        {\n+            for (int c = 0; c < channels; c++)\n+            {\n+                for (int oh = 0; oh < outH; oh++)\n+                {\n+                    for (int ow = 0; ow < outW; ow++)\n+                    {\n+                        int hStart = oh * strideH;\n+                        int wStart = ow * strideW;\n+\n+                        var maxVal = a.Value[b * channels * inH * inW +\n+                                           c * inH * inW +\n+                                           hStart * inW +\n+                                           wStart];\n+                        int maxHOffset = 0;\n+                        int maxWOffset = 0;\n+\n+                        // Find max in pooling window\n+                        for (int ph = 0; ph < poolH; ph++)\n+                        {\n+                            for (int pw = 0; pw < poolW; pw++)\n+                            {\n+                                int h = hStart + ph;\n+                                int w = wStart + pw;\n+                                if (h < inH && w < inW)\n+                                {\n+                                    var val = a.Value[b * channels * inH * inW +\n+                                                     c * inH * inW +\n+                                                     h * inW +\n+                                                     w];\n+                                    if (numOps.GreaterThan(val, maxVal))\n+                                    {\n+                                        maxVal = val;\n+                                        maxHOffset = ph;\n+                                        maxWOffset = pw;\n+                                    }\n+                                }\n+                            }\n+                        }\n+\n+                        result[b, c, oh, ow] = maxVal;\n+                        maxPositions[b, c, oh, ow, 0] = maxHOffset;\n+                        maxPositions[b, c, oh, ow, 1] = maxWOffset;\n+                    }\n+                }\n+            }\n+        }\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            if (a.RequiresGradient)\n+            {\n+                var gradA = new Tensor<T>(shape);\n+\n+                // Route gradients to max positions\n+                for (int b = 0; b < batch; b++)\n+                {\n+                    for (int c = 0; c < channels; c++)\n+                    {\n+                        for (int oh = 0; oh < outH; oh++)\n+                        {\n+                            for (int ow = 0; ow < outW; ow++)\n+                            {\n+                                int hStart = oh * strideH;\n+                                int wStart = ow * strideW;\n+                                int maxHOffset = maxPositions[b, c, oh, ow, 0];\n+                                int maxWOffset = maxPositions[b, c, oh, ow, 1];\n+\n+                                int maxH = hStart + maxHOffset;\n+                                int maxW = wStart + maxWOffset;\n+\n+                                int gradIdx = b * channels * inH * inW +\n+                                             c * inH * inW +\n+                                            maxH * inW +\n+                                             maxW;\n+\n+                                gradA[gradIdx] = numOps.Add(gradA[gradIdx], gradient[b, c, oh, ow]);\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (a.Gradient == null)\n+                    a.Gradient = gradA;\n+                else\n+                    a.Gradient = a.Gradient.Add(gradA);\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: a.RequiresGradient,\n+            parents: new List<ComputationNode<T>> { a },\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Performs 2D average pooling on a 4D tensor (batch, channels, height, width).\n+    /// </summary>\n+    /// <param name=\"a\">The input node with shape [batch, channels, height, width].</param>\n+    /// <param name=\"poolSize\">The size of the pooling window [poolH, poolW].</param>\n+    /// <param name=\"strides\">The stride for the pooling operation [strideH, strideW]. If null, uses poolSize.</param>\n+    /// <returns>A new computation node containing the average pooled result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method performs average pooling over 2D spatial dimensions.\n+    /// The backward function distributes gradients equally across the pooling window.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> AvgPool downsamples by taking the average value in each window.\n+    ///\n+    /// For average pooling:\n+    /// - The forward pass slides a window and computes the average\n+    /// - This smoothly reduces spatial dimensions\n+    /// - The backward pass distributes gradients equally to all elements in the window\n+    /// - Each element gets gradient / pool_area\n+    ///\n+    /// Used in:\n+    /// - CNNs for smoother downsampling than max pooling\n+    /// - Global average pooling (replacing fully connected layers)\n+    /// - Reducing overfitting\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> AvgPool2D(\n+        ComputationNode<T> a,\n+        int[] poolSize,\n+        int[]? strides = null)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        if (shape.Length != 4)\n+            throw new ArgumentException(\"AvgPool2D requires 4D input [batch, channels, height, width]\");\n+\n+        strides ??= poolSize;\n+\n+        int batch = shape[0];\n+        int channels = shape[1];\n+        int inH = shape[2];\n+        int inW = shape[3];\n+        int poolH = poolSize[0];\n+        int poolW = poolSize[1];\n+        int strideH = strides[0];\n+        int strideW = strides[1];\n+\n+        int outH = (inH - poolH) / strideH + 1;\n+        int outW = (inW - poolW) / strideW + 1;\n+\n+        var result = new Tensor<T>(new int[] { batch, channels, outH, outW });\n+        var poolArea = numOps.FromDouble(poolH * poolW);\n+\n+        // Forward pass: compute average pooling\n+        for (int b = 0; b < batch; b++)\n+        {\n+            for (int c = 0; c < channels; c++)\n+            {\n+                for (int oh = 0; oh < outH; oh++)\n+                {\n+                    for (int ow = 0; ow < outW; ow++)\n+                    {\n+                        int hStart = oh * strideH;\n+                        int wStart = ow * strideW;\n+\n+                        var sum = numOps.Zero;\n+\n+                        // Sum values in pooling window\n+                        for (int ph = 0; ph < poolH; ph++)\n+                        {\n+                            for (int pw = 0; pw < poolW; pw++)\n+                            {\n+                                int h = hStart + ph;\n+                                int w = wStart + pw;\n+                                if (h < inH && w < inW)\n+                                {\n+                                    sum = numOps.Add(sum, a.Value[b, c, h, w]);\n+                                }\n+                            }\n+                        }\n+\n+                        result[b, c, oh, ow] = numOps.Divide(sum, poolArea);\n+                    }\n+                }\n+            }\n+        }\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            if (a.RequiresGradient)\n+            {\n+                var gradA = new Tensor<T>(shape);\n+\n+                // Distribute gradients equally across pooling windows\n+                for (int b = 0; b < batch; b++)\n+                {\n+                    for (int c = 0; c < channels; c++)\n+                    {\n+                        for (int oh = 0; oh < outH; oh++)\n+                        {\n+                            for (int ow = 0; ow < outW; ow++)\n+                            {\n+                                int hStart = oh * strideH;\n+                                int wStart = ow * strideW;\n+\n+                                var gradValue = numOps.Divide(gradient[b, c, oh, ow], poolArea);\n+\n+                                // Distribute to all elements in window\n+                                for (int ph = 0; ph < poolH; ph++)\n+                                {\n+                                    for (int pw = 0; pw < poolW; pw++)\n+                                    {\n+                                        int h = hStart + ph;\n+                                        int w = wStart + pw;\n+                                        if (h < inH && w < inW)\n+                                        {\n+                                            gradA[b, c, h, w] = numOps.Add(gradA[b, c, h, w], gradValue);\n+                                        }\n+                                    }\n+                                }\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (a.Gradient == null)\n+                    a.Gradient = gradA;\n+                else\n+                    a.Gradient = a.Gradient.Add(gradA);\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: a.RequiresGradient,\n+            parents: new List<ComputationNode<T>> { a },\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Applies layer normalization to a computation node.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"normalizedShape\">The shape over which to normalize (typically the feature dimensions).</param>\n+    /// <param name=\"gamma\">Optional scale parameter (learnable). If null, uses ones.</param>\n+    /// <param name=\"beta\">Optional shift parameter (learnable). If null, uses zeros.</param>\n+    /// <param name=\"epsilon\">Small constant for numerical stability. Default is 1e-5.</param>\n+    /// <returns>A new computation node containing the layer normalized result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Layer normalization normalizes inputs across the feature dimension for each sample independently.\n+    /// Formula: y = gamma * (x - mean) / sqrt(variance + epsilon) + beta\n+    /// Unlike batch normalization, this doesn't depend on batch statistics.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> LayerNorm standardizes features for each sample independently.\n+    ///\n+    /// For layer normalization:\n+    /// - Computes mean and variance for each sample's features\n+    /// - Normalizes: (x - mean) / sqrt(variance)\n+    /// - Scales and shifts: result * gamma + beta\n+    /// - Works the same during training and inference (no batch dependency)\n+    ///\n+    /// Used in:\n+    /// - Transformers (critical component)\n+    /// - RNNs (stabilizes training)\n+    /// - Any architecture needing sample-independent normalization\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> LayerNorm(\n+        ComputationNode<T> a,\n+        int[] normalizedShape,\n+        ComputationNode<T>? gamma = null,\n+        ComputationNode<T>? beta = null,\n+        double epsilon = 1e-5)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+        var eps = numOps.FromDouble(epsilon);\n+\n+        // For 2D input [batch, features], normalize over features\n+        if (shape.Length == 2 && normalizedShape.Length == 1 && normalizedShape[0] == shape[1])\n+        {\n+            int batchSize = shape[0];\n+            int features = shape[1];\n+\n+            // Create default gamma (ones) and beta (zeros) if not provided\n+            if (gamma == null)\n+            {\n+                var gammaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    gammaTensor[i] = numOps.One;\n+                gamma = Variable(gammaTensor, requiresGradient: false);\n+            }\n+\n+            if (beta == null)\n+            {\n+                var betaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    betaTensor[i] = numOps.Zero;\n+                beta = Variable(betaTensor, requiresGradient: false);\n+            }\n+\n+            var result = new Tensor<T>(shape);\n+            var means = new T[batchSize];\n+            var variances = new T[batchSize];\n+            var normalized = new Tensor<T>(shape);\n+\n+            // Forward pass: compute mean and variance per sample\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                // Compute mean\n+                var sum = numOps.Zero;\n+                for (int f = 0; f < features; f++)\n+                {\n+                    sum = numOps.Add(sum, a.Value[b, f]);\n+                }\n+                means[b] = numOps.Divide(sum, numOps.FromDouble(features));\n+\n+                // Compute variance\n+                var varSum = numOps.Zero;\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var diff = numOps.Subtract(a.Value[b, f], means[b]);\n+                    varSum = numOps.Add(varSum, numOps.Multiply(diff, diff));\n+                }\n+                variances[b] = numOps.Divide(varSum, numOps.FromDouble(features));\n+\n+                // Normalize and scale\n+                var std = numOps.Sqrt(numOps.Add(variances[b], eps));\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var norm = numOps.Divide(\n+                        numOps.Subtract(a.Value[b, f], means[b]),\n+                        std);\n+                    normalized[b, f] = norm;\n+                    result[b, f] = numOps.Add(\n+                        numOps.Multiply(norm, gamma.Value[f]),\n+                        beta.Value[f]);\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                // Gradients for gamma and beta\n+                if (gamma.RequiresGradient)\n+                {\n+                    var gradGamma = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum,\n+                                numOps.Multiply(gradient[b, f], normalized[b, f]));\n+                        }\n+                        gradGamma[f] = sum;\n+                    }\n+\n+                    if (gamma.Gradient == null)\n+                        gamma.Gradient = gradGamma;\n+                    else\n+                        gamma.Gradient = gamma.Gradient.Add(gradGamma);\n+                }\n+\n+                if (beta.RequiresGradient)\n+                {\n+                    var gradBeta = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum, gradient[b, f]);\n+                        }\n+                        gradBeta[f] = sum;\n+                    }\n+\n+                    if (beta.Gradient == null)\n+                        beta.Gradient = gradBeta;\n+                    else\n+                        beta.Gradient = beta.Gradient.Add(gradBeta);\n+                }\n+\n+                // Gradient for input\n+                if (a.RequiresGradient)\n+                {\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        var std = numOps.Sqrt(numOps.Add(variances[b], eps));\n+                        var invStd = numOps.Divide(numOps.One, std);\n+\n+                        // Compute gradient components\n+                        var gradNormSum = numOps.Zero;\n+                        var gradNormDotNorm = numOps.Zero;\n+\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            var gradNorm = numOps.Multiply(gradient[b, f], gamma.Value[f]);\n+                            gradNormSum = numOps.Add(gradNormSum, gradNorm);\n+                            gradNormDotNorm = numOps.Add(gradNormDotNorm,\n+                                numOps.Multiply(gradNorm, normalized[b, f]));\n+                        }\n+\n+                        // Apply gradient formula\n+                        var featuresT = numOps.FromDouble(features);\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            var gradNorm = numOps.Multiply(gradient[b, f], gamma.Value[f]);\n+\n+                            var term1 = gradNorm;\n+                            var term2 = numOps.Divide(gradNormSum, featuresT);\n+                            var term3 = numOps.Divide(\n+                                numOps.Multiply(normalized[b, f], gradNormDotNorm),\n+                                featuresT);\n+\n+                            var grad = numOps.Multiply(\n+                                numOps.Subtract(numOps.Subtract(term1, term2), term3),\n+                                invStd);\n+\n+                            gradA[b, f] = grad;\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var parents = new List<ComputationNode<T>> { a };\n+            if (gamma != null) parents.Add(gamma);\n+            if (beta != null) parents.Add(beta);\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient || (gamma?.RequiresGradient ?? false) || (beta?.RequiresGradient ?? false),\n+                parents: parents,\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"LayerNorm is currently only implemented for 2D tensors normalizing over last dimension. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}], normalizedShape=[{string.Join(\", \", normalizedShape)}]\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Applies batch normalization to a computation node.\n+    /// </summary>\n+    /// <param name=\"a\">The input node with shape [batch, features].</param>\n+    /// <param name=\"gamma\">Optional scale parameter (learnable). If null, uses ones.</param>\n+    /// <param name=\"beta\">Optional shift parameter (learnable). If null, uses zeros.</param>\n+    /// <param name=\"runningMean\">Running mean for inference (not updated during this operation).</param>\n+    /// <param name=\"runningVar\">Running variance for inference (not updated during this operation).</param>\n+    /// <param name=\"training\">Whether in training mode (uses batch statistics) or inference mode (uses running statistics).</param>\n+    /// <param name=\"epsilon\">Small constant for numerical stability. Default is 1e-5.</param>\n+    /// <returns>A new computation node containing the batch normalized result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Batch normalization normalizes inputs across the batch dimension.\n+    /// During training: Uses batch statistics (mean and variance computed from current batch).\n+    /// During inference: Uses running statistics (accumulated during training).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> BatchNorm standardizes features across the batch.\n+    ///\n+    /// For batch normalization:\n+    /// - Training mode: Uses current batch's mean and variance\n+    /// - Inference mode: Uses running mean/variance from training\n+    /// - Normalizes: (x - mean) / sqrt(variance)\n+    /// - Scales and shifts: result * gamma + beta\n+    ///\n+    /// Benefits:\n+    /// - Stabilizes training (reduces internal covariate shift)\n+    /// - Allows higher learning rates\n+    /// - Acts as regularization\n+    ///\n+    /// Used in:\n+    /// - CNNs (after convolutional layers)\n+    /// - Deep feedforward networks\n+    /// - GANs and many other architectures\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> BatchNorm(\n+        ComputationNode<T> a,\n+        ComputationNode<T>? gamma = null,\n+        ComputationNode<T>? beta = null,\n+        Tensor<T>? runningMean = null,\n+        Tensor<T>? runningVar = null,\n+        bool training = true,\n+        double epsilon = 1e-5)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+        var eps = numOps.FromDouble(epsilon);\n+\n+        // Handle 2D case [batch, features]\n+        if (shape.Length == 2)\n+        {\n+            int batchSize = shape[0];\n+            int features = shape[1];\n+\n+            // Create default gamma and beta if not provided\n+            if (gamma == null)\n+            {\n+                var gammaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    gammaTensor[i] = numOps.One;\n+                gamma = Variable(gammaTensor, requiresGradient: false);\n+            }\n+\n+            if (beta == null)\n+            {\n+                var betaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    betaTensor[i] = numOps.Zero;\n+                beta = Variable(betaTensor, requiresGradient: false);\n+            }\n+\n+            var result = new Tensor<T>(shape);\n+            T[] batchMean;\n+            T[] batchVar;\n+            var normalized = new Tensor<T>(shape);\n+\n+            if (training)\n+            {\n+                // Compute batch statistics\n+                batchMean = new T[features];\n+                batchVar = new T[features];\n+\n+                // Compute mean per feature\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var sum = numOps.Zero;\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        sum = numOps.Add(sum, a.Value[b, f]);\n+                    }\n+                    batchMean[f] = numOps.Divide(sum, numOps.FromDouble(batchSize));\n+                }\n+\n+                // Compute variance per feature\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var varSum = numOps.Zero;\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        var diff = numOps.Subtract(a.Value[b, f], batchMean[f]);\n+                        varSum = numOps.Add(varSum, numOps.Multiply(diff, diff));\n+                    }\n+                    batchVar[f] = numOps.Divide(varSum, numOps.FromDouble(batchSize));\n+                }\n+            }\n+            else\n+            {\n+                // Use running statistics for inference\n+                if (runningMean == null || runningVar == null)\n+                    throw new ArgumentException(\"Running statistics required for inference mode\");\n+\n+                batchMean = new T[features];\n+                batchVar = new T[features];\n+                for (int f = 0; f < features; f++)\n+                {\n+                    batchMean[f] = runningMean[f];\n+                    batchVar[f] = runningVar[f];\n+                }\n+            }\n+\n+            // Normalize and scale\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var std = numOps.Sqrt(numOps.Add(batchVar[f], eps));\n+                    var norm = numOps.Divide(\n+                        numOps.Subtract(a.Value[b, f], batchMean[f]),\n+                        std);\n+                    normalized[b, f] = norm;\n+                    result[b, f] = numOps.Add(\n+                        numOps.Multiply(norm, gamma.Value[f]),\n+                        beta.Value[f]);\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (!training)\n+                {\n+                    // Inference mode: simpler gradient (no batch statistics gradient)\n+                    if (a.RequiresGradient)\n+                    {\n+                        var gradA = new Tensor<T>(shape);\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            for (int f = 0; f < features; f++)\n+                            {\n+                                var std = numOps.Sqrt(numOps.Add(batchVar[f], eps));\n+                                var invStd = numOps.Divide(numOps.One, std);\n+                                gradA[b, f] = numOps.Multiply(\n+                                    numOps.Multiply(gradient[b, f], gamma.Value[f]),\n+                                    invStd);\n+                            }\n+                        }\n+\n+                        if (a.Gradient == null)\n+                            a.Gradient = gradA;\n+                        else\n+                            a.Gradient = a.Gradient.Add(gradA);\n+                    }\n+                    return;\n+                }\n+\n+                // Training mode: full gradient computation\n+                // Gradients for gamma and beta\n+                if (gamma.RequiresGradient)\n+                {\n+                    var gradGamma = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum,\n+                                numOps.Multiply(gradient[b, f], normalized[b, f]));\n+                        }\n+                        gradGamma[f] = sum;\n+                    }\n+\n+                    if (gamma.Gradient == null)\n+                        gamma.Gradient = gradGamma;\n+                    else\n+                        gamma.Gradient = gamma.Gradient.Add(gradGamma);"}]}},{"id":"PRRT_kwDOKSXUF85hck-q","isResolved":false,"isOutdated":false,"path":"src/Autodiff/TensorOperations.cs","line":2379,"originalLine":2379,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzD-_","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement write to the same variable - consider using '?' to express intent better.\n```suggestion\n                    beta.Gradient = beta.Gradient == null\n                        ? gradBeta\n                        : beta.Gradient.Add(gradBeta);\n```","createdAt":"2025-11-11T07:53:26Z","path":"src/Autodiff/TensorOperations.cs","line":2379,"originalLine":2379,"diffHunk":"@@ -1219,4 +1219,1769 @@ void BackwardFunction(Tensor<T> gradient)\n \n         return node;\n     }\n+\n+    /// <summary>\n+    /// Computes the softmax function for a computation node along a specified axis.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"axis\">The axis along which to compute softmax. Default is -1 (last axis).</param>\n+    /// <returns>A new computation node containing the softmax result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method computes softmax (σ(x_i) = exp(x_i) / Σexp(x_j)) along the specified axis.\n+    /// Uses numerical stability trick: subtract max before exponentiating.\n+    /// The backward function uses: ∂softmax/∂x = softmax(x) * (grad - Σ(grad * softmax(x))).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Softmax converts a vector of numbers into probabilities.\n+    ///\n+    /// For softmax:\n+    /// - The forward pass exponentiates each element, then normalizes so they sum to 1\n+    /// - The result is a probability distribution (all values between 0 and 1, summing to 1)\n+    /// - The backward pass is complex but efficient: uses the Jacobian of softmax\n+    ///\n+    /// Softmax is crucial for:\n+    /// - Multi-class classification (final layer outputs)\n+    /// - Attention mechanisms (computing attention weights)\n+    /// - Anywhere you need to convert scores to probabilities\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Softmax(ComputationNode<T> a, int axis = -1)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        // Normalize axis to positive index\n+        if (axis < 0)\n+            axis = shape.Length + axis;\n+\n+        // For simplicity, handle 2D case (batch, features) with axis=-1\n+        if (shape.Length == 2 && axis == 1)\n+        {\n+            int batchSize = shape[0];\n+            int features = shape[1];\n+            var result = new Tensor<T>(shape);\n+\n+            // Compute softmax for each row\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                // Find max for numerical stability\n+                var maxVal = a.Value[b, 0];\n+                for (int f = 1; f < features; f++)\n+                {\n+                    if (numOps.GreaterThan(a.Value[b, f], maxVal))\n+                        maxVal = a.Value[b, f];\n+                }\n+\n+                // Compute exp(x - max) and sum\n+                var expSum = numOps.Zero;\n+                var expValues = new T[features];\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var shifted = numOps.Subtract(a.Value[b, f], maxVal);\n+                    expValues[f] = numOps.Exp(shifted);\n+                    expSum = numOps.Add(expSum, expValues[f]);\n+                }\n+\n+                // Normalize\n+                for (int f = 0; f < features; f++)\n+                {\n+                    result[b, f] = numOps.Divide(expValues[f], expSum);\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (a.RequiresGradient)\n+                {\n+                    // ∂softmax/∂x_i = softmax_i * (∂L/∂y_i - Σ_j(∂L/∂y_j * softmax_j))\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        // Compute sum of (gradient * softmax)\n+                        var dotProduct = numOps.Zero;\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            dotProduct = numOps.Add(dotProduct,\n+                                numOps.Multiply(gradient[b, f], result[b, f]));\n+                        }\n+\n+                        // Compute gradient for each element\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            var gradMinusDot = numOps.Subtract(gradient[b, f], dotProduct);\n+                            gradA[b, f] = numOps.Multiply(result[b, f], gradMinusDot);\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient,\n+                parents: new List<ComputationNode<T>> { a },\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Softmax is currently only implemented for 2D tensors along axis=-1. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}], axis={axis}\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Concatenates multiple computation nodes along a specified axis.\n+    /// </summary>\n+    /// <param name=\"nodes\">The list of nodes to concatenate.</param>\n+    /// <param name=\"axis\">The axis along which to concatenate. Default is 0.</param>\n+    /// <returns>A new computation node containing the concatenated result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method concatenates tensors along the specified axis.\n+    /// All tensors must have the same shape except along the concatenation axis.\n+    /// The backward function splits the gradient and sends each portion to the corresponding input.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Concat stacks tensors together along a dimension.\n+    ///\n+    /// For concatenation:\n+    /// - The forward pass combines multiple tensors into one larger tensor\n+    /// - The backward pass splits the gradient back to each input\n+    /// - Think of it like gluing arrays together end-to-end\n+    ///\n+    /// Used in:\n+    /// - Skip connections (concatenating features from different layers)\n+    /// - Multi-input architectures\n+    /// - Feature fusion in neural networks\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Concat(List<ComputationNode<T>> nodes, int axis = 0)\n+    {\n+        if (nodes.Count == 0)\n+            throw new ArgumentException(\"Cannot concatenate empty list of nodes\");\n+\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var firstShape = nodes[0].Value.Shape;\n+\n+        // Normalize axis\n+        if (axis < 0)\n+            axis = firstShape.Length + axis;\n+\n+        // Validate shapes match except on concat axis\n+        for (int i = 1; i < nodes.Count; i++)\n+        {\n+            var shape = nodes[i].Value.Shape;\n+            if (shape.Length != firstShape.Length)\n+                throw new ArgumentException(\"All tensors must have the same rank\");\n+\n+            for (int d = 0; d < firstShape.Length; d++)\n+            {\n+                if (d != axis && shape[d] != firstShape[d])\n+                    throw new ArgumentException(\n+                        $\"Shape mismatch at dimension {d}: {shape[d]} vs {firstShape[d]}\");\n+            }\n+        }\n+\n+        // Compute output shape\n+        int[] outputShape = (int[])firstShape.Clone();\n+        for (int i = 1; i < nodes.Count; i++)\n+        {\n+            outputShape[axis] += nodes[i].Value.Shape[axis];\n+        }\n+\n+        // Perform concatenation (handle 2D case for simplicity)\n+        Tensor<T> result;\n+        if (firstShape.Length == 2 && axis == 1)\n+        {\n+            // Concatenate along columns (features)\n+            int rows = firstShape[0];\n+            int totalCols = outputShape[1];\n+            result = new Tensor<T>(new int[] { rows, totalCols });\n+\n+            int colOffset = 0;\n+            foreach (var node in nodes)\n+            {\n+                int cols = node.Value.Shape[1];\n+                for (int r = 0; r < rows; r++)\n+                {\n+                    for (int c = 0; c < cols; c++)\n+                    {\n+                        result[r, colOffset + c] = node.Value[r, c];\n+                    }\n+                }\n+                colOffset += cols;\n+            }\n+        }\n+        else if (firstShape.Length == 2 && axis == 0)\n+        {\n+            // Concatenate along rows (batch)\n+            int cols = firstShape[1];\n+            int totalRows = outputShape[0];\n+            result = new Tensor<T>(new int[] { totalRows, cols });\n+\n+            int rowOffset = 0;\n+            foreach (var node in nodes)\n+            {\n+                int rows = node.Value.Shape[0];\n+                for (int r = 0; r < rows; r++)\n+                {\n+                    for (int c = 0; c < cols; c++)\n+                    {\n+                        result[rowOffset + r, c] = node.Value[r, c];\n+                    }\n+                }\n+                rowOffset += rows;\n+            }\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Concat is currently only implemented for 2D tensors. \" +\n+                $\"Got shape=[{string.Join(\", \", firstShape)}]\");\n+        }\n+\n+        // Store sizes for gradient splitting\n+        var sizes = nodes.Select(n => n.Value.Shape[axis]).ToList();\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            // Split gradient along concat axis and distribute to inputs\n+            if (firstShape.Length == 2 && axis == 1)\n+            {\n+                int rows = firstShape[0];\n+                int colOffset = 0;\n+                for (int i = 0; i < nodes.Count; i++)\n+                {\n+                    if (!nodes[i].RequiresGradient)\n+                    {\n+                        colOffset += sizes[i];\n+                        continue;\n+                    }\n+\n+                    int cols = sizes[i];\n+                    var gradPart = new Tensor<T>(new int[] { rows, cols });\n+\n+                    for (int r = 0; r < rows; r++)\n+                    {\n+                        for (int c = 0; c < cols; c++)\n+                        {\n+                            gradPart[r, c] = gradient[r, colOffset + c];\n+                        }\n+                    }\n+\n+                    if (nodes[i].Gradient == null)\n+                        nodes[i].Gradient = gradPart;\n+                    else\n+                        nodes[i].Gradient = nodes[i].Gradient.Add(gradPart);\n+\n+                    colOffset += cols;\n+                }\n+            }\n+            else if (firstShape.Length == 2 && axis == 0)\n+            {\n+                int cols = firstShape[1];\n+                int rowOffset = 0;\n+                for (int i = 0; i < nodes.Count; i++)\n+                {\n+                    if (!nodes[i].RequiresGradient)\n+                    {\n+                        rowOffset += sizes[i];\n+                        continue;\n+                    }\n+\n+                    int rows = sizes[i];\n+                    var gradPart = new Tensor<T>(new int[] { rows, cols });\n+\n+                    for (int r = 0; r < rows; r++)\n+                    {\n+                        for (int c = 0; c < cols; c++)\n+                        {\n+                            gradPart[r, c] = gradient[rowOffset + r, c];\n+                        }\n+                    }\n+\n+                    if (nodes[i].Gradient == null)\n+                        nodes[i].Gradient = gradPart;\n+                    else\n+                        nodes[i].Gradient = nodes[i].Gradient.Add(gradPart);\n+\n+                    rowOffset += rows;\n+                }\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: nodes.Any(n => n.RequiresGradient),\n+            parents: nodes,\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Pads a tensor with a constant value along specified dimensions.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"padWidth\">Padding width for each dimension as (before, after) pairs.</param>\n+    /// <param name=\"value\">The value to use for padding. Default is zero.</param>\n+    /// <returns>A new computation node containing the padded result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method adds padding around the tensor.\n+    /// The backward function simply crops the gradient back to the original size (gradients for padding are zero).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Pad adds extra elements around a tensor.\n+    ///\n+    /// For padding:\n+    /// - The forward pass adds border elements with a constant value\n+    /// - The backward pass removes those border gradients (they don't affect the original tensor)\n+    /// - Think of it like adding margins to an image\n+    ///\n+    /// Used in:\n+    /// - Convolutional layers (to maintain spatial dimensions)\n+    /// - Handling variable-length sequences\n+    /// - Data augmentation\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Pad(ComputationNode<T> a, int[,] padWidth, T? value = default)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var padValue = value ?? numOps.Zero;\n+        var shape = a.Value.Shape;\n+\n+        // Validate padWidth dimensions\n+        if (padWidth.GetLength(0) != shape.Length)\n+            throw new ArgumentException(\"padWidth must have same number of dimensions as input tensor\");\n+\n+        // Compute output shape\n+        var outputShape = new int[shape.Length];\n+        for (int d = 0; d < shape.Length; d++)\n+        {\n+            outputShape[d] = shape[d] + padWidth[d, 0] + padWidth[d, 1];\n+        }\n+\n+        // Handle 2D case\n+        if (shape.Length == 2)\n+        {\n+            int inputRows = shape[0];\n+            int inputCols = shape[1];\n+            int padTop = padWidth[0, 0];\n+            int padBottom = padWidth[0, 1];\n+            int padLeft = padWidth[1, 0];\n+            int padRight = padWidth[1, 1];\n+\n+            var result = new Tensor<T>(outputShape);\n+\n+            // Initialize with pad value\n+            for (int i = 0; i < result.Length; i++)\n+            {\n+                result[i] = padValue;\n+            }\n+\n+            // Copy input data to center\n+            for (int r = 0; r < inputRows; r++)\n+            {\n+                for (int c = 0; c < inputCols; c++)\n+                {\n+                    result[padTop + r, padLeft + c] = a.Value[r, c];\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (a.RequiresGradient)\n+                {\n+                    // Extract gradient for original (unpadded) region\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int r = 0; r < inputRows; r++)\n+                    {\n+                        for (int c = 0; c < inputCols; c++)\n+                        {\n+                            gradA[r, c] = gradient[padTop + r, padLeft + c];\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient,\n+                parents: new List<ComputationNode<T>> { a },\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Pad is currently only implemented for 2D tensors. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}]\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs 2D max pooling on a 4D tensor (batch, channels, height, width).\n+    /// </summary>\n+    /// <param name=\"a\">The input node with shape [batch, channels, height, width].</param>\n+    /// <param name=\"poolSize\">The size of the pooling window [poolH, poolW].</param>\n+    /// <param name=\"strides\">The stride for the pooling operation [strideH, strideW]. If null, uses poolSize.</param>\n+    /// <returns>A new computation node containing the max pooled result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method performs max pooling over 2D spatial dimensions.\n+    /// During forward pass, it tracks which element was the max for routing gradients during backward pass.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> MaxPool downsamples by taking the maximum value in each window.\n+    ///\n+    /// For max pooling:\n+    /// - The forward pass slides a window and takes the max value in each position\n+    /// - This reduces spatial dimensions (downsampling)\n+    /// - The backward pass routes gradients only to the positions that were max\n+    /// - Other positions get zero gradient (they didn't contribute to the output)\n+    ///\n+    /// Used in:\n+    /// - CNNs for translation invariance\n+    /// - Reducing spatial resolution\n+    /// - Building hierarchical features\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> MaxPool2D(\n+        ComputationNode<T> a,\n+        int[] poolSize,\n+        int[]? strides = null)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        if (shape.Length != 4)\n+            throw new ArgumentException(\"MaxPool2D requires 4D input [batch, channels, height, width]\");\n+\n+        strides ??= poolSize;\n+\n+        int batch = shape[0];\n+        int channels = shape[1];\n+        int inH = shape[2];\n+        int inW = shape[3];\n+        int poolH = poolSize[0];\n+        int poolW = poolSize[1];\n+        int strideH = strides[0];\n+        int strideW = strides[1];\n+\n+        int outH = (inH - poolH) / strideH + 1;\n+        int outW = (inW - poolW) / strideW + 1;\n+\n+        var result = new Tensor<T>(new int[] { batch, channels, outH, outW });\n+        // Store max positions for backprop\n+        var maxPositions = new int[batch, channels, outH, outW, 2]; // [h_offset, w_offset]\n+\n+        // Forward pass: compute max pooling and track positions\n+        for (int b = 0; b < batch; b++)\n+        {\n+            for (int c = 0; c < channels; c++)\n+            {\n+                for (int oh = 0; oh < outH; oh++)\n+                {\n+                    for (int ow = 0; ow < outW; ow++)\n+                    {\n+                        int hStart = oh * strideH;\n+                        int wStart = ow * strideW;\n+\n+                        var maxVal = a.Value[b * channels * inH * inW +\n+                                           c * inH * inW +\n+                                           hStart * inW +\n+                                           wStart];\n+                        int maxHOffset = 0;\n+                        int maxWOffset = 0;\n+\n+                        // Find max in pooling window\n+                        for (int ph = 0; ph < poolH; ph++)\n+                        {\n+                            for (int pw = 0; pw < poolW; pw++)\n+                            {\n+                                int h = hStart + ph;\n+                                int w = wStart + pw;\n+                                if (h < inH && w < inW)\n+                                {\n+                                    var val = a.Value[b * channels * inH * inW +\n+                                                     c * inH * inW +\n+                                                     h * inW +\n+                                                     w];\n+                                    if (numOps.GreaterThan(val, maxVal))\n+                                    {\n+                                        maxVal = val;\n+                                        maxHOffset = ph;\n+                                        maxWOffset = pw;\n+                                    }\n+                                }\n+                            }\n+                        }\n+\n+                        result[b, c, oh, ow] = maxVal;\n+                        maxPositions[b, c, oh, ow, 0] = maxHOffset;\n+                        maxPositions[b, c, oh, ow, 1] = maxWOffset;\n+                    }\n+                }\n+            }\n+        }\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            if (a.RequiresGradient)\n+            {\n+                var gradA = new Tensor<T>(shape);\n+\n+                // Route gradients to max positions\n+                for (int b = 0; b < batch; b++)\n+                {\n+                    for (int c = 0; c < channels; c++)\n+                    {\n+                        for (int oh = 0; oh < outH; oh++)\n+                        {\n+                            for (int ow = 0; ow < outW; ow++)\n+                            {\n+                                int hStart = oh * strideH;\n+                                int wStart = ow * strideW;\n+                                int maxHOffset = maxPositions[b, c, oh, ow, 0];\n+                                int maxWOffset = maxPositions[b, c, oh, ow, 1];\n+\n+                                int maxH = hStart + maxHOffset;\n+                                int maxW = wStart + maxWOffset;\n+\n+                                int gradIdx = b * channels * inH * inW +\n+                                             c * inH * inW +\n+                                            maxH * inW +\n+                                             maxW;\n+\n+                                gradA[gradIdx] = numOps.Add(gradA[gradIdx], gradient[b, c, oh, ow]);\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (a.Gradient == null)\n+                    a.Gradient = gradA;\n+                else\n+                    a.Gradient = a.Gradient.Add(gradA);\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: a.RequiresGradient,\n+            parents: new List<ComputationNode<T>> { a },\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Performs 2D average pooling on a 4D tensor (batch, channels, height, width).\n+    /// </summary>\n+    /// <param name=\"a\">The input node with shape [batch, channels, height, width].</param>\n+    /// <param name=\"poolSize\">The size of the pooling window [poolH, poolW].</param>\n+    /// <param name=\"strides\">The stride for the pooling operation [strideH, strideW]. If null, uses poolSize.</param>\n+    /// <returns>A new computation node containing the average pooled result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method performs average pooling over 2D spatial dimensions.\n+    /// The backward function distributes gradients equally across the pooling window.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> AvgPool downsamples by taking the average value in each window.\n+    ///\n+    /// For average pooling:\n+    /// - The forward pass slides a window and computes the average\n+    /// - This smoothly reduces spatial dimensions\n+    /// - The backward pass distributes gradients equally to all elements in the window\n+    /// - Each element gets gradient / pool_area\n+    ///\n+    /// Used in:\n+    /// - CNNs for smoother downsampling than max pooling\n+    /// - Global average pooling (replacing fully connected layers)\n+    /// - Reducing overfitting\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> AvgPool2D(\n+        ComputationNode<T> a,\n+        int[] poolSize,\n+        int[]? strides = null)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        if (shape.Length != 4)\n+            throw new ArgumentException(\"AvgPool2D requires 4D input [batch, channels, height, width]\");\n+\n+        strides ??= poolSize;\n+\n+        int batch = shape[0];\n+        int channels = shape[1];\n+        int inH = shape[2];\n+        int inW = shape[3];\n+        int poolH = poolSize[0];\n+        int poolW = poolSize[1];\n+        int strideH = strides[0];\n+        int strideW = strides[1];\n+\n+        int outH = (inH - poolH) / strideH + 1;\n+        int outW = (inW - poolW) / strideW + 1;\n+\n+        var result = new Tensor<T>(new int[] { batch, channels, outH, outW });\n+        var poolArea = numOps.FromDouble(poolH * poolW);\n+\n+        // Forward pass: compute average pooling\n+        for (int b = 0; b < batch; b++)\n+        {\n+            for (int c = 0; c < channels; c++)\n+            {\n+                for (int oh = 0; oh < outH; oh++)\n+                {\n+                    for (int ow = 0; ow < outW; ow++)\n+                    {\n+                        int hStart = oh * strideH;\n+                        int wStart = ow * strideW;\n+\n+                        var sum = numOps.Zero;\n+\n+                        // Sum values in pooling window\n+                        for (int ph = 0; ph < poolH; ph++)\n+                        {\n+                            for (int pw = 0; pw < poolW; pw++)\n+                            {\n+                                int h = hStart + ph;\n+                                int w = wStart + pw;\n+                                if (h < inH && w < inW)\n+                                {\n+                                    sum = numOps.Add(sum, a.Value[b, c, h, w]);\n+                                }\n+                            }\n+                        }\n+\n+                        result[b, c, oh, ow] = numOps.Divide(sum, poolArea);\n+                    }\n+                }\n+            }\n+        }\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            if (a.RequiresGradient)\n+            {\n+                var gradA = new Tensor<T>(shape);\n+\n+                // Distribute gradients equally across pooling windows\n+                for (int b = 0; b < batch; b++)\n+                {\n+                    for (int c = 0; c < channels; c++)\n+                    {\n+                        for (int oh = 0; oh < outH; oh++)\n+                        {\n+                            for (int ow = 0; ow < outW; ow++)\n+                            {\n+                                int hStart = oh * strideH;\n+                                int wStart = ow * strideW;\n+\n+                                var gradValue = numOps.Divide(gradient[b, c, oh, ow], poolArea);\n+\n+                                // Distribute to all elements in window\n+                                for (int ph = 0; ph < poolH; ph++)\n+                                {\n+                                    for (int pw = 0; pw < poolW; pw++)\n+                                    {\n+                                        int h = hStart + ph;\n+                                        int w = wStart + pw;\n+                                        if (h < inH && w < inW)\n+                                        {\n+                                            gradA[b, c, h, w] = numOps.Add(gradA[b, c, h, w], gradValue);\n+                                        }\n+                                    }\n+                                }\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (a.Gradient == null)\n+                    a.Gradient = gradA;\n+                else\n+                    a.Gradient = a.Gradient.Add(gradA);\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: a.RequiresGradient,\n+            parents: new List<ComputationNode<T>> { a },\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Applies layer normalization to a computation node.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"normalizedShape\">The shape over which to normalize (typically the feature dimensions).</param>\n+    /// <param name=\"gamma\">Optional scale parameter (learnable). If null, uses ones.</param>\n+    /// <param name=\"beta\">Optional shift parameter (learnable). If null, uses zeros.</param>\n+    /// <param name=\"epsilon\">Small constant for numerical stability. Default is 1e-5.</param>\n+    /// <returns>A new computation node containing the layer normalized result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Layer normalization normalizes inputs across the feature dimension for each sample independently.\n+    /// Formula: y = gamma * (x - mean) / sqrt(variance + epsilon) + beta\n+    /// Unlike batch normalization, this doesn't depend on batch statistics.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> LayerNorm standardizes features for each sample independently.\n+    ///\n+    /// For layer normalization:\n+    /// - Computes mean and variance for each sample's features\n+    /// - Normalizes: (x - mean) / sqrt(variance)\n+    /// - Scales and shifts: result * gamma + beta\n+    /// - Works the same during training and inference (no batch dependency)\n+    ///\n+    /// Used in:\n+    /// - Transformers (critical component)\n+    /// - RNNs (stabilizes training)\n+    /// - Any architecture needing sample-independent normalization\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> LayerNorm(\n+        ComputationNode<T> a,\n+        int[] normalizedShape,\n+        ComputationNode<T>? gamma = null,\n+        ComputationNode<T>? beta = null,\n+        double epsilon = 1e-5)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+        var eps = numOps.FromDouble(epsilon);\n+\n+        // For 2D input [batch, features], normalize over features\n+        if (shape.Length == 2 && normalizedShape.Length == 1 && normalizedShape[0] == shape[1])\n+        {\n+            int batchSize = shape[0];\n+            int features = shape[1];\n+\n+            // Create default gamma (ones) and beta (zeros) if not provided\n+            if (gamma == null)\n+            {\n+                var gammaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    gammaTensor[i] = numOps.One;\n+                gamma = Variable(gammaTensor, requiresGradient: false);\n+            }\n+\n+            if (beta == null)\n+            {\n+                var betaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    betaTensor[i] = numOps.Zero;\n+                beta = Variable(betaTensor, requiresGradient: false);\n+            }\n+\n+            var result = new Tensor<T>(shape);\n+            var means = new T[batchSize];\n+            var variances = new T[batchSize];\n+            var normalized = new Tensor<T>(shape);\n+\n+            // Forward pass: compute mean and variance per sample\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                // Compute mean\n+                var sum = numOps.Zero;\n+                for (int f = 0; f < features; f++)\n+                {\n+                    sum = numOps.Add(sum, a.Value[b, f]);\n+                }\n+                means[b] = numOps.Divide(sum, numOps.FromDouble(features));\n+\n+                // Compute variance\n+                var varSum = numOps.Zero;\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var diff = numOps.Subtract(a.Value[b, f], means[b]);\n+                    varSum = numOps.Add(varSum, numOps.Multiply(diff, diff));\n+                }\n+                variances[b] = numOps.Divide(varSum, numOps.FromDouble(features));\n+\n+                // Normalize and scale\n+                var std = numOps.Sqrt(numOps.Add(variances[b], eps));\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var norm = numOps.Divide(\n+                        numOps.Subtract(a.Value[b, f], means[b]),\n+                        std);\n+                    normalized[b, f] = norm;\n+                    result[b, f] = numOps.Add(\n+                        numOps.Multiply(norm, gamma.Value[f]),\n+                        beta.Value[f]);\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                // Gradients for gamma and beta\n+                if (gamma.RequiresGradient)\n+                {\n+                    var gradGamma = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum,\n+                                numOps.Multiply(gradient[b, f], normalized[b, f]));\n+                        }\n+                        gradGamma[f] = sum;\n+                    }\n+\n+                    if (gamma.Gradient == null)\n+                        gamma.Gradient = gradGamma;\n+                    else\n+                        gamma.Gradient = gamma.Gradient.Add(gradGamma);\n+                }\n+\n+                if (beta.RequiresGradient)\n+                {\n+                    var gradBeta = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum, gradient[b, f]);\n+                        }\n+                        gradBeta[f] = sum;\n+                    }\n+\n+                    if (beta.Gradient == null)\n+                        beta.Gradient = gradBeta;\n+                    else\n+                        beta.Gradient = beta.Gradient.Add(gradBeta);\n+                }\n+\n+                // Gradient for input\n+                if (a.RequiresGradient)\n+                {\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        var std = numOps.Sqrt(numOps.Add(variances[b], eps));\n+                        var invStd = numOps.Divide(numOps.One, std);\n+\n+                        // Compute gradient components\n+                        var gradNormSum = numOps.Zero;\n+                        var gradNormDotNorm = numOps.Zero;\n+\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            var gradNorm = numOps.Multiply(gradient[b, f], gamma.Value[f]);\n+                            gradNormSum = numOps.Add(gradNormSum, gradNorm);\n+                            gradNormDotNorm = numOps.Add(gradNormDotNorm,\n+                                numOps.Multiply(gradNorm, normalized[b, f]));\n+                        }\n+\n+                        // Apply gradient formula\n+                        var featuresT = numOps.FromDouble(features);\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            var gradNorm = numOps.Multiply(gradient[b, f], gamma.Value[f]);\n+\n+                            var term1 = gradNorm;\n+                            var term2 = numOps.Divide(gradNormSum, featuresT);\n+                            var term3 = numOps.Divide(\n+                                numOps.Multiply(normalized[b, f], gradNormDotNorm),\n+                                featuresT);\n+\n+                            var grad = numOps.Multiply(\n+                                numOps.Subtract(numOps.Subtract(term1, term2), term3),\n+                                invStd);\n+\n+                            gradA[b, f] = grad;\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var parents = new List<ComputationNode<T>> { a };\n+            if (gamma != null) parents.Add(gamma);\n+            if (beta != null) parents.Add(beta);\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient || (gamma?.RequiresGradient ?? false) || (beta?.RequiresGradient ?? false),\n+                parents: parents,\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"LayerNorm is currently only implemented for 2D tensors normalizing over last dimension. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}], normalizedShape=[{string.Join(\", \", normalizedShape)}]\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Applies batch normalization to a computation node.\n+    /// </summary>\n+    /// <param name=\"a\">The input node with shape [batch, features].</param>\n+    /// <param name=\"gamma\">Optional scale parameter (learnable). If null, uses ones.</param>\n+    /// <param name=\"beta\">Optional shift parameter (learnable). If null, uses zeros.</param>\n+    /// <param name=\"runningMean\">Running mean for inference (not updated during this operation).</param>\n+    /// <param name=\"runningVar\">Running variance for inference (not updated during this operation).</param>\n+    /// <param name=\"training\">Whether in training mode (uses batch statistics) or inference mode (uses running statistics).</param>\n+    /// <param name=\"epsilon\">Small constant for numerical stability. Default is 1e-5.</param>\n+    /// <returns>A new computation node containing the batch normalized result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Batch normalization normalizes inputs across the batch dimension.\n+    /// During training: Uses batch statistics (mean and variance computed from current batch).\n+    /// During inference: Uses running statistics (accumulated during training).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> BatchNorm standardizes features across the batch.\n+    ///\n+    /// For batch normalization:\n+    /// - Training mode: Uses current batch's mean and variance\n+    /// - Inference mode: Uses running mean/variance from training\n+    /// - Normalizes: (x - mean) / sqrt(variance)\n+    /// - Scales and shifts: result * gamma + beta\n+    ///\n+    /// Benefits:\n+    /// - Stabilizes training (reduces internal covariate shift)\n+    /// - Allows higher learning rates\n+    /// - Acts as regularization\n+    ///\n+    /// Used in:\n+    /// - CNNs (after convolutional layers)\n+    /// - Deep feedforward networks\n+    /// - GANs and many other architectures\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> BatchNorm(\n+        ComputationNode<T> a,\n+        ComputationNode<T>? gamma = null,\n+        ComputationNode<T>? beta = null,\n+        Tensor<T>? runningMean = null,\n+        Tensor<T>? runningVar = null,\n+        bool training = true,\n+        double epsilon = 1e-5)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+        var eps = numOps.FromDouble(epsilon);\n+\n+        // Handle 2D case [batch, features]\n+        if (shape.Length == 2)\n+        {\n+            int batchSize = shape[0];\n+            int features = shape[1];\n+\n+            // Create default gamma and beta if not provided\n+            if (gamma == null)\n+            {\n+                var gammaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    gammaTensor[i] = numOps.One;\n+                gamma = Variable(gammaTensor, requiresGradient: false);\n+            }\n+\n+            if (beta == null)\n+            {\n+                var betaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    betaTensor[i] = numOps.Zero;\n+                beta = Variable(betaTensor, requiresGradient: false);\n+            }\n+\n+            var result = new Tensor<T>(shape);\n+            T[] batchMean;\n+            T[] batchVar;\n+            var normalized = new Tensor<T>(shape);\n+\n+            if (training)\n+            {\n+                // Compute batch statistics\n+                batchMean = new T[features];\n+                batchVar = new T[features];\n+\n+                // Compute mean per feature\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var sum = numOps.Zero;\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        sum = numOps.Add(sum, a.Value[b, f]);\n+                    }\n+                    batchMean[f] = numOps.Divide(sum, numOps.FromDouble(batchSize));\n+                }\n+\n+                // Compute variance per feature\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var varSum = numOps.Zero;\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        var diff = numOps.Subtract(a.Value[b, f], batchMean[f]);\n+                        varSum = numOps.Add(varSum, numOps.Multiply(diff, diff));\n+                    }\n+                    batchVar[f] = numOps.Divide(varSum, numOps.FromDouble(batchSize));\n+                }\n+            }\n+            else\n+            {\n+                // Use running statistics for inference\n+                if (runningMean == null || runningVar == null)\n+                    throw new ArgumentException(\"Running statistics required for inference mode\");\n+\n+                batchMean = new T[features];\n+                batchVar = new T[features];\n+                for (int f = 0; f < features; f++)\n+                {\n+                    batchMean[f] = runningMean[f];\n+                    batchVar[f] = runningVar[f];\n+                }\n+            }\n+\n+            // Normalize and scale\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var std = numOps.Sqrt(numOps.Add(batchVar[f], eps));\n+                    var norm = numOps.Divide(\n+                        numOps.Subtract(a.Value[b, f], batchMean[f]),\n+                        std);\n+                    normalized[b, f] = norm;\n+                    result[b, f] = numOps.Add(\n+                        numOps.Multiply(norm, gamma.Value[f]),\n+                        beta.Value[f]);\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (!training)\n+                {\n+                    // Inference mode: simpler gradient (no batch statistics gradient)\n+                    if (a.RequiresGradient)\n+                    {\n+                        var gradA = new Tensor<T>(shape);\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            for (int f = 0; f < features; f++)\n+                            {\n+                                var std = numOps.Sqrt(numOps.Add(batchVar[f], eps));\n+                                var invStd = numOps.Divide(numOps.One, std);\n+                                gradA[b, f] = numOps.Multiply(\n+                                    numOps.Multiply(gradient[b, f], gamma.Value[f]),\n+                                    invStd);\n+                            }\n+                        }\n+\n+                        if (a.Gradient == null)\n+                            a.Gradient = gradA;\n+                        else\n+                            a.Gradient = a.Gradient.Add(gradA);\n+                    }\n+                    return;\n+                }\n+\n+                // Training mode: full gradient computation\n+                // Gradients for gamma and beta\n+                if (gamma.RequiresGradient)\n+                {\n+                    var gradGamma = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum,\n+                                numOps.Multiply(gradient[b, f], normalized[b, f]));\n+                        }\n+                        gradGamma[f] = sum;\n+                    }\n+\n+                    if (gamma.Gradient == null)\n+                        gamma.Gradient = gradGamma;\n+                    else\n+                        gamma.Gradient = gamma.Gradient.Add(gradGamma);\n+                }\n+\n+                if (beta.RequiresGradient)\n+                {\n+                    var gradBeta = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum, gradient[b, f]);\n+                        }\n+                        gradBeta[f] = sum;\n+                    }\n+\n+                    if (beta.Gradient == null)\n+                        beta.Gradient = gradBeta;\n+                    else\n+                        beta.Gradient = beta.Gradient.Add(gradBeta);"}]}},{"id":"PRRT_kwDOKSXUF85hck-u","isResolved":false,"isOutdated":false,"path":"src/Autodiff/TensorOperations.cs","line":2427,"originalLine":2427,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzD_F","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement write to the same variable - consider using '?' to express intent better.\n```suggestion\n                    a.Gradient = (a.Gradient?.Add(gradA)) ?? gradA;\n```","createdAt":"2025-11-11T07:53:26Z","path":"src/Autodiff/TensorOperations.cs","line":2427,"originalLine":2427,"diffHunk":"@@ -1219,4 +1219,1769 @@ void BackwardFunction(Tensor<T> gradient)\n \n         return node;\n     }\n+\n+    /// <summary>\n+    /// Computes the softmax function for a computation node along a specified axis.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"axis\">The axis along which to compute softmax. Default is -1 (last axis).</param>\n+    /// <returns>A new computation node containing the softmax result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method computes softmax (σ(x_i) = exp(x_i) / Σexp(x_j)) along the specified axis.\n+    /// Uses numerical stability trick: subtract max before exponentiating.\n+    /// The backward function uses: ∂softmax/∂x = softmax(x) * (grad - Σ(grad * softmax(x))).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Softmax converts a vector of numbers into probabilities.\n+    ///\n+    /// For softmax:\n+    /// - The forward pass exponentiates each element, then normalizes so they sum to 1\n+    /// - The result is a probability distribution (all values between 0 and 1, summing to 1)\n+    /// - The backward pass is complex but efficient: uses the Jacobian of softmax\n+    ///\n+    /// Softmax is crucial for:\n+    /// - Multi-class classification (final layer outputs)\n+    /// - Attention mechanisms (computing attention weights)\n+    /// - Anywhere you need to convert scores to probabilities\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Softmax(ComputationNode<T> a, int axis = -1)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        // Normalize axis to positive index\n+        if (axis < 0)\n+            axis = shape.Length + axis;\n+\n+        // For simplicity, handle 2D case (batch, features) with axis=-1\n+        if (shape.Length == 2 && axis == 1)\n+        {\n+            int batchSize = shape[0];\n+            int features = shape[1];\n+            var result = new Tensor<T>(shape);\n+\n+            // Compute softmax for each row\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                // Find max for numerical stability\n+                var maxVal = a.Value[b, 0];\n+                for (int f = 1; f < features; f++)\n+                {\n+                    if (numOps.GreaterThan(a.Value[b, f], maxVal))\n+                        maxVal = a.Value[b, f];\n+                }\n+\n+                // Compute exp(x - max) and sum\n+                var expSum = numOps.Zero;\n+                var expValues = new T[features];\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var shifted = numOps.Subtract(a.Value[b, f], maxVal);\n+                    expValues[f] = numOps.Exp(shifted);\n+                    expSum = numOps.Add(expSum, expValues[f]);\n+                }\n+\n+                // Normalize\n+                for (int f = 0; f < features; f++)\n+                {\n+                    result[b, f] = numOps.Divide(expValues[f], expSum);\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (a.RequiresGradient)\n+                {\n+                    // ∂softmax/∂x_i = softmax_i * (∂L/∂y_i - Σ_j(∂L/∂y_j * softmax_j))\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        // Compute sum of (gradient * softmax)\n+                        var dotProduct = numOps.Zero;\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            dotProduct = numOps.Add(dotProduct,\n+                                numOps.Multiply(gradient[b, f], result[b, f]));\n+                        }\n+\n+                        // Compute gradient for each element\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            var gradMinusDot = numOps.Subtract(gradient[b, f], dotProduct);\n+                            gradA[b, f] = numOps.Multiply(result[b, f], gradMinusDot);\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient,\n+                parents: new List<ComputationNode<T>> { a },\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Softmax is currently only implemented for 2D tensors along axis=-1. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}], axis={axis}\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Concatenates multiple computation nodes along a specified axis.\n+    /// </summary>\n+    /// <param name=\"nodes\">The list of nodes to concatenate.</param>\n+    /// <param name=\"axis\">The axis along which to concatenate. Default is 0.</param>\n+    /// <returns>A new computation node containing the concatenated result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method concatenates tensors along the specified axis.\n+    /// All tensors must have the same shape except along the concatenation axis.\n+    /// The backward function splits the gradient and sends each portion to the corresponding input.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Concat stacks tensors together along a dimension.\n+    ///\n+    /// For concatenation:\n+    /// - The forward pass combines multiple tensors into one larger tensor\n+    /// - The backward pass splits the gradient back to each input\n+    /// - Think of it like gluing arrays together end-to-end\n+    ///\n+    /// Used in:\n+    /// - Skip connections (concatenating features from different layers)\n+    /// - Multi-input architectures\n+    /// - Feature fusion in neural networks\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Concat(List<ComputationNode<T>> nodes, int axis = 0)\n+    {\n+        if (nodes.Count == 0)\n+            throw new ArgumentException(\"Cannot concatenate empty list of nodes\");\n+\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var firstShape = nodes[0].Value.Shape;\n+\n+        // Normalize axis\n+        if (axis < 0)\n+            axis = firstShape.Length + axis;\n+\n+        // Validate shapes match except on concat axis\n+        for (int i = 1; i < nodes.Count; i++)\n+        {\n+            var shape = nodes[i].Value.Shape;\n+            if (shape.Length != firstShape.Length)\n+                throw new ArgumentException(\"All tensors must have the same rank\");\n+\n+            for (int d = 0; d < firstShape.Length; d++)\n+            {\n+                if (d != axis && shape[d] != firstShape[d])\n+                    throw new ArgumentException(\n+                        $\"Shape mismatch at dimension {d}: {shape[d]} vs {firstShape[d]}\");\n+            }\n+        }\n+\n+        // Compute output shape\n+        int[] outputShape = (int[])firstShape.Clone();\n+        for (int i = 1; i < nodes.Count; i++)\n+        {\n+            outputShape[axis] += nodes[i].Value.Shape[axis];\n+        }\n+\n+        // Perform concatenation (handle 2D case for simplicity)\n+        Tensor<T> result;\n+        if (firstShape.Length == 2 && axis == 1)\n+        {\n+            // Concatenate along columns (features)\n+            int rows = firstShape[0];\n+            int totalCols = outputShape[1];\n+            result = new Tensor<T>(new int[] { rows, totalCols });\n+\n+            int colOffset = 0;\n+            foreach (var node in nodes)\n+            {\n+                int cols = node.Value.Shape[1];\n+                for (int r = 0; r < rows; r++)\n+                {\n+                    for (int c = 0; c < cols; c++)\n+                    {\n+                        result[r, colOffset + c] = node.Value[r, c];\n+                    }\n+                }\n+                colOffset += cols;\n+            }\n+        }\n+        else if (firstShape.Length == 2 && axis == 0)\n+        {\n+            // Concatenate along rows (batch)\n+            int cols = firstShape[1];\n+            int totalRows = outputShape[0];\n+            result = new Tensor<T>(new int[] { totalRows, cols });\n+\n+            int rowOffset = 0;\n+            foreach (var node in nodes)\n+            {\n+                int rows = node.Value.Shape[0];\n+                for (int r = 0; r < rows; r++)\n+                {\n+                    for (int c = 0; c < cols; c++)\n+                    {\n+                        result[rowOffset + r, c] = node.Value[r, c];\n+                    }\n+                }\n+                rowOffset += rows;\n+            }\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Concat is currently only implemented for 2D tensors. \" +\n+                $\"Got shape=[{string.Join(\", \", firstShape)}]\");\n+        }\n+\n+        // Store sizes for gradient splitting\n+        var sizes = nodes.Select(n => n.Value.Shape[axis]).ToList();\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            // Split gradient along concat axis and distribute to inputs\n+            if (firstShape.Length == 2 && axis == 1)\n+            {\n+                int rows = firstShape[0];\n+                int colOffset = 0;\n+                for (int i = 0; i < nodes.Count; i++)\n+                {\n+                    if (!nodes[i].RequiresGradient)\n+                    {\n+                        colOffset += sizes[i];\n+                        continue;\n+                    }\n+\n+                    int cols = sizes[i];\n+                    var gradPart = new Tensor<T>(new int[] { rows, cols });\n+\n+                    for (int r = 0; r < rows; r++)\n+                    {\n+                        for (int c = 0; c < cols; c++)\n+                        {\n+                            gradPart[r, c] = gradient[r, colOffset + c];\n+                        }\n+                    }\n+\n+                    if (nodes[i].Gradient == null)\n+                        nodes[i].Gradient = gradPart;\n+                    else\n+                        nodes[i].Gradient = nodes[i].Gradient.Add(gradPart);\n+\n+                    colOffset += cols;\n+                }\n+            }\n+            else if (firstShape.Length == 2 && axis == 0)\n+            {\n+                int cols = firstShape[1];\n+                int rowOffset = 0;\n+                for (int i = 0; i < nodes.Count; i++)\n+                {\n+                    if (!nodes[i].RequiresGradient)\n+                    {\n+                        rowOffset += sizes[i];\n+                        continue;\n+                    }\n+\n+                    int rows = sizes[i];\n+                    var gradPart = new Tensor<T>(new int[] { rows, cols });\n+\n+                    for (int r = 0; r < rows; r++)\n+                    {\n+                        for (int c = 0; c < cols; c++)\n+                        {\n+                            gradPart[r, c] = gradient[rowOffset + r, c];\n+                        }\n+                    }\n+\n+                    if (nodes[i].Gradient == null)\n+                        nodes[i].Gradient = gradPart;\n+                    else\n+                        nodes[i].Gradient = nodes[i].Gradient.Add(gradPart);\n+\n+                    rowOffset += rows;\n+                }\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: nodes.Any(n => n.RequiresGradient),\n+            parents: nodes,\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Pads a tensor with a constant value along specified dimensions.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"padWidth\">Padding width for each dimension as (before, after) pairs.</param>\n+    /// <param name=\"value\">The value to use for padding. Default is zero.</param>\n+    /// <returns>A new computation node containing the padded result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method adds padding around the tensor.\n+    /// The backward function simply crops the gradient back to the original size (gradients for padding are zero).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Pad adds extra elements around a tensor.\n+    ///\n+    /// For padding:\n+    /// - The forward pass adds border elements with a constant value\n+    /// - The backward pass removes those border gradients (they don't affect the original tensor)\n+    /// - Think of it like adding margins to an image\n+    ///\n+    /// Used in:\n+    /// - Convolutional layers (to maintain spatial dimensions)\n+    /// - Handling variable-length sequences\n+    /// - Data augmentation\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Pad(ComputationNode<T> a, int[,] padWidth, T? value = default)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var padValue = value ?? numOps.Zero;\n+        var shape = a.Value.Shape;\n+\n+        // Validate padWidth dimensions\n+        if (padWidth.GetLength(0) != shape.Length)\n+            throw new ArgumentException(\"padWidth must have same number of dimensions as input tensor\");\n+\n+        // Compute output shape\n+        var outputShape = new int[shape.Length];\n+        for (int d = 0; d < shape.Length; d++)\n+        {\n+            outputShape[d] = shape[d] + padWidth[d, 0] + padWidth[d, 1];\n+        }\n+\n+        // Handle 2D case\n+        if (shape.Length == 2)\n+        {\n+            int inputRows = shape[0];\n+            int inputCols = shape[1];\n+            int padTop = padWidth[0, 0];\n+            int padBottom = padWidth[0, 1];\n+            int padLeft = padWidth[1, 0];\n+            int padRight = padWidth[1, 1];\n+\n+            var result = new Tensor<T>(outputShape);\n+\n+            // Initialize with pad value\n+            for (int i = 0; i < result.Length; i++)\n+            {\n+                result[i] = padValue;\n+            }\n+\n+            // Copy input data to center\n+            for (int r = 0; r < inputRows; r++)\n+            {\n+                for (int c = 0; c < inputCols; c++)\n+                {\n+                    result[padTop + r, padLeft + c] = a.Value[r, c];\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (a.RequiresGradient)\n+                {\n+                    // Extract gradient for original (unpadded) region\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int r = 0; r < inputRows; r++)\n+                    {\n+                        for (int c = 0; c < inputCols; c++)\n+                        {\n+                            gradA[r, c] = gradient[padTop + r, padLeft + c];\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient,\n+                parents: new List<ComputationNode<T>> { a },\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Pad is currently only implemented for 2D tensors. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}]\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs 2D max pooling on a 4D tensor (batch, channels, height, width).\n+    /// </summary>\n+    /// <param name=\"a\">The input node with shape [batch, channels, height, width].</param>\n+    /// <param name=\"poolSize\">The size of the pooling window [poolH, poolW].</param>\n+    /// <param name=\"strides\">The stride for the pooling operation [strideH, strideW]. If null, uses poolSize.</param>\n+    /// <returns>A new computation node containing the max pooled result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method performs max pooling over 2D spatial dimensions.\n+    /// During forward pass, it tracks which element was the max for routing gradients during backward pass.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> MaxPool downsamples by taking the maximum value in each window.\n+    ///\n+    /// For max pooling:\n+    /// - The forward pass slides a window and takes the max value in each position\n+    /// - This reduces spatial dimensions (downsampling)\n+    /// - The backward pass routes gradients only to the positions that were max\n+    /// - Other positions get zero gradient (they didn't contribute to the output)\n+    ///\n+    /// Used in:\n+    /// - CNNs for translation invariance\n+    /// - Reducing spatial resolution\n+    /// - Building hierarchical features\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> MaxPool2D(\n+        ComputationNode<T> a,\n+        int[] poolSize,\n+        int[]? strides = null)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        if (shape.Length != 4)\n+            throw new ArgumentException(\"MaxPool2D requires 4D input [batch, channels, height, width]\");\n+\n+        strides ??= poolSize;\n+\n+        int batch = shape[0];\n+        int channels = shape[1];\n+        int inH = shape[2];\n+        int inW = shape[3];\n+        int poolH = poolSize[0];\n+        int poolW = poolSize[1];\n+        int strideH = strides[0];\n+        int strideW = strides[1];\n+\n+        int outH = (inH - poolH) / strideH + 1;\n+        int outW = (inW - poolW) / strideW + 1;\n+\n+        var result = new Tensor<T>(new int[] { batch, channels, outH, outW });\n+        // Store max positions for backprop\n+        var maxPositions = new int[batch, channels, outH, outW, 2]; // [h_offset, w_offset]\n+\n+        // Forward pass: compute max pooling and track positions\n+        for (int b = 0; b < batch; b++)\n+        {\n+            for (int c = 0; c < channels; c++)\n+            {\n+                for (int oh = 0; oh < outH; oh++)\n+                {\n+                    for (int ow = 0; ow < outW; ow++)\n+                    {\n+                        int hStart = oh * strideH;\n+                        int wStart = ow * strideW;\n+\n+                        var maxVal = a.Value[b * channels * inH * inW +\n+                                           c * inH * inW +\n+                                           hStart * inW +\n+                                           wStart];\n+                        int maxHOffset = 0;\n+                        int maxWOffset = 0;\n+\n+                        // Find max in pooling window\n+                        for (int ph = 0; ph < poolH; ph++)\n+                        {\n+                            for (int pw = 0; pw < poolW; pw++)\n+                            {\n+                                int h = hStart + ph;\n+                                int w = wStart + pw;\n+                                if (h < inH && w < inW)\n+                                {\n+                                    var val = a.Value[b * channels * inH * inW +\n+                                                     c * inH * inW +\n+                                                     h * inW +\n+                                                     w];\n+                                    if (numOps.GreaterThan(val, maxVal))\n+                                    {\n+                                        maxVal = val;\n+                                        maxHOffset = ph;\n+                                        maxWOffset = pw;\n+                                    }\n+                                }\n+                            }\n+                        }\n+\n+                        result[b, c, oh, ow] = maxVal;\n+                        maxPositions[b, c, oh, ow, 0] = maxHOffset;\n+                        maxPositions[b, c, oh, ow, 1] = maxWOffset;\n+                    }\n+                }\n+            }\n+        }\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            if (a.RequiresGradient)\n+            {\n+                var gradA = new Tensor<T>(shape);\n+\n+                // Route gradients to max positions\n+                for (int b = 0; b < batch; b++)\n+                {\n+                    for (int c = 0; c < channels; c++)\n+                    {\n+                        for (int oh = 0; oh < outH; oh++)\n+                        {\n+                            for (int ow = 0; ow < outW; ow++)\n+                            {\n+                                int hStart = oh * strideH;\n+                                int wStart = ow * strideW;\n+                                int maxHOffset = maxPositions[b, c, oh, ow, 0];\n+                                int maxWOffset = maxPositions[b, c, oh, ow, 1];\n+\n+                                int maxH = hStart + maxHOffset;\n+                                int maxW = wStart + maxWOffset;\n+\n+                                int gradIdx = b * channels * inH * inW +\n+                                             c * inH * inW +\n+                                            maxH * inW +\n+                                             maxW;\n+\n+                                gradA[gradIdx] = numOps.Add(gradA[gradIdx], gradient[b, c, oh, ow]);\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (a.Gradient == null)\n+                    a.Gradient = gradA;\n+                else\n+                    a.Gradient = a.Gradient.Add(gradA);\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: a.RequiresGradient,\n+            parents: new List<ComputationNode<T>> { a },\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Performs 2D average pooling on a 4D tensor (batch, channels, height, width).\n+    /// </summary>\n+    /// <param name=\"a\">The input node with shape [batch, channels, height, width].</param>\n+    /// <param name=\"poolSize\">The size of the pooling window [poolH, poolW].</param>\n+    /// <param name=\"strides\">The stride for the pooling operation [strideH, strideW]. If null, uses poolSize.</param>\n+    /// <returns>A new computation node containing the average pooled result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method performs average pooling over 2D spatial dimensions.\n+    /// The backward function distributes gradients equally across the pooling window.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> AvgPool downsamples by taking the average value in each window.\n+    ///\n+    /// For average pooling:\n+    /// - The forward pass slides a window and computes the average\n+    /// - This smoothly reduces spatial dimensions\n+    /// - The backward pass distributes gradients equally to all elements in the window\n+    /// - Each element gets gradient / pool_area\n+    ///\n+    /// Used in:\n+    /// - CNNs for smoother downsampling than max pooling\n+    /// - Global average pooling (replacing fully connected layers)\n+    /// - Reducing overfitting\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> AvgPool2D(\n+        ComputationNode<T> a,\n+        int[] poolSize,\n+        int[]? strides = null)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        if (shape.Length != 4)\n+            throw new ArgumentException(\"AvgPool2D requires 4D input [batch, channels, height, width]\");\n+\n+        strides ??= poolSize;\n+\n+        int batch = shape[0];\n+        int channels = shape[1];\n+        int inH = shape[2];\n+        int inW = shape[3];\n+        int poolH = poolSize[0];\n+        int poolW = poolSize[1];\n+        int strideH = strides[0];\n+        int strideW = strides[1];\n+\n+        int outH = (inH - poolH) / strideH + 1;\n+        int outW = (inW - poolW) / strideW + 1;\n+\n+        var result = new Tensor<T>(new int[] { batch, channels, outH, outW });\n+        var poolArea = numOps.FromDouble(poolH * poolW);\n+\n+        // Forward pass: compute average pooling\n+        for (int b = 0; b < batch; b++)\n+        {\n+            for (int c = 0; c < channels; c++)\n+            {\n+                for (int oh = 0; oh < outH; oh++)\n+                {\n+                    for (int ow = 0; ow < outW; ow++)\n+                    {\n+                        int hStart = oh * strideH;\n+                        int wStart = ow * strideW;\n+\n+                        var sum = numOps.Zero;\n+\n+                        // Sum values in pooling window\n+                        for (int ph = 0; ph < poolH; ph++)\n+                        {\n+                            for (int pw = 0; pw < poolW; pw++)\n+                            {\n+                                int h = hStart + ph;\n+                                int w = wStart + pw;\n+                                if (h < inH && w < inW)\n+                                {\n+                                    sum = numOps.Add(sum, a.Value[b, c, h, w]);\n+                                }\n+                            }\n+                        }\n+\n+                        result[b, c, oh, ow] = numOps.Divide(sum, poolArea);\n+                    }\n+                }\n+            }\n+        }\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            if (a.RequiresGradient)\n+            {\n+                var gradA = new Tensor<T>(shape);\n+\n+                // Distribute gradients equally across pooling windows\n+                for (int b = 0; b < batch; b++)\n+                {\n+                    for (int c = 0; c < channels; c++)\n+                    {\n+                        for (int oh = 0; oh < outH; oh++)\n+                        {\n+                            for (int ow = 0; ow < outW; ow++)\n+                            {\n+                                int hStart = oh * strideH;\n+                                int wStart = ow * strideW;\n+\n+                                var gradValue = numOps.Divide(gradient[b, c, oh, ow], poolArea);\n+\n+                                // Distribute to all elements in window\n+                                for (int ph = 0; ph < poolH; ph++)\n+                                {\n+                                    for (int pw = 0; pw < poolW; pw++)\n+                                    {\n+                                        int h = hStart + ph;\n+                                        int w = wStart + pw;\n+                                        if (h < inH && w < inW)\n+                                        {\n+                                            gradA[b, c, h, w] = numOps.Add(gradA[b, c, h, w], gradValue);\n+                                        }\n+                                    }\n+                                }\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (a.Gradient == null)\n+                    a.Gradient = gradA;\n+                else\n+                    a.Gradient = a.Gradient.Add(gradA);\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: a.RequiresGradient,\n+            parents: new List<ComputationNode<T>> { a },\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Applies layer normalization to a computation node.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"normalizedShape\">The shape over which to normalize (typically the feature dimensions).</param>\n+    /// <param name=\"gamma\">Optional scale parameter (learnable). If null, uses ones.</param>\n+    /// <param name=\"beta\">Optional shift parameter (learnable). If null, uses zeros.</param>\n+    /// <param name=\"epsilon\">Small constant for numerical stability. Default is 1e-5.</param>\n+    /// <returns>A new computation node containing the layer normalized result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Layer normalization normalizes inputs across the feature dimension for each sample independently.\n+    /// Formula: y = gamma * (x - mean) / sqrt(variance + epsilon) + beta\n+    /// Unlike batch normalization, this doesn't depend on batch statistics.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> LayerNorm standardizes features for each sample independently.\n+    ///\n+    /// For layer normalization:\n+    /// - Computes mean and variance for each sample's features\n+    /// - Normalizes: (x - mean) / sqrt(variance)\n+    /// - Scales and shifts: result * gamma + beta\n+    /// - Works the same during training and inference (no batch dependency)\n+    ///\n+    /// Used in:\n+    /// - Transformers (critical component)\n+    /// - RNNs (stabilizes training)\n+    /// - Any architecture needing sample-independent normalization\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> LayerNorm(\n+        ComputationNode<T> a,\n+        int[] normalizedShape,\n+        ComputationNode<T>? gamma = null,\n+        ComputationNode<T>? beta = null,\n+        double epsilon = 1e-5)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+        var eps = numOps.FromDouble(epsilon);\n+\n+        // For 2D input [batch, features], normalize over features\n+        if (shape.Length == 2 && normalizedShape.Length == 1 && normalizedShape[0] == shape[1])\n+        {\n+            int batchSize = shape[0];\n+            int features = shape[1];\n+\n+            // Create default gamma (ones) and beta (zeros) if not provided\n+            if (gamma == null)\n+            {\n+                var gammaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    gammaTensor[i] = numOps.One;\n+                gamma = Variable(gammaTensor, requiresGradient: false);\n+            }\n+\n+            if (beta == null)\n+            {\n+                var betaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    betaTensor[i] = numOps.Zero;\n+                beta = Variable(betaTensor, requiresGradient: false);\n+            }\n+\n+            var result = new Tensor<T>(shape);\n+            var means = new T[batchSize];\n+            var variances = new T[batchSize];\n+            var normalized = new Tensor<T>(shape);\n+\n+            // Forward pass: compute mean and variance per sample\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                // Compute mean\n+                var sum = numOps.Zero;\n+                for (int f = 0; f < features; f++)\n+                {\n+                    sum = numOps.Add(sum, a.Value[b, f]);\n+                }\n+                means[b] = numOps.Divide(sum, numOps.FromDouble(features));\n+\n+                // Compute variance\n+                var varSum = numOps.Zero;\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var diff = numOps.Subtract(a.Value[b, f], means[b]);\n+                    varSum = numOps.Add(varSum, numOps.Multiply(diff, diff));\n+                }\n+                variances[b] = numOps.Divide(varSum, numOps.FromDouble(features));\n+\n+                // Normalize and scale\n+                var std = numOps.Sqrt(numOps.Add(variances[b], eps));\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var norm = numOps.Divide(\n+                        numOps.Subtract(a.Value[b, f], means[b]),\n+                        std);\n+                    normalized[b, f] = norm;\n+                    result[b, f] = numOps.Add(\n+                        numOps.Multiply(norm, gamma.Value[f]),\n+                        beta.Value[f]);\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                // Gradients for gamma and beta\n+                if (gamma.RequiresGradient)\n+                {\n+                    var gradGamma = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum,\n+                                numOps.Multiply(gradient[b, f], normalized[b, f]));\n+                        }\n+                        gradGamma[f] = sum;\n+                    }\n+\n+                    if (gamma.Gradient == null)\n+                        gamma.Gradient = gradGamma;\n+                    else\n+                        gamma.Gradient = gamma.Gradient.Add(gradGamma);\n+                }\n+\n+                if (beta.RequiresGradient)\n+                {\n+                    var gradBeta = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum, gradient[b, f]);\n+                        }\n+                        gradBeta[f] = sum;\n+                    }\n+\n+                    if (beta.Gradient == null)\n+                        beta.Gradient = gradBeta;\n+                    else\n+                        beta.Gradient = beta.Gradient.Add(gradBeta);\n+                }\n+\n+                // Gradient for input\n+                if (a.RequiresGradient)\n+                {\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        var std = numOps.Sqrt(numOps.Add(variances[b], eps));\n+                        var invStd = numOps.Divide(numOps.One, std);\n+\n+                        // Compute gradient components\n+                        var gradNormSum = numOps.Zero;\n+                        var gradNormDotNorm = numOps.Zero;\n+\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            var gradNorm = numOps.Multiply(gradient[b, f], gamma.Value[f]);\n+                            gradNormSum = numOps.Add(gradNormSum, gradNorm);\n+                            gradNormDotNorm = numOps.Add(gradNormDotNorm,\n+                                numOps.Multiply(gradNorm, normalized[b, f]));\n+                        }\n+\n+                        // Apply gradient formula\n+                        var featuresT = numOps.FromDouble(features);\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            var gradNorm = numOps.Multiply(gradient[b, f], gamma.Value[f]);\n+\n+                            var term1 = gradNorm;\n+                            var term2 = numOps.Divide(gradNormSum, featuresT);\n+                            var term3 = numOps.Divide(\n+                                numOps.Multiply(normalized[b, f], gradNormDotNorm),\n+                                featuresT);\n+\n+                            var grad = numOps.Multiply(\n+                                numOps.Subtract(numOps.Subtract(term1, term2), term3),\n+                                invStd);\n+\n+                            gradA[b, f] = grad;\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var parents = new List<ComputationNode<T>> { a };\n+            if (gamma != null) parents.Add(gamma);\n+            if (beta != null) parents.Add(beta);\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient || (gamma?.RequiresGradient ?? false) || (beta?.RequiresGradient ?? false),\n+                parents: parents,\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"LayerNorm is currently only implemented for 2D tensors normalizing over last dimension. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}], normalizedShape=[{string.Join(\", \", normalizedShape)}]\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Applies batch normalization to a computation node.\n+    /// </summary>\n+    /// <param name=\"a\">The input node with shape [batch, features].</param>\n+    /// <param name=\"gamma\">Optional scale parameter (learnable). If null, uses ones.</param>\n+    /// <param name=\"beta\">Optional shift parameter (learnable). If null, uses zeros.</param>\n+    /// <param name=\"runningMean\">Running mean for inference (not updated during this operation).</param>\n+    /// <param name=\"runningVar\">Running variance for inference (not updated during this operation).</param>\n+    /// <param name=\"training\">Whether in training mode (uses batch statistics) or inference mode (uses running statistics).</param>\n+    /// <param name=\"epsilon\">Small constant for numerical stability. Default is 1e-5.</param>\n+    /// <returns>A new computation node containing the batch normalized result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Batch normalization normalizes inputs across the batch dimension.\n+    /// During training: Uses batch statistics (mean and variance computed from current batch).\n+    /// During inference: Uses running statistics (accumulated during training).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> BatchNorm standardizes features across the batch.\n+    ///\n+    /// For batch normalization:\n+    /// - Training mode: Uses current batch's mean and variance\n+    /// - Inference mode: Uses running mean/variance from training\n+    /// - Normalizes: (x - mean) / sqrt(variance)\n+    /// - Scales and shifts: result * gamma + beta\n+    ///\n+    /// Benefits:\n+    /// - Stabilizes training (reduces internal covariate shift)\n+    /// - Allows higher learning rates\n+    /// - Acts as regularization\n+    ///\n+    /// Used in:\n+    /// - CNNs (after convolutional layers)\n+    /// - Deep feedforward networks\n+    /// - GANs and many other architectures\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> BatchNorm(\n+        ComputationNode<T> a,\n+        ComputationNode<T>? gamma = null,\n+        ComputationNode<T>? beta = null,\n+        Tensor<T>? runningMean = null,\n+        Tensor<T>? runningVar = null,\n+        bool training = true,\n+        double epsilon = 1e-5)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+        var eps = numOps.FromDouble(epsilon);\n+\n+        // Handle 2D case [batch, features]\n+        if (shape.Length == 2)\n+        {\n+            int batchSize = shape[0];\n+            int features = shape[1];\n+\n+            // Create default gamma and beta if not provided\n+            if (gamma == null)\n+            {\n+                var gammaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    gammaTensor[i] = numOps.One;\n+                gamma = Variable(gammaTensor, requiresGradient: false);\n+            }\n+\n+            if (beta == null)\n+            {\n+                var betaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    betaTensor[i] = numOps.Zero;\n+                beta = Variable(betaTensor, requiresGradient: false);\n+            }\n+\n+            var result = new Tensor<T>(shape);\n+            T[] batchMean;\n+            T[] batchVar;\n+            var normalized = new Tensor<T>(shape);\n+\n+            if (training)\n+            {\n+                // Compute batch statistics\n+                batchMean = new T[features];\n+                batchVar = new T[features];\n+\n+                // Compute mean per feature\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var sum = numOps.Zero;\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        sum = numOps.Add(sum, a.Value[b, f]);\n+                    }\n+                    batchMean[f] = numOps.Divide(sum, numOps.FromDouble(batchSize));\n+                }\n+\n+                // Compute variance per feature\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var varSum = numOps.Zero;\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        var diff = numOps.Subtract(a.Value[b, f], batchMean[f]);\n+                        varSum = numOps.Add(varSum, numOps.Multiply(diff, diff));\n+                    }\n+                    batchVar[f] = numOps.Divide(varSum, numOps.FromDouble(batchSize));\n+                }\n+            }\n+            else\n+            {\n+                // Use running statistics for inference\n+                if (runningMean == null || runningVar == null)\n+                    throw new ArgumentException(\"Running statistics required for inference mode\");\n+\n+                batchMean = new T[features];\n+                batchVar = new T[features];\n+                for (int f = 0; f < features; f++)\n+                {\n+                    batchMean[f] = runningMean[f];\n+                    batchVar[f] = runningVar[f];\n+                }\n+            }\n+\n+            // Normalize and scale\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var std = numOps.Sqrt(numOps.Add(batchVar[f], eps));\n+                    var norm = numOps.Divide(\n+                        numOps.Subtract(a.Value[b, f], batchMean[f]),\n+                        std);\n+                    normalized[b, f] = norm;\n+                    result[b, f] = numOps.Add(\n+                        numOps.Multiply(norm, gamma.Value[f]),\n+                        beta.Value[f]);\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (!training)\n+                {\n+                    // Inference mode: simpler gradient (no batch statistics gradient)\n+                    if (a.RequiresGradient)\n+                    {\n+                        var gradA = new Tensor<T>(shape);\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            for (int f = 0; f < features; f++)\n+                            {\n+                                var std = numOps.Sqrt(numOps.Add(batchVar[f], eps));\n+                                var invStd = numOps.Divide(numOps.One, std);\n+                                gradA[b, f] = numOps.Multiply(\n+                                    numOps.Multiply(gradient[b, f], gamma.Value[f]),\n+                                    invStd);\n+                            }\n+                        }\n+\n+                        if (a.Gradient == null)\n+                            a.Gradient = gradA;\n+                        else\n+                            a.Gradient = a.Gradient.Add(gradA);\n+                    }\n+                    return;\n+                }\n+\n+                // Training mode: full gradient computation\n+                // Gradients for gamma and beta\n+                if (gamma.RequiresGradient)\n+                {\n+                    var gradGamma = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum,\n+                                numOps.Multiply(gradient[b, f], normalized[b, f]));\n+                        }\n+                        gradGamma[f] = sum;\n+                    }\n+\n+                    if (gamma.Gradient == null)\n+                        gamma.Gradient = gradGamma;\n+                    else\n+                        gamma.Gradient = gamma.Gradient.Add(gradGamma);\n+                }\n+\n+                if (beta.RequiresGradient)\n+                {\n+                    var gradBeta = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum, gradient[b, f]);\n+                        }\n+                        gradBeta[f] = sum;\n+                    }\n+\n+                    if (beta.Gradient == null)\n+                        beta.Gradient = gradBeta;\n+                    else\n+                        beta.Gradient = beta.Gradient.Add(gradBeta);\n+                }\n+\n+                // Gradient for input (complex due to batch statistics)\n+                if (a.RequiresGradient)\n+                {\n+                    var gradA = new Tensor<T>(shape);\n+                    var batchSizeT = numOps.FromDouble(batchSize);\n+\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var std = numOps.Sqrt(numOps.Add(batchVar[f], eps));\n+                        var invStd = numOps.Divide(numOps.One, std);\n+\n+                        // Sum of gradients and gradient*normalized\n+                        var gradSum = numOps.Zero;\n+                        var gradNormSum = numOps.Zero;\n+\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            var grad = numOps.Multiply(gradient[b, f], gamma.Value[f]);\n+                            gradSum = numOps.Add(gradSum, grad);\n+                            gradNormSum = numOps.Add(gradNormSum,\n+                                numOps.Multiply(grad, normalized[b, f]));\n+                        }\n+\n+                        // Apply gradient formula\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            var grad = numOps.Multiply(gradient[b, f], gamma.Value[f]);\n+\n+                            var term1 = grad;\n+                            var term2 = numOps.Divide(gradSum, batchSizeT);\n+                            var term3 = numOps.Divide(\n+                                numOps.Multiply(normalized[b, f], gradNormSum),\n+                                batchSizeT);\n+\n+                            var gradInput = numOps.Multiply(\n+                                numOps.Subtract(numOps.Subtract(term1, term2), term3),\n+                                invStd);\n+\n+                            gradA[b, f] = gradInput;\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);"}]}},{"id":"PRRT_kwDOKSXUF85hck-3","isResolved":false,"isOutdated":false,"path":"src/Autodiff/TensorOperations.cs","line":2629,"originalLine":2629,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzD_N","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement write to the same variable - consider using '?' to express intent better.\n```suggestion\n                input.Gradient = input.Gradient?.Add(gradInput) ?? gradInput;\n```","createdAt":"2025-11-11T07:53:26Z","path":"src/Autodiff/TensorOperations.cs","line":2629,"originalLine":2629,"diffHunk":"@@ -1219,4 +1219,1769 @@ void BackwardFunction(Tensor<T> gradient)\n \n         return node;\n     }\n+\n+    /// <summary>\n+    /// Computes the softmax function for a computation node along a specified axis.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"axis\">The axis along which to compute softmax. Default is -1 (last axis).</param>\n+    /// <returns>A new computation node containing the softmax result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method computes softmax (σ(x_i) = exp(x_i) / Σexp(x_j)) along the specified axis.\n+    /// Uses numerical stability trick: subtract max before exponentiating.\n+    /// The backward function uses: ∂softmax/∂x = softmax(x) * (grad - Σ(grad * softmax(x))).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Softmax converts a vector of numbers into probabilities.\n+    ///\n+    /// For softmax:\n+    /// - The forward pass exponentiates each element, then normalizes so they sum to 1\n+    /// - The result is a probability distribution (all values between 0 and 1, summing to 1)\n+    /// - The backward pass is complex but efficient: uses the Jacobian of softmax\n+    ///\n+    /// Softmax is crucial for:\n+    /// - Multi-class classification (final layer outputs)\n+    /// - Attention mechanisms (computing attention weights)\n+    /// - Anywhere you need to convert scores to probabilities\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Softmax(ComputationNode<T> a, int axis = -1)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        // Normalize axis to positive index\n+        if (axis < 0)\n+            axis = shape.Length + axis;\n+\n+        // For simplicity, handle 2D case (batch, features) with axis=-1\n+        if (shape.Length == 2 && axis == 1)\n+        {\n+            int batchSize = shape[0];\n+            int features = shape[1];\n+            var result = new Tensor<T>(shape);\n+\n+            // Compute softmax for each row\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                // Find max for numerical stability\n+                var maxVal = a.Value[b, 0];\n+                for (int f = 1; f < features; f++)\n+                {\n+                    if (numOps.GreaterThan(a.Value[b, f], maxVal))\n+                        maxVal = a.Value[b, f];\n+                }\n+\n+                // Compute exp(x - max) and sum\n+                var expSum = numOps.Zero;\n+                var expValues = new T[features];\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var shifted = numOps.Subtract(a.Value[b, f], maxVal);\n+                    expValues[f] = numOps.Exp(shifted);\n+                    expSum = numOps.Add(expSum, expValues[f]);\n+                }\n+\n+                // Normalize\n+                for (int f = 0; f < features; f++)\n+                {\n+                    result[b, f] = numOps.Divide(expValues[f], expSum);\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (a.RequiresGradient)\n+                {\n+                    // ∂softmax/∂x_i = softmax_i * (∂L/∂y_i - Σ_j(∂L/∂y_j * softmax_j))\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        // Compute sum of (gradient * softmax)\n+                        var dotProduct = numOps.Zero;\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            dotProduct = numOps.Add(dotProduct,\n+                                numOps.Multiply(gradient[b, f], result[b, f]));\n+                        }\n+\n+                        // Compute gradient for each element\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            var gradMinusDot = numOps.Subtract(gradient[b, f], dotProduct);\n+                            gradA[b, f] = numOps.Multiply(result[b, f], gradMinusDot);\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient,\n+                parents: new List<ComputationNode<T>> { a },\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Softmax is currently only implemented for 2D tensors along axis=-1. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}], axis={axis}\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Concatenates multiple computation nodes along a specified axis.\n+    /// </summary>\n+    /// <param name=\"nodes\">The list of nodes to concatenate.</param>\n+    /// <param name=\"axis\">The axis along which to concatenate. Default is 0.</param>\n+    /// <returns>A new computation node containing the concatenated result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method concatenates tensors along the specified axis.\n+    /// All tensors must have the same shape except along the concatenation axis.\n+    /// The backward function splits the gradient and sends each portion to the corresponding input.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Concat stacks tensors together along a dimension.\n+    ///\n+    /// For concatenation:\n+    /// - The forward pass combines multiple tensors into one larger tensor\n+    /// - The backward pass splits the gradient back to each input\n+    /// - Think of it like gluing arrays together end-to-end\n+    ///\n+    /// Used in:\n+    /// - Skip connections (concatenating features from different layers)\n+    /// - Multi-input architectures\n+    /// - Feature fusion in neural networks\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Concat(List<ComputationNode<T>> nodes, int axis = 0)\n+    {\n+        if (nodes.Count == 0)\n+            throw new ArgumentException(\"Cannot concatenate empty list of nodes\");\n+\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var firstShape = nodes[0].Value.Shape;\n+\n+        // Normalize axis\n+        if (axis < 0)\n+            axis = firstShape.Length + axis;\n+\n+        // Validate shapes match except on concat axis\n+        for (int i = 1; i < nodes.Count; i++)\n+        {\n+            var shape = nodes[i].Value.Shape;\n+            if (shape.Length != firstShape.Length)\n+                throw new ArgumentException(\"All tensors must have the same rank\");\n+\n+            for (int d = 0; d < firstShape.Length; d++)\n+            {\n+                if (d != axis && shape[d] != firstShape[d])\n+                    throw new ArgumentException(\n+                        $\"Shape mismatch at dimension {d}: {shape[d]} vs {firstShape[d]}\");\n+            }\n+        }\n+\n+        // Compute output shape\n+        int[] outputShape = (int[])firstShape.Clone();\n+        for (int i = 1; i < nodes.Count; i++)\n+        {\n+            outputShape[axis] += nodes[i].Value.Shape[axis];\n+        }\n+\n+        // Perform concatenation (handle 2D case for simplicity)\n+        Tensor<T> result;\n+        if (firstShape.Length == 2 && axis == 1)\n+        {\n+            // Concatenate along columns (features)\n+            int rows = firstShape[0];\n+            int totalCols = outputShape[1];\n+            result = new Tensor<T>(new int[] { rows, totalCols });\n+\n+            int colOffset = 0;\n+            foreach (var node in nodes)\n+            {\n+                int cols = node.Value.Shape[1];\n+                for (int r = 0; r < rows; r++)\n+                {\n+                    for (int c = 0; c < cols; c++)\n+                    {\n+                        result[r, colOffset + c] = node.Value[r, c];\n+                    }\n+                }\n+                colOffset += cols;\n+            }\n+        }\n+        else if (firstShape.Length == 2 && axis == 0)\n+        {\n+            // Concatenate along rows (batch)\n+            int cols = firstShape[1];\n+            int totalRows = outputShape[0];\n+            result = new Tensor<T>(new int[] { totalRows, cols });\n+\n+            int rowOffset = 0;\n+            foreach (var node in nodes)\n+            {\n+                int rows = node.Value.Shape[0];\n+                for (int r = 0; r < rows; r++)\n+                {\n+                    for (int c = 0; c < cols; c++)\n+                    {\n+                        result[rowOffset + r, c] = node.Value[r, c];\n+                    }\n+                }\n+                rowOffset += rows;\n+            }\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Concat is currently only implemented for 2D tensors. \" +\n+                $\"Got shape=[{string.Join(\", \", firstShape)}]\");\n+        }\n+\n+        // Store sizes for gradient splitting\n+        var sizes = nodes.Select(n => n.Value.Shape[axis]).ToList();\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            // Split gradient along concat axis and distribute to inputs\n+            if (firstShape.Length == 2 && axis == 1)\n+            {\n+                int rows = firstShape[0];\n+                int colOffset = 0;\n+                for (int i = 0; i < nodes.Count; i++)\n+                {\n+                    if (!nodes[i].RequiresGradient)\n+                    {\n+                        colOffset += sizes[i];\n+                        continue;\n+                    }\n+\n+                    int cols = sizes[i];\n+                    var gradPart = new Tensor<T>(new int[] { rows, cols });\n+\n+                    for (int r = 0; r < rows; r++)\n+                    {\n+                        for (int c = 0; c < cols; c++)\n+                        {\n+                            gradPart[r, c] = gradient[r, colOffset + c];\n+                        }\n+                    }\n+\n+                    if (nodes[i].Gradient == null)\n+                        nodes[i].Gradient = gradPart;\n+                    else\n+                        nodes[i].Gradient = nodes[i].Gradient.Add(gradPart);\n+\n+                    colOffset += cols;\n+                }\n+            }\n+            else if (firstShape.Length == 2 && axis == 0)\n+            {\n+                int cols = firstShape[1];\n+                int rowOffset = 0;\n+                for (int i = 0; i < nodes.Count; i++)\n+                {\n+                    if (!nodes[i].RequiresGradient)\n+                    {\n+                        rowOffset += sizes[i];\n+                        continue;\n+                    }\n+\n+                    int rows = sizes[i];\n+                    var gradPart = new Tensor<T>(new int[] { rows, cols });\n+\n+                    for (int r = 0; r < rows; r++)\n+                    {\n+                        for (int c = 0; c < cols; c++)\n+                        {\n+                            gradPart[r, c] = gradient[rowOffset + r, c];\n+                        }\n+                    }\n+\n+                    if (nodes[i].Gradient == null)\n+                        nodes[i].Gradient = gradPart;\n+                    else\n+                        nodes[i].Gradient = nodes[i].Gradient.Add(gradPart);\n+\n+                    rowOffset += rows;\n+                }\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: nodes.Any(n => n.RequiresGradient),\n+            parents: nodes,\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Pads a tensor with a constant value along specified dimensions.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"padWidth\">Padding width for each dimension as (before, after) pairs.</param>\n+    /// <param name=\"value\">The value to use for padding. Default is zero.</param>\n+    /// <returns>A new computation node containing the padded result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method adds padding around the tensor.\n+    /// The backward function simply crops the gradient back to the original size (gradients for padding are zero).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Pad adds extra elements around a tensor.\n+    ///\n+    /// For padding:\n+    /// - The forward pass adds border elements with a constant value\n+    /// - The backward pass removes those border gradients (they don't affect the original tensor)\n+    /// - Think of it like adding margins to an image\n+    ///\n+    /// Used in:\n+    /// - Convolutional layers (to maintain spatial dimensions)\n+    /// - Handling variable-length sequences\n+    /// - Data augmentation\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Pad(ComputationNode<T> a, int[,] padWidth, T? value = default)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var padValue = value ?? numOps.Zero;\n+        var shape = a.Value.Shape;\n+\n+        // Validate padWidth dimensions\n+        if (padWidth.GetLength(0) != shape.Length)\n+            throw new ArgumentException(\"padWidth must have same number of dimensions as input tensor\");\n+\n+        // Compute output shape\n+        var outputShape = new int[shape.Length];\n+        for (int d = 0; d < shape.Length; d++)\n+        {\n+            outputShape[d] = shape[d] + padWidth[d, 0] + padWidth[d, 1];\n+        }\n+\n+        // Handle 2D case\n+        if (shape.Length == 2)\n+        {\n+            int inputRows = shape[0];\n+            int inputCols = shape[1];\n+            int padTop = padWidth[0, 0];\n+            int padBottom = padWidth[0, 1];\n+            int padLeft = padWidth[1, 0];\n+            int padRight = padWidth[1, 1];\n+\n+            var result = new Tensor<T>(outputShape);\n+\n+            // Initialize with pad value\n+            for (int i = 0; i < result.Length; i++)\n+            {\n+                result[i] = padValue;\n+            }\n+\n+            // Copy input data to center\n+            for (int r = 0; r < inputRows; r++)\n+            {\n+                for (int c = 0; c < inputCols; c++)\n+                {\n+                    result[padTop + r, padLeft + c] = a.Value[r, c];\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (a.RequiresGradient)\n+                {\n+                    // Extract gradient for original (unpadded) region\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int r = 0; r < inputRows; r++)\n+                    {\n+                        for (int c = 0; c < inputCols; c++)\n+                        {\n+                            gradA[r, c] = gradient[padTop + r, padLeft + c];\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient,\n+                parents: new List<ComputationNode<T>> { a },\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Pad is currently only implemented for 2D tensors. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}]\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs 2D max pooling on a 4D tensor (batch, channels, height, width).\n+    /// </summary>\n+    /// <param name=\"a\">The input node with shape [batch, channels, height, width].</param>\n+    /// <param name=\"poolSize\">The size of the pooling window [poolH, poolW].</param>\n+    /// <param name=\"strides\">The stride for the pooling operation [strideH, strideW]. If null, uses poolSize.</param>\n+    /// <returns>A new computation node containing the max pooled result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method performs max pooling over 2D spatial dimensions.\n+    /// During forward pass, it tracks which element was the max for routing gradients during backward pass.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> MaxPool downsamples by taking the maximum value in each window.\n+    ///\n+    /// For max pooling:\n+    /// - The forward pass slides a window and takes the max value in each position\n+    /// - This reduces spatial dimensions (downsampling)\n+    /// - The backward pass routes gradients only to the positions that were max\n+    /// - Other positions get zero gradient (they didn't contribute to the output)\n+    ///\n+    /// Used in:\n+    /// - CNNs for translation invariance\n+    /// - Reducing spatial resolution\n+    /// - Building hierarchical features\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> MaxPool2D(\n+        ComputationNode<T> a,\n+        int[] poolSize,\n+        int[]? strides = null)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        if (shape.Length != 4)\n+            throw new ArgumentException(\"MaxPool2D requires 4D input [batch, channels, height, width]\");\n+\n+        strides ??= poolSize;\n+\n+        int batch = shape[0];\n+        int channels = shape[1];\n+        int inH = shape[2];\n+        int inW = shape[3];\n+        int poolH = poolSize[0];\n+        int poolW = poolSize[1];\n+        int strideH = strides[0];\n+        int strideW = strides[1];\n+\n+        int outH = (inH - poolH) / strideH + 1;\n+        int outW = (inW - poolW) / strideW + 1;\n+\n+        var result = new Tensor<T>(new int[] { batch, channels, outH, outW });\n+        // Store max positions for backprop\n+        var maxPositions = new int[batch, channels, outH, outW, 2]; // [h_offset, w_offset]\n+\n+        // Forward pass: compute max pooling and track positions\n+        for (int b = 0; b < batch; b++)\n+        {\n+            for (int c = 0; c < channels; c++)\n+            {\n+                for (int oh = 0; oh < outH; oh++)\n+                {\n+                    for (int ow = 0; ow < outW; ow++)\n+                    {\n+                        int hStart = oh * strideH;\n+                        int wStart = ow * strideW;\n+\n+                        var maxVal = a.Value[b * channels * inH * inW +\n+                                           c * inH * inW +\n+                                           hStart * inW +\n+                                           wStart];\n+                        int maxHOffset = 0;\n+                        int maxWOffset = 0;\n+\n+                        // Find max in pooling window\n+                        for (int ph = 0; ph < poolH; ph++)\n+                        {\n+                            for (int pw = 0; pw < poolW; pw++)\n+                            {\n+                                int h = hStart + ph;\n+                                int w = wStart + pw;\n+                                if (h < inH && w < inW)\n+                                {\n+                                    var val = a.Value[b * channels * inH * inW +\n+                                                     c * inH * inW +\n+                                                     h * inW +\n+                                                     w];\n+                                    if (numOps.GreaterThan(val, maxVal))\n+                                    {\n+                                        maxVal = val;\n+                                        maxHOffset = ph;\n+                                        maxWOffset = pw;\n+                                    }\n+                                }\n+                            }\n+                        }\n+\n+                        result[b, c, oh, ow] = maxVal;\n+                        maxPositions[b, c, oh, ow, 0] = maxHOffset;\n+                        maxPositions[b, c, oh, ow, 1] = maxWOffset;\n+                    }\n+                }\n+            }\n+        }\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            if (a.RequiresGradient)\n+            {\n+                var gradA = new Tensor<T>(shape);\n+\n+                // Route gradients to max positions\n+                for (int b = 0; b < batch; b++)\n+                {\n+                    for (int c = 0; c < channels; c++)\n+                    {\n+                        for (int oh = 0; oh < outH; oh++)\n+                        {\n+                            for (int ow = 0; ow < outW; ow++)\n+                            {\n+                                int hStart = oh * strideH;\n+                                int wStart = ow * strideW;\n+                                int maxHOffset = maxPositions[b, c, oh, ow, 0];\n+                                int maxWOffset = maxPositions[b, c, oh, ow, 1];\n+\n+                                int maxH = hStart + maxHOffset;\n+                                int maxW = wStart + maxWOffset;\n+\n+                                int gradIdx = b * channels * inH * inW +\n+                                             c * inH * inW +\n+                                            maxH * inW +\n+                                             maxW;\n+\n+                                gradA[gradIdx] = numOps.Add(gradA[gradIdx], gradient[b, c, oh, ow]);\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (a.Gradient == null)\n+                    a.Gradient = gradA;\n+                else\n+                    a.Gradient = a.Gradient.Add(gradA);\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: a.RequiresGradient,\n+            parents: new List<ComputationNode<T>> { a },\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Performs 2D average pooling on a 4D tensor (batch, channels, height, width).\n+    /// </summary>\n+    /// <param name=\"a\">The input node with shape [batch, channels, height, width].</param>\n+    /// <param name=\"poolSize\">The size of the pooling window [poolH, poolW].</param>\n+    /// <param name=\"strides\">The stride for the pooling operation [strideH, strideW]. If null, uses poolSize.</param>\n+    /// <returns>A new computation node containing the average pooled result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method performs average pooling over 2D spatial dimensions.\n+    /// The backward function distributes gradients equally across the pooling window.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> AvgPool downsamples by taking the average value in each window.\n+    ///\n+    /// For average pooling:\n+    /// - The forward pass slides a window and computes the average\n+    /// - This smoothly reduces spatial dimensions\n+    /// - The backward pass distributes gradients equally to all elements in the window\n+    /// - Each element gets gradient / pool_area\n+    ///\n+    /// Used in:\n+    /// - CNNs for smoother downsampling than max pooling\n+    /// - Global average pooling (replacing fully connected layers)\n+    /// - Reducing overfitting\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> AvgPool2D(\n+        ComputationNode<T> a,\n+        int[] poolSize,\n+        int[]? strides = null)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        if (shape.Length != 4)\n+            throw new ArgumentException(\"AvgPool2D requires 4D input [batch, channels, height, width]\");\n+\n+        strides ??= poolSize;\n+\n+        int batch = shape[0];\n+        int channels = shape[1];\n+        int inH = shape[2];\n+        int inW = shape[3];\n+        int poolH = poolSize[0];\n+        int poolW = poolSize[1];\n+        int strideH = strides[0];\n+        int strideW = strides[1];\n+\n+        int outH = (inH - poolH) / strideH + 1;\n+        int outW = (inW - poolW) / strideW + 1;\n+\n+        var result = new Tensor<T>(new int[] { batch, channels, outH, outW });\n+        var poolArea = numOps.FromDouble(poolH * poolW);\n+\n+        // Forward pass: compute average pooling\n+        for (int b = 0; b < batch; b++)\n+        {\n+            for (int c = 0; c < channels; c++)\n+            {\n+                for (int oh = 0; oh < outH; oh++)\n+                {\n+                    for (int ow = 0; ow < outW; ow++)\n+                    {\n+                        int hStart = oh * strideH;\n+                        int wStart = ow * strideW;\n+\n+                        var sum = numOps.Zero;\n+\n+                        // Sum values in pooling window\n+                        for (int ph = 0; ph < poolH; ph++)\n+                        {\n+                            for (int pw = 0; pw < poolW; pw++)\n+                            {\n+                                int h = hStart + ph;\n+                                int w = wStart + pw;\n+                                if (h < inH && w < inW)\n+                                {\n+                                    sum = numOps.Add(sum, a.Value[b, c, h, w]);\n+                                }\n+                            }\n+                        }\n+\n+                        result[b, c, oh, ow] = numOps.Divide(sum, poolArea);\n+                    }\n+                }\n+            }\n+        }\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            if (a.RequiresGradient)\n+            {\n+                var gradA = new Tensor<T>(shape);\n+\n+                // Distribute gradients equally across pooling windows\n+                for (int b = 0; b < batch; b++)\n+                {\n+                    for (int c = 0; c < channels; c++)\n+                    {\n+                        for (int oh = 0; oh < outH; oh++)\n+                        {\n+                            for (int ow = 0; ow < outW; ow++)\n+                            {\n+                                int hStart = oh * strideH;\n+                                int wStart = ow * strideW;\n+\n+                                var gradValue = numOps.Divide(gradient[b, c, oh, ow], poolArea);\n+\n+                                // Distribute to all elements in window\n+                                for (int ph = 0; ph < poolH; ph++)\n+                                {\n+                                    for (int pw = 0; pw < poolW; pw++)\n+                                    {\n+                                        int h = hStart + ph;\n+                                        int w = wStart + pw;\n+                                        if (h < inH && w < inW)\n+                                        {\n+                                            gradA[b, c, h, w] = numOps.Add(gradA[b, c, h, w], gradValue);\n+                                        }\n+                                    }\n+                                }\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (a.Gradient == null)\n+                    a.Gradient = gradA;\n+                else\n+                    a.Gradient = a.Gradient.Add(gradA);\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: a.RequiresGradient,\n+            parents: new List<ComputationNode<T>> { a },\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Applies layer normalization to a computation node.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"normalizedShape\">The shape over which to normalize (typically the feature dimensions).</param>\n+    /// <param name=\"gamma\">Optional scale parameter (learnable). If null, uses ones.</param>\n+    /// <param name=\"beta\">Optional shift parameter (learnable). If null, uses zeros.</param>\n+    /// <param name=\"epsilon\">Small constant for numerical stability. Default is 1e-5.</param>\n+    /// <returns>A new computation node containing the layer normalized result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Layer normalization normalizes inputs across the feature dimension for each sample independently.\n+    /// Formula: y = gamma * (x - mean) / sqrt(variance + epsilon) + beta\n+    /// Unlike batch normalization, this doesn't depend on batch statistics.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> LayerNorm standardizes features for each sample independently.\n+    ///\n+    /// For layer normalization:\n+    /// - Computes mean and variance for each sample's features\n+    /// - Normalizes: (x - mean) / sqrt(variance)\n+    /// - Scales and shifts: result * gamma + beta\n+    /// - Works the same during training and inference (no batch dependency)\n+    ///\n+    /// Used in:\n+    /// - Transformers (critical component)\n+    /// - RNNs (stabilizes training)\n+    /// - Any architecture needing sample-independent normalization\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> LayerNorm(\n+        ComputationNode<T> a,\n+        int[] normalizedShape,\n+        ComputationNode<T>? gamma = null,\n+        ComputationNode<T>? beta = null,\n+        double epsilon = 1e-5)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+        var eps = numOps.FromDouble(epsilon);\n+\n+        // For 2D input [batch, features], normalize over features\n+        if (shape.Length == 2 && normalizedShape.Length == 1 && normalizedShape[0] == shape[1])\n+        {\n+            int batchSize = shape[0];\n+            int features = shape[1];\n+\n+            // Create default gamma (ones) and beta (zeros) if not provided\n+            if (gamma == null)\n+            {\n+                var gammaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    gammaTensor[i] = numOps.One;\n+                gamma = Variable(gammaTensor, requiresGradient: false);\n+            }\n+\n+            if (beta == null)\n+            {\n+                var betaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    betaTensor[i] = numOps.Zero;\n+                beta = Variable(betaTensor, requiresGradient: false);\n+            }\n+\n+            var result = new Tensor<T>(shape);\n+            var means = new T[batchSize];\n+            var variances = new T[batchSize];\n+            var normalized = new Tensor<T>(shape);\n+\n+            // Forward pass: compute mean and variance per sample\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                // Compute mean\n+                var sum = numOps.Zero;\n+                for (int f = 0; f < features; f++)\n+                {\n+                    sum = numOps.Add(sum, a.Value[b, f]);\n+                }\n+                means[b] = numOps.Divide(sum, numOps.FromDouble(features));\n+\n+                // Compute variance\n+                var varSum = numOps.Zero;\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var diff = numOps.Subtract(a.Value[b, f], means[b]);\n+                    varSum = numOps.Add(varSum, numOps.Multiply(diff, diff));\n+                }\n+                variances[b] = numOps.Divide(varSum, numOps.FromDouble(features));\n+\n+                // Normalize and scale\n+                var std = numOps.Sqrt(numOps.Add(variances[b], eps));\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var norm = numOps.Divide(\n+                        numOps.Subtract(a.Value[b, f], means[b]),\n+                        std);\n+                    normalized[b, f] = norm;\n+                    result[b, f] = numOps.Add(\n+                        numOps.Multiply(norm, gamma.Value[f]),\n+                        beta.Value[f]);\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                // Gradients for gamma and beta\n+                if (gamma.RequiresGradient)\n+                {\n+                    var gradGamma = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum,\n+                                numOps.Multiply(gradient[b, f], normalized[b, f]));\n+                        }\n+                        gradGamma[f] = sum;\n+                    }\n+\n+                    if (gamma.Gradient == null)\n+                        gamma.Gradient = gradGamma;\n+                    else\n+                        gamma.Gradient = gamma.Gradient.Add(gradGamma);\n+                }\n+\n+                if (beta.RequiresGradient)\n+                {\n+                    var gradBeta = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum, gradient[b, f]);\n+                        }\n+                        gradBeta[f] = sum;\n+                    }\n+\n+                    if (beta.Gradient == null)\n+                        beta.Gradient = gradBeta;\n+                    else\n+                        beta.Gradient = beta.Gradient.Add(gradBeta);\n+                }\n+\n+                // Gradient for input\n+                if (a.RequiresGradient)\n+                {\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        var std = numOps.Sqrt(numOps.Add(variances[b], eps));\n+                        var invStd = numOps.Divide(numOps.One, std);\n+\n+                        // Compute gradient components\n+                        var gradNormSum = numOps.Zero;\n+                        var gradNormDotNorm = numOps.Zero;\n+\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            var gradNorm = numOps.Multiply(gradient[b, f], gamma.Value[f]);\n+                            gradNormSum = numOps.Add(gradNormSum, gradNorm);\n+                            gradNormDotNorm = numOps.Add(gradNormDotNorm,\n+                                numOps.Multiply(gradNorm, normalized[b, f]));\n+                        }\n+\n+                        // Apply gradient formula\n+                        var featuresT = numOps.FromDouble(features);\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            var gradNorm = numOps.Multiply(gradient[b, f], gamma.Value[f]);\n+\n+                            var term1 = gradNorm;\n+                            var term2 = numOps.Divide(gradNormSum, featuresT);\n+                            var term3 = numOps.Divide(\n+                                numOps.Multiply(normalized[b, f], gradNormDotNorm),\n+                                featuresT);\n+\n+                            var grad = numOps.Multiply(\n+                                numOps.Subtract(numOps.Subtract(term1, term2), term3),\n+                                invStd);\n+\n+                            gradA[b, f] = grad;\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var parents = new List<ComputationNode<T>> { a };\n+            if (gamma != null) parents.Add(gamma);\n+            if (beta != null) parents.Add(beta);\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient || (gamma?.RequiresGradient ?? false) || (beta?.RequiresGradient ?? false),\n+                parents: parents,\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"LayerNorm is currently only implemented for 2D tensors normalizing over last dimension. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}], normalizedShape=[{string.Join(\", \", normalizedShape)}]\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Applies batch normalization to a computation node.\n+    /// </summary>\n+    /// <param name=\"a\">The input node with shape [batch, features].</param>\n+    /// <param name=\"gamma\">Optional scale parameter (learnable). If null, uses ones.</param>\n+    /// <param name=\"beta\">Optional shift parameter (learnable). If null, uses zeros.</param>\n+    /// <param name=\"runningMean\">Running mean for inference (not updated during this operation).</param>\n+    /// <param name=\"runningVar\">Running variance for inference (not updated during this operation).</param>\n+    /// <param name=\"training\">Whether in training mode (uses batch statistics) or inference mode (uses running statistics).</param>\n+    /// <param name=\"epsilon\">Small constant for numerical stability. Default is 1e-5.</param>\n+    /// <returns>A new computation node containing the batch normalized result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Batch normalization normalizes inputs across the batch dimension.\n+    /// During training: Uses batch statistics (mean and variance computed from current batch).\n+    /// During inference: Uses running statistics (accumulated during training).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> BatchNorm standardizes features across the batch.\n+    ///\n+    /// For batch normalization:\n+    /// - Training mode: Uses current batch's mean and variance\n+    /// - Inference mode: Uses running mean/variance from training\n+    /// - Normalizes: (x - mean) / sqrt(variance)\n+    /// - Scales and shifts: result * gamma + beta\n+    ///\n+    /// Benefits:\n+    /// - Stabilizes training (reduces internal covariate shift)\n+    /// - Allows higher learning rates\n+    /// - Acts as regularization\n+    ///\n+    /// Used in:\n+    /// - CNNs (after convolutional layers)\n+    /// - Deep feedforward networks\n+    /// - GANs and many other architectures\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> BatchNorm(\n+        ComputationNode<T> a,\n+        ComputationNode<T>? gamma = null,\n+        ComputationNode<T>? beta = null,\n+        Tensor<T>? runningMean = null,\n+        Tensor<T>? runningVar = null,\n+        bool training = true,\n+        double epsilon = 1e-5)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+        var eps = numOps.FromDouble(epsilon);\n+\n+        // Handle 2D case [batch, features]\n+        if (shape.Length == 2)\n+        {\n+            int batchSize = shape[0];\n+            int features = shape[1];\n+\n+            // Create default gamma and beta if not provided\n+            if (gamma == null)\n+            {\n+                var gammaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    gammaTensor[i] = numOps.One;\n+                gamma = Variable(gammaTensor, requiresGradient: false);\n+            }\n+\n+            if (beta == null)\n+            {\n+                var betaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    betaTensor[i] = numOps.Zero;\n+                beta = Variable(betaTensor, requiresGradient: false);\n+            }\n+\n+            var result = new Tensor<T>(shape);\n+            T[] batchMean;\n+            T[] batchVar;\n+            var normalized = new Tensor<T>(shape);\n+\n+            if (training)\n+            {\n+                // Compute batch statistics\n+                batchMean = new T[features];\n+                batchVar = new T[features];\n+\n+                // Compute mean per feature\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var sum = numOps.Zero;\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        sum = numOps.Add(sum, a.Value[b, f]);\n+                    }\n+                    batchMean[f] = numOps.Divide(sum, numOps.FromDouble(batchSize));\n+                }\n+\n+                // Compute variance per feature\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var varSum = numOps.Zero;\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        var diff = numOps.Subtract(a.Value[b, f], batchMean[f]);\n+                        varSum = numOps.Add(varSum, numOps.Multiply(diff, diff));\n+                    }\n+                    batchVar[f] = numOps.Divide(varSum, numOps.FromDouble(batchSize));\n+                }\n+            }\n+            else\n+            {\n+                // Use running statistics for inference\n+                if (runningMean == null || runningVar == null)\n+                    throw new ArgumentException(\"Running statistics required for inference mode\");\n+\n+                batchMean = new T[features];\n+                batchVar = new T[features];\n+                for (int f = 0; f < features; f++)\n+                {\n+                    batchMean[f] = runningMean[f];\n+                    batchVar[f] = runningVar[f];\n+                }\n+            }\n+\n+            // Normalize and scale\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var std = numOps.Sqrt(numOps.Add(batchVar[f], eps));\n+                    var norm = numOps.Divide(\n+                        numOps.Subtract(a.Value[b, f], batchMean[f]),\n+                        std);\n+                    normalized[b, f] = norm;\n+                    result[b, f] = numOps.Add(\n+                        numOps.Multiply(norm, gamma.Value[f]),\n+                        beta.Value[f]);\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (!training)\n+                {\n+                    // Inference mode: simpler gradient (no batch statistics gradient)\n+                    if (a.RequiresGradient)\n+                    {\n+                        var gradA = new Tensor<T>(shape);\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            for (int f = 0; f < features; f++)\n+                            {\n+                                var std = numOps.Sqrt(numOps.Add(batchVar[f], eps));\n+                                var invStd = numOps.Divide(numOps.One, std);\n+                                gradA[b, f] = numOps.Multiply(\n+                                    numOps.Multiply(gradient[b, f], gamma.Value[f]),\n+                                    invStd);\n+                            }\n+                        }\n+\n+                        if (a.Gradient == null)\n+                            a.Gradient = gradA;\n+                        else\n+                            a.Gradient = a.Gradient.Add(gradA);\n+                    }\n+                    return;\n+                }\n+\n+                // Training mode: full gradient computation\n+                // Gradients for gamma and beta\n+                if (gamma.RequiresGradient)\n+                {\n+                    var gradGamma = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum,\n+                                numOps.Multiply(gradient[b, f], normalized[b, f]));\n+                        }\n+                        gradGamma[f] = sum;\n+                    }\n+\n+                    if (gamma.Gradient == null)\n+                        gamma.Gradient = gradGamma;\n+                    else\n+                        gamma.Gradient = gamma.Gradient.Add(gradGamma);\n+                }\n+\n+                if (beta.RequiresGradient)\n+                {\n+                    var gradBeta = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum, gradient[b, f]);\n+                        }\n+                        gradBeta[f] = sum;\n+                    }\n+\n+                    if (beta.Gradient == null)\n+                        beta.Gradient = gradBeta;\n+                    else\n+                        beta.Gradient = beta.Gradient.Add(gradBeta);\n+                }\n+\n+                // Gradient for input (complex due to batch statistics)\n+                if (a.RequiresGradient)\n+                {\n+                    var gradA = new Tensor<T>(shape);\n+                    var batchSizeT = numOps.FromDouble(batchSize);\n+\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var std = numOps.Sqrt(numOps.Add(batchVar[f], eps));\n+                        var invStd = numOps.Divide(numOps.One, std);\n+\n+                        // Sum of gradients and gradient*normalized\n+                        var gradSum = numOps.Zero;\n+                        var gradNormSum = numOps.Zero;\n+\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            var grad = numOps.Multiply(gradient[b, f], gamma.Value[f]);\n+                            gradSum = numOps.Add(gradSum, grad);\n+                            gradNormSum = numOps.Add(gradNormSum,\n+                                numOps.Multiply(grad, normalized[b, f]));\n+                        }\n+\n+                        // Apply gradient formula\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            var grad = numOps.Multiply(gradient[b, f], gamma.Value[f]);\n+\n+                            var term1 = grad;\n+                            var term2 = numOps.Divide(gradSum, batchSizeT);\n+                            var term3 = numOps.Divide(\n+                                numOps.Multiply(normalized[b, f], gradNormSum),\n+                                batchSizeT);\n+\n+                            var gradInput = numOps.Multiply(\n+                                numOps.Subtract(numOps.Subtract(term1, term2), term3),\n+                                invStd);\n+\n+                            gradA[b, f] = gradInput;\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var parents = new List<ComputationNode<T>> { a };\n+            if (gamma != null) parents.Add(gamma);\n+            if (beta != null) parents.Add(beta);\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient || (gamma?.RequiresGradient ?? false) || (beta?.RequiresGradient ?? false),\n+                parents: parents,\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"BatchNorm is currently only implemented for 2D tensors [batch, features]. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}]\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs 2D convolution on a 4D tensor (batch, channels, height, width).\n+    /// </summary>\n+    /// <param name=\"input\">The input node with shape [batch, inChannels, height, width].</param>\n+    /// <param name=\"kernel\">The kernel/filter with shape [outChannels, inChannels, kernelH, kernelW].</param>\n+    /// <param name=\"bias\">Optional bias with shape [outChannels]. If null, no bias is added.</param>\n+    /// <param name=\"stride\">The stride [strideH, strideW]. Default is [1, 1].</param>\n+    /// <param name=\"padding\">The padding [padH, padW]. Default is [0, 0].</param>\n+    /// <returns>A new computation node containing the convolution result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method performs 2D convolution, the fundamental operation in CNNs.\n+    /// Forward: Slides the kernel over the input computing dot products.\n+    /// Backward: Computes gradients for both input and kernel using transposed convolutions.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Conv2D is the core operation of convolutional neural networks.\n+    ///\n+    /// For 2D convolution:\n+    /// - The kernel \"slides\" over the input, computing weighted sums\n+    /// - Each output position is a dot product of the kernel with input patch\n+    /// - Stride controls how far the kernel moves each step\n+    /// - Padding adds borders to control output size\n+    ///\n+    /// Gradient computation:\n+    /// - Gradient w.r.t. input: \"full\" convolution with flipped kernel\n+    /// - Gradient w.r.t. kernel: cross-correlation between input and output gradient\n+    ///\n+    /// Used in:\n+    /// - All CNNs (image classification, object detection, segmentation)\n+    /// - Feature extraction in vision models\n+    /// - Learning spatial hierarchies\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Conv2D(\n+        ComputationNode<T> input,\n+        ComputationNode<T> kernel,\n+        ComputationNode<T>? bias = null,\n+        int[]? stride = null,\n+        int[]? padding = null)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var inputShape = input.Value.Shape;\n+        var kernelShape = kernel.Value.Shape;\n+\n+        if (inputShape.Length != 4)\n+            throw new ArgumentException(\"Conv2D requires 4D input [batch, inChannels, height, width]\");\n+        if (kernelShape.Length != 4)\n+            throw new ArgumentException(\"Conv2D requires 4D kernel [outChannels, inChannels, kernelH, kernelW]\");\n+\n+        stride ??= new int[] { 1, 1 };\n+        padding ??= new int[] { 0, 0 };\n+\n+        int batch = inputShape[0];\n+        int inChannels = inputShape[1];\n+        int inH = inputShape[2];\n+        int inW = inputShape[3];\n+\n+        int outChannels = kernelShape[0];\n+        int kernelInChannels = kernelShape[1];\n+        int kernelH = kernelShape[2];\n+        int kernelW = kernelShape[3];\n+\n+        if (inChannels != kernelInChannels)\n+            throw new ArgumentException($\"Input channels ({inChannels}) must match kernel input channels ({kernelInChannels})\");\n+\n+        int strideH = stride[0];\n+        int strideW = stride[1];\n+        int padH = padding[0];\n+        int padW = padding[1];\n+\n+        int outH = (inH + 2 * padH - kernelH) / strideH + 1;\n+        int outW = (inW + 2 * padW - kernelW) / strideW + 1;\n+\n+        var result = new Tensor<T>(new int[] { batch, outChannels, outH, outW });\n+\n+        // Forward pass: convolution\n+        for (int b = 0; b < batch; b++)\n+        {\n+            for (int oc = 0; oc < outChannels; oc++)\n+            {\n+                for (int oh = 0; oh < outH; oh++)\n+                {\n+                    for (int ow = 0; ow < outW; ow++)\n+                    {\n+                        var sum = numOps.Zero;\n+\n+                        // Convolve kernel over input\n+                        for (int ic = 0; ic < inChannels; ic++)\n+                        {\n+                            for (int kh = 0; kh < kernelH; kh++)\n+                            {\n+                                for (int kw = 0; kw < kernelW; kw++)\n+                                {\n+                                    int ih = oh * strideH + kh - padH;\n+                                    int iw = ow * strideW + kw - padW;\n+\n+                                    // Check bounds (padding)\n+                                    if (ih >= 0 && ih < inH && iw >= 0 && iw < inW)\n+                                    {\n+                                        var inputVal = input.Value[b, ic, ih, iw];\n+                                        var kernelVal = kernel.Value[oc, ic, kh, kw];\n+                                        sum = numOps.Add(sum, numOps.Multiply(inputVal, kernelVal));\n+                                    }\n+                                }\n+                            }\n+                        }\n+\n+                        // Add bias if provided\n+                        if (bias != null)\n+                        {\n+                            sum = numOps.Add(sum, bias.Value[oc]);\n+                        }\n+\n+                        result[b, oc, oh, ow] = sum;\n+                    }\n+                }\n+            }\n+        }\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            // Gradient w.r.t. input\n+            if (input.RequiresGradient)\n+            {\n+                var gradInput = new Tensor<T>(inputShape);\n+\n+                // Full convolution with flipped kernel\n+                for (int b = 0; b < batch; b++)\n+                {\n+                    for (int ic = 0; ic < inChannels; ic++)\n+                    {\n+                        for (int ih = 0; ih < inH; ih++)\n+                        {\n+                            for (int iw = 0; iw < inW; iw++)\n+                            {\n+                                var sum = numOps.Zero;\n+\n+                                // Iterate over all output positions that used this input position\n+                                for (int oc = 0; oc < outChannels; oc++)\n+                                {\n+                                    for (int kh = 0; kh < kernelH; kh++)\n+                                    {\n+                                        for (int kw = 0; kw < kernelW; kw++)\n+                                        {\n+                                            // Compute output position\n+                                            int ohShifted = ih + padH - kh;\n+                                            int owShifted = iw + padW - kw;\n+\n+                                            if (ohShifted % strideH == 0 && owShifted % strideW == 0)\n+                                            {\n+                                                int oh = ohShifted / strideH;\n+                                                int ow = owShifted / strideW;\n+\n+                                                if (oh >= 0 && oh < outH && ow >= 0 && ow < outW)\n+                                                {\n+                                                    var gradVal = gradient[b, oc, oh, ow];\n+                                                    var kernelVal = kernel.Value[oc, ic, kh, kw];\n+                                                    sum = numOps.Add(sum, numOps.Multiply(gradVal, kernelVal));\n+                                                }\n+                                            }\n+                                        }\n+                                    }\n+                                }\n+\n+                                gradInput[b, ic, ih, iw] = sum;\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (input.Gradient == null)\n+                    input.Gradient = gradInput;\n+                else\n+                    input.Gradient = input.Gradient.Add(gradInput);"}]}},{"id":"PRRT_kwDOKSXUF85hck-_","isResolved":false,"isOutdated":false,"path":"src/Autodiff/TensorOperations.cs","line":2676,"originalLine":2676,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzD_c","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement write to the same variable - consider using '?' to express intent better.","createdAt":"2025-11-11T07:53:26Z","path":"src/Autodiff/TensorOperations.cs","line":2676,"originalLine":2676,"diffHunk":"@@ -1219,4 +1219,1769 @@ void BackwardFunction(Tensor<T> gradient)\n \n         return node;\n     }\n+\n+    /// <summary>\n+    /// Computes the softmax function for a computation node along a specified axis.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"axis\">The axis along which to compute softmax. Default is -1 (last axis).</param>\n+    /// <returns>A new computation node containing the softmax result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method computes softmax (σ(x_i) = exp(x_i) / Σexp(x_j)) along the specified axis.\n+    /// Uses numerical stability trick: subtract max before exponentiating.\n+    /// The backward function uses: ∂softmax/∂x = softmax(x) * (grad - Σ(grad * softmax(x))).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Softmax converts a vector of numbers into probabilities.\n+    ///\n+    /// For softmax:\n+    /// - The forward pass exponentiates each element, then normalizes so they sum to 1\n+    /// - The result is a probability distribution (all values between 0 and 1, summing to 1)\n+    /// - The backward pass is complex but efficient: uses the Jacobian of softmax\n+    ///\n+    /// Softmax is crucial for:\n+    /// - Multi-class classification (final layer outputs)\n+    /// - Attention mechanisms (computing attention weights)\n+    /// - Anywhere you need to convert scores to probabilities\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Softmax(ComputationNode<T> a, int axis = -1)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        // Normalize axis to positive index\n+        if (axis < 0)\n+            axis = shape.Length + axis;\n+\n+        // For simplicity, handle 2D case (batch, features) with axis=-1\n+        if (shape.Length == 2 && axis == 1)\n+        {\n+            int batchSize = shape[0];\n+            int features = shape[1];\n+            var result = new Tensor<T>(shape);\n+\n+            // Compute softmax for each row\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                // Find max for numerical stability\n+                var maxVal = a.Value[b, 0];\n+                for (int f = 1; f < features; f++)\n+                {\n+                    if (numOps.GreaterThan(a.Value[b, f], maxVal))\n+                        maxVal = a.Value[b, f];\n+                }\n+\n+                // Compute exp(x - max) and sum\n+                var expSum = numOps.Zero;\n+                var expValues = new T[features];\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var shifted = numOps.Subtract(a.Value[b, f], maxVal);\n+                    expValues[f] = numOps.Exp(shifted);\n+                    expSum = numOps.Add(expSum, expValues[f]);\n+                }\n+\n+                // Normalize\n+                for (int f = 0; f < features; f++)\n+                {\n+                    result[b, f] = numOps.Divide(expValues[f], expSum);\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (a.RequiresGradient)\n+                {\n+                    // ∂softmax/∂x_i = softmax_i * (∂L/∂y_i - Σ_j(∂L/∂y_j * softmax_j))\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        // Compute sum of (gradient * softmax)\n+                        var dotProduct = numOps.Zero;\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            dotProduct = numOps.Add(dotProduct,\n+                                numOps.Multiply(gradient[b, f], result[b, f]));\n+                        }\n+\n+                        // Compute gradient for each element\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            var gradMinusDot = numOps.Subtract(gradient[b, f], dotProduct);\n+                            gradA[b, f] = numOps.Multiply(result[b, f], gradMinusDot);\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient,\n+                parents: new List<ComputationNode<T>> { a },\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Softmax is currently only implemented for 2D tensors along axis=-1. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}], axis={axis}\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Concatenates multiple computation nodes along a specified axis.\n+    /// </summary>\n+    /// <param name=\"nodes\">The list of nodes to concatenate.</param>\n+    /// <param name=\"axis\">The axis along which to concatenate. Default is 0.</param>\n+    /// <returns>A new computation node containing the concatenated result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method concatenates tensors along the specified axis.\n+    /// All tensors must have the same shape except along the concatenation axis.\n+    /// The backward function splits the gradient and sends each portion to the corresponding input.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Concat stacks tensors together along a dimension.\n+    ///\n+    /// For concatenation:\n+    /// - The forward pass combines multiple tensors into one larger tensor\n+    /// - The backward pass splits the gradient back to each input\n+    /// - Think of it like gluing arrays together end-to-end\n+    ///\n+    /// Used in:\n+    /// - Skip connections (concatenating features from different layers)\n+    /// - Multi-input architectures\n+    /// - Feature fusion in neural networks\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Concat(List<ComputationNode<T>> nodes, int axis = 0)\n+    {\n+        if (nodes.Count == 0)\n+            throw new ArgumentException(\"Cannot concatenate empty list of nodes\");\n+\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var firstShape = nodes[0].Value.Shape;\n+\n+        // Normalize axis\n+        if (axis < 0)\n+            axis = firstShape.Length + axis;\n+\n+        // Validate shapes match except on concat axis\n+        for (int i = 1; i < nodes.Count; i++)\n+        {\n+            var shape = nodes[i].Value.Shape;\n+            if (shape.Length != firstShape.Length)\n+                throw new ArgumentException(\"All tensors must have the same rank\");\n+\n+            for (int d = 0; d < firstShape.Length; d++)\n+            {\n+                if (d != axis && shape[d] != firstShape[d])\n+                    throw new ArgumentException(\n+                        $\"Shape mismatch at dimension {d}: {shape[d]} vs {firstShape[d]}\");\n+            }\n+        }\n+\n+        // Compute output shape\n+        int[] outputShape = (int[])firstShape.Clone();\n+        for (int i = 1; i < nodes.Count; i++)\n+        {\n+            outputShape[axis] += nodes[i].Value.Shape[axis];\n+        }\n+\n+        // Perform concatenation (handle 2D case for simplicity)\n+        Tensor<T> result;\n+        if (firstShape.Length == 2 && axis == 1)\n+        {\n+            // Concatenate along columns (features)\n+            int rows = firstShape[0];\n+            int totalCols = outputShape[1];\n+            result = new Tensor<T>(new int[] { rows, totalCols });\n+\n+            int colOffset = 0;\n+            foreach (var node in nodes)\n+            {\n+                int cols = node.Value.Shape[1];\n+                for (int r = 0; r < rows; r++)\n+                {\n+                    for (int c = 0; c < cols; c++)\n+                    {\n+                        result[r, colOffset + c] = node.Value[r, c];\n+                    }\n+                }\n+                colOffset += cols;\n+            }\n+        }\n+        else if (firstShape.Length == 2 && axis == 0)\n+        {\n+            // Concatenate along rows (batch)\n+            int cols = firstShape[1];\n+            int totalRows = outputShape[0];\n+            result = new Tensor<T>(new int[] { totalRows, cols });\n+\n+            int rowOffset = 0;\n+            foreach (var node in nodes)\n+            {\n+                int rows = node.Value.Shape[0];\n+                for (int r = 0; r < rows; r++)\n+                {\n+                    for (int c = 0; c < cols; c++)\n+                    {\n+                        result[rowOffset + r, c] = node.Value[r, c];\n+                    }\n+                }\n+                rowOffset += rows;\n+            }\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Concat is currently only implemented for 2D tensors. \" +\n+                $\"Got shape=[{string.Join(\", \", firstShape)}]\");\n+        }\n+\n+        // Store sizes for gradient splitting\n+        var sizes = nodes.Select(n => n.Value.Shape[axis]).ToList();\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            // Split gradient along concat axis and distribute to inputs\n+            if (firstShape.Length == 2 && axis == 1)\n+            {\n+                int rows = firstShape[0];\n+                int colOffset = 0;\n+                for (int i = 0; i < nodes.Count; i++)\n+                {\n+                    if (!nodes[i].RequiresGradient)\n+                    {\n+                        colOffset += sizes[i];\n+                        continue;\n+                    }\n+\n+                    int cols = sizes[i];\n+                    var gradPart = new Tensor<T>(new int[] { rows, cols });\n+\n+                    for (int r = 0; r < rows; r++)\n+                    {\n+                        for (int c = 0; c < cols; c++)\n+                        {\n+                            gradPart[r, c] = gradient[r, colOffset + c];\n+                        }\n+                    }\n+\n+                    if (nodes[i].Gradient == null)\n+                        nodes[i].Gradient = gradPart;\n+                    else\n+                        nodes[i].Gradient = nodes[i].Gradient.Add(gradPart);\n+\n+                    colOffset += cols;\n+                }\n+            }\n+            else if (firstShape.Length == 2 && axis == 0)\n+            {\n+                int cols = firstShape[1];\n+                int rowOffset = 0;\n+                for (int i = 0; i < nodes.Count; i++)\n+                {\n+                    if (!nodes[i].RequiresGradient)\n+                    {\n+                        rowOffset += sizes[i];\n+                        continue;\n+                    }\n+\n+                    int rows = sizes[i];\n+                    var gradPart = new Tensor<T>(new int[] { rows, cols });\n+\n+                    for (int r = 0; r < rows; r++)\n+                    {\n+                        for (int c = 0; c < cols; c++)\n+                        {\n+                            gradPart[r, c] = gradient[rowOffset + r, c];\n+                        }\n+                    }\n+\n+                    if (nodes[i].Gradient == null)\n+                        nodes[i].Gradient = gradPart;\n+                    else\n+                        nodes[i].Gradient = nodes[i].Gradient.Add(gradPart);\n+\n+                    rowOffset += rows;\n+                }\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: nodes.Any(n => n.RequiresGradient),\n+            parents: nodes,\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Pads a tensor with a constant value along specified dimensions.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"padWidth\">Padding width for each dimension as (before, after) pairs.</param>\n+    /// <param name=\"value\">The value to use for padding. Default is zero.</param>\n+    /// <returns>A new computation node containing the padded result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method adds padding around the tensor.\n+    /// The backward function simply crops the gradient back to the original size (gradients for padding are zero).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Pad adds extra elements around a tensor.\n+    ///\n+    /// For padding:\n+    /// - The forward pass adds border elements with a constant value\n+    /// - The backward pass removes those border gradients (they don't affect the original tensor)\n+    /// - Think of it like adding margins to an image\n+    ///\n+    /// Used in:\n+    /// - Convolutional layers (to maintain spatial dimensions)\n+    /// - Handling variable-length sequences\n+    /// - Data augmentation\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Pad(ComputationNode<T> a, int[,] padWidth, T? value = default)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var padValue = value ?? numOps.Zero;\n+        var shape = a.Value.Shape;\n+\n+        // Validate padWidth dimensions\n+        if (padWidth.GetLength(0) != shape.Length)\n+            throw new ArgumentException(\"padWidth must have same number of dimensions as input tensor\");\n+\n+        // Compute output shape\n+        var outputShape = new int[shape.Length];\n+        for (int d = 0; d < shape.Length; d++)\n+        {\n+            outputShape[d] = shape[d] + padWidth[d, 0] + padWidth[d, 1];\n+        }\n+\n+        // Handle 2D case\n+        if (shape.Length == 2)\n+        {\n+            int inputRows = shape[0];\n+            int inputCols = shape[1];\n+            int padTop = padWidth[0, 0];\n+            int padBottom = padWidth[0, 1];\n+            int padLeft = padWidth[1, 0];\n+            int padRight = padWidth[1, 1];\n+\n+            var result = new Tensor<T>(outputShape);\n+\n+            // Initialize with pad value\n+            for (int i = 0; i < result.Length; i++)\n+            {\n+                result[i] = padValue;\n+            }\n+\n+            // Copy input data to center\n+            for (int r = 0; r < inputRows; r++)\n+            {\n+                for (int c = 0; c < inputCols; c++)\n+                {\n+                    result[padTop + r, padLeft + c] = a.Value[r, c];\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (a.RequiresGradient)\n+                {\n+                    // Extract gradient for original (unpadded) region\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int r = 0; r < inputRows; r++)\n+                    {\n+                        for (int c = 0; c < inputCols; c++)\n+                        {\n+                            gradA[r, c] = gradient[padTop + r, padLeft + c];\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient,\n+                parents: new List<ComputationNode<T>> { a },\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Pad is currently only implemented for 2D tensors. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}]\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs 2D max pooling on a 4D tensor (batch, channels, height, width).\n+    /// </summary>\n+    /// <param name=\"a\">The input node with shape [batch, channels, height, width].</param>\n+    /// <param name=\"poolSize\">The size of the pooling window [poolH, poolW].</param>\n+    /// <param name=\"strides\">The stride for the pooling operation [strideH, strideW]. If null, uses poolSize.</param>\n+    /// <returns>A new computation node containing the max pooled result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method performs max pooling over 2D spatial dimensions.\n+    /// During forward pass, it tracks which element was the max for routing gradients during backward pass.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> MaxPool downsamples by taking the maximum value in each window.\n+    ///\n+    /// For max pooling:\n+    /// - The forward pass slides a window and takes the max value in each position\n+    /// - This reduces spatial dimensions (downsampling)\n+    /// - The backward pass routes gradients only to the positions that were max\n+    /// - Other positions get zero gradient (they didn't contribute to the output)\n+    ///\n+    /// Used in:\n+    /// - CNNs for translation invariance\n+    /// - Reducing spatial resolution\n+    /// - Building hierarchical features\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> MaxPool2D(\n+        ComputationNode<T> a,\n+        int[] poolSize,\n+        int[]? strides = null)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        if (shape.Length != 4)\n+            throw new ArgumentException(\"MaxPool2D requires 4D input [batch, channels, height, width]\");\n+\n+        strides ??= poolSize;\n+\n+        int batch = shape[0];\n+        int channels = shape[1];\n+        int inH = shape[2];\n+        int inW = shape[3];\n+        int poolH = poolSize[0];\n+        int poolW = poolSize[1];\n+        int strideH = strides[0];\n+        int strideW = strides[1];\n+\n+        int outH = (inH - poolH) / strideH + 1;\n+        int outW = (inW - poolW) / strideW + 1;\n+\n+        var result = new Tensor<T>(new int[] { batch, channels, outH, outW });\n+        // Store max positions for backprop\n+        var maxPositions = new int[batch, channels, outH, outW, 2]; // [h_offset, w_offset]\n+\n+        // Forward pass: compute max pooling and track positions\n+        for (int b = 0; b < batch; b++)\n+        {\n+            for (int c = 0; c < channels; c++)\n+            {\n+                for (int oh = 0; oh < outH; oh++)\n+                {\n+                    for (int ow = 0; ow < outW; ow++)\n+                    {\n+                        int hStart = oh * strideH;\n+                        int wStart = ow * strideW;\n+\n+                        var maxVal = a.Value[b * channels * inH * inW +\n+                                           c * inH * inW +\n+                                           hStart * inW +\n+                                           wStart];\n+                        int maxHOffset = 0;\n+                        int maxWOffset = 0;\n+\n+                        // Find max in pooling window\n+                        for (int ph = 0; ph < poolH; ph++)\n+                        {\n+                            for (int pw = 0; pw < poolW; pw++)\n+                            {\n+                                int h = hStart + ph;\n+                                int w = wStart + pw;\n+                                if (h < inH && w < inW)\n+                                {\n+                                    var val = a.Value[b * channels * inH * inW +\n+                                                     c * inH * inW +\n+                                                     h * inW +\n+                                                     w];\n+                                    if (numOps.GreaterThan(val, maxVal))\n+                                    {\n+                                        maxVal = val;\n+                                        maxHOffset = ph;\n+                                        maxWOffset = pw;\n+                                    }\n+                                }\n+                            }\n+                        }\n+\n+                        result[b, c, oh, ow] = maxVal;\n+                        maxPositions[b, c, oh, ow, 0] = maxHOffset;\n+                        maxPositions[b, c, oh, ow, 1] = maxWOffset;\n+                    }\n+                }\n+            }\n+        }\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            if (a.RequiresGradient)\n+            {\n+                var gradA = new Tensor<T>(shape);\n+\n+                // Route gradients to max positions\n+                for (int b = 0; b < batch; b++)\n+                {\n+                    for (int c = 0; c < channels; c++)\n+                    {\n+                        for (int oh = 0; oh < outH; oh++)\n+                        {\n+                            for (int ow = 0; ow < outW; ow++)\n+                            {\n+                                int hStart = oh * strideH;\n+                                int wStart = ow * strideW;\n+                                int maxHOffset = maxPositions[b, c, oh, ow, 0];\n+                                int maxWOffset = maxPositions[b, c, oh, ow, 1];\n+\n+                                int maxH = hStart + maxHOffset;\n+                                int maxW = wStart + maxWOffset;\n+\n+                                int gradIdx = b * channels * inH * inW +\n+                                             c * inH * inW +\n+                                            maxH * inW +\n+                                             maxW;\n+\n+                                gradA[gradIdx] = numOps.Add(gradA[gradIdx], gradient[b, c, oh, ow]);\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (a.Gradient == null)\n+                    a.Gradient = gradA;\n+                else\n+                    a.Gradient = a.Gradient.Add(gradA);\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: a.RequiresGradient,\n+            parents: new List<ComputationNode<T>> { a },\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Performs 2D average pooling on a 4D tensor (batch, channels, height, width).\n+    /// </summary>\n+    /// <param name=\"a\">The input node with shape [batch, channels, height, width].</param>\n+    /// <param name=\"poolSize\">The size of the pooling window [poolH, poolW].</param>\n+    /// <param name=\"strides\">The stride for the pooling operation [strideH, strideW]. If null, uses poolSize.</param>\n+    /// <returns>A new computation node containing the average pooled result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method performs average pooling over 2D spatial dimensions.\n+    /// The backward function distributes gradients equally across the pooling window.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> AvgPool downsamples by taking the average value in each window.\n+    ///\n+    /// For average pooling:\n+    /// - The forward pass slides a window and computes the average\n+    /// - This smoothly reduces spatial dimensions\n+    /// - The backward pass distributes gradients equally to all elements in the window\n+    /// - Each element gets gradient / pool_area\n+    ///\n+    /// Used in:\n+    /// - CNNs for smoother downsampling than max pooling\n+    /// - Global average pooling (replacing fully connected layers)\n+    /// - Reducing overfitting\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> AvgPool2D(\n+        ComputationNode<T> a,\n+        int[] poolSize,\n+        int[]? strides = null)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        if (shape.Length != 4)\n+            throw new ArgumentException(\"AvgPool2D requires 4D input [batch, channels, height, width]\");\n+\n+        strides ??= poolSize;\n+\n+        int batch = shape[0];\n+        int channels = shape[1];\n+        int inH = shape[2];\n+        int inW = shape[3];\n+        int poolH = poolSize[0];\n+        int poolW = poolSize[1];\n+        int strideH = strides[0];\n+        int strideW = strides[1];\n+\n+        int outH = (inH - poolH) / strideH + 1;\n+        int outW = (inW - poolW) / strideW + 1;\n+\n+        var result = new Tensor<T>(new int[] { batch, channels, outH, outW });\n+        var poolArea = numOps.FromDouble(poolH * poolW);\n+\n+        // Forward pass: compute average pooling\n+        for (int b = 0; b < batch; b++)\n+        {\n+            for (int c = 0; c < channels; c++)\n+            {\n+                for (int oh = 0; oh < outH; oh++)\n+                {\n+                    for (int ow = 0; ow < outW; ow++)\n+                    {\n+                        int hStart = oh * strideH;\n+                        int wStart = ow * strideW;\n+\n+                        var sum = numOps.Zero;\n+\n+                        // Sum values in pooling window\n+                        for (int ph = 0; ph < poolH; ph++)\n+                        {\n+                            for (int pw = 0; pw < poolW; pw++)\n+                            {\n+                                int h = hStart + ph;\n+                                int w = wStart + pw;\n+                                if (h < inH && w < inW)\n+                                {\n+                                    sum = numOps.Add(sum, a.Value[b, c, h, w]);\n+                                }\n+                            }\n+                        }\n+\n+                        result[b, c, oh, ow] = numOps.Divide(sum, poolArea);\n+                    }\n+                }\n+            }\n+        }\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            if (a.RequiresGradient)\n+            {\n+                var gradA = new Tensor<T>(shape);\n+\n+                // Distribute gradients equally across pooling windows\n+                for (int b = 0; b < batch; b++)\n+                {\n+                    for (int c = 0; c < channels; c++)\n+                    {\n+                        for (int oh = 0; oh < outH; oh++)\n+                        {\n+                            for (int ow = 0; ow < outW; ow++)\n+                            {\n+                                int hStart = oh * strideH;\n+                                int wStart = ow * strideW;\n+\n+                                var gradValue = numOps.Divide(gradient[b, c, oh, ow], poolArea);\n+\n+                                // Distribute to all elements in window\n+                                for (int ph = 0; ph < poolH; ph++)\n+                                {\n+                                    for (int pw = 0; pw < poolW; pw++)\n+                                    {\n+                                        int h = hStart + ph;\n+                                        int w = wStart + pw;\n+                                        if (h < inH && w < inW)\n+                                        {\n+                                            gradA[b, c, h, w] = numOps.Add(gradA[b, c, h, w], gradValue);\n+                                        }\n+                                    }\n+                                }\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (a.Gradient == null)\n+                    a.Gradient = gradA;\n+                else\n+                    a.Gradient = a.Gradient.Add(gradA);\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: a.RequiresGradient,\n+            parents: new List<ComputationNode<T>> { a },\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Applies layer normalization to a computation node.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"normalizedShape\">The shape over which to normalize (typically the feature dimensions).</param>\n+    /// <param name=\"gamma\">Optional scale parameter (learnable). If null, uses ones.</param>\n+    /// <param name=\"beta\">Optional shift parameter (learnable). If null, uses zeros.</param>\n+    /// <param name=\"epsilon\">Small constant for numerical stability. Default is 1e-5.</param>\n+    /// <returns>A new computation node containing the layer normalized result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Layer normalization normalizes inputs across the feature dimension for each sample independently.\n+    /// Formula: y = gamma * (x - mean) / sqrt(variance + epsilon) + beta\n+    /// Unlike batch normalization, this doesn't depend on batch statistics.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> LayerNorm standardizes features for each sample independently.\n+    ///\n+    /// For layer normalization:\n+    /// - Computes mean and variance for each sample's features\n+    /// - Normalizes: (x - mean) / sqrt(variance)\n+    /// - Scales and shifts: result * gamma + beta\n+    /// - Works the same during training and inference (no batch dependency)\n+    ///\n+    /// Used in:\n+    /// - Transformers (critical component)\n+    /// - RNNs (stabilizes training)\n+    /// - Any architecture needing sample-independent normalization\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> LayerNorm(\n+        ComputationNode<T> a,\n+        int[] normalizedShape,\n+        ComputationNode<T>? gamma = null,\n+        ComputationNode<T>? beta = null,\n+        double epsilon = 1e-5)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+        var eps = numOps.FromDouble(epsilon);\n+\n+        // For 2D input [batch, features], normalize over features\n+        if (shape.Length == 2 && normalizedShape.Length == 1 && normalizedShape[0] == shape[1])\n+        {\n+            int batchSize = shape[0];\n+            int features = shape[1];\n+\n+            // Create default gamma (ones) and beta (zeros) if not provided\n+            if (gamma == null)\n+            {\n+                var gammaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    gammaTensor[i] = numOps.One;\n+                gamma = Variable(gammaTensor, requiresGradient: false);\n+            }\n+\n+            if (beta == null)\n+            {\n+                var betaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    betaTensor[i] = numOps.Zero;\n+                beta = Variable(betaTensor, requiresGradient: false);\n+            }\n+\n+            var result = new Tensor<T>(shape);\n+            var means = new T[batchSize];\n+            var variances = new T[batchSize];\n+            var normalized = new Tensor<T>(shape);\n+\n+            // Forward pass: compute mean and variance per sample\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                // Compute mean\n+                var sum = numOps.Zero;\n+                for (int f = 0; f < features; f++)\n+                {\n+                    sum = numOps.Add(sum, a.Value[b, f]);\n+                }\n+                means[b] = numOps.Divide(sum, numOps.FromDouble(features));\n+\n+                // Compute variance\n+                var varSum = numOps.Zero;\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var diff = numOps.Subtract(a.Value[b, f], means[b]);\n+                    varSum = numOps.Add(varSum, numOps.Multiply(diff, diff));\n+                }\n+                variances[b] = numOps.Divide(varSum, numOps.FromDouble(features));\n+\n+                // Normalize and scale\n+                var std = numOps.Sqrt(numOps.Add(variances[b], eps));\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var norm = numOps.Divide(\n+                        numOps.Subtract(a.Value[b, f], means[b]),\n+                        std);\n+                    normalized[b, f] = norm;\n+                    result[b, f] = numOps.Add(\n+                        numOps.Multiply(norm, gamma.Value[f]),\n+                        beta.Value[f]);\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                // Gradients for gamma and beta\n+                if (gamma.RequiresGradient)\n+                {\n+                    var gradGamma = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum,\n+                                numOps.Multiply(gradient[b, f], normalized[b, f]));\n+                        }\n+                        gradGamma[f] = sum;\n+                    }\n+\n+                    if (gamma.Gradient == null)\n+                        gamma.Gradient = gradGamma;\n+                    else\n+                        gamma.Gradient = gamma.Gradient.Add(gradGamma);\n+                }\n+\n+                if (beta.RequiresGradient)\n+                {\n+                    var gradBeta = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum, gradient[b, f]);\n+                        }\n+                        gradBeta[f] = sum;\n+                    }\n+\n+                    if (beta.Gradient == null)\n+                        beta.Gradient = gradBeta;\n+                    else\n+                        beta.Gradient = beta.Gradient.Add(gradBeta);\n+                }\n+\n+                // Gradient for input\n+                if (a.RequiresGradient)\n+                {\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        var std = numOps.Sqrt(numOps.Add(variances[b], eps));\n+                        var invStd = numOps.Divide(numOps.One, std);\n+\n+                        // Compute gradient components\n+                        var gradNormSum = numOps.Zero;\n+                        var gradNormDotNorm = numOps.Zero;\n+\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            var gradNorm = numOps.Multiply(gradient[b, f], gamma.Value[f]);\n+                            gradNormSum = numOps.Add(gradNormSum, gradNorm);\n+                            gradNormDotNorm = numOps.Add(gradNormDotNorm,\n+                                numOps.Multiply(gradNorm, normalized[b, f]));\n+                        }\n+\n+                        // Apply gradient formula\n+                        var featuresT = numOps.FromDouble(features);\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            var gradNorm = numOps.Multiply(gradient[b, f], gamma.Value[f]);\n+\n+                            var term1 = gradNorm;\n+                            var term2 = numOps.Divide(gradNormSum, featuresT);\n+                            var term3 = numOps.Divide(\n+                                numOps.Multiply(normalized[b, f], gradNormDotNorm),\n+                                featuresT);\n+\n+                            var grad = numOps.Multiply(\n+                                numOps.Subtract(numOps.Subtract(term1, term2), term3),\n+                                invStd);\n+\n+                            gradA[b, f] = grad;\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var parents = new List<ComputationNode<T>> { a };\n+            if (gamma != null) parents.Add(gamma);\n+            if (beta != null) parents.Add(beta);\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient || (gamma?.RequiresGradient ?? false) || (beta?.RequiresGradient ?? false),\n+                parents: parents,\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"LayerNorm is currently only implemented for 2D tensors normalizing over last dimension. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}], normalizedShape=[{string.Join(\", \", normalizedShape)}]\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Applies batch normalization to a computation node.\n+    /// </summary>\n+    /// <param name=\"a\">The input node with shape [batch, features].</param>\n+    /// <param name=\"gamma\">Optional scale parameter (learnable). If null, uses ones.</param>\n+    /// <param name=\"beta\">Optional shift parameter (learnable). If null, uses zeros.</param>\n+    /// <param name=\"runningMean\">Running mean for inference (not updated during this operation).</param>\n+    /// <param name=\"runningVar\">Running variance for inference (not updated during this operation).</param>\n+    /// <param name=\"training\">Whether in training mode (uses batch statistics) or inference mode (uses running statistics).</param>\n+    /// <param name=\"epsilon\">Small constant for numerical stability. Default is 1e-5.</param>\n+    /// <returns>A new computation node containing the batch normalized result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Batch normalization normalizes inputs across the batch dimension.\n+    /// During training: Uses batch statistics (mean and variance computed from current batch).\n+    /// During inference: Uses running statistics (accumulated during training).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> BatchNorm standardizes features across the batch.\n+    ///\n+    /// For batch normalization:\n+    /// - Training mode: Uses current batch's mean and variance\n+    /// - Inference mode: Uses running mean/variance from training\n+    /// - Normalizes: (x - mean) / sqrt(variance)\n+    /// - Scales and shifts: result * gamma + beta\n+    ///\n+    /// Benefits:\n+    /// - Stabilizes training (reduces internal covariate shift)\n+    /// - Allows higher learning rates\n+    /// - Acts as regularization\n+    ///\n+    /// Used in:\n+    /// - CNNs (after convolutional layers)\n+    /// - Deep feedforward networks\n+    /// - GANs and many other architectures\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> BatchNorm(\n+        ComputationNode<T> a,\n+        ComputationNode<T>? gamma = null,\n+        ComputationNode<T>? beta = null,\n+        Tensor<T>? runningMean = null,\n+        Tensor<T>? runningVar = null,\n+        bool training = true,\n+        double epsilon = 1e-5)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+        var eps = numOps.FromDouble(epsilon);\n+\n+        // Handle 2D case [batch, features]\n+        if (shape.Length == 2)\n+        {\n+            int batchSize = shape[0];\n+            int features = shape[1];\n+\n+            // Create default gamma and beta if not provided\n+            if (gamma == null)\n+            {\n+                var gammaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    gammaTensor[i] = numOps.One;\n+                gamma = Variable(gammaTensor, requiresGradient: false);\n+            }\n+\n+            if (beta == null)\n+            {\n+                var betaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    betaTensor[i] = numOps.Zero;\n+                beta = Variable(betaTensor, requiresGradient: false);\n+            }\n+\n+            var result = new Tensor<T>(shape);\n+            T[] batchMean;\n+            T[] batchVar;\n+            var normalized = new Tensor<T>(shape);\n+\n+            if (training)\n+            {\n+                // Compute batch statistics\n+                batchMean = new T[features];\n+                batchVar = new T[features];\n+\n+                // Compute mean per feature\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var sum = numOps.Zero;\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        sum = numOps.Add(sum, a.Value[b, f]);\n+                    }\n+                    batchMean[f] = numOps.Divide(sum, numOps.FromDouble(batchSize));\n+                }\n+\n+                // Compute variance per feature\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var varSum = numOps.Zero;\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        var diff = numOps.Subtract(a.Value[b, f], batchMean[f]);\n+                        varSum = numOps.Add(varSum, numOps.Multiply(diff, diff));\n+                    }\n+                    batchVar[f] = numOps.Divide(varSum, numOps.FromDouble(batchSize));\n+                }\n+            }\n+            else\n+            {\n+                // Use running statistics for inference\n+                if (runningMean == null || runningVar == null)\n+                    throw new ArgumentException(\"Running statistics required for inference mode\");\n+\n+                batchMean = new T[features];\n+                batchVar = new T[features];\n+                for (int f = 0; f < features; f++)\n+                {\n+                    batchMean[f] = runningMean[f];\n+                    batchVar[f] = runningVar[f];\n+                }\n+            }\n+\n+            // Normalize and scale\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var std = numOps.Sqrt(numOps.Add(batchVar[f], eps));\n+                    var norm = numOps.Divide(\n+                        numOps.Subtract(a.Value[b, f], batchMean[f]),\n+                        std);\n+                    normalized[b, f] = norm;\n+                    result[b, f] = numOps.Add(\n+                        numOps.Multiply(norm, gamma.Value[f]),\n+                        beta.Value[f]);\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (!training)\n+                {\n+                    // Inference mode: simpler gradient (no batch statistics gradient)\n+                    if (a.RequiresGradient)\n+                    {\n+                        var gradA = new Tensor<T>(shape);\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            for (int f = 0; f < features; f++)\n+                            {\n+                                var std = numOps.Sqrt(numOps.Add(batchVar[f], eps));\n+                                var invStd = numOps.Divide(numOps.One, std);\n+                                gradA[b, f] = numOps.Multiply(\n+                                    numOps.Multiply(gradient[b, f], gamma.Value[f]),\n+                                    invStd);\n+                            }\n+                        }\n+\n+                        if (a.Gradient == null)\n+                            a.Gradient = gradA;\n+                        else\n+                            a.Gradient = a.Gradient.Add(gradA);\n+                    }\n+                    return;\n+                }\n+\n+                // Training mode: full gradient computation\n+                // Gradients for gamma and beta\n+                if (gamma.RequiresGradient)\n+                {\n+                    var gradGamma = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum,\n+                                numOps.Multiply(gradient[b, f], normalized[b, f]));\n+                        }\n+                        gradGamma[f] = sum;\n+                    }\n+\n+                    if (gamma.Gradient == null)\n+                        gamma.Gradient = gradGamma;\n+                    else\n+                        gamma.Gradient = gamma.Gradient.Add(gradGamma);\n+                }\n+\n+                if (beta.RequiresGradient)\n+                {\n+                    var gradBeta = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum, gradient[b, f]);\n+                        }\n+                        gradBeta[f] = sum;\n+                    }\n+\n+                    if (beta.Gradient == null)\n+                        beta.Gradient = gradBeta;\n+                    else\n+                        beta.Gradient = beta.Gradient.Add(gradBeta);\n+                }\n+\n+                // Gradient for input (complex due to batch statistics)\n+                if (a.RequiresGradient)\n+                {\n+                    var gradA = new Tensor<T>(shape);\n+                    var batchSizeT = numOps.FromDouble(batchSize);\n+\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var std = numOps.Sqrt(numOps.Add(batchVar[f], eps));\n+                        var invStd = numOps.Divide(numOps.One, std);\n+\n+                        // Sum of gradients and gradient*normalized\n+                        var gradSum = numOps.Zero;\n+                        var gradNormSum = numOps.Zero;\n+\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            var grad = numOps.Multiply(gradient[b, f], gamma.Value[f]);\n+                            gradSum = numOps.Add(gradSum, grad);\n+                            gradNormSum = numOps.Add(gradNormSum,\n+                                numOps.Multiply(grad, normalized[b, f]));\n+                        }\n+\n+                        // Apply gradient formula\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            var grad = numOps.Multiply(gradient[b, f], gamma.Value[f]);\n+\n+                            var term1 = grad;\n+                            var term2 = numOps.Divide(gradSum, batchSizeT);\n+                            var term3 = numOps.Divide(\n+                                numOps.Multiply(normalized[b, f], gradNormSum),\n+                                batchSizeT);\n+\n+                            var gradInput = numOps.Multiply(\n+                                numOps.Subtract(numOps.Subtract(term1, term2), term3),\n+                                invStd);\n+\n+                            gradA[b, f] = gradInput;\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var parents = new List<ComputationNode<T>> { a };\n+            if (gamma != null) parents.Add(gamma);\n+            if (beta != null) parents.Add(beta);\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient || (gamma?.RequiresGradient ?? false) || (beta?.RequiresGradient ?? false),\n+                parents: parents,\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"BatchNorm is currently only implemented for 2D tensors [batch, features]. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}]\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs 2D convolution on a 4D tensor (batch, channels, height, width).\n+    /// </summary>\n+    /// <param name=\"input\">The input node with shape [batch, inChannels, height, width].</param>\n+    /// <param name=\"kernel\">The kernel/filter with shape [outChannels, inChannels, kernelH, kernelW].</param>\n+    /// <param name=\"bias\">Optional bias with shape [outChannels]. If null, no bias is added.</param>\n+    /// <param name=\"stride\">The stride [strideH, strideW]. Default is [1, 1].</param>\n+    /// <param name=\"padding\">The padding [padH, padW]. Default is [0, 0].</param>\n+    /// <returns>A new computation node containing the convolution result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method performs 2D convolution, the fundamental operation in CNNs.\n+    /// Forward: Slides the kernel over the input computing dot products.\n+    /// Backward: Computes gradients for both input and kernel using transposed convolutions.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Conv2D is the core operation of convolutional neural networks.\n+    ///\n+    /// For 2D convolution:\n+    /// - The kernel \"slides\" over the input, computing weighted sums\n+    /// - Each output position is a dot product of the kernel with input patch\n+    /// - Stride controls how far the kernel moves each step\n+    /// - Padding adds borders to control output size\n+    ///\n+    /// Gradient computation:\n+    /// - Gradient w.r.t. input: \"full\" convolution with flipped kernel\n+    /// - Gradient w.r.t. kernel: cross-correlation between input and output gradient\n+    ///\n+    /// Used in:\n+    /// - All CNNs (image classification, object detection, segmentation)\n+    /// - Feature extraction in vision models\n+    /// - Learning spatial hierarchies\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Conv2D(\n+        ComputationNode<T> input,\n+        ComputationNode<T> kernel,\n+        ComputationNode<T>? bias = null,\n+        int[]? stride = null,\n+        int[]? padding = null)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var inputShape = input.Value.Shape;\n+        var kernelShape = kernel.Value.Shape;\n+\n+        if (inputShape.Length != 4)\n+            throw new ArgumentException(\"Conv2D requires 4D input [batch, inChannels, height, width]\");\n+        if (kernelShape.Length != 4)\n+            throw new ArgumentException(\"Conv2D requires 4D kernel [outChannels, inChannels, kernelH, kernelW]\");\n+\n+        stride ??= new int[] { 1, 1 };\n+        padding ??= new int[] { 0, 0 };\n+\n+        int batch = inputShape[0];\n+        int inChannels = inputShape[1];\n+        int inH = inputShape[2];\n+        int inW = inputShape[3];\n+\n+        int outChannels = kernelShape[0];\n+        int kernelInChannels = kernelShape[1];\n+        int kernelH = kernelShape[2];\n+        int kernelW = kernelShape[3];\n+\n+        if (inChannels != kernelInChannels)\n+            throw new ArgumentException($\"Input channels ({inChannels}) must match kernel input channels ({kernelInChannels})\");\n+\n+        int strideH = stride[0];\n+        int strideW = stride[1];\n+        int padH = padding[0];\n+        int padW = padding[1];\n+\n+        int outH = (inH + 2 * padH - kernelH) / strideH + 1;\n+        int outW = (inW + 2 * padW - kernelW) / strideW + 1;\n+\n+        var result = new Tensor<T>(new int[] { batch, outChannels, outH, outW });\n+\n+        // Forward pass: convolution\n+        for (int b = 0; b < batch; b++)\n+        {\n+            for (int oc = 0; oc < outChannels; oc++)\n+            {\n+                for (int oh = 0; oh < outH; oh++)\n+                {\n+                    for (int ow = 0; ow < outW; ow++)\n+                    {\n+                        var sum = numOps.Zero;\n+\n+                        // Convolve kernel over input\n+                        for (int ic = 0; ic < inChannels; ic++)\n+                        {\n+                            for (int kh = 0; kh < kernelH; kh++)\n+                            {\n+                                for (int kw = 0; kw < kernelW; kw++)\n+                                {\n+                                    int ih = oh * strideH + kh - padH;\n+                                    int iw = ow * strideW + kw - padW;\n+\n+                                    // Check bounds (padding)\n+                                    if (ih >= 0 && ih < inH && iw >= 0 && iw < inW)\n+                                    {\n+                                        var inputVal = input.Value[b, ic, ih, iw];\n+                                        var kernelVal = kernel.Value[oc, ic, kh, kw];\n+                                        sum = numOps.Add(sum, numOps.Multiply(inputVal, kernelVal));\n+                                    }\n+                                }\n+                            }\n+                        }\n+\n+                        // Add bias if provided\n+                        if (bias != null)\n+                        {\n+                            sum = numOps.Add(sum, bias.Value[oc]);\n+                        }\n+\n+                        result[b, oc, oh, ow] = sum;\n+                    }\n+                }\n+            }\n+        }\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            // Gradient w.r.t. input\n+            if (input.RequiresGradient)\n+            {\n+                var gradInput = new Tensor<T>(inputShape);\n+\n+                // Full convolution with flipped kernel\n+                for (int b = 0; b < batch; b++)\n+                {\n+                    for (int ic = 0; ic < inChannels; ic++)\n+                    {\n+                        for (int ih = 0; ih < inH; ih++)\n+                        {\n+                            for (int iw = 0; iw < inW; iw++)\n+                            {\n+                                var sum = numOps.Zero;\n+\n+                                // Iterate over all output positions that used this input position\n+                                for (int oc = 0; oc < outChannels; oc++)\n+                                {\n+                                    for (int kh = 0; kh < kernelH; kh++)\n+                                    {\n+                                        for (int kw = 0; kw < kernelW; kw++)\n+                                        {\n+                                            // Compute output position\n+                                            int ohShifted = ih + padH - kh;\n+                                            int owShifted = iw + padW - kw;\n+\n+                                            if (ohShifted % strideH == 0 && owShifted % strideW == 0)\n+                                            {\n+                                                int oh = ohShifted / strideH;\n+                                                int ow = owShifted / strideW;\n+\n+                                                if (oh >= 0 && oh < outH && ow >= 0 && ow < outW)\n+                                                {\n+                                                    var gradVal = gradient[b, oc, oh, ow];\n+                                                    var kernelVal = kernel.Value[oc, ic, kh, kw];\n+                                                    sum = numOps.Add(sum, numOps.Multiply(gradVal, kernelVal));\n+                                                }\n+                                            }\n+                                        }\n+                                    }\n+                                }\n+\n+                                gradInput[b, ic, ih, iw] = sum;\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (input.Gradient == null)\n+                    input.Gradient = gradInput;\n+                else\n+                    input.Gradient = input.Gradient.Add(gradInput);\n+            }\n+\n+            // Gradient w.r.t. kernel\n+            if (kernel.RequiresGradient)\n+            {\n+                var gradKernel = new Tensor<T>(kernelShape);\n+\n+                // Cross-correlation between input and output gradient\n+                for (int oc = 0; oc < outChannels; oc++)\n+                {\n+                    for (int ic = 0; ic < inChannels; ic++)\n+                    {\n+                        for (int kh = 0; kh < kernelH; kh++)\n+                        {\n+                            for (int kw = 0; kw < kernelW; kw++)\n+                            {\n+                                var sum = numOps.Zero;\n+\n+                                for (int b = 0; b < batch; b++)\n+                                {\n+                                    for (int oh = 0; oh < outH; oh++)\n+                                    {\n+                                        for (int ow = 0; ow < outW; ow++)\n+                                        {\n+                                            int ih = oh * strideH + kh - padH;\n+                                            int iw = ow * strideW + kw - padW;\n+\n+                                            if (ih >= 0 && ih < inH && iw >= 0 && iw < inW)\n+                                            {\n+                                                var gradVal = gradient[b, oc, oh, ow];\n+                                                var inputVal = input.Value[b, ic, ih, iw];\n+                                                sum = numOps.Add(sum, numOps.Multiply(gradVal, inputVal));\n+                                            }\n+                                        }\n+                                    }\n+                                }\n+\n+                                gradKernel[oc, ic, kh, kw] = sum;\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (kernel.Gradient == null)\n+                    kernel.Gradient = gradKernel;\n+                else\n+                    kernel.Gradient = kernel.Gradient.Add(gradKernel);"}]}},{"id":"PRRT_kwDOKSXUF85hck_E","isResolved":false,"isOutdated":false,"path":"src/Autodiff/TensorOperations.cs","line":2703,"originalLine":2703,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzD_i","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement write to the same variable - consider using '?' to express intent better.\n```suggestion\n                bias.Gradient = bias.Gradient == null ? gradBias : bias.Gradient.Add(gradBias);\n```","createdAt":"2025-11-11T07:53:27Z","path":"src/Autodiff/TensorOperations.cs","line":2703,"originalLine":2703,"diffHunk":"@@ -1219,4 +1219,1769 @@ void BackwardFunction(Tensor<T> gradient)\n \n         return node;\n     }\n+\n+    /// <summary>\n+    /// Computes the softmax function for a computation node along a specified axis.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"axis\">The axis along which to compute softmax. Default is -1 (last axis).</param>\n+    /// <returns>A new computation node containing the softmax result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method computes softmax (σ(x_i) = exp(x_i) / Σexp(x_j)) along the specified axis.\n+    /// Uses numerical stability trick: subtract max before exponentiating.\n+    /// The backward function uses: ∂softmax/∂x = softmax(x) * (grad - Σ(grad * softmax(x))).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Softmax converts a vector of numbers into probabilities.\n+    ///\n+    /// For softmax:\n+    /// - The forward pass exponentiates each element, then normalizes so they sum to 1\n+    /// - The result is a probability distribution (all values between 0 and 1, summing to 1)\n+    /// - The backward pass is complex but efficient: uses the Jacobian of softmax\n+    ///\n+    /// Softmax is crucial for:\n+    /// - Multi-class classification (final layer outputs)\n+    /// - Attention mechanisms (computing attention weights)\n+    /// - Anywhere you need to convert scores to probabilities\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Softmax(ComputationNode<T> a, int axis = -1)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        // Normalize axis to positive index\n+        if (axis < 0)\n+            axis = shape.Length + axis;\n+\n+        // For simplicity, handle 2D case (batch, features) with axis=-1\n+        if (shape.Length == 2 && axis == 1)\n+        {\n+            int batchSize = shape[0];\n+            int features = shape[1];\n+            var result = new Tensor<T>(shape);\n+\n+            // Compute softmax for each row\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                // Find max for numerical stability\n+                var maxVal = a.Value[b, 0];\n+                for (int f = 1; f < features; f++)\n+                {\n+                    if (numOps.GreaterThan(a.Value[b, f], maxVal))\n+                        maxVal = a.Value[b, f];\n+                }\n+\n+                // Compute exp(x - max) and sum\n+                var expSum = numOps.Zero;\n+                var expValues = new T[features];\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var shifted = numOps.Subtract(a.Value[b, f], maxVal);\n+                    expValues[f] = numOps.Exp(shifted);\n+                    expSum = numOps.Add(expSum, expValues[f]);\n+                }\n+\n+                // Normalize\n+                for (int f = 0; f < features; f++)\n+                {\n+                    result[b, f] = numOps.Divide(expValues[f], expSum);\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (a.RequiresGradient)\n+                {\n+                    // ∂softmax/∂x_i = softmax_i * (∂L/∂y_i - Σ_j(∂L/∂y_j * softmax_j))\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        // Compute sum of (gradient * softmax)\n+                        var dotProduct = numOps.Zero;\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            dotProduct = numOps.Add(dotProduct,\n+                                numOps.Multiply(gradient[b, f], result[b, f]));\n+                        }\n+\n+                        // Compute gradient for each element\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            var gradMinusDot = numOps.Subtract(gradient[b, f], dotProduct);\n+                            gradA[b, f] = numOps.Multiply(result[b, f], gradMinusDot);\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient,\n+                parents: new List<ComputationNode<T>> { a },\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Softmax is currently only implemented for 2D tensors along axis=-1. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}], axis={axis}\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Concatenates multiple computation nodes along a specified axis.\n+    /// </summary>\n+    /// <param name=\"nodes\">The list of nodes to concatenate.</param>\n+    /// <param name=\"axis\">The axis along which to concatenate. Default is 0.</param>\n+    /// <returns>A new computation node containing the concatenated result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method concatenates tensors along the specified axis.\n+    /// All tensors must have the same shape except along the concatenation axis.\n+    /// The backward function splits the gradient and sends each portion to the corresponding input.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Concat stacks tensors together along a dimension.\n+    ///\n+    /// For concatenation:\n+    /// - The forward pass combines multiple tensors into one larger tensor\n+    /// - The backward pass splits the gradient back to each input\n+    /// - Think of it like gluing arrays together end-to-end\n+    ///\n+    /// Used in:\n+    /// - Skip connections (concatenating features from different layers)\n+    /// - Multi-input architectures\n+    /// - Feature fusion in neural networks\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Concat(List<ComputationNode<T>> nodes, int axis = 0)\n+    {\n+        if (nodes.Count == 0)\n+            throw new ArgumentException(\"Cannot concatenate empty list of nodes\");\n+\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var firstShape = nodes[0].Value.Shape;\n+\n+        // Normalize axis\n+        if (axis < 0)\n+            axis = firstShape.Length + axis;\n+\n+        // Validate shapes match except on concat axis\n+        for (int i = 1; i < nodes.Count; i++)\n+        {\n+            var shape = nodes[i].Value.Shape;\n+            if (shape.Length != firstShape.Length)\n+                throw new ArgumentException(\"All tensors must have the same rank\");\n+\n+            for (int d = 0; d < firstShape.Length; d++)\n+            {\n+                if (d != axis && shape[d] != firstShape[d])\n+                    throw new ArgumentException(\n+                        $\"Shape mismatch at dimension {d}: {shape[d]} vs {firstShape[d]}\");\n+            }\n+        }\n+\n+        // Compute output shape\n+        int[] outputShape = (int[])firstShape.Clone();\n+        for (int i = 1; i < nodes.Count; i++)\n+        {\n+            outputShape[axis] += nodes[i].Value.Shape[axis];\n+        }\n+\n+        // Perform concatenation (handle 2D case for simplicity)\n+        Tensor<T> result;\n+        if (firstShape.Length == 2 && axis == 1)\n+        {\n+            // Concatenate along columns (features)\n+            int rows = firstShape[0];\n+            int totalCols = outputShape[1];\n+            result = new Tensor<T>(new int[] { rows, totalCols });\n+\n+            int colOffset = 0;\n+            foreach (var node in nodes)\n+            {\n+                int cols = node.Value.Shape[1];\n+                for (int r = 0; r < rows; r++)\n+                {\n+                    for (int c = 0; c < cols; c++)\n+                    {\n+                        result[r, colOffset + c] = node.Value[r, c];\n+                    }\n+                }\n+                colOffset += cols;\n+            }\n+        }\n+        else if (firstShape.Length == 2 && axis == 0)\n+        {\n+            // Concatenate along rows (batch)\n+            int cols = firstShape[1];\n+            int totalRows = outputShape[0];\n+            result = new Tensor<T>(new int[] { totalRows, cols });\n+\n+            int rowOffset = 0;\n+            foreach (var node in nodes)\n+            {\n+                int rows = node.Value.Shape[0];\n+                for (int r = 0; r < rows; r++)\n+                {\n+                    for (int c = 0; c < cols; c++)\n+                    {\n+                        result[rowOffset + r, c] = node.Value[r, c];\n+                    }\n+                }\n+                rowOffset += rows;\n+            }\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Concat is currently only implemented for 2D tensors. \" +\n+                $\"Got shape=[{string.Join(\", \", firstShape)}]\");\n+        }\n+\n+        // Store sizes for gradient splitting\n+        var sizes = nodes.Select(n => n.Value.Shape[axis]).ToList();\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            // Split gradient along concat axis and distribute to inputs\n+            if (firstShape.Length == 2 && axis == 1)\n+            {\n+                int rows = firstShape[0];\n+                int colOffset = 0;\n+                for (int i = 0; i < nodes.Count; i++)\n+                {\n+                    if (!nodes[i].RequiresGradient)\n+                    {\n+                        colOffset += sizes[i];\n+                        continue;\n+                    }\n+\n+                    int cols = sizes[i];\n+                    var gradPart = new Tensor<T>(new int[] { rows, cols });\n+\n+                    for (int r = 0; r < rows; r++)\n+                    {\n+                        for (int c = 0; c < cols; c++)\n+                        {\n+                            gradPart[r, c] = gradient[r, colOffset + c];\n+                        }\n+                    }\n+\n+                    if (nodes[i].Gradient == null)\n+                        nodes[i].Gradient = gradPart;\n+                    else\n+                        nodes[i].Gradient = nodes[i].Gradient.Add(gradPart);\n+\n+                    colOffset += cols;\n+                }\n+            }\n+            else if (firstShape.Length == 2 && axis == 0)\n+            {\n+                int cols = firstShape[1];\n+                int rowOffset = 0;\n+                for (int i = 0; i < nodes.Count; i++)\n+                {\n+                    if (!nodes[i].RequiresGradient)\n+                    {\n+                        rowOffset += sizes[i];\n+                        continue;\n+                    }\n+\n+                    int rows = sizes[i];\n+                    var gradPart = new Tensor<T>(new int[] { rows, cols });\n+\n+                    for (int r = 0; r < rows; r++)\n+                    {\n+                        for (int c = 0; c < cols; c++)\n+                        {\n+                            gradPart[r, c] = gradient[rowOffset + r, c];\n+                        }\n+                    }\n+\n+                    if (nodes[i].Gradient == null)\n+                        nodes[i].Gradient = gradPart;\n+                    else\n+                        nodes[i].Gradient = nodes[i].Gradient.Add(gradPart);\n+\n+                    rowOffset += rows;\n+                }\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: nodes.Any(n => n.RequiresGradient),\n+            parents: nodes,\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Pads a tensor with a constant value along specified dimensions.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"padWidth\">Padding width for each dimension as (before, after) pairs.</param>\n+    /// <param name=\"value\">The value to use for padding. Default is zero.</param>\n+    /// <returns>A new computation node containing the padded result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method adds padding around the tensor.\n+    /// The backward function simply crops the gradient back to the original size (gradients for padding are zero).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Pad adds extra elements around a tensor.\n+    ///\n+    /// For padding:\n+    /// - The forward pass adds border elements with a constant value\n+    /// - The backward pass removes those border gradients (they don't affect the original tensor)\n+    /// - Think of it like adding margins to an image\n+    ///\n+    /// Used in:\n+    /// - Convolutional layers (to maintain spatial dimensions)\n+    /// - Handling variable-length sequences\n+    /// - Data augmentation\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Pad(ComputationNode<T> a, int[,] padWidth, T? value = default)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var padValue = value ?? numOps.Zero;\n+        var shape = a.Value.Shape;\n+\n+        // Validate padWidth dimensions\n+        if (padWidth.GetLength(0) != shape.Length)\n+            throw new ArgumentException(\"padWidth must have same number of dimensions as input tensor\");\n+\n+        // Compute output shape\n+        var outputShape = new int[shape.Length];\n+        for (int d = 0; d < shape.Length; d++)\n+        {\n+            outputShape[d] = shape[d] + padWidth[d, 0] + padWidth[d, 1];\n+        }\n+\n+        // Handle 2D case\n+        if (shape.Length == 2)\n+        {\n+            int inputRows = shape[0];\n+            int inputCols = shape[1];\n+            int padTop = padWidth[0, 0];\n+            int padBottom = padWidth[0, 1];\n+            int padLeft = padWidth[1, 0];\n+            int padRight = padWidth[1, 1];\n+\n+            var result = new Tensor<T>(outputShape);\n+\n+            // Initialize with pad value\n+            for (int i = 0; i < result.Length; i++)\n+            {\n+                result[i] = padValue;\n+            }\n+\n+            // Copy input data to center\n+            for (int r = 0; r < inputRows; r++)\n+            {\n+                for (int c = 0; c < inputCols; c++)\n+                {\n+                    result[padTop + r, padLeft + c] = a.Value[r, c];\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (a.RequiresGradient)\n+                {\n+                    // Extract gradient for original (unpadded) region\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int r = 0; r < inputRows; r++)\n+                    {\n+                        for (int c = 0; c < inputCols; c++)\n+                        {\n+                            gradA[r, c] = gradient[padTop + r, padLeft + c];\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient,\n+                parents: new List<ComputationNode<T>> { a },\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Pad is currently only implemented for 2D tensors. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}]\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs 2D max pooling on a 4D tensor (batch, channels, height, width).\n+    /// </summary>\n+    /// <param name=\"a\">The input node with shape [batch, channels, height, width].</param>\n+    /// <param name=\"poolSize\">The size of the pooling window [poolH, poolW].</param>\n+    /// <param name=\"strides\">The stride for the pooling operation [strideH, strideW]. If null, uses poolSize.</param>\n+    /// <returns>A new computation node containing the max pooled result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method performs max pooling over 2D spatial dimensions.\n+    /// During forward pass, it tracks which element was the max for routing gradients during backward pass.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> MaxPool downsamples by taking the maximum value in each window.\n+    ///\n+    /// For max pooling:\n+    /// - The forward pass slides a window and takes the max value in each position\n+    /// - This reduces spatial dimensions (downsampling)\n+    /// - The backward pass routes gradients only to the positions that were max\n+    /// - Other positions get zero gradient (they didn't contribute to the output)\n+    ///\n+    /// Used in:\n+    /// - CNNs for translation invariance\n+    /// - Reducing spatial resolution\n+    /// - Building hierarchical features\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> MaxPool2D(\n+        ComputationNode<T> a,\n+        int[] poolSize,\n+        int[]? strides = null)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        if (shape.Length != 4)\n+            throw new ArgumentException(\"MaxPool2D requires 4D input [batch, channels, height, width]\");\n+\n+        strides ??= poolSize;\n+\n+        int batch = shape[0];\n+        int channels = shape[1];\n+        int inH = shape[2];\n+        int inW = shape[3];\n+        int poolH = poolSize[0];\n+        int poolW = poolSize[1];\n+        int strideH = strides[0];\n+        int strideW = strides[1];\n+\n+        int outH = (inH - poolH) / strideH + 1;\n+        int outW = (inW - poolW) / strideW + 1;\n+\n+        var result = new Tensor<T>(new int[] { batch, channels, outH, outW });\n+        // Store max positions for backprop\n+        var maxPositions = new int[batch, channels, outH, outW, 2]; // [h_offset, w_offset]\n+\n+        // Forward pass: compute max pooling and track positions\n+        for (int b = 0; b < batch; b++)\n+        {\n+            for (int c = 0; c < channels; c++)\n+            {\n+                for (int oh = 0; oh < outH; oh++)\n+                {\n+                    for (int ow = 0; ow < outW; ow++)\n+                    {\n+                        int hStart = oh * strideH;\n+                        int wStart = ow * strideW;\n+\n+                        var maxVal = a.Value[b * channels * inH * inW +\n+                                           c * inH * inW +\n+                                           hStart * inW +\n+                                           wStart];\n+                        int maxHOffset = 0;\n+                        int maxWOffset = 0;\n+\n+                        // Find max in pooling window\n+                        for (int ph = 0; ph < poolH; ph++)\n+                        {\n+                            for (int pw = 0; pw < poolW; pw++)\n+                            {\n+                                int h = hStart + ph;\n+                                int w = wStart + pw;\n+                                if (h < inH && w < inW)\n+                                {\n+                                    var val = a.Value[b * channels * inH * inW +\n+                                                     c * inH * inW +\n+                                                     h * inW +\n+                                                     w];\n+                                    if (numOps.GreaterThan(val, maxVal))\n+                                    {\n+                                        maxVal = val;\n+                                        maxHOffset = ph;\n+                                        maxWOffset = pw;\n+                                    }\n+                                }\n+                            }\n+                        }\n+\n+                        result[b, c, oh, ow] = maxVal;\n+                        maxPositions[b, c, oh, ow, 0] = maxHOffset;\n+                        maxPositions[b, c, oh, ow, 1] = maxWOffset;\n+                    }\n+                }\n+            }\n+        }\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            if (a.RequiresGradient)\n+            {\n+                var gradA = new Tensor<T>(shape);\n+\n+                // Route gradients to max positions\n+                for (int b = 0; b < batch; b++)\n+                {\n+                    for (int c = 0; c < channels; c++)\n+                    {\n+                        for (int oh = 0; oh < outH; oh++)\n+                        {\n+                            for (int ow = 0; ow < outW; ow++)\n+                            {\n+                                int hStart = oh * strideH;\n+                                int wStart = ow * strideW;\n+                                int maxHOffset = maxPositions[b, c, oh, ow, 0];\n+                                int maxWOffset = maxPositions[b, c, oh, ow, 1];\n+\n+                                int maxH = hStart + maxHOffset;\n+                                int maxW = wStart + maxWOffset;\n+\n+                                int gradIdx = b * channels * inH * inW +\n+                                             c * inH * inW +\n+                                            maxH * inW +\n+                                             maxW;\n+\n+                                gradA[gradIdx] = numOps.Add(gradA[gradIdx], gradient[b, c, oh, ow]);\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (a.Gradient == null)\n+                    a.Gradient = gradA;\n+                else\n+                    a.Gradient = a.Gradient.Add(gradA);\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: a.RequiresGradient,\n+            parents: new List<ComputationNode<T>> { a },\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Performs 2D average pooling on a 4D tensor (batch, channels, height, width).\n+    /// </summary>\n+    /// <param name=\"a\">The input node with shape [batch, channels, height, width].</param>\n+    /// <param name=\"poolSize\">The size of the pooling window [poolH, poolW].</param>\n+    /// <param name=\"strides\">The stride for the pooling operation [strideH, strideW]. If null, uses poolSize.</param>\n+    /// <returns>A new computation node containing the average pooled result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method performs average pooling over 2D spatial dimensions.\n+    /// The backward function distributes gradients equally across the pooling window.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> AvgPool downsamples by taking the average value in each window.\n+    ///\n+    /// For average pooling:\n+    /// - The forward pass slides a window and computes the average\n+    /// - This smoothly reduces spatial dimensions\n+    /// - The backward pass distributes gradients equally to all elements in the window\n+    /// - Each element gets gradient / pool_area\n+    ///\n+    /// Used in:\n+    /// - CNNs for smoother downsampling than max pooling\n+    /// - Global average pooling (replacing fully connected layers)\n+    /// - Reducing overfitting\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> AvgPool2D(\n+        ComputationNode<T> a,\n+        int[] poolSize,\n+        int[]? strides = null)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        if (shape.Length != 4)\n+            throw new ArgumentException(\"AvgPool2D requires 4D input [batch, channels, height, width]\");\n+\n+        strides ??= poolSize;\n+\n+        int batch = shape[0];\n+        int channels = shape[1];\n+        int inH = shape[2];\n+        int inW = shape[3];\n+        int poolH = poolSize[0];\n+        int poolW = poolSize[1];\n+        int strideH = strides[0];\n+        int strideW = strides[1];\n+\n+        int outH = (inH - poolH) / strideH + 1;\n+        int outW = (inW - poolW) / strideW + 1;\n+\n+        var result = new Tensor<T>(new int[] { batch, channels, outH, outW });\n+        var poolArea = numOps.FromDouble(poolH * poolW);\n+\n+        // Forward pass: compute average pooling\n+        for (int b = 0; b < batch; b++)\n+        {\n+            for (int c = 0; c < channels; c++)\n+            {\n+                for (int oh = 0; oh < outH; oh++)\n+                {\n+                    for (int ow = 0; ow < outW; ow++)\n+                    {\n+                        int hStart = oh * strideH;\n+                        int wStart = ow * strideW;\n+\n+                        var sum = numOps.Zero;\n+\n+                        // Sum values in pooling window\n+                        for (int ph = 0; ph < poolH; ph++)\n+                        {\n+                            for (int pw = 0; pw < poolW; pw++)\n+                            {\n+                                int h = hStart + ph;\n+                                int w = wStart + pw;\n+                                if (h < inH && w < inW)\n+                                {\n+                                    sum = numOps.Add(sum, a.Value[b, c, h, w]);\n+                                }\n+                            }\n+                        }\n+\n+                        result[b, c, oh, ow] = numOps.Divide(sum, poolArea);\n+                    }\n+                }\n+            }\n+        }\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            if (a.RequiresGradient)\n+            {\n+                var gradA = new Tensor<T>(shape);\n+\n+                // Distribute gradients equally across pooling windows\n+                for (int b = 0; b < batch; b++)\n+                {\n+                    for (int c = 0; c < channels; c++)\n+                    {\n+                        for (int oh = 0; oh < outH; oh++)\n+                        {\n+                            for (int ow = 0; ow < outW; ow++)\n+                            {\n+                                int hStart = oh * strideH;\n+                                int wStart = ow * strideW;\n+\n+                                var gradValue = numOps.Divide(gradient[b, c, oh, ow], poolArea);\n+\n+                                // Distribute to all elements in window\n+                                for (int ph = 0; ph < poolH; ph++)\n+                                {\n+                                    for (int pw = 0; pw < poolW; pw++)\n+                                    {\n+                                        int h = hStart + ph;\n+                                        int w = wStart + pw;\n+                                        if (h < inH && w < inW)\n+                                        {\n+                                            gradA[b, c, h, w] = numOps.Add(gradA[b, c, h, w], gradValue);\n+                                        }\n+                                    }\n+                                }\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (a.Gradient == null)\n+                    a.Gradient = gradA;\n+                else\n+                    a.Gradient = a.Gradient.Add(gradA);\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: a.RequiresGradient,\n+            parents: new List<ComputationNode<T>> { a },\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Applies layer normalization to a computation node.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"normalizedShape\">The shape over which to normalize (typically the feature dimensions).</param>\n+    /// <param name=\"gamma\">Optional scale parameter (learnable). If null, uses ones.</param>\n+    /// <param name=\"beta\">Optional shift parameter (learnable). If null, uses zeros.</param>\n+    /// <param name=\"epsilon\">Small constant for numerical stability. Default is 1e-5.</param>\n+    /// <returns>A new computation node containing the layer normalized result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Layer normalization normalizes inputs across the feature dimension for each sample independently.\n+    /// Formula: y = gamma * (x - mean) / sqrt(variance + epsilon) + beta\n+    /// Unlike batch normalization, this doesn't depend on batch statistics.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> LayerNorm standardizes features for each sample independently.\n+    ///\n+    /// For layer normalization:\n+    /// - Computes mean and variance for each sample's features\n+    /// - Normalizes: (x - mean) / sqrt(variance)\n+    /// - Scales and shifts: result * gamma + beta\n+    /// - Works the same during training and inference (no batch dependency)\n+    ///\n+    /// Used in:\n+    /// - Transformers (critical component)\n+    /// - RNNs (stabilizes training)\n+    /// - Any architecture needing sample-independent normalization\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> LayerNorm(\n+        ComputationNode<T> a,\n+        int[] normalizedShape,\n+        ComputationNode<T>? gamma = null,\n+        ComputationNode<T>? beta = null,\n+        double epsilon = 1e-5)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+        var eps = numOps.FromDouble(epsilon);\n+\n+        // For 2D input [batch, features], normalize over features\n+        if (shape.Length == 2 && normalizedShape.Length == 1 && normalizedShape[0] == shape[1])\n+        {\n+            int batchSize = shape[0];\n+            int features = shape[1];\n+\n+            // Create default gamma (ones) and beta (zeros) if not provided\n+            if (gamma == null)\n+            {\n+                var gammaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    gammaTensor[i] = numOps.One;\n+                gamma = Variable(gammaTensor, requiresGradient: false);\n+            }\n+\n+            if (beta == null)\n+            {\n+                var betaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    betaTensor[i] = numOps.Zero;\n+                beta = Variable(betaTensor, requiresGradient: false);\n+            }\n+\n+            var result = new Tensor<T>(shape);\n+            var means = new T[batchSize];\n+            var variances = new T[batchSize];\n+            var normalized = new Tensor<T>(shape);\n+\n+            // Forward pass: compute mean and variance per sample\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                // Compute mean\n+                var sum = numOps.Zero;\n+                for (int f = 0; f < features; f++)\n+                {\n+                    sum = numOps.Add(sum, a.Value[b, f]);\n+                }\n+                means[b] = numOps.Divide(sum, numOps.FromDouble(features));\n+\n+                // Compute variance\n+                var varSum = numOps.Zero;\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var diff = numOps.Subtract(a.Value[b, f], means[b]);\n+                    varSum = numOps.Add(varSum, numOps.Multiply(diff, diff));\n+                }\n+                variances[b] = numOps.Divide(varSum, numOps.FromDouble(features));\n+\n+                // Normalize and scale\n+                var std = numOps.Sqrt(numOps.Add(variances[b], eps));\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var norm = numOps.Divide(\n+                        numOps.Subtract(a.Value[b, f], means[b]),\n+                        std);\n+                    normalized[b, f] = norm;\n+                    result[b, f] = numOps.Add(\n+                        numOps.Multiply(norm, gamma.Value[f]),\n+                        beta.Value[f]);\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                // Gradients for gamma and beta\n+                if (gamma.RequiresGradient)\n+                {\n+                    var gradGamma = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum,\n+                                numOps.Multiply(gradient[b, f], normalized[b, f]));\n+                        }\n+                        gradGamma[f] = sum;\n+                    }\n+\n+                    if (gamma.Gradient == null)\n+                        gamma.Gradient = gradGamma;\n+                    else\n+                        gamma.Gradient = gamma.Gradient.Add(gradGamma);\n+                }\n+\n+                if (beta.RequiresGradient)\n+                {\n+                    var gradBeta = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum, gradient[b, f]);\n+                        }\n+                        gradBeta[f] = sum;\n+                    }\n+\n+                    if (beta.Gradient == null)\n+                        beta.Gradient = gradBeta;\n+                    else\n+                        beta.Gradient = beta.Gradient.Add(gradBeta);\n+                }\n+\n+                // Gradient for input\n+                if (a.RequiresGradient)\n+                {\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        var std = numOps.Sqrt(numOps.Add(variances[b], eps));\n+                        var invStd = numOps.Divide(numOps.One, std);\n+\n+                        // Compute gradient components\n+                        var gradNormSum = numOps.Zero;\n+                        var gradNormDotNorm = numOps.Zero;\n+\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            var gradNorm = numOps.Multiply(gradient[b, f], gamma.Value[f]);\n+                            gradNormSum = numOps.Add(gradNormSum, gradNorm);\n+                            gradNormDotNorm = numOps.Add(gradNormDotNorm,\n+                                numOps.Multiply(gradNorm, normalized[b, f]));\n+                        }\n+\n+                        // Apply gradient formula\n+                        var featuresT = numOps.FromDouble(features);\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            var gradNorm = numOps.Multiply(gradient[b, f], gamma.Value[f]);\n+\n+                            var term1 = gradNorm;\n+                            var term2 = numOps.Divide(gradNormSum, featuresT);\n+                            var term3 = numOps.Divide(\n+                                numOps.Multiply(normalized[b, f], gradNormDotNorm),\n+                                featuresT);\n+\n+                            var grad = numOps.Multiply(\n+                                numOps.Subtract(numOps.Subtract(term1, term2), term3),\n+                                invStd);\n+\n+                            gradA[b, f] = grad;\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var parents = new List<ComputationNode<T>> { a };\n+            if (gamma != null) parents.Add(gamma);\n+            if (beta != null) parents.Add(beta);\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient || (gamma?.RequiresGradient ?? false) || (beta?.RequiresGradient ?? false),\n+                parents: parents,\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"LayerNorm is currently only implemented for 2D tensors normalizing over last dimension. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}], normalizedShape=[{string.Join(\", \", normalizedShape)}]\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Applies batch normalization to a computation node.\n+    /// </summary>\n+    /// <param name=\"a\">The input node with shape [batch, features].</param>\n+    /// <param name=\"gamma\">Optional scale parameter (learnable). If null, uses ones.</param>\n+    /// <param name=\"beta\">Optional shift parameter (learnable). If null, uses zeros.</param>\n+    /// <param name=\"runningMean\">Running mean for inference (not updated during this operation).</param>\n+    /// <param name=\"runningVar\">Running variance for inference (not updated during this operation).</param>\n+    /// <param name=\"training\">Whether in training mode (uses batch statistics) or inference mode (uses running statistics).</param>\n+    /// <param name=\"epsilon\">Small constant for numerical stability. Default is 1e-5.</param>\n+    /// <returns>A new computation node containing the batch normalized result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Batch normalization normalizes inputs across the batch dimension.\n+    /// During training: Uses batch statistics (mean and variance computed from current batch).\n+    /// During inference: Uses running statistics (accumulated during training).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> BatchNorm standardizes features across the batch.\n+    ///\n+    /// For batch normalization:\n+    /// - Training mode: Uses current batch's mean and variance\n+    /// - Inference mode: Uses running mean/variance from training\n+    /// - Normalizes: (x - mean) / sqrt(variance)\n+    /// - Scales and shifts: result * gamma + beta\n+    ///\n+    /// Benefits:\n+    /// - Stabilizes training (reduces internal covariate shift)\n+    /// - Allows higher learning rates\n+    /// - Acts as regularization\n+    ///\n+    /// Used in:\n+    /// - CNNs (after convolutional layers)\n+    /// - Deep feedforward networks\n+    /// - GANs and many other architectures\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> BatchNorm(\n+        ComputationNode<T> a,\n+        ComputationNode<T>? gamma = null,\n+        ComputationNode<T>? beta = null,\n+        Tensor<T>? runningMean = null,\n+        Tensor<T>? runningVar = null,\n+        bool training = true,\n+        double epsilon = 1e-5)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+        var eps = numOps.FromDouble(epsilon);\n+\n+        // Handle 2D case [batch, features]\n+        if (shape.Length == 2)\n+        {\n+            int batchSize = shape[0];\n+            int features = shape[1];\n+\n+            // Create default gamma and beta if not provided\n+            if (gamma == null)\n+            {\n+                var gammaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    gammaTensor[i] = numOps.One;\n+                gamma = Variable(gammaTensor, requiresGradient: false);\n+            }\n+\n+            if (beta == null)\n+            {\n+                var betaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    betaTensor[i] = numOps.Zero;\n+                beta = Variable(betaTensor, requiresGradient: false);\n+            }\n+\n+            var result = new Tensor<T>(shape);\n+            T[] batchMean;\n+            T[] batchVar;\n+            var normalized = new Tensor<T>(shape);\n+\n+            if (training)\n+            {\n+                // Compute batch statistics\n+                batchMean = new T[features];\n+                batchVar = new T[features];\n+\n+                // Compute mean per feature\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var sum = numOps.Zero;\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        sum = numOps.Add(sum, a.Value[b, f]);\n+                    }\n+                    batchMean[f] = numOps.Divide(sum, numOps.FromDouble(batchSize));\n+                }\n+\n+                // Compute variance per feature\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var varSum = numOps.Zero;\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        var diff = numOps.Subtract(a.Value[b, f], batchMean[f]);\n+                        varSum = numOps.Add(varSum, numOps.Multiply(diff, diff));\n+                    }\n+                    batchVar[f] = numOps.Divide(varSum, numOps.FromDouble(batchSize));\n+                }\n+            }\n+            else\n+            {\n+                // Use running statistics for inference\n+                if (runningMean == null || runningVar == null)\n+                    throw new ArgumentException(\"Running statistics required for inference mode\");\n+\n+                batchMean = new T[features];\n+                batchVar = new T[features];\n+                for (int f = 0; f < features; f++)\n+                {\n+                    batchMean[f] = runningMean[f];\n+                    batchVar[f] = runningVar[f];\n+                }\n+            }\n+\n+            // Normalize and scale\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var std = numOps.Sqrt(numOps.Add(batchVar[f], eps));\n+                    var norm = numOps.Divide(\n+                        numOps.Subtract(a.Value[b, f], batchMean[f]),\n+                        std);\n+                    normalized[b, f] = norm;\n+                    result[b, f] = numOps.Add(\n+                        numOps.Multiply(norm, gamma.Value[f]),\n+                        beta.Value[f]);\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (!training)\n+                {\n+                    // Inference mode: simpler gradient (no batch statistics gradient)\n+                    if (a.RequiresGradient)\n+                    {\n+                        var gradA = new Tensor<T>(shape);\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            for (int f = 0; f < features; f++)\n+                            {\n+                                var std = numOps.Sqrt(numOps.Add(batchVar[f], eps));\n+                                var invStd = numOps.Divide(numOps.One, std);\n+                                gradA[b, f] = numOps.Multiply(\n+                                    numOps.Multiply(gradient[b, f], gamma.Value[f]),\n+                                    invStd);\n+                            }\n+                        }\n+\n+                        if (a.Gradient == null)\n+                            a.Gradient = gradA;\n+                        else\n+                            a.Gradient = a.Gradient.Add(gradA);\n+                    }\n+                    return;\n+                }\n+\n+                // Training mode: full gradient computation\n+                // Gradients for gamma and beta\n+                if (gamma.RequiresGradient)\n+                {\n+                    var gradGamma = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum,\n+                                numOps.Multiply(gradient[b, f], normalized[b, f]));\n+                        }\n+                        gradGamma[f] = sum;\n+                    }\n+\n+                    if (gamma.Gradient == null)\n+                        gamma.Gradient = gradGamma;\n+                    else\n+                        gamma.Gradient = gamma.Gradient.Add(gradGamma);\n+                }\n+\n+                if (beta.RequiresGradient)\n+                {\n+                    var gradBeta = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum, gradient[b, f]);\n+                        }\n+                        gradBeta[f] = sum;\n+                    }\n+\n+                    if (beta.Gradient == null)\n+                        beta.Gradient = gradBeta;\n+                    else\n+                        beta.Gradient = beta.Gradient.Add(gradBeta);\n+                }\n+\n+                // Gradient for input (complex due to batch statistics)\n+                if (a.RequiresGradient)\n+                {\n+                    var gradA = new Tensor<T>(shape);\n+                    var batchSizeT = numOps.FromDouble(batchSize);\n+\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var std = numOps.Sqrt(numOps.Add(batchVar[f], eps));\n+                        var invStd = numOps.Divide(numOps.One, std);\n+\n+                        // Sum of gradients and gradient*normalized\n+                        var gradSum = numOps.Zero;\n+                        var gradNormSum = numOps.Zero;\n+\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            var grad = numOps.Multiply(gradient[b, f], gamma.Value[f]);\n+                            gradSum = numOps.Add(gradSum, grad);\n+                            gradNormSum = numOps.Add(gradNormSum,\n+                                numOps.Multiply(grad, normalized[b, f]));\n+                        }\n+\n+                        // Apply gradient formula\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            var grad = numOps.Multiply(gradient[b, f], gamma.Value[f]);\n+\n+                            var term1 = grad;\n+                            var term2 = numOps.Divide(gradSum, batchSizeT);\n+                            var term3 = numOps.Divide(\n+                                numOps.Multiply(normalized[b, f], gradNormSum),\n+                                batchSizeT);\n+\n+                            var gradInput = numOps.Multiply(\n+                                numOps.Subtract(numOps.Subtract(term1, term2), term3),\n+                                invStd);\n+\n+                            gradA[b, f] = gradInput;\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var parents = new List<ComputationNode<T>> { a };\n+            if (gamma != null) parents.Add(gamma);\n+            if (beta != null) parents.Add(beta);\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient || (gamma?.RequiresGradient ?? false) || (beta?.RequiresGradient ?? false),\n+                parents: parents,\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"BatchNorm is currently only implemented for 2D tensors [batch, features]. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}]\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs 2D convolution on a 4D tensor (batch, channels, height, width).\n+    /// </summary>\n+    /// <param name=\"input\">The input node with shape [batch, inChannels, height, width].</param>\n+    /// <param name=\"kernel\">The kernel/filter with shape [outChannels, inChannels, kernelH, kernelW].</param>\n+    /// <param name=\"bias\">Optional bias with shape [outChannels]. If null, no bias is added.</param>\n+    /// <param name=\"stride\">The stride [strideH, strideW]. Default is [1, 1].</param>\n+    /// <param name=\"padding\">The padding [padH, padW]. Default is [0, 0].</param>\n+    /// <returns>A new computation node containing the convolution result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method performs 2D convolution, the fundamental operation in CNNs.\n+    /// Forward: Slides the kernel over the input computing dot products.\n+    /// Backward: Computes gradients for both input and kernel using transposed convolutions.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Conv2D is the core operation of convolutional neural networks.\n+    ///\n+    /// For 2D convolution:\n+    /// - The kernel \"slides\" over the input, computing weighted sums\n+    /// - Each output position is a dot product of the kernel with input patch\n+    /// - Stride controls how far the kernel moves each step\n+    /// - Padding adds borders to control output size\n+    ///\n+    /// Gradient computation:\n+    /// - Gradient w.r.t. input: \"full\" convolution with flipped kernel\n+    /// - Gradient w.r.t. kernel: cross-correlation between input and output gradient\n+    ///\n+    /// Used in:\n+    /// - All CNNs (image classification, object detection, segmentation)\n+    /// - Feature extraction in vision models\n+    /// - Learning spatial hierarchies\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Conv2D(\n+        ComputationNode<T> input,\n+        ComputationNode<T> kernel,\n+        ComputationNode<T>? bias = null,\n+        int[]? stride = null,\n+        int[]? padding = null)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var inputShape = input.Value.Shape;\n+        var kernelShape = kernel.Value.Shape;\n+\n+        if (inputShape.Length != 4)\n+            throw new ArgumentException(\"Conv2D requires 4D input [batch, inChannels, height, width]\");\n+        if (kernelShape.Length != 4)\n+            throw new ArgumentException(\"Conv2D requires 4D kernel [outChannels, inChannels, kernelH, kernelW]\");\n+\n+        stride ??= new int[] { 1, 1 };\n+        padding ??= new int[] { 0, 0 };\n+\n+        int batch = inputShape[0];\n+        int inChannels = inputShape[1];\n+        int inH = inputShape[2];\n+        int inW = inputShape[3];\n+\n+        int outChannels = kernelShape[0];\n+        int kernelInChannels = kernelShape[1];\n+        int kernelH = kernelShape[2];\n+        int kernelW = kernelShape[3];\n+\n+        if (inChannels != kernelInChannels)\n+            throw new ArgumentException($\"Input channels ({inChannels}) must match kernel input channels ({kernelInChannels})\");\n+\n+        int strideH = stride[0];\n+        int strideW = stride[1];\n+        int padH = padding[0];\n+        int padW = padding[1];\n+\n+        int outH = (inH + 2 * padH - kernelH) / strideH + 1;\n+        int outW = (inW + 2 * padW - kernelW) / strideW + 1;\n+\n+        var result = new Tensor<T>(new int[] { batch, outChannels, outH, outW });\n+\n+        // Forward pass: convolution\n+        for (int b = 0; b < batch; b++)\n+        {\n+            for (int oc = 0; oc < outChannels; oc++)\n+            {\n+                for (int oh = 0; oh < outH; oh++)\n+                {\n+                    for (int ow = 0; ow < outW; ow++)\n+                    {\n+                        var sum = numOps.Zero;\n+\n+                        // Convolve kernel over input\n+                        for (int ic = 0; ic < inChannels; ic++)\n+                        {\n+                            for (int kh = 0; kh < kernelH; kh++)\n+                            {\n+                                for (int kw = 0; kw < kernelW; kw++)\n+                                {\n+                                    int ih = oh * strideH + kh - padH;\n+                                    int iw = ow * strideW + kw - padW;\n+\n+                                    // Check bounds (padding)\n+                                    if (ih >= 0 && ih < inH && iw >= 0 && iw < inW)\n+                                    {\n+                                        var inputVal = input.Value[b, ic, ih, iw];\n+                                        var kernelVal = kernel.Value[oc, ic, kh, kw];\n+                                        sum = numOps.Add(sum, numOps.Multiply(inputVal, kernelVal));\n+                                    }\n+                                }\n+                            }\n+                        }\n+\n+                        // Add bias if provided\n+                        if (bias != null)\n+                        {\n+                            sum = numOps.Add(sum, bias.Value[oc]);\n+                        }\n+\n+                        result[b, oc, oh, ow] = sum;\n+                    }\n+                }\n+            }\n+        }\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            // Gradient w.r.t. input\n+            if (input.RequiresGradient)\n+            {\n+                var gradInput = new Tensor<T>(inputShape);\n+\n+                // Full convolution with flipped kernel\n+                for (int b = 0; b < batch; b++)\n+                {\n+                    for (int ic = 0; ic < inChannels; ic++)\n+                    {\n+                        for (int ih = 0; ih < inH; ih++)\n+                        {\n+                            for (int iw = 0; iw < inW; iw++)\n+                            {\n+                                var sum = numOps.Zero;\n+\n+                                // Iterate over all output positions that used this input position\n+                                for (int oc = 0; oc < outChannels; oc++)\n+                                {\n+                                    for (int kh = 0; kh < kernelH; kh++)\n+                                    {\n+                                        for (int kw = 0; kw < kernelW; kw++)\n+                                        {\n+                                            // Compute output position\n+                                            int ohShifted = ih + padH - kh;\n+                                            int owShifted = iw + padW - kw;\n+\n+                                            if (ohShifted % strideH == 0 && owShifted % strideW == 0)\n+                                            {\n+                                                int oh = ohShifted / strideH;\n+                                                int ow = owShifted / strideW;\n+\n+                                                if (oh >= 0 && oh < outH && ow >= 0 && ow < outW)\n+                                                {\n+                                                    var gradVal = gradient[b, oc, oh, ow];\n+                                                    var kernelVal = kernel.Value[oc, ic, kh, kw];\n+                                                    sum = numOps.Add(sum, numOps.Multiply(gradVal, kernelVal));\n+                                                }\n+                                            }\n+                                        }\n+                                    }\n+                                }\n+\n+                                gradInput[b, ic, ih, iw] = sum;\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (input.Gradient == null)\n+                    input.Gradient = gradInput;\n+                else\n+                    input.Gradient = input.Gradient.Add(gradInput);\n+            }\n+\n+            // Gradient w.r.t. kernel\n+            if (kernel.RequiresGradient)\n+            {\n+                var gradKernel = new Tensor<T>(kernelShape);\n+\n+                // Cross-correlation between input and output gradient\n+                for (int oc = 0; oc < outChannels; oc++)\n+                {\n+                    for (int ic = 0; ic < inChannels; ic++)\n+                    {\n+                        for (int kh = 0; kh < kernelH; kh++)\n+                        {\n+                            for (int kw = 0; kw < kernelW; kw++)\n+                            {\n+                                var sum = numOps.Zero;\n+\n+                                for (int b = 0; b < batch; b++)\n+                                {\n+                                    for (int oh = 0; oh < outH; oh++)\n+                                    {\n+                                        for (int ow = 0; ow < outW; ow++)\n+                                        {\n+                                            int ih = oh * strideH + kh - padH;\n+                                            int iw = ow * strideW + kw - padW;\n+\n+                                            if (ih >= 0 && ih < inH && iw >= 0 && iw < inW)\n+                                            {\n+                                                var gradVal = gradient[b, oc, oh, ow];\n+                                                var inputVal = input.Value[b, ic, ih, iw];\n+                                                sum = numOps.Add(sum, numOps.Multiply(gradVal, inputVal));\n+                                            }\n+                                        }\n+                                    }\n+                                }\n+\n+                                gradKernel[oc, ic, kh, kw] = sum;\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (kernel.Gradient == null)\n+                    kernel.Gradient = gradKernel;\n+                else\n+                    kernel.Gradient = kernel.Gradient.Add(gradKernel);\n+            }\n+\n+            // Gradient w.r.t. bias\n+            if (bias != null && bias.RequiresGradient)\n+            {\n+                var gradBias = new Tensor<T>(new int[] { outChannels });\n+\n+                for (int oc = 0; oc < outChannels; oc++)\n+                {\n+                    var sum = numOps.Zero;\n+                    for (int b = 0; b < batch; b++)\n+                    {\n+                        for (int oh = 0; oh < outH; oh++)\n+                        {\n+                            for (int ow = 0; ow < outW; ow++)\n+                            {\n+                                sum = numOps.Add(sum, gradient[b, oc, oh, ow]);\n+                            }\n+                        }\n+                    }\n+                    gradBias[oc] = sum;\n+                }\n+\n+                if (bias.Gradient == null)\n+                    bias.Gradient = gradBias;\n+                else\n+                    bias.Gradient = bias.Gradient.Add(gradBias);"}]}},{"id":"PRRT_kwDOKSXUF85hck_K","isResolved":false,"isOutdated":false,"path":"src/Autodiff/TensorOperations.cs","line":2894,"originalLine":2894,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzD_q","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement write to the same variable - consider using '?' to express intent better.","createdAt":"2025-11-11T07:53:27Z","path":"src/Autodiff/TensorOperations.cs","line":2894,"originalLine":2894,"diffHunk":"@@ -1219,4 +1219,1769 @@ void BackwardFunction(Tensor<T> gradient)\n \n         return node;\n     }\n+\n+    /// <summary>\n+    /// Computes the softmax function for a computation node along a specified axis.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"axis\">The axis along which to compute softmax. Default is -1 (last axis).</param>\n+    /// <returns>A new computation node containing the softmax result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method computes softmax (σ(x_i) = exp(x_i) / Σexp(x_j)) along the specified axis.\n+    /// Uses numerical stability trick: subtract max before exponentiating.\n+    /// The backward function uses: ∂softmax/∂x = softmax(x) * (grad - Σ(grad * softmax(x))).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Softmax converts a vector of numbers into probabilities.\n+    ///\n+    /// For softmax:\n+    /// - The forward pass exponentiates each element, then normalizes so they sum to 1\n+    /// - The result is a probability distribution (all values between 0 and 1, summing to 1)\n+    /// - The backward pass is complex but efficient: uses the Jacobian of softmax\n+    ///\n+    /// Softmax is crucial for:\n+    /// - Multi-class classification (final layer outputs)\n+    /// - Attention mechanisms (computing attention weights)\n+    /// - Anywhere you need to convert scores to probabilities\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Softmax(ComputationNode<T> a, int axis = -1)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        // Normalize axis to positive index\n+        if (axis < 0)\n+            axis = shape.Length + axis;\n+\n+        // For simplicity, handle 2D case (batch, features) with axis=-1\n+        if (shape.Length == 2 && axis == 1)\n+        {\n+            int batchSize = shape[0];\n+            int features = shape[1];\n+            var result = new Tensor<T>(shape);\n+\n+            // Compute softmax for each row\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                // Find max for numerical stability\n+                var maxVal = a.Value[b, 0];\n+                for (int f = 1; f < features; f++)\n+                {\n+                    if (numOps.GreaterThan(a.Value[b, f], maxVal))\n+                        maxVal = a.Value[b, f];\n+                }\n+\n+                // Compute exp(x - max) and sum\n+                var expSum = numOps.Zero;\n+                var expValues = new T[features];\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var shifted = numOps.Subtract(a.Value[b, f], maxVal);\n+                    expValues[f] = numOps.Exp(shifted);\n+                    expSum = numOps.Add(expSum, expValues[f]);\n+                }\n+\n+                // Normalize\n+                for (int f = 0; f < features; f++)\n+                {\n+                    result[b, f] = numOps.Divide(expValues[f], expSum);\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (a.RequiresGradient)\n+                {\n+                    // ∂softmax/∂x_i = softmax_i * (∂L/∂y_i - Σ_j(∂L/∂y_j * softmax_j))\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        // Compute sum of (gradient * softmax)\n+                        var dotProduct = numOps.Zero;\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            dotProduct = numOps.Add(dotProduct,\n+                                numOps.Multiply(gradient[b, f], result[b, f]));\n+                        }\n+\n+                        // Compute gradient for each element\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            var gradMinusDot = numOps.Subtract(gradient[b, f], dotProduct);\n+                            gradA[b, f] = numOps.Multiply(result[b, f], gradMinusDot);\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient,\n+                parents: new List<ComputationNode<T>> { a },\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Softmax is currently only implemented for 2D tensors along axis=-1. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}], axis={axis}\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Concatenates multiple computation nodes along a specified axis.\n+    /// </summary>\n+    /// <param name=\"nodes\">The list of nodes to concatenate.</param>\n+    /// <param name=\"axis\">The axis along which to concatenate. Default is 0.</param>\n+    /// <returns>A new computation node containing the concatenated result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method concatenates tensors along the specified axis.\n+    /// All tensors must have the same shape except along the concatenation axis.\n+    /// The backward function splits the gradient and sends each portion to the corresponding input.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Concat stacks tensors together along a dimension.\n+    ///\n+    /// For concatenation:\n+    /// - The forward pass combines multiple tensors into one larger tensor\n+    /// - The backward pass splits the gradient back to each input\n+    /// - Think of it like gluing arrays together end-to-end\n+    ///\n+    /// Used in:\n+    /// - Skip connections (concatenating features from different layers)\n+    /// - Multi-input architectures\n+    /// - Feature fusion in neural networks\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Concat(List<ComputationNode<T>> nodes, int axis = 0)\n+    {\n+        if (nodes.Count == 0)\n+            throw new ArgumentException(\"Cannot concatenate empty list of nodes\");\n+\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var firstShape = nodes[0].Value.Shape;\n+\n+        // Normalize axis\n+        if (axis < 0)\n+            axis = firstShape.Length + axis;\n+\n+        // Validate shapes match except on concat axis\n+        for (int i = 1; i < nodes.Count; i++)\n+        {\n+            var shape = nodes[i].Value.Shape;\n+            if (shape.Length != firstShape.Length)\n+                throw new ArgumentException(\"All tensors must have the same rank\");\n+\n+            for (int d = 0; d < firstShape.Length; d++)\n+            {\n+                if (d != axis && shape[d] != firstShape[d])\n+                    throw new ArgumentException(\n+                        $\"Shape mismatch at dimension {d}: {shape[d]} vs {firstShape[d]}\");\n+            }\n+        }\n+\n+        // Compute output shape\n+        int[] outputShape = (int[])firstShape.Clone();\n+        for (int i = 1; i < nodes.Count; i++)\n+        {\n+            outputShape[axis] += nodes[i].Value.Shape[axis];\n+        }\n+\n+        // Perform concatenation (handle 2D case for simplicity)\n+        Tensor<T> result;\n+        if (firstShape.Length == 2 && axis == 1)\n+        {\n+            // Concatenate along columns (features)\n+            int rows = firstShape[0];\n+            int totalCols = outputShape[1];\n+            result = new Tensor<T>(new int[] { rows, totalCols });\n+\n+            int colOffset = 0;\n+            foreach (var node in nodes)\n+            {\n+                int cols = node.Value.Shape[1];\n+                for (int r = 0; r < rows; r++)\n+                {\n+                    for (int c = 0; c < cols; c++)\n+                    {\n+                        result[r, colOffset + c] = node.Value[r, c];\n+                    }\n+                }\n+                colOffset += cols;\n+            }\n+        }\n+        else if (firstShape.Length == 2 && axis == 0)\n+        {\n+            // Concatenate along rows (batch)\n+            int cols = firstShape[1];\n+            int totalRows = outputShape[0];\n+            result = new Tensor<T>(new int[] { totalRows, cols });\n+\n+            int rowOffset = 0;\n+            foreach (var node in nodes)\n+            {\n+                int rows = node.Value.Shape[0];\n+                for (int r = 0; r < rows; r++)\n+                {\n+                    for (int c = 0; c < cols; c++)\n+                    {\n+                        result[rowOffset + r, c] = node.Value[r, c];\n+                    }\n+                }\n+                rowOffset += rows;\n+            }\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Concat is currently only implemented for 2D tensors. \" +\n+                $\"Got shape=[{string.Join(\", \", firstShape)}]\");\n+        }\n+\n+        // Store sizes for gradient splitting\n+        var sizes = nodes.Select(n => n.Value.Shape[axis]).ToList();\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            // Split gradient along concat axis and distribute to inputs\n+            if (firstShape.Length == 2 && axis == 1)\n+            {\n+                int rows = firstShape[0];\n+                int colOffset = 0;\n+                for (int i = 0; i < nodes.Count; i++)\n+                {\n+                    if (!nodes[i].RequiresGradient)\n+                    {\n+                        colOffset += sizes[i];\n+                        continue;\n+                    }\n+\n+                    int cols = sizes[i];\n+                    var gradPart = new Tensor<T>(new int[] { rows, cols });\n+\n+                    for (int r = 0; r < rows; r++)\n+                    {\n+                        for (int c = 0; c < cols; c++)\n+                        {\n+                            gradPart[r, c] = gradient[r, colOffset + c];\n+                        }\n+                    }\n+\n+                    if (nodes[i].Gradient == null)\n+                        nodes[i].Gradient = gradPart;\n+                    else\n+                        nodes[i].Gradient = nodes[i].Gradient.Add(gradPart);\n+\n+                    colOffset += cols;\n+                }\n+            }\n+            else if (firstShape.Length == 2 && axis == 0)\n+            {\n+                int cols = firstShape[1];\n+                int rowOffset = 0;\n+                for (int i = 0; i < nodes.Count; i++)\n+                {\n+                    if (!nodes[i].RequiresGradient)\n+                    {\n+                        rowOffset += sizes[i];\n+                        continue;\n+                    }\n+\n+                    int rows = sizes[i];\n+                    var gradPart = new Tensor<T>(new int[] { rows, cols });\n+\n+                    for (int r = 0; r < rows; r++)\n+                    {\n+                        for (int c = 0; c < cols; c++)\n+                        {\n+                            gradPart[r, c] = gradient[rowOffset + r, c];\n+                        }\n+                    }\n+\n+                    if (nodes[i].Gradient == null)\n+                        nodes[i].Gradient = gradPart;\n+                    else\n+                        nodes[i].Gradient = nodes[i].Gradient.Add(gradPart);\n+\n+                    rowOffset += rows;\n+                }\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: nodes.Any(n => n.RequiresGradient),\n+            parents: nodes,\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Pads a tensor with a constant value along specified dimensions.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"padWidth\">Padding width for each dimension as (before, after) pairs.</param>\n+    /// <param name=\"value\">The value to use for padding. Default is zero.</param>\n+    /// <returns>A new computation node containing the padded result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method adds padding around the tensor.\n+    /// The backward function simply crops the gradient back to the original size (gradients for padding are zero).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Pad adds extra elements around a tensor.\n+    ///\n+    /// For padding:\n+    /// - The forward pass adds border elements with a constant value\n+    /// - The backward pass removes those border gradients (they don't affect the original tensor)\n+    /// - Think of it like adding margins to an image\n+    ///\n+    /// Used in:\n+    /// - Convolutional layers (to maintain spatial dimensions)\n+    /// - Handling variable-length sequences\n+    /// - Data augmentation\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Pad(ComputationNode<T> a, int[,] padWidth, T? value = default)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var padValue = value ?? numOps.Zero;\n+        var shape = a.Value.Shape;\n+\n+        // Validate padWidth dimensions\n+        if (padWidth.GetLength(0) != shape.Length)\n+            throw new ArgumentException(\"padWidth must have same number of dimensions as input tensor\");\n+\n+        // Compute output shape\n+        var outputShape = new int[shape.Length];\n+        for (int d = 0; d < shape.Length; d++)\n+        {\n+            outputShape[d] = shape[d] + padWidth[d, 0] + padWidth[d, 1];\n+        }\n+\n+        // Handle 2D case\n+        if (shape.Length == 2)\n+        {\n+            int inputRows = shape[0];\n+            int inputCols = shape[1];\n+            int padTop = padWidth[0, 0];\n+            int padBottom = padWidth[0, 1];\n+            int padLeft = padWidth[1, 0];\n+            int padRight = padWidth[1, 1];\n+\n+            var result = new Tensor<T>(outputShape);\n+\n+            // Initialize with pad value\n+            for (int i = 0; i < result.Length; i++)\n+            {\n+                result[i] = padValue;\n+            }\n+\n+            // Copy input data to center\n+            for (int r = 0; r < inputRows; r++)\n+            {\n+                for (int c = 0; c < inputCols; c++)\n+                {\n+                    result[padTop + r, padLeft + c] = a.Value[r, c];\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (a.RequiresGradient)\n+                {\n+                    // Extract gradient for original (unpadded) region\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int r = 0; r < inputRows; r++)\n+                    {\n+                        for (int c = 0; c < inputCols; c++)\n+                        {\n+                            gradA[r, c] = gradient[padTop + r, padLeft + c];\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient,\n+                parents: new List<ComputationNode<T>> { a },\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Pad is currently only implemented for 2D tensors. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}]\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs 2D max pooling on a 4D tensor (batch, channels, height, width).\n+    /// </summary>\n+    /// <param name=\"a\">The input node with shape [batch, channels, height, width].</param>\n+    /// <param name=\"poolSize\">The size of the pooling window [poolH, poolW].</param>\n+    /// <param name=\"strides\">The stride for the pooling operation [strideH, strideW]. If null, uses poolSize.</param>\n+    /// <returns>A new computation node containing the max pooled result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method performs max pooling over 2D spatial dimensions.\n+    /// During forward pass, it tracks which element was the max for routing gradients during backward pass.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> MaxPool downsamples by taking the maximum value in each window.\n+    ///\n+    /// For max pooling:\n+    /// - The forward pass slides a window and takes the max value in each position\n+    /// - This reduces spatial dimensions (downsampling)\n+    /// - The backward pass routes gradients only to the positions that were max\n+    /// - Other positions get zero gradient (they didn't contribute to the output)\n+    ///\n+    /// Used in:\n+    /// - CNNs for translation invariance\n+    /// - Reducing spatial resolution\n+    /// - Building hierarchical features\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> MaxPool2D(\n+        ComputationNode<T> a,\n+        int[] poolSize,\n+        int[]? strides = null)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        if (shape.Length != 4)\n+            throw new ArgumentException(\"MaxPool2D requires 4D input [batch, channels, height, width]\");\n+\n+        strides ??= poolSize;\n+\n+        int batch = shape[0];\n+        int channels = shape[1];\n+        int inH = shape[2];\n+        int inW = shape[3];\n+        int poolH = poolSize[0];\n+        int poolW = poolSize[1];\n+        int strideH = strides[0];\n+        int strideW = strides[1];\n+\n+        int outH = (inH - poolH) / strideH + 1;\n+        int outW = (inW - poolW) / strideW + 1;\n+\n+        var result = new Tensor<T>(new int[] { batch, channels, outH, outW });\n+        // Store max positions for backprop\n+        var maxPositions = new int[batch, channels, outH, outW, 2]; // [h_offset, w_offset]\n+\n+        // Forward pass: compute max pooling and track positions\n+        for (int b = 0; b < batch; b++)\n+        {\n+            for (int c = 0; c < channels; c++)\n+            {\n+                for (int oh = 0; oh < outH; oh++)\n+                {\n+                    for (int ow = 0; ow < outW; ow++)\n+                    {\n+                        int hStart = oh * strideH;\n+                        int wStart = ow * strideW;\n+\n+                        var maxVal = a.Value[b * channels * inH * inW +\n+                                           c * inH * inW +\n+                                           hStart * inW +\n+                                           wStart];\n+                        int maxHOffset = 0;\n+                        int maxWOffset = 0;\n+\n+                        // Find max in pooling window\n+                        for (int ph = 0; ph < poolH; ph++)\n+                        {\n+                            for (int pw = 0; pw < poolW; pw++)\n+                            {\n+                                int h = hStart + ph;\n+                                int w = wStart + pw;\n+                                if (h < inH && w < inW)\n+                                {\n+                                    var val = a.Value[b * channels * inH * inW +\n+                                                     c * inH * inW +\n+                                                     h * inW +\n+                                                     w];\n+                                    if (numOps.GreaterThan(val, maxVal))\n+                                    {\n+                                        maxVal = val;\n+                                        maxHOffset = ph;\n+                                        maxWOffset = pw;\n+                                    }\n+                                }\n+                            }\n+                        }\n+\n+                        result[b, c, oh, ow] = maxVal;\n+                        maxPositions[b, c, oh, ow, 0] = maxHOffset;\n+                        maxPositions[b, c, oh, ow, 1] = maxWOffset;\n+                    }\n+                }\n+            }\n+        }\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            if (a.RequiresGradient)\n+            {\n+                var gradA = new Tensor<T>(shape);\n+\n+                // Route gradients to max positions\n+                for (int b = 0; b < batch; b++)\n+                {\n+                    for (int c = 0; c < channels; c++)\n+                    {\n+                        for (int oh = 0; oh < outH; oh++)\n+                        {\n+                            for (int ow = 0; ow < outW; ow++)\n+                            {\n+                                int hStart = oh * strideH;\n+                                int wStart = ow * strideW;\n+                                int maxHOffset = maxPositions[b, c, oh, ow, 0];\n+                                int maxWOffset = maxPositions[b, c, oh, ow, 1];\n+\n+                                int maxH = hStart + maxHOffset;\n+                                int maxW = wStart + maxWOffset;\n+\n+                                int gradIdx = b * channels * inH * inW +\n+                                             c * inH * inW +\n+                                            maxH * inW +\n+                                             maxW;\n+\n+                                gradA[gradIdx] = numOps.Add(gradA[gradIdx], gradient[b, c, oh, ow]);\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (a.Gradient == null)\n+                    a.Gradient = gradA;\n+                else\n+                    a.Gradient = a.Gradient.Add(gradA);\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: a.RequiresGradient,\n+            parents: new List<ComputationNode<T>> { a },\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Performs 2D average pooling on a 4D tensor (batch, channels, height, width).\n+    /// </summary>\n+    /// <param name=\"a\">The input node with shape [batch, channels, height, width].</param>\n+    /// <param name=\"poolSize\">The size of the pooling window [poolH, poolW].</param>\n+    /// <param name=\"strides\">The stride for the pooling operation [strideH, strideW]. If null, uses poolSize.</param>\n+    /// <returns>A new computation node containing the average pooled result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method performs average pooling over 2D spatial dimensions.\n+    /// The backward function distributes gradients equally across the pooling window.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> AvgPool downsamples by taking the average value in each window.\n+    ///\n+    /// For average pooling:\n+    /// - The forward pass slides a window and computes the average\n+    /// - This smoothly reduces spatial dimensions\n+    /// - The backward pass distributes gradients equally to all elements in the window\n+    /// - Each element gets gradient / pool_area\n+    ///\n+    /// Used in:\n+    /// - CNNs for smoother downsampling than max pooling\n+    /// - Global average pooling (replacing fully connected layers)\n+    /// - Reducing overfitting\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> AvgPool2D(\n+        ComputationNode<T> a,\n+        int[] poolSize,\n+        int[]? strides = null)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        if (shape.Length != 4)\n+            throw new ArgumentException(\"AvgPool2D requires 4D input [batch, channels, height, width]\");\n+\n+        strides ??= poolSize;\n+\n+        int batch = shape[0];\n+        int channels = shape[1];\n+        int inH = shape[2];\n+        int inW = shape[3];\n+        int poolH = poolSize[0];\n+        int poolW = poolSize[1];\n+        int strideH = strides[0];\n+        int strideW = strides[1];\n+\n+        int outH = (inH - poolH) / strideH + 1;\n+        int outW = (inW - poolW) / strideW + 1;\n+\n+        var result = new Tensor<T>(new int[] { batch, channels, outH, outW });\n+        var poolArea = numOps.FromDouble(poolH * poolW);\n+\n+        // Forward pass: compute average pooling\n+        for (int b = 0; b < batch; b++)\n+        {\n+            for (int c = 0; c < channels; c++)\n+            {\n+                for (int oh = 0; oh < outH; oh++)\n+                {\n+                    for (int ow = 0; ow < outW; ow++)\n+                    {\n+                        int hStart = oh * strideH;\n+                        int wStart = ow * strideW;\n+\n+                        var sum = numOps.Zero;\n+\n+                        // Sum values in pooling window\n+                        for (int ph = 0; ph < poolH; ph++)\n+                        {\n+                            for (int pw = 0; pw < poolW; pw++)\n+                            {\n+                                int h = hStart + ph;\n+                                int w = wStart + pw;\n+                                if (h < inH && w < inW)\n+                                {\n+                                    sum = numOps.Add(sum, a.Value[b, c, h, w]);\n+                                }\n+                            }\n+                        }\n+\n+                        result[b, c, oh, ow] = numOps.Divide(sum, poolArea);\n+                    }\n+                }\n+            }\n+        }\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            if (a.RequiresGradient)\n+            {\n+                var gradA = new Tensor<T>(shape);\n+\n+                // Distribute gradients equally across pooling windows\n+                for (int b = 0; b < batch; b++)\n+                {\n+                    for (int c = 0; c < channels; c++)\n+                    {\n+                        for (int oh = 0; oh < outH; oh++)\n+                        {\n+                            for (int ow = 0; ow < outW; ow++)\n+                            {\n+                                int hStart = oh * strideH;\n+                                int wStart = ow * strideW;\n+\n+                                var gradValue = numOps.Divide(gradient[b, c, oh, ow], poolArea);\n+\n+                                // Distribute to all elements in window\n+                                for (int ph = 0; ph < poolH; ph++)\n+                                {\n+                                    for (int pw = 0; pw < poolW; pw++)\n+                                    {\n+                                        int h = hStart + ph;\n+                                        int w = wStart + pw;\n+                                        if (h < inH && w < inW)\n+                                        {\n+                                            gradA[b, c, h, w] = numOps.Add(gradA[b, c, h, w], gradValue);\n+                                        }\n+                                    }\n+                                }\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (a.Gradient == null)\n+                    a.Gradient = gradA;\n+                else\n+                    a.Gradient = a.Gradient.Add(gradA);\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: a.RequiresGradient,\n+            parents: new List<ComputationNode<T>> { a },\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Applies layer normalization to a computation node.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"normalizedShape\">The shape over which to normalize (typically the feature dimensions).</param>\n+    /// <param name=\"gamma\">Optional scale parameter (learnable). If null, uses ones.</param>\n+    /// <param name=\"beta\">Optional shift parameter (learnable). If null, uses zeros.</param>\n+    /// <param name=\"epsilon\">Small constant for numerical stability. Default is 1e-5.</param>\n+    /// <returns>A new computation node containing the layer normalized result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Layer normalization normalizes inputs across the feature dimension for each sample independently.\n+    /// Formula: y = gamma * (x - mean) / sqrt(variance + epsilon) + beta\n+    /// Unlike batch normalization, this doesn't depend on batch statistics.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> LayerNorm standardizes features for each sample independently.\n+    ///\n+    /// For layer normalization:\n+    /// - Computes mean and variance for each sample's features\n+    /// - Normalizes: (x - mean) / sqrt(variance)\n+    /// - Scales and shifts: result * gamma + beta\n+    /// - Works the same during training and inference (no batch dependency)\n+    ///\n+    /// Used in:\n+    /// - Transformers (critical component)\n+    /// - RNNs (stabilizes training)\n+    /// - Any architecture needing sample-independent normalization\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> LayerNorm(\n+        ComputationNode<T> a,\n+        int[] normalizedShape,\n+        ComputationNode<T>? gamma = null,\n+        ComputationNode<T>? beta = null,\n+        double epsilon = 1e-5)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+        var eps = numOps.FromDouble(epsilon);\n+\n+        // For 2D input [batch, features], normalize over features\n+        if (shape.Length == 2 && normalizedShape.Length == 1 && normalizedShape[0] == shape[1])\n+        {\n+            int batchSize = shape[0];\n+            int features = shape[1];\n+\n+            // Create default gamma (ones) and beta (zeros) if not provided\n+            if (gamma == null)\n+            {\n+                var gammaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    gammaTensor[i] = numOps.One;\n+                gamma = Variable(gammaTensor, requiresGradient: false);\n+            }\n+\n+            if (beta == null)\n+            {\n+                var betaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    betaTensor[i] = numOps.Zero;\n+                beta = Variable(betaTensor, requiresGradient: false);\n+            }\n+\n+            var result = new Tensor<T>(shape);\n+            var means = new T[batchSize];\n+            var variances = new T[batchSize];\n+            var normalized = new Tensor<T>(shape);\n+\n+            // Forward pass: compute mean and variance per sample\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                // Compute mean\n+                var sum = numOps.Zero;\n+                for (int f = 0; f < features; f++)\n+                {\n+                    sum = numOps.Add(sum, a.Value[b, f]);\n+                }\n+                means[b] = numOps.Divide(sum, numOps.FromDouble(features));\n+\n+                // Compute variance\n+                var varSum = numOps.Zero;\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var diff = numOps.Subtract(a.Value[b, f], means[b]);\n+                    varSum = numOps.Add(varSum, numOps.Multiply(diff, diff));\n+                }\n+                variances[b] = numOps.Divide(varSum, numOps.FromDouble(features));\n+\n+                // Normalize and scale\n+                var std = numOps.Sqrt(numOps.Add(variances[b], eps));\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var norm = numOps.Divide(\n+                        numOps.Subtract(a.Value[b, f], means[b]),\n+                        std);\n+                    normalized[b, f] = norm;\n+                    result[b, f] = numOps.Add(\n+                        numOps.Multiply(norm, gamma.Value[f]),\n+                        beta.Value[f]);\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                // Gradients for gamma and beta\n+                if (gamma.RequiresGradient)\n+                {\n+                    var gradGamma = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum,\n+                                numOps.Multiply(gradient[b, f], normalized[b, f]));\n+                        }\n+                        gradGamma[f] = sum;\n+                    }\n+\n+                    if (gamma.Gradient == null)\n+                        gamma.Gradient = gradGamma;\n+                    else\n+                        gamma.Gradient = gamma.Gradient.Add(gradGamma);\n+                }\n+\n+                if (beta.RequiresGradient)\n+                {\n+                    var gradBeta = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum, gradient[b, f]);\n+                        }\n+                        gradBeta[f] = sum;\n+                    }\n+\n+                    if (beta.Gradient == null)\n+                        beta.Gradient = gradBeta;\n+                    else\n+                        beta.Gradient = beta.Gradient.Add(gradBeta);\n+                }\n+\n+                // Gradient for input\n+                if (a.RequiresGradient)\n+                {\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        var std = numOps.Sqrt(numOps.Add(variances[b], eps));\n+                        var invStd = numOps.Divide(numOps.One, std);\n+\n+                        // Compute gradient components\n+                        var gradNormSum = numOps.Zero;\n+                        var gradNormDotNorm = numOps.Zero;\n+\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            var gradNorm = numOps.Multiply(gradient[b, f], gamma.Value[f]);\n+                            gradNormSum = numOps.Add(gradNormSum, gradNorm);\n+                            gradNormDotNorm = numOps.Add(gradNormDotNorm,\n+                                numOps.Multiply(gradNorm, normalized[b, f]));\n+                        }\n+\n+                        // Apply gradient formula\n+                        var featuresT = numOps.FromDouble(features);\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            var gradNorm = numOps.Multiply(gradient[b, f], gamma.Value[f]);\n+\n+                            var term1 = gradNorm;\n+                            var term2 = numOps.Divide(gradNormSum, featuresT);\n+                            var term3 = numOps.Divide(\n+                                numOps.Multiply(normalized[b, f], gradNormDotNorm),\n+                                featuresT);\n+\n+                            var grad = numOps.Multiply(\n+                                numOps.Subtract(numOps.Subtract(term1, term2), term3),\n+                                invStd);\n+\n+                            gradA[b, f] = grad;\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var parents = new List<ComputationNode<T>> { a };\n+            if (gamma != null) parents.Add(gamma);\n+            if (beta != null) parents.Add(beta);\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient || (gamma?.RequiresGradient ?? false) || (beta?.RequiresGradient ?? false),\n+                parents: parents,\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"LayerNorm is currently only implemented for 2D tensors normalizing over last dimension. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}], normalizedShape=[{string.Join(\", \", normalizedShape)}]\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Applies batch normalization to a computation node.\n+    /// </summary>\n+    /// <param name=\"a\">The input node with shape [batch, features].</param>\n+    /// <param name=\"gamma\">Optional scale parameter (learnable). If null, uses ones.</param>\n+    /// <param name=\"beta\">Optional shift parameter (learnable). If null, uses zeros.</param>\n+    /// <param name=\"runningMean\">Running mean for inference (not updated during this operation).</param>\n+    /// <param name=\"runningVar\">Running variance for inference (not updated during this operation).</param>\n+    /// <param name=\"training\">Whether in training mode (uses batch statistics) or inference mode (uses running statistics).</param>\n+    /// <param name=\"epsilon\">Small constant for numerical stability. Default is 1e-5.</param>\n+    /// <returns>A new computation node containing the batch normalized result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Batch normalization normalizes inputs across the batch dimension.\n+    /// During training: Uses batch statistics (mean and variance computed from current batch).\n+    /// During inference: Uses running statistics (accumulated during training).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> BatchNorm standardizes features across the batch.\n+    ///\n+    /// For batch normalization:\n+    /// - Training mode: Uses current batch's mean and variance\n+    /// - Inference mode: Uses running mean/variance from training\n+    /// - Normalizes: (x - mean) / sqrt(variance)\n+    /// - Scales and shifts: result * gamma + beta\n+    ///\n+    /// Benefits:\n+    /// - Stabilizes training (reduces internal covariate shift)\n+    /// - Allows higher learning rates\n+    /// - Acts as regularization\n+    ///\n+    /// Used in:\n+    /// - CNNs (after convolutional layers)\n+    /// - Deep feedforward networks\n+    /// - GANs and many other architectures\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> BatchNorm(\n+        ComputationNode<T> a,\n+        ComputationNode<T>? gamma = null,\n+        ComputationNode<T>? beta = null,\n+        Tensor<T>? runningMean = null,\n+        Tensor<T>? runningVar = null,\n+        bool training = true,\n+        double epsilon = 1e-5)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+        var eps = numOps.FromDouble(epsilon);\n+\n+        // Handle 2D case [batch, features]\n+        if (shape.Length == 2)\n+        {\n+            int batchSize = shape[0];\n+            int features = shape[1];\n+\n+            // Create default gamma and beta if not provided\n+            if (gamma == null)\n+            {\n+                var gammaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    gammaTensor[i] = numOps.One;\n+                gamma = Variable(gammaTensor, requiresGradient: false);\n+            }\n+\n+            if (beta == null)\n+            {\n+                var betaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    betaTensor[i] = numOps.Zero;\n+                beta = Variable(betaTensor, requiresGradient: false);\n+            }\n+\n+            var result = new Tensor<T>(shape);\n+            T[] batchMean;\n+            T[] batchVar;\n+            var normalized = new Tensor<T>(shape);\n+\n+            if (training)\n+            {\n+                // Compute batch statistics\n+                batchMean = new T[features];\n+                batchVar = new T[features];\n+\n+                // Compute mean per feature\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var sum = numOps.Zero;\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        sum = numOps.Add(sum, a.Value[b, f]);\n+                    }\n+                    batchMean[f] = numOps.Divide(sum, numOps.FromDouble(batchSize));\n+                }\n+\n+                // Compute variance per feature\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var varSum = numOps.Zero;\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        var diff = numOps.Subtract(a.Value[b, f], batchMean[f]);\n+                        varSum = numOps.Add(varSum, numOps.Multiply(diff, diff));\n+                    }\n+                    batchVar[f] = numOps.Divide(varSum, numOps.FromDouble(batchSize));\n+                }\n+            }\n+            else\n+            {\n+                // Use running statistics for inference\n+                if (runningMean == null || runningVar == null)\n+                    throw new ArgumentException(\"Running statistics required for inference mode\");\n+\n+                batchMean = new T[features];\n+                batchVar = new T[features];\n+                for (int f = 0; f < features; f++)\n+                {\n+                    batchMean[f] = runningMean[f];\n+                    batchVar[f] = runningVar[f];\n+                }\n+            }\n+\n+            // Normalize and scale\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var std = numOps.Sqrt(numOps.Add(batchVar[f], eps));\n+                    var norm = numOps.Divide(\n+                        numOps.Subtract(a.Value[b, f], batchMean[f]),\n+                        std);\n+                    normalized[b, f] = norm;\n+                    result[b, f] = numOps.Add(\n+                        numOps.Multiply(norm, gamma.Value[f]),\n+                        beta.Value[f]);\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (!training)\n+                {\n+                    // Inference mode: simpler gradient (no batch statistics gradient)\n+                    if (a.RequiresGradient)\n+                    {\n+                        var gradA = new Tensor<T>(shape);\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            for (int f = 0; f < features; f++)\n+                            {\n+                                var std = numOps.Sqrt(numOps.Add(batchVar[f], eps));\n+                                var invStd = numOps.Divide(numOps.One, std);\n+                                gradA[b, f] = numOps.Multiply(\n+                                    numOps.Multiply(gradient[b, f], gamma.Value[f]),\n+                                    invStd);\n+                            }\n+                        }\n+\n+                        if (a.Gradient == null)\n+                            a.Gradient = gradA;\n+                        else\n+                            a.Gradient = a.Gradient.Add(gradA);\n+                    }\n+                    return;\n+                }\n+\n+                // Training mode: full gradient computation\n+                // Gradients for gamma and beta\n+                if (gamma.RequiresGradient)\n+                {\n+                    var gradGamma = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum,\n+                                numOps.Multiply(gradient[b, f], normalized[b, f]));\n+                        }\n+                        gradGamma[f] = sum;\n+                    }\n+\n+                    if (gamma.Gradient == null)\n+                        gamma.Gradient = gradGamma;\n+                    else\n+                        gamma.Gradient = gamma.Gradient.Add(gradGamma);\n+                }\n+\n+                if (beta.RequiresGradient)\n+                {\n+                    var gradBeta = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum, gradient[b, f]);\n+                        }\n+                        gradBeta[f] = sum;\n+                    }\n+\n+                    if (beta.Gradient == null)\n+                        beta.Gradient = gradBeta;\n+                    else\n+                        beta.Gradient = beta.Gradient.Add(gradBeta);\n+                }\n+\n+                // Gradient for input (complex due to batch statistics)\n+                if (a.RequiresGradient)\n+                {\n+                    var gradA = new Tensor<T>(shape);\n+                    var batchSizeT = numOps.FromDouble(batchSize);\n+\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var std = numOps.Sqrt(numOps.Add(batchVar[f], eps));\n+                        var invStd = numOps.Divide(numOps.One, std);\n+\n+                        // Sum of gradients and gradient*normalized\n+                        var gradSum = numOps.Zero;\n+                        var gradNormSum = numOps.Zero;\n+\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            var grad = numOps.Multiply(gradient[b, f], gamma.Value[f]);\n+                            gradSum = numOps.Add(gradSum, grad);\n+                            gradNormSum = numOps.Add(gradNormSum,\n+                                numOps.Multiply(grad, normalized[b, f]));\n+                        }\n+\n+                        // Apply gradient formula\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            var grad = numOps.Multiply(gradient[b, f], gamma.Value[f]);\n+\n+                            var term1 = grad;\n+                            var term2 = numOps.Divide(gradSum, batchSizeT);\n+                            var term3 = numOps.Divide(\n+                                numOps.Multiply(normalized[b, f], gradNormSum),\n+                                batchSizeT);\n+\n+                            var gradInput = numOps.Multiply(\n+                                numOps.Subtract(numOps.Subtract(term1, term2), term3),\n+                                invStd);\n+\n+                            gradA[b, f] = gradInput;\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var parents = new List<ComputationNode<T>> { a };\n+            if (gamma != null) parents.Add(gamma);\n+            if (beta != null) parents.Add(beta);\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient || (gamma?.RequiresGradient ?? false) || (beta?.RequiresGradient ?? false),\n+                parents: parents,\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"BatchNorm is currently only implemented for 2D tensors [batch, features]. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}]\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs 2D convolution on a 4D tensor (batch, channels, height, width).\n+    /// </summary>\n+    /// <param name=\"input\">The input node with shape [batch, inChannels, height, width].</param>\n+    /// <param name=\"kernel\">The kernel/filter with shape [outChannels, inChannels, kernelH, kernelW].</param>\n+    /// <param name=\"bias\">Optional bias with shape [outChannels]. If null, no bias is added.</param>\n+    /// <param name=\"stride\">The stride [strideH, strideW]. Default is [1, 1].</param>\n+    /// <param name=\"padding\">The padding [padH, padW]. Default is [0, 0].</param>\n+    /// <returns>A new computation node containing the convolution result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method performs 2D convolution, the fundamental operation in CNNs.\n+    /// Forward: Slides the kernel over the input computing dot products.\n+    /// Backward: Computes gradients for both input and kernel using transposed convolutions.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Conv2D is the core operation of convolutional neural networks.\n+    ///\n+    /// For 2D convolution:\n+    /// - The kernel \"slides\" over the input, computing weighted sums\n+    /// - Each output position is a dot product of the kernel with input patch\n+    /// - Stride controls how far the kernel moves each step\n+    /// - Padding adds borders to control output size\n+    ///\n+    /// Gradient computation:\n+    /// - Gradient w.r.t. input: \"full\" convolution with flipped kernel\n+    /// - Gradient w.r.t. kernel: cross-correlation between input and output gradient\n+    ///\n+    /// Used in:\n+    /// - All CNNs (image classification, object detection, segmentation)\n+    /// - Feature extraction in vision models\n+    /// - Learning spatial hierarchies\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Conv2D(\n+        ComputationNode<T> input,\n+        ComputationNode<T> kernel,\n+        ComputationNode<T>? bias = null,\n+        int[]? stride = null,\n+        int[]? padding = null)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var inputShape = input.Value.Shape;\n+        var kernelShape = kernel.Value.Shape;\n+\n+        if (inputShape.Length != 4)\n+            throw new ArgumentException(\"Conv2D requires 4D input [batch, inChannels, height, width]\");\n+        if (kernelShape.Length != 4)\n+            throw new ArgumentException(\"Conv2D requires 4D kernel [outChannels, inChannels, kernelH, kernelW]\");\n+\n+        stride ??= new int[] { 1, 1 };\n+        padding ??= new int[] { 0, 0 };\n+\n+        int batch = inputShape[0];\n+        int inChannels = inputShape[1];\n+        int inH = inputShape[2];\n+        int inW = inputShape[3];\n+\n+        int outChannels = kernelShape[0];\n+        int kernelInChannels = kernelShape[1];\n+        int kernelH = kernelShape[2];\n+        int kernelW = kernelShape[3];\n+\n+        if (inChannels != kernelInChannels)\n+            throw new ArgumentException($\"Input channels ({inChannels}) must match kernel input channels ({kernelInChannels})\");\n+\n+        int strideH = stride[0];\n+        int strideW = stride[1];\n+        int padH = padding[0];\n+        int padW = padding[1];\n+\n+        int outH = (inH + 2 * padH - kernelH) / strideH + 1;\n+        int outW = (inW + 2 * padW - kernelW) / strideW + 1;\n+\n+        var result = new Tensor<T>(new int[] { batch, outChannels, outH, outW });\n+\n+        // Forward pass: convolution\n+        for (int b = 0; b < batch; b++)\n+        {\n+            for (int oc = 0; oc < outChannels; oc++)\n+            {\n+                for (int oh = 0; oh < outH; oh++)\n+                {\n+                    for (int ow = 0; ow < outW; ow++)\n+                    {\n+                        var sum = numOps.Zero;\n+\n+                        // Convolve kernel over input\n+                        for (int ic = 0; ic < inChannels; ic++)\n+                        {\n+                            for (int kh = 0; kh < kernelH; kh++)\n+                            {\n+                                for (int kw = 0; kw < kernelW; kw++)\n+                                {\n+                                    int ih = oh * strideH + kh - padH;\n+                                    int iw = ow * strideW + kw - padW;\n+\n+                                    // Check bounds (padding)\n+                                    if (ih >= 0 && ih < inH && iw >= 0 && iw < inW)\n+                                    {\n+                                        var inputVal = input.Value[b, ic, ih, iw];\n+                                        var kernelVal = kernel.Value[oc, ic, kh, kw];\n+                                        sum = numOps.Add(sum, numOps.Multiply(inputVal, kernelVal));\n+                                    }\n+                                }\n+                            }\n+                        }\n+\n+                        // Add bias if provided\n+                        if (bias != null)\n+                        {\n+                            sum = numOps.Add(sum, bias.Value[oc]);\n+                        }\n+\n+                        result[b, oc, oh, ow] = sum;\n+                    }\n+                }\n+            }\n+        }\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            // Gradient w.r.t. input\n+            if (input.RequiresGradient)\n+            {\n+                var gradInput = new Tensor<T>(inputShape);\n+\n+                // Full convolution with flipped kernel\n+                for (int b = 0; b < batch; b++)\n+                {\n+                    for (int ic = 0; ic < inChannels; ic++)\n+                    {\n+                        for (int ih = 0; ih < inH; ih++)\n+                        {\n+                            for (int iw = 0; iw < inW; iw++)\n+                            {\n+                                var sum = numOps.Zero;\n+\n+                                // Iterate over all output positions that used this input position\n+                                for (int oc = 0; oc < outChannels; oc++)\n+                                {\n+                                    for (int kh = 0; kh < kernelH; kh++)\n+                                    {\n+                                        for (int kw = 0; kw < kernelW; kw++)\n+                                        {\n+                                            // Compute output position\n+                                            int ohShifted = ih + padH - kh;\n+                                            int owShifted = iw + padW - kw;\n+\n+                                            if (ohShifted % strideH == 0 && owShifted % strideW == 0)\n+                                            {\n+                                                int oh = ohShifted / strideH;\n+                                                int ow = owShifted / strideW;\n+\n+                                                if (oh >= 0 && oh < outH && ow >= 0 && ow < outW)\n+                                                {\n+                                                    var gradVal = gradient[b, oc, oh, ow];\n+                                                    var kernelVal = kernel.Value[oc, ic, kh, kw];\n+                                                    sum = numOps.Add(sum, numOps.Multiply(gradVal, kernelVal));\n+                                                }\n+                                            }\n+                                        }\n+                                    }\n+                                }\n+\n+                                gradInput[b, ic, ih, iw] = sum;\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (input.Gradient == null)\n+                    input.Gradient = gradInput;\n+                else\n+                    input.Gradient = input.Gradient.Add(gradInput);\n+            }\n+\n+            // Gradient w.r.t. kernel\n+            if (kernel.RequiresGradient)\n+            {\n+                var gradKernel = new Tensor<T>(kernelShape);\n+\n+                // Cross-correlation between input and output gradient\n+                for (int oc = 0; oc < outChannels; oc++)\n+                {\n+                    for (int ic = 0; ic < inChannels; ic++)\n+                    {\n+                        for (int kh = 0; kh < kernelH; kh++)\n+                        {\n+                            for (int kw = 0; kw < kernelW; kw++)\n+                            {\n+                                var sum = numOps.Zero;\n+\n+                                for (int b = 0; b < batch; b++)\n+                                {\n+                                    for (int oh = 0; oh < outH; oh++)\n+                                    {\n+                                        for (int ow = 0; ow < outW; ow++)\n+                                        {\n+                                            int ih = oh * strideH + kh - padH;\n+                                            int iw = ow * strideW + kw - padW;\n+\n+                                            if (ih >= 0 && ih < inH && iw >= 0 && iw < inW)\n+                                            {\n+                                                var gradVal = gradient[b, oc, oh, ow];\n+                                                var inputVal = input.Value[b, ic, ih, iw];\n+                                                sum = numOps.Add(sum, numOps.Multiply(gradVal, inputVal));\n+                                            }\n+                                        }\n+                                    }\n+                                }\n+\n+                                gradKernel[oc, ic, kh, kw] = sum;\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (kernel.Gradient == null)\n+                    kernel.Gradient = gradKernel;\n+                else\n+                    kernel.Gradient = kernel.Gradient.Add(gradKernel);\n+            }\n+\n+            // Gradient w.r.t. bias\n+            if (bias != null && bias.RequiresGradient)\n+            {\n+                var gradBias = new Tensor<T>(new int[] { outChannels });\n+\n+                for (int oc = 0; oc < outChannels; oc++)\n+                {\n+                    var sum = numOps.Zero;\n+                    for (int b = 0; b < batch; b++)\n+                    {\n+                        for (int oh = 0; oh < outH; oh++)\n+                        {\n+                            for (int ow = 0; ow < outW; ow++)\n+                            {\n+                                sum = numOps.Add(sum, gradient[b, oc, oh, ow]);\n+                            }\n+                        }\n+                    }\n+                    gradBias[oc] = sum;\n+                }\n+\n+                if (bias.Gradient == null)\n+                    bias.Gradient = gradBias;\n+                else\n+                    bias.Gradient = bias.Gradient.Add(gradBias);\n+            }\n+        }\n+\n+        var parents = new List<ComputationNode<T>> { input, kernel };\n+        if (bias != null) parents.Add(bias);\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: input.RequiresGradient || kernel.RequiresGradient || (bias?.RequiresGradient ?? false),\n+            parents: parents,\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Performs 2D transposed convolution (deconvolution) on a 4D tensor.\n+    /// </summary>\n+    /// <param name=\"input\">The input node with shape [batch, inChannels, height, width].</param>\n+    /// <param name=\"kernel\">The kernel with shape [inChannels, outChannels, kernelH, kernelW] (note: reversed from Conv2D).</param>\n+    /// <param name=\"bias\">Optional bias with shape [outChannels]. If null, no bias is added.</param>\n+    /// <param name=\"stride\">The stride [strideH, strideW]. Default is [1, 1].</param>\n+    /// <param name=\"padding\">The padding [padH, padW]. Default is [0, 0].</param>\n+    /// <param name=\"outputPadding\">Output padding [outPadH, outPadW] for size adjustment. Default is [0, 0].</param>\n+    /// <returns>A new computation node containing the transposed convolution result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Transposed convolution (often called deconvolution) upsamples the input.\n+    /// It's the gradient of Conv2D with respect to its input, used as a forward operation.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> ConvTranspose2D upsamples spatial dimensions.\n+    ///\n+    /// For transposed convolution:\n+    /// - Inserts zeros between input elements according to stride\n+    /// - Applies regular convolution to the expanded input\n+    /// - Results in larger spatial dimensions (upsampling)\n+    ///\n+    /// Used in:\n+    /// - Image generation (GANs, VAEs)\n+    /// - Semantic segmentation (U-Net decoder)\n+    /// - Super-resolution\n+    /// - Any task requiring upsampling\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> ConvTranspose2D(\n+        ComputationNode<T> input,\n+        ComputationNode<T> kernel,\n+        ComputationNode<T>? bias = null,\n+        int[]? stride = null,\n+        int[]? padding = null,\n+        int[]? outputPadding = null)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var inputShape = input.Value.Shape;\n+        var kernelShape = kernel.Value.Shape;\n+\n+        if (inputShape.Length != 4)\n+            throw new ArgumentException(\"ConvTranspose2D requires 4D input [batch, inChannels, height, width]\");\n+        if (kernelShape.Length != 4)\n+            throw new ArgumentException(\"ConvTranspose2D requires 4D kernel [inChannels, outChannels, kernelH, kernelW]\");\n+\n+        stride ??= new int[] { 1, 1 };\n+        padding ??= new int[] { 0, 0 };\n+        outputPadding ??= new int[] { 0, 0 };\n+\n+        int batch = inputShape[0];\n+        int inChannels = inputShape[1];\n+        int inH = inputShape[2];\n+        int inW = inputShape[3];\n+\n+        int kernelInChannels = kernelShape[0];\n+        int outChannels = kernelShape[1];\n+        int kernelH = kernelShape[2];\n+        int kernelW = kernelShape[3];\n+\n+        if (inChannels != kernelInChannels)\n+            throw new ArgumentException($\"Input channels ({inChannels}) must match kernel input channels ({kernelInChannels})\");\n+\n+        int strideH = stride[0];\n+        int strideW = stride[1];\n+        int padH = padding[0];\n+        int padW = padding[1];\n+        int outPadH = outputPadding[0];\n+        int outPadW = outputPadding[1];\n+\n+        int outH = (inH - 1) * strideH - 2 * padH + kernelH + outPadH;\n+        int outW = (inW - 1) * strideW - 2 * padW + kernelW + outPadW;\n+\n+        var result = new Tensor<T>(new int[] { batch, outChannels, outH, outW });\n+\n+        // Forward pass: transposed convolution\n+        for (int b = 0; b < batch; b++)\n+        {\n+            for (int ic = 0; ic < inChannels; ic++)\n+            {\n+                for (int ih = 0; ih < inH; ih++)\n+                {\n+                    for (int iw = 0; iw < inW; iw++)\n+                    {\n+                        var inputVal = input.Value[b, ic, ih, iw];\n+\n+                        // Distribute this input value to output using kernel\n+                        for (int oc = 0; oc < outChannels; oc++)\n+                        {\n+                            for (int kh = 0; kh < kernelH; kh++)\n+                            {\n+                                for (int kw = 0; kw < kernelW; kw++)\n+                                {\n+                                    int oh = ih * strideH + kh - padH;\n+                                    int ow = iw * strideW + kw - padW;\n+\n+                                    if (oh >= 0 && oh < outH && ow >= 0 && ow < outW)\n+                                    {\n+                                        var kernelVal = kernel.Value[ic, oc, kh, kw];\n+                                        var contribution = numOps.Multiply(inputVal, kernelVal);\n+                                        result[b, oc, oh, ow] = numOps.Add(result[b, oc, oh, ow], contribution);\n+                                    }\n+                                }\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n+\n+            // Add bias if provided\n+            if (bias != null)\n+            {\n+                for (int oc = 0; oc < outChannels; oc++)\n+                {\n+                    for (int oh = 0; oh < outH; oh++)\n+                    {\n+                        for (int ow = 0; ow < outW; ow++)\n+                        {\n+                            result[b, oc, oh, ow] = numOps.Add(result[b, oc, oh, ow], bias.Value[oc]);\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            // Gradient w.r.t. input (this is a forward Conv2D!)\n+            if (input.RequiresGradient)\n+            {\n+                var gradInput = new Tensor<T>(inputShape);\n+\n+                for (int b = 0; b < batch; b++)\n+                {\n+                    for (int ic = 0; ic < inChannels; ic++)\n+                    {\n+                        for (int ih = 0; ih < inH; ih++)\n+                        {\n+                            for (int iw = 0; iw < inW; iw++)\n+                            {\n+                                var sum = numOps.Zero;\n+\n+                                for (int oc = 0; oc < outChannels; oc++)\n+                                {\n+                                    for (int kh = 0; kh < kernelH; kh++)\n+                                    {\n+                                        for (int kw = 0; kw < kernelW; kw++)\n+                                        {\n+                                            int oh = ih * strideH + kh - padH;\n+                                            int ow = iw * strideW + kw - padW;\n+\n+                                            if (oh >= 0 && oh < outH && ow >= 0 && ow < outW)\n+                                            {\n+                                                var gradVal = gradient[b, oc, oh, ow];\n+                                                var kernelVal = kernel.Value[ic, oc, kh, kw];\n+                                                sum = numOps.Add(sum, numOps.Multiply(gradVal, kernelVal));\n+                                            }\n+                                        }\n+                                    }\n+                                }\n+\n+                                gradInput[b, ic, ih, iw] = sum;\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (input.Gradient == null)\n+                    input.Gradient = gradInput;\n+                else\n+                    input.Gradient = input.Gradient.Add(gradInput);"}]}},{"id":"PRRT_kwDOKSXUF85hck_P","isResolved":false,"isOutdated":false,"path":"src/Autodiff/TensorOperations.cs","line":2940,"originalLine":2940,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzD_w","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement write to the same variable - consider using '?' to express intent better.\n```suggestion\n                kernel.Gradient = (kernel.Gradient == null) ? gradKernel : kernel.Gradient.Add(gradKernel);\n```","createdAt":"2025-11-11T07:53:27Z","path":"src/Autodiff/TensorOperations.cs","line":2940,"originalLine":2940,"diffHunk":"@@ -1219,4 +1219,1769 @@ void BackwardFunction(Tensor<T> gradient)\n \n         return node;\n     }\n+\n+    /// <summary>\n+    /// Computes the softmax function for a computation node along a specified axis.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"axis\">The axis along which to compute softmax. Default is -1 (last axis).</param>\n+    /// <returns>A new computation node containing the softmax result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method computes softmax (σ(x_i) = exp(x_i) / Σexp(x_j)) along the specified axis.\n+    /// Uses numerical stability trick: subtract max before exponentiating.\n+    /// The backward function uses: ∂softmax/∂x = softmax(x) * (grad - Σ(grad * softmax(x))).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Softmax converts a vector of numbers into probabilities.\n+    ///\n+    /// For softmax:\n+    /// - The forward pass exponentiates each element, then normalizes so they sum to 1\n+    /// - The result is a probability distribution (all values between 0 and 1, summing to 1)\n+    /// - The backward pass is complex but efficient: uses the Jacobian of softmax\n+    ///\n+    /// Softmax is crucial for:\n+    /// - Multi-class classification (final layer outputs)\n+    /// - Attention mechanisms (computing attention weights)\n+    /// - Anywhere you need to convert scores to probabilities\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Softmax(ComputationNode<T> a, int axis = -1)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        // Normalize axis to positive index\n+        if (axis < 0)\n+            axis = shape.Length + axis;\n+\n+        // For simplicity, handle 2D case (batch, features) with axis=-1\n+        if (shape.Length == 2 && axis == 1)\n+        {\n+            int batchSize = shape[0];\n+            int features = shape[1];\n+            var result = new Tensor<T>(shape);\n+\n+            // Compute softmax for each row\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                // Find max for numerical stability\n+                var maxVal = a.Value[b, 0];\n+                for (int f = 1; f < features; f++)\n+                {\n+                    if (numOps.GreaterThan(a.Value[b, f], maxVal))\n+                        maxVal = a.Value[b, f];\n+                }\n+\n+                // Compute exp(x - max) and sum\n+                var expSum = numOps.Zero;\n+                var expValues = new T[features];\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var shifted = numOps.Subtract(a.Value[b, f], maxVal);\n+                    expValues[f] = numOps.Exp(shifted);\n+                    expSum = numOps.Add(expSum, expValues[f]);\n+                }\n+\n+                // Normalize\n+                for (int f = 0; f < features; f++)\n+                {\n+                    result[b, f] = numOps.Divide(expValues[f], expSum);\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (a.RequiresGradient)\n+                {\n+                    // ∂softmax/∂x_i = softmax_i * (∂L/∂y_i - Σ_j(∂L/∂y_j * softmax_j))\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        // Compute sum of (gradient * softmax)\n+                        var dotProduct = numOps.Zero;\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            dotProduct = numOps.Add(dotProduct,\n+                                numOps.Multiply(gradient[b, f], result[b, f]));\n+                        }\n+\n+                        // Compute gradient for each element\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            var gradMinusDot = numOps.Subtract(gradient[b, f], dotProduct);\n+                            gradA[b, f] = numOps.Multiply(result[b, f], gradMinusDot);\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient,\n+                parents: new List<ComputationNode<T>> { a },\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Softmax is currently only implemented for 2D tensors along axis=-1. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}], axis={axis}\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Concatenates multiple computation nodes along a specified axis.\n+    /// </summary>\n+    /// <param name=\"nodes\">The list of nodes to concatenate.</param>\n+    /// <param name=\"axis\">The axis along which to concatenate. Default is 0.</param>\n+    /// <returns>A new computation node containing the concatenated result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method concatenates tensors along the specified axis.\n+    /// All tensors must have the same shape except along the concatenation axis.\n+    /// The backward function splits the gradient and sends each portion to the corresponding input.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Concat stacks tensors together along a dimension.\n+    ///\n+    /// For concatenation:\n+    /// - The forward pass combines multiple tensors into one larger tensor\n+    /// - The backward pass splits the gradient back to each input\n+    /// - Think of it like gluing arrays together end-to-end\n+    ///\n+    /// Used in:\n+    /// - Skip connections (concatenating features from different layers)\n+    /// - Multi-input architectures\n+    /// - Feature fusion in neural networks\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Concat(List<ComputationNode<T>> nodes, int axis = 0)\n+    {\n+        if (nodes.Count == 0)\n+            throw new ArgumentException(\"Cannot concatenate empty list of nodes\");\n+\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var firstShape = nodes[0].Value.Shape;\n+\n+        // Normalize axis\n+        if (axis < 0)\n+            axis = firstShape.Length + axis;\n+\n+        // Validate shapes match except on concat axis\n+        for (int i = 1; i < nodes.Count; i++)\n+        {\n+            var shape = nodes[i].Value.Shape;\n+            if (shape.Length != firstShape.Length)\n+                throw new ArgumentException(\"All tensors must have the same rank\");\n+\n+            for (int d = 0; d < firstShape.Length; d++)\n+            {\n+                if (d != axis && shape[d] != firstShape[d])\n+                    throw new ArgumentException(\n+                        $\"Shape mismatch at dimension {d}: {shape[d]} vs {firstShape[d]}\");\n+            }\n+        }\n+\n+        // Compute output shape\n+        int[] outputShape = (int[])firstShape.Clone();\n+        for (int i = 1; i < nodes.Count; i++)\n+        {\n+            outputShape[axis] += nodes[i].Value.Shape[axis];\n+        }\n+\n+        // Perform concatenation (handle 2D case for simplicity)\n+        Tensor<T> result;\n+        if (firstShape.Length == 2 && axis == 1)\n+        {\n+            // Concatenate along columns (features)\n+            int rows = firstShape[0];\n+            int totalCols = outputShape[1];\n+            result = new Tensor<T>(new int[] { rows, totalCols });\n+\n+            int colOffset = 0;\n+            foreach (var node in nodes)\n+            {\n+                int cols = node.Value.Shape[1];\n+                for (int r = 0; r < rows; r++)\n+                {\n+                    for (int c = 0; c < cols; c++)\n+                    {\n+                        result[r, colOffset + c] = node.Value[r, c];\n+                    }\n+                }\n+                colOffset += cols;\n+            }\n+        }\n+        else if (firstShape.Length == 2 && axis == 0)\n+        {\n+            // Concatenate along rows (batch)\n+            int cols = firstShape[1];\n+            int totalRows = outputShape[0];\n+            result = new Tensor<T>(new int[] { totalRows, cols });\n+\n+            int rowOffset = 0;\n+            foreach (var node in nodes)\n+            {\n+                int rows = node.Value.Shape[0];\n+                for (int r = 0; r < rows; r++)\n+                {\n+                    for (int c = 0; c < cols; c++)\n+                    {\n+                        result[rowOffset + r, c] = node.Value[r, c];\n+                    }\n+                }\n+                rowOffset += rows;\n+            }\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Concat is currently only implemented for 2D tensors. \" +\n+                $\"Got shape=[{string.Join(\", \", firstShape)}]\");\n+        }\n+\n+        // Store sizes for gradient splitting\n+        var sizes = nodes.Select(n => n.Value.Shape[axis]).ToList();\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            // Split gradient along concat axis and distribute to inputs\n+            if (firstShape.Length == 2 && axis == 1)\n+            {\n+                int rows = firstShape[0];\n+                int colOffset = 0;\n+                for (int i = 0; i < nodes.Count; i++)\n+                {\n+                    if (!nodes[i].RequiresGradient)\n+                    {\n+                        colOffset += sizes[i];\n+                        continue;\n+                    }\n+\n+                    int cols = sizes[i];\n+                    var gradPart = new Tensor<T>(new int[] { rows, cols });\n+\n+                    for (int r = 0; r < rows; r++)\n+                    {\n+                        for (int c = 0; c < cols; c++)\n+                        {\n+                            gradPart[r, c] = gradient[r, colOffset + c];\n+                        }\n+                    }\n+\n+                    if (nodes[i].Gradient == null)\n+                        nodes[i].Gradient = gradPart;\n+                    else\n+                        nodes[i].Gradient = nodes[i].Gradient.Add(gradPart);\n+\n+                    colOffset += cols;\n+                }\n+            }\n+            else if (firstShape.Length == 2 && axis == 0)\n+            {\n+                int cols = firstShape[1];\n+                int rowOffset = 0;\n+                for (int i = 0; i < nodes.Count; i++)\n+                {\n+                    if (!nodes[i].RequiresGradient)\n+                    {\n+                        rowOffset += sizes[i];\n+                        continue;\n+                    }\n+\n+                    int rows = sizes[i];\n+                    var gradPart = new Tensor<T>(new int[] { rows, cols });\n+\n+                    for (int r = 0; r < rows; r++)\n+                    {\n+                        for (int c = 0; c < cols; c++)\n+                        {\n+                            gradPart[r, c] = gradient[rowOffset + r, c];\n+                        }\n+                    }\n+\n+                    if (nodes[i].Gradient == null)\n+                        nodes[i].Gradient = gradPart;\n+                    else\n+                        nodes[i].Gradient = nodes[i].Gradient.Add(gradPart);\n+\n+                    rowOffset += rows;\n+                }\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: nodes.Any(n => n.RequiresGradient),\n+            parents: nodes,\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Pads a tensor with a constant value along specified dimensions.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"padWidth\">Padding width for each dimension as (before, after) pairs.</param>\n+    /// <param name=\"value\">The value to use for padding. Default is zero.</param>\n+    /// <returns>A new computation node containing the padded result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method adds padding around the tensor.\n+    /// The backward function simply crops the gradient back to the original size (gradients for padding are zero).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Pad adds extra elements around a tensor.\n+    ///\n+    /// For padding:\n+    /// - The forward pass adds border elements with a constant value\n+    /// - The backward pass removes those border gradients (they don't affect the original tensor)\n+    /// - Think of it like adding margins to an image\n+    ///\n+    /// Used in:\n+    /// - Convolutional layers (to maintain spatial dimensions)\n+    /// - Handling variable-length sequences\n+    /// - Data augmentation\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Pad(ComputationNode<T> a, int[,] padWidth, T? value = default)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var padValue = value ?? numOps.Zero;\n+        var shape = a.Value.Shape;\n+\n+        // Validate padWidth dimensions\n+        if (padWidth.GetLength(0) != shape.Length)\n+            throw new ArgumentException(\"padWidth must have same number of dimensions as input tensor\");\n+\n+        // Compute output shape\n+        var outputShape = new int[shape.Length];\n+        for (int d = 0; d < shape.Length; d++)\n+        {\n+            outputShape[d] = shape[d] + padWidth[d, 0] + padWidth[d, 1];\n+        }\n+\n+        // Handle 2D case\n+        if (shape.Length == 2)\n+        {\n+            int inputRows = shape[0];\n+            int inputCols = shape[1];\n+            int padTop = padWidth[0, 0];\n+            int padBottom = padWidth[0, 1];\n+            int padLeft = padWidth[1, 0];\n+            int padRight = padWidth[1, 1];\n+\n+            var result = new Tensor<T>(outputShape);\n+\n+            // Initialize with pad value\n+            for (int i = 0; i < result.Length; i++)\n+            {\n+                result[i] = padValue;\n+            }\n+\n+            // Copy input data to center\n+            for (int r = 0; r < inputRows; r++)\n+            {\n+                for (int c = 0; c < inputCols; c++)\n+                {\n+                    result[padTop + r, padLeft + c] = a.Value[r, c];\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (a.RequiresGradient)\n+                {\n+                    // Extract gradient for original (unpadded) region\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int r = 0; r < inputRows; r++)\n+                    {\n+                        for (int c = 0; c < inputCols; c++)\n+                        {\n+                            gradA[r, c] = gradient[padTop + r, padLeft + c];\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient,\n+                parents: new List<ComputationNode<T>> { a },\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Pad is currently only implemented for 2D tensors. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}]\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs 2D max pooling on a 4D tensor (batch, channels, height, width).\n+    /// </summary>\n+    /// <param name=\"a\">The input node with shape [batch, channels, height, width].</param>\n+    /// <param name=\"poolSize\">The size of the pooling window [poolH, poolW].</param>\n+    /// <param name=\"strides\">The stride for the pooling operation [strideH, strideW]. If null, uses poolSize.</param>\n+    /// <returns>A new computation node containing the max pooled result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method performs max pooling over 2D spatial dimensions.\n+    /// During forward pass, it tracks which element was the max for routing gradients during backward pass.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> MaxPool downsamples by taking the maximum value in each window.\n+    ///\n+    /// For max pooling:\n+    /// - The forward pass slides a window and takes the max value in each position\n+    /// - This reduces spatial dimensions (downsampling)\n+    /// - The backward pass routes gradients only to the positions that were max\n+    /// - Other positions get zero gradient (they didn't contribute to the output)\n+    ///\n+    /// Used in:\n+    /// - CNNs for translation invariance\n+    /// - Reducing spatial resolution\n+    /// - Building hierarchical features\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> MaxPool2D(\n+        ComputationNode<T> a,\n+        int[] poolSize,\n+        int[]? strides = null)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        if (shape.Length != 4)\n+            throw new ArgumentException(\"MaxPool2D requires 4D input [batch, channels, height, width]\");\n+\n+        strides ??= poolSize;\n+\n+        int batch = shape[0];\n+        int channels = shape[1];\n+        int inH = shape[2];\n+        int inW = shape[3];\n+        int poolH = poolSize[0];\n+        int poolW = poolSize[1];\n+        int strideH = strides[0];\n+        int strideW = strides[1];\n+\n+        int outH = (inH - poolH) / strideH + 1;\n+        int outW = (inW - poolW) / strideW + 1;\n+\n+        var result = new Tensor<T>(new int[] { batch, channels, outH, outW });\n+        // Store max positions for backprop\n+        var maxPositions = new int[batch, channels, outH, outW, 2]; // [h_offset, w_offset]\n+\n+        // Forward pass: compute max pooling and track positions\n+        for (int b = 0; b < batch; b++)\n+        {\n+            for (int c = 0; c < channels; c++)\n+            {\n+                for (int oh = 0; oh < outH; oh++)\n+                {\n+                    for (int ow = 0; ow < outW; ow++)\n+                    {\n+                        int hStart = oh * strideH;\n+                        int wStart = ow * strideW;\n+\n+                        var maxVal = a.Value[b * channels * inH * inW +\n+                                           c * inH * inW +\n+                                           hStart * inW +\n+                                           wStart];\n+                        int maxHOffset = 0;\n+                        int maxWOffset = 0;\n+\n+                        // Find max in pooling window\n+                        for (int ph = 0; ph < poolH; ph++)\n+                        {\n+                            for (int pw = 0; pw < poolW; pw++)\n+                            {\n+                                int h = hStart + ph;\n+                                int w = wStart + pw;\n+                                if (h < inH && w < inW)\n+                                {\n+                                    var val = a.Value[b * channels * inH * inW +\n+                                                     c * inH * inW +\n+                                                     h * inW +\n+                                                     w];\n+                                    if (numOps.GreaterThan(val, maxVal))\n+                                    {\n+                                        maxVal = val;\n+                                        maxHOffset = ph;\n+                                        maxWOffset = pw;\n+                                    }\n+                                }\n+                            }\n+                        }\n+\n+                        result[b, c, oh, ow] = maxVal;\n+                        maxPositions[b, c, oh, ow, 0] = maxHOffset;\n+                        maxPositions[b, c, oh, ow, 1] = maxWOffset;\n+                    }\n+                }\n+            }\n+        }\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            if (a.RequiresGradient)\n+            {\n+                var gradA = new Tensor<T>(shape);\n+\n+                // Route gradients to max positions\n+                for (int b = 0; b < batch; b++)\n+                {\n+                    for (int c = 0; c < channels; c++)\n+                    {\n+                        for (int oh = 0; oh < outH; oh++)\n+                        {\n+                            for (int ow = 0; ow < outW; ow++)\n+                            {\n+                                int hStart = oh * strideH;\n+                                int wStart = ow * strideW;\n+                                int maxHOffset = maxPositions[b, c, oh, ow, 0];\n+                                int maxWOffset = maxPositions[b, c, oh, ow, 1];\n+\n+                                int maxH = hStart + maxHOffset;\n+                                int maxW = wStart + maxWOffset;\n+\n+                                int gradIdx = b * channels * inH * inW +\n+                                             c * inH * inW +\n+                                            maxH * inW +\n+                                             maxW;\n+\n+                                gradA[gradIdx] = numOps.Add(gradA[gradIdx], gradient[b, c, oh, ow]);\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (a.Gradient == null)\n+                    a.Gradient = gradA;\n+                else\n+                    a.Gradient = a.Gradient.Add(gradA);\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: a.RequiresGradient,\n+            parents: new List<ComputationNode<T>> { a },\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Performs 2D average pooling on a 4D tensor (batch, channels, height, width).\n+    /// </summary>\n+    /// <param name=\"a\">The input node with shape [batch, channels, height, width].</param>\n+    /// <param name=\"poolSize\">The size of the pooling window [poolH, poolW].</param>\n+    /// <param name=\"strides\">The stride for the pooling operation [strideH, strideW]. If null, uses poolSize.</param>\n+    /// <returns>A new computation node containing the average pooled result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method performs average pooling over 2D spatial dimensions.\n+    /// The backward function distributes gradients equally across the pooling window.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> AvgPool downsamples by taking the average value in each window.\n+    ///\n+    /// For average pooling:\n+    /// - The forward pass slides a window and computes the average\n+    /// - This smoothly reduces spatial dimensions\n+    /// - The backward pass distributes gradients equally to all elements in the window\n+    /// - Each element gets gradient / pool_area\n+    ///\n+    /// Used in:\n+    /// - CNNs for smoother downsampling than max pooling\n+    /// - Global average pooling (replacing fully connected layers)\n+    /// - Reducing overfitting\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> AvgPool2D(\n+        ComputationNode<T> a,\n+        int[] poolSize,\n+        int[]? strides = null)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        if (shape.Length != 4)\n+            throw new ArgumentException(\"AvgPool2D requires 4D input [batch, channels, height, width]\");\n+\n+        strides ??= poolSize;\n+\n+        int batch = shape[0];\n+        int channels = shape[1];\n+        int inH = shape[2];\n+        int inW = shape[3];\n+        int poolH = poolSize[0];\n+        int poolW = poolSize[1];\n+        int strideH = strides[0];\n+        int strideW = strides[1];\n+\n+        int outH = (inH - poolH) / strideH + 1;\n+        int outW = (inW - poolW) / strideW + 1;\n+\n+        var result = new Tensor<T>(new int[] { batch, channels, outH, outW });\n+        var poolArea = numOps.FromDouble(poolH * poolW);\n+\n+        // Forward pass: compute average pooling\n+        for (int b = 0; b < batch; b++)\n+        {\n+            for (int c = 0; c < channels; c++)\n+            {\n+                for (int oh = 0; oh < outH; oh++)\n+                {\n+                    for (int ow = 0; ow < outW; ow++)\n+                    {\n+                        int hStart = oh * strideH;\n+                        int wStart = ow * strideW;\n+\n+                        var sum = numOps.Zero;\n+\n+                        // Sum values in pooling window\n+                        for (int ph = 0; ph < poolH; ph++)\n+                        {\n+                            for (int pw = 0; pw < poolW; pw++)\n+                            {\n+                                int h = hStart + ph;\n+                                int w = wStart + pw;\n+                                if (h < inH && w < inW)\n+                                {\n+                                    sum = numOps.Add(sum, a.Value[b, c, h, w]);\n+                                }\n+                            }\n+                        }\n+\n+                        result[b, c, oh, ow] = numOps.Divide(sum, poolArea);\n+                    }\n+                }\n+            }\n+        }\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            if (a.RequiresGradient)\n+            {\n+                var gradA = new Tensor<T>(shape);\n+\n+                // Distribute gradients equally across pooling windows\n+                for (int b = 0; b < batch; b++)\n+                {\n+                    for (int c = 0; c < channels; c++)\n+                    {\n+                        for (int oh = 0; oh < outH; oh++)\n+                        {\n+                            for (int ow = 0; ow < outW; ow++)\n+                            {\n+                                int hStart = oh * strideH;\n+                                int wStart = ow * strideW;\n+\n+                                var gradValue = numOps.Divide(gradient[b, c, oh, ow], poolArea);\n+\n+                                // Distribute to all elements in window\n+                                for (int ph = 0; ph < poolH; ph++)\n+                                {\n+                                    for (int pw = 0; pw < poolW; pw++)\n+                                    {\n+                                        int h = hStart + ph;\n+                                        int w = wStart + pw;\n+                                        if (h < inH && w < inW)\n+                                        {\n+                                            gradA[b, c, h, w] = numOps.Add(gradA[b, c, h, w], gradValue);\n+                                        }\n+                                    }\n+                                }\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (a.Gradient == null)\n+                    a.Gradient = gradA;\n+                else\n+                    a.Gradient = a.Gradient.Add(gradA);\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: a.RequiresGradient,\n+            parents: new List<ComputationNode<T>> { a },\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Applies layer normalization to a computation node.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"normalizedShape\">The shape over which to normalize (typically the feature dimensions).</param>\n+    /// <param name=\"gamma\">Optional scale parameter (learnable). If null, uses ones.</param>\n+    /// <param name=\"beta\">Optional shift parameter (learnable). If null, uses zeros.</param>\n+    /// <param name=\"epsilon\">Small constant for numerical stability. Default is 1e-5.</param>\n+    /// <returns>A new computation node containing the layer normalized result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Layer normalization normalizes inputs across the feature dimension for each sample independently.\n+    /// Formula: y = gamma * (x - mean) / sqrt(variance + epsilon) + beta\n+    /// Unlike batch normalization, this doesn't depend on batch statistics.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> LayerNorm standardizes features for each sample independently.\n+    ///\n+    /// For layer normalization:\n+    /// - Computes mean and variance for each sample's features\n+    /// - Normalizes: (x - mean) / sqrt(variance)\n+    /// - Scales and shifts: result * gamma + beta\n+    /// - Works the same during training and inference (no batch dependency)\n+    ///\n+    /// Used in:\n+    /// - Transformers (critical component)\n+    /// - RNNs (stabilizes training)\n+    /// - Any architecture needing sample-independent normalization\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> LayerNorm(\n+        ComputationNode<T> a,\n+        int[] normalizedShape,\n+        ComputationNode<T>? gamma = null,\n+        ComputationNode<T>? beta = null,\n+        double epsilon = 1e-5)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+        var eps = numOps.FromDouble(epsilon);\n+\n+        // For 2D input [batch, features], normalize over features\n+        if (shape.Length == 2 && normalizedShape.Length == 1 && normalizedShape[0] == shape[1])\n+        {\n+            int batchSize = shape[0];\n+            int features = shape[1];\n+\n+            // Create default gamma (ones) and beta (zeros) if not provided\n+            if (gamma == null)\n+            {\n+                var gammaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    gammaTensor[i] = numOps.One;\n+                gamma = Variable(gammaTensor, requiresGradient: false);\n+            }\n+\n+            if (beta == null)\n+            {\n+                var betaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    betaTensor[i] = numOps.Zero;\n+                beta = Variable(betaTensor, requiresGradient: false);\n+            }\n+\n+            var result = new Tensor<T>(shape);\n+            var means = new T[batchSize];\n+            var variances = new T[batchSize];\n+            var normalized = new Tensor<T>(shape);\n+\n+            // Forward pass: compute mean and variance per sample\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                // Compute mean\n+                var sum = numOps.Zero;\n+                for (int f = 0; f < features; f++)\n+                {\n+                    sum = numOps.Add(sum, a.Value[b, f]);\n+                }\n+                means[b] = numOps.Divide(sum, numOps.FromDouble(features));\n+\n+                // Compute variance\n+                var varSum = numOps.Zero;\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var diff = numOps.Subtract(a.Value[b, f], means[b]);\n+                    varSum = numOps.Add(varSum, numOps.Multiply(diff, diff));\n+                }\n+                variances[b] = numOps.Divide(varSum, numOps.FromDouble(features));\n+\n+                // Normalize and scale\n+                var std = numOps.Sqrt(numOps.Add(variances[b], eps));\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var norm = numOps.Divide(\n+                        numOps.Subtract(a.Value[b, f], means[b]),\n+                        std);\n+                    normalized[b, f] = norm;\n+                    result[b, f] = numOps.Add(\n+                        numOps.Multiply(norm, gamma.Value[f]),\n+                        beta.Value[f]);\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                // Gradients for gamma and beta\n+                if (gamma.RequiresGradient)\n+                {\n+                    var gradGamma = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum,\n+                                numOps.Multiply(gradient[b, f], normalized[b, f]));\n+                        }\n+                        gradGamma[f] = sum;\n+                    }\n+\n+                    if (gamma.Gradient == null)\n+                        gamma.Gradient = gradGamma;\n+                    else\n+                        gamma.Gradient = gamma.Gradient.Add(gradGamma);\n+                }\n+\n+                if (beta.RequiresGradient)\n+                {\n+                    var gradBeta = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum, gradient[b, f]);\n+                        }\n+                        gradBeta[f] = sum;\n+                    }\n+\n+                    if (beta.Gradient == null)\n+                        beta.Gradient = gradBeta;\n+                    else\n+                        beta.Gradient = beta.Gradient.Add(gradBeta);\n+                }\n+\n+                // Gradient for input\n+                if (a.RequiresGradient)\n+                {\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        var std = numOps.Sqrt(numOps.Add(variances[b], eps));\n+                        var invStd = numOps.Divide(numOps.One, std);\n+\n+                        // Compute gradient components\n+                        var gradNormSum = numOps.Zero;\n+                        var gradNormDotNorm = numOps.Zero;\n+\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            var gradNorm = numOps.Multiply(gradient[b, f], gamma.Value[f]);\n+                            gradNormSum = numOps.Add(gradNormSum, gradNorm);\n+                            gradNormDotNorm = numOps.Add(gradNormDotNorm,\n+                                numOps.Multiply(gradNorm, normalized[b, f]));\n+                        }\n+\n+                        // Apply gradient formula\n+                        var featuresT = numOps.FromDouble(features);\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            var gradNorm = numOps.Multiply(gradient[b, f], gamma.Value[f]);\n+\n+                            var term1 = gradNorm;\n+                            var term2 = numOps.Divide(gradNormSum, featuresT);\n+                            var term3 = numOps.Divide(\n+                                numOps.Multiply(normalized[b, f], gradNormDotNorm),\n+                                featuresT);\n+\n+                            var grad = numOps.Multiply(\n+                                numOps.Subtract(numOps.Subtract(term1, term2), term3),\n+                                invStd);\n+\n+                            gradA[b, f] = grad;\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var parents = new List<ComputationNode<T>> { a };\n+            if (gamma != null) parents.Add(gamma);\n+            if (beta != null) parents.Add(beta);\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient || (gamma?.RequiresGradient ?? false) || (beta?.RequiresGradient ?? false),\n+                parents: parents,\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"LayerNorm is currently only implemented for 2D tensors normalizing over last dimension. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}], normalizedShape=[{string.Join(\", \", normalizedShape)}]\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Applies batch normalization to a computation node.\n+    /// </summary>\n+    /// <param name=\"a\">The input node with shape [batch, features].</param>\n+    /// <param name=\"gamma\">Optional scale parameter (learnable). If null, uses ones.</param>\n+    /// <param name=\"beta\">Optional shift parameter (learnable). If null, uses zeros.</param>\n+    /// <param name=\"runningMean\">Running mean for inference (not updated during this operation).</param>\n+    /// <param name=\"runningVar\">Running variance for inference (not updated during this operation).</param>\n+    /// <param name=\"training\">Whether in training mode (uses batch statistics) or inference mode (uses running statistics).</param>\n+    /// <param name=\"epsilon\">Small constant for numerical stability. Default is 1e-5.</param>\n+    /// <returns>A new computation node containing the batch normalized result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Batch normalization normalizes inputs across the batch dimension.\n+    /// During training: Uses batch statistics (mean and variance computed from current batch).\n+    /// During inference: Uses running statistics (accumulated during training).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> BatchNorm standardizes features across the batch.\n+    ///\n+    /// For batch normalization:\n+    /// - Training mode: Uses current batch's mean and variance\n+    /// - Inference mode: Uses running mean/variance from training\n+    /// - Normalizes: (x - mean) / sqrt(variance)\n+    /// - Scales and shifts: result * gamma + beta\n+    ///\n+    /// Benefits:\n+    /// - Stabilizes training (reduces internal covariate shift)\n+    /// - Allows higher learning rates\n+    /// - Acts as regularization\n+    ///\n+    /// Used in:\n+    /// - CNNs (after convolutional layers)\n+    /// - Deep feedforward networks\n+    /// - GANs and many other architectures\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> BatchNorm(\n+        ComputationNode<T> a,\n+        ComputationNode<T>? gamma = null,\n+        ComputationNode<T>? beta = null,\n+        Tensor<T>? runningMean = null,\n+        Tensor<T>? runningVar = null,\n+        bool training = true,\n+        double epsilon = 1e-5)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+        var eps = numOps.FromDouble(epsilon);\n+\n+        // Handle 2D case [batch, features]\n+        if (shape.Length == 2)\n+        {\n+            int batchSize = shape[0];\n+            int features = shape[1];\n+\n+            // Create default gamma and beta if not provided\n+            if (gamma == null)\n+            {\n+                var gammaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    gammaTensor[i] = numOps.One;\n+                gamma = Variable(gammaTensor, requiresGradient: false);\n+            }\n+\n+            if (beta == null)\n+            {\n+                var betaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    betaTensor[i] = numOps.Zero;\n+                beta = Variable(betaTensor, requiresGradient: false);\n+            }\n+\n+            var result = new Tensor<T>(shape);\n+            T[] batchMean;\n+            T[] batchVar;\n+            var normalized = new Tensor<T>(shape);\n+\n+            if (training)\n+            {\n+                // Compute batch statistics\n+                batchMean = new T[features];\n+                batchVar = new T[features];\n+\n+                // Compute mean per feature\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var sum = numOps.Zero;\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        sum = numOps.Add(sum, a.Value[b, f]);\n+                    }\n+                    batchMean[f] = numOps.Divide(sum, numOps.FromDouble(batchSize));\n+                }\n+\n+                // Compute variance per feature\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var varSum = numOps.Zero;\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        var diff = numOps.Subtract(a.Value[b, f], batchMean[f]);\n+                        varSum = numOps.Add(varSum, numOps.Multiply(diff, diff));\n+                    }\n+                    batchVar[f] = numOps.Divide(varSum, numOps.FromDouble(batchSize));\n+                }\n+            }\n+            else\n+            {\n+                // Use running statistics for inference\n+                if (runningMean == null || runningVar == null)\n+                    throw new ArgumentException(\"Running statistics required for inference mode\");\n+\n+                batchMean = new T[features];\n+                batchVar = new T[features];\n+                for (int f = 0; f < features; f++)\n+                {\n+                    batchMean[f] = runningMean[f];\n+                    batchVar[f] = runningVar[f];\n+                }\n+            }\n+\n+            // Normalize and scale\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var std = numOps.Sqrt(numOps.Add(batchVar[f], eps));\n+                    var norm = numOps.Divide(\n+                        numOps.Subtract(a.Value[b, f], batchMean[f]),\n+                        std);\n+                    normalized[b, f] = norm;\n+                    result[b, f] = numOps.Add(\n+                        numOps.Multiply(norm, gamma.Value[f]),\n+                        beta.Value[f]);\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (!training)\n+                {\n+                    // Inference mode: simpler gradient (no batch statistics gradient)\n+                    if (a.RequiresGradient)\n+                    {\n+                        var gradA = new Tensor<T>(shape);\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            for (int f = 0; f < features; f++)\n+                            {\n+                                var std = numOps.Sqrt(numOps.Add(batchVar[f], eps));\n+                                var invStd = numOps.Divide(numOps.One, std);\n+                                gradA[b, f] = numOps.Multiply(\n+                                    numOps.Multiply(gradient[b, f], gamma.Value[f]),\n+                                    invStd);\n+                            }\n+                        }\n+\n+                        if (a.Gradient == null)\n+                            a.Gradient = gradA;\n+                        else\n+                            a.Gradient = a.Gradient.Add(gradA);\n+                    }\n+                    return;\n+                }\n+\n+                // Training mode: full gradient computation\n+                // Gradients for gamma and beta\n+                if (gamma.RequiresGradient)\n+                {\n+                    var gradGamma = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum,\n+                                numOps.Multiply(gradient[b, f], normalized[b, f]));\n+                        }\n+                        gradGamma[f] = sum;\n+                    }\n+\n+                    if (gamma.Gradient == null)\n+                        gamma.Gradient = gradGamma;\n+                    else\n+                        gamma.Gradient = gamma.Gradient.Add(gradGamma);\n+                }\n+\n+                if (beta.RequiresGradient)\n+                {\n+                    var gradBeta = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum, gradient[b, f]);\n+                        }\n+                        gradBeta[f] = sum;\n+                    }\n+\n+                    if (beta.Gradient == null)\n+                        beta.Gradient = gradBeta;\n+                    else\n+                        beta.Gradient = beta.Gradient.Add(gradBeta);\n+                }\n+\n+                // Gradient for input (complex due to batch statistics)\n+                if (a.RequiresGradient)\n+                {\n+                    var gradA = new Tensor<T>(shape);\n+                    var batchSizeT = numOps.FromDouble(batchSize);\n+\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var std = numOps.Sqrt(numOps.Add(batchVar[f], eps));\n+                        var invStd = numOps.Divide(numOps.One, std);\n+\n+                        // Sum of gradients and gradient*normalized\n+                        var gradSum = numOps.Zero;\n+                        var gradNormSum = numOps.Zero;\n+\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            var grad = numOps.Multiply(gradient[b, f], gamma.Value[f]);\n+                            gradSum = numOps.Add(gradSum, grad);\n+                            gradNormSum = numOps.Add(gradNormSum,\n+                                numOps.Multiply(grad, normalized[b, f]));\n+                        }\n+\n+                        // Apply gradient formula\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            var grad = numOps.Multiply(gradient[b, f], gamma.Value[f]);\n+\n+                            var term1 = grad;\n+                            var term2 = numOps.Divide(gradSum, batchSizeT);\n+                            var term3 = numOps.Divide(\n+                                numOps.Multiply(normalized[b, f], gradNormSum),\n+                                batchSizeT);\n+\n+                            var gradInput = numOps.Multiply(\n+                                numOps.Subtract(numOps.Subtract(term1, term2), term3),\n+                                invStd);\n+\n+                            gradA[b, f] = gradInput;\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var parents = new List<ComputationNode<T>> { a };\n+            if (gamma != null) parents.Add(gamma);\n+            if (beta != null) parents.Add(beta);\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient || (gamma?.RequiresGradient ?? false) || (beta?.RequiresGradient ?? false),\n+                parents: parents,\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"BatchNorm is currently only implemented for 2D tensors [batch, features]. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}]\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs 2D convolution on a 4D tensor (batch, channels, height, width).\n+    /// </summary>\n+    /// <param name=\"input\">The input node with shape [batch, inChannels, height, width].</param>\n+    /// <param name=\"kernel\">The kernel/filter with shape [outChannels, inChannels, kernelH, kernelW].</param>\n+    /// <param name=\"bias\">Optional bias with shape [outChannels]. If null, no bias is added.</param>\n+    /// <param name=\"stride\">The stride [strideH, strideW]. Default is [1, 1].</param>\n+    /// <param name=\"padding\">The padding [padH, padW]. Default is [0, 0].</param>\n+    /// <returns>A new computation node containing the convolution result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method performs 2D convolution, the fundamental operation in CNNs.\n+    /// Forward: Slides the kernel over the input computing dot products.\n+    /// Backward: Computes gradients for both input and kernel using transposed convolutions.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Conv2D is the core operation of convolutional neural networks.\n+    ///\n+    /// For 2D convolution:\n+    /// - The kernel \"slides\" over the input, computing weighted sums\n+    /// - Each output position is a dot product of the kernel with input patch\n+    /// - Stride controls how far the kernel moves each step\n+    /// - Padding adds borders to control output size\n+    ///\n+    /// Gradient computation:\n+    /// - Gradient w.r.t. input: \"full\" convolution with flipped kernel\n+    /// - Gradient w.r.t. kernel: cross-correlation between input and output gradient\n+    ///\n+    /// Used in:\n+    /// - All CNNs (image classification, object detection, segmentation)\n+    /// - Feature extraction in vision models\n+    /// - Learning spatial hierarchies\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Conv2D(\n+        ComputationNode<T> input,\n+        ComputationNode<T> kernel,\n+        ComputationNode<T>? bias = null,\n+        int[]? stride = null,\n+        int[]? padding = null)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var inputShape = input.Value.Shape;\n+        var kernelShape = kernel.Value.Shape;\n+\n+        if (inputShape.Length != 4)\n+            throw new ArgumentException(\"Conv2D requires 4D input [batch, inChannels, height, width]\");\n+        if (kernelShape.Length != 4)\n+            throw new ArgumentException(\"Conv2D requires 4D kernel [outChannels, inChannels, kernelH, kernelW]\");\n+\n+        stride ??= new int[] { 1, 1 };\n+        padding ??= new int[] { 0, 0 };\n+\n+        int batch = inputShape[0];\n+        int inChannels = inputShape[1];\n+        int inH = inputShape[2];\n+        int inW = inputShape[3];\n+\n+        int outChannels = kernelShape[0];\n+        int kernelInChannels = kernelShape[1];\n+        int kernelH = kernelShape[2];\n+        int kernelW = kernelShape[3];\n+\n+        if (inChannels != kernelInChannels)\n+            throw new ArgumentException($\"Input channels ({inChannels}) must match kernel input channels ({kernelInChannels})\");\n+\n+        int strideH = stride[0];\n+        int strideW = stride[1];\n+        int padH = padding[0];\n+        int padW = padding[1];\n+\n+        int outH = (inH + 2 * padH - kernelH) / strideH + 1;\n+        int outW = (inW + 2 * padW - kernelW) / strideW + 1;\n+\n+        var result = new Tensor<T>(new int[] { batch, outChannels, outH, outW });\n+\n+        // Forward pass: convolution\n+        for (int b = 0; b < batch; b++)\n+        {\n+            for (int oc = 0; oc < outChannels; oc++)\n+            {\n+                for (int oh = 0; oh < outH; oh++)\n+                {\n+                    for (int ow = 0; ow < outW; ow++)\n+                    {\n+                        var sum = numOps.Zero;\n+\n+                        // Convolve kernel over input\n+                        for (int ic = 0; ic < inChannels; ic++)\n+                        {\n+                            for (int kh = 0; kh < kernelH; kh++)\n+                            {\n+                                for (int kw = 0; kw < kernelW; kw++)\n+                                {\n+                                    int ih = oh * strideH + kh - padH;\n+                                    int iw = ow * strideW + kw - padW;\n+\n+                                    // Check bounds (padding)\n+                                    if (ih >= 0 && ih < inH && iw >= 0 && iw < inW)\n+                                    {\n+                                        var inputVal = input.Value[b, ic, ih, iw];\n+                                        var kernelVal = kernel.Value[oc, ic, kh, kw];\n+                                        sum = numOps.Add(sum, numOps.Multiply(inputVal, kernelVal));\n+                                    }\n+                                }\n+                            }\n+                        }\n+\n+                        // Add bias if provided\n+                        if (bias != null)\n+                        {\n+                            sum = numOps.Add(sum, bias.Value[oc]);\n+                        }\n+\n+                        result[b, oc, oh, ow] = sum;\n+                    }\n+                }\n+            }\n+        }\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            // Gradient w.r.t. input\n+            if (input.RequiresGradient)\n+            {\n+                var gradInput = new Tensor<T>(inputShape);\n+\n+                // Full convolution with flipped kernel\n+                for (int b = 0; b < batch; b++)\n+                {\n+                    for (int ic = 0; ic < inChannels; ic++)\n+                    {\n+                        for (int ih = 0; ih < inH; ih++)\n+                        {\n+                            for (int iw = 0; iw < inW; iw++)\n+                            {\n+                                var sum = numOps.Zero;\n+\n+                                // Iterate over all output positions that used this input position\n+                                for (int oc = 0; oc < outChannels; oc++)\n+                                {\n+                                    for (int kh = 0; kh < kernelH; kh++)\n+                                    {\n+                                        for (int kw = 0; kw < kernelW; kw++)\n+                                        {\n+                                            // Compute output position\n+                                            int ohShifted = ih + padH - kh;\n+                                            int owShifted = iw + padW - kw;\n+\n+                                            if (ohShifted % strideH == 0 && owShifted % strideW == 0)\n+                                            {\n+                                                int oh = ohShifted / strideH;\n+                                                int ow = owShifted / strideW;\n+\n+                                                if (oh >= 0 && oh < outH && ow >= 0 && ow < outW)\n+                                                {\n+                                                    var gradVal = gradient[b, oc, oh, ow];\n+                                                    var kernelVal = kernel.Value[oc, ic, kh, kw];\n+                                                    sum = numOps.Add(sum, numOps.Multiply(gradVal, kernelVal));\n+                                                }\n+                                            }\n+                                        }\n+                                    }\n+                                }\n+\n+                                gradInput[b, ic, ih, iw] = sum;\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (input.Gradient == null)\n+                    input.Gradient = gradInput;\n+                else\n+                    input.Gradient = input.Gradient.Add(gradInput);\n+            }\n+\n+            // Gradient w.r.t. kernel\n+            if (kernel.RequiresGradient)\n+            {\n+                var gradKernel = new Tensor<T>(kernelShape);\n+\n+                // Cross-correlation between input and output gradient\n+                for (int oc = 0; oc < outChannels; oc++)\n+                {\n+                    for (int ic = 0; ic < inChannels; ic++)\n+                    {\n+                        for (int kh = 0; kh < kernelH; kh++)\n+                        {\n+                            for (int kw = 0; kw < kernelW; kw++)\n+                            {\n+                                var sum = numOps.Zero;\n+\n+                                for (int b = 0; b < batch; b++)\n+                                {\n+                                    for (int oh = 0; oh < outH; oh++)\n+                                    {\n+                                        for (int ow = 0; ow < outW; ow++)\n+                                        {\n+                                            int ih = oh * strideH + kh - padH;\n+                                            int iw = ow * strideW + kw - padW;\n+\n+                                            if (ih >= 0 && ih < inH && iw >= 0 && iw < inW)\n+                                            {\n+                                                var gradVal = gradient[b, oc, oh, ow];\n+                                                var inputVal = input.Value[b, ic, ih, iw];\n+                                                sum = numOps.Add(sum, numOps.Multiply(gradVal, inputVal));\n+                                            }\n+                                        }\n+                                    }\n+                                }\n+\n+                                gradKernel[oc, ic, kh, kw] = sum;\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (kernel.Gradient == null)\n+                    kernel.Gradient = gradKernel;\n+                else\n+                    kernel.Gradient = kernel.Gradient.Add(gradKernel);\n+            }\n+\n+            // Gradient w.r.t. bias\n+            if (bias != null && bias.RequiresGradient)\n+            {\n+                var gradBias = new Tensor<T>(new int[] { outChannels });\n+\n+                for (int oc = 0; oc < outChannels; oc++)\n+                {\n+                    var sum = numOps.Zero;\n+                    for (int b = 0; b < batch; b++)\n+                    {\n+                        for (int oh = 0; oh < outH; oh++)\n+                        {\n+                            for (int ow = 0; ow < outW; ow++)\n+                            {\n+                                sum = numOps.Add(sum, gradient[b, oc, oh, ow]);\n+                            }\n+                        }\n+                    }\n+                    gradBias[oc] = sum;\n+                }\n+\n+                if (bias.Gradient == null)\n+                    bias.Gradient = gradBias;\n+                else\n+                    bias.Gradient = bias.Gradient.Add(gradBias);\n+            }\n+        }\n+\n+        var parents = new List<ComputationNode<T>> { input, kernel };\n+        if (bias != null) parents.Add(bias);\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: input.RequiresGradient || kernel.RequiresGradient || (bias?.RequiresGradient ?? false),\n+            parents: parents,\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Performs 2D transposed convolution (deconvolution) on a 4D tensor.\n+    /// </summary>\n+    /// <param name=\"input\">The input node with shape [batch, inChannels, height, width].</param>\n+    /// <param name=\"kernel\">The kernel with shape [inChannels, outChannels, kernelH, kernelW] (note: reversed from Conv2D).</param>\n+    /// <param name=\"bias\">Optional bias with shape [outChannels]. If null, no bias is added.</param>\n+    /// <param name=\"stride\">The stride [strideH, strideW]. Default is [1, 1].</param>\n+    /// <param name=\"padding\">The padding [padH, padW]. Default is [0, 0].</param>\n+    /// <param name=\"outputPadding\">Output padding [outPadH, outPadW] for size adjustment. Default is [0, 0].</param>\n+    /// <returns>A new computation node containing the transposed convolution result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Transposed convolution (often called deconvolution) upsamples the input.\n+    /// It's the gradient of Conv2D with respect to its input, used as a forward operation.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> ConvTranspose2D upsamples spatial dimensions.\n+    ///\n+    /// For transposed convolution:\n+    /// - Inserts zeros between input elements according to stride\n+    /// - Applies regular convolution to the expanded input\n+    /// - Results in larger spatial dimensions (upsampling)\n+    ///\n+    /// Used in:\n+    /// - Image generation (GANs, VAEs)\n+    /// - Semantic segmentation (U-Net decoder)\n+    /// - Super-resolution\n+    /// - Any task requiring upsampling\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> ConvTranspose2D(\n+        ComputationNode<T> input,\n+        ComputationNode<T> kernel,\n+        ComputationNode<T>? bias = null,\n+        int[]? stride = null,\n+        int[]? padding = null,\n+        int[]? outputPadding = null)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var inputShape = input.Value.Shape;\n+        var kernelShape = kernel.Value.Shape;\n+\n+        if (inputShape.Length != 4)\n+            throw new ArgumentException(\"ConvTranspose2D requires 4D input [batch, inChannels, height, width]\");\n+        if (kernelShape.Length != 4)\n+            throw new ArgumentException(\"ConvTranspose2D requires 4D kernel [inChannels, outChannels, kernelH, kernelW]\");\n+\n+        stride ??= new int[] { 1, 1 };\n+        padding ??= new int[] { 0, 0 };\n+        outputPadding ??= new int[] { 0, 0 };\n+\n+        int batch = inputShape[0];\n+        int inChannels = inputShape[1];\n+        int inH = inputShape[2];\n+        int inW = inputShape[3];\n+\n+        int kernelInChannels = kernelShape[0];\n+        int outChannels = kernelShape[1];\n+        int kernelH = kernelShape[2];\n+        int kernelW = kernelShape[3];\n+\n+        if (inChannels != kernelInChannels)\n+            throw new ArgumentException($\"Input channels ({inChannels}) must match kernel input channels ({kernelInChannels})\");\n+\n+        int strideH = stride[0];\n+        int strideW = stride[1];\n+        int padH = padding[0];\n+        int padW = padding[1];\n+        int outPadH = outputPadding[0];\n+        int outPadW = outputPadding[1];\n+\n+        int outH = (inH - 1) * strideH - 2 * padH + kernelH + outPadH;\n+        int outW = (inW - 1) * strideW - 2 * padW + kernelW + outPadW;\n+\n+        var result = new Tensor<T>(new int[] { batch, outChannels, outH, outW });\n+\n+        // Forward pass: transposed convolution\n+        for (int b = 0; b < batch; b++)\n+        {\n+            for (int ic = 0; ic < inChannels; ic++)\n+            {\n+                for (int ih = 0; ih < inH; ih++)\n+                {\n+                    for (int iw = 0; iw < inW; iw++)\n+                    {\n+                        var inputVal = input.Value[b, ic, ih, iw];\n+\n+                        // Distribute this input value to output using kernel\n+                        for (int oc = 0; oc < outChannels; oc++)\n+                        {\n+                            for (int kh = 0; kh < kernelH; kh++)\n+                            {\n+                                for (int kw = 0; kw < kernelW; kw++)\n+                                {\n+                                    int oh = ih * strideH + kh - padH;\n+                                    int ow = iw * strideW + kw - padW;\n+\n+                                    if (oh >= 0 && oh < outH && ow >= 0 && ow < outW)\n+                                    {\n+                                        var kernelVal = kernel.Value[ic, oc, kh, kw];\n+                                        var contribution = numOps.Multiply(inputVal, kernelVal);\n+                                        result[b, oc, oh, ow] = numOps.Add(result[b, oc, oh, ow], contribution);\n+                                    }\n+                                }\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n+\n+            // Add bias if provided\n+            if (bias != null)\n+            {\n+                for (int oc = 0; oc < outChannels; oc++)\n+                {\n+                    for (int oh = 0; oh < outH; oh++)\n+                    {\n+                        for (int ow = 0; ow < outW; ow++)\n+                        {\n+                            result[b, oc, oh, ow] = numOps.Add(result[b, oc, oh, ow], bias.Value[oc]);\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            // Gradient w.r.t. input (this is a forward Conv2D!)\n+            if (input.RequiresGradient)\n+            {\n+                var gradInput = new Tensor<T>(inputShape);\n+\n+                for (int b = 0; b < batch; b++)\n+                {\n+                    for (int ic = 0; ic < inChannels; ic++)\n+                    {\n+                        for (int ih = 0; ih < inH; ih++)\n+                        {\n+                            for (int iw = 0; iw < inW; iw++)\n+                            {\n+                                var sum = numOps.Zero;\n+\n+                                for (int oc = 0; oc < outChannels; oc++)\n+                                {\n+                                    for (int kh = 0; kh < kernelH; kh++)\n+                                    {\n+                                        for (int kw = 0; kw < kernelW; kw++)\n+                                        {\n+                                            int oh = ih * strideH + kh - padH;\n+                                            int ow = iw * strideW + kw - padW;\n+\n+                                            if (oh >= 0 && oh < outH && ow >= 0 && ow < outW)\n+                                            {\n+                                                var gradVal = gradient[b, oc, oh, ow];\n+                                                var kernelVal = kernel.Value[ic, oc, kh, kw];\n+                                                sum = numOps.Add(sum, numOps.Multiply(gradVal, kernelVal));\n+                                            }\n+                                        }\n+                                    }\n+                                }\n+\n+                                gradInput[b, ic, ih, iw] = sum;\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (input.Gradient == null)\n+                    input.Gradient = gradInput;\n+                else\n+                    input.Gradient = input.Gradient.Add(gradInput);\n+            }\n+\n+            // Gradient w.r.t. kernel\n+            if (kernel.RequiresGradient)\n+            {\n+                var gradKernel = new Tensor<T>(kernelShape);\n+\n+                for (int ic = 0; ic < inChannels; ic++)\n+                {\n+                    for (int oc = 0; oc < outChannels; oc++)\n+                    {\n+                        for (int kh = 0; kh < kernelH; kh++)\n+                        {\n+                            for (int kw = 0; kw < kernelW; kw++)\n+                            {\n+                                var sum = numOps.Zero;\n+\n+                                for (int b = 0; b < batch; b++)\n+                                {\n+                                    for (int ih = 0; ih < inH; ih++)\n+                                    {\n+                                        for (int iw = 0; iw < inW; iw++)\n+                                        {\n+                                            int oh = ih * strideH + kh - padH;\n+                                            int ow = iw * strideW + kw - padW;\n+\n+                                            if (oh >= 0 && oh < outH && ow >= 0 && ow < outW)\n+                                            {\n+                                                var inputVal = input.Value[b, ic, ih, iw];\n+                                                var gradVal = gradient[b, oc, oh, ow];\n+                                                sum = numOps.Add(sum, numOps.Multiply(inputVal, gradVal));\n+                                            }\n+                                        }\n+                                    }\n+                                }\n+\n+                                gradKernel[ic, oc, kh, kw] = sum;\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (kernel.Gradient == null)\n+                    kernel.Gradient = gradKernel;\n+                else\n+                    kernel.Gradient = kernel.Gradient.Add(gradKernel);"}]}},{"id":"PRRT_kwDOKSXUF85hck_a","isResolved":false,"isOutdated":false,"path":"src/Autodiff/TensorOperations.cs","line":2967,"originalLine":2967,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzD_8","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement write to the same variable - consider using '?' to express intent better.\n```suggestion\n                bias.Gradient = bias.Gradient == null ? gradBias : bias.Gradient.Add(gradBias);\n```","createdAt":"2025-11-11T07:53:28Z","path":"src/Autodiff/TensorOperations.cs","line":2967,"originalLine":2967,"diffHunk":"@@ -1219,4 +1219,1769 @@ void BackwardFunction(Tensor<T> gradient)\n \n         return node;\n     }\n+\n+    /// <summary>\n+    /// Computes the softmax function for a computation node along a specified axis.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"axis\">The axis along which to compute softmax. Default is -1 (last axis).</param>\n+    /// <returns>A new computation node containing the softmax result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method computes softmax (σ(x_i) = exp(x_i) / Σexp(x_j)) along the specified axis.\n+    /// Uses numerical stability trick: subtract max before exponentiating.\n+    /// The backward function uses: ∂softmax/∂x = softmax(x) * (grad - Σ(grad * softmax(x))).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Softmax converts a vector of numbers into probabilities.\n+    ///\n+    /// For softmax:\n+    /// - The forward pass exponentiates each element, then normalizes so they sum to 1\n+    /// - The result is a probability distribution (all values between 0 and 1, summing to 1)\n+    /// - The backward pass is complex but efficient: uses the Jacobian of softmax\n+    ///\n+    /// Softmax is crucial for:\n+    /// - Multi-class classification (final layer outputs)\n+    /// - Attention mechanisms (computing attention weights)\n+    /// - Anywhere you need to convert scores to probabilities\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Softmax(ComputationNode<T> a, int axis = -1)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        // Normalize axis to positive index\n+        if (axis < 0)\n+            axis = shape.Length + axis;\n+\n+        // For simplicity, handle 2D case (batch, features) with axis=-1\n+        if (shape.Length == 2 && axis == 1)\n+        {\n+            int batchSize = shape[0];\n+            int features = shape[1];\n+            var result = new Tensor<T>(shape);\n+\n+            // Compute softmax for each row\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                // Find max for numerical stability\n+                var maxVal = a.Value[b, 0];\n+                for (int f = 1; f < features; f++)\n+                {\n+                    if (numOps.GreaterThan(a.Value[b, f], maxVal))\n+                        maxVal = a.Value[b, f];\n+                }\n+\n+                // Compute exp(x - max) and sum\n+                var expSum = numOps.Zero;\n+                var expValues = new T[features];\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var shifted = numOps.Subtract(a.Value[b, f], maxVal);\n+                    expValues[f] = numOps.Exp(shifted);\n+                    expSum = numOps.Add(expSum, expValues[f]);\n+                }\n+\n+                // Normalize\n+                for (int f = 0; f < features; f++)\n+                {\n+                    result[b, f] = numOps.Divide(expValues[f], expSum);\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (a.RequiresGradient)\n+                {\n+                    // ∂softmax/∂x_i = softmax_i * (∂L/∂y_i - Σ_j(∂L/∂y_j * softmax_j))\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        // Compute sum of (gradient * softmax)\n+                        var dotProduct = numOps.Zero;\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            dotProduct = numOps.Add(dotProduct,\n+                                numOps.Multiply(gradient[b, f], result[b, f]));\n+                        }\n+\n+                        // Compute gradient for each element\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            var gradMinusDot = numOps.Subtract(gradient[b, f], dotProduct);\n+                            gradA[b, f] = numOps.Multiply(result[b, f], gradMinusDot);\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient,\n+                parents: new List<ComputationNode<T>> { a },\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Softmax is currently only implemented for 2D tensors along axis=-1. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}], axis={axis}\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Concatenates multiple computation nodes along a specified axis.\n+    /// </summary>\n+    /// <param name=\"nodes\">The list of nodes to concatenate.</param>\n+    /// <param name=\"axis\">The axis along which to concatenate. Default is 0.</param>\n+    /// <returns>A new computation node containing the concatenated result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method concatenates tensors along the specified axis.\n+    /// All tensors must have the same shape except along the concatenation axis.\n+    /// The backward function splits the gradient and sends each portion to the corresponding input.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Concat stacks tensors together along a dimension.\n+    ///\n+    /// For concatenation:\n+    /// - The forward pass combines multiple tensors into one larger tensor\n+    /// - The backward pass splits the gradient back to each input\n+    /// - Think of it like gluing arrays together end-to-end\n+    ///\n+    /// Used in:\n+    /// - Skip connections (concatenating features from different layers)\n+    /// - Multi-input architectures\n+    /// - Feature fusion in neural networks\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Concat(List<ComputationNode<T>> nodes, int axis = 0)\n+    {\n+        if (nodes.Count == 0)\n+            throw new ArgumentException(\"Cannot concatenate empty list of nodes\");\n+\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var firstShape = nodes[0].Value.Shape;\n+\n+        // Normalize axis\n+        if (axis < 0)\n+            axis = firstShape.Length + axis;\n+\n+        // Validate shapes match except on concat axis\n+        for (int i = 1; i < nodes.Count; i++)\n+        {\n+            var shape = nodes[i].Value.Shape;\n+            if (shape.Length != firstShape.Length)\n+                throw new ArgumentException(\"All tensors must have the same rank\");\n+\n+            for (int d = 0; d < firstShape.Length; d++)\n+            {\n+                if (d != axis && shape[d] != firstShape[d])\n+                    throw new ArgumentException(\n+                        $\"Shape mismatch at dimension {d}: {shape[d]} vs {firstShape[d]}\");\n+            }\n+        }\n+\n+        // Compute output shape\n+        int[] outputShape = (int[])firstShape.Clone();\n+        for (int i = 1; i < nodes.Count; i++)\n+        {\n+            outputShape[axis] += nodes[i].Value.Shape[axis];\n+        }\n+\n+        // Perform concatenation (handle 2D case for simplicity)\n+        Tensor<T> result;\n+        if (firstShape.Length == 2 && axis == 1)\n+        {\n+            // Concatenate along columns (features)\n+            int rows = firstShape[0];\n+            int totalCols = outputShape[1];\n+            result = new Tensor<T>(new int[] { rows, totalCols });\n+\n+            int colOffset = 0;\n+            foreach (var node in nodes)\n+            {\n+                int cols = node.Value.Shape[1];\n+                for (int r = 0; r < rows; r++)\n+                {\n+                    for (int c = 0; c < cols; c++)\n+                    {\n+                        result[r, colOffset + c] = node.Value[r, c];\n+                    }\n+                }\n+                colOffset += cols;\n+            }\n+        }\n+        else if (firstShape.Length == 2 && axis == 0)\n+        {\n+            // Concatenate along rows (batch)\n+            int cols = firstShape[1];\n+            int totalRows = outputShape[0];\n+            result = new Tensor<T>(new int[] { totalRows, cols });\n+\n+            int rowOffset = 0;\n+            foreach (var node in nodes)\n+            {\n+                int rows = node.Value.Shape[0];\n+                for (int r = 0; r < rows; r++)\n+                {\n+                    for (int c = 0; c < cols; c++)\n+                    {\n+                        result[rowOffset + r, c] = node.Value[r, c];\n+                    }\n+                }\n+                rowOffset += rows;\n+            }\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Concat is currently only implemented for 2D tensors. \" +\n+                $\"Got shape=[{string.Join(\", \", firstShape)}]\");\n+        }\n+\n+        // Store sizes for gradient splitting\n+        var sizes = nodes.Select(n => n.Value.Shape[axis]).ToList();\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            // Split gradient along concat axis and distribute to inputs\n+            if (firstShape.Length == 2 && axis == 1)\n+            {\n+                int rows = firstShape[0];\n+                int colOffset = 0;\n+                for (int i = 0; i < nodes.Count; i++)\n+                {\n+                    if (!nodes[i].RequiresGradient)\n+                    {\n+                        colOffset += sizes[i];\n+                        continue;\n+                    }\n+\n+                    int cols = sizes[i];\n+                    var gradPart = new Tensor<T>(new int[] { rows, cols });\n+\n+                    for (int r = 0; r < rows; r++)\n+                    {\n+                        for (int c = 0; c < cols; c++)\n+                        {\n+                            gradPart[r, c] = gradient[r, colOffset + c];\n+                        }\n+                    }\n+\n+                    if (nodes[i].Gradient == null)\n+                        nodes[i].Gradient = gradPart;\n+                    else\n+                        nodes[i].Gradient = nodes[i].Gradient.Add(gradPart);\n+\n+                    colOffset += cols;\n+                }\n+            }\n+            else if (firstShape.Length == 2 && axis == 0)\n+            {\n+                int cols = firstShape[1];\n+                int rowOffset = 0;\n+                for (int i = 0; i < nodes.Count; i++)\n+                {\n+                    if (!nodes[i].RequiresGradient)\n+                    {\n+                        rowOffset += sizes[i];\n+                        continue;\n+                    }\n+\n+                    int rows = sizes[i];\n+                    var gradPart = new Tensor<T>(new int[] { rows, cols });\n+\n+                    for (int r = 0; r < rows; r++)\n+                    {\n+                        for (int c = 0; c < cols; c++)\n+                        {\n+                            gradPart[r, c] = gradient[rowOffset + r, c];\n+                        }\n+                    }\n+\n+                    if (nodes[i].Gradient == null)\n+                        nodes[i].Gradient = gradPart;\n+                    else\n+                        nodes[i].Gradient = nodes[i].Gradient.Add(gradPart);\n+\n+                    rowOffset += rows;\n+                }\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: nodes.Any(n => n.RequiresGradient),\n+            parents: nodes,\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Pads a tensor with a constant value along specified dimensions.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"padWidth\">Padding width for each dimension as (before, after) pairs.</param>\n+    /// <param name=\"value\">The value to use for padding. Default is zero.</param>\n+    /// <returns>A new computation node containing the padded result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method adds padding around the tensor.\n+    /// The backward function simply crops the gradient back to the original size (gradients for padding are zero).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Pad adds extra elements around a tensor.\n+    ///\n+    /// For padding:\n+    /// - The forward pass adds border elements with a constant value\n+    /// - The backward pass removes those border gradients (they don't affect the original tensor)\n+    /// - Think of it like adding margins to an image\n+    ///\n+    /// Used in:\n+    /// - Convolutional layers (to maintain spatial dimensions)\n+    /// - Handling variable-length sequences\n+    /// - Data augmentation\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Pad(ComputationNode<T> a, int[,] padWidth, T? value = default)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var padValue = value ?? numOps.Zero;\n+        var shape = a.Value.Shape;\n+\n+        // Validate padWidth dimensions\n+        if (padWidth.GetLength(0) != shape.Length)\n+            throw new ArgumentException(\"padWidth must have same number of dimensions as input tensor\");\n+\n+        // Compute output shape\n+        var outputShape = new int[shape.Length];\n+        for (int d = 0; d < shape.Length; d++)\n+        {\n+            outputShape[d] = shape[d] + padWidth[d, 0] + padWidth[d, 1];\n+        }\n+\n+        // Handle 2D case\n+        if (shape.Length == 2)\n+        {\n+            int inputRows = shape[0];\n+            int inputCols = shape[1];\n+            int padTop = padWidth[0, 0];\n+            int padBottom = padWidth[0, 1];\n+            int padLeft = padWidth[1, 0];\n+            int padRight = padWidth[1, 1];\n+\n+            var result = new Tensor<T>(outputShape);\n+\n+            // Initialize with pad value\n+            for (int i = 0; i < result.Length; i++)\n+            {\n+                result[i] = padValue;\n+            }\n+\n+            // Copy input data to center\n+            for (int r = 0; r < inputRows; r++)\n+            {\n+                for (int c = 0; c < inputCols; c++)\n+                {\n+                    result[padTop + r, padLeft + c] = a.Value[r, c];\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (a.RequiresGradient)\n+                {\n+                    // Extract gradient for original (unpadded) region\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int r = 0; r < inputRows; r++)\n+                    {\n+                        for (int c = 0; c < inputCols; c++)\n+                        {\n+                            gradA[r, c] = gradient[padTop + r, padLeft + c];\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient,\n+                parents: new List<ComputationNode<T>> { a },\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Pad is currently only implemented for 2D tensors. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}]\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs 2D max pooling on a 4D tensor (batch, channels, height, width).\n+    /// </summary>\n+    /// <param name=\"a\">The input node with shape [batch, channels, height, width].</param>\n+    /// <param name=\"poolSize\">The size of the pooling window [poolH, poolW].</param>\n+    /// <param name=\"strides\">The stride for the pooling operation [strideH, strideW]. If null, uses poolSize.</param>\n+    /// <returns>A new computation node containing the max pooled result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method performs max pooling over 2D spatial dimensions.\n+    /// During forward pass, it tracks which element was the max for routing gradients during backward pass.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> MaxPool downsamples by taking the maximum value in each window.\n+    ///\n+    /// For max pooling:\n+    /// - The forward pass slides a window and takes the max value in each position\n+    /// - This reduces spatial dimensions (downsampling)\n+    /// - The backward pass routes gradients only to the positions that were max\n+    /// - Other positions get zero gradient (they didn't contribute to the output)\n+    ///\n+    /// Used in:\n+    /// - CNNs for translation invariance\n+    /// - Reducing spatial resolution\n+    /// - Building hierarchical features\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> MaxPool2D(\n+        ComputationNode<T> a,\n+        int[] poolSize,\n+        int[]? strides = null)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        if (shape.Length != 4)\n+            throw new ArgumentException(\"MaxPool2D requires 4D input [batch, channels, height, width]\");\n+\n+        strides ??= poolSize;\n+\n+        int batch = shape[0];\n+        int channels = shape[1];\n+        int inH = shape[2];\n+        int inW = shape[3];\n+        int poolH = poolSize[0];\n+        int poolW = poolSize[1];\n+        int strideH = strides[0];\n+        int strideW = strides[1];\n+\n+        int outH = (inH - poolH) / strideH + 1;\n+        int outW = (inW - poolW) / strideW + 1;\n+\n+        var result = new Tensor<T>(new int[] { batch, channels, outH, outW });\n+        // Store max positions for backprop\n+        var maxPositions = new int[batch, channels, outH, outW, 2]; // [h_offset, w_offset]\n+\n+        // Forward pass: compute max pooling and track positions\n+        for (int b = 0; b < batch; b++)\n+        {\n+            for (int c = 0; c < channels; c++)\n+            {\n+                for (int oh = 0; oh < outH; oh++)\n+                {\n+                    for (int ow = 0; ow < outW; ow++)\n+                    {\n+                        int hStart = oh * strideH;\n+                        int wStart = ow * strideW;\n+\n+                        var maxVal = a.Value[b * channels * inH * inW +\n+                                           c * inH * inW +\n+                                           hStart * inW +\n+                                           wStart];\n+                        int maxHOffset = 0;\n+                        int maxWOffset = 0;\n+\n+                        // Find max in pooling window\n+                        for (int ph = 0; ph < poolH; ph++)\n+                        {\n+                            for (int pw = 0; pw < poolW; pw++)\n+                            {\n+                                int h = hStart + ph;\n+                                int w = wStart + pw;\n+                                if (h < inH && w < inW)\n+                                {\n+                                    var val = a.Value[b * channels * inH * inW +\n+                                                     c * inH * inW +\n+                                                     h * inW +\n+                                                     w];\n+                                    if (numOps.GreaterThan(val, maxVal))\n+                                    {\n+                                        maxVal = val;\n+                                        maxHOffset = ph;\n+                                        maxWOffset = pw;\n+                                    }\n+                                }\n+                            }\n+                        }\n+\n+                        result[b, c, oh, ow] = maxVal;\n+                        maxPositions[b, c, oh, ow, 0] = maxHOffset;\n+                        maxPositions[b, c, oh, ow, 1] = maxWOffset;\n+                    }\n+                }\n+            }\n+        }\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            if (a.RequiresGradient)\n+            {\n+                var gradA = new Tensor<T>(shape);\n+\n+                // Route gradients to max positions\n+                for (int b = 0; b < batch; b++)\n+                {\n+                    for (int c = 0; c < channels; c++)\n+                    {\n+                        for (int oh = 0; oh < outH; oh++)\n+                        {\n+                            for (int ow = 0; ow < outW; ow++)\n+                            {\n+                                int hStart = oh * strideH;\n+                                int wStart = ow * strideW;\n+                                int maxHOffset = maxPositions[b, c, oh, ow, 0];\n+                                int maxWOffset = maxPositions[b, c, oh, ow, 1];\n+\n+                                int maxH = hStart + maxHOffset;\n+                                int maxW = wStart + maxWOffset;\n+\n+                                int gradIdx = b * channels * inH * inW +\n+                                             c * inH * inW +\n+                                            maxH * inW +\n+                                             maxW;\n+\n+                                gradA[gradIdx] = numOps.Add(gradA[gradIdx], gradient[b, c, oh, ow]);\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (a.Gradient == null)\n+                    a.Gradient = gradA;\n+                else\n+                    a.Gradient = a.Gradient.Add(gradA);\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: a.RequiresGradient,\n+            parents: new List<ComputationNode<T>> { a },\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Performs 2D average pooling on a 4D tensor (batch, channels, height, width).\n+    /// </summary>\n+    /// <param name=\"a\">The input node with shape [batch, channels, height, width].</param>\n+    /// <param name=\"poolSize\">The size of the pooling window [poolH, poolW].</param>\n+    /// <param name=\"strides\">The stride for the pooling operation [strideH, strideW]. If null, uses poolSize.</param>\n+    /// <returns>A new computation node containing the average pooled result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method performs average pooling over 2D spatial dimensions.\n+    /// The backward function distributes gradients equally across the pooling window.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> AvgPool downsamples by taking the average value in each window.\n+    ///\n+    /// For average pooling:\n+    /// - The forward pass slides a window and computes the average\n+    /// - This smoothly reduces spatial dimensions\n+    /// - The backward pass distributes gradients equally to all elements in the window\n+    /// - Each element gets gradient / pool_area\n+    ///\n+    /// Used in:\n+    /// - CNNs for smoother downsampling than max pooling\n+    /// - Global average pooling (replacing fully connected layers)\n+    /// - Reducing overfitting\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> AvgPool2D(\n+        ComputationNode<T> a,\n+        int[] poolSize,\n+        int[]? strides = null)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        if (shape.Length != 4)\n+            throw new ArgumentException(\"AvgPool2D requires 4D input [batch, channels, height, width]\");\n+\n+        strides ??= poolSize;\n+\n+        int batch = shape[0];\n+        int channels = shape[1];\n+        int inH = shape[2];\n+        int inW = shape[3];\n+        int poolH = poolSize[0];\n+        int poolW = poolSize[1];\n+        int strideH = strides[0];\n+        int strideW = strides[1];\n+\n+        int outH = (inH - poolH) / strideH + 1;\n+        int outW = (inW - poolW) / strideW + 1;\n+\n+        var result = new Tensor<T>(new int[] { batch, channels, outH, outW });\n+        var poolArea = numOps.FromDouble(poolH * poolW);\n+\n+        // Forward pass: compute average pooling\n+        for (int b = 0; b < batch; b++)\n+        {\n+            for (int c = 0; c < channels; c++)\n+            {\n+                for (int oh = 0; oh < outH; oh++)\n+                {\n+                    for (int ow = 0; ow < outW; ow++)\n+                    {\n+                        int hStart = oh * strideH;\n+                        int wStart = ow * strideW;\n+\n+                        var sum = numOps.Zero;\n+\n+                        // Sum values in pooling window\n+                        for (int ph = 0; ph < poolH; ph++)\n+                        {\n+                            for (int pw = 0; pw < poolW; pw++)\n+                            {\n+                                int h = hStart + ph;\n+                                int w = wStart + pw;\n+                                if (h < inH && w < inW)\n+                                {\n+                                    sum = numOps.Add(sum, a.Value[b, c, h, w]);\n+                                }\n+                            }\n+                        }\n+\n+                        result[b, c, oh, ow] = numOps.Divide(sum, poolArea);\n+                    }\n+                }\n+            }\n+        }\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            if (a.RequiresGradient)\n+            {\n+                var gradA = new Tensor<T>(shape);\n+\n+                // Distribute gradients equally across pooling windows\n+                for (int b = 0; b < batch; b++)\n+                {\n+                    for (int c = 0; c < channels; c++)\n+                    {\n+                        for (int oh = 0; oh < outH; oh++)\n+                        {\n+                            for (int ow = 0; ow < outW; ow++)\n+                            {\n+                                int hStart = oh * strideH;\n+                                int wStart = ow * strideW;\n+\n+                                var gradValue = numOps.Divide(gradient[b, c, oh, ow], poolArea);\n+\n+                                // Distribute to all elements in window\n+                                for (int ph = 0; ph < poolH; ph++)\n+                                {\n+                                    for (int pw = 0; pw < poolW; pw++)\n+                                    {\n+                                        int h = hStart + ph;\n+                                        int w = wStart + pw;\n+                                        if (h < inH && w < inW)\n+                                        {\n+                                            gradA[b, c, h, w] = numOps.Add(gradA[b, c, h, w], gradValue);\n+                                        }\n+                                    }\n+                                }\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (a.Gradient == null)\n+                    a.Gradient = gradA;\n+                else\n+                    a.Gradient = a.Gradient.Add(gradA);\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: a.RequiresGradient,\n+            parents: new List<ComputationNode<T>> { a },\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Applies layer normalization to a computation node.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"normalizedShape\">The shape over which to normalize (typically the feature dimensions).</param>\n+    /// <param name=\"gamma\">Optional scale parameter (learnable). If null, uses ones.</param>\n+    /// <param name=\"beta\">Optional shift parameter (learnable). If null, uses zeros.</param>\n+    /// <param name=\"epsilon\">Small constant for numerical stability. Default is 1e-5.</param>\n+    /// <returns>A new computation node containing the layer normalized result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Layer normalization normalizes inputs across the feature dimension for each sample independently.\n+    /// Formula: y = gamma * (x - mean) / sqrt(variance + epsilon) + beta\n+    /// Unlike batch normalization, this doesn't depend on batch statistics.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> LayerNorm standardizes features for each sample independently.\n+    ///\n+    /// For layer normalization:\n+    /// - Computes mean and variance for each sample's features\n+    /// - Normalizes: (x - mean) / sqrt(variance)\n+    /// - Scales and shifts: result * gamma + beta\n+    /// - Works the same during training and inference (no batch dependency)\n+    ///\n+    /// Used in:\n+    /// - Transformers (critical component)\n+    /// - RNNs (stabilizes training)\n+    /// - Any architecture needing sample-independent normalization\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> LayerNorm(\n+        ComputationNode<T> a,\n+        int[] normalizedShape,\n+        ComputationNode<T>? gamma = null,\n+        ComputationNode<T>? beta = null,\n+        double epsilon = 1e-5)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+        var eps = numOps.FromDouble(epsilon);\n+\n+        // For 2D input [batch, features], normalize over features\n+        if (shape.Length == 2 && normalizedShape.Length == 1 && normalizedShape[0] == shape[1])\n+        {\n+            int batchSize = shape[0];\n+            int features = shape[1];\n+\n+            // Create default gamma (ones) and beta (zeros) if not provided\n+            if (gamma == null)\n+            {\n+                var gammaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    gammaTensor[i] = numOps.One;\n+                gamma = Variable(gammaTensor, requiresGradient: false);\n+            }\n+\n+            if (beta == null)\n+            {\n+                var betaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    betaTensor[i] = numOps.Zero;\n+                beta = Variable(betaTensor, requiresGradient: false);\n+            }\n+\n+            var result = new Tensor<T>(shape);\n+            var means = new T[batchSize];\n+            var variances = new T[batchSize];\n+            var normalized = new Tensor<T>(shape);\n+\n+            // Forward pass: compute mean and variance per sample\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                // Compute mean\n+                var sum = numOps.Zero;\n+                for (int f = 0; f < features; f++)\n+                {\n+                    sum = numOps.Add(sum, a.Value[b, f]);\n+                }\n+                means[b] = numOps.Divide(sum, numOps.FromDouble(features));\n+\n+                // Compute variance\n+                var varSum = numOps.Zero;\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var diff = numOps.Subtract(a.Value[b, f], means[b]);\n+                    varSum = numOps.Add(varSum, numOps.Multiply(diff, diff));\n+                }\n+                variances[b] = numOps.Divide(varSum, numOps.FromDouble(features));\n+\n+                // Normalize and scale\n+                var std = numOps.Sqrt(numOps.Add(variances[b], eps));\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var norm = numOps.Divide(\n+                        numOps.Subtract(a.Value[b, f], means[b]),\n+                        std);\n+                    normalized[b, f] = norm;\n+                    result[b, f] = numOps.Add(\n+                        numOps.Multiply(norm, gamma.Value[f]),\n+                        beta.Value[f]);\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                // Gradients for gamma and beta\n+                if (gamma.RequiresGradient)\n+                {\n+                    var gradGamma = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum,\n+                                numOps.Multiply(gradient[b, f], normalized[b, f]));\n+                        }\n+                        gradGamma[f] = sum;\n+                    }\n+\n+                    if (gamma.Gradient == null)\n+                        gamma.Gradient = gradGamma;\n+                    else\n+                        gamma.Gradient = gamma.Gradient.Add(gradGamma);\n+                }\n+\n+                if (beta.RequiresGradient)\n+                {\n+                    var gradBeta = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum, gradient[b, f]);\n+                        }\n+                        gradBeta[f] = sum;\n+                    }\n+\n+                    if (beta.Gradient == null)\n+                        beta.Gradient = gradBeta;\n+                    else\n+                        beta.Gradient = beta.Gradient.Add(gradBeta);\n+                }\n+\n+                // Gradient for input\n+                if (a.RequiresGradient)\n+                {\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        var std = numOps.Sqrt(numOps.Add(variances[b], eps));\n+                        var invStd = numOps.Divide(numOps.One, std);\n+\n+                        // Compute gradient components\n+                        var gradNormSum = numOps.Zero;\n+                        var gradNormDotNorm = numOps.Zero;\n+\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            var gradNorm = numOps.Multiply(gradient[b, f], gamma.Value[f]);\n+                            gradNormSum = numOps.Add(gradNormSum, gradNorm);\n+                            gradNormDotNorm = numOps.Add(gradNormDotNorm,\n+                                numOps.Multiply(gradNorm, normalized[b, f]));\n+                        }\n+\n+                        // Apply gradient formula\n+                        var featuresT = numOps.FromDouble(features);\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            var gradNorm = numOps.Multiply(gradient[b, f], gamma.Value[f]);\n+\n+                            var term1 = gradNorm;\n+                            var term2 = numOps.Divide(gradNormSum, featuresT);\n+                            var term3 = numOps.Divide(\n+                                numOps.Multiply(normalized[b, f], gradNormDotNorm),\n+                                featuresT);\n+\n+                            var grad = numOps.Multiply(\n+                                numOps.Subtract(numOps.Subtract(term1, term2), term3),\n+                                invStd);\n+\n+                            gradA[b, f] = grad;\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var parents = new List<ComputationNode<T>> { a };\n+            if (gamma != null) parents.Add(gamma);\n+            if (beta != null) parents.Add(beta);\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient || (gamma?.RequiresGradient ?? false) || (beta?.RequiresGradient ?? false),\n+                parents: parents,\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"LayerNorm is currently only implemented for 2D tensors normalizing over last dimension. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}], normalizedShape=[{string.Join(\", \", normalizedShape)}]\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Applies batch normalization to a computation node.\n+    /// </summary>\n+    /// <param name=\"a\">The input node with shape [batch, features].</param>\n+    /// <param name=\"gamma\">Optional scale parameter (learnable). If null, uses ones.</param>\n+    /// <param name=\"beta\">Optional shift parameter (learnable). If null, uses zeros.</param>\n+    /// <param name=\"runningMean\">Running mean for inference (not updated during this operation).</param>\n+    /// <param name=\"runningVar\">Running variance for inference (not updated during this operation).</param>\n+    /// <param name=\"training\">Whether in training mode (uses batch statistics) or inference mode (uses running statistics).</param>\n+    /// <param name=\"epsilon\">Small constant for numerical stability. Default is 1e-5.</param>\n+    /// <returns>A new computation node containing the batch normalized result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Batch normalization normalizes inputs across the batch dimension.\n+    /// During training: Uses batch statistics (mean and variance computed from current batch).\n+    /// During inference: Uses running statistics (accumulated during training).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> BatchNorm standardizes features across the batch.\n+    ///\n+    /// For batch normalization:\n+    /// - Training mode: Uses current batch's mean and variance\n+    /// - Inference mode: Uses running mean/variance from training\n+    /// - Normalizes: (x - mean) / sqrt(variance)\n+    /// - Scales and shifts: result * gamma + beta\n+    ///\n+    /// Benefits:\n+    /// - Stabilizes training (reduces internal covariate shift)\n+    /// - Allows higher learning rates\n+    /// - Acts as regularization\n+    ///\n+    /// Used in:\n+    /// - CNNs (after convolutional layers)\n+    /// - Deep feedforward networks\n+    /// - GANs and many other architectures\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> BatchNorm(\n+        ComputationNode<T> a,\n+        ComputationNode<T>? gamma = null,\n+        ComputationNode<T>? beta = null,\n+        Tensor<T>? runningMean = null,\n+        Tensor<T>? runningVar = null,\n+        bool training = true,\n+        double epsilon = 1e-5)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+        var eps = numOps.FromDouble(epsilon);\n+\n+        // Handle 2D case [batch, features]\n+        if (shape.Length == 2)\n+        {\n+            int batchSize = shape[0];\n+            int features = shape[1];\n+\n+            // Create default gamma and beta if not provided\n+            if (gamma == null)\n+            {\n+                var gammaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    gammaTensor[i] = numOps.One;\n+                gamma = Variable(gammaTensor, requiresGradient: false);\n+            }\n+\n+            if (beta == null)\n+            {\n+                var betaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    betaTensor[i] = numOps.Zero;\n+                beta = Variable(betaTensor, requiresGradient: false);\n+            }\n+\n+            var result = new Tensor<T>(shape);\n+            T[] batchMean;\n+            T[] batchVar;\n+            var normalized = new Tensor<T>(shape);\n+\n+            if (training)\n+            {\n+                // Compute batch statistics\n+                batchMean = new T[features];\n+                batchVar = new T[features];\n+\n+                // Compute mean per feature\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var sum = numOps.Zero;\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        sum = numOps.Add(sum, a.Value[b, f]);\n+                    }\n+                    batchMean[f] = numOps.Divide(sum, numOps.FromDouble(batchSize));\n+                }\n+\n+                // Compute variance per feature\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var varSum = numOps.Zero;\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        var diff = numOps.Subtract(a.Value[b, f], batchMean[f]);\n+                        varSum = numOps.Add(varSum, numOps.Multiply(diff, diff));\n+                    }\n+                    batchVar[f] = numOps.Divide(varSum, numOps.FromDouble(batchSize));\n+                }\n+            }\n+            else\n+            {\n+                // Use running statistics for inference\n+                if (runningMean == null || runningVar == null)\n+                    throw new ArgumentException(\"Running statistics required for inference mode\");\n+\n+                batchMean = new T[features];\n+                batchVar = new T[features];\n+                for (int f = 0; f < features; f++)\n+                {\n+                    batchMean[f] = runningMean[f];\n+                    batchVar[f] = runningVar[f];\n+                }\n+            }\n+\n+            // Normalize and scale\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var std = numOps.Sqrt(numOps.Add(batchVar[f], eps));\n+                    var norm = numOps.Divide(\n+                        numOps.Subtract(a.Value[b, f], batchMean[f]),\n+                        std);\n+                    normalized[b, f] = norm;\n+                    result[b, f] = numOps.Add(\n+                        numOps.Multiply(norm, gamma.Value[f]),\n+                        beta.Value[f]);\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (!training)\n+                {\n+                    // Inference mode: simpler gradient (no batch statistics gradient)\n+                    if (a.RequiresGradient)\n+                    {\n+                        var gradA = new Tensor<T>(shape);\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            for (int f = 0; f < features; f++)\n+                            {\n+                                var std = numOps.Sqrt(numOps.Add(batchVar[f], eps));\n+                                var invStd = numOps.Divide(numOps.One, std);\n+                                gradA[b, f] = numOps.Multiply(\n+                                    numOps.Multiply(gradient[b, f], gamma.Value[f]),\n+                                    invStd);\n+                            }\n+                        }\n+\n+                        if (a.Gradient == null)\n+                            a.Gradient = gradA;\n+                        else\n+                            a.Gradient = a.Gradient.Add(gradA);\n+                    }\n+                    return;\n+                }\n+\n+                // Training mode: full gradient computation\n+                // Gradients for gamma and beta\n+                if (gamma.RequiresGradient)\n+                {\n+                    var gradGamma = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum,\n+                                numOps.Multiply(gradient[b, f], normalized[b, f]));\n+                        }\n+                        gradGamma[f] = sum;\n+                    }\n+\n+                    if (gamma.Gradient == null)\n+                        gamma.Gradient = gradGamma;\n+                    else\n+                        gamma.Gradient = gamma.Gradient.Add(gradGamma);\n+                }\n+\n+                if (beta.RequiresGradient)\n+                {\n+                    var gradBeta = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum, gradient[b, f]);\n+                        }\n+                        gradBeta[f] = sum;\n+                    }\n+\n+                    if (beta.Gradient == null)\n+                        beta.Gradient = gradBeta;\n+                    else\n+                        beta.Gradient = beta.Gradient.Add(gradBeta);\n+                }\n+\n+                // Gradient for input (complex due to batch statistics)\n+                if (a.RequiresGradient)\n+                {\n+                    var gradA = new Tensor<T>(shape);\n+                    var batchSizeT = numOps.FromDouble(batchSize);\n+\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var std = numOps.Sqrt(numOps.Add(batchVar[f], eps));\n+                        var invStd = numOps.Divide(numOps.One, std);\n+\n+                        // Sum of gradients and gradient*normalized\n+                        var gradSum = numOps.Zero;\n+                        var gradNormSum = numOps.Zero;\n+\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            var grad = numOps.Multiply(gradient[b, f], gamma.Value[f]);\n+                            gradSum = numOps.Add(gradSum, grad);\n+                            gradNormSum = numOps.Add(gradNormSum,\n+                                numOps.Multiply(grad, normalized[b, f]));\n+                        }\n+\n+                        // Apply gradient formula\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            var grad = numOps.Multiply(gradient[b, f], gamma.Value[f]);\n+\n+                            var term1 = grad;\n+                            var term2 = numOps.Divide(gradSum, batchSizeT);\n+                            var term3 = numOps.Divide(\n+                                numOps.Multiply(normalized[b, f], gradNormSum),\n+                                batchSizeT);\n+\n+                            var gradInput = numOps.Multiply(\n+                                numOps.Subtract(numOps.Subtract(term1, term2), term3),\n+                                invStd);\n+\n+                            gradA[b, f] = gradInput;\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var parents = new List<ComputationNode<T>> { a };\n+            if (gamma != null) parents.Add(gamma);\n+            if (beta != null) parents.Add(beta);\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient || (gamma?.RequiresGradient ?? false) || (beta?.RequiresGradient ?? false),\n+                parents: parents,\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"BatchNorm is currently only implemented for 2D tensors [batch, features]. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}]\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs 2D convolution on a 4D tensor (batch, channels, height, width).\n+    /// </summary>\n+    /// <param name=\"input\">The input node with shape [batch, inChannels, height, width].</param>\n+    /// <param name=\"kernel\">The kernel/filter with shape [outChannels, inChannels, kernelH, kernelW].</param>\n+    /// <param name=\"bias\">Optional bias with shape [outChannels]. If null, no bias is added.</param>\n+    /// <param name=\"stride\">The stride [strideH, strideW]. Default is [1, 1].</param>\n+    /// <param name=\"padding\">The padding [padH, padW]. Default is [0, 0].</param>\n+    /// <returns>A new computation node containing the convolution result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method performs 2D convolution, the fundamental operation in CNNs.\n+    /// Forward: Slides the kernel over the input computing dot products.\n+    /// Backward: Computes gradients for both input and kernel using transposed convolutions.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Conv2D is the core operation of convolutional neural networks.\n+    ///\n+    /// For 2D convolution:\n+    /// - The kernel \"slides\" over the input, computing weighted sums\n+    /// - Each output position is a dot product of the kernel with input patch\n+    /// - Stride controls how far the kernel moves each step\n+    /// - Padding adds borders to control output size\n+    ///\n+    /// Gradient computation:\n+    /// - Gradient w.r.t. input: \"full\" convolution with flipped kernel\n+    /// - Gradient w.r.t. kernel: cross-correlation between input and output gradient\n+    ///\n+    /// Used in:\n+    /// - All CNNs (image classification, object detection, segmentation)\n+    /// - Feature extraction in vision models\n+    /// - Learning spatial hierarchies\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Conv2D(\n+        ComputationNode<T> input,\n+        ComputationNode<T> kernel,\n+        ComputationNode<T>? bias = null,\n+        int[]? stride = null,\n+        int[]? padding = null)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var inputShape = input.Value.Shape;\n+        var kernelShape = kernel.Value.Shape;\n+\n+        if (inputShape.Length != 4)\n+            throw new ArgumentException(\"Conv2D requires 4D input [batch, inChannels, height, width]\");\n+        if (kernelShape.Length != 4)\n+            throw new ArgumentException(\"Conv2D requires 4D kernel [outChannels, inChannels, kernelH, kernelW]\");\n+\n+        stride ??= new int[] { 1, 1 };\n+        padding ??= new int[] { 0, 0 };\n+\n+        int batch = inputShape[0];\n+        int inChannels = inputShape[1];\n+        int inH = inputShape[2];\n+        int inW = inputShape[3];\n+\n+        int outChannels = kernelShape[0];\n+        int kernelInChannels = kernelShape[1];\n+        int kernelH = kernelShape[2];\n+        int kernelW = kernelShape[3];\n+\n+        if (inChannels != kernelInChannels)\n+            throw new ArgumentException($\"Input channels ({inChannels}) must match kernel input channels ({kernelInChannels})\");\n+\n+        int strideH = stride[0];\n+        int strideW = stride[1];\n+        int padH = padding[0];\n+        int padW = padding[1];\n+\n+        int outH = (inH + 2 * padH - kernelH) / strideH + 1;\n+        int outW = (inW + 2 * padW - kernelW) / strideW + 1;\n+\n+        var result = new Tensor<T>(new int[] { batch, outChannels, outH, outW });\n+\n+        // Forward pass: convolution\n+        for (int b = 0; b < batch; b++)\n+        {\n+            for (int oc = 0; oc < outChannels; oc++)\n+            {\n+                for (int oh = 0; oh < outH; oh++)\n+                {\n+                    for (int ow = 0; ow < outW; ow++)\n+                    {\n+                        var sum = numOps.Zero;\n+\n+                        // Convolve kernel over input\n+                        for (int ic = 0; ic < inChannels; ic++)\n+                        {\n+                            for (int kh = 0; kh < kernelH; kh++)\n+                            {\n+                                for (int kw = 0; kw < kernelW; kw++)\n+                                {\n+                                    int ih = oh * strideH + kh - padH;\n+                                    int iw = ow * strideW + kw - padW;\n+\n+                                    // Check bounds (padding)\n+                                    if (ih >= 0 && ih < inH && iw >= 0 && iw < inW)\n+                                    {\n+                                        var inputVal = input.Value[b, ic, ih, iw];\n+                                        var kernelVal = kernel.Value[oc, ic, kh, kw];\n+                                        sum = numOps.Add(sum, numOps.Multiply(inputVal, kernelVal));\n+                                    }\n+                                }\n+                            }\n+                        }\n+\n+                        // Add bias if provided\n+                        if (bias != null)\n+                        {\n+                            sum = numOps.Add(sum, bias.Value[oc]);\n+                        }\n+\n+                        result[b, oc, oh, ow] = sum;\n+                    }\n+                }\n+            }\n+        }\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            // Gradient w.r.t. input\n+            if (input.RequiresGradient)\n+            {\n+                var gradInput = new Tensor<T>(inputShape);\n+\n+                // Full convolution with flipped kernel\n+                for (int b = 0; b < batch; b++)\n+                {\n+                    for (int ic = 0; ic < inChannels; ic++)\n+                    {\n+                        for (int ih = 0; ih < inH; ih++)\n+                        {\n+                            for (int iw = 0; iw < inW; iw++)\n+                            {\n+                                var sum = numOps.Zero;\n+\n+                                // Iterate over all output positions that used this input position\n+                                for (int oc = 0; oc < outChannels; oc++)\n+                                {\n+                                    for (int kh = 0; kh < kernelH; kh++)\n+                                    {\n+                                        for (int kw = 0; kw < kernelW; kw++)\n+                                        {\n+                                            // Compute output position\n+                                            int ohShifted = ih + padH - kh;\n+                                            int owShifted = iw + padW - kw;\n+\n+                                            if (ohShifted % strideH == 0 && owShifted % strideW == 0)\n+                                            {\n+                                                int oh = ohShifted / strideH;\n+                                                int ow = owShifted / strideW;\n+\n+                                                if (oh >= 0 && oh < outH && ow >= 0 && ow < outW)\n+                                                {\n+                                                    var gradVal = gradient[b, oc, oh, ow];\n+                                                    var kernelVal = kernel.Value[oc, ic, kh, kw];\n+                                                    sum = numOps.Add(sum, numOps.Multiply(gradVal, kernelVal));\n+                                                }\n+                                            }\n+                                        }\n+                                    }\n+                                }\n+\n+                                gradInput[b, ic, ih, iw] = sum;\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (input.Gradient == null)\n+                    input.Gradient = gradInput;\n+                else\n+                    input.Gradient = input.Gradient.Add(gradInput);\n+            }\n+\n+            // Gradient w.r.t. kernel\n+            if (kernel.RequiresGradient)\n+            {\n+                var gradKernel = new Tensor<T>(kernelShape);\n+\n+                // Cross-correlation between input and output gradient\n+                for (int oc = 0; oc < outChannels; oc++)\n+                {\n+                    for (int ic = 0; ic < inChannels; ic++)\n+                    {\n+                        for (int kh = 0; kh < kernelH; kh++)\n+                        {\n+                            for (int kw = 0; kw < kernelW; kw++)\n+                            {\n+                                var sum = numOps.Zero;\n+\n+                                for (int b = 0; b < batch; b++)\n+                                {\n+                                    for (int oh = 0; oh < outH; oh++)\n+                                    {\n+                                        for (int ow = 0; ow < outW; ow++)\n+                                        {\n+                                            int ih = oh * strideH + kh - padH;\n+                                            int iw = ow * strideW + kw - padW;\n+\n+                                            if (ih >= 0 && ih < inH && iw >= 0 && iw < inW)\n+                                            {\n+                                                var gradVal = gradient[b, oc, oh, ow];\n+                                                var inputVal = input.Value[b, ic, ih, iw];\n+                                                sum = numOps.Add(sum, numOps.Multiply(gradVal, inputVal));\n+                                            }\n+                                        }\n+                                    }\n+                                }\n+\n+                                gradKernel[oc, ic, kh, kw] = sum;\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (kernel.Gradient == null)\n+                    kernel.Gradient = gradKernel;\n+                else\n+                    kernel.Gradient = kernel.Gradient.Add(gradKernel);\n+            }\n+\n+            // Gradient w.r.t. bias\n+            if (bias != null && bias.RequiresGradient)\n+            {\n+                var gradBias = new Tensor<T>(new int[] { outChannels });\n+\n+                for (int oc = 0; oc < outChannels; oc++)\n+                {\n+                    var sum = numOps.Zero;\n+                    for (int b = 0; b < batch; b++)\n+                    {\n+                        for (int oh = 0; oh < outH; oh++)\n+                        {\n+                            for (int ow = 0; ow < outW; ow++)\n+                            {\n+                                sum = numOps.Add(sum, gradient[b, oc, oh, ow]);\n+                            }\n+                        }\n+                    }\n+                    gradBias[oc] = sum;\n+                }\n+\n+                if (bias.Gradient == null)\n+                    bias.Gradient = gradBias;\n+                else\n+                    bias.Gradient = bias.Gradient.Add(gradBias);\n+            }\n+        }\n+\n+        var parents = new List<ComputationNode<T>> { input, kernel };\n+        if (bias != null) parents.Add(bias);\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: input.RequiresGradient || kernel.RequiresGradient || (bias?.RequiresGradient ?? false),\n+            parents: parents,\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Performs 2D transposed convolution (deconvolution) on a 4D tensor.\n+    /// </summary>\n+    /// <param name=\"input\">The input node with shape [batch, inChannels, height, width].</param>\n+    /// <param name=\"kernel\">The kernel with shape [inChannels, outChannels, kernelH, kernelW] (note: reversed from Conv2D).</param>\n+    /// <param name=\"bias\">Optional bias with shape [outChannels]. If null, no bias is added.</param>\n+    /// <param name=\"stride\">The stride [strideH, strideW]. Default is [1, 1].</param>\n+    /// <param name=\"padding\">The padding [padH, padW]. Default is [0, 0].</param>\n+    /// <param name=\"outputPadding\">Output padding [outPadH, outPadW] for size adjustment. Default is [0, 0].</param>\n+    /// <returns>A new computation node containing the transposed convolution result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Transposed convolution (often called deconvolution) upsamples the input.\n+    /// It's the gradient of Conv2D with respect to its input, used as a forward operation.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> ConvTranspose2D upsamples spatial dimensions.\n+    ///\n+    /// For transposed convolution:\n+    /// - Inserts zeros between input elements according to stride\n+    /// - Applies regular convolution to the expanded input\n+    /// - Results in larger spatial dimensions (upsampling)\n+    ///\n+    /// Used in:\n+    /// - Image generation (GANs, VAEs)\n+    /// - Semantic segmentation (U-Net decoder)\n+    /// - Super-resolution\n+    /// - Any task requiring upsampling\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> ConvTranspose2D(\n+        ComputationNode<T> input,\n+        ComputationNode<T> kernel,\n+        ComputationNode<T>? bias = null,\n+        int[]? stride = null,\n+        int[]? padding = null,\n+        int[]? outputPadding = null)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var inputShape = input.Value.Shape;\n+        var kernelShape = kernel.Value.Shape;\n+\n+        if (inputShape.Length != 4)\n+            throw new ArgumentException(\"ConvTranspose2D requires 4D input [batch, inChannels, height, width]\");\n+        if (kernelShape.Length != 4)\n+            throw new ArgumentException(\"ConvTranspose2D requires 4D kernel [inChannels, outChannels, kernelH, kernelW]\");\n+\n+        stride ??= new int[] { 1, 1 };\n+        padding ??= new int[] { 0, 0 };\n+        outputPadding ??= new int[] { 0, 0 };\n+\n+        int batch = inputShape[0];\n+        int inChannels = inputShape[1];\n+        int inH = inputShape[2];\n+        int inW = inputShape[3];\n+\n+        int kernelInChannels = kernelShape[0];\n+        int outChannels = kernelShape[1];\n+        int kernelH = kernelShape[2];\n+        int kernelW = kernelShape[3];\n+\n+        if (inChannels != kernelInChannels)\n+            throw new ArgumentException($\"Input channels ({inChannels}) must match kernel input channels ({kernelInChannels})\");\n+\n+        int strideH = stride[0];\n+        int strideW = stride[1];\n+        int padH = padding[0];\n+        int padW = padding[1];\n+        int outPadH = outputPadding[0];\n+        int outPadW = outputPadding[1];\n+\n+        int outH = (inH - 1) * strideH - 2 * padH + kernelH + outPadH;\n+        int outW = (inW - 1) * strideW - 2 * padW + kernelW + outPadW;\n+\n+        var result = new Tensor<T>(new int[] { batch, outChannels, outH, outW });\n+\n+        // Forward pass: transposed convolution\n+        for (int b = 0; b < batch; b++)\n+        {\n+            for (int ic = 0; ic < inChannels; ic++)\n+            {\n+                for (int ih = 0; ih < inH; ih++)\n+                {\n+                    for (int iw = 0; iw < inW; iw++)\n+                    {\n+                        var inputVal = input.Value[b, ic, ih, iw];\n+\n+                        // Distribute this input value to output using kernel\n+                        for (int oc = 0; oc < outChannels; oc++)\n+                        {\n+                            for (int kh = 0; kh < kernelH; kh++)\n+                            {\n+                                for (int kw = 0; kw < kernelW; kw++)\n+                                {\n+                                    int oh = ih * strideH + kh - padH;\n+                                    int ow = iw * strideW + kw - padW;\n+\n+                                    if (oh >= 0 && oh < outH && ow >= 0 && ow < outW)\n+                                    {\n+                                        var kernelVal = kernel.Value[ic, oc, kh, kw];\n+                                        var contribution = numOps.Multiply(inputVal, kernelVal);\n+                                        result[b, oc, oh, ow] = numOps.Add(result[b, oc, oh, ow], contribution);\n+                                    }\n+                                }\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n+\n+            // Add bias if provided\n+            if (bias != null)\n+            {\n+                for (int oc = 0; oc < outChannels; oc++)\n+                {\n+                    for (int oh = 0; oh < outH; oh++)\n+                    {\n+                        for (int ow = 0; ow < outW; ow++)\n+                        {\n+                            result[b, oc, oh, ow] = numOps.Add(result[b, oc, oh, ow], bias.Value[oc]);\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            // Gradient w.r.t. input (this is a forward Conv2D!)\n+            if (input.RequiresGradient)\n+            {\n+                var gradInput = new Tensor<T>(inputShape);\n+\n+                for (int b = 0; b < batch; b++)\n+                {\n+                    for (int ic = 0; ic < inChannels; ic++)\n+                    {\n+                        for (int ih = 0; ih < inH; ih++)\n+                        {\n+                            for (int iw = 0; iw < inW; iw++)\n+                            {\n+                                var sum = numOps.Zero;\n+\n+                                for (int oc = 0; oc < outChannels; oc++)\n+                                {\n+                                    for (int kh = 0; kh < kernelH; kh++)\n+                                    {\n+                                        for (int kw = 0; kw < kernelW; kw++)\n+                                        {\n+                                            int oh = ih * strideH + kh - padH;\n+                                            int ow = iw * strideW + kw - padW;\n+\n+                                            if (oh >= 0 && oh < outH && ow >= 0 && ow < outW)\n+                                            {\n+                                                var gradVal = gradient[b, oc, oh, ow];\n+                                                var kernelVal = kernel.Value[ic, oc, kh, kw];\n+                                                sum = numOps.Add(sum, numOps.Multiply(gradVal, kernelVal));\n+                                            }\n+                                        }\n+                                    }\n+                                }\n+\n+                                gradInput[b, ic, ih, iw] = sum;\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (input.Gradient == null)\n+                    input.Gradient = gradInput;\n+                else\n+                    input.Gradient = input.Gradient.Add(gradInput);\n+            }\n+\n+            // Gradient w.r.t. kernel\n+            if (kernel.RequiresGradient)\n+            {\n+                var gradKernel = new Tensor<T>(kernelShape);\n+\n+                for (int ic = 0; ic < inChannels; ic++)\n+                {\n+                    for (int oc = 0; oc < outChannels; oc++)\n+                    {\n+                        for (int kh = 0; kh < kernelH; kh++)\n+                        {\n+                            for (int kw = 0; kw < kernelW; kw++)\n+                            {\n+                                var sum = numOps.Zero;\n+\n+                                for (int b = 0; b < batch; b++)\n+                                {\n+                                    for (int ih = 0; ih < inH; ih++)\n+                                    {\n+                                        for (int iw = 0; iw < inW; iw++)\n+                                        {\n+                                            int oh = ih * strideH + kh - padH;\n+                                            int ow = iw * strideW + kw - padW;\n+\n+                                            if (oh >= 0 && oh < outH && ow >= 0 && ow < outW)\n+                                            {\n+                                                var inputVal = input.Value[b, ic, ih, iw];\n+                                                var gradVal = gradient[b, oc, oh, ow];\n+                                                sum = numOps.Add(sum, numOps.Multiply(inputVal, gradVal));\n+                                            }\n+                                        }\n+                                    }\n+                                }\n+\n+                                gradKernel[ic, oc, kh, kw] = sum;\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (kernel.Gradient == null)\n+                    kernel.Gradient = gradKernel;\n+                else\n+                    kernel.Gradient = kernel.Gradient.Add(gradKernel);\n+            }\n+\n+            // Gradient w.r.t. bias\n+            if (bias != null && bias.RequiresGradient)\n+            {\n+                var gradBias = new Tensor<T>(new int[] { outChannels });\n+\n+                for (int oc = 0; oc < outChannels; oc++)\n+                {\n+                    var sum = numOps.Zero;\n+                    for (int b = 0; b < batch; b++)\n+                    {\n+                        for (int oh = 0; oh < outH; oh++)\n+                        {\n+                            for (int ow = 0; ow < outW; ow++)\n+                            {\n+                                sum = numOps.Add(sum, gradient[b, oc, oh, ow]);\n+                            }\n+                        }\n+                    }\n+                    gradBias[oc] = sum;\n+                }\n+\n+                if (bias.Gradient == null)\n+                    bias.Gradient = gradBias;\n+                else\n+                    bias.Gradient = bias.Gradient.Add(gradBias);"}]}},{"id":"PRRT_kwDOKSXUF85hck_f","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/ActivationLayer.cs","line":259,"originalLine":259,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEAC","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.\n```suggestion\n        return UseAutodiff\n            ? BackwardViaAutodiff(outputGradient)\n            : BackwardManual(outputGradient);\n```","createdAt":"2025-11-11T07:53:28Z","path":"src/NeuralNetworks/Layers/ActivationLayer.cs","line":259,"originalLine":259,"diffHunk":"@@ -234,35 +234,145 @@ public override Tensor<T> Forward(Tensor<T> input)\n     /// multiplied by the output gradient. For vector activation, the derivative tensor is multiplied by the output gradient.\n     /// </para>\n     /// <para><b>For Beginners:</b> This method calculates how the error gradient flows backward through this layer.\n-    /// \n+    ///\n     /// During backpropagation, the network calculates how each part contributed to the error.\n     /// This method:\n     /// 1. Checks that Forward() was called first (we need the saved input)\n     /// 2. Verifies the gradient has the correct shape\n     /// 3. Calculates how the gradient changes as it passes through this layer\n     /// 4. Returns the modified gradient\n-    /// \n+    ///\n     /// For example, with ReLU activation:\n     /// - If the input was positive, the gradient passes through unchanged\n     /// - If the input was negative, the gradient is blocked (becomes 0)\n-    /// \n+    ///\n     /// This is because ReLU's derivative is 1 for positive inputs and 0 for negative inputs.\n-    /// \n+    ///\n     /// This process helps the network understand which neurons to adjust during training.\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hck_m","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/AddLayer.cs","line":291,"originalLine":291,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEAN","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.\n```suggestion\n        return UseAutodiff\n            ? BackwardViaAutodiff(outputGradient)\n            : BackwardManual(outputGradient);\n```","createdAt":"2025-11-11T07:53:28Z","path":"src/NeuralNetworks/Layers/AddLayer.cs","line":291,"originalLine":291,"diffHunk":"@@ -268,22 +268,35 @@ public override Tensor<T> Forward(params Tensor<T>[] inputs)\n     /// interface, but internally it calculates gradients for all inputs.\n     /// </para>\n     /// <para><b>For Beginners:</b> This method calculates how the error gradient flows backward through this layer.\n-    /// \n+    ///\n     /// During backpropagation, this method:\n     /// 1. Checks that Forward() was called first\n     /// 2. Calculates how the gradient changes due to the activation function (if any)\n     /// 3. Creates a copy of this gradient for each input\n     /// 4. Returns the gradient for the first input\n-    /// \n+    ///\n     /// For addition, the gradient flows equally to all inputs. This means if the output\n     /// needs to change by some amount, each input contributes equally to that change.\n-    /// \n+    ///\n     /// Note: This method only returns the gradient for the first input due to interface\n     /// constraints. In a real network, you would need to handle returning all gradients\n     /// to their respective sources.\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hck_w","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/AnomalyDetectorLayer.cs","line":483,"originalLine":483,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEAd","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.\n```suggestion\n        return UseAutodiff\n            ? BackwardViaAutodiff(outputGradient)\n            : BackwardManual(outputGradient);\n```","createdAt":"2025-11-11T07:53:29Z","path":"src/NeuralNetworks/Layers/AnomalyDetectorLayer.cs","line":483,"originalLine":483,"diffHunk":"@@ -464,34 +464,65 @@ public Dictionary<string, double> GetAnomalyStatistics()\n     /// trainable parameters, it simply passes the gradient through to the previous layer.\n     /// </para>\n     /// <para><b>For Beginners:</b> This method passes error information backward during training.\n-    /// \n+    ///\n     /// The backward pass:\n     /// - Takes an error gradient from the next layer\n     /// - Propagates it back to the previous layer\n     /// - Doesn't modify any parameters since this layer doesn't learn\n-    /// \n+    ///\n     /// This method exists to maintain compatibility with the neural network\n     /// backpropagation mechanism, but it doesn't do much in this layer\n     /// since there are no weights to adjust.\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hck_6","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/AttentionLayer.cs","line":448,"originalLine":448,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEAp","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.\n```suggestion\n        return UseAutodiff\n            ? BackwardViaAutodiff(outputGradient)\n            : BackwardManual(outputGradient);\n```","createdAt":"2025-11-11T07:53:29Z","path":"src/NeuralNetworks/Layers/AttentionLayer.cs","line":448,"originalLine":448,"diffHunk":"@@ -432,15 +432,28 @@ private Tensor<T> ForwardCrossAttention(Tensor<T> queryInput, Tensor<T> keyValue\n     /// the gradients of the loss with respect to the layer's parameters and input.\n     /// </para>\n     /// <para><b>For Beginners:</b> This is how the layer learns from its mistakes.\n-    /// \n+    ///\n     /// The method takes the gradient of the error with respect to the layer's output and works backwards to figure out:\n     /// 1. How much each weight contributed to the error (stored in _dWq, _dWk, _dWv)\n     /// 2. How the input itself contributed to the error (the returned value)\n-    /// \n+    ///\n     /// This information is then used to update the weights and improve the layer's performance.\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclAC","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/BatchNormalizationLayer.cs","line":344,"originalLine":344,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEA2","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.\n```suggestion\n        return UseAutodiff\n            ? BackwardViaAutodiff(outputGradient)\n            : BackwardManual(outputGradient);\n```","createdAt":"2025-11-11T07:53:29Z","path":"src/NeuralNetworks/Layers/BatchNormalizationLayer.cs","line":344,"originalLine":344,"diffHunk":"@@ -323,20 +323,33 @@ public override Tensor<T> Forward(Tensor<T> input)\n     ///    - Sums the product of output gradients and normalized values\n     ///    - Calculates gradients for gamma and beta parameters\n     ///    - Calculates gradients for each input value\n-    /// \n+    ///\n     /// The calculation is complex because in batch normalization, each input affects:\n     /// - Its own normalized value directly\n     /// - The mean of the batch (which affects all normalized values)\n     /// - The variance of the batch (which affects all normalized values)\n-    /// \n+    ///\n     /// The formula accounts for all these dependencies using the chain rule of calculus.\n-    /// \n+    ///\n     /// This method stores the gradients for gamma and beta to use during parameter updates,\n     /// and returns the gradient for the input to pass to previous layers.\n     /// </para>\n     /// </remarks>\n     /// <exception cref=\"InvalidOperationException\">Thrown when backward is called before forward.</exception>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclAI","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/CapsuleLayer.cs","line":545,"originalLine":545,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEA-","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.\n```suggestion\n        return UseAutodiff\n            ? BackwardViaAutodiff(outputGradient)\n            : BackwardManual(outputGradient);\n```","createdAt":"2025-11-11T07:53:29Z","path":"src/NeuralNetworks/Layers/CapsuleLayer.cs","line":545,"originalLine":545,"diffHunk":"@@ -518,26 +518,39 @@ public override Tensor<T> Forward(Tensor<T> input)\n     /// parameters (transformation matrix and bias) and the layer's input. The gradients are stored internally and\n     /// used during the parameter update step.\n     /// </para>\n-    /// <para><b>For Beginners:</b> This method is used during training to calculate how the layer's inputs \n+    /// <para><b>For Beginners:</b> This method is used during training to calculate how the layer's inputs\n     /// and parameters should change to reduce errors.\n     ///\n     /// The backward pass:\n     /// 1. Takes in gradients (directions of improvement) from the next layer\n     /// 2. Applies the derivative of the activation function\n     /// 3. Calculates how much each parameter (transformation matrix and bias) contributed to the error\n     /// 4. Calculates how the input contributed to the error, to pass gradients to the previous layer\n-    /// \n+    ///\n     /// During this process, the method:\n     /// - Creates gradient tensors for the transformation matrix and bias\n     /// - Uses the coupling coefficients (connection strengths) calculated during the forward pass\n     /// - Produces gradients that will be used to update the parameters\n-    /// \n+    ///\n     /// This is part of the \"backpropagation\" algorithm that helps neural networks learn.\n     /// The error flows backward through the network, and each layer determines how it\n     /// should change to reduce that error.\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclAR","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/ConditionalRandomFieldLayer.cs","line":437,"originalLine":437,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEBL","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.\n```suggestion\n        return UseAutodiff\n            ? BackwardViaAutodiff(outputGradient)\n            : BackwardManual(outputGradient);\n    }\n```","createdAt":"2025-11-11T07:53:30Z","path":"src/NeuralNetworks/Layers/ConditionalRandomFieldLayer.cs","line":437,"originalLine":437,"diffHunk":"@@ -417,17 +417,30 @@ public override Tensor<T> Forward(Tensor<T> input)\n     ///    - How start and end scores should change\n     /// 3. It calculates how the input features contributed to the error\n     /// 4. If an activation function was used, its derivative is applied\n-    /// \n+    ///\n     /// This lets the network learn:\n     /// - Which label is likely to follow another\n     /// - Which labels commonly appear at the start or end of sequences\n     /// - How input features relate to labels\n-    /// \n+    ///\n     /// This is part of the \"backpropagation\" algorithm that helps neural networks learn\n     /// from their mistakes and improve over time.\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);\n+    }\n+"}]}},{"id":"PRRT_kwDOKSXUF85hclAX","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/ConvolutionalLayer.cs","line":776,"originalLine":776,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEBT","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.\n```suggestion\n        return UseAutodiff\n            ? BackwardViaAutodiff(outputGradient)\n            : BackwardManual(outputGradient);\n```","createdAt":"2025-11-11T07:53:30Z","path":"src/NeuralNetworks/Layers/ConvolutionalLayer.cs","line":776,"originalLine":776,"diffHunk":"@@ -756,19 +756,32 @@ public override Tensor<T> Forward(Tensor<T> input)\n     /// earlier layers.\n     /// </para>\n     /// <para><b>For Beginners:</b> This method helps the layer learn from its mistakes.\n-    /// \n+    ///\n     /// During the backward pass:\n     /// - The layer receives information about how wrong its output was\n     /// - It calculates how to adjust its pattern detectors to be more accurate\n     /// - It updates the kernels and biases to improve future predictions\n     /// - It passes information back to previous layers so they can learn too\n-    /// \n+    ///\n     /// This is where the actual \"learning\" happens in the neural network.\n     /// The layer gradually improves its pattern recognition based on feedback\n     /// about its performance.\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclAc","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/CroppingLayer.cs","line":337,"originalLine":335,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEBa","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.\n```suggestion\n        return UseAutodiff\n            ? BackwardViaAutodiff(outputGradient)\n            : BackwardManual(outputGradient);\n```","createdAt":"2025-11-11T07:53:30Z","path":"src/NeuralNetworks/Layers/CroppingLayer.cs","line":337,"originalLine":335,"diffHunk":"@@ -328,6 +328,19 @@ public override Tensor<T> Forward(Tensor<T> input)\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclAo","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/DecoderLayer.cs","line":211,"originalLine":211,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEBn","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.\n```suggestion\n        return UseAutodiff\n            ? BackwardViaAutodiff(outputGradient)\n            : BackwardManual(outputGradient);\n```","createdAt":"2025-11-11T07:53:31Z","path":"src/NeuralNetworks/Layers/DecoderLayer.cs","line":211,"originalLine":211,"diffHunk":"@@ -204,6 +204,19 @@ private Tensor<T> ForwardInternal(Tensor<T> input, Tensor<T> encoderOutput, Tens\n     /// to access these gradients separately.</para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclAv","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/DeconvolutionalLayer.cs","line":556,"originalLine":556,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEBx","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.\n```suggestion\n        return UseAutodiff\n            ? BackwardViaAutodiff(outputGradient)\n            : BackwardManual(outputGradient);\n```","createdAt":"2025-11-11T07:53:31Z","path":"src/NeuralNetworks/Layers/DeconvolutionalLayer.cs","line":556,"originalLine":556,"diffHunk":"@@ -537,18 +537,32 @@ public override Tensor<T> Forward(Tensor<T> input)\n     /// gradient is returned for propagation to earlier layers.\n     /// </para>\n     /// <para><b>For Beginners:</b> This method helps the layer learn from its mistakes.\n-    /// \n+    ///\n     /// During the backward pass:\n     /// - The layer receives information about how wrong its output was\n     /// - It calculates how to adjust its pattern generators to be more accurate\n     /// - It prepares the gradients for updating kernels and biases\n     /// - It passes information back to previous layers so they can learn too\n-    /// \n+    ///\n     /// This is where the actual \"learning\" happens. The layer figures out how to\n     /// adjust all its internal values to make better outputs next time.\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclA_","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/DenseLayer.cs","line":654,"originalLine":654,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzECF","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.\n```suggestion\n        return UseAutodiff\n            ? BackwardViaAutodiff(outputGradient)\n            : BackwardManual(outputGradient);\n```","createdAt":"2025-11-11T07:53:31Z","path":"src/NeuralNetworks/Layers/DenseLayer.cs","line":654,"originalLine":654,"diffHunk":"@@ -635,18 +635,31 @@ public override Tensor<T> Forward(Tensor<T> input)\n     /// input gradient is returned for propagation to earlier layers.\n     /// </para>\n     /// <para><b>For Beginners:</b> This method helps the layer learn from its mistakes.\n-    /// \n+    ///\n     /// During the backward pass:\n     /// - The layer receives information about how wrong its output was\n     /// - It calculates how to adjust its weights and biases to be more accurate\n     /// - It prepares the adjustments but doesn't apply them yet\n     /// - It passes information back to previous layers so they can learn too\n-    /// \n+    ///\n     /// This is where the actual \"learning\" happens. The layer figures out which connections\n     /// should be strengthened and which should be weakened based on the error in its output.\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclBO","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/DepthwiseSeparableConvolutionalLayer.cs","line":880,"originalLine":880,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzECW","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.\n```suggestion\n        return UseAutodiff\n            ? BackwardViaAutodiff(outputGradient)\n            : BackwardManual(outputGradient);\n```","createdAt":"2025-11-11T07:53:31Z","path":"src/NeuralNetworks/Layers/DepthwiseSeparableConvolutionalLayer.cs","line":880,"originalLine":880,"diffHunk":"@@ -857,22 +857,36 @@ private Tensor<T> PointwiseConvolution(Tensor<T> input)\n     /// These gradients indicate how each parameter should be adjusted to reduce the loss.\n     /// </para>\n     /// <para><b>For Beginners:</b> This method helps the layer learn from its mistakes.\n-    /// \n+    ///\n     /// During the backward pass:\n     /// - The layer receives information about how wrong its output was\n     /// - It calculates how to adjust each of its filters to be more accurate\n     /// - It prepares the adjustments but doesn't apply them yet\n     /// - It passes information back to previous layers so they can learn too\n-    /// \n+    ///\n     /// The layer has to figure out:\n     /// - How to adjust the depthwise filters (first step)\n     /// - How to adjust the pointwise filters (second step)\n     /// - How to adjust the biases\n-    /// \n+    ///\n     /// This is where the actual \"learning\" happens in the neural network.\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclBd","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/DigitCapsuleLayer.cs","line":435,"originalLine":435,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzECl","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.\n```suggestion\n        return UseAutodiff\n            ? BackwardViaAutodiff(outputGradient)\n            : BackwardManual(outputGradient);\n```","createdAt":"2025-11-11T07:53:32Z","path":"src/NeuralNetworks/Layers/DigitCapsuleLayer.cs","line":435,"originalLine":435,"diffHunk":"@@ -428,6 +428,19 @@ public override Tensor<T> Forward(Tensor<T> input)\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclBp","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/DilatedConvolutionalLayer.cs","line":586,"originalLine":586,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEC4","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.\n```suggestion\n        return UseAutodiff\n            ? BackwardViaAutodiff(outputGradient)\n            : BackwardManual(outputGradient);\n```","createdAt":"2025-11-11T07:53:32Z","path":"src/NeuralNetworks/Layers/DilatedConvolutionalLayer.cs","line":586,"originalLine":586,"diffHunk":"@@ -560,25 +560,39 @@ public override Tensor<T> Forward(Tensor<T> input)\n     /// <exception cref=\"InvalidOperationException\">Thrown when backward is called before forward.</exception>\n     /// <remarks>\n     /// <para>\n-    /// This method implements the backward pass (backpropagation) of the dilated convolutional layer. \n+    /// This method implements the backward pass (backpropagation) of the dilated convolutional layer.\n     /// It computes the gradients of the loss with respect to the layer's weights, biases, and inputs.\n     /// These gradients are used to update the parameters during training.\n     /// </para>\n     /// <para><b>For Beginners:</b> This is where the layer learns from its mistakes during training.\n-    /// \n+    ///\n     /// During the backward pass:\n     /// 1. The layer receives information about how its output contributed to errors\n     /// 2. It calculates three things:\n     ///    - How to adjust each filter value (kernel gradients)\n     ///    - How to adjust each bias value (bias gradients)\n     ///    - How the error flows back to the previous layer (input gradients)\n     /// 3. These gradients are used to update the filters and biases\n-    /// \n+    ///\n     /// The dilation is also taken into account when calculating these gradients,\n     /// ensuring that the learning process understands the dilated nature of the convolution.\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclB1","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/DropoutLayer.cs","line":304,"originalLine":304,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEDE","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.\n```suggestion\n        return UseAutodiff\n            ? BackwardViaAutodiff(outputGradient)\n            : BackwardManual(outputGradient);\n```","createdAt":"2025-11-11T07:53:32Z","path":"src/NeuralNetworks/Layers/DropoutLayer.cs","line":304,"originalLine":304,"diffHunk":"@@ -279,24 +279,37 @@ public override Tensor<T> Forward(Tensor<T> input)\n     /// no dropout was applied in the forward pass.\n     /// </para>\n     /// <para><b>For Beginners:</b> This is where the layer passes error information back to previous layers.\n-    /// \n+    ///\n     /// During training:\n     /// - Gradients represent how each neuron should change to improve\n     /// - We only want to update neurons that were active during the forward pass\n     /// - The dropout mask (which recorded which neurons were on/off) is applied to the gradients\n     /// - Dropped neurons receive zero gradient (no update)\n     /// - Active neurons receive the scaled gradient\n-    /// \n+    ///\n     /// During inference:\n     /// - All gradients pass through unchanged\n     /// - This matches the behavior of the forward pass where all neurons were active\n-    /// \n+    ///\n     /// This consistency between forward and backward passes is essential for proper training.\n     /// </para>\n     /// <exception cref=\"InvalidOperationException\">\n     /// Thrown when backward is called before a forward pass has been performed.\n     /// </exception>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclB8","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/ExpertLayer.cs","line":226,"originalLine":226,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEDO","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.\n```suggestion\n        return UseAutodiff\n            ? BackwardViaAutodiff(outputGradient)\n            : BackwardManual(outputGradient);\n```","createdAt":"2025-11-11T07:53:33Z","path":"src/NeuralNetworks/Layers/ExpertLayer.cs","line":226,"originalLine":226,"diffHunk":"@@ -219,6 +219,19 @@ public override Tensor<T> Forward(Tensor<T> input)\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclCG","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/GaussianNoiseLayer.cs","line":243,"originalLine":243,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEDc","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.\n```suggestion\n        return UseAutodiff\n            ? BackwardViaAutodiff(outputGradient)\n            : BackwardManual(outputGradient);\n```","createdAt":"2025-11-11T07:53:33Z","path":"src/NeuralNetworks/Layers/GaussianNoiseLayer.cs","line":243,"originalLine":243,"diffHunk":"@@ -236,11 +236,42 @@ public override Tensor<T> Forward(Tensor<T> input)\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclCR","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/GatedLinearUnitLayer.cs","line":543,"originalLine":543,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEDt","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.\n```suggestion\n        return UseAutodiff\n            ? BackwardViaAutodiff(outputGradient)\n            : BackwardManual(outputGradient);\n```","createdAt":"2025-11-11T07:53:33Z","path":"src/NeuralNetworks/Layers/GatedLinearUnitLayer.cs","line":543,"originalLine":543,"diffHunk":"@@ -536,6 +536,19 @@ public override Tensor<T> Forward(Tensor<T> input)\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclCd","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/GlobalPoolingLayer.cs","line":360,"originalLine":360,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzED7","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.\n```suggestion\n        return UseAutodiff\n            ? BackwardViaAutodiff(outputGradient)\n            : BackwardManual(outputGradient);\n```","createdAt":"2025-11-11T07:53:34Z","path":"src/NeuralNetworks/Layers/GlobalPoolingLayer.cs","line":360,"originalLine":360,"diffHunk":"@@ -335,24 +335,38 @@ public override Tensor<T> Forward(Tensor<T> input)\n     /// contributed to the output during the forward pass.\n     /// </para>\n     /// <para><b>For Beginners:</b> This is where the layer passes error information back to previous layers.\n-    /// \n+    ///\n     /// The backward pass works differently depending on the pooling type:\n-    /// \n+    ///\n     /// For average pooling:\n     /// - The gradient for each output value is divided equally among all input positions\n     /// - Every position in a feature map gets the same small portion of the gradient\n     /// - This reflects that each input position contributed equally to the average\n-    /// \n+    ///\n     /// For max pooling:\n     /// - The gradient for each output value is assigned only to the input position that had the maximum value\n     /// - Only the \"winning\" position gets the gradient, all others get zero\n     /// - This reflects that only the maximum value contributed to the output\n-    /// \n+    ///\n     /// This process ensures that the network learns appropriately based on how\n     /// each input position influenced the pooled output.\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclCn","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/GraphConvolutionalLayer.cs","line":539,"originalLine":539,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEEQ","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.\n```suggestion\n        return UseAutodiff\n            ? BackwardViaAutodiff(outputGradient)\n            : BackwardManual(outputGradient);\n```","createdAt":"2025-11-11T07:53:34Z","path":"src/NeuralNetworks/Layers/GraphConvolutionalLayer.cs","line":539,"originalLine":539,"diffHunk":"@@ -532,6 +532,19 @@ public override Tensor<T> Forward(Tensor<T> input)\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclCv","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/HighwayLayer.cs","line":537,"originalLine":537,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEEb","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.\n```suggestion\n        return UseAutodiff\n            ? BackwardViaAutodiff(outputGradient)\n            : BackwardManual(outputGradient);\n```","createdAt":"2025-11-11T07:53:34Z","path":"src/NeuralNetworks/Layers/HighwayLayer.cs","line":537,"originalLine":537,"diffHunk":"@@ -530,6 +530,82 @@ private Tensor<T> ApplyActivation(Tensor<T> input, IActivationFunction<T>? scala\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclC2","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/LambdaLayer.cs","line":252,"originalLine":252,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEEk","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.","createdAt":"2025-11-11T07:53:35Z","path":"src/NeuralNetworks/Layers/LambdaLayer.cs","line":252,"originalLine":252,"diffHunk":"@@ -245,6 +245,19 @@ public override Tensor<T> Forward(Tensor<T> input)\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclC6","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/LayerNormalizationLayer.cs","line":272,"originalLine":272,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEEr","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.","createdAt":"2025-11-11T07:53:35Z","path":"src/NeuralNetworks/Layers/LayerNormalizationLayer.cs","line":272,"originalLine":272,"diffHunk":"@@ -252,19 +252,32 @@ public override Tensor<T> Forward(Tensor<T> input)\n     /// </para>\n     /// <para><b>For Beginners:</b> This method is used during training to calculate how the layer's input\n     /// and parameters should change to reduce errors.\n-    /// \n+    ///\n     /// During the backward pass:\n     /// 1. The layer receives information about how its output contributed to errors\n     /// 2. It calculates how the gamma and beta parameters should change to reduce errors\n     /// 3. It calculates how the input should change, which will be used by earlier layers\n-    /// \n+    ///\n     /// This backward computation is complex because changing the mean and standard deviation\n     /// of a sample affects all features, creating interdependencies in the gradients.\n-    /// \n+    ///\n     /// The method will throw an error if you try to run it before performing a forward pass.\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclDA","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/LocallyConnectedLayer.cs","line":543,"originalLine":543,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEEy","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.","createdAt":"2025-11-11T07:53:35Z","path":"src/NeuralNetworks/Layers/LocallyConnectedLayer.cs","line":543,"originalLine":543,"diffHunk":"@@ -520,22 +520,36 @@ public override Tensor<T> Forward(Tensor<T> input)\n     /// </para>\n     /// <para><b>For Beginners:</b> This method is used during training to calculate how the layer's input\n     /// and parameters should change to reduce errors.\n-    /// \n+    ///\n     /// During the backward pass:\n     /// 1. The layer receives information about how its output contributed to errors\n     /// 2. It calculates how the weights and biases should change to reduce errors\n     /// 3. It calculates how the input should change, which will be used by earlier layers\n-    /// \n+    ///\n     /// This process involves:\n     /// - Applying the derivative of the activation function\n     /// - Computing gradients for each unique filter\n     /// - Computing gradients for biases\n     /// - Computing how the input should change\n-    /// \n+    ///\n     /// The method will throw an error if you try to run it before performing a forward pass.\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclDI","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/LogVarianceLayer.cs","line":281,"originalLine":281,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEFC","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.","createdAt":"2025-11-11T07:53:35Z","path":"src/NeuralNetworks/Layers/LogVarianceLayer.cs","line":281,"originalLine":281,"diffHunk":"@@ -274,6 +274,19 @@ public override Tensor<T> Forward(Tensor<T> input)\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclDR","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/MaskingLayer.cs","line":207,"originalLine":207,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEFP","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.","createdAt":"2025-11-11T07:53:36Z","path":"src/NeuralNetworks/Layers/MaskingLayer.cs","line":207,"originalLine":207,"diffHunk":"@@ -200,13 +200,44 @@ public override Tensor<T> Forward(Tensor<T> input)\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclDY","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/MaxPoolingLayer.cs","line":188,"originalLine":188,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEFY","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.","createdAt":"2025-11-11T07:53:36Z","path":"src/NeuralNetworks/Layers/MaxPoolingLayer.cs","line":188,"originalLine":188,"diffHunk":"@@ -173,14 +181,28 @@ public override Tensor<T> Forward(Tensor<T> input)\n     /// <remarks>\n     /// <b>For Beginners:</b> During training, neural networks need to adjust their parameters based on\n     /// how much error they made. This adjustment flows backward through the network.\n-    /// \n+    ///\n     /// In max pooling, only the maximum value from each window contributed to the output.\n     /// So during the backward pass, the gradient only flows back to that maximum value's position.\n     /// All other positions receive zero gradient because they didn't contribute to the output.\n-    /// \n+    ///"}]}},{"id":"PRRT_kwDOKSXUF85hclDe","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/MeasurementLayer.cs","line":190,"originalLine":190,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEFg","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.","createdAt":"2025-11-11T07:53:36Z","path":"src/NeuralNetworks/Layers/MeasurementLayer.cs","line":190,"originalLine":190,"diffHunk":"@@ -183,6 +183,19 @@ public override Tensor<T> Forward(Tensor<T> input)\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclDl","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/MixtureOfExpertsLayer.cs","line":604,"originalLine":604,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEFr","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.","createdAt":"2025-11-11T07:53:36Z","path":"src/NeuralNetworks/Layers/MixtureOfExpertsLayer.cs","line":604,"originalLine":604,"diffHunk":"@@ -597,6 +597,19 @@ public override Tensor<T> Forward(Tensor<T> input)\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclDt","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/MultiplyLayer.cs","line":288,"originalLine":288,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEF2","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.","createdAt":"2025-11-11T07:53:37Z","path":"src/NeuralNetworks/Layers/MultiplyLayer.cs","line":288,"originalLine":288,"diffHunk":"@@ -264,23 +264,36 @@ public override Tensor<T> Forward(params Tensor<T>[] inputs)\n     /// the gradients for all input tensors.\n     /// </para>\n     /// <para><b>For Beginners:</b> This method calculates how changes in each input affect the final output.\n-    /// \n+    ///\n     /// During the backward pass:\n     /// - The layer receives gradients indicating how the output should change\n     /// - It calculates how each input tensor contributed to the output\n     /// - For each input, its gradient is the product of:\n     ///   - The output gradient (after applying the activation function derivative)\n     ///   - All OTHER input tensors (not including itself)\n-    /// \n+    ///\n     /// This follows the chain rule of calculus for multiplication:\n     /// If z = x * y, then:\n     /// - dz/dx = y * (gradient flowing back from later layers)\n     /// - dz/dy = x * (gradient flowing back from later layers)\n-    /// \n+    ///\n     /// The method returns a stacked tensor containing gradients for all inputs.\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclD2","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/MultiHeadAttentionLayer.cs","line":544,"originalLine":544,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEGE","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.","createdAt":"2025-11-11T07:53:37Z","path":"src/NeuralNetworks/Layers/MultiHeadAttentionLayer.cs","line":544,"originalLine":544,"diffHunk":"@@ -524,19 +524,32 @@ public override Tensor<T> Forward(Tensor<T> input)\n     /// <returns>The gradient to be passed to the previous layer.</returns>\n     /// <remarks>\n     /// <para>\n-    /// <b>For Beginners:</b> The backward pass is how neural networks learn. Think of it like figuring out \n+    /// <b>For Beginners:</b> The backward pass is how neural networks learn. Think of it like figuring out\n     /// which parts of a recipe need adjustment after tasting the final dish:\n-    /// \n+    ///\n     /// 1. We first check how our output differs from what was expected (the gradient)\n     /// 2. Then we trace backward through all the calculations we did in the forward pass\n     /// 3. We determine how much each weight contributed to any errors\n     /// 4. These contributions become our gradients, which we'll use to update the weights\n-    /// \n-    /// The complex matrix operations are just a mathematical way of figuring out \n+    ///\n+    /// The complex matrix operations are just a mathematical way of figuring out\n     /// \"if I change this weight a little bit, how much would it improve the output?\"\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclD7","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/PoolingLayer.cs","line":332,"originalLine":332,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEGN","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.","createdAt":"2025-11-11T07:53:37Z","path":"src/NeuralNetworks/Layers/PoolingLayer.cs","line":332,"originalLine":332,"diffHunk":"@@ -306,25 +306,39 @@ public override Tensor<T> Forward(Tensor<T> input)\n     /// equally across all positions in each pooling region.\n     /// </para>\n     /// <para><b>For Beginners:</b> This method calculates how changes in the input would affect the final output.\n-    /// \n+    ///\n     /// During the backward pass:\n     /// - The layer receives gradients for each position in the output tensor\n     /// - It needs to pass these gradients back to the appropriate positions in the input tensor\n-    /// \n+    ///\n     /// For max pooling:\n     /// - Only the position that had the maximum value gets the gradient\n     /// - All other positions in the pooling region get zero gradient\n     /// - This is because changing non-maximum values wouldn't affect the output\n-    /// \n+    ///\n     /// For average pooling:\n     /// - The gradient is divided equally among all positions in the pooling region\n-    /// - Each position gets (output gradient) � (pool size � pool size)\n+    /// - Each position gets (output gradient) / (pool size � pool size)\n     /// - This is because each input position contributes equally to the average\n-    /// \n+    ///\n     /// This approach follows the chain rule of calculus for the respective pooling operations.\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclD-","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/PatchEmbeddingLayer.cs","line":280,"originalLine":280,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEGR","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.","createdAt":"2025-11-11T07:53:37Z","path":"src/NeuralNetworks/Layers/PatchEmbeddingLayer.cs","line":280,"originalLine":280,"diffHunk":"@@ -273,6 +273,82 @@ public override Tensor<T> Forward(Tensor<T> input)\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclEG","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/PositionalEncodingLayer.cs","line":238,"originalLine":238,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEGb","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.","createdAt":"2025-11-11T07:53:38Z","path":"src/NeuralNetworks/Layers/PositionalEncodingLayer.cs","line":238,"originalLine":238,"diffHunk":"@@ -231,6 +231,82 @@ public override Tensor<T> Forward(Tensor<T> input)\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclEN","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/PrimaryCapsuleLayer.cs","line":434,"originalLine":434,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEGk","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.","createdAt":"2025-11-11T07:53:38Z","path":"src/NeuralNetworks/Layers/PrimaryCapsuleLayer.cs","line":434,"originalLine":434,"diffHunk":"@@ -427,6 +427,19 @@ private Vector<T> ExtractPatch(Tensor<T> input, int batch, int startY, int start\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclEW","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/QuantumLayer.cs","line":231,"originalLine":231,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEGu","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.","createdAt":"2025-11-11T07:53:38Z","path":"src/NeuralNetworks/Layers/QuantumLayer.cs","line":231,"originalLine":231,"diffHunk":"@@ -224,6 +224,19 @@ public override Tensor<T> Forward(Tensor<T> input)\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclEf","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/RBFLayer.cs","line":243,"originalLine":243,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEG5","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.","createdAt":"2025-11-11T07:53:38Z","path":"src/NeuralNetworks/Layers/RBFLayer.cs","line":243,"originalLine":243,"diffHunk":"@@ -236,6 +236,19 @@ public override Tensor<T> Forward(Tensor<T> input)\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclEp","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/RBMLayer.cs","line":533,"originalLine":533,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEHI","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.","createdAt":"2025-11-11T07:53:39Z","path":"src/NeuralNetworks/Layers/RBMLayer.cs","line":533,"originalLine":533,"diffHunk":"@@ -526,6 +526,19 @@ private Vector<T> SampleBinaryStates(Vector<T> probabilities)\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclE2","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/ReadoutLayer.cs","line":265,"originalLine":265,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEHY","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.","createdAt":"2025-11-11T07:53:39Z","path":"src/NeuralNetworks/Layers/ReadoutLayer.cs","line":265,"originalLine":265,"diffHunk":"@@ -258,6 +258,19 @@ public override Tensor<T> Forward(Tensor<T> input)\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclE7","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/ReconstructionLayer.cs","line":289,"originalLine":289,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEHf","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.","createdAt":"2025-11-11T07:53:39Z","path":"src/NeuralNetworks/Layers/ReconstructionLayer.cs","line":289,"originalLine":289,"diffHunk":"@@ -282,13 +282,44 @@ public override Tensor<T> Forward(Tensor<T> input)\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclE_","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/RepParameterizationLayer.cs","line":212,"originalLine":212,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEHk","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.","createdAt":"2025-11-11T07:53:39Z","path":"src/NeuralNetworks/Layers/RepParameterizationLayer.cs","line":212,"originalLine":212,"diffHunk":"@@ -205,6 +205,19 @@ public override Tensor<T> Forward(Tensor<T> input)\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclFE","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/ResidualLayer.cs","line":287,"originalLine":287,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEHq","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.","createdAt":"2025-11-11T07:53:40Z","path":"src/NeuralNetworks/Layers/ResidualLayer.cs","line":287,"originalLine":287,"diffHunk":"@@ -266,20 +266,33 @@ public override Tensor<T> Forward(Tensor<T> input)\n     /// </para>\n     /// <para><b>For Beginners:</b> This method is used during training to calculate how the layer's input\n     /// should change to reduce errors.\n-    /// \n+    ///\n     /// During the backward pass:\n     /// - The method throws an error if the forward pass hasn't been called first\n     /// - The gradient is computed for the combined output after the addition\n     /// - If there's an inner layer, the gradient is propagated through it\n     /// - The original gradient and the inner layer gradient are combined\n     /// - The combined gradient is returned for further backpropagation\n-    /// \n+    ///\n     /// This process ensures that gradient information flows both through the inner layer\n     /// and directly back to earlier layers, preventing the vanishing gradient problem\n     /// in deep networks.\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclFJ","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/SelfAttentionLayer.cs","line":465,"originalLine":465,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEHy","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.","createdAt":"2025-11-11T07:53:40Z","path":"src/NeuralNetworks/Layers/SelfAttentionLayer.cs","line":465,"originalLine":465,"diffHunk":"@@ -447,17 +447,30 @@ public override Tensor<T> Forward(Tensor<T> input)\n     ///    - How the value weights should change\n     ///    - How the output biases should change\n     /// 3. It also calculates how the error should propagate back to the previous layer\n-    /// \n+    ///\n     /// This involves complex matrix mathematics, but the basic idea is:\n     /// - Finding which attention patterns led to errors\n     /// - Adjusting the weights to improve these patterns\n     /// - Sending appropriate feedback to the previous layer\n-    /// \n+    ///\n     /// The backward pass is what allows the self-attention mechanism to learn which relationships\n     /// in the sequence are important for the specific task.\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclFX","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/SpatialPoolerLayer.cs","line":448,"originalLine":448,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEIC","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.","createdAt":"2025-11-11T07:53:40Z","path":"src/NeuralNetworks/Layers/SpatialPoolerLayer.cs","line":448,"originalLine":448,"diffHunk":"@@ -441,6 +441,19 @@ private void NormalizeConnections()\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclFc","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/SeparableConvolutionalLayer.cs","line":625,"originalLine":625,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEIO","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.","createdAt":"2025-11-11T07:53:40Z","path":"src/NeuralNetworks/Layers/SeparableConvolutionalLayer.cs","line":625,"originalLine":625,"diffHunk":"@@ -603,21 +603,34 @@ public override Tensor<T> Forward(Tensor<T> input)\n     /// </para>\n     /// <para><b>For Beginners:</b> This method is used during training to calculate how the layer's inputs\n     /// and parameters should change to reduce errors.\n-    /// \n+    ///\n     /// The backward pass:\n     /// 1. Starts with gradients (error signals) from the next layer\n     /// 2. Computes how to adjust the layer's parameters (kernels and biases)\n     /// 3. Calculates how to adjust the input that was received\n-    /// \n+    ///\n     /// This happens in reverse order compared to the forward pass:\n     /// - First backpropagates through the pointwise convolution\n     /// - Then backpropagates through the depthwise convolution\n-    /// \n+    ///\n     /// The calculated gradients are stored for later use when updating the parameters,\n     /// and the input gradient is returned to continue the backpropagation process.\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclFm","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/SpatialTransformerLayer.cs","line":774,"originalLine":774,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEIa","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.","createdAt":"2025-11-11T07:53:41Z","path":"src/NeuralNetworks/Layers/SpatialTransformerLayer.cs","line":774,"originalLine":774,"diffHunk":"@@ -767,6 +767,19 @@ private Tensor<T> SampleInputImage(Tensor<T> input, Tensor<T> outputGrid, Matrix\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclFw","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/SplitLayer.cs","line":221,"originalLine":221,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEIk","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.","createdAt":"2025-11-11T07:53:41Z","path":"src/NeuralNetworks/Layers/SplitLayer.cs","line":221,"originalLine":221,"diffHunk":"@@ -214,6 +214,19 @@ public override Tensor<T> Forward(Tensor<T> input)\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclF8","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/SpikingLayer.cs","line":1428,"originalLine":1428,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEIy","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.","createdAt":"2025-11-11T07:53:41Z","path":"src/NeuralNetworks/Layers/SpikingLayer.cs","line":1428,"originalLine":1428,"diffHunk":"@@ -1421,6 +1421,19 @@ public override void Deserialize(BinaryReader reader)\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclGE","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/SqueezeAndExcitationLayer.cs","line":893,"originalLine":893,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEI7","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.","createdAt":"2025-11-11T07:53:41Z","path":"src/NeuralNetworks/Layers/SqueezeAndExcitationLayer.cs","line":893,"originalLine":893,"diffHunk":"@@ -886,6 +886,82 @@ private static Matrix<T> ApplyScalarActivation(Matrix<T> input, IActivationFunct\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclGJ","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/SubpixelConvolutionalLayer.cs","line":627,"originalLine":627,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEJB","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.","createdAt":"2025-11-11T07:53:42Z","path":"src/NeuralNetworks/Layers/SubpixelConvolutionalLayer.cs","line":627,"originalLine":627,"diffHunk":"@@ -601,25 +601,39 @@ public override Tensor<T> Forward(Tensor<T> input)\n     /// </para>\n     /// <para><b>For Beginners:</b> This method is used during training to calculate how the layer's input\n     /// and parameters should change to reduce errors.\n-    /// \n+    ///\n     /// During the backward pass, we reverse the steps from the forward pass:\n-    /// \n+    ///\n     /// 1. First, calculate how the activation function affects the gradient\n-    /// \n+    ///\n     /// 2. Reverse the pixel shuffling:\n     ///    - Convert the gradient from high resolution back to the lower resolution with more channels\n     ///    - This helps determine how each output channel contributed to the errors\n-    /// \n+    ///\n     /// 3. Calculate three types of gradients:\n     ///    - How the input should change (inputGradient)\n     ///    - How the kernels should change (kernelGradients)\n     ///    - How the biases should change (biasGradients)\n-    /// \n+    ///\n     /// These gradients tell the network how to adjust its parameters during the update step\n     /// to improve its performance on the next forward pass.\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclGS","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/SynapticPlasticityLayer.cs","line":444,"originalLine":444,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEJO","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.","createdAt":"2025-11-11T07:53:42Z","path":"src/NeuralNetworks/Layers/SynapticPlasticityLayer.cs","line":444,"originalLine":444,"diffHunk":"@@ -437,12 +437,43 @@ public override Tensor<T> Forward(Tensor<T> input)\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclGf","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/TemporalMemoryLayer.cs","line":407,"originalLine":407,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEJf","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.","createdAt":"2025-11-11T07:53:42Z","path":"src/NeuralNetworks/Layers/TemporalMemoryLayer.cs","line":407,"originalLine":407,"diffHunk":"@@ -400,6 +400,19 @@ public Vector<T> GetPredictions()\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclGm","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/TransformerDecoderLayer.cs","line":698,"originalLine":698,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEJq","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.","createdAt":"2025-11-11T07:53:42Z","path":"src/NeuralNetworks/Layers/TransformerDecoderLayer.cs","line":698,"originalLine":698,"diffHunk":"@@ -691,6 +691,82 @@ public Tensor<T> Forward(Tensor<T> input, Tensor<T> encoderOutput)\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclGv","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/TransformerEncoderLayer.cs","line":384,"originalLine":384,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEJ2","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.","createdAt":"2025-11-11T07:53:43Z","path":"src/NeuralNetworks/Layers/TransformerEncoderLayer.cs","line":384,"originalLine":384,"diffHunk":"@@ -356,31 +356,44 @@ public override Tensor<T> Forward(Tensor<T> input)\n     /// of the forward pass, ensuring that residual connections are properly handled.\n     /// </para>\n     /// <para><b>For Beginners:</b> This method calculates how the layer's inputs should change to reduce errors.\n-    /// \n+    ///\n     /// During the backward pass, we go through the same steps as the forward pass, but in reverse order:\n-    /// \n+    ///\n     /// 1. Final Layer Normalization:\n     ///    - Compute how the normalization's input should change based on output errors\n-    /// \n+    ///\n     /// 2. Feed-Forward Network:\n     ///    - Determine how the feed-forward network's input should change\n     ///    - Account for the residual connection by adding gradients\n-    /// \n+    ///\n     /// 3. First Layer Normalization:\n     ///    - Compute how the first normalization's input should change\n-    /// \n+    ///\n     /// 4. Self-Attention:\n     ///    - Determine how the self-attention's input should change\n     ///    - Account for the residual connection\n-    /// \n+    ///\n     /// This reverse flow of gradients allows each component to learn how it contributed to any errors.\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hclG7","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/UpsamplingLayer.cs","line":238,"originalLine":238,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VzEKF","author":{"login":"copilot-pull-request-reviewer"},"body":"Both branches of this 'if' statement return - consider using '?' to express intent better.","createdAt":"2025-11-11T07:53:43Z","path":"src/NeuralNetworks/Layers/UpsamplingLayer.cs","line":238,"originalLine":238,"diffHunk":"@@ -231,6 +231,19 @@ public override Tensor<T> Forward(Tensor<T> input)\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);"}]}},{"id":"PRRT_kwDOKSXUF85hgbL5","isResolved":true,"isOutdated":false,"path":"docs/AutodiffImplementation.md","line":7,"originalLine":7,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V4Qr3","author":{"login":"coderabbitai"},"body":"_⚠️ Potential issue_ | _🟡 Minor_\n\n**Update the last updated date.**\n\nThe date shows \"2025-01-11\" (January) but the PR was created on 2025-11-11 (November). Please update to the correct date.\n\n\n\n```diff\n-**Last Updated:** 2025-01-11\n+**Last Updated:** 2025-11-11\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>📝 Committable suggestion</summary>\n\n> ‼️ **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n**Last Updated:** 2025-11-11\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>🤖 Prompt for AI Agents</summary>\n\n```\nIn docs/AutodiffImplementation.md around line 7, the \"Last Updated\" date is\nincorrect (shows 2025-01-11); update that line to the correct date 2025-11-11 so\nthe document reflects the PR creation date.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n✅ Addressed in commits 32986e9 to 1c9b79d","createdAt":"2025-11-11T14:57:22Z","path":"docs/AutodiffImplementation.md","line":7,"originalLine":7,"diffHunk":"@@ -0,0 +1,433 @@\n+# Automatic Differentiation Implementation Status\n+\n+## Overview\n+\n+This document tracks the implementation status of automatic differentiation (autodiff) support across all neural network layers in AiDotNet.\n+\n+**Last Updated:** 2025-01-11"}]}},{"id":"PRRT_kwDOKSXUF85hgbMF","isResolved":false,"isOutdated":false,"path":"src/Autodiff/TensorOperations.cs","line":3076,"originalLine":3076,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V4QsL","author":{"login":"coderabbitai"},"body":"_⚠️ Potential issue_ | _🔴 Critical_\n\n**Fix ReduceMax index bookkeeping**\n\n`ComputeMax` reuses `outputIndices[0]` as if it were the slot index for the current output dimension, but that array stores coordinate values, not metadata. Once the first non-reduced dimension writes a coordinate into `outputIndices[0]`, the next non-reduced dimension treats that coordinate as an index, so anything beyond a single kept axis either overwrites the wrong slot or immediately throws `IndexOutOfRangeException`. The same faulty bookkeeping propagates to the gradient dictionary, so autodiff callers (e.g., `GlobalPoolingLayer` reducing over spatial axes) crash or receive garbage grads. Please rework the traversal to track the current output dimension explicitly (e.g., pass an `outDim` parameter alongside `dim`) so each kept axis writes to its own coordinate slot, and mirror that fix inside `BackwardFunction`. Also update `ReduceMean`’s helper (which follows the same pattern) while you are here.\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","createdAt":"2025-11-11T14:57:23Z","path":"src/Autodiff/TensorOperations.cs","line":3076,"originalLine":3076,"diffHunk":"@@ -1219,4 +1219,2035 @@\n \n         return node;\n     }\n+\n+    /// <summary>\n+    /// Computes the softmax function for a computation node along a specified axis.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"axis\">The axis along which to compute softmax. Default is -1 (last axis).</param>\n+    /// <returns>A new computation node containing the softmax result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method computes softmax (σ(x_i) = exp(x_i) / Σexp(x_j)) along the specified axis.\n+    /// Uses numerical stability trick: subtract max before exponentiating.\n+    /// The backward function uses: ∂softmax/∂x = softmax(x) * (grad - Σ(grad * softmax(x))).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Softmax converts a vector of numbers into probabilities.\n+    ///\n+    /// For softmax:\n+    /// - The forward pass exponentiates each element, then normalizes so they sum to 1\n+    /// - The result is a probability distribution (all values between 0 and 1, summing to 1)\n+    /// - The backward pass is complex but efficient: uses the Jacobian of softmax\n+    ///\n+    /// Softmax is crucial for:\n+    /// - Multi-class classification (final layer outputs)\n+    /// - Attention mechanisms (computing attention weights)\n+    /// - Anywhere you need to convert scores to probabilities\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Softmax(ComputationNode<T> a, int axis = -1)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        // Normalize axis to positive index\n+        if (axis < 0)\n+            axis = shape.Length + axis;\n+\n+        // For simplicity, handle 2D case (batch, features) with axis=-1\n+        if (shape.Length == 2 && axis == 1)\n+        {\n+            int batchSize = shape[0];\n+            int features = shape[1];\n+            var result = new Tensor<T>(shape);\n+\n+            // Compute softmax for each row\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                // Find max for numerical stability\n+                var maxVal = a.Value[b, 0];\n+                for (int f = 1; f < features; f++)\n+                {\n+                    if (numOps.GreaterThan(a.Value[b, f], maxVal))\n+                        maxVal = a.Value[b, f];\n+                }\n+\n+                // Compute exp(x - max) and sum\n+                var expSum = numOps.Zero;\n+                var expValues = new T[features];\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var shifted = numOps.Subtract(a.Value[b, f], maxVal);\n+                    expValues[f] = numOps.Exp(shifted);\n+                    expSum = numOps.Add(expSum, expValues[f]);\n+                }\n+\n+                // Normalize\n+                for (int f = 0; f < features; f++)\n+                {\n+                    result[b, f] = numOps.Divide(expValues[f], expSum);\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (a.RequiresGradient)\n+                {\n+                    // ∂softmax/∂x_i = softmax_i * (∂L/∂y_i - Σ_j(∂L/∂y_j * softmax_j))\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        // Compute sum of (gradient * softmax)\n+                        var dotProduct = numOps.Zero;\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            dotProduct = numOps.Add(dotProduct,\n+                                numOps.Multiply(gradient[b, f], result[b, f]));\n+                        }\n+\n+                        // Compute gradient for each element\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            var gradMinusDot = numOps.Subtract(gradient[b, f], dotProduct);\n+                            gradA[b, f] = numOps.Multiply(result[b, f], gradMinusDot);\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient,\n+                parents: new List<ComputationNode<T>> { a },\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Softmax is currently only implemented for 2D tensors along axis=-1. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}], axis={axis}\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Concatenates multiple computation nodes along a specified axis.\n+    /// </summary>\n+    /// <param name=\"nodes\">The list of nodes to concatenate.</param>\n+    /// <param name=\"axis\">The axis along which to concatenate. Default is 0.</param>\n+    /// <returns>A new computation node containing the concatenated result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method concatenates tensors along the specified axis.\n+    /// All tensors must have the same shape except along the concatenation axis.\n+    /// The backward function splits the gradient and sends each portion to the corresponding input.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Concat stacks tensors together along a dimension.\n+    ///\n+    /// For concatenation:\n+    /// - The forward pass combines multiple tensors into one larger tensor\n+    /// - The backward pass splits the gradient back to each input\n+    /// - Think of it like gluing arrays together end-to-end\n+    ///\n+    /// Used in:\n+    /// - Skip connections (concatenating features from different layers)\n+    /// - Multi-input architectures\n+    /// - Feature fusion in neural networks\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Concat(List<ComputationNode<T>> nodes, int axis = 0)\n+    {\n+        if (nodes.Count == 0)\n+            throw new ArgumentException(\"Cannot concatenate empty list of nodes\");\n+\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var firstShape = nodes[0].Value.Shape;\n+\n+        // Normalize axis\n+        if (axis < 0)\n+            axis = firstShape.Length + axis;\n+\n+        // Validate shapes match except on concat axis\n+        for (int i = 1; i < nodes.Count; i++)\n+        {\n+            var shape = nodes[i].Value.Shape;\n+            if (shape.Length != firstShape.Length)\n+                throw new ArgumentException(\"All tensors must have the same rank\");\n+\n+            for (int d = 0; d < firstShape.Length; d++)\n+            {\n+                if (d != axis && shape[d] != firstShape[d])\n+                    throw new ArgumentException(\n+                        $\"Shape mismatch at dimension {d}: {shape[d]} vs {firstShape[d]}\");\n+            }\n+        }\n+\n+        // Compute output shape\n+        int[] outputShape = (int[])firstShape.Clone();\n+        for (int i = 1; i < nodes.Count; i++)\n+        {\n+            outputShape[axis] += nodes[i].Value.Shape[axis];\n+        }\n+\n+        // Perform concatenation (handle 2D case for simplicity)\n+        Tensor<T> result;\n+        if (firstShape.Length == 2 && axis == 1)\n+        {\n+            // Concatenate along columns (features)\n+            int rows = firstShape[0];\n+            int totalCols = outputShape[1];\n+            result = new Tensor<T>(new int[] { rows, totalCols });\n+\n+            int colOffset = 0;\n+            foreach (var node in nodes)\n+            {\n+                int cols = node.Value.Shape[1];\n+                for (int r = 0; r < rows; r++)\n+                {\n+                    for (int c = 0; c < cols; c++)\n+                    {\n+                        result[r, colOffset + c] = node.Value[r, c];\n+                    }\n+                }\n+                colOffset += cols;\n+            }\n+        }\n+        else if (firstShape.Length == 2 && axis == 0)\n+        {\n+            // Concatenate along rows (batch)\n+            int cols = firstShape[1];\n+            int totalRows = outputShape[0];\n+            result = new Tensor<T>(new int[] { totalRows, cols });\n+\n+            int rowOffset = 0;\n+            foreach (var node in nodes)\n+            {\n+                int rows = node.Value.Shape[0];\n+                for (int r = 0; r < rows; r++)\n+                {\n+                    for (int c = 0; c < cols; c++)\n+                    {\n+                        result[rowOffset + r, c] = node.Value[r, c];\n+                    }\n+                }\n+                rowOffset += rows;\n+            }\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Concat is currently only implemented for 2D tensors. \" +\n+                $\"Got shape=[{string.Join(\", \", firstShape)}]\");\n+        }\n+\n+        // Store sizes for gradient splitting\n+        var sizes = nodes.Select(n => n.Value.Shape[axis]).ToList();\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            // Split gradient along concat axis and distribute to inputs\n+            if (firstShape.Length == 2 && axis == 1)\n+            {\n+                int rows = firstShape[0];\n+                int colOffset = 0;\n+                for (int i = 0; i < nodes.Count; i++)\n+                {\n+                    if (!nodes[i].RequiresGradient)\n+                    {\n+                        colOffset += sizes[i];\n+                        continue;\n+                    }\n+\n+                    int cols = sizes[i];\n+                    var gradPart = new Tensor<T>(new int[] { rows, cols });\n+\n+                    for (int r = 0; r < rows; r++)\n+                    {\n+                        for (int c = 0; c < cols; c++)\n+                        {\n+                            gradPart[r, c] = gradient[r, colOffset + c];\n+                        }\n+                    }\n+\n+                    if (nodes[i].Gradient == null)\n+                        nodes[i].Gradient = gradPart;\n+                    else\n+                        nodes[i].Gradient = nodes[i].Gradient.Add(gradPart);\n+\n+                    colOffset += cols;\n+                }\n+            }\n+            else if (firstShape.Length == 2 && axis == 0)\n+            {\n+                int cols = firstShape[1];\n+                int rowOffset = 0;\n+                for (int i = 0; i < nodes.Count; i++)\n+                {\n+                    if (!nodes[i].RequiresGradient)\n+                    {\n+                        rowOffset += sizes[i];\n+                        continue;\n+                    }\n+\n+                    int rows = sizes[i];\n+                    var gradPart = new Tensor<T>(new int[] { rows, cols });\n+\n+                    for (int r = 0; r < rows; r++)\n+                    {\n+                        for (int c = 0; c < cols; c++)\n+                        {\n+                            gradPart[r, c] = gradient[rowOffset + r, c];\n+                        }\n+                    }\n+\n+                    if (nodes[i].Gradient == null)\n+                        nodes[i].Gradient = gradPart;\n+                    else\n+                        nodes[i].Gradient = nodes[i].Gradient.Add(gradPart);\n+\n+                    rowOffset += rows;\n+                }\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: nodes.Any(n => n.RequiresGradient),\n+            parents: nodes,\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Pads a tensor with a constant value along specified dimensions.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"padWidth\">Padding width for each dimension as (before, after) pairs.</param>\n+    /// <param name=\"value\">The value to use for padding. Default is zero.</param>\n+    /// <returns>A new computation node containing the padded result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method adds padding around the tensor.\n+    /// The backward function simply crops the gradient back to the original size (gradients for padding are zero).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Pad adds extra elements around a tensor.\n+    ///\n+    /// For padding:\n+    /// - The forward pass adds border elements with a constant value\n+    /// - The backward pass removes those border gradients (they don't affect the original tensor)\n+    /// - Think of it like adding margins to an image\n+    ///\n+    /// Used in:\n+    /// - Convolutional layers (to maintain spatial dimensions)\n+    /// - Handling variable-length sequences\n+    /// - Data augmentation\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Pad(ComputationNode<T> a, int[,] padWidth, T? value = default)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var padValue = value ?? numOps.Zero;\n+        var shape = a.Value.Shape;\n+\n+        // Validate padWidth dimensions\n+        if (padWidth.GetLength(0) != shape.Length)\n+            throw new ArgumentException(\"padWidth must have same number of dimensions as input tensor\");\n+\n+        // Compute output shape\n+        var outputShape = new int[shape.Length];\n+        for (int d = 0; d < shape.Length; d++)\n+        {\n+            outputShape[d] = shape[d] + padWidth[d, 0] + padWidth[d, 1];\n+        }\n+\n+        // Handle 2D case\n+        if (shape.Length == 2)\n+        {\n+            int inputRows = shape[0];\n+            int inputCols = shape[1];\n+            int padTop = padWidth[0, 0];\n+            int padBottom = padWidth[0, 1];\n+            int padLeft = padWidth[1, 0];\n+            int padRight = padWidth[1, 1];\n+\n+            var result = new Tensor<T>(outputShape);\n+\n+            // Initialize with pad value\n+            for (int i = 0; i < result.Length; i++)\n+            {\n+                result[i] = padValue;\n+            }\n+\n+            // Copy input data to center\n+            for (int r = 0; r < inputRows; r++)\n+            {\n+                for (int c = 0; c < inputCols; c++)\n+                {\n+                    result[padTop + r, padLeft + c] = a.Value[r, c];\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (a.RequiresGradient)\n+                {\n+                    // Extract gradient for original (unpadded) region\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int r = 0; r < inputRows; r++)\n+                    {\n+                        for (int c = 0; c < inputCols; c++)\n+                        {\n+                            gradA[r, c] = gradient[padTop + r, padLeft + c];\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient,\n+                parents: new List<ComputationNode<T>> { a },\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Pad is currently only implemented for 2D tensors. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}]\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs 2D max pooling on a 4D tensor (batch, channels, height, width).\n+    /// </summary>\n+    /// <param name=\"a\">The input node with shape [batch, channels, height, width].</param>\n+    /// <param name=\"poolSize\">The size of the pooling window [poolH, poolW].</param>\n+    /// <param name=\"strides\">The stride for the pooling operation [strideH, strideW]. If null, uses poolSize.</param>\n+    /// <returns>A new computation node containing the max pooled result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method performs max pooling over 2D spatial dimensions.\n+    /// During forward pass, it tracks which element was the max for routing gradients during backward pass.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> MaxPool downsamples by taking the maximum value in each window.\n+    ///\n+    /// For max pooling:\n+    /// - The forward pass slides a window and takes the max value in each position\n+    /// - This reduces spatial dimensions (downsampling)\n+    /// - The backward pass routes gradients only to the positions that were max\n+    /// - Other positions get zero gradient (they didn't contribute to the output)\n+    ///\n+    /// Used in:\n+    /// - CNNs for translation invariance\n+    /// - Reducing spatial resolution\n+    /// - Building hierarchical features\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> MaxPool2D(\n+        ComputationNode<T> a,\n+        int[] poolSize,\n+        int[]? strides = null)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        if (shape.Length != 4)\n+            throw new ArgumentException(\"MaxPool2D requires 4D input [batch, channels, height, width]\");\n+\n+        strides ??= poolSize;\n+\n+        int batch = shape[0];\n+        int channels = shape[1];\n+        int inH = shape[2];\n+        int inW = shape[3];\n+        int poolH = poolSize[0];\n+        int poolW = poolSize[1];\n+        int strideH = strides[0];\n+        int strideW = strides[1];\n+\n+        int outH = (inH - poolH) / strideH + 1;\n+        int outW = (inW - poolW) / strideW + 1;\n+\n+        var result = new Tensor<T>(new int[] { batch, channels, outH, outW });\n+        // Store max positions for backprop\n+        var maxPositions = new int[batch, channels, outH, outW, 2]; // [h_offset, w_offset]\n+\n+        // Forward pass: compute max pooling and track positions\n+        for (int b = 0; b < batch; b++)\n+        {\n+            for (int c = 0; c < channels; c++)\n+            {\n+                for (int oh = 0; oh < outH; oh++)\n+                {\n+                    for (int ow = 0; ow < outW; ow++)\n+                    {\n+                        int hStart = oh * strideH;\n+                        int wStart = ow * strideW;\n+\n+                        var maxVal = a.Value[b * channels * inH * inW +\n+                                           c * inH * inW +\n+                                           hStart * inW +\n+                                           wStart];\n+                        int maxHOffset = 0;\n+                        int maxWOffset = 0;\n+\n+                        // Find max in pooling window\n+                        for (int ph = 0; ph < poolH; ph++)\n+                        {\n+                            for (int pw = 0; pw < poolW; pw++)\n+                            {\n+                                int h = hStart + ph;\n+                                int w = wStart + pw;\n+                                if (h < inH && w < inW)\n+                                {\n+                                    var val = a.Value[b * channels * inH * inW +\n+                                                     c * inH * inW +\n+                                                     h * inW +\n+                                                     w];\n+                                    if (numOps.GreaterThan(val, maxVal))\n+                                    {\n+                                        maxVal = val;\n+                                        maxHOffset = ph;\n+                                        maxWOffset = pw;\n+                                    }\n+                                }\n+                            }\n+                        }\n+\n+                        result[b, c, oh, ow] = maxVal;\n+                        maxPositions[b, c, oh, ow, 0] = maxHOffset;\n+                        maxPositions[b, c, oh, ow, 1] = maxWOffset;\n+                    }\n+                }\n+            }\n+        }\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            if (a.RequiresGradient)\n+            {\n+                var gradA = new Tensor<T>(shape);\n+\n+                // Route gradients to max positions\n+                for (int b = 0; b < batch; b++)\n+                {\n+                    for (int c = 0; c < channels; c++)\n+                    {\n+                        for (int oh = 0; oh < outH; oh++)\n+                        {\n+                            for (int ow = 0; ow < outW; ow++)\n+                            {\n+                                int hStart = oh * strideH;\n+                                int wStart = ow * strideW;\n+                                int maxHOffset = maxPositions[b, c, oh, ow, 0];\n+                                int maxWOffset = maxPositions[b, c, oh, ow, 1];\n+\n+                                int maxH = hStart + maxHOffset;\n+                                int maxW = wStart + maxWOffset;\n+\n+                                int gradIdx = b * channels * inH * inW +\n+                                             c * inH * inW +\n+                                            maxH * inW +\n+                                             maxW;\n+\n+                                gradA[gradIdx] = numOps.Add(gradA[gradIdx], gradient[b, c, oh, ow]);\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (a.Gradient == null)\n+                    a.Gradient = gradA;\n+                else\n+                    a.Gradient = a.Gradient.Add(gradA);\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: a.RequiresGradient,\n+            parents: new List<ComputationNode<T>> { a },\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Performs 2D average pooling on a 4D tensor (batch, channels, height, width).\n+    /// </summary>\n+    /// <param name=\"a\">The input node with shape [batch, channels, height, width].</param>\n+    /// <param name=\"poolSize\">The size of the pooling window [poolH, poolW].</param>\n+    /// <param name=\"strides\">The stride for the pooling operation [strideH, strideW]. If null, uses poolSize.</param>\n+    /// <returns>A new computation node containing the average pooled result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method performs average pooling over 2D spatial dimensions.\n+    /// The backward function distributes gradients equally across the pooling window.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> AvgPool downsamples by taking the average value in each window.\n+    ///\n+    /// For average pooling:\n+    /// - The forward pass slides a window and computes the average\n+    /// - This smoothly reduces spatial dimensions\n+    /// - The backward pass distributes gradients equally to all elements in the window\n+    /// - Each element gets gradient / pool_area\n+    ///\n+    /// Used in:\n+    /// - CNNs for smoother downsampling than max pooling\n+    /// - Global average pooling (replacing fully connected layers)\n+    /// - Reducing overfitting\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> AvgPool2D(\n+        ComputationNode<T> a,\n+        int[] poolSize,\n+        int[]? strides = null)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        if (shape.Length != 4)\n+            throw new ArgumentException(\"AvgPool2D requires 4D input [batch, channels, height, width]\");\n+\n+        strides ??= poolSize;\n+\n+        int batch = shape[0];\n+        int channels = shape[1];\n+        int inH = shape[2];\n+        int inW = shape[3];\n+        int poolH = poolSize[0];\n+        int poolW = poolSize[1];\n+        int strideH = strides[0];\n+        int strideW = strides[1];\n+\n+        int outH = (inH - poolH) / strideH + 1;\n+        int outW = (inW - poolW) / strideW + 1;\n+\n+        var result = new Tensor<T>(new int[] { batch, channels, outH, outW });\n+        var poolArea = numOps.FromDouble(poolH * poolW);\n+\n+        // Forward pass: compute average pooling\n+        for (int b = 0; b < batch; b++)\n+        {\n+            for (int c = 0; c < channels; c++)\n+            {\n+                for (int oh = 0; oh < outH; oh++)\n+                {\n+                    for (int ow = 0; ow < outW; ow++)\n+                    {\n+                        int hStart = oh * strideH;\n+                        int wStart = ow * strideW;\n+\n+                        var sum = numOps.Zero;\n+\n+                        // Sum values in pooling window\n+                        for (int ph = 0; ph < poolH; ph++)\n+                        {\n+                            for (int pw = 0; pw < poolW; pw++)\n+                            {\n+                                int h = hStart + ph;\n+                                int w = wStart + pw;\n+                                if (h < inH && w < inW)\n+                                {\n+                                    sum = numOps.Add(sum, a.Value[b, c, h, w]);\n+                                }\n+                            }\n+                        }\n+\n+                        result[b, c, oh, ow] = numOps.Divide(sum, poolArea);\n+                    }\n+                }\n+            }\n+        }\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            if (a.RequiresGradient)\n+            {\n+                var gradA = new Tensor<T>(shape);\n+\n+                // Distribute gradients equally across pooling windows\n+                for (int b = 0; b < batch; b++)\n+                {\n+                    for (int c = 0; c < channels; c++)\n+                    {\n+                        for (int oh = 0; oh < outH; oh++)\n+                        {\n+                            for (int ow = 0; ow < outW; ow++)\n+                            {\n+                                int hStart = oh * strideH;\n+                                int wStart = ow * strideW;\n+\n+                                var gradValue = numOps.Divide(gradient[b, c, oh, ow], poolArea);\n+\n+                                // Distribute to all elements in window\n+                                for (int ph = 0; ph < poolH; ph++)\n+                                {\n+                                    for (int pw = 0; pw < poolW; pw++)\n+                                    {\n+                                        int h = hStart + ph;\n+                                        int w = wStart + pw;\n+                                        if (h < inH && w < inW)\n+                                        {\n+                                            gradA[b, c, h, w] = numOps.Add(gradA[b, c, h, w], gradValue);\n+                                        }\n+                                    }\n+                                }\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (a.Gradient == null)\n+                    a.Gradient = gradA;\n+                else\n+                    a.Gradient = a.Gradient.Add(gradA);\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: a.RequiresGradient,\n+            parents: new List<ComputationNode<T>> { a },\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Applies layer normalization to a computation node.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"normalizedShape\">The shape over which to normalize (typically the feature dimensions).</param>\n+    /// <param name=\"gamma\">Optional scale parameter (learnable). If null, uses ones.</param>\n+    /// <param name=\"beta\">Optional shift parameter (learnable). If null, uses zeros.</param>\n+    /// <param name=\"epsilon\">Small constant for numerical stability. Default is 1e-5.</param>\n+    /// <returns>A new computation node containing the layer normalized result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Layer normalization normalizes inputs across the feature dimension for each sample independently.\n+    /// Formula: y = gamma * (x - mean) / sqrt(variance + epsilon) + beta\n+    /// Unlike batch normalization, this doesn't depend on batch statistics.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> LayerNorm standardizes features for each sample independently.\n+    ///\n+    /// For layer normalization:\n+    /// - Computes mean and variance for each sample's features\n+    /// - Normalizes: (x - mean) / sqrt(variance)\n+    /// - Scales and shifts: result * gamma + beta\n+    /// - Works the same during training and inference (no batch dependency)\n+    ///\n+    /// Used in:\n+    /// - Transformers (critical component)\n+    /// - RNNs (stabilizes training)\n+    /// - Any architecture needing sample-independent normalization\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> LayerNorm(\n+        ComputationNode<T> a,\n+        int[] normalizedShape,\n+        ComputationNode<T>? gamma = null,\n+        ComputationNode<T>? beta = null,\n+        double epsilon = 1e-5)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+        var eps = numOps.FromDouble(epsilon);\n+\n+        // For 2D input [batch, features], normalize over features\n+        if (shape.Length == 2 && normalizedShape.Length == 1 && normalizedShape[0] == shape[1])\n+        {\n+            int batchSize = shape[0];\n+            int features = shape[1];\n+\n+            // Create default gamma (ones) and beta (zeros) if not provided\n+            if (gamma == null)\n+            {\n+                var gammaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    gammaTensor[i] = numOps.One;\n+                gamma = Variable(gammaTensor, requiresGradient: false);\n+            }\n+\n+            if (beta == null)\n+            {\n+                var betaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    betaTensor[i] = numOps.Zero;\n+                beta = Variable(betaTensor, requiresGradient: false);\n+            }\n+\n+            var result = new Tensor<T>(shape);\n+            var means = new T[batchSize];\n+            var variances = new T[batchSize];\n+            var normalized = new Tensor<T>(shape);\n+\n+            // Forward pass: compute mean and variance per sample\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                // Compute mean\n+                var sum = numOps.Zero;\n+                for (int f = 0; f < features; f++)\n+                {\n+                    sum = numOps.Add(sum, a.Value[b, f]);\n+                }\n+                means[b] = numOps.Divide(sum, numOps.FromDouble(features));\n+\n+                // Compute variance\n+                var varSum = numOps.Zero;\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var diff = numOps.Subtract(a.Value[b, f], means[b]);\n+                    varSum = numOps.Add(varSum, numOps.Multiply(diff, diff));\n+                }\n+                variances[b] = numOps.Divide(varSum, numOps.FromDouble(features));\n+\n+                // Normalize and scale\n+                var std = numOps.Sqrt(numOps.Add(variances[b], eps));\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var norm = numOps.Divide(\n+                        numOps.Subtract(a.Value[b, f], means[b]),\n+                        std);\n+                    normalized[b, f] = norm;\n+                    result[b, f] = numOps.Add(\n+                        numOps.Multiply(norm, gamma.Value[f]),\n+                        beta.Value[f]);\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                // Gradients for gamma and beta\n+                if (gamma.RequiresGradient)\n+                {\n+                    var gradGamma = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum,\n+                                numOps.Multiply(gradient[b, f], normalized[b, f]));\n+                        }\n+                        gradGamma[f] = sum;\n+                    }\n+\n+                    if (gamma.Gradient == null)\n+                        gamma.Gradient = gradGamma;\n+                    else\n+                        gamma.Gradient = gamma.Gradient.Add(gradGamma);\n+                }\n+\n+                if (beta.RequiresGradient)\n+                {\n+                    var gradBeta = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum, gradient[b, f]);\n+                        }\n+                        gradBeta[f] = sum;\n+                    }\n+\n+                    if (beta.Gradient == null)\n+                        beta.Gradient = gradBeta;\n+                    else\n+                        beta.Gradient = beta.Gradient.Add(gradBeta);\n+                }\n+\n+                // Gradient for input\n+                if (a.RequiresGradient)\n+                {\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        var std = numOps.Sqrt(numOps.Add(variances[b], eps));\n+                        var invStd = numOps.Divide(numOps.One, std);\n+\n+                        // Compute gradient components\n+                        var gradNormSum = numOps.Zero;\n+                        var gradNormDotNorm = numOps.Zero;\n+\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            var gradNorm = numOps.Multiply(gradient[b, f], gamma.Value[f]);\n+                            gradNormSum = numOps.Add(gradNormSum, gradNorm);\n+                            gradNormDotNorm = numOps.Add(gradNormDotNorm,\n+                                numOps.Multiply(gradNorm, normalized[b, f]));\n+                        }\n+\n+                        // Apply gradient formula\n+                        var featuresT = numOps.FromDouble(features);\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            var gradNorm = numOps.Multiply(gradient[b, f], gamma.Value[f]);\n+\n+                            var term1 = gradNorm;\n+                            var term2 = numOps.Divide(gradNormSum, featuresT);\n+                            var term3 = numOps.Divide(\n+                                numOps.Multiply(normalized[b, f], gradNormDotNorm),\n+                                featuresT);\n+\n+                            var grad = numOps.Multiply(\n+                                numOps.Subtract(numOps.Subtract(term1, term2), term3),\n+                                invStd);\n+\n+                            gradA[b, f] = grad;\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var parents = new List<ComputationNode<T>> { a };\n+            if (gamma != null) parents.Add(gamma);\n+            if (beta != null) parents.Add(beta);\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient || (gamma?.RequiresGradient ?? false) || (beta?.RequiresGradient ?? false),\n+                parents: parents,\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"LayerNorm is currently only implemented for 2D tensors normalizing over last dimension. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}], normalizedShape=[{string.Join(\", \", normalizedShape)}]\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Applies batch normalization to a computation node.\n+    /// </summary>\n+    /// <param name=\"a\">The input node with shape [batch, features].</param>\n+    /// <param name=\"gamma\">Optional scale parameter (learnable). If null, uses ones.</param>\n+    /// <param name=\"beta\">Optional shift parameter (learnable). If null, uses zeros.</param>\n+    /// <param name=\"runningMean\">Running mean for inference (not updated during this operation).</param>\n+    /// <param name=\"runningVar\">Running variance for inference (not updated during this operation).</param>\n+    /// <param name=\"training\">Whether in training mode (uses batch statistics) or inference mode (uses running statistics).</param>\n+    /// <param name=\"epsilon\">Small constant for numerical stability. Default is 1e-5.</param>\n+    /// <returns>A new computation node containing the batch normalized result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Batch normalization normalizes inputs across the batch dimension.\n+    /// During training: Uses batch statistics (mean and variance computed from current batch).\n+    /// During inference: Uses running statistics (accumulated during training).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> BatchNorm standardizes features across the batch.\n+    ///\n+    /// For batch normalization:\n+    /// - Training mode: Uses current batch's mean and variance\n+    /// - Inference mode: Uses running mean/variance from training\n+    /// - Normalizes: (x - mean) / sqrt(variance)\n+    /// - Scales and shifts: result * gamma + beta\n+    ///\n+    /// Benefits:\n+    /// - Stabilizes training (reduces internal covariate shift)\n+    /// - Allows higher learning rates\n+    /// - Acts as regularization\n+    ///\n+    /// Used in:\n+    /// - CNNs (after convolutional layers)\n+    /// - Deep feedforward networks\n+    /// - GANs and many other architectures\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> BatchNorm(\n+        ComputationNode<T> a,\n+        ComputationNode<T>? gamma = null,\n+        ComputationNode<T>? beta = null,\n+        Tensor<T>? runningMean = null,\n+        Tensor<T>? runningVar = null,\n+        bool training = true,\n+        double epsilon = 1e-5)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+        var eps = numOps.FromDouble(epsilon);\n+\n+        // Handle 2D case [batch, features]\n+        if (shape.Length == 2)\n+        {\n+            int batchSize = shape[0];\n+            int features = shape[1];\n+\n+            // Create default gamma and beta if not provided\n+            if (gamma == null)\n+            {\n+                var gammaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    gammaTensor[i] = numOps.One;\n+                gamma = Variable(gammaTensor, requiresGradient: false);\n+            }\n+\n+            if (beta == null)\n+            {\n+                var betaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    betaTensor[i] = numOps.Zero;\n+                beta = Variable(betaTensor, requiresGradient: false);\n+            }\n+\n+            var result = new Tensor<T>(shape);\n+            T[] batchMean;\n+            T[] batchVar;\n+            var normalized = new Tensor<T>(shape);\n+\n+            if (training)\n+            {\n+                // Compute batch statistics\n+                batchMean = new T[features];\n+                batchVar = new T[features];\n+\n+                // Compute mean per feature\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var sum = numOps.Zero;\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        sum = numOps.Add(sum, a.Value[b, f]);\n+                    }\n+                    batchMean[f] = numOps.Divide(sum, numOps.FromDouble(batchSize));\n+                }\n+\n+                // Compute variance per feature\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var varSum = numOps.Zero;\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        var diff = numOps.Subtract(a.Value[b, f], batchMean[f]);\n+                        varSum = numOps.Add(varSum, numOps.Multiply(diff, diff));\n+                    }\n+                    batchVar[f] = numOps.Divide(varSum, numOps.FromDouble(batchSize));\n+                }\n+            }\n+            else\n+            {\n+                // Use running statistics for inference\n+                if (runningMean == null || runningVar == null)\n+                    throw new ArgumentException(\"Running statistics required for inference mode\");\n+\n+                batchMean = new T[features];\n+                batchVar = new T[features];\n+                for (int f = 0; f < features; f++)\n+                {\n+                    batchMean[f] = runningMean[f];\n+                    batchVar[f] = runningVar[f];\n+                }\n+            }\n+\n+            // Normalize and scale\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var std = numOps.Sqrt(numOps.Add(batchVar[f], eps));\n+                    var norm = numOps.Divide(\n+                        numOps.Subtract(a.Value[b, f], batchMean[f]),\n+                        std);\n+                    normalized[b, f] = norm;\n+                    result[b, f] = numOps.Add(\n+                        numOps.Multiply(norm, gamma.Value[f]),\n+                        beta.Value[f]);\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (!training)\n+                {\n+                    // Inference mode: simpler gradient (no batch statistics gradient)\n+                    if (a.RequiresGradient)\n+                    {\n+                        var gradA = new Tensor<T>(shape);\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            for (int f = 0; f < features; f++)\n+                            {\n+                                var std = numOps.Sqrt(numOps.Add(batchVar[f], eps));\n+                                var invStd = numOps.Divide(numOps.One, std);\n+                                gradA[b, f] = numOps.Multiply(\n+                                    numOps.Multiply(gradient[b, f], gamma.Value[f]),\n+                                    invStd);\n+                            }\n+                        }\n+\n+                        if (a.Gradient == null)\n+                            a.Gradient = gradA;\n+                        else\n+                            a.Gradient = a.Gradient.Add(gradA);\n+                    }\n+                    return;\n+                }\n+\n+                // Training mode: full gradient computation\n+                // Gradients for gamma and beta\n+                if (gamma.RequiresGradient)\n+                {\n+                    var gradGamma = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum,\n+                                numOps.Multiply(gradient[b, f], normalized[b, f]));\n+                        }\n+                        gradGamma[f] = sum;\n+                    }\n+\n+                    if (gamma.Gradient == null)\n+                        gamma.Gradient = gradGamma;\n+                    else\n+                        gamma.Gradient = gamma.Gradient.Add(gradGamma);\n+                }\n+\n+                if (beta.RequiresGradient)\n+                {\n+                    var gradBeta = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum, gradient[b, f]);\n+                        }\n+                        gradBeta[f] = sum;\n+                    }\n+\n+                    if (beta.Gradient == null)\n+                        beta.Gradient = gradBeta;\n+                    else\n+                        beta.Gradient = beta.Gradient.Add(gradBeta);\n+                }\n+\n+                // Gradient for input (complex due to batch statistics)\n+                if (a.RequiresGradient)\n+                {\n+                    var gradA = new Tensor<T>(shape);\n+                    var batchSizeT = numOps.FromDouble(batchSize);\n+\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var std = numOps.Sqrt(numOps.Add(batchVar[f], eps));\n+                        var invStd = numOps.Divide(numOps.One, std);\n+\n+                        // Sum of gradients and gradient*normalized\n+                        var gradSum = numOps.Zero;\n+                        var gradNormSum = numOps.Zero;\n+\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            var grad = numOps.Multiply(gradient[b, f], gamma.Value[f]);\n+                            gradSum = numOps.Add(gradSum, grad);\n+                            gradNormSum = numOps.Add(gradNormSum,\n+                                numOps.Multiply(grad, normalized[b, f]));\n+                        }\n+\n+                        // Apply gradient formula\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            var grad = numOps.Multiply(gradient[b, f], gamma.Value[f]);\n+\n+                            var term1 = grad;\n+                            var term2 = numOps.Divide(gradSum, batchSizeT);\n+                            var term3 = numOps.Divide(\n+                                numOps.Multiply(normalized[b, f], gradNormSum),\n+                                batchSizeT);\n+\n+                            var gradInput = numOps.Multiply(\n+                                numOps.Subtract(numOps.Subtract(term1, term2), term3),\n+                                invStd);\n+\n+                            gradA[b, f] = gradInput;\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var parents = new List<ComputationNode<T>> { a };\n+            if (gamma != null) parents.Add(gamma);\n+            if (beta != null) parents.Add(beta);\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient || (gamma?.RequiresGradient ?? false) || (beta?.RequiresGradient ?? false),\n+                parents: parents,\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"BatchNorm is currently only implemented for 2D tensors [batch, features]. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}]\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs 2D convolution on a 4D tensor (batch, channels, height, width).\n+    /// </summary>\n+    /// <param name=\"input\">The input node with shape [batch, inChannels, height, width].</param>\n+    /// <param name=\"kernel\">The kernel/filter with shape [outChannels, inChannels, kernelH, kernelW].</param>\n+    /// <param name=\"bias\">Optional bias with shape [outChannels]. If null, no bias is added.</param>\n+    /// <param name=\"stride\">The stride [strideH, strideW]. Default is [1, 1].</param>\n+    /// <param name=\"padding\">The padding [padH, padW]. Default is [0, 0].</param>\n+    /// <returns>A new computation node containing the convolution result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method performs 2D convolution, the fundamental operation in CNNs.\n+    /// Forward: Slides the kernel over the input computing dot products.\n+    /// Backward: Computes gradients for both input and kernel using transposed convolutions.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Conv2D is the core operation of convolutional neural networks.\n+    ///\n+    /// For 2D convolution:\n+    /// - The kernel \"slides\" over the input, computing weighted sums\n+    /// - Each output position is a dot product of the kernel with input patch\n+    /// - Stride controls how far the kernel moves each step\n+    /// - Padding adds borders to control output size\n+    ///\n+    /// Gradient computation:\n+    /// - Gradient w.r.t. input: \"full\" convolution with flipped kernel\n+    /// - Gradient w.r.t. kernel: cross-correlation between input and output gradient\n+    ///\n+    /// Used in:\n+    /// - All CNNs (image classification, object detection, segmentation)\n+    /// - Feature extraction in vision models\n+    /// - Learning spatial hierarchies\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Conv2D(\n+        ComputationNode<T> input,\n+        ComputationNode<T> kernel,\n+        ComputationNode<T>? bias = null,\n+        int[]? stride = null,\n+        int[]? padding = null)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var inputShape = input.Value.Shape;\n+        var kernelShape = kernel.Value.Shape;\n+\n+        if (inputShape.Length != 4)\n+            throw new ArgumentException(\"Conv2D requires 4D input [batch, inChannels, height, width]\");\n+        if (kernelShape.Length != 4)\n+            throw new ArgumentException(\"Conv2D requires 4D kernel [outChannels, inChannels, kernelH, kernelW]\");\n+\n+        stride ??= new int[] { 1, 1 };\n+        padding ??= new int[] { 0, 0 };\n+\n+        int batch = inputShape[0];\n+        int inChannels = inputShape[1];\n+        int inH = inputShape[2];\n+        int inW = inputShape[3];\n+\n+        int outChannels = kernelShape[0];\n+        int kernelInChannels = kernelShape[1];\n+        int kernelH = kernelShape[2];\n+        int kernelW = kernelShape[3];\n+\n+        if (inChannels != kernelInChannels)\n+            throw new ArgumentException($\"Input channels ({inChannels}) must match kernel input channels ({kernelInChannels})\");\n+\n+        int strideH = stride[0];\n+        int strideW = stride[1];\n+        int padH = padding[0];\n+        int padW = padding[1];\n+\n+        int outH = (inH + 2 * padH - kernelH) / strideH + 1;\n+        int outW = (inW + 2 * padW - kernelW) / strideW + 1;\n+\n+        var result = new Tensor<T>(new int[] { batch, outChannels, outH, outW });\n+\n+        // Forward pass: convolution\n+        for (int b = 0; b < batch; b++)\n+        {\n+            for (int oc = 0; oc < outChannels; oc++)\n+            {\n+                for (int oh = 0; oh < outH; oh++)\n+                {\n+                    for (int ow = 0; ow < outW; ow++)\n+                    {\n+                        var sum = numOps.Zero;\n+\n+                        // Convolve kernel over input\n+                        for (int ic = 0; ic < inChannels; ic++)\n+                        {\n+                            for (int kh = 0; kh < kernelH; kh++)\n+                            {\n+                                for (int kw = 0; kw < kernelW; kw++)\n+                                {\n+                                    int ih = oh * strideH + kh - padH;\n+                                    int iw = ow * strideW + kw - padW;\n+\n+                                    // Check bounds (padding)\n+                                    if (ih >= 0 && ih < inH && iw >= 0 && iw < inW)\n+                                    {\n+                                        var inputVal = input.Value[b, ic, ih, iw];\n+                                        var kernelVal = kernel.Value[oc, ic, kh, kw];\n+                                        sum = numOps.Add(sum, numOps.Multiply(inputVal, kernelVal));\n+                                    }\n+                                }\n+                            }\n+                        }\n+\n+                        // Add bias if provided\n+                        if (bias != null)\n+                        {\n+                            sum = numOps.Add(sum, bias.Value[oc]);\n+                        }\n+\n+                        result[b, oc, oh, ow] = sum;\n+                    }\n+                }\n+            }\n+        }\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            // Gradient w.r.t. input\n+            if (input.RequiresGradient)\n+            {\n+                var gradInput = new Tensor<T>(inputShape);\n+\n+                // Full convolution with flipped kernel\n+                for (int b = 0; b < batch; b++)\n+                {\n+                    for (int ic = 0; ic < inChannels; ic++)\n+                    {\n+                        for (int ih = 0; ih < inH; ih++)\n+                        {\n+                            for (int iw = 0; iw < inW; iw++)\n+                            {\n+                                var sum = numOps.Zero;\n+\n+                                // Iterate over all output positions that used this input position\n+                                for (int oc = 0; oc < outChannels; oc++)\n+                                {\n+                                    for (int kh = 0; kh < kernelH; kh++)\n+                                    {\n+                                        for (int kw = 0; kw < kernelW; kw++)\n+                                        {\n+                                            // Compute output position\n+                                            int ohShifted = ih + padH - kh;\n+                                            int owShifted = iw + padW - kw;\n+\n+                                            if (ohShifted % strideH == 0 && owShifted % strideW == 0)\n+                                            {\n+                                                int oh = ohShifted / strideH;\n+                                                int ow = owShifted / strideW;\n+\n+                                                if (oh >= 0 && oh < outH && ow >= 0 && ow < outW)\n+                                                {\n+                                                    var gradVal = gradient[b, oc, oh, ow];\n+                                                    var kernelVal = kernel.Value[oc, ic, kh, kw];\n+                                                    sum = numOps.Add(sum, numOps.Multiply(gradVal, kernelVal));\n+                                                }\n+                                            }\n+                                        }\n+                                    }\n+                                }\n+\n+                                gradInput[b, ic, ih, iw] = sum;\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (input.Gradient == null)\n+                    input.Gradient = gradInput;\n+                else\n+                    input.Gradient = input.Gradient.Add(gradInput);\n+            }\n+\n+            // Gradient w.r.t. kernel\n+            if (kernel.RequiresGradient)\n+            {\n+                var gradKernel = new Tensor<T>(kernelShape);\n+\n+                // Cross-correlation between input and output gradient\n+                for (int oc = 0; oc < outChannels; oc++)\n+                {\n+                    for (int ic = 0; ic < inChannels; ic++)\n+                    {\n+                        for (int kh = 0; kh < kernelH; kh++)\n+                        {\n+                            for (int kw = 0; kw < kernelW; kw++)\n+                            {\n+                                var sum = numOps.Zero;\n+\n+                                for (int b = 0; b < batch; b++)\n+                                {\n+                                    for (int oh = 0; oh < outH; oh++)\n+                                    {\n+                                        for (int ow = 0; ow < outW; ow++)\n+                                        {\n+                                            int ih = oh * strideH + kh - padH;\n+                                            int iw = ow * strideW + kw - padW;\n+\n+                                            if (ih >= 0 && ih < inH && iw >= 0 && iw < inW)\n+                                            {\n+                                                var gradVal = gradient[b, oc, oh, ow];\n+                                                var inputVal = input.Value[b, ic, ih, iw];\n+                                                sum = numOps.Add(sum, numOps.Multiply(gradVal, inputVal));\n+                                            }\n+                                        }\n+                                    }\n+                                }\n+\n+                                gradKernel[oc, ic, kh, kw] = sum;\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (kernel.Gradient == null)\n+                    kernel.Gradient = gradKernel;\n+                else\n+                    kernel.Gradient = kernel.Gradient.Add(gradKernel);\n+            }\n+\n+            // Gradient w.r.t. bias\n+            if (bias != null && bias.RequiresGradient)\n+            {\n+                var gradBias = new Tensor<T>(new int[] { outChannels });\n+\n+                for (int oc = 0; oc < outChannels; oc++)\n+                {\n+                    var sum = numOps.Zero;\n+                    for (int b = 0; b < batch; b++)\n+                    {\n+                        for (int oh = 0; oh < outH; oh++)\n+                        {\n+                            for (int ow = 0; ow < outW; ow++)\n+                            {\n+                                sum = numOps.Add(sum, gradient[b, oc, oh, ow]);\n+                            }\n+                        }\n+                    }\n+                    gradBias[oc] = sum;\n+                }\n+\n+                if (bias.Gradient == null)\n+                    bias.Gradient = gradBias;\n+                else\n+                    bias.Gradient = bias.Gradient.Add(gradBias);\n+            }\n+        }\n+\n+        var parents = new List<ComputationNode<T>> { input, kernel };\n+        if (bias != null) parents.Add(bias);\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: input.RequiresGradient || kernel.RequiresGradient || (bias?.RequiresGradient ?? false),\n+            parents: parents,\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Performs 2D transposed convolution (deconvolution) on a 4D tensor.\n+    /// </summary>\n+    /// <param name=\"input\">The input node with shape [batch, inChannels, height, width].</param>\n+    /// <param name=\"kernel\">The kernel with shape [inChannels, outChannels, kernelH, kernelW] (note: reversed from Conv2D).</param>\n+    /// <param name=\"bias\">Optional bias with shape [outChannels]. If null, no bias is added.</param>\n+    /// <param name=\"stride\">The stride [strideH, strideW]. Default is [1, 1].</param>\n+    /// <param name=\"padding\">The padding [padH, padW]. Default is [0, 0].</param>\n+    /// <param name=\"outputPadding\">Output padding [outPadH, outPadW] for size adjustment. Default is [0, 0].</param>\n+    /// <returns>A new computation node containing the transposed convolution result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Transposed convolution (often called deconvolution) upsamples the input.\n+    /// It's the gradient of Conv2D with respect to its input, used as a forward operation.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> ConvTranspose2D upsamples spatial dimensions.\n+    ///\n+    /// For transposed convolution:\n+    /// - Inserts zeros between input elements according to stride\n+    /// - Applies regular convolution to the expanded input\n+    /// - Results in larger spatial dimensions (upsampling)\n+    ///\n+    /// Used in:\n+    /// - Image generation (GANs, VAEs)\n+    /// - Semantic segmentation (U-Net decoder)\n+    /// - Super-resolution\n+    /// - Any task requiring upsampling\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> ConvTranspose2D(\n+        ComputationNode<T> input,\n+        ComputationNode<T> kernel,\n+        ComputationNode<T>? bias = null,\n+        int[]? stride = null,\n+        int[]? padding = null,\n+        int[]? outputPadding = null)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var inputShape = input.Value.Shape;\n+        var kernelShape = kernel.Value.Shape;\n+\n+        if (inputShape.Length != 4)\n+            throw new ArgumentException(\"ConvTranspose2D requires 4D input [batch, inChannels, height, width]\");\n+        if (kernelShape.Length != 4)\n+            throw new ArgumentException(\"ConvTranspose2D requires 4D kernel [inChannels, outChannels, kernelH, kernelW]\");\n+\n+        stride ??= new int[] { 1, 1 };\n+        padding ??= new int[] { 0, 0 };\n+        outputPadding ??= new int[] { 0, 0 };\n+\n+        int batch = inputShape[0];\n+        int inChannels = inputShape[1];\n+        int inH = inputShape[2];\n+        int inW = inputShape[3];\n+\n+        int kernelInChannels = kernelShape[0];\n+        int outChannels = kernelShape[1];\n+        int kernelH = kernelShape[2];\n+        int kernelW = kernelShape[3];\n+\n+        if (inChannels != kernelInChannels)\n+            throw new ArgumentException($\"Input channels ({inChannels}) must match kernel input channels ({kernelInChannels})\");\n+\n+        int strideH = stride[0];\n+        int strideW = stride[1];\n+        int padH = padding[0];\n+        int padW = padding[1];\n+        int outPadH = outputPadding[0];\n+        int outPadW = outputPadding[1];\n+\n+        int outH = (inH - 1) * strideH - 2 * padH + kernelH + outPadH;\n+        int outW = (inW - 1) * strideW - 2 * padW + kernelW + outPadW;\n+\n+        var result = new Tensor<T>(new int[] { batch, outChannels, outH, outW });\n+\n+        // Forward pass: transposed convolution\n+        for (int b = 0; b < batch; b++)\n+        {\n+            for (int ic = 0; ic < inChannels; ic++)\n+            {\n+                for (int ih = 0; ih < inH; ih++)\n+                {\n+                    for (int iw = 0; iw < inW; iw++)\n+                    {\n+                        var inputVal = input.Value[b, ic, ih, iw];\n+\n+                        // Distribute this input value to output using kernel\n+                        for (int oc = 0; oc < outChannels; oc++)\n+                        {\n+                            for (int kh = 0; kh < kernelH; kh++)\n+                            {\n+                                for (int kw = 0; kw < kernelW; kw++)\n+                                {\n+                                    int oh = ih * strideH + kh - padH;\n+                                    int ow = iw * strideW + kw - padW;\n+\n+                                    if (oh >= 0 && oh < outH && ow >= 0 && ow < outW)\n+                                    {\n+                                        var kernelVal = kernel.Value[ic, oc, kh, kw];\n+                                        var contribution = numOps.Multiply(inputVal, kernelVal);\n+                                        result[b, oc, oh, ow] = numOps.Add(result[b, oc, oh, ow], contribution);\n+                                    }\n+                                }\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n+\n+            // Add bias if provided\n+            if (bias != null)\n+            {\n+                for (int oc = 0; oc < outChannels; oc++)\n+                {\n+                    for (int oh = 0; oh < outH; oh++)\n+                    {\n+                        for (int ow = 0; ow < outW; ow++)\n+                        {\n+                            result[b, oc, oh, ow] = numOps.Add(result[b, oc, oh, ow], bias.Value[oc]);\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            // Gradient w.r.t. input (this is a forward Conv2D!)\n+            if (input.RequiresGradient)\n+            {\n+                var gradInput = new Tensor<T>(inputShape);\n+\n+                for (int b = 0; b < batch; b++)\n+                {\n+                    for (int ic = 0; ic < inChannels; ic++)\n+                    {\n+                        for (int ih = 0; ih < inH; ih++)\n+                        {\n+                            for (int iw = 0; iw < inW; iw++)\n+                            {\n+                                var sum = numOps.Zero;\n+\n+                                for (int oc = 0; oc < outChannels; oc++)\n+                                {\n+                                    for (int kh = 0; kh < kernelH; kh++)\n+                                    {\n+                                        for (int kw = 0; kw < kernelW; kw++)\n+                                        {\n+                                            int oh = ih * strideH + kh - padH;\n+                                            int ow = iw * strideW + kw - padW;\n+\n+                                            if (oh >= 0 && oh < outH && ow >= 0 && ow < outW)\n+                                            {\n+                                                var gradVal = gradient[b, oc, oh, ow];\n+                                                var kernelVal = kernel.Value[ic, oc, kh, kw];\n+                                                sum = numOps.Add(sum, numOps.Multiply(gradVal, kernelVal));\n+                                            }\n+                                        }\n+                                    }\n+                                }\n+\n+                                gradInput[b, ic, ih, iw] = sum;\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (input.Gradient == null)\n+                    input.Gradient = gradInput;\n+                else\n+                    input.Gradient = input.Gradient.Add(gradInput);\n+            }\n+\n+            // Gradient w.r.t. kernel\n+            if (kernel.RequiresGradient)\n+            {\n+                var gradKernel = new Tensor<T>(kernelShape);\n+\n+                for (int ic = 0; ic < inChannels; ic++)\n+                {\n+                    for (int oc = 0; oc < outChannels; oc++)\n+                    {\n+                        for (int kh = 0; kh < kernelH; kh++)\n+                        {\n+                            for (int kw = 0; kw < kernelW; kw++)\n+                            {\n+                                var sum = numOps.Zero;\n+\n+                                for (int b = 0; b < batch; b++)\n+                                {\n+                                    for (int ih = 0; ih < inH; ih++)\n+                                    {\n+                                        for (int iw = 0; iw < inW; iw++)\n+                                        {\n+                                            int oh = ih * strideH + kh - padH;\n+                                            int ow = iw * strideW + kw - padW;\n+\n+                                            if (oh >= 0 && oh < outH && ow >= 0 && ow < outW)\n+                                            {\n+                                                var inputVal = input.Value[b, ic, ih, iw];\n+                                                var gradVal = gradient[b, oc, oh, ow];\n+                                                sum = numOps.Add(sum, numOps.Multiply(inputVal, gradVal));\n+                                            }\n+                                        }\n+                                    }\n+                                }\n+\n+                                gradKernel[ic, oc, kh, kw] = sum;\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (kernel.Gradient == null)\n+                    kernel.Gradient = gradKernel;\n+                else\n+                    kernel.Gradient = kernel.Gradient.Add(gradKernel);\n+            }\n+\n+            // Gradient w.r.t. bias\n+            if (bias != null && bias.RequiresGradient)\n+            {\n+                var gradBias = new Tensor<T>(new int[] { outChannels });\n+\n+                for (int oc = 0; oc < outChannels; oc++)\n+                {\n+                    var sum = numOps.Zero;\n+                    for (int b = 0; b < batch; b++)\n+                    {\n+                        for (int oh = 0; oh < outH; oh++)\n+                        {\n+                            for (int ow = 0; ow < outW; ow++)\n+                            {\n+                                sum = numOps.Add(sum, gradient[b, oc, oh, ow]);\n+                            }\n+                        }\n+                    }\n+                    gradBias[oc] = sum;\n+                }\n+\n+                if (bias.Gradient == null)\n+                    bias.Gradient = gradBias;\n+                else\n+                    bias.Gradient = bias.Gradient.Add(gradBias);\n+            }\n+        }\n+\n+        var parents = new List<ComputationNode<T>> { input, kernel };\n+        if (bias != null) parents.Add(bias);\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: input.RequiresGradient || kernel.RequiresGradient || (bias?.RequiresGradient ?? false),\n+            parents: parents,\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Reduces a tensor by computing the maximum value along specified axes.\n+    /// </summary>\n+    /// <param name=\"a\">The input computation node.</param>\n+    /// <param name=\"axes\">The axes along which to compute the maximum. If null, reduces over all axes.</param>\n+    /// <param name=\"keepDims\">Whether to keep the reduced dimensions with size 1.</param>\n+    /// <returns>A computation node representing the result of the reduce max operation.</returns>\n+    public static ComputationNode<T> ReduceMax(ComputationNode<T> a, int[]? axes = null, bool keepDims = false)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var inputShape = a.Value.Shape;\n+\n+        // If axes is null, reduce all dimensions\n+        if (axes == null)\n+        {\n+            axes = Enumerable.Range(0, inputShape.Length).ToArray();\n+        }\n+\n+        // Compute output shape\n+        var outputShape = new List<int>();\n+        for (int i = 0; i < inputShape.Length; i++)\n+        {\n+            if (!axes.Contains(i))\n+            {\n+                outputShape.Add(inputShape[i]);\n+            }\n+            else if (keepDims)\n+            {\n+                outputShape.Add(1);\n+            }\n+        }\n+\n+        if (outputShape.Count == 0)\n+            outputShape.Add(1);\n+\n+        var result = new Tensor<T>(outputShape.ToArray());\n+\n+        // Store max indices for gradient routing\n+        var maxIndices = new Dictionary<string, int[]>();\n+\n+        // Compute forward pass: find max values\n+        void ComputeMax(int[] currentIndices, int dim, int[] outputIndices)\n+        {\n+            if (dim == inputShape.Length)\n+            {\n+                // Reached a leaf, update result\n+                var value = a.Value[currentIndices];\n+                var outKey = string.Join(\",\", outputIndices.Take(outputShape.Count));\n+\n+                if (!maxIndices.ContainsKey(outKey))\n+                {\n+                    result[outputIndices] = value;\n+                    maxIndices[outKey] = (int[])currentIndices.Clone();\n+                }\n+                else\n+                {\n+                    if (numOps.GreaterThan(value, result[outputIndices]))\n+                    {\n+                        result[outputIndices] = value;\n+                        maxIndices[outKey] = (int[])currentIndices.Clone();\n+                    }\n+                }\n+                return;\n+            }\n+\n+            if (axes.Contains(dim))\n+            {\n+                // Reduce along this dimension\n+                for (int i = 0; i < inputShape[dim]; i++)\n+                {\n+                    currentIndices[dim] = i;\n+                    ComputeMax(currentIndices, dim + 1, outputIndices);\n+                }\n+            }\n+            else\n+            {\n+                // Keep this dimension\n+                int outIdx = outputIndices[0];\n+                for (int i = 0; i < inputShape[dim]; i++)\n+                {\n+                    currentIndices[dim] = i;\n+                    outputIndices[outIdx] = i;\n+                    ComputeMax(currentIndices, dim + 1, outputIndices);\n+                }\n+            }\n+        }\n+\n+        ComputeMax(new int[inputShape.Length], 0, new int[outputShape.Count]);\n+"}]}},{"id":"PRRT_kwDOKSXUF85hgbMK","isResolved":false,"isOutdated":false,"path":"src/Autodiff/TensorOperations.cs","line":5368,"originalLine":3253,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V4QsP","author":{"login":"coderabbitai"},"body":"_⚠️ Potential issue_ | _🔴 Critical_\n\n**Remove the stray closing brace**\n\nThis file starts with a file-scoped namespace (`namespace AiDotNet.Autodiff;`), so only the class body should be closed. The extra `}` at the very end has nothing to match, triggering the GA build failure (`CS1022: Type or namespace definition, or end-of-file expected`). Drop that trailing brace so the build can succeed.\n\n```diff\n-}\n }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>📝 Committable suggestion</summary>\n\n> ‼️ **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n}\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>🧰 Tools</summary>\n\n<details>\n<summary>🪛 GitHub Actions: Build</summary>\n\n[error] 3253-3253: dotnet build failed. Command: 'dotnet build --no-restore --configuration Debug'. CS1022: Type or namespace definition, or end-of-file expected.\n\n</details>\n<details>\n<summary>🪛 GitHub Actions: Quality Gates (.NET)</summary>\n\n[error] 3253-3253: CS1022: Type or namespace definition, or end-of-file expected\n\n</details>\n<details>\n<summary>🪛 GitHub Check: Build All Frameworks</summary>\n\n[failure] 3253-3253: \nType or namespace definition, or end-of-file expected\n\n---\n\n[failure] 3253-3253: \nType or namespace definition, or end-of-file expected\n\n---\n\n[failure] 3253-3253: \nType or namespace definition, or end-of-file expected\n\n---\n\n[failure] 3253-3253: \nType or namespace definition, or end-of-file expected\n\n</details>\n<details>\n<summary>🪛 GitHub Check: Publish Size Analysis</summary>\n\n[failure] 3253-3253: \nType or namespace definition, or end-of-file expected\n\n</details>\n\n</details>\n\n<details>\n<summary>🤖 Prompt for AI Agents</summary>\n\n```\nIn src/Autodiff/TensorOperations.cs around lines 3252 to 3253 there is a stray\nclosing brace at the end of the file; because this file uses a file-scoped\nnamespace (namespace AiDotNet.Autodiff;), remove the extra trailing '}' so only\nthe class body is closed (i.e., delete the unmatched brace), then save and\nrebuild to confirm CS1022 is resolved.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","createdAt":"2025-11-11T14:57:23Z","path":"src/Autodiff/TensorOperations.cs","line":5368,"originalLine":3253,"diffHunk":"@@ -1219,4 +1219,2035 @@\n \n         return node;\n     }\n+\n+    /// <summary>\n+    /// Computes the softmax function for a computation node along a specified axis.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"axis\">The axis along which to compute softmax. Default is -1 (last axis).</param>\n+    /// <returns>A new computation node containing the softmax result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method computes softmax (σ(x_i) = exp(x_i) / Σexp(x_j)) along the specified axis.\n+    /// Uses numerical stability trick: subtract max before exponentiating.\n+    /// The backward function uses: ∂softmax/∂x = softmax(x) * (grad - Σ(grad * softmax(x))).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Softmax converts a vector of numbers into probabilities.\n+    ///\n+    /// For softmax:\n+    /// - The forward pass exponentiates each element, then normalizes so they sum to 1\n+    /// - The result is a probability distribution (all values between 0 and 1, summing to 1)\n+    /// - The backward pass is complex but efficient: uses the Jacobian of softmax\n+    ///\n+    /// Softmax is crucial for:\n+    /// - Multi-class classification (final layer outputs)\n+    /// - Attention mechanisms (computing attention weights)\n+    /// - Anywhere you need to convert scores to probabilities\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Softmax(ComputationNode<T> a, int axis = -1)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        // Normalize axis to positive index\n+        if (axis < 0)\n+            axis = shape.Length + axis;\n+\n+        // For simplicity, handle 2D case (batch, features) with axis=-1\n+        if (shape.Length == 2 && axis == 1)\n+        {\n+            int batchSize = shape[0];\n+            int features = shape[1];\n+            var result = new Tensor<T>(shape);\n+\n+            // Compute softmax for each row\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                // Find max for numerical stability\n+                var maxVal = a.Value[b, 0];\n+                for (int f = 1; f < features; f++)\n+                {\n+                    if (numOps.GreaterThan(a.Value[b, f], maxVal))\n+                        maxVal = a.Value[b, f];\n+                }\n+\n+                // Compute exp(x - max) and sum\n+                var expSum = numOps.Zero;\n+                var expValues = new T[features];\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var shifted = numOps.Subtract(a.Value[b, f], maxVal);\n+                    expValues[f] = numOps.Exp(shifted);\n+                    expSum = numOps.Add(expSum, expValues[f]);\n+                }\n+\n+                // Normalize\n+                for (int f = 0; f < features; f++)\n+                {\n+                    result[b, f] = numOps.Divide(expValues[f], expSum);\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (a.RequiresGradient)\n+                {\n+                    // ∂softmax/∂x_i = softmax_i * (∂L/∂y_i - Σ_j(∂L/∂y_j * softmax_j))\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        // Compute sum of (gradient * softmax)\n+                        var dotProduct = numOps.Zero;\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            dotProduct = numOps.Add(dotProduct,\n+                                numOps.Multiply(gradient[b, f], result[b, f]));\n+                        }\n+\n+                        // Compute gradient for each element\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            var gradMinusDot = numOps.Subtract(gradient[b, f], dotProduct);\n+                            gradA[b, f] = numOps.Multiply(result[b, f], gradMinusDot);\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient,\n+                parents: new List<ComputationNode<T>> { a },\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Softmax is currently only implemented for 2D tensors along axis=-1. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}], axis={axis}\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Concatenates multiple computation nodes along a specified axis.\n+    /// </summary>\n+    /// <param name=\"nodes\">The list of nodes to concatenate.</param>\n+    /// <param name=\"axis\">The axis along which to concatenate. Default is 0.</param>\n+    /// <returns>A new computation node containing the concatenated result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method concatenates tensors along the specified axis.\n+    /// All tensors must have the same shape except along the concatenation axis.\n+    /// The backward function splits the gradient and sends each portion to the corresponding input.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Concat stacks tensors together along a dimension.\n+    ///\n+    /// For concatenation:\n+    /// - The forward pass combines multiple tensors into one larger tensor\n+    /// - The backward pass splits the gradient back to each input\n+    /// - Think of it like gluing arrays together end-to-end\n+    ///\n+    /// Used in:\n+    /// - Skip connections (concatenating features from different layers)\n+    /// - Multi-input architectures\n+    /// - Feature fusion in neural networks\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Concat(List<ComputationNode<T>> nodes, int axis = 0)\n+    {\n+        if (nodes.Count == 0)\n+            throw new ArgumentException(\"Cannot concatenate empty list of nodes\");\n+\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var firstShape = nodes[0].Value.Shape;\n+\n+        // Normalize axis\n+        if (axis < 0)\n+            axis = firstShape.Length + axis;\n+\n+        // Validate shapes match except on concat axis\n+        for (int i = 1; i < nodes.Count; i++)\n+        {\n+            var shape = nodes[i].Value.Shape;\n+            if (shape.Length != firstShape.Length)\n+                throw new ArgumentException(\"All tensors must have the same rank\");\n+\n+            for (int d = 0; d < firstShape.Length; d++)\n+            {\n+                if (d != axis && shape[d] != firstShape[d])\n+                    throw new ArgumentException(\n+                        $\"Shape mismatch at dimension {d}: {shape[d]} vs {firstShape[d]}\");\n+            }\n+        }\n+\n+        // Compute output shape\n+        int[] outputShape = (int[])firstShape.Clone();\n+        for (int i = 1; i < nodes.Count; i++)\n+        {\n+            outputShape[axis] += nodes[i].Value.Shape[axis];\n+        }\n+\n+        // Perform concatenation (handle 2D case for simplicity)\n+        Tensor<T> result;\n+        if (firstShape.Length == 2 && axis == 1)\n+        {\n+            // Concatenate along columns (features)\n+            int rows = firstShape[0];\n+            int totalCols = outputShape[1];\n+            result = new Tensor<T>(new int[] { rows, totalCols });\n+\n+            int colOffset = 0;\n+            foreach (var node in nodes)\n+            {\n+                int cols = node.Value.Shape[1];\n+                for (int r = 0; r < rows; r++)\n+                {\n+                    for (int c = 0; c < cols; c++)\n+                    {\n+                        result[r, colOffset + c] = node.Value[r, c];\n+                    }\n+                }\n+                colOffset += cols;\n+            }\n+        }\n+        else if (firstShape.Length == 2 && axis == 0)\n+        {\n+            // Concatenate along rows (batch)\n+            int cols = firstShape[1];\n+            int totalRows = outputShape[0];\n+            result = new Tensor<T>(new int[] { totalRows, cols });\n+\n+            int rowOffset = 0;\n+            foreach (var node in nodes)\n+            {\n+                int rows = node.Value.Shape[0];\n+                for (int r = 0; r < rows; r++)\n+                {\n+                    for (int c = 0; c < cols; c++)\n+                    {\n+                        result[rowOffset + r, c] = node.Value[r, c];\n+                    }\n+                }\n+                rowOffset += rows;\n+            }\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Concat is currently only implemented for 2D tensors. \" +\n+                $\"Got shape=[{string.Join(\", \", firstShape)}]\");\n+        }\n+\n+        // Store sizes for gradient splitting\n+        var sizes = nodes.Select(n => n.Value.Shape[axis]).ToList();\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            // Split gradient along concat axis and distribute to inputs\n+            if (firstShape.Length == 2 && axis == 1)\n+            {\n+                int rows = firstShape[0];\n+                int colOffset = 0;\n+                for (int i = 0; i < nodes.Count; i++)\n+                {\n+                    if (!nodes[i].RequiresGradient)\n+                    {\n+                        colOffset += sizes[i];\n+                        continue;\n+                    }\n+\n+                    int cols = sizes[i];\n+                    var gradPart = new Tensor<T>(new int[] { rows, cols });\n+\n+                    for (int r = 0; r < rows; r++)\n+                    {\n+                        for (int c = 0; c < cols; c++)\n+                        {\n+                            gradPart[r, c] = gradient[r, colOffset + c];\n+                        }\n+                    }\n+\n+                    if (nodes[i].Gradient == null)\n+                        nodes[i].Gradient = gradPart;\n+                    else\n+                        nodes[i].Gradient = nodes[i].Gradient.Add(gradPart);\n+\n+                    colOffset += cols;\n+                }\n+            }\n+            else if (firstShape.Length == 2 && axis == 0)\n+            {\n+                int cols = firstShape[1];\n+                int rowOffset = 0;\n+                for (int i = 0; i < nodes.Count; i++)\n+                {\n+                    if (!nodes[i].RequiresGradient)\n+                    {\n+                        rowOffset += sizes[i];\n+                        continue;\n+                    }\n+\n+                    int rows = sizes[i];\n+                    var gradPart = new Tensor<T>(new int[] { rows, cols });\n+\n+                    for (int r = 0; r < rows; r++)\n+                    {\n+                        for (int c = 0; c < cols; c++)\n+                        {\n+                            gradPart[r, c] = gradient[rowOffset + r, c];\n+                        }\n+                    }\n+\n+                    if (nodes[i].Gradient == null)\n+                        nodes[i].Gradient = gradPart;\n+                    else\n+                        nodes[i].Gradient = nodes[i].Gradient.Add(gradPart);\n+\n+                    rowOffset += rows;\n+                }\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: nodes.Any(n => n.RequiresGradient),\n+            parents: nodes,\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Pads a tensor with a constant value along specified dimensions.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"padWidth\">Padding width for each dimension as (before, after) pairs.</param>\n+    /// <param name=\"value\">The value to use for padding. Default is zero.</param>\n+    /// <returns>A new computation node containing the padded result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method adds padding around the tensor.\n+    /// The backward function simply crops the gradient back to the original size (gradients for padding are zero).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Pad adds extra elements around a tensor.\n+    ///\n+    /// For padding:\n+    /// - The forward pass adds border elements with a constant value\n+    /// - The backward pass removes those border gradients (they don't affect the original tensor)\n+    /// - Think of it like adding margins to an image\n+    ///\n+    /// Used in:\n+    /// - Convolutional layers (to maintain spatial dimensions)\n+    /// - Handling variable-length sequences\n+    /// - Data augmentation\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Pad(ComputationNode<T> a, int[,] padWidth, T? value = default)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var padValue = value ?? numOps.Zero;\n+        var shape = a.Value.Shape;\n+\n+        // Validate padWidth dimensions\n+        if (padWidth.GetLength(0) != shape.Length)\n+            throw new ArgumentException(\"padWidth must have same number of dimensions as input tensor\");\n+\n+        // Compute output shape\n+        var outputShape = new int[shape.Length];\n+        for (int d = 0; d < shape.Length; d++)\n+        {\n+            outputShape[d] = shape[d] + padWidth[d, 0] + padWidth[d, 1];\n+        }\n+\n+        // Handle 2D case\n+        if (shape.Length == 2)\n+        {\n+            int inputRows = shape[0];\n+            int inputCols = shape[1];\n+            int padTop = padWidth[0, 0];\n+            int padBottom = padWidth[0, 1];\n+            int padLeft = padWidth[1, 0];\n+            int padRight = padWidth[1, 1];\n+\n+            var result = new Tensor<T>(outputShape);\n+\n+            // Initialize with pad value\n+            for (int i = 0; i < result.Length; i++)\n+            {\n+                result[i] = padValue;\n+            }\n+\n+            // Copy input data to center\n+            for (int r = 0; r < inputRows; r++)\n+            {\n+                for (int c = 0; c < inputCols; c++)\n+                {\n+                    result[padTop + r, padLeft + c] = a.Value[r, c];\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (a.RequiresGradient)\n+                {\n+                    // Extract gradient for original (unpadded) region\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int r = 0; r < inputRows; r++)\n+                    {\n+                        for (int c = 0; c < inputCols; c++)\n+                        {\n+                            gradA[r, c] = gradient[padTop + r, padLeft + c];\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient,\n+                parents: new List<ComputationNode<T>> { a },\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"Pad is currently only implemented for 2D tensors. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}]\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs 2D max pooling on a 4D tensor (batch, channels, height, width).\n+    /// </summary>\n+    /// <param name=\"a\">The input node with shape [batch, channels, height, width].</param>\n+    /// <param name=\"poolSize\">The size of the pooling window [poolH, poolW].</param>\n+    /// <param name=\"strides\">The stride for the pooling operation [strideH, strideW]. If null, uses poolSize.</param>\n+    /// <returns>A new computation node containing the max pooled result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method performs max pooling over 2D spatial dimensions.\n+    /// During forward pass, it tracks which element was the max for routing gradients during backward pass.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> MaxPool downsamples by taking the maximum value in each window.\n+    ///\n+    /// For max pooling:\n+    /// - The forward pass slides a window and takes the max value in each position\n+    /// - This reduces spatial dimensions (downsampling)\n+    /// - The backward pass routes gradients only to the positions that were max\n+    /// - Other positions get zero gradient (they didn't contribute to the output)\n+    ///\n+    /// Used in:\n+    /// - CNNs for translation invariance\n+    /// - Reducing spatial resolution\n+    /// - Building hierarchical features\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> MaxPool2D(\n+        ComputationNode<T> a,\n+        int[] poolSize,\n+        int[]? strides = null)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        if (shape.Length != 4)\n+            throw new ArgumentException(\"MaxPool2D requires 4D input [batch, channels, height, width]\");\n+\n+        strides ??= poolSize;\n+\n+        int batch = shape[0];\n+        int channels = shape[1];\n+        int inH = shape[2];\n+        int inW = shape[3];\n+        int poolH = poolSize[0];\n+        int poolW = poolSize[1];\n+        int strideH = strides[0];\n+        int strideW = strides[1];\n+\n+        int outH = (inH - poolH) / strideH + 1;\n+        int outW = (inW - poolW) / strideW + 1;\n+\n+        var result = new Tensor<T>(new int[] { batch, channels, outH, outW });\n+        // Store max positions for backprop\n+        var maxPositions = new int[batch, channels, outH, outW, 2]; // [h_offset, w_offset]\n+\n+        // Forward pass: compute max pooling and track positions\n+        for (int b = 0; b < batch; b++)\n+        {\n+            for (int c = 0; c < channels; c++)\n+            {\n+                for (int oh = 0; oh < outH; oh++)\n+                {\n+                    for (int ow = 0; ow < outW; ow++)\n+                    {\n+                        int hStart = oh * strideH;\n+                        int wStart = ow * strideW;\n+\n+                        var maxVal = a.Value[b * channels * inH * inW +\n+                                           c * inH * inW +\n+                                           hStart * inW +\n+                                           wStart];\n+                        int maxHOffset = 0;\n+                        int maxWOffset = 0;\n+\n+                        // Find max in pooling window\n+                        for (int ph = 0; ph < poolH; ph++)\n+                        {\n+                            for (int pw = 0; pw < poolW; pw++)\n+                            {\n+                                int h = hStart + ph;\n+                                int w = wStart + pw;\n+                                if (h < inH && w < inW)\n+                                {\n+                                    var val = a.Value[b * channels * inH * inW +\n+                                                     c * inH * inW +\n+                                                     h * inW +\n+                                                     w];\n+                                    if (numOps.GreaterThan(val, maxVal))\n+                                    {\n+                                        maxVal = val;\n+                                        maxHOffset = ph;\n+                                        maxWOffset = pw;\n+                                    }\n+                                }\n+                            }\n+                        }\n+\n+                        result[b, c, oh, ow] = maxVal;\n+                        maxPositions[b, c, oh, ow, 0] = maxHOffset;\n+                        maxPositions[b, c, oh, ow, 1] = maxWOffset;\n+                    }\n+                }\n+            }\n+        }\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            if (a.RequiresGradient)\n+            {\n+                var gradA = new Tensor<T>(shape);\n+\n+                // Route gradients to max positions\n+                for (int b = 0; b < batch; b++)\n+                {\n+                    for (int c = 0; c < channels; c++)\n+                    {\n+                        for (int oh = 0; oh < outH; oh++)\n+                        {\n+                            for (int ow = 0; ow < outW; ow++)\n+                            {\n+                                int hStart = oh * strideH;\n+                                int wStart = ow * strideW;\n+                                int maxHOffset = maxPositions[b, c, oh, ow, 0];\n+                                int maxWOffset = maxPositions[b, c, oh, ow, 1];\n+\n+                                int maxH = hStart + maxHOffset;\n+                                int maxW = wStart + maxWOffset;\n+\n+                                int gradIdx = b * channels * inH * inW +\n+                                             c * inH * inW +\n+                                            maxH * inW +\n+                                             maxW;\n+\n+                                gradA[gradIdx] = numOps.Add(gradA[gradIdx], gradient[b, c, oh, ow]);\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (a.Gradient == null)\n+                    a.Gradient = gradA;\n+                else\n+                    a.Gradient = a.Gradient.Add(gradA);\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: a.RequiresGradient,\n+            parents: new List<ComputationNode<T>> { a },\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Performs 2D average pooling on a 4D tensor (batch, channels, height, width).\n+    /// </summary>\n+    /// <param name=\"a\">The input node with shape [batch, channels, height, width].</param>\n+    /// <param name=\"poolSize\">The size of the pooling window [poolH, poolW].</param>\n+    /// <param name=\"strides\">The stride for the pooling operation [strideH, strideW]. If null, uses poolSize.</param>\n+    /// <returns>A new computation node containing the average pooled result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method performs average pooling over 2D spatial dimensions.\n+    /// The backward function distributes gradients equally across the pooling window.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> AvgPool downsamples by taking the average value in each window.\n+    ///\n+    /// For average pooling:\n+    /// - The forward pass slides a window and computes the average\n+    /// - This smoothly reduces spatial dimensions\n+    /// - The backward pass distributes gradients equally to all elements in the window\n+    /// - Each element gets gradient / pool_area\n+    ///\n+    /// Used in:\n+    /// - CNNs for smoother downsampling than max pooling\n+    /// - Global average pooling (replacing fully connected layers)\n+    /// - Reducing overfitting\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> AvgPool2D(\n+        ComputationNode<T> a,\n+        int[] poolSize,\n+        int[]? strides = null)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+\n+        if (shape.Length != 4)\n+            throw new ArgumentException(\"AvgPool2D requires 4D input [batch, channels, height, width]\");\n+\n+        strides ??= poolSize;\n+\n+        int batch = shape[0];\n+        int channels = shape[1];\n+        int inH = shape[2];\n+        int inW = shape[3];\n+        int poolH = poolSize[0];\n+        int poolW = poolSize[1];\n+        int strideH = strides[0];\n+        int strideW = strides[1];\n+\n+        int outH = (inH - poolH) / strideH + 1;\n+        int outW = (inW - poolW) / strideW + 1;\n+\n+        var result = new Tensor<T>(new int[] { batch, channels, outH, outW });\n+        var poolArea = numOps.FromDouble(poolH * poolW);\n+\n+        // Forward pass: compute average pooling\n+        for (int b = 0; b < batch; b++)\n+        {\n+            for (int c = 0; c < channels; c++)\n+            {\n+                for (int oh = 0; oh < outH; oh++)\n+                {\n+                    for (int ow = 0; ow < outW; ow++)\n+                    {\n+                        int hStart = oh * strideH;\n+                        int wStart = ow * strideW;\n+\n+                        var sum = numOps.Zero;\n+\n+                        // Sum values in pooling window\n+                        for (int ph = 0; ph < poolH; ph++)\n+                        {\n+                            for (int pw = 0; pw < poolW; pw++)\n+                            {\n+                                int h = hStart + ph;\n+                                int w = wStart + pw;\n+                                if (h < inH && w < inW)\n+                                {\n+                                    sum = numOps.Add(sum, a.Value[b, c, h, w]);\n+                                }\n+                            }\n+                        }\n+\n+                        result[b, c, oh, ow] = numOps.Divide(sum, poolArea);\n+                    }\n+                }\n+            }\n+        }\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            if (a.RequiresGradient)\n+            {\n+                var gradA = new Tensor<T>(shape);\n+\n+                // Distribute gradients equally across pooling windows\n+                for (int b = 0; b < batch; b++)\n+                {\n+                    for (int c = 0; c < channels; c++)\n+                    {\n+                        for (int oh = 0; oh < outH; oh++)\n+                        {\n+                            for (int ow = 0; ow < outW; ow++)\n+                            {\n+                                int hStart = oh * strideH;\n+                                int wStart = ow * strideW;\n+\n+                                var gradValue = numOps.Divide(gradient[b, c, oh, ow], poolArea);\n+\n+                                // Distribute to all elements in window\n+                                for (int ph = 0; ph < poolH; ph++)\n+                                {\n+                                    for (int pw = 0; pw < poolW; pw++)\n+                                    {\n+                                        int h = hStart + ph;\n+                                        int w = wStart + pw;\n+                                        if (h < inH && w < inW)\n+                                        {\n+                                            gradA[b, c, h, w] = numOps.Add(gradA[b, c, h, w], gradValue);\n+                                        }\n+                                    }\n+                                }\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (a.Gradient == null)\n+                    a.Gradient = gradA;\n+                else\n+                    a.Gradient = a.Gradient.Add(gradA);\n+            }\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: a.RequiresGradient,\n+            parents: new List<ComputationNode<T>> { a },\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Applies layer normalization to a computation node.\n+    /// </summary>\n+    /// <param name=\"a\">The input node.</param>\n+    /// <param name=\"normalizedShape\">The shape over which to normalize (typically the feature dimensions).</param>\n+    /// <param name=\"gamma\">Optional scale parameter (learnable). If null, uses ones.</param>\n+    /// <param name=\"beta\">Optional shift parameter (learnable). If null, uses zeros.</param>\n+    /// <param name=\"epsilon\">Small constant for numerical stability. Default is 1e-5.</param>\n+    /// <returns>A new computation node containing the layer normalized result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Layer normalization normalizes inputs across the feature dimension for each sample independently.\n+    /// Formula: y = gamma * (x - mean) / sqrt(variance + epsilon) + beta\n+    /// Unlike batch normalization, this doesn't depend on batch statistics.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> LayerNorm standardizes features for each sample independently.\n+    ///\n+    /// For layer normalization:\n+    /// - Computes mean and variance for each sample's features\n+    /// - Normalizes: (x - mean) / sqrt(variance)\n+    /// - Scales and shifts: result * gamma + beta\n+    /// - Works the same during training and inference (no batch dependency)\n+    ///\n+    /// Used in:\n+    /// - Transformers (critical component)\n+    /// - RNNs (stabilizes training)\n+    /// - Any architecture needing sample-independent normalization\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> LayerNorm(\n+        ComputationNode<T> a,\n+        int[] normalizedShape,\n+        ComputationNode<T>? gamma = null,\n+        ComputationNode<T>? beta = null,\n+        double epsilon = 1e-5)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+        var eps = numOps.FromDouble(epsilon);\n+\n+        // For 2D input [batch, features], normalize over features\n+        if (shape.Length == 2 && normalizedShape.Length == 1 && normalizedShape[0] == shape[1])\n+        {\n+            int batchSize = shape[0];\n+            int features = shape[1];\n+\n+            // Create default gamma (ones) and beta (zeros) if not provided\n+            if (gamma == null)\n+            {\n+                var gammaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    gammaTensor[i] = numOps.One;\n+                gamma = Variable(gammaTensor, requiresGradient: false);\n+            }\n+\n+            if (beta == null)\n+            {\n+                var betaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    betaTensor[i] = numOps.Zero;\n+                beta = Variable(betaTensor, requiresGradient: false);\n+            }\n+\n+            var result = new Tensor<T>(shape);\n+            var means = new T[batchSize];\n+            var variances = new T[batchSize];\n+            var normalized = new Tensor<T>(shape);\n+\n+            // Forward pass: compute mean and variance per sample\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                // Compute mean\n+                var sum = numOps.Zero;\n+                for (int f = 0; f < features; f++)\n+                {\n+                    sum = numOps.Add(sum, a.Value[b, f]);\n+                }\n+                means[b] = numOps.Divide(sum, numOps.FromDouble(features));\n+\n+                // Compute variance\n+                var varSum = numOps.Zero;\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var diff = numOps.Subtract(a.Value[b, f], means[b]);\n+                    varSum = numOps.Add(varSum, numOps.Multiply(diff, diff));\n+                }\n+                variances[b] = numOps.Divide(varSum, numOps.FromDouble(features));\n+\n+                // Normalize and scale\n+                var std = numOps.Sqrt(numOps.Add(variances[b], eps));\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var norm = numOps.Divide(\n+                        numOps.Subtract(a.Value[b, f], means[b]),\n+                        std);\n+                    normalized[b, f] = norm;\n+                    result[b, f] = numOps.Add(\n+                        numOps.Multiply(norm, gamma.Value[f]),\n+                        beta.Value[f]);\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                // Gradients for gamma and beta\n+                if (gamma.RequiresGradient)\n+                {\n+                    var gradGamma = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum,\n+                                numOps.Multiply(gradient[b, f], normalized[b, f]));\n+                        }\n+                        gradGamma[f] = sum;\n+                    }\n+\n+                    if (gamma.Gradient == null)\n+                        gamma.Gradient = gradGamma;\n+                    else\n+                        gamma.Gradient = gamma.Gradient.Add(gradGamma);\n+                }\n+\n+                if (beta.RequiresGradient)\n+                {\n+                    var gradBeta = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum, gradient[b, f]);\n+                        }\n+                        gradBeta[f] = sum;\n+                    }\n+\n+                    if (beta.Gradient == null)\n+                        beta.Gradient = gradBeta;\n+                    else\n+                        beta.Gradient = beta.Gradient.Add(gradBeta);\n+                }\n+\n+                // Gradient for input\n+                if (a.RequiresGradient)\n+                {\n+                    var gradA = new Tensor<T>(shape);\n+\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        var std = numOps.Sqrt(numOps.Add(variances[b], eps));\n+                        var invStd = numOps.Divide(numOps.One, std);\n+\n+                        // Compute gradient components\n+                        var gradNormSum = numOps.Zero;\n+                        var gradNormDotNorm = numOps.Zero;\n+\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            var gradNorm = numOps.Multiply(gradient[b, f], gamma.Value[f]);\n+                            gradNormSum = numOps.Add(gradNormSum, gradNorm);\n+                            gradNormDotNorm = numOps.Add(gradNormDotNorm,\n+                                numOps.Multiply(gradNorm, normalized[b, f]));\n+                        }\n+\n+                        // Apply gradient formula\n+                        var featuresT = numOps.FromDouble(features);\n+                        for (int f = 0; f < features; f++)\n+                        {\n+                            var gradNorm = numOps.Multiply(gradient[b, f], gamma.Value[f]);\n+\n+                            var term1 = gradNorm;\n+                            var term2 = numOps.Divide(gradNormSum, featuresT);\n+                            var term3 = numOps.Divide(\n+                                numOps.Multiply(normalized[b, f], gradNormDotNorm),\n+                                featuresT);\n+\n+                            var grad = numOps.Multiply(\n+                                numOps.Subtract(numOps.Subtract(term1, term2), term3),\n+                                invStd);\n+\n+                            gradA[b, f] = grad;\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var parents = new List<ComputationNode<T>> { a };\n+            if (gamma != null) parents.Add(gamma);\n+            if (beta != null) parents.Add(beta);\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient || (gamma?.RequiresGradient ?? false) || (beta?.RequiresGradient ?? false),\n+                parents: parents,\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"LayerNorm is currently only implemented for 2D tensors normalizing over last dimension. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}], normalizedShape=[{string.Join(\", \", normalizedShape)}]\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Applies batch normalization to a computation node.\n+    /// </summary>\n+    /// <param name=\"a\">The input node with shape [batch, features].</param>\n+    /// <param name=\"gamma\">Optional scale parameter (learnable). If null, uses ones.</param>\n+    /// <param name=\"beta\">Optional shift parameter (learnable). If null, uses zeros.</param>\n+    /// <param name=\"runningMean\">Running mean for inference (not updated during this operation).</param>\n+    /// <param name=\"runningVar\">Running variance for inference (not updated during this operation).</param>\n+    /// <param name=\"training\">Whether in training mode (uses batch statistics) or inference mode (uses running statistics).</param>\n+    /// <param name=\"epsilon\">Small constant for numerical stability. Default is 1e-5.</param>\n+    /// <returns>A new computation node containing the batch normalized result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Batch normalization normalizes inputs across the batch dimension.\n+    /// During training: Uses batch statistics (mean and variance computed from current batch).\n+    /// During inference: Uses running statistics (accumulated during training).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> BatchNorm standardizes features across the batch.\n+    ///\n+    /// For batch normalization:\n+    /// - Training mode: Uses current batch's mean and variance\n+    /// - Inference mode: Uses running mean/variance from training\n+    /// - Normalizes: (x - mean) / sqrt(variance)\n+    /// - Scales and shifts: result * gamma + beta\n+    ///\n+    /// Benefits:\n+    /// - Stabilizes training (reduces internal covariate shift)\n+    /// - Allows higher learning rates\n+    /// - Acts as regularization\n+    ///\n+    /// Used in:\n+    /// - CNNs (after convolutional layers)\n+    /// - Deep feedforward networks\n+    /// - GANs and many other architectures\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> BatchNorm(\n+        ComputationNode<T> a,\n+        ComputationNode<T>? gamma = null,\n+        ComputationNode<T>? beta = null,\n+        Tensor<T>? runningMean = null,\n+        Tensor<T>? runningVar = null,\n+        bool training = true,\n+        double epsilon = 1e-5)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var shape = a.Value.Shape;\n+        var eps = numOps.FromDouble(epsilon);\n+\n+        // Handle 2D case [batch, features]\n+        if (shape.Length == 2)\n+        {\n+            int batchSize = shape[0];\n+            int features = shape[1];\n+\n+            // Create default gamma and beta if not provided\n+            if (gamma == null)\n+            {\n+                var gammaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    gammaTensor[i] = numOps.One;\n+                gamma = Variable(gammaTensor, requiresGradient: false);\n+            }\n+\n+            if (beta == null)\n+            {\n+                var betaTensor = new Tensor<T>(new int[] { features });\n+                for (int i = 0; i < features; i++)\n+                    betaTensor[i] = numOps.Zero;\n+                beta = Variable(betaTensor, requiresGradient: false);\n+            }\n+\n+            var result = new Tensor<T>(shape);\n+            T[] batchMean;\n+            T[] batchVar;\n+            var normalized = new Tensor<T>(shape);\n+\n+            if (training)\n+            {\n+                // Compute batch statistics\n+                batchMean = new T[features];\n+                batchVar = new T[features];\n+\n+                // Compute mean per feature\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var sum = numOps.Zero;\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        sum = numOps.Add(sum, a.Value[b, f]);\n+                    }\n+                    batchMean[f] = numOps.Divide(sum, numOps.FromDouble(batchSize));\n+                }\n+\n+                // Compute variance per feature\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var varSum = numOps.Zero;\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        var diff = numOps.Subtract(a.Value[b, f], batchMean[f]);\n+                        varSum = numOps.Add(varSum, numOps.Multiply(diff, diff));\n+                    }\n+                    batchVar[f] = numOps.Divide(varSum, numOps.FromDouble(batchSize));\n+                }\n+            }\n+            else\n+            {\n+                // Use running statistics for inference\n+                if (runningMean == null || runningVar == null)\n+                    throw new ArgumentException(\"Running statistics required for inference mode\");\n+\n+                batchMean = new T[features];\n+                batchVar = new T[features];\n+                for (int f = 0; f < features; f++)\n+                {\n+                    batchMean[f] = runningMean[f];\n+                    batchVar[f] = runningVar[f];\n+                }\n+            }\n+\n+            // Normalize and scale\n+            for (int b = 0; b < batchSize; b++)\n+            {\n+                for (int f = 0; f < features; f++)\n+                {\n+                    var std = numOps.Sqrt(numOps.Add(batchVar[f], eps));\n+                    var norm = numOps.Divide(\n+                        numOps.Subtract(a.Value[b, f], batchMean[f]),\n+                        std);\n+                    normalized[b, f] = norm;\n+                    result[b, f] = numOps.Add(\n+                        numOps.Multiply(norm, gamma.Value[f]),\n+                        beta.Value[f]);\n+                }\n+            }\n+\n+            void BackwardFunction(Tensor<T> gradient)\n+            {\n+                if (!training)\n+                {\n+                    // Inference mode: simpler gradient (no batch statistics gradient)\n+                    if (a.RequiresGradient)\n+                    {\n+                        var gradA = new Tensor<T>(shape);\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            for (int f = 0; f < features; f++)\n+                            {\n+                                var std = numOps.Sqrt(numOps.Add(batchVar[f], eps));\n+                                var invStd = numOps.Divide(numOps.One, std);\n+                                gradA[b, f] = numOps.Multiply(\n+                                    numOps.Multiply(gradient[b, f], gamma.Value[f]),\n+                                    invStd);\n+                            }\n+                        }\n+\n+                        if (a.Gradient == null)\n+                            a.Gradient = gradA;\n+                        else\n+                            a.Gradient = a.Gradient.Add(gradA);\n+                    }\n+                    return;\n+                }\n+\n+                // Training mode: full gradient computation\n+                // Gradients for gamma and beta\n+                if (gamma.RequiresGradient)\n+                {\n+                    var gradGamma = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum,\n+                                numOps.Multiply(gradient[b, f], normalized[b, f]));\n+                        }\n+                        gradGamma[f] = sum;\n+                    }\n+\n+                    if (gamma.Gradient == null)\n+                        gamma.Gradient = gradGamma;\n+                    else\n+                        gamma.Gradient = gamma.Gradient.Add(gradGamma);\n+                }\n+\n+                if (beta.RequiresGradient)\n+                {\n+                    var gradBeta = new Tensor<T>(new int[] { features });\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var sum = numOps.Zero;\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            sum = numOps.Add(sum, gradient[b, f]);\n+                        }\n+                        gradBeta[f] = sum;\n+                    }\n+\n+                    if (beta.Gradient == null)\n+                        beta.Gradient = gradBeta;\n+                    else\n+                        beta.Gradient = beta.Gradient.Add(gradBeta);\n+                }\n+\n+                // Gradient for input (complex due to batch statistics)\n+                if (a.RequiresGradient)\n+                {\n+                    var gradA = new Tensor<T>(shape);\n+                    var batchSizeT = numOps.FromDouble(batchSize);\n+\n+                    for (int f = 0; f < features; f++)\n+                    {\n+                        var std = numOps.Sqrt(numOps.Add(batchVar[f], eps));\n+                        var invStd = numOps.Divide(numOps.One, std);\n+\n+                        // Sum of gradients and gradient*normalized\n+                        var gradSum = numOps.Zero;\n+                        var gradNormSum = numOps.Zero;\n+\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            var grad = numOps.Multiply(gradient[b, f], gamma.Value[f]);\n+                            gradSum = numOps.Add(gradSum, grad);\n+                            gradNormSum = numOps.Add(gradNormSum,\n+                                numOps.Multiply(grad, normalized[b, f]));\n+                        }\n+\n+                        // Apply gradient formula\n+                        for (int b = 0; b < batchSize; b++)\n+                        {\n+                            var grad = numOps.Multiply(gradient[b, f], gamma.Value[f]);\n+\n+                            var term1 = grad;\n+                            var term2 = numOps.Divide(gradSum, batchSizeT);\n+                            var term3 = numOps.Divide(\n+                                numOps.Multiply(normalized[b, f], gradNormSum),\n+                                batchSizeT);\n+\n+                            var gradInput = numOps.Multiply(\n+                                numOps.Subtract(numOps.Subtract(term1, term2), term3),\n+                                invStd);\n+\n+                            gradA[b, f] = gradInput;\n+                        }\n+                    }\n+\n+                    if (a.Gradient == null)\n+                        a.Gradient = gradA;\n+                    else\n+                        a.Gradient = a.Gradient.Add(gradA);\n+                }\n+            }\n+\n+            var parents = new List<ComputationNode<T>> { a };\n+            if (gamma != null) parents.Add(gamma);\n+            if (beta != null) parents.Add(beta);\n+\n+            var node = new ComputationNode<T>(\n+                value: result,\n+                requiresGradient: a.RequiresGradient || (gamma?.RequiresGradient ?? false) || (beta?.RequiresGradient ?? false),\n+                parents: parents,\n+                backwardFunction: BackwardFunction,\n+                name: null);\n+\n+            var tape = GradientTape<T>.Current;\n+            if (tape != null && tape.IsRecording)\n+                tape.RecordOperation(node);\n+\n+            return node;\n+        }\n+        else\n+        {\n+            throw new NotImplementedException(\n+                $\"BatchNorm is currently only implemented for 2D tensors [batch, features]. \" +\n+                $\"Got shape=[{string.Join(\", \", shape)}]\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs 2D convolution on a 4D tensor (batch, channels, height, width).\n+    /// </summary>\n+    /// <param name=\"input\">The input node with shape [batch, inChannels, height, width].</param>\n+    /// <param name=\"kernel\">The kernel/filter with shape [outChannels, inChannels, kernelH, kernelW].</param>\n+    /// <param name=\"bias\">Optional bias with shape [outChannels]. If null, no bias is added.</param>\n+    /// <param name=\"stride\">The stride [strideH, strideW]. Default is [1, 1].</param>\n+    /// <param name=\"padding\">The padding [padH, padW]. Default is [0, 0].</param>\n+    /// <returns>A new computation node containing the convolution result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method performs 2D convolution, the fundamental operation in CNNs.\n+    /// Forward: Slides the kernel over the input computing dot products.\n+    /// Backward: Computes gradients for both input and kernel using transposed convolutions.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Conv2D is the core operation of convolutional neural networks.\n+    ///\n+    /// For 2D convolution:\n+    /// - The kernel \"slides\" over the input, computing weighted sums\n+    /// - Each output position is a dot product of the kernel with input patch\n+    /// - Stride controls how far the kernel moves each step\n+    /// - Padding adds borders to control output size\n+    ///\n+    /// Gradient computation:\n+    /// - Gradient w.r.t. input: \"full\" convolution with flipped kernel\n+    /// - Gradient w.r.t. kernel: cross-correlation between input and output gradient\n+    ///\n+    /// Used in:\n+    /// - All CNNs (image classification, object detection, segmentation)\n+    /// - Feature extraction in vision models\n+    /// - Learning spatial hierarchies\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> Conv2D(\n+        ComputationNode<T> input,\n+        ComputationNode<T> kernel,\n+        ComputationNode<T>? bias = null,\n+        int[]? stride = null,\n+        int[]? padding = null)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var inputShape = input.Value.Shape;\n+        var kernelShape = kernel.Value.Shape;\n+\n+        if (inputShape.Length != 4)\n+            throw new ArgumentException(\"Conv2D requires 4D input [batch, inChannels, height, width]\");\n+        if (kernelShape.Length != 4)\n+            throw new ArgumentException(\"Conv2D requires 4D kernel [outChannels, inChannels, kernelH, kernelW]\");\n+\n+        stride ??= new int[] { 1, 1 };\n+        padding ??= new int[] { 0, 0 };\n+\n+        int batch = inputShape[0];\n+        int inChannels = inputShape[1];\n+        int inH = inputShape[2];\n+        int inW = inputShape[3];\n+\n+        int outChannels = kernelShape[0];\n+        int kernelInChannels = kernelShape[1];\n+        int kernelH = kernelShape[2];\n+        int kernelW = kernelShape[3];\n+\n+        if (inChannels != kernelInChannels)\n+            throw new ArgumentException($\"Input channels ({inChannels}) must match kernel input channels ({kernelInChannels})\");\n+\n+        int strideH = stride[0];\n+        int strideW = stride[1];\n+        int padH = padding[0];\n+        int padW = padding[1];\n+\n+        int outH = (inH + 2 * padH - kernelH) / strideH + 1;\n+        int outW = (inW + 2 * padW - kernelW) / strideW + 1;\n+\n+        var result = new Tensor<T>(new int[] { batch, outChannels, outH, outW });\n+\n+        // Forward pass: convolution\n+        for (int b = 0; b < batch; b++)\n+        {\n+            for (int oc = 0; oc < outChannels; oc++)\n+            {\n+                for (int oh = 0; oh < outH; oh++)\n+                {\n+                    for (int ow = 0; ow < outW; ow++)\n+                    {\n+                        var sum = numOps.Zero;\n+\n+                        // Convolve kernel over input\n+                        for (int ic = 0; ic < inChannels; ic++)\n+                        {\n+                            for (int kh = 0; kh < kernelH; kh++)\n+                            {\n+                                for (int kw = 0; kw < kernelW; kw++)\n+                                {\n+                                    int ih = oh * strideH + kh - padH;\n+                                    int iw = ow * strideW + kw - padW;\n+\n+                                    // Check bounds (padding)\n+                                    if (ih >= 0 && ih < inH && iw >= 0 && iw < inW)\n+                                    {\n+                                        var inputVal = input.Value[b, ic, ih, iw];\n+                                        var kernelVal = kernel.Value[oc, ic, kh, kw];\n+                                        sum = numOps.Add(sum, numOps.Multiply(inputVal, kernelVal));\n+                                    }\n+                                }\n+                            }\n+                        }\n+\n+                        // Add bias if provided\n+                        if (bias != null)\n+                        {\n+                            sum = numOps.Add(sum, bias.Value[oc]);\n+                        }\n+\n+                        result[b, oc, oh, ow] = sum;\n+                    }\n+                }\n+            }\n+        }\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            // Gradient w.r.t. input\n+            if (input.RequiresGradient)\n+            {\n+                var gradInput = new Tensor<T>(inputShape);\n+\n+                // Full convolution with flipped kernel\n+                for (int b = 0; b < batch; b++)\n+                {\n+                    for (int ic = 0; ic < inChannels; ic++)\n+                    {\n+                        for (int ih = 0; ih < inH; ih++)\n+                        {\n+                            for (int iw = 0; iw < inW; iw++)\n+                            {\n+                                var sum = numOps.Zero;\n+\n+                                // Iterate over all output positions that used this input position\n+                                for (int oc = 0; oc < outChannels; oc++)\n+                                {\n+                                    for (int kh = 0; kh < kernelH; kh++)\n+                                    {\n+                                        for (int kw = 0; kw < kernelW; kw++)\n+                                        {\n+                                            // Compute output position\n+                                            int ohShifted = ih + padH - kh;\n+                                            int owShifted = iw + padW - kw;\n+\n+                                            if (ohShifted % strideH == 0 && owShifted % strideW == 0)\n+                                            {\n+                                                int oh = ohShifted / strideH;\n+                                                int ow = owShifted / strideW;\n+\n+                                                if (oh >= 0 && oh < outH && ow >= 0 && ow < outW)\n+                                                {\n+                                                    var gradVal = gradient[b, oc, oh, ow];\n+                                                    var kernelVal = kernel.Value[oc, ic, kh, kw];\n+                                                    sum = numOps.Add(sum, numOps.Multiply(gradVal, kernelVal));\n+                                                }\n+                                            }\n+                                        }\n+                                    }\n+                                }\n+\n+                                gradInput[b, ic, ih, iw] = sum;\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (input.Gradient == null)\n+                    input.Gradient = gradInput;\n+                else\n+                    input.Gradient = input.Gradient.Add(gradInput);\n+            }\n+\n+            // Gradient w.r.t. kernel\n+            if (kernel.RequiresGradient)\n+            {\n+                var gradKernel = new Tensor<T>(kernelShape);\n+\n+                // Cross-correlation between input and output gradient\n+                for (int oc = 0; oc < outChannels; oc++)\n+                {\n+                    for (int ic = 0; ic < inChannels; ic++)\n+                    {\n+                        for (int kh = 0; kh < kernelH; kh++)\n+                        {\n+                            for (int kw = 0; kw < kernelW; kw++)\n+                            {\n+                                var sum = numOps.Zero;\n+\n+                                for (int b = 0; b < batch; b++)\n+                                {\n+                                    for (int oh = 0; oh < outH; oh++)\n+                                    {\n+                                        for (int ow = 0; ow < outW; ow++)\n+                                        {\n+                                            int ih = oh * strideH + kh - padH;\n+                                            int iw = ow * strideW + kw - padW;\n+\n+                                            if (ih >= 0 && ih < inH && iw >= 0 && iw < inW)\n+                                            {\n+                                                var gradVal = gradient[b, oc, oh, ow];\n+                                                var inputVal = input.Value[b, ic, ih, iw];\n+                                                sum = numOps.Add(sum, numOps.Multiply(gradVal, inputVal));\n+                                            }\n+                                        }\n+                                    }\n+                                }\n+\n+                                gradKernel[oc, ic, kh, kw] = sum;\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (kernel.Gradient == null)\n+                    kernel.Gradient = gradKernel;\n+                else\n+                    kernel.Gradient = kernel.Gradient.Add(gradKernel);\n+            }\n+\n+            // Gradient w.r.t. bias\n+            if (bias != null && bias.RequiresGradient)\n+            {\n+                var gradBias = new Tensor<T>(new int[] { outChannels });\n+\n+                for (int oc = 0; oc < outChannels; oc++)\n+                {\n+                    var sum = numOps.Zero;\n+                    for (int b = 0; b < batch; b++)\n+                    {\n+                        for (int oh = 0; oh < outH; oh++)\n+                        {\n+                            for (int ow = 0; ow < outW; ow++)\n+                            {\n+                                sum = numOps.Add(sum, gradient[b, oc, oh, ow]);\n+                            }\n+                        }\n+                    }\n+                    gradBias[oc] = sum;\n+                }\n+\n+                if (bias.Gradient == null)\n+                    bias.Gradient = gradBias;\n+                else\n+                    bias.Gradient = bias.Gradient.Add(gradBias);\n+            }\n+        }\n+\n+        var parents = new List<ComputationNode<T>> { input, kernel };\n+        if (bias != null) parents.Add(bias);\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: input.RequiresGradient || kernel.RequiresGradient || (bias?.RequiresGradient ?? false),\n+            parents: parents,\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Performs 2D transposed convolution (deconvolution) on a 4D tensor.\n+    /// </summary>\n+    /// <param name=\"input\">The input node with shape [batch, inChannels, height, width].</param>\n+    /// <param name=\"kernel\">The kernel with shape [inChannels, outChannels, kernelH, kernelW] (note: reversed from Conv2D).</param>\n+    /// <param name=\"bias\">Optional bias with shape [outChannels]. If null, no bias is added.</param>\n+    /// <param name=\"stride\">The stride [strideH, strideW]. Default is [1, 1].</param>\n+    /// <param name=\"padding\">The padding [padH, padW]. Default is [0, 0].</param>\n+    /// <param name=\"outputPadding\">Output padding [outPadH, outPadW] for size adjustment. Default is [0, 0].</param>\n+    /// <returns>A new computation node containing the transposed convolution result.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Transposed convolution (often called deconvolution) upsamples the input.\n+    /// It's the gradient of Conv2D with respect to its input, used as a forward operation.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> ConvTranspose2D upsamples spatial dimensions.\n+    ///\n+    /// For transposed convolution:\n+    /// - Inserts zeros between input elements according to stride\n+    /// - Applies regular convolution to the expanded input\n+    /// - Results in larger spatial dimensions (upsampling)\n+    ///\n+    /// Used in:\n+    /// - Image generation (GANs, VAEs)\n+    /// - Semantic segmentation (U-Net decoder)\n+    /// - Super-resolution\n+    /// - Any task requiring upsampling\n+    /// </para>\n+    /// </remarks>\n+    public static ComputationNode<T> ConvTranspose2D(\n+        ComputationNode<T> input,\n+        ComputationNode<T> kernel,\n+        ComputationNode<T>? bias = null,\n+        int[]? stride = null,\n+        int[]? padding = null,\n+        int[]? outputPadding = null)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var inputShape = input.Value.Shape;\n+        var kernelShape = kernel.Value.Shape;\n+\n+        if (inputShape.Length != 4)\n+            throw new ArgumentException(\"ConvTranspose2D requires 4D input [batch, inChannels, height, width]\");\n+        if (kernelShape.Length != 4)\n+            throw new ArgumentException(\"ConvTranspose2D requires 4D kernel [inChannels, outChannels, kernelH, kernelW]\");\n+\n+        stride ??= new int[] { 1, 1 };\n+        padding ??= new int[] { 0, 0 };\n+        outputPadding ??= new int[] { 0, 0 };\n+\n+        int batch = inputShape[0];\n+        int inChannels = inputShape[1];\n+        int inH = inputShape[2];\n+        int inW = inputShape[3];\n+\n+        int kernelInChannels = kernelShape[0];\n+        int outChannels = kernelShape[1];\n+        int kernelH = kernelShape[2];\n+        int kernelW = kernelShape[3];\n+\n+        if (inChannels != kernelInChannels)\n+            throw new ArgumentException($\"Input channels ({inChannels}) must match kernel input channels ({kernelInChannels})\");\n+\n+        int strideH = stride[0];\n+        int strideW = stride[1];\n+        int padH = padding[0];\n+        int padW = padding[1];\n+        int outPadH = outputPadding[0];\n+        int outPadW = outputPadding[1];\n+\n+        int outH = (inH - 1) * strideH - 2 * padH + kernelH + outPadH;\n+        int outW = (inW - 1) * strideW - 2 * padW + kernelW + outPadW;\n+\n+        var result = new Tensor<T>(new int[] { batch, outChannels, outH, outW });\n+\n+        // Forward pass: transposed convolution\n+        for (int b = 0; b < batch; b++)\n+        {\n+            for (int ic = 0; ic < inChannels; ic++)\n+            {\n+                for (int ih = 0; ih < inH; ih++)\n+                {\n+                    for (int iw = 0; iw < inW; iw++)\n+                    {\n+                        var inputVal = input.Value[b, ic, ih, iw];\n+\n+                        // Distribute this input value to output using kernel\n+                        for (int oc = 0; oc < outChannels; oc++)\n+                        {\n+                            for (int kh = 0; kh < kernelH; kh++)\n+                            {\n+                                for (int kw = 0; kw < kernelW; kw++)\n+                                {\n+                                    int oh = ih * strideH + kh - padH;\n+                                    int ow = iw * strideW + kw - padW;\n+\n+                                    if (oh >= 0 && oh < outH && ow >= 0 && ow < outW)\n+                                    {\n+                                        var kernelVal = kernel.Value[ic, oc, kh, kw];\n+                                        var contribution = numOps.Multiply(inputVal, kernelVal);\n+                                        result[b, oc, oh, ow] = numOps.Add(result[b, oc, oh, ow], contribution);\n+                                    }\n+                                }\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n+\n+            // Add bias if provided\n+            if (bias != null)\n+            {\n+                for (int oc = 0; oc < outChannels; oc++)\n+                {\n+                    for (int oh = 0; oh < outH; oh++)\n+                    {\n+                        for (int ow = 0; ow < outW; ow++)\n+                        {\n+                            result[b, oc, oh, ow] = numOps.Add(result[b, oc, oh, ow], bias.Value[oc]);\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            // Gradient w.r.t. input (this is a forward Conv2D!)\n+            if (input.RequiresGradient)\n+            {\n+                var gradInput = new Tensor<T>(inputShape);\n+\n+                for (int b = 0; b < batch; b++)\n+                {\n+                    for (int ic = 0; ic < inChannels; ic++)\n+                    {\n+                        for (int ih = 0; ih < inH; ih++)\n+                        {\n+                            for (int iw = 0; iw < inW; iw++)\n+                            {\n+                                var sum = numOps.Zero;\n+\n+                                for (int oc = 0; oc < outChannels; oc++)\n+                                {\n+                                    for (int kh = 0; kh < kernelH; kh++)\n+                                    {\n+                                        for (int kw = 0; kw < kernelW; kw++)\n+                                        {\n+                                            int oh = ih * strideH + kh - padH;\n+                                            int ow = iw * strideW + kw - padW;\n+\n+                                            if (oh >= 0 && oh < outH && ow >= 0 && ow < outW)\n+                                            {\n+                                                var gradVal = gradient[b, oc, oh, ow];\n+                                                var kernelVal = kernel.Value[ic, oc, kh, kw];\n+                                                sum = numOps.Add(sum, numOps.Multiply(gradVal, kernelVal));\n+                                            }\n+                                        }\n+                                    }\n+                                }\n+\n+                                gradInput[b, ic, ih, iw] = sum;\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (input.Gradient == null)\n+                    input.Gradient = gradInput;\n+                else\n+                    input.Gradient = input.Gradient.Add(gradInput);\n+            }\n+\n+            // Gradient w.r.t. kernel\n+            if (kernel.RequiresGradient)\n+            {\n+                var gradKernel = new Tensor<T>(kernelShape);\n+\n+                for (int ic = 0; ic < inChannels; ic++)\n+                {\n+                    for (int oc = 0; oc < outChannels; oc++)\n+                    {\n+                        for (int kh = 0; kh < kernelH; kh++)\n+                        {\n+                            for (int kw = 0; kw < kernelW; kw++)\n+                            {\n+                                var sum = numOps.Zero;\n+\n+                                for (int b = 0; b < batch; b++)\n+                                {\n+                                    for (int ih = 0; ih < inH; ih++)\n+                                    {\n+                                        for (int iw = 0; iw < inW; iw++)\n+                                        {\n+                                            int oh = ih * strideH + kh - padH;\n+                                            int ow = iw * strideW + kw - padW;\n+\n+                                            if (oh >= 0 && oh < outH && ow >= 0 && ow < outW)\n+                                            {\n+                                                var inputVal = input.Value[b, ic, ih, iw];\n+                                                var gradVal = gradient[b, oc, oh, ow];\n+                                                sum = numOps.Add(sum, numOps.Multiply(inputVal, gradVal));\n+                                            }\n+                                        }\n+                                    }\n+                                }\n+\n+                                gradKernel[ic, oc, kh, kw] = sum;\n+                            }\n+                        }\n+                    }\n+                }\n+\n+                if (kernel.Gradient == null)\n+                    kernel.Gradient = gradKernel;\n+                else\n+                    kernel.Gradient = kernel.Gradient.Add(gradKernel);\n+            }\n+\n+            // Gradient w.r.t. bias\n+            if (bias != null && bias.RequiresGradient)\n+            {\n+                var gradBias = new Tensor<T>(new int[] { outChannels });\n+\n+                for (int oc = 0; oc < outChannels; oc++)\n+                {\n+                    var sum = numOps.Zero;\n+                    for (int b = 0; b < batch; b++)\n+                    {\n+                        for (int oh = 0; oh < outH; oh++)\n+                        {\n+                            for (int ow = 0; ow < outW; ow++)\n+                            {\n+                                sum = numOps.Add(sum, gradient[b, oc, oh, ow]);\n+                            }\n+                        }\n+                    }\n+                    gradBias[oc] = sum;\n+                }\n+\n+                if (bias.Gradient == null)\n+                    bias.Gradient = gradBias;\n+                else\n+                    bias.Gradient = bias.Gradient.Add(gradBias);\n+            }\n+        }\n+\n+        var parents = new List<ComputationNode<T>> { input, kernel };\n+        if (bias != null) parents.Add(bias);\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: input.RequiresGradient || kernel.RequiresGradient || (bias?.RequiresGradient ?? false),\n+            parents: parents,\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Reduces a tensor by computing the maximum value along specified axes.\n+    /// </summary>\n+    /// <param name=\"a\">The input computation node.</param>\n+    /// <param name=\"axes\">The axes along which to compute the maximum. If null, reduces over all axes.</param>\n+    /// <param name=\"keepDims\">Whether to keep the reduced dimensions with size 1.</param>\n+    /// <returns>A computation node representing the result of the reduce max operation.</returns>\n+    public static ComputationNode<T> ReduceMax(ComputationNode<T> a, int[]? axes = null, bool keepDims = false)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var inputShape = a.Value.Shape;\n+\n+        // If axes is null, reduce all dimensions\n+        if (axes == null)\n+        {\n+            axes = Enumerable.Range(0, inputShape.Length).ToArray();\n+        }\n+\n+        // Compute output shape\n+        var outputShape = new List<int>();\n+        for (int i = 0; i < inputShape.Length; i++)\n+        {\n+            if (!axes.Contains(i))\n+            {\n+                outputShape.Add(inputShape[i]);\n+            }\n+            else if (keepDims)\n+            {\n+                outputShape.Add(1);\n+            }\n+        }\n+\n+        if (outputShape.Count == 0)\n+            outputShape.Add(1);\n+\n+        var result = new Tensor<T>(outputShape.ToArray());\n+\n+        // Store max indices for gradient routing\n+        var maxIndices = new Dictionary<string, int[]>();\n+\n+        // Compute forward pass: find max values\n+        void ComputeMax(int[] currentIndices, int dim, int[] outputIndices)\n+        {\n+            if (dim == inputShape.Length)\n+            {\n+                // Reached a leaf, update result\n+                var value = a.Value[currentIndices];\n+                var outKey = string.Join(\",\", outputIndices.Take(outputShape.Count));\n+\n+                if (!maxIndices.ContainsKey(outKey))\n+                {\n+                    result[outputIndices] = value;\n+                    maxIndices[outKey] = (int[])currentIndices.Clone();\n+                }\n+                else\n+                {\n+                    if (numOps.GreaterThan(value, result[outputIndices]))\n+                    {\n+                        result[outputIndices] = value;\n+                        maxIndices[outKey] = (int[])currentIndices.Clone();\n+                    }\n+                }\n+                return;\n+            }\n+\n+            if (axes.Contains(dim))\n+            {\n+                // Reduce along this dimension\n+                for (int i = 0; i < inputShape[dim]; i++)\n+                {\n+                    currentIndices[dim] = i;\n+                    ComputeMax(currentIndices, dim + 1, outputIndices);\n+                }\n+            }\n+            else\n+            {\n+                // Keep this dimension\n+                int outIdx = outputIndices[0];\n+                for (int i = 0; i < inputShape[dim]; i++)\n+                {\n+                    currentIndices[dim] = i;\n+                    outputIndices[outIdx] = i;\n+                    ComputeMax(currentIndices, dim + 1, outputIndices);\n+                }\n+            }\n+        }\n+\n+        ComputeMax(new int[inputShape.Length], 0, new int[outputShape.Count]);\n+\n+        // Backward function\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            if (!a.RequiresGradient) return;\n+\n+            var gradInput = new Tensor<T>(inputShape);\n+\n+            // Route gradients only to max positions\n+            foreach (var kvp in maxIndices)\n+            {\n+                var outIndices = kvp.Key.Split(',').Select(int.Parse).ToArray();\n+                var inIndices = kvp.Value;\n+\n+                gradInput[inIndices] = numOps.Add(gradInput[inIndices], gradient[outIndices]);\n+            }\n+\n+            if (a.Gradient == null)\n+                a.Gradient = gradInput;\n+            else\n+                a.Gradient = a.Gradient.Add(gradInput);\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: a.RequiresGradient,\n+            parents: new List<ComputationNode<T>> { a },\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+\n+    /// <summary>\n+    /// Reduces a tensor by computing the mean value along specified axes.\n+    /// </summary>\n+    /// <param name=\"a\">The input computation node.</param>\n+    /// <param name=\"axes\">The axes along which to compute the mean. If null, reduces over all axes.</param>\n+    /// <param name=\"keepDims\">Whether to keep the reduced dimensions with size 1.</param>\n+    /// <returns>A computation node representing the result of the reduce mean operation.</returns>\n+    public static ComputationNode<T> ReduceMean(ComputationNode<T> a, int[]? axes = null, bool keepDims = false)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var inputShape = a.Value.Shape;\n+\n+        // If axes is null, reduce all dimensions\n+        if (axes == null)\n+        {\n+            axes = Enumerable.Range(0, inputShape.Length).ToArray();\n+        }\n+\n+        // Compute output shape and count for averaging\n+        var outputShape = new List<int>();\n+        int reduceCount = 1;\n+        for (int i = 0; i < inputShape.Length; i++)\n+        {\n+            if (!axes.Contains(i))\n+            {\n+                outputShape.Add(inputShape[i]);\n+            }\n+            else\n+            {\n+                reduceCount *= inputShape[i];\n+                if (keepDims)\n+                {\n+                    outputShape.Add(1);\n+                }\n+            }\n+        }\n+\n+        if (outputShape.Count == 0)\n+            outputShape.Add(1);\n+\n+        var result = new Tensor<T>(outputShape.ToArray());\n+        var divisor = numOps.FromInt(reduceCount);\n+\n+        // Compute forward pass: sum and then divide\n+        void ComputeSum(int[] currentIndices, int dim, int[] outputIndices)\n+        {\n+            if (dim == inputShape.Length)\n+            {\n+                var value = a.Value[currentIndices];\n+                result[outputIndices] = numOps.Add(result[outputIndices], value);\n+                return;\n+            }\n+\n+            if (axes.Contains(dim))\n+            {\n+                for (int i = 0; i < inputShape[dim]; i++)\n+                {\n+                    currentIndices[dim] = i;\n+                    ComputeSum(currentIndices, dim + 1, outputIndices);\n+                }\n+            }\n+            else\n+            {\n+                int outIdx = Array.IndexOf(outputShape.ToArray(), inputShape[dim]);\n+                for (int i = 0; i < inputShape[dim]; i++)\n+                {\n+                    currentIndices[dim] = i;\n+                    outputIndices[outIdx] = i;\n+                    ComputeSum(currentIndices, dim + 1, outputIndices);\n+                }\n+            }\n+        }\n+\n+        ComputeSum(new int[inputShape.Length], 0, new int[outputShape.Count]);\n+\n+        // Divide by count to get mean\n+        for (int i = 0; i < result.Length; i++)\n+        {\n+            result[i] = numOps.Divide(result[i], divisor);\n+        }\n+\n+        // Backward function\n+        void BackwardFunction(Tensor<T> gradient)\n+        {\n+            if (!a.RequiresGradient) return;\n+\n+            var gradInput = new Tensor<T>(inputShape);\n+            var gradScale = numOps.Divide(numOps.One, divisor);\n+\n+            // Broadcast gradient back to input shape\n+            void BroadcastGrad(int[] currentIndices, int dim, int[] outputIndices)\n+            {\n+                if (dim == inputShape.Length)\n+                {\n+                    gradInput[currentIndices] = numOps.Multiply(gradient[outputIndices], gradScale);\n+                    return;\n+                }\n+\n+                if (axes.Contains(dim))\n+                {\n+                    for (int i = 0; i < inputShape[dim]; i++)\n+                    {\n+                        currentIndices[dim] = i;\n+                        BroadcastGrad(currentIndices, dim + 1, outputIndices);\n+                    }\n+                }\n+                else\n+                {\n+                    int outIdx = Array.IndexOf(outputShape.ToArray(), inputShape[dim]);\n+                    for (int i = 0; i < inputShape[dim]; i++)\n+                    {\n+                        currentIndices[dim] = i;\n+                        outputIndices[outIdx] = i;\n+                        BroadcastGrad(currentIndices, dim + 1, outputIndices);\n+                    }\n+                }\n+            }\n+\n+            BroadcastGrad(new int[inputShape.Length], 0, new int[outputShape.Count]);\n+\n+            if (a.Gradient == null)\n+                a.Gradient = gradInput;\n+            else\n+                a.Gradient = a.Gradient.Add(gradInput);\n+        }\n+\n+        var node = new ComputationNode<T>(\n+            value: result,\n+            requiresGradient: a.RequiresGradient,\n+            parents: new List<ComputationNode<T>> { a },\n+            backwardFunction: BackwardFunction,\n+            name: null);\n+\n+        var tape = GradientTape<T>.Current;\n+        if (tape != null && tape.IsRecording)\n+            tape.RecordOperation(node);\n+\n+        return node;\n+    }\n+}\n }"}]}},{"id":"PRRT_kwDOKSXUF85hgbMV","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/ActivationLayer.cs","line":332,"originalLine":332,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V4Qsf","author":{"login":"coderabbitai"},"body":"_⚠️ Potential issue_ | _🔴 Critical_\n\n**Guard autodiff path for vector activations.**\n\n`ApplyActivationAutodiff` only supports scalar activations. When `_useVectorActivation` is true we still enter the autodiff branch, but because `ScalarActivation` is null the helper returns the input unchanged. That makes the autodiff path behave like an identity layer, so gradients for vector activations (e.g., Softmax) are now wrong. Please fall back to `BackwardManual` whenever `_useVectorActivation` is true (or add proper autodiff support).\n\n<details>\n<summary>🤖 Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/Layers/ActivationLayer.cs around lines 280-332, the\nautodiff backward path doesn't guard for vector activations and will treat\nvector activations (when _useVectorActivation is true and ScalarActivation is\nnull) as identity, producing incorrect gradients; modify BackwardViaAutodiff to\ndetect _useVectorActivation at the start and immediately delegate to the\nexisting BackwardManual(outputGradient) (or throw and document that autodiff\nsupport for vector activations is not implemented) instead of building an\nautodiff graph; alternatively implement proper autodiff ops for vector\nactivations and use them, but the minimal fix is to short-circuit to\nBackwardManual when _useVectorActivation is true.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","createdAt":"2025-11-11T14:57:23Z","path":"src/NeuralNetworks/Layers/ActivationLayer.cs","line":332,"originalLine":332,"diffHunk":"@@ -234,35 +234,145 @@ public override Tensor<T> Forward(Tensor<T> input)\n     /// multiplied by the output gradient. For vector activation, the derivative tensor is multiplied by the output gradient.\n     /// </para>\n     /// <para><b>For Beginners:</b> This method calculates how the error gradient flows backward through this layer.\n-    /// \n+    ///\n     /// During backpropagation, the network calculates how each part contributed to the error.\n     /// This method:\n     /// 1. Checks that Forward() was called first (we need the saved input)\n     /// 2. Verifies the gradient has the correct shape\n     /// 3. Calculates how the gradient changes as it passes through this layer\n     /// 4. Returns the modified gradient\n-    /// \n+    ///\n     /// For example, with ReLU activation:\n     /// - If the input was positive, the gradient passes through unchanged\n     /// - If the input was negative, the gradient is blocked (becomes 0)\n-    /// \n+    ///\n     /// This is because ReLU's derivative is 1 for positive inputs and 0 for negative inputs.\n-    /// \n+    ///\n     /// This process helps the network understand which neurons to adjust during training.\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);\n+    }\n+\n+    /// <summary>\n+    /// Manual backward pass implementation.\n+    /// </summary>\n+    private Tensor<T> BackwardManual(Tensor<T> outputGradient)\n     {\n         if (_lastInput == null)\n             throw new ForwardPassRequiredException(\"ActivationLayer\", GetType().Name);\n \n         TensorValidator.ValidateShapesMatch(_lastInput, outputGradient, \"Activation Layer\", \"Backward Pass\");\n \n-        return _useVectorActivation \n-            ? BackwardVectorActivation(outputGradient) \n+        return _useVectorActivation\n+            ? BackwardVectorActivation(outputGradient)\n             : BackwardScalarActivation(outputGradient);\n     }\n \n+    /// <summary>\n+    /// Backward pass implementation using automatic differentiation.\n+    /// </summary>\n+    private Tensor<T> BackwardViaAutodiff(Tensor<T> outputGradient)\n+    {\n+        if (_lastInput == null)\n+            throw new ForwardPassRequiredException(\"ActivationLayer\", GetType().Name);\n+\n+        TensorValidator.ValidateShapesMatch(_lastInput, outputGradient, \"Activation Layer\", \"Backward Pass\");\n+\n+        // Create computation node for input\n+        var input = Autodiff.TensorOperations<T>.Variable(_lastInput, \"input\", requiresGradient: true);\n+\n+        // Apply activation using autodiff\n+        var output = ApplyActivationAutodiff(input);\n+\n+        // Set the gradient and propagate backward\n+        output.Gradient = outputGradient;\n+\n+        // Perform backward pass\n+        var topoOrder = GetTopologicalOrder(output);\n+        for (int i = topoOrder.Count - 1; i >= 0; i--)\n+        {\n+            var node = topoOrder[i];\n+            if (node.RequiresGradient && node.BackwardFunction != null && node.Gradient != null)\n+            {\n+                node.BackwardFunction(node.Gradient);\n+            }\n+        }\n+\n+        return input.Gradient!;\n+    }\n+\n+    /// <summary>\n+    /// Applies activation function using autodiff operations.\n+    /// </summary>\n+    private Autodiff.ComputationNode<T> ApplyActivationAutodiff(Autodiff.ComputationNode<T> input)\n+    {\n+        if (ScalarActivation is ReLUActivation<T>)\n+        {\n+            return Autodiff.TensorOperations<T>.ReLU(input);\n+        }\n+        else if (ScalarActivation is SigmoidActivation<T>)\n+        {\n+            return Autodiff.TensorOperations<T>.Sigmoid(input);\n+        }\n+        else if (ScalarActivation is TanhActivation<T>)\n+        {\n+            return Autodiff.TensorOperations<T>.Tanh(input);\n+        }\n+        else\n+        {\n+            // For unsupported activations, return input unchanged\n+            return input;\n+        }\n+    }"}]}},{"id":"PRRT_kwDOKSXUF85hgbMe","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/AddLayer.cs","line":378,"originalLine":378,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V4Qsq","author":{"login":"coderabbitai"},"body":"_⚠️ Potential issue_ | _🔴 Critical_\n\n**Fallback to manual path when a vector activation is configured.**  \n`ApplyActivationAutodiff` only covers scalar activations. When `UsingVectorActivation` (e.g., Softmax) and `UseAutodiff` is true, the autodiff path skips the activation derivative entirely, so gradients are wrong. Please short-circuit to `BackwardManual` until vector activations are wired into the autodiff graph.  \n\n\n```diff\n         if (_lastInputs == null || _lastOutput == null)\n         {\n             throw new InvalidOperationException(\"Forward pass must be called before backward pass.\");\n         }\n \n+        if (UsingVectorActivation && VectorActivation != null)\n+        {\n+            return BackwardManual(outputGradient);\n+        }\n```\n\n<details>\n<summary>🤖 Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/Layers/AddLayer.cs around lines 340 to 378, detect when a\nvector activation is configured and autodiff is enabled and short‑circuit to the\nmanual backward implementation: if UsingVectorActivation (e.g. Softmax) AND\nUseAutodiff is true, do not build the autodiff graph or call\nApplyActivationAutodiff; instead call BackwardManual(outputGradient) and return\nits appropriate input gradient (matching existing return behavior). Keep the\nexisting early null checks, and only use this short‑circuit path until vector\nactivations are fully implemented in the autodiff graph.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","createdAt":"2025-11-11T14:57:23Z","path":"src/NeuralNetworks/Layers/AddLayer.cs","line":378,"originalLine":378,"diffHunk":"@@ -313,6 +326,123 @@ public override Tensor<T> Backward(Tensor<T> outputGradient)\n         return inputGradients[0];\n     }\n \n+    /// <summary>\n+    /// Backward pass implementation using automatic differentiation.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">The gradient of the loss with respect to the layer's output.</param>\n+    /// <returns>The gradient of the loss with respect to the layer's input.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method uses automatic differentiation to compute gradients for addition operation.\n+    /// </para>\n+    /// </remarks>\n+    private Tensor<T> BackwardViaAutodiff(Tensor<T> outputGradient)\n+    {\n+        if (_lastInputs == null || _lastOutput == null)\n+        {\n+            throw new InvalidOperationException(\"Forward pass must be called before backward pass.\");\n+        }\n+\n+        // Convert to computation nodes\n+        var inputNodes = new List<Autodiff.ComputationNode<T>>();\n+        inputNodes.Add(Autodiff.TensorOperations<T>.Variable(_lastInputs[0], \"input_0\", requiresGradient: true));\n+\n+        // Forward computation using autodiff ops: result = input[0] + input[1] + ...\n+        var result = inputNodes[0];\n+        for (int i = 1; i < _lastInputs.Length; i++)\n+        {\n+            var inputNode = Autodiff.TensorOperations<T>.Variable(_lastInputs[i], $\"input_{i}\", requiresGradient: false);\n+            result = Autodiff.TensorOperations<T>.Add(result, inputNode);\n+        }\n+\n+        // Apply activation using autodiff\n+        var activated = ApplyActivationAutodiff(result);\n+\n+        // Set the gradient at the output\n+        activated.Gradient = outputGradient;\n+\n+        // Perform topological sort and backward pass\n+        var topoOrder = GetTopologicalOrder(activated);\n+\n+        // Execute backward pass in reverse topological order\n+        for (int i = topoOrder.Count - 1; i >= 0; i--)\n+        {\n+            var node = topoOrder[i];\n+            if (node.RequiresGradient && node.BackwardFunction != null && node.Gradient != null)\n+            {\n+                node.BackwardFunction(node.Gradient);\n+            }\n+        }\n+\n+        return inputNodes[0].Gradient!;\n+    }"}]}},{"id":"PRRT_kwDOKSXUF85hgbMp","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/AttentionLayer.cs","line":552,"originalLine":552,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V4Qs6","author":{"login":"coderabbitai"},"body":"_⚠️ Potential issue_ | _🔴 Critical_\n\n**Autodiff path ignores cross-attention inputs and masks.**  \n`BackwardViaAutodiff` rebuilds Q/K/V from `_lastInput` alone and never reapplies the stored attention mask, so whenever the layer was last invoked via `Forward(query, keyValue)` or any masked path the replayed computation no longer matches the forward pass. Gradients for cross-attention (and masked self-attention) are therefore incorrect. Please persist the actual key/value tensors and mask used during the forward pass and feed them back into the autodiff graph (or fall back to the manual path whenever that context is unavailable) before enabling autodiff here.\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","createdAt":"2025-11-11T14:57:23Z","path":"src/NeuralNetworks/Layers/AttentionLayer.cs","line":552,"originalLine":552,"diffHunk":"@@ -470,6 +483,159 @@ public override Tensor<T> Backward(Tensor<T> outputGradient)\n         return dinput;\n     }\n \n+    /// <summary>\n+    /// Backward pass implementation using automatic differentiation.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">The gradient of the loss with respect to the layer's output.</param>\n+    /// <returns>The gradient of the loss with respect to the layer's input.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method uses automatic differentiation to compute gradients. It's slower than the\n+    /// manual implementation but can be useful for:\n+    /// - Verifying gradient correctness\n+    /// - Rapid prototyping with custom modifications\n+    /// - Research and experimentation\n+    /// </para>\n+    /// </remarks>\n+    private Tensor<T> BackwardViaAutodiff(Tensor<T> outputGradient)\n+    {\n+        if (_lastInput == null || _lastAttentionWeights == null)\n+            throw new InvalidOperationException(\"Forward pass must be called before backward pass.\");\n+\n+        // Create computation graph\n+        var input = Autodiff.TensorOperations<T>.Variable(_lastInput, \"input\", requiresGradient: true);\n+        var Wq = Autodiff.TensorOperations<T>.Variable(_Wq, \"Wq\", requiresGradient: true);\n+        var Wk = Autodiff.TensorOperations<T>.Variable(_Wk, \"Wk\", requiresGradient: true);\n+        var Wv = Autodiff.TensorOperations<T>.Variable(_Wv, \"Wv\", requiresGradient: true);\n+\n+        // Forward computation using autodiff ops\n+        // Q = input @ Wq, K = input @ Wk, V = input @ Wv\n+        var Q = Autodiff.TensorOperations<T>.MatrixMultiply(input, Wq);\n+        var K = Autodiff.TensorOperations<T>.MatrixMultiply(input, Wk);\n+        var V = Autodiff.TensorOperations<T>.MatrixMultiply(input, Wv);\n+\n+        // Attention scores = Q @ K^T\n+        var K_T = Autodiff.TensorOperations<T>.Transpose(K);\n+        var attentionScores = Autodiff.TensorOperations<T>.MatrixMultiply(Q, K_T);\n+\n+        // Apply scaling\n+        var scaleFactor = NumOps.Sqrt(NumOps.FromDouble(_Wk.Shape[_Wk.Shape.Length - 1]));\n+        var scale = NumOps.Divide(NumOps.One, scaleFactor);\n+        var scaleTensor = CreateScalarTensor(scale, attentionScores.Value.Shape);\n+        var scaleNode = Autodiff.TensorOperations<T>.Variable(scaleTensor, \"scale\", requiresGradient: false);\n+        var scaledScores = Autodiff.TensorOperations<T>.ElementwiseMultiply(attentionScores, scaleNode);\n+\n+        // Apply activation (softmax approximation using available ops)\n+        var attentionWeights = ApplyActivationAutodiff(scaledScores);\n+\n+        // Output = attentionWeights @ V\n+        var output = Autodiff.TensorOperations<T>.MatrixMultiply(attentionWeights, V);\n+\n+        // Set gradient and perform backward pass\n+        output.Gradient = outputGradient;\n+\n+        var topoOrder = GetTopologicalOrder(output);\n+        for (int i = topoOrder.Count - 1; i >= 0; i--)\n+        {\n+            var node = topoOrder[i];\n+            if (node.RequiresGradient && node.BackwardFunction != null && node.Gradient != null)\n+            {\n+                node.BackwardFunction(node.Gradient);\n+            }\n+        }\n+\n+        // Extract gradients\n+        _dWq = Wq.Gradient!;\n+        _dWk = Wk.Gradient!;\n+        _dWv = Wv.Gradient!;\n+\n+        return input.Gradient!;"}]}},{"id":"PRRT_kwDOKSXUF85hgbMy","isResolved":true,"isOutdated":false,"path":"src/NeuralNetworks/Layers/DepthwiseSeparableConvolutionalLayer.cs","line":1236,"originalLine":1036,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V4QtJ","author":{"login":"coderabbitai"},"body":"_⚠️ Potential issue_ | _🔴 Critical_\n\n<details>\n<summary>🧩 Analysis chain</summary>\n\n**Extract topological sort to shared utility to eliminate duplication.**\n\nThis 38-line method is duplicated identically across GlobalPoolingLayer, ConvolutionalLayer, DeconvolutionalLayer, and now DepthwiseSeparableConvolutionalLayer. The algorithm has no layer-specific logic and should be centralized.\n\n\n\n\nRecommended approach: add a static helper to the Autodiff namespace or as a protected method in LayerBase<T>:\n\n```csharp\n// Option 1: In Autodiff namespace (reusable across all layers)\nnamespace AiDotNet.Autodiff;\n\npublic static class TopologicalSortHelper\n{\n    public static List<ComputationNode<T>> GetTopologicalOrder<T>(ComputationNode<T> root)\n    {\n        var visited = new HashSet<ComputationNode<T>>();\n        var result = new List<ComputationNode<T>>();\n\n        var stack = new Stack<(ComputationNode<T> node, bool processed)>();\n        stack.Push((root, false));\n\n        while (stack.Count > 0)\n        {\n            var (node, processed) = stack.Pop();\n\n            if (visited.Contains(node))\n                continue;\n\n            if (processed)\n            {\n                visited.Add(node);\n                result.Add(node);\n            }\n            else\n            {\n                stack.Push((node, true));\n                foreach (var parent in node.Parents)\n                {\n                    if (!visited.Contains(parent))\n                        stack.Push((parent, false));\n                }\n            }\n        }\n\n        return result;\n    }\n}\n\n// Option 2: In LayerBase<T> as protected method (if tightly coupled to layer logic)\nprotected List<Autodiff.ComputationNode<T>> GetTopologicalOrder(Autodiff.ComputationNode<T> root)\n{\n    // ... implementation ...\n}\n```\n\nThen replace all layer implementations with calls to the centralized version.\n\n---\n\n```\n\n---\n\n🏁 Script executed:\n\n```shell\n#!/bin/bash\n# Locate layer class files\nfd -e cs \"GlobalPoolingLayer|ConvolutionalLayer|DeconvolutionalLayer|DepthwiseSeparableConvolutionalLayer\" --type f\n```\n\nLength of output: 432\n\n---\n\n🏁 Script executed:\n\n```shell\n#!/bin/bash\n# Extract GetTopologicalOrder from GlobalPoolingLayer at lines 493-530\nif [ -f \"src/NeuralNetworks/Layers/GlobalPoolingLayer.cs\" ]; then\n  sed -n '493,530p' \"src/NeuralNetworks/Layers/GlobalPoolingLayer.cs\"\nfi\n```\n\nLength of output: 1090\n\n---\n\n🏁 Script executed:\n\n```shell\n#!/bin/bash\n# Extract GetTopologicalOrder from ConvolutionalLayer at lines 955-992\nif [ -f \"src/NeuralNetworks/Layers/ConvolutionalLayer.cs\" ]; then\n  sed -n '955,992p' \"src/NeuralNetworks/Layers/ConvolutionalLayer.cs\"\nfi\n```\n\nLength of output: 1090\n\n---\n\n🏁 Script executed:\n\n```shell\n#!/bin/bash\n# Search for GetTopologicalOrder method in DeconvolutionalLayer\nrg -n \"GetTopologicalOrder\" \"src/NeuralNetworks/Layers/DeconvolutionalLayer.cs\" -A 35\n```\n\nLength of output: 2614\n\n---\n\n🏁 Script executed:\n\n```shell\n#!/bin/bash\n# Verify all files contain GetTopologicalOrder\nrg -l \"GetTopologicalOrder\" \"src/NeuralNetworks/Layers/\" --type cs\n```\n\nLength of output: 1416\n\n---\n\n🏁 Script executed:\n\n```shell\n#!/bin/bash\n# Count occurrences of GetTopologicalOrder across all layer files\nrg \"private List<Autodiff.ComputationNode<T>> GetTopologicalOrder\" \"src/NeuralNetworks/Layers/\" --type cs -c\n```\n\nLength of output: 1474\n\n---\n\n```\n\n---\n\n🏁 Script executed:\n\n```shell\n#!/bin/bash\n# Extract GetTopologicalOrder from SeparableConvolutionalLayer to confirm identical\nrg -n \"private List<Autodiff.ComputationNode<T>> GetTopologicalOrder\" \"src/NeuralNetworks/Layers/SeparableConvolutionalLayer.cs\" -A 35\n```\n\nLength of output: 1146\n\n---\n\n🏁 Script executed:\n\n```shell\n#!/bin/bash\n# Extract GetTopologicalOrder from DenseLayer to confirm identical\nrg -n \"private List<Autodiff.ComputationNode<T>> GetTopologicalOrder\" \"src/NeuralNetworks/Layers/DenseLayer.cs\" -A 35\n```\n\nLength of output: 1146\n\n---\n\n🏁 Script executed:\n\n```shell\n#!/bin/bash\n# Check if base class LayerBase exists and contains shared utilities\nfd \"LayerBase\" --type cs\n```\n\nLength of output: 169\n\n---\n\n🏁 Script executed:\n\n```shell\n#!/bin/bash\n# Search for any existing utility classes for topological sorting\nrg \"TopologicalSort\" \"src/\" --type cs\n```\n\nLength of output: 362\n\n---\n\n```\n\n---\n\n🏁 Script executed:\n\n```shell\n#!/bin/bash\n# Extract TopologicalSort from ComputationNode.cs to see if it matches\nrg -n \"private List<ComputationNode<T>> TopologicalSort\" \"src/Autodiff/ComputationNode.cs\" -A 35\n```\n\nLength of output: 1311\n\n---\n\n🏁 Script executed:\n\n```shell\n#!/bin/bash\n# Check if TopologicalSort is public or instance method\nrg -n \"TopologicalSort\\(\\)\" \"src/Autodiff/ComputationNode.cs\" -B 2 -A 2\n```\n\nLength of output: 358\n\n---\n\n🏁 Script executed:\n\n```shell\n#!/bin/bash\n# Check how GetTopologicalOrder is called in layer classes\nrg \"GetTopologicalOrder\" \"src/NeuralNetworks/Layers/DenseLayer.cs\" -B 2 -A 2\n```\n\nLength of output: 437\n\n---\n\n🏁 Script executed:\n\n```shell\n#!/bin/bash\n# Verify method signature and access level\nrg -n \"def.*TopologicalSort|public.*TopologicalSort|private.*TopologicalSort\" \"src/Autodiff/ComputationNode.cs\" -A 1\n```\n\nLength of output: 68\n\n---\n\n\n\n</details>\n\n**Extract topological sort to shared utility—duplication spans 29 layer classes.**\n\nThis 38-line method is duplicated identically across 29 layer classes. The review initially identified only 4 (GlobalPoolingLayer, ConvolutionalLayer, DeconvolutionalLayer, DepthwiseSeparableConvolutionalLayer), but the implementation appears in all autodiff-capable layers: ActivationLayer, AddLayer, AttentionLayer, BatchNormalizationLayer, ConvolutionalLayer, DeconvolutionalLayer, DepthwiseSeparableConvolutionalLayer, DilatedConvolutionalLayer, DenseLayer, DropoutLayer, GatedLinearUnitLayer, GlobalPoolingLayer, HighwayLayer, LayerNormalizationLayer, LocallyConnectedLayer, MaskingLayer, MaxPoolingLayer, MultiHeadAttentionLayer, MultiplyLayer, PatchEmbeddingLayer, PoolingLayer, PositionalEncodingLayer, ResidualLayer, SelfAttentionLayer, SeparableConvolutionalLayer, SqueezeAndExcitationLayer, SubpixelConvolutionalLayer, TransformerDecoderLayer, and TransformerEncoderLayer.\n\nThe algorithm has no layer-specific logic and should be centralized in a static utility class (Autodiff.TopologicalSortHelper or similar) to eliminate maintenance burden across nearly 30 files.\n\n<details>\n<summary>🤖 Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/Layers/DepthwiseSeparableConvolutionalLayer.cs around\nlines 999 to 1036, the GetTopologicalOrder method is a duplicate of the same\n38-line topological sort found across ~29 layer classes; extract this algorithm\ninto a single static helper (e.g.,\nAutodiff.TopologicalSortHelper.GetTopologicalOrder<T>(ComputationNode<T> root)\nor similar) placed in the Autodiff namespace, make it generic to match\nComputationNode<T>, and have each layer replace its local method with a call to\nthat new helper to eliminate duplication and centralize maintenance.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n✅ Addressed in commits 32986e9 to 45025b0","createdAt":"2025-11-11T14:57:24Z","path":"src/NeuralNetworks/Layers/DepthwiseSeparableConvolutionalLayer.cs","line":1236,"originalLine":1036,"diffHunk":"@@ -952,6 +966,75 @@ public override Tensor<T> Backward(Tensor<T> outputGradient)\n         return inputGradient;\n     }\n \n+    /// <summary>\n+    /// Backward pass implementation using automatic differentiation.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">The gradient of the loss with respect to the layer's output.</param>\n+    /// <returns>The gradient of the loss with respect to the layer's input.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method uses automatic differentiation to compute gradients. Currently, depthwise separable convolution operations\n+    /// are not yet available in TensorOperations, so this method falls back to the manual implementation.\n+    /// </para>\n+    /// <para>\n+    /// Once depthwise separable convolution operations are added to TensorOperations, this method will provide:\n+    /// - Automatic gradient computation through the computation graph\n+    /// - Verification of manual gradient implementations\n+    /// - Support for rapid prototyping with custom modifications\n+    /// </para>\n+    /// </remarks>\n+    private Tensor<T> BackwardViaAutodiff(Tensor<T> outputGradient)\n+    {\n+        // TODO: Implement autodiff backward pass once depthwise separable convolution operations are available in TensorOperations\n+        // Convolution operation not yet available in TensorOperations\n+        // Falling back to manual implementation\n+        return BackwardManual(outputGradient);\n+    }\n+\n+    /// <summary>\n+    /// Gets the topological order of nodes in the computation graph.\n+    /// </summary>\n+    /// <param name=\"root\">The root node of the computation graph.</param>\n+    /// <returns>A list of nodes in topological order.</returns>\n+    private List<Autodiff.ComputationNode<T>> GetTopologicalOrder(Autodiff.ComputationNode<T> root)\n+    {\n+        var visited = new HashSet<Autodiff.ComputationNode<T>>();\n+        var result = new List<Autodiff.ComputationNode<T>>();\n+\n+        var stack = new Stack<(Autodiff.ComputationNode<T> node, bool processed)>();\n+        stack.Push((root, false));\n+\n+        while (stack.Count > 0)\n+        {\n+            var (node, processed) = stack.Pop();\n+\n+            if (visited.Contains(node))\n+            {\n+                continue;\n+            }\n+\n+            if (processed)\n+            {\n+                visited.Add(node);\n+                result.Add(node);\n+            }\n+            else\n+            {\n+                stack.Push((node, true));\n+\n+                foreach (var parent in node.Parents)\n+                {\n+                    if (!visited.Contains(parent))\n+                    {\n+                        stack.Push((parent, false));\n+                    }\n+                }\n+            }\n+        }\n+\n+        return result;\n+    }"}]}},{"id":"PRRT_kwDOKSXUF85hgbM6","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/GaussianNoiseLayer.cs","line":286,"originalLine":286,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V4QtU","author":{"login":"coderabbitai"},"body":"_⚠️ Potential issue_ | _🔴 Critical_\n\n**Cache the forward input before autodiff backward.**\n\n`_lastInput` isn’t defined or populated in this layer, so this block won’t compile. Even after adding the field, the autodiff path would always hit the guard. Cache the input during `Forward`, reuse it here, and clear it in `ResetState`, e.g.:\n\n```diff\n public class GaussianNoiseLayer<T> : LayerBase<T>\n {\n+    private Tensor<T>? _lastInput;\n```\n\n```diff\n     public override Tensor<T> Forward(Tensor<T> input)\n     {\n+        _lastInput = input;\n         if (IsTrainingMode)\n         {\n             _lastNoise = GenerateNoise(input.Shape);\n             return input.Add(_lastNoise);\n         }\n```\n\n```diff\n     public override void ResetState()\n     {\n         // Clear cached values from forward pass\n         _lastNoise = null;\n+        _lastInput = null;\n     }\n```\n\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>🤖 Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/Layers/GaussianNoiseLayer.cs around lines 271 to 286, the\nbackward path references _lastInput which is not defined/populated; add a\nprivate field to cache the forward input (matching the layer's tensor type), set\nthat field at the start/end of Forward(...) each time a forward pass runs, use\nit here instead of relying on an uninitialized value, and clear it in\nResetState() (or after backward) so subsequent passes don't hit the guard;\nensure the field's lifetime and nullability match existing patterns and update\nany null-check accordingly.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","createdAt":"2025-11-11T14:57:24Z","path":"src/NeuralNetworks/Layers/GaussianNoiseLayer.cs","line":286,"originalLine":286,"diffHunk":"@@ -236,11 +236,57 @@ public override Tensor<T> Forward(Tensor<T> input)\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);\n+    }\n+\n+    /// <summary>\n+    /// Manual backward pass implementation using optimized gradient calculations.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">The gradient of the loss with respect to the layer's output.</param>\n+    /// <returns>The gradient of the loss with respect to the layer's input.</returns>\n+    private Tensor<T> BackwardManual(Tensor<T> outputGradient)\n     {\n         // The gradient flows through unchanged\n         return outputGradient;\n     }\n \n+    /// <summary>\n+    /// Backward pass implementation using automatic differentiation.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">The gradient of the loss with respect to the layer's output.</param>\n+    /// <returns>The gradient of the loss with respect to the layer's input.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Gaussian noise is added independently to the input during forward pass. During backward pass,\n+    /// the gradient simply flows through unchanged because the noise doesn't depend on the input.\n+    /// This is equivalent to an identity operation in the computation graph.\n+    /// </para>\n+    /// </remarks>\n+    private Tensor<T> BackwardViaAutodiff(Tensor<T> outputGradient)\n+    {\n+        if (_lastInput == null)\n+            throw new InvalidOperationException(\"Forward pass must be called before backward pass.\");\n+\n+        // Create a simple computation graph where output = input + noise\n+        // Since noise is independent, gradient just flows through\n+        var inputNode = Autodiff.TensorOperations<T>.Variable(_lastInput, \"input\", requiresGradient: true);\n+\n+        // In autodiff terms, this is just an identity operation\n+        // The forward pass adds noise, but noise doesn't depend on input, so gradient passes through\n+        var outputNode = inputNode; // Identity for gradient purposes\n+\n+        // Set gradient and perform backward pass\n+        outputNode.Gradient = outputGradient;\n+\n+        // For a simple identity operation, just return the gradient\n+        return outputGradient;"}]}},{"id":"PRRT_kwDOKSXUF85hgbNA","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/LayerNormalizationLayer.cs","line":497,"originalLine":497,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V4Qte","author":{"login":"coderabbitai"},"body":"_⚠️ Potential issue_ | _🔴 Critical_\n\n<details>\n<summary>🧩 Analysis chain</summary>\n\n**Consider removing unused helper method.**\n\nThe `BroadcastVector` method is defined but doesn't appear to be used anywhere in this file. If it's not needed for the autodiff implementation, consider removing it to reduce code clutter.\n\n\n\nRun the following script to verify if this method is used elsewhere:\n\n---\n\n🏁 Script executed:\n\n```shell\n#!/bin/bash\n# Description: Check if BroadcastVector is used in LayerNormalizationLayer or elsewhere\n\n# Search for calls to BroadcastVector\nrg -n \"BroadcastVector\" --type=cs\n```\n\nLength of output: 255\n\n---\n\n🏁 Script executed:\n\n```shell\n#!/bin/bash\n# Search for actual calls/invocations of BroadcastVector\n# Exclude the method definitions and look for calls\nrg -nP \"this\\.BroadcastVector\\s*\\(|BroadcastVector\\s*\\(\" --type=cs -B 2 -A 2\n```\n\nLength of output: 1008\n\n---\n\n🏁 Script executed:\n\n```shell\n#!/bin/bash\n# Alternative: search in LayerNormalizationLayer.cs specifically\n# to see all method calls and understand what methods are actually used\ncat src/NeuralNetworks/Layers/LayerNormalizationLayer.cs | grep -n \"BroadcastVector\"\n```\n\nLength of output: 74\n\n---\n\n🏁 Script executed:\n\n```shell\n#!/bin/bash\n# Search for all method invocations in LayerNormalizationLayer.cs\n# to see which private helper methods are actually used\nrg -nP \"this\\.\\w+\\s*\\(\" src/NeuralNetworks/Layers/LayerNormalizationLayer.cs\n```\n\nLength of output: 0\n\n---\n\n🏁 Script executed:\n\n```shell\n#!/bin/bash\n# Extract the BackwardViaAutodiff method to see what it actually calls\nsed -n '348,397p' src/NeuralNetworks/Layers/LayerNormalizationLayer.cs\n```\n\nLength of output: 1942\n\n---\n\n🏁 Script executed:\n\n```shell\n#!/bin/bash\n# Search for any method names being called (simpler pattern without this.)\nrg -nP \"\\b[A-Z][a-zA-Z0-9]*\\s*\\(\" src/NeuralNetworks/Layers/LayerNormalizationLayer.cs | head -20\n```\n\nLength of output: 1806\n\n---\n\n\n\n</details>\n\n**Remove unused `BroadcastVector` method from LayerNormalizationLayer.cs (line 483-497).**\n\nThe method is never called within this file. Verification confirms no invocations exist anywhere in the codebase—only the two separate definitions in LayerNormalizationLayer.cs and BatchNormalizationLayer.cs. Removing this dead code reduces clutter and maintenance burden.\n\n<details>\n<summary>🤖 Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/Layers/LayerNormalizationLayer.cs around lines 480 to 497,\nremove the unused private method BroadcastVector(Tensor<T> vector, int\nbatchSize) entirely (delete the method declaration and body); ensure no other\ncode in the file references it and then rebuild/run tests to confirm no compile\nor runtime regressions.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","createdAt":"2025-11-11T14:57:24Z","path":"src/NeuralNetworks/Layers/LayerNormalizationLayer.cs","line":497,"originalLine":497,"diffHunk":"@@ -321,6 +334,168 @@ public override Tensor<T> Backward(Tensor<T> outputGradient)\n         return inputGradient;\n     }\n \n+    /// <summary>\n+    /// Backward pass implementation using automatic differentiation.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">The gradient of the loss with respect to the layer's output.</param>\n+    /// <returns>The gradient of the loss with respect to the layer's input.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method uses automatic differentiation to compute gradients. It recreates the forward\n+    /// computation graph and propagates gradients through it.\n+    /// </para>\n+    /// </remarks>\n+    private Tensor<T> BackwardViaAutodiff(Tensor<T> outputGradient)\n+    {\n+        if (_lastInput == null)\n+            throw new InvalidOperationException(\"Forward pass must be called before backward pass.\");\n+\n+        int batchSize = _lastInput.Shape[0];\n+        int featureSize = _lastInput.Shape[1];\n+\n+        // Convert to computation nodes\n+        var input = Autodiff.TensorOperations<T>.Variable(_lastInput, \"input\", requiresGradient: true);\n+\n+        // Convert gamma and beta vectors to tensors\n+        var gammaTensor = VectorToTensor(_gamma);\n+        var betaTensor = VectorToTensor(_beta);\n+        var gammaNode = Autodiff.TensorOperations<T>.Variable(gammaTensor, \"gamma\", requiresGradient: true);\n+        var betaNode = Autodiff.TensorOperations<T>.Variable(betaTensor, \"beta\", requiresGradient: true);\n+\n+        // Use LayerNorm operation for full gradient computation\n+        var normalizedShape = new int[] { featureSize };\n+        var output = Autodiff.TensorOperations<T>.LayerNorm(input, normalizedShape, gammaNode, betaNode, _epsilon);\n+\n+        // Set the gradient at the output\n+        output.Gradient = outputGradient;\n+\n+        // Perform topological sort and backward pass\n+        var topoOrder = GetTopologicalOrder(output);\n+\n+        // Execute backward pass in reverse topological order\n+        for (int i = topoOrder.Count - 1; i >= 0; i--)\n+        {\n+            var node = topoOrder[i];\n+            if (node.RequiresGradient && node.BackwardFunction != null && node.Gradient != null)\n+            {\n+                node.BackwardFunction(node.Gradient);\n+            }\n+        }\n+\n+        // Extract gradients from the computation graph\n+        if (gammaNode.Gradient != null)\n+        {\n+            _gammaGradient = TensorToVector(gammaNode.Gradient);\n+        }\n+\n+        if (betaNode.Gradient != null)\n+        {\n+            _betaGradient = TensorToVector(betaNode.Gradient);\n+        }\n+\n+        return input.Gradient!;\n+    }\n+\n+    /// <summary>\n+    /// Converts a Vector to a 1D Tensor.\n+    /// </summary>\n+    private Tensor<T> VectorToTensor(Vector<T> vector)\n+    {\n+        var tensor = new Tensor<T>(new int[] { vector.Length });\n+        for (int i = 0; i < vector.Length; i++)\n+        {\n+            tensor[i] = vector[i];\n+        }\n+        return tensor;\n+    }\n+\n+    /// <summary>\n+    /// Converts a 1D Tensor to a Vector.\n+    /// </summary>\n+    private Vector<T> TensorToVector(Tensor<T> tensor)\n+    {\n+        var vector = new Vector<T>(tensor.Length);\n+        for (int i = 0; i < tensor.Length; i++)\n+        {\n+            vector[i] = tensor[i];\n+        }\n+        return vector;\n+    }\n+\n+    /// <summary>\n+    /// Gets the topological order of nodes in the computation graph.\n+    /// </summary>\n+    private List<Autodiff.ComputationNode<T>> GetTopologicalOrder(Autodiff.ComputationNode<T> root)\n+    {\n+        var visited = new HashSet<Autodiff.ComputationNode<T>>();\n+        var result = new List<Autodiff.ComputationNode<T>>();\n+\n+        var stack = new Stack<(Autodiff.ComputationNode<T> node, bool processed)>();\n+        stack.Push((root, false));\n+\n+        while (stack.Count > 0)\n+        {\n+            var (node, processed) = stack.Pop();\n+\n+            if (visited.Contains(node))\n+            {\n+                continue;\n+            }\n+\n+            if (processed)\n+            {\n+                visited.Add(node);\n+                result.Add(node);\n+            }\n+            else\n+            {\n+                stack.Push((node, true));\n+\n+                foreach (var parent in node.Parents)\n+                {\n+                    if (!visited.Contains(parent))\n+                    {\n+                        stack.Push((parent, false));\n+                    }\n+                }\n+            }\n+        }\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Converts a Vector to a 1D Tensor.\n+    /// </summary>\n+    private Tensor<T> VectorToTensor(Vector<T> vector)\n+    {\n+        var tensor = new Tensor<T>(new int[] { vector.Length });\n+        for (int i = 0; i < vector.Length; i++)\n+        {\n+            tensor[i] = vector[i];\n+        }\n+        return tensor;\n+    }\n+\n+    /// <summary>\n+    /// Broadcasts a 1D vector across the batch dimension to create a 2D tensor.\n+    /// </summary>\n+    private Tensor<T> BroadcastVector(Tensor<T> vector, int batchSize)\n+    {\n+        var featureSize = vector.Length;\n+        var broadcasted = new Tensor<T>(new int[] { batchSize, featureSize });\n+\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            for (int j = 0; j < featureSize; j++)\n+            {\n+                broadcasted[i, j] = vector[j];\n+            }\n+        }\n+\n+        return broadcasted;\n+    }"}]}},{"id":"PRRT_kwDOKSXUF85hgbNJ","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/ResidualLayer.cs","line":367,"originalLine":367,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V4Qtn","author":{"login":"coderabbitai"},"body":"_⚠️ Potential issue_ | _🔴 Critical_\n\n**Autodiff path drops inner-layer gradients**\n\nHere we treat the inner branch as a constant (`requiresGradient: false`) and never route its accumulated gradient back into `_innerLayer`. As a result, `input.Gradient` only captures the skip-connection contribution and the inner layer’s parameters never receive updates when `UseAutodiff` is true—effectively breaking training for any residual block with a learnable inner layer. Let the autodiff tape track the inner branch and pass its gradient back through `_innerLayer.Backward(...)` (or otherwise delegate to the inner layer’s autodiff builder) before returning the input gradient.\n\n```diff\n-        Autodiff.ComputationNode<T> result;\n+        Autodiff.ComputationNode<T> result;\n+        Autodiff.ComputationNode<T>? innerNode = null;\n...\n-            var innerOutput = _innerLayer.Forward(_lastInput);\n-            var innerNode = Autodiff.TensorOperations<T>.Variable(innerOutput, \"inner_output\", requiresGradient: false);\n+            var innerOutput = _innerLayer.Forward(_lastInput);\n+            innerNode = Autodiff.TensorOperations<T>.Variable(innerOutput, \"inner_output\", requiresGradient: true);\n             // output = input + innerOutput\n             result = Autodiff.TensorOperations<T>.Add(input, innerNode);\n...\n-        return input.Gradient!;\n+        var inputGradient = input.Gradient ?? throw new InvalidOperationException(\"Autodiff failed to populate the input gradient.\");\n+        if (_innerLayer != null)\n+        {\n+            if (innerNode?.Gradient == null)\n+                throw new InvalidOperationException(\"Autodiff failed to populate the inner branch gradient.\");\n+            var innerGradient = _innerLayer.Backward(innerNode.Gradient);\n+            inputGradient = inputGradient.Add(innerGradient);\n+        }\n+        return inputGradient;\n```\n\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","createdAt":"2025-11-11T14:57:24Z","path":"src/NeuralNetworks/Layers/ResidualLayer.cs","line":367,"originalLine":367,"diffHunk":"@@ -297,6 +310,128 @@ public override Tensor<T> Backward(Tensor<T> outputGradient)\n         return combinedGradient.Add(innerGradient);\n     }\n \n+    /// <summary>\n+    /// Backward pass implementation using automatic differentiation.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">The gradient of the loss with respect to the layer's output.</param>\n+    /// <returns>The gradient of the loss with respect to the layer's input.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method uses automatic differentiation to compute gradients. It recreates the forward\n+    /// computation graph for the residual connection (input + inner layer output) and propagates\n+    /// gradients through it.\n+    /// </para>\n+    /// </remarks>\n+    private Tensor<T> BackwardViaAutodiff(Tensor<T> outputGradient)\n+    {\n+        if (_lastInput == null)\n+            throw new InvalidOperationException(\"Forward pass must be called before backward pass.\");\n+\n+        // Convert to computation nodes\n+        var input = Autodiff.TensorOperations<T>.Variable(_lastInput, \"input\", requiresGradient: true);\n+\n+        // Compute inner layer output if present\n+        Autodiff.ComputationNode<T> result;\n+        if (_innerLayer != null)\n+        {\n+            var innerOutput = _innerLayer.Forward(_lastInput);\n+            var innerNode = Autodiff.TensorOperations<T>.Variable(innerOutput, \"inner_output\", requiresGradient: false);\n+            // output = input + innerOutput\n+            result = Autodiff.TensorOperations<T>.Add(input, innerNode);\n+        }\n+        else\n+        {\n+            result = input;\n+        }\n+\n+        // Apply activation using autodiff\n+        var activated = ApplyActivationAutodiff(result);\n+\n+        // Set the gradient at the output\n+        activated.Gradient = outputGradient;\n+\n+        // Perform topological sort and backward pass\n+        var topoOrder = GetTopologicalOrder(activated);\n+\n+        // Execute backward pass in reverse topological order\n+        for (int i = topoOrder.Count - 1; i >= 0; i--)\n+        {\n+            var node = topoOrder[i];\n+            if (node.RequiresGradient && node.BackwardFunction != null && node.Gradient != null)\n+            {\n+                node.BackwardFunction(node.Gradient);\n+            }\n+        }\n+\n+        return input.Gradient!;\n+    }"}]}},{"id":"PRRT_kwDOKSXUF85hlmpl","isResolved":false,"isOutdated":false,"path":"AUTODIFF_HANDOFF.md","line":223,"originalLine":223,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V_izH","author":{"login":"coderabbitai"},"body":"_⚠️ Potential issue_ | _🟡 Minor_\n\n**Specify language for fenced code block to comply with Markdown linting.**\n\nLine 179 starts a fenced code block without a language identifier. Add `csharp` after the opening backticks to match the existing codebase convention.\n\n\n```diff\n-\\```\n+\\```csharp\n /// <summary>\n /// [Operation description]\n /// </summary>\n```\n\n<details>\n<summary>🤖 Prompt for AI Agents</summary>\n\n````\nIn AUTODIFF_HANDOFF.md around lines 179 to 223 the fenced code block is missing\na language identifier; add \"csharp\" immediately after the opening triple\nbackticks on the code block starting at line 179 so the block begins with\n```csharp to satisfy Markdown linting and match the repository convention.\n````\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","createdAt":"2025-11-12T02:20:31Z","path":"AUTODIFF_HANDOFF.md","line":223,"originalLine":223,"diffHunk":"@@ -0,0 +1,335 @@\n+# Autodiff Layer Integration - Session Handoff\n+\n+## Current Status (as of last commit)\n+\n+**Branch:** `claude/autodiff-layer-integration-011CV1K5xX5dTYfGRXKXodCN`\n+\n+### Completed Work\n+\n+**TensorOperations Implemented:** 41 total\n+- Base operations (19): Add, Subtract, Multiply, Divide, MatMul, Transpose, Reshape, ReLU, Sigmoid, Tanh, ElementwiseMultiply, Sum, Mean, Variance, Exp, Log, Pow, Sqrt, Abs\n+- Session additions (22): Conv2D, ConvTranspose2D, MaxPool2D, AvgPool2D, Softmax, Concat, Pad, LayerNorm, BatchNorm, ReduceMax, ReduceMean, Split, Crop, Upsample, PixelShuffle, DilatedConv2D, DepthwiseConv2D, LocallyConnectedConv2D, ReduceLogVariance, RBFKernel, AffineGrid, GridSample\n+\n+**Layers with Full Autodiff:** 32\n+1. DenseLayer\n+2. ActivationLayer\n+3. DropoutLayer\n+4. AddLayer\n+5. MultiplyLayer\n+6. ConvolutionalLayer\n+7. DeconvolutionalLayer\n+8. MaxPoolingLayer\n+9. PoolingLayer\n+10. BatchNormalizationLayer\n+11. LayerNormalizationLayer\n+12. AttentionLayer\n+13. GlobalPoolingLayer\n+14. GaussianNoiseLayer\n+15. MaskingLayer\n+16. SubpixelConvolutionalLayer\n+17. UpsamplingLayer\n+18. DepthwiseSeparableConvolutionalLayer\n+19. CroppingLayer\n+20. SplitLayer\n+21. DilatedConvolutionalLayer\n+22. SeparableConvolutionalLayer\n+23. LocallyConnectedLayer\n+24. LogVarianceLayer\n+25. RBFLayer\n+26. SpatialTransformerLayer\n+27. RepParameterizationLayer\n+28. ReadoutLayer\n+29. ReconstructionLayer\n+30. DecoderLayer\n+31. ExpertLayer\n+32. MixtureOfExpertsLayer\n+\n+### Remaining Work: 11 Layers\n+\n+## ✅ HIGH PRIORITY COMPLETED: Production-Ready Layers (3/3 layers)\n+\n+All high-priority production layers now have full autodiff support:\n+\n+### 1. ✅ SpatialTransformerLayer\n+**Operations Added:** AffineGrid + GridSample\n+- AffineGrid: Generates sampling grid from [batch, 2, 3] affine transformation matrices\n+- GridSample: Bilinear interpolation sampling with gradients for both input and grid\n+- Full gradient support for learnable spatial transformations\n+\n+### 2. ✅ RBFLayer\n+**Operation Added:** RBFKernel\n+- Gaussian RBF computation: exp(-epsilon * distance²)\n+- Gradients computed for input, centers, and epsilon parameters\n+- Supports batch processing with efficient distance computation\n+\n+### 3. ✅ LogVarianceLayer\n+**Operation Added:** ReduceLogVariance\n+- Computes log(variance + epsilon) along specified axis\n+- Full gradient support for variance reduction operations\n+- Numerically stable with configurable epsilon\n+\n+## MEDIUM PRIORITY: Specialized Research Layers (17 layers)\n+\n+These are research-oriented and require complex domain-specific implementations:\n+\n+### Capsule Networks (3 layers)\n+- **CapsuleLayer** - Dynamic routing algorithm\n+- **DigitCapsuleLayer** - Digit capsule routing\n+- **PrimaryCapsuleLayer** - Primary capsule convolutions\n+\n+**Operation Needed:** `DynamicRouting`\n+**Complexity:** ~300 lines\n+**Notes:**\n+- Iterative routing-by-agreement algorithm\n+- Coupling coefficients updated via softmax\n+- Multiple routing iterations\n+- Complex gradient through routing iterations\n+\n+### Quantum Computing (2 layers)\n+- **QuantumLayer** - Quantum gate operations\n+- **MeasurementLayer** - Quantum measurement\n+\n+**Operations Needed:** `QuantumGate`, `QuantumMeasurement`\n+**Complexity:** ~400 lines total\n+**Notes:**\n+- Complex number operations\n+- Unitary matrix constraints\n+- Measurement collapse\n+- May require complex number Tensor support\n+\n+### Conditional Random Fields (1 layer)\n+- **ConditionalRandomFieldLayer** - CRF for structured prediction\n+\n+**Operation Needed:** `ViterbiDecode`, `CRFForwardBackward`\n+**Complexity:** ~500 lines\n+**Notes:**\n+- Forward-backward algorithm\n+- Viterbi decoding\n+- Log-space computations for numerical stability\n+- Transition matrix gradients\n+\n+### Graph Neural Networks (2 layers)\n+- **GraphConvolutionalLayer** - Graph convolution\n+- **SpatialPoolerLayer** - Hierarchical temporal memory\n+\n+**Operations Needed:** `GraphConv`, `MessagePassing`\n+**Complexity:** ~300 lines\n+**Notes:**\n+- Adjacency matrix operations\n+- Message passing between nodes\n+- Aggregation functions\n+- Graph topology gradients\n+\n+### Neuromorphic/Spiking (3 layers)\n+- **SpikingLayer** - Spiking neuron dynamics\n+- **SynapticPlasticityLayer** - STDP learning\n+- **TemporalMemoryLayer** - HTM temporal memory\n+\n+**Operations Needed:** `SpikeDynamics`, `STDP`, `TemporalPooling`\n+**Complexity:** ~600 lines total\n+**Notes:**\n+- Temporal dynamics\n+- Spike-timing dependent plasticity\n+- Non-differentiable spikes (surrogate gradients)\n+- Complex state updates\n+\n+### Other Specialized (6 layers)\n+- **RBMLayer** - Restricted Boltzmann Machine (needs `ContrastiveDivergence`)\n+- **AnomalyDetectorLayer** - Anomaly detection (domain-specific)\n+- **RepParameterizationLayer** - Reparameterization trick (may use existing ops)\n+- **ReadoutLayer** - Reservoir computing readout (may use existing ops)\n+- **DecoderLayer** - Sequence decoder (may use existing ops)\n+- **ExpertLayer** / **MixtureOfExpertsLayer** - Expert routing (may use existing ops)\n+- **ReconstructionLayer** - Autoencoder reconstruction (may use existing ops)\n+\n+**Notes:** Some of these may be implementable with existing operations - needs investigation.\n+\n+## Implementation Strategy\n+\n+### Recommended Order\n+\n+**Phase 1 (Completed):** Production Conv Variants ✅\n+1. ✅ Added `DilatedConv2D` operation\n+2. ✅ Updated DilatedConvolutionalLayer\n+3. ✅ Added `DepthwiseConv2D` operation\n+4. ✅ Updated DepthwiseSeparableConvolutionalLayer\n+5. ✅ Composed from DepthwiseConv2D + Conv2D\n+6. ✅ Updated SeparableConvolutionalLayer\n+\n+**Phase 2 (Completed):** Simple Layers Using Existing Ops ✅\n+1. ✅ Updated CroppingLayer to use Crop\n+2. ✅ Updated SplitLayer to use Reshape (not Split - layer does reshape)\n+3. Investigate and update LogVarianceLayer, RepParameterizationLayer, ReadoutLayer\n+\n+**Phase 3 (Next):** Advanced Production Ops\n+1. Add `LocallyConnectedConv2D` operation\n+2. Update LocallyConnectedLayer\n+3. Add `AffineGrid` + `GridSample` operations\n+4. Update SpatialTransformerLayer\n+5. Add `RBFKernel` operation\n+6. Update RBFLayer\n+\n+**Phase 4:** Research Layers (Optional based on priority)\n+1. Capsule networks if needed by users\n+2. Graph neural networks if needed\n+3. Others as required\n+\n+### Code Pattern for Adding TensorOperations\n+\n+```csharp\n+/// <summary>\n+/// [Operation description]\n+/// </summary>\n+public static ComputationNode<T> OperationName(\n+    ComputationNode<T> input,\n+    /* other parameters */)\n+{\n+    var numOps = MathHelper.GetNumericOperations<T>();\n+    var inputShape = input.Value.Shape;\n+\n+    // Validate inputs\n+    // ...\n+\n+    // Compute output shape\n+    var outputShape = /* ... */;\n+    var result = new Tensor<T>(outputShape);\n+\n+    // Forward pass implementation\n+    // ...\n+\n+    void BackwardFunction(Tensor<T> gradient)\n+    {\n+        if (!input.RequiresGradient) return;\n+\n+        if (input.Gradient == null)\n+            input.Gradient = new Tensor<T>(inputShape);\n+\n+        // Backward pass implementation\n+        // ...\n+    }\n+\n+    var node = new ComputationNode<T>(\n+        value: result,\n+        requiresGradient: input.RequiresGradient,\n+        parents: new List<ComputationNode<T>> { input },\n+        backwardFunction: BackwardFunction,\n+        name: null);\n+\n+    var tape = GradientTape<T>.Current;\n+    if (tape != null && tape.IsRecording)\n+        tape.RecordOperation(node);\n+\n+    return node;\n+}"}]}},{"id":"PRRT_kwDOKSXUF85hlmpr","isResolved":false,"isOutdated":false,"path":"AUTODIFF_HANDOFF.md","line":301,"originalLine":301,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V_izO","author":{"login":"coderabbitai"},"body":"_⚠️ Potential issue_ | _🟡 Minor_\n\n**Specify language for fenced code block to comply with Markdown linting.**\n\nLine 297 starts a fenced code block without a language identifier. Add `bash` after the opening backticks.\n\n\n```diff\n-\\```\n+\\```bash\n branch: claude/autodiff-layer-integration-011CV1K5xX5dTYfGRXKXodCN\n status: Clean, all changes committed and pushed\n```\n\n<details>\n<summary>🧰 Tools</summary>\n\n<details>\n<summary>🪛 markdownlint-cli2 (0.18.1)</summary>\n\n297-297: Fenced code blocks should have a language specified\n\n(MD040, fenced-code-language)\n\n</details>\n\n</details>\n\n<details>\n<summary>🤖 Prompt for AI Agents</summary>\n\n````\nIn AUTODIFF_HANDOFF.md around lines 297 to 301, the fenced code block opening\ncurrently lacks a language identifier; edit the opening fence from ``` to\n```bash so the block reads as a bash code block (leave the closing fence\nunchanged) to satisfy Markdown linting.\n````\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","createdAt":"2025-11-12T02:20:31Z","path":"AUTODIFF_HANDOFF.md","line":301,"originalLine":301,"diffHunk":"@@ -0,0 +1,335 @@\n+# Autodiff Layer Integration - Session Handoff\n+\n+## Current Status (as of last commit)\n+\n+**Branch:** `claude/autodiff-layer-integration-011CV1K5xX5dTYfGRXKXodCN`\n+\n+### Completed Work\n+\n+**TensorOperations Implemented:** 41 total\n+- Base operations (19): Add, Subtract, Multiply, Divide, MatMul, Transpose, Reshape, ReLU, Sigmoid, Tanh, ElementwiseMultiply, Sum, Mean, Variance, Exp, Log, Pow, Sqrt, Abs\n+- Session additions (22): Conv2D, ConvTranspose2D, MaxPool2D, AvgPool2D, Softmax, Concat, Pad, LayerNorm, BatchNorm, ReduceMax, ReduceMean, Split, Crop, Upsample, PixelShuffle, DilatedConv2D, DepthwiseConv2D, LocallyConnectedConv2D, ReduceLogVariance, RBFKernel, AffineGrid, GridSample\n+\n+**Layers with Full Autodiff:** 32\n+1. DenseLayer\n+2. ActivationLayer\n+3. DropoutLayer\n+4. AddLayer\n+5. MultiplyLayer\n+6. ConvolutionalLayer\n+7. DeconvolutionalLayer\n+8. MaxPoolingLayer\n+9. PoolingLayer\n+10. BatchNormalizationLayer\n+11. LayerNormalizationLayer\n+12. AttentionLayer\n+13. GlobalPoolingLayer\n+14. GaussianNoiseLayer\n+15. MaskingLayer\n+16. SubpixelConvolutionalLayer\n+17. UpsamplingLayer\n+18. DepthwiseSeparableConvolutionalLayer\n+19. CroppingLayer\n+20. SplitLayer\n+21. DilatedConvolutionalLayer\n+22. SeparableConvolutionalLayer\n+23. LocallyConnectedLayer\n+24. LogVarianceLayer\n+25. RBFLayer\n+26. SpatialTransformerLayer\n+27. RepParameterizationLayer\n+28. ReadoutLayer\n+29. ReconstructionLayer\n+30. DecoderLayer\n+31. ExpertLayer\n+32. MixtureOfExpertsLayer\n+\n+### Remaining Work: 11 Layers\n+\n+## ✅ HIGH PRIORITY COMPLETED: Production-Ready Layers (3/3 layers)\n+\n+All high-priority production layers now have full autodiff support:\n+\n+### 1. ✅ SpatialTransformerLayer\n+**Operations Added:** AffineGrid + GridSample\n+- AffineGrid: Generates sampling grid from [batch, 2, 3] affine transformation matrices\n+- GridSample: Bilinear interpolation sampling with gradients for both input and grid\n+- Full gradient support for learnable spatial transformations\n+\n+### 2. ✅ RBFLayer\n+**Operation Added:** RBFKernel\n+- Gaussian RBF computation: exp(-epsilon * distance²)\n+- Gradients computed for input, centers, and epsilon parameters\n+- Supports batch processing with efficient distance computation\n+\n+### 3. ✅ LogVarianceLayer\n+**Operation Added:** ReduceLogVariance\n+- Computes log(variance + epsilon) along specified axis\n+- Full gradient support for variance reduction operations\n+- Numerically stable with configurable epsilon\n+\n+## MEDIUM PRIORITY: Specialized Research Layers (17 layers)\n+\n+These are research-oriented and require complex domain-specific implementations:\n+\n+### Capsule Networks (3 layers)\n+- **CapsuleLayer** - Dynamic routing algorithm\n+- **DigitCapsuleLayer** - Digit capsule routing\n+- **PrimaryCapsuleLayer** - Primary capsule convolutions\n+\n+**Operation Needed:** `DynamicRouting`\n+**Complexity:** ~300 lines\n+**Notes:**\n+- Iterative routing-by-agreement algorithm\n+- Coupling coefficients updated via softmax\n+- Multiple routing iterations\n+- Complex gradient through routing iterations\n+\n+### Quantum Computing (2 layers)\n+- **QuantumLayer** - Quantum gate operations\n+- **MeasurementLayer** - Quantum measurement\n+\n+**Operations Needed:** `QuantumGate`, `QuantumMeasurement`\n+**Complexity:** ~400 lines total\n+**Notes:**\n+- Complex number operations\n+- Unitary matrix constraints\n+- Measurement collapse\n+- May require complex number Tensor support\n+\n+### Conditional Random Fields (1 layer)\n+- **ConditionalRandomFieldLayer** - CRF for structured prediction\n+\n+**Operation Needed:** `ViterbiDecode`, `CRFForwardBackward`\n+**Complexity:** ~500 lines\n+**Notes:**\n+- Forward-backward algorithm\n+- Viterbi decoding\n+- Log-space computations for numerical stability\n+- Transition matrix gradients\n+\n+### Graph Neural Networks (2 layers)\n+- **GraphConvolutionalLayer** - Graph convolution\n+- **SpatialPoolerLayer** - Hierarchical temporal memory\n+\n+**Operations Needed:** `GraphConv`, `MessagePassing`\n+**Complexity:** ~300 lines\n+**Notes:**\n+- Adjacency matrix operations\n+- Message passing between nodes\n+- Aggregation functions\n+- Graph topology gradients\n+\n+### Neuromorphic/Spiking (3 layers)\n+- **SpikingLayer** - Spiking neuron dynamics\n+- **SynapticPlasticityLayer** - STDP learning\n+- **TemporalMemoryLayer** - HTM temporal memory\n+\n+**Operations Needed:** `SpikeDynamics`, `STDP`, `TemporalPooling`\n+**Complexity:** ~600 lines total\n+**Notes:**\n+- Temporal dynamics\n+- Spike-timing dependent plasticity\n+- Non-differentiable spikes (surrogate gradients)\n+- Complex state updates\n+\n+### Other Specialized (6 layers)\n+- **RBMLayer** - Restricted Boltzmann Machine (needs `ContrastiveDivergence`)\n+- **AnomalyDetectorLayer** - Anomaly detection (domain-specific)\n+- **RepParameterizationLayer** - Reparameterization trick (may use existing ops)\n+- **ReadoutLayer** - Reservoir computing readout (may use existing ops)\n+- **DecoderLayer** - Sequence decoder (may use existing ops)\n+- **ExpertLayer** / **MixtureOfExpertsLayer** - Expert routing (may use existing ops)\n+- **ReconstructionLayer** - Autoencoder reconstruction (may use existing ops)\n+\n+**Notes:** Some of these may be implementable with existing operations - needs investigation.\n+\n+## Implementation Strategy\n+\n+### Recommended Order\n+\n+**Phase 1 (Completed):** Production Conv Variants ✅\n+1. ✅ Added `DilatedConv2D` operation\n+2. ✅ Updated DilatedConvolutionalLayer\n+3. ✅ Added `DepthwiseConv2D` operation\n+4. ✅ Updated DepthwiseSeparableConvolutionalLayer\n+5. ✅ Composed from DepthwiseConv2D + Conv2D\n+6. ✅ Updated SeparableConvolutionalLayer\n+\n+**Phase 2 (Completed):** Simple Layers Using Existing Ops ✅\n+1. ✅ Updated CroppingLayer to use Crop\n+2. ✅ Updated SplitLayer to use Reshape (not Split - layer does reshape)\n+3. Investigate and update LogVarianceLayer, RepParameterizationLayer, ReadoutLayer\n+\n+**Phase 3 (Next):** Advanced Production Ops\n+1. Add `LocallyConnectedConv2D` operation\n+2. Update LocallyConnectedLayer\n+3. Add `AffineGrid` + `GridSample` operations\n+4. Update SpatialTransformerLayer\n+5. Add `RBFKernel` operation\n+6. Update RBFLayer\n+\n+**Phase 4:** Research Layers (Optional based on priority)\n+1. Capsule networks if needed by users\n+2. Graph neural networks if needed\n+3. Others as required\n+\n+### Code Pattern for Adding TensorOperations\n+\n+```csharp\n+/// <summary>\n+/// [Operation description]\n+/// </summary>\n+public static ComputationNode<T> OperationName(\n+    ComputationNode<T> input,\n+    /* other parameters */)\n+{\n+    var numOps = MathHelper.GetNumericOperations<T>();\n+    var inputShape = input.Value.Shape;\n+\n+    // Validate inputs\n+    // ...\n+\n+    // Compute output shape\n+    var outputShape = /* ... */;\n+    var result = new Tensor<T>(outputShape);\n+\n+    // Forward pass implementation\n+    // ...\n+\n+    void BackwardFunction(Tensor<T> gradient)\n+    {\n+        if (!input.RequiresGradient) return;\n+\n+        if (input.Gradient == null)\n+            input.Gradient = new Tensor<T>(inputShape);\n+\n+        // Backward pass implementation\n+        // ...\n+    }\n+\n+    var node = new ComputationNode<T>(\n+        value: result,\n+        requiresGradient: input.RequiresGradient,\n+        parents: new List<ComputationNode<T>> { input },\n+        backwardFunction: BackwardFunction,\n+        name: null);\n+\n+    var tape = GradientTape<T>.Current;\n+    if (tape != null && tape.IsRecording)\n+        tape.RecordOperation(node);\n+\n+    return node;\n+}\n+```\n+\n+### Code Pattern for Updating Layers\n+\n+```csharp\n+private Tensor<T> BackwardViaAutodiff(Tensor<T> outputGradient)\n+{\n+    if (_lastInput == null)\n+        throw new InvalidOperationException(\"Forward pass must be called before backward pass.\");\n+\n+    // Convert to computation nodes\n+    var inputNode = Autodiff.TensorOperations<T>.Variable(_lastInput, \"input\", requiresGradient: true);\n+\n+    // Apply TensorOperation\n+    var outputNode = Autodiff.TensorOperations<T>.OperationName(inputNode, /* params */);\n+\n+    // Backward pass\n+    outputNode.Gradient = outputGradient;\n+    var topoOrder = GetTopologicalOrder(outputNode);\n+    for (int i = topoOrder.Count - 1; i >= 0; i--)\n+    {\n+        var node = topoOrder[i];\n+        if (node.RequiresGradient && node.BackwardFunction != null && node.Gradient != null)\n+        {\n+            node.BackwardFunction(node.Gradient);\n+        }\n+    }\n+\n+    return inputNode.Gradient ?? throw new InvalidOperationException(\"Gradient computation failed.\");\n+}\n+\n+// Helper method (copy from other layers if needed)\n+private List<Autodiff.ComputationNode<T>> GetTopologicalOrder(Autodiff.ComputationNode<T> root)\n+{\n+    // Standard topological sort implementation\n+    // ...\n+}\n+```\n+\n+## Testing\n+\n+After implementing each operation:\n+1. Add test to `tests/AiDotNet.Tests/UnitTests/Autodiff/GradientCorrectnessTests.cs`\n+2. Compare autodiff gradients vs numerical gradients\n+3. Use tolerance of 1e-4 for float, 1e-6 for double\n+\n+Example test structure:\n+```csharp\n+[Fact]\n+public void OperationName_AutodiffGradients_MatchNumericalGradients()\n+{\n+    // Setup\n+    var input = CreateTestTensor();\n+    var inputNode = TensorOperations<float>.Variable(input, \"input\", requiresGradient: true);\n+\n+    // Forward\n+    var output = TensorOperations<float>.OperationName(inputNode, /* params */);\n+\n+    // Backward\n+    var outputGrad = CreateTestGradient();\n+    output.Gradient = outputGrad;\n+    output.Backward();\n+\n+    // Numerical gradient\n+    var numerical = ComputeNumericalGradient(input, /* ... */);\n+\n+    // Assert\n+    AssertGradientsMatch(inputNode.Gradient, numerical, tolerance: 1e-4);\n+}\n+```\n+\n+## Current Branch Status\n+\n+```\n+branch: claude/autodiff-layer-integration-011CV1K5xX5dTYfGRXKXodCN\n+status: Clean, all changes committed and pushed\n+latest commit: \"feat: Update UpsamplingLayer to use Upsample operation\"\n+```"}]}},{"id":"PRRT_kwDOKSXUF85hlmpt","isResolved":false,"isOutdated":false,"path":"docs/AutodiffImplementation.md","line":273,"originalLine":273,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V_izR","author":{"login":"coderabbitai"},"body":"_⚠️ Potential issue_ | _🟡 Minor_\n\n**Specify language for fenced code block to comply with Markdown linting.**\n\nLine 273 starts a fenced code block without a language identifier. Add `bash` after the opening backticks.\n\n\n```diff\n-\\```\n+\\```bash\n dotnet test --filter \"FullyQualifiedName~GradientCorrectnessTests\"\n```\n\n<details>\n<summary>🧰 Tools</summary>\n\n<details>\n<summary>🪛 markdownlint-cli2 (0.18.1)</summary>\n\n273-273: Fenced code blocks should have a language specified\n\n(MD040, fenced-code-language)\n\n</details>\n\n</details>\n\n<details>\n<summary>🤖 Prompt for AI Agents</summary>\n\n````\nIn docs/AutodiffImplementation.md around line 273 the fenced code block is\nmissing a language specifier; update the opening backticks to include \"bash\"\n(i.e. change ``` to ```bash) so the code block is flagged as Bash for Markdown\nlinting and syntax highlighting.\n````\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","createdAt":"2025-11-12T02:20:31Z","path":"docs/AutodiffImplementation.md","line":273,"originalLine":273,"diffHunk":"@@ -0,0 +1,448 @@\n+# Automatic Differentiation Implementation Status\n+\n+## Overview\n+\n+This document tracks the implementation status of automatic differentiation (autodiff) support across all neural network layers in AiDotNet.\n+\n+**Last Updated:** 2025-01-11\n+**Total Layers:** 75\n+**Layers with Autodiff Infrastructure:** 75 (100%)\n+**Layers with Full Autodiff Support:** 32 core layers (43%)\n+**TensorOperations Implemented:** 41 (19 base + 22 new: Conv2D, ConvTranspose2D, MaxPool2D, AvgPool2D, Softmax, Concat, Pad, LayerNorm, BatchNorm, ReduceMax, ReduceMean, Split, Crop, Upsample, PixelShuffle, DilatedConv2D, DepthwiseConv2D, LocallyConnectedConv2D, ReduceLogVariance, RBFKernel, AffineGrid, GridSample)\n+**Higher-Order Gradients:** ✅ Fully supported via GradientTape.Gradient(createGraph: true)\n+**Graph Caching Optimization:** ✅ Automatic for persistent tapes\n+\n+## Implementation Status\n+\n+### ✅ Fully Implemented (Working Autodiff)\n+\n+These layers have complete autodiff support using TensorOperations:\n+\n+1. **DenseLayer** - Matrix multiply, add, activation functions\n+2. **ActivationLayer** - ReLU, Sigmoid, Tanh\n+3. **DropoutLayer** - Stochastic masking\n+4. **AddLayer** - Element-wise addition\n+5. **MultiplyLayer** - Element-wise multiplication\n+6. **ConvolutionalLayer** - Conv2D operation with activation support\n+7. **DeconvolutionalLayer** - ConvTranspose2D operation with activation support\n+8. **MaxPoolingLayer** - MaxPool2D operation with gradient routing\n+9. **PoolingLayer** - MaxPool2D and AvgPool2D operations\n+10. **BatchNormalizationLayer** - BatchNorm operation with training/inference modes\n+11. **LayerNormalizationLayer** - LayerNorm operation\n+12. **AttentionLayer** - Softmax operation for attention weights\n+13. **GlobalPoolingLayer** - ReduceMax and ReduceMean operations\n+14. **GaussianNoiseLayer** - Identity gradient (noise is independent)\n+15. **MaskingLayer** - ElementwiseMultiply for masking operation\n+16. **SubpixelConvolutionalLayer** - PixelShuffle operation for depth-to-space\n+17. **UpsamplingLayer** - Upsample operation for nearest-neighbor upsampling\n+18. **DepthwiseSeparableConvolutionalLayer** - DepthwiseConv2D + Conv2D operations\n+19. **CroppingLayer** - Crop operation for spatial cropping\n+20. **SplitLayer** - Reshape operation for tensor splitting\n+21. **DilatedConvolutionalLayer** - DilatedConv2D operation with dilation support\n+22. **SeparableConvolutionalLayer** - DepthwiseConv2D + Conv2D composition\n+23. **LocallyConnectedLayer** - LocallyConnectedConv2D operation with position-specific weights\n+24. **LogVarianceLayer** - ReduceLogVariance operation for log-variance computation\n+25. **RBFLayer** - RBFKernel operation for Gaussian RBF activations\n+26. **SpatialTransformerLayer** - AffineGrid + GridSample operations for learnable spatial transformations\n+27. **RepParameterizationLayer** - VAE reparameterization using Exp, Multiply, Add operations\n+28. **ReadoutLayer** - MatMul and Add operations for output mapping\n+29. **ReconstructionLayer** - Composite of three FullyConnectedLayers\n+30. **DecoderLayer** - Composite Transformer decoder with attention and normalization\n+31. **ExpertLayer** - Composite expert module for MoE architectures\n+32. **MixtureOfExpertsLayer** - Sparse MoE with expert routing and combination\n+\n+### 🔄 Partial Implementation (Infrastructure Ready)\n+\n+These layers have the autodiff pattern implemented but fall back to manual gradients due to missing TensorOperations or complexity:\n+\n+**Attention & Transformer Layers:**\n+- SelfAttentionLayer (complex multi-head attention, delegates to manual)\n+- MultiHeadAttentionLayer (complex multi-head attention, delegates to manual)\n+- TransformerEncoderLayer (composite layer)\n+- TransformerDecoderLayer (composite layer)\n+- PositionalEncodingLayer\n+- PatchEmbeddingLayer\n+\n+**Convolutional Layers (Need Specialized Operations):**\n+- SeparableConvolutionalLayer (needs depthwise conv operation)\n+- DepthwiseSeparableConvolutionalLayer (needs depthwise conv operation)\n+- DilatedConvolutionalLayer (needs dilated conv operation)\n+- SubpixelConvolutionalLayer (needs pixel shuffle operation)\n+- LocallyConnectedLayer (needs locally connected operation)\n+\n+**Recurrent Layers:**\n+- LSTMLayer (manual BPTT preserved for numerical stability)\n+- GRULayer (manual BPTT preserved for numerical stability)\n+- RecurrentLayer\n+- ConvLSTMLayer\n+- MemoryReadLayer\n+- MemoryWriteLayer\n+\n+**Specialized Research Layers (Manual by Design):**\n+\n+The following layers use manual gradient implementations by design, as they require highly specialized operations that would only be used by these specific layers:\n+\n+- **Capsule Networks:** CapsuleLayer, DigitCapsuleLayer, PrimaryCapsuleLayer (dynamic routing algorithms)\n+- **Structured Prediction:** ConditionalRandomFieldLayer (Viterbi decoding, CRF inference)\n+- **Quantum Computing:** QuantumLayer, MeasurementLayer (quantum state operations)\n+- **Graph Neural Networks:** GraphConvolutionalLayer, SpatialPoolerLayer (graph convolution, message passing)\n+- **Neuromorphic:** SpikingLayer, SynapticPlasticityLayer, TemporalMemoryLayer (spiking dynamics)\n+- **Specialized Architectures:** RBMLayer, AnomalyDetectorLayer, RepParameterizationLayer\n+- **Utility Layers:** ReadoutLayer, DecoderLayer, ExpertLayer, MixtureOfExpertsLayer, ReconstructionLayer\n+\n+These layers have working, optimized manual implementations. Adding TensorOperations for them would create maintenance burden for single-use operations.\n+\n+## Missing TensorOperations\n+\n+To achieve full autodiff support across all layers, the following operations need to be added to `TensorOperations.cs`:\n+\n+### High Priority\n+\n+1. ✅ **Softmax** - IMPLEMENTED\n+   - Used by: AttentionLayer, MultiHeadAttentionLayer, SelfAttentionLayer\n+   - Status: Ready for integration into attention layers\n+\n+2. ✅ **Conv2D** - IMPLEMENTED\n+   - Used by: ConvolutionalLayer, DepthwiseSeparableConvolutionalLayer\n+   - Status: Full implementation with stride, padding, bias support\n+   - Features: Correct forward/backward for input, kernel, and bias\n+\n+3. ✅ **ConvTranspose2D** - IMPLEMENTED\n+   - Used by: DeconvolutionalLayer, SubpixelConvolutionalLayer\n+   - Status: Full implementation with stride, padding, outputPadding support\n+   - Features: Upsampling convolution for GANs, segmentation, super-resolution\n+\n+### Medium Priority\n+\n+4. ✅ **MaxPool2D** - IMPLEMENTED\n+   - Used by: MaxPoolingLayer, PoolingLayer\n+   - Status: Ready for integration into pooling layers\n+\n+5. ✅ **AvgPool2D** - IMPLEMENTED\n+   - Used by: PoolingLayer, GlobalPoolingLayer\n+   - Status: Ready for integration into pooling layers\n+\n+6. ✅ **LayerNorm** - IMPLEMENTED\n+   - Used by: LayerNormalizationLayer\n+   - Status: Ready for integration, supports learnable gamma/beta\n+   - Features: Per-sample normalization, no batch dependency\n+\n+7. ✅ **BatchNorm** - IMPLEMENTED\n+   - Used by: BatchNormalizationLayer\n+   - Status: Ready for integration, supports train/inference modes\n+   - Features: Batch statistics, running mean/variance, learnable gamma/beta\n+\n+### Low Priority\n+\n+8. ✅ **Concat** - IMPLEMENTED - Tensor concatenation along axis\n+9. **Split** - Tensor splitting\n+10. ✅ **Pad** - IMPLEMENTED - Tensor padding operations with constant values\n+11. **Gather** - Advanced indexing\n+12. **Scatter** - Scatter operations\n+\n+### Specialized Operations (As-Needed)\n+\n+The following operations are required for specialized research layers and can be implemented on demand:\n+\n+- **DynamicRouting** - For capsule networks\n+- **ViterbiDecode** - For CRF layers\n+- **RBFKernel** - For RBF layers\n+- **QuantumCircuit** - For quantum layers\n+- **SpatialTransform** - For spatial transformer networks\n+- **SparseDot** - For graph neural networks\n+\n+## Current TensorOperations Support\n+\n+All 24 operations currently implemented in `src/Autodiff/TensorOperations.cs`:\n+\n+✅ **Basic Arithmetic (6 operations):**\n+1. Add - Element-wise addition with broadcasting\n+2. Subtract - Element-wise subtraction with broadcasting\n+3. Negate - Unary negation\n+4. ElementwiseMultiply - Hadamard product with broadcasting\n+5. Divide - Element-wise division with broadcasting\n+6. Power - Exponentiation (x^n)\n+\n+✅ **Matrix Operations (2 operations):**\n+7. MatrixMultiply - Matrix/tensor multiplication\n+8. Transpose - Matrix/tensor transpose\n+\n+✅ **Reduction Operations (2 operations):**\n+9. Sum - Sum along specified axes with keepDims support\n+10. Mean - Average along all dimensions\n+\n+✅ **Activation Functions (4 operations):**\n+11. ReLU - Rectified Linear Unit\n+12. Sigmoid - Logistic sigmoid\n+13. Tanh - Hyperbolic tangent\n+14. Softmax - Softmax activation (for attention and classification)\n+\n+✅ **Tensor Manipulation (3 operations):**\n+15. Reshape - Change tensor shape\n+16. Concat - Concatenate tensors along axis\n+17. Pad - Pad tensors with constant values\n+\n+✅ **Pooling Operations (2 operations):**\n+18. MaxPool2D - 2D max pooling with gradient routing\n+19. AvgPool2D - 2D average pooling\n+\n+✅ **Normalization Operations (2 operations):**\n+20. LayerNorm - Layer normalization (per-sample normalization)\n+21. BatchNorm - Batch normalization (across-batch normalization)\n+\n+✅ **Convolutional Operations (2 operations):**\n+22. Conv2D - 2D convolution with stride, padding, and bias\n+23. ConvTranspose2D - Transposed convolution (deconvolution) for upsampling\n+\n+✅ **Advanced Math (3 operations):**\n+24. Exp - Exponential function\n+25. Log - Natural logarithm\n+26. Sqrt - Square root\n+\n+✅ **Utility (2 operations):**\n+27. Variable - Create differentiable variable node\n+28. Constant - Create non-differentiable constant node\n+\n+**All operations support:**\n+- Automatic gradient computation via backward functions\n+- Broadcasting semantics where applicable\n+- Integration with GradientTape for graph building\n+- Higher-order gradients (gradients of gradients)\n+\n+## Usage Guidelines\n+\n+### For Library Users\n+\n+**Default (Recommended):**\n+```csharp\n+// UseAutodiff defaults to false\n+var layer = new DenseLayer<float>(inputSize, outputSize);\n+// Uses fast optimized manual backward pass\n+```\n+\n+**For Research/Verification:**\n+```csharp\n+var layer = new DenseLayer<float>(inputSize, outputSize);\n+layer.UseAutodiff = true; // Enable autodiff\n+// Uses automatic differentiation (slower but verifiable)\n+```\n+\n+### For Layer Developers\n+\n+When implementing a new layer:\n+\n+1. **Implement manual backward pass first** (for performance)\n+2. **Add autodiff support using this pattern:**\n+\n+```csharp\n+public override Tensor<T> Backward(Tensor<T> outputGradient)\n+{\n+    if (UseAutodiff)\n+        return BackwardViaAutodiff(outputGradient);\n+    else\n+        return BackwardManual(outputGradient);\n+}\n+\n+private Tensor<T> BackwardManual(Tensor<T> outputGradient)\n+{\n+    // Original optimized manual implementation\n+}\n+\n+private Tensor<T> BackwardViaAutodiff(Tensor<T> outputGradient)\n+{\n+    // If required TensorOperations available:\n+    // - Recreate forward pass using TensorOperations\n+    // - Set output gradient\n+    // - Perform topological sort and backward pass\n+    // - Extract and return gradients\n+\n+    // Otherwise:\n+    // TODO: Operation not yet available in TensorOperations\n+    return BackwardManual(outputGradient);\n+}\n+\n+private List<Autodiff.ComputationNode<T>> GetTopologicalOrder(...)\n+{\n+    // Standard topological sort implementation\n+}\n+```\n+\n+## Testing\n+\n+Gradient correctness tests are available in:\n+```"}]}},{"id":"PRRT_kwDOKSXUF85hlmp2","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/CroppingLayer.cs","line":397,"originalLine":397,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V_izc","author":{"login":"coderabbitai"},"body":"_⚠️ Potential issue_ | _🔴 Critical_\n\n<details>\n<summary>🧩 Analysis chain</summary>\n\n**Verify crop array indices are valid, or add validation.**\n\nThe code assumes `_cropTop`, `_cropBottom`, `_cropLeft`, and `_cropRight` arrays have at least indices `[1]` and `[2]`, but there's no validation in the constructor or this method. If these arrays are too short, this line will throw `IndexOutOfBoundsException`.\n\n\n\nRun the following script to verify if crop array validation exists elsewhere or if constructor usage patterns guarantee valid indices:\n\n\n\nIf validation is missing, consider adding checks in the constructor:\n\n```diff\n public CroppingLayer(\n     int[] inputShape,\n     int[] cropTop,\n     int[] cropBottom,\n     int[] cropLeft,\n     int[] cropRight,\n     IActivationFunction<T>? scalarActivation = null)\n     : base(inputShape, CalculateOutputShape(inputShape, cropTop, cropBottom, cropLeft, cropRight), scalarActivation ?? new IdentityActivation<T>())\n {\n+    if (cropTop.Length < 2 || cropBottom.Length < 2 || cropLeft.Length < 3 || cropRight.Length < 3)\n+        throw new ArgumentException(\"Crop arrays must have sufficient length for 4D NHWC tensors.\");\n+    \n     _cropTop = cropTop;\n     _cropBottom = cropBottom;\n     _cropLeft = cropLeft;\n     _cropRight = cropRight;\n }\n```\n\n---\n\n🏁 Script executed:\n\n```shell\n#!/bin/bash\n# Search for CroppingLayer constructor calls and validation\nrg -nP -C5 'new\\s+CroppingLayer<' --type=cs\n\n# Search for validation of crop arrays in constructor or elsewhere\nast-grep --pattern $'class CroppingLayer<$_> {\n  $$$\n  public CroppingLayer($$$) {\n    $$$\n  }\n  $$$\n}'\n```\n\nLength of output: 0\n\n---\n\nThe initial search produced no results. Let me verify the file structure and examine the CroppingLayer constructor directly.\n\n---\n\n🏁 Script executed:\n\n```shell\n#!/bin/bash\n# Find the CroppingLayer.cs file\nfd -t f \"CroppingLayer.cs\" --type f\n\n# If found, examine the constructor and crop array usage\nif [ -f \"src/NeuralNetworks/Layers/CroppingLayer.cs\" ]; then\n  echo \"=== Constructor and crop array initialization ===\"\n  cat \"src/NeuralNetworks/Layers/CroppingLayer.cs\" | head -100\n  \n  echo -e \"\\n=== Searching for crop array access patterns ===\"\n  rg -n \"cropTop\\[|cropBottom\\[|cropLeft\\[|cropRight\\[\" \"src/NeuralNetworks/Layers/CroppingLayer.cs\"\nfi\n```\n\nLength of output: 5932\n\n---\n\nI can see the crop array access patterns. Let me examine the constructor and the CalculateOutputShape method to check for validation:\n\n---\n\n🏁 Script executed:\n\n```shell\n#!/bin/bash\n# Extract constructor and CalculateOutputShape method\nrg -n -A30 \"public CroppingLayer<T>\" \"src/NeuralNetworks/Layers/CroppingLayer.cs\"\n\necho -e \"\\n=== CalculateOutputShape method ===\"\nrg -n -A15 \"private static int\\[\\] CalculateOutputShape\" \"src/NeuralNetworks/Layers/CroppingLayer.cs\"\n```\n\nLength of output: 741\n\n---\n\nLet me search for the full constructor definition more broadly:\n\n---\n\n🏁 Script executed:\n\n```shell\n#!/bin/bash\n# Search for constructor with different patterns\nrg -n -B2 -A20 \"CroppingLayer\\(\" \"src/NeuralNetworks/Layers/CroppingLayer.cs\" | head -80\n\necho -e \"\\n=== Check for any validation or throw statements ===\"\nrg -n \"throw|ArgumentException|ArgumentOutOfRange\" \"src/NeuralNetworks/Layers/CroppingLayer.cs\"\n```\n\nLength of output: 2529\n\n---\n\nLet me check how CroppingLayer is instantiated to see if validation happens at the call site:\n\n---\n\n🏁 Script executed:\n\n```shell\n#!/bin/bash\n# Search for CroppingLayer instantiation patterns across codebase\nrg -n \"new\\s+CroppingLayer\" --type=cs | head -20\n\n# Also check if there's any factory method or validation wrapper\nrg -n \"CroppingLayer<\" --type=cs -B2 -A2 | grep -E \"(factory|create|validate)\" -i -B2 -A2\n```\n\nLength of output: 0\n\n---\n\n\n\n</details>\n\n**Add explicit validation for crop array lengths in the constructor.**\n\nBased on my verification, the constructor (lines 163-176 and 206-219) has no explicit validation of crop array lengths. While `CalculateOutputShape` implicitly validates by accessing all indices, this approach:\n\n1. **No clear error message** – If crop arrays are too short, an `IndexOutOfRangeException` is thrown instead of a helpful `ArgumentException`.\n2. **Late validation** – The error occurs during method call, not at construction time.\n3. **Hardcoded indices at line 397** – The code assumes indices `[1]` and `[2]` exist, requiring at least 3-element arrays for 4D tensors (NHWC format).\n\nThe suggested validation in the review comment is appropriate and improves code robustness by providing explicit, early error detection with a descriptive message.\n\n<details>\n<summary>🤖 Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/Layers/CroppingLayer.cs around line 397, the code assumes\ncrop arrays contain at least three elements (accessing indices [1] and [2])\nwhich can cause IndexOutOfRangeException later; add explicit validation in the\nconstructor(s) to check that each provided crop array (_cropTop, _cropBottom,\n_cropLeft, _cropRight) is not null and has Length >= 3 (or the minimum required\nfor your tensor format), and if not throw an ArgumentException with a clear\nmessage identifying which array is invalid and the expected length; perform\nthese checks immediately in the constructor(s) so callers get early, descriptive\nerrors instead of runtime index exceptions.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","createdAt":"2025-11-12T02:20:31Z","path":"src/NeuralNetworks/Layers/CroppingLayer.cs","line":397,"originalLine":397,"diffHunk":"@@ -348,6 +363,175 @@ public override Tensor<T> Backward(Tensor<T> outputGradient)\n         }\n         return ApplyActivationDerivative(inputGradient, outputGradient);\n     }\n+\n+    /// <summary>\n+    /// Stores the last input for use in autodiff backward pass.\n+    /// </summary>\n+    private Tensor<T>? _lastInput;\n+\n+    /// <summary>\n+    /// Backward pass implementation using automatic differentiation.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">The gradient of the loss with respect to the layer's output.</param>\n+    /// <returns>The gradient of the loss with respect to the layer's input.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method uses automatic differentiation to compute gradients using the Crop operation.\n+    /// The layer uses NHWC format [batch, H, W, channels], while TensorOperations uses NCHW format,\n+    /// so format conversion is performed.\n+    /// </para>\n+    /// </remarks>\n+    private Tensor<T> BackwardViaAutodiff(Tensor<T> outputGradient)\n+    {\n+        if (_lastInput == null)\n+            throw new InvalidOperationException(\"Forward pass must be called before backward pass.\");\n+\n+        // Convert from NHWC [batch, H, W, channels] to NCHW [batch, channels, H, W]\n+        var inputNCHW = ConvertNHWCtoNCHW(_lastInput);\n+\n+        // Create computation node\n+        var inputNode = Autodiff.TensorOperations<T>.Variable(inputNCHW, \"input\", requiresGradient: true);\n+\n+        // Apply crop operation\n+        // Crop expects [top, bottom, left, right] for 4D tensors in NCHW format\n+        var cropping = new int[] { _cropTop[1], _cropBottom[1], _cropLeft[2], _cropRight[2] };"}]}},{"id":"PRRT_kwDOKSXUF85hlmp5","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/CroppingLayer.cs","line":534,"originalLine":534,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V_izg","author":{"login":"coderabbitai"},"body":"_🛠️ Refactor suggestion_ | _🟠 Major_\n\n**Extract duplicated helper methods into shared utilities.**\n\nBoth `ApplyActivationAutodiff` and `GetTopologicalOrder` are duplicated across multiple layer files (including ReadoutLayer, SplitLayer, and now CroppingLayer). This violates the DRY principle and increases maintenance burden.\n\n\n\nConsider refactoring these methods into a shared location:\n\n**Option 1: Create a shared utility class**\n```csharp\n// src/NeuralNetworks/Layers/AutodiffHelpers.cs\ninternal static class AutodiffHelpers<T>\n{\n    public static ComputationNode<T> ApplyActivation(\n        ComputationNode<T> input,\n        IActivationFunction<T>? scalarActivation,\n        IVectorActivationFunction<T>? vectorActivation,\n        bool usingVectorActivation)\n    {\n        // Consolidated implementation\n    }\n\n    public static List<ComputationNode<T>> GetTopologicalOrder(ComputationNode<T> root)\n    {\n        // Consolidated implementation\n    }\n}\n```\n\n**Option 2: Add protected methods to LayerBase<T>**\n```csharp\n// In LayerBase<T>\nprotected ComputationNode<T> ApplyActivationAutodiff(ComputationNode<T> input) { ... }\nprotected List<ComputationNode<T>> GetTopologicalOrder(ComputationNode<T> root) { ... }\n```\n\nThis would eliminate duplication and ensure consistent behavior across all layers using autodiff.\n\n<!-- This is an auto-generated comment by CodeRabbit -->","createdAt":"2025-11-12T02:20:32Z","path":"src/NeuralNetworks/Layers/CroppingLayer.cs","line":534,"originalLine":534,"diffHunk":"@@ -348,6 +363,175 @@ public override Tensor<T> Backward(Tensor<T> outputGradient)\n         }\n         return ApplyActivationDerivative(inputGradient, outputGradient);\n     }\n+\n+    /// <summary>\n+    /// Stores the last input for use in autodiff backward pass.\n+    /// </summary>\n+    private Tensor<T>? _lastInput;\n+\n+    /// <summary>\n+    /// Backward pass implementation using automatic differentiation.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">The gradient of the loss with respect to the layer's output.</param>\n+    /// <returns>The gradient of the loss with respect to the layer's input.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method uses automatic differentiation to compute gradients using the Crop operation.\n+    /// The layer uses NHWC format [batch, H, W, channels], while TensorOperations uses NCHW format,\n+    /// so format conversion is performed.\n+    /// </para>\n+    /// </remarks>\n+    private Tensor<T> BackwardViaAutodiff(Tensor<T> outputGradient)\n+    {\n+        if (_lastInput == null)\n+            throw new InvalidOperationException(\"Forward pass must be called before backward pass.\");\n+\n+        // Convert from NHWC [batch, H, W, channels] to NCHW [batch, channels, H, W]\n+        var inputNCHW = ConvertNHWCtoNCHW(_lastInput);\n+\n+        // Create computation node\n+        var inputNode = Autodiff.TensorOperations<T>.Variable(inputNCHW, \"input\", requiresGradient: true);\n+\n+        // Apply crop operation\n+        // Crop expects [top, bottom, left, right] for 4D tensors in NCHW format\n+        var cropping = new int[] { _cropTop[1], _cropBottom[1], _cropLeft[2], _cropRight[2] };\n+        var outputNode = Autodiff.TensorOperations<T>.Crop(inputNode, cropping);\n+\n+        // Apply activation if needed (usually Identity for cropping layers)\n+        outputNode = ApplyActivationAutodiff(outputNode);\n+\n+        // Convert output gradient from NHWC to NCHW\n+        var outputGradientNCHW = ConvertNHWCtoNCHW(outputGradient);\n+\n+        // Perform backward pass\n+        outputNode.Gradient = outputGradientNCHW;\n+        var topoOrder = GetTopologicalOrder(outputNode);\n+        for (int i = topoOrder.Count - 1; i >= 0; i--)\n+        {\n+            var node = topoOrder[i];\n+            if (node.RequiresGradient && node.BackwardFunction != null && node.Gradient != null)\n+            {\n+                node.BackwardFunction(node.Gradient);\n+            }\n+        }\n+\n+        // Convert input gradient from NCHW back to NHWC\n+        var inputGradientNCHW = inputNode.Gradient ?? throw new InvalidOperationException(\"Gradient computation failed.\");\n+        return ConvertNCHWtoNHWC(inputGradientNCHW);\n+    }\n+\n+    /// <summary>\n+    /// Converts tensor from NHWC [batch, H, W, channels] to NCHW [batch, channels, H, W] format.\n+    /// </summary>\n+    private Tensor<T> ConvertNHWCtoNCHW(Tensor<T> nhwc)\n+    {\n+        int batch = nhwc.Shape[0];\n+        int height = nhwc.Shape[1];\n+        int width = nhwc.Shape[2];\n+        int channels = nhwc.Shape[3];\n+\n+        var nchw = new Tensor<T>([batch, channels, height, width]);\n+        for (int b = 0; b < batch; b++)\n+            for (int c = 0; c < channels; c++)\n+                for (int h = 0; h < height; h++)\n+                    for (int w = 0; w < width; w++)\n+                        nchw[b, c, h, w] = nhwc[b, h, w, c];\n+\n+        return nchw;\n+    }\n+\n+    /// <summary>\n+    /// Converts tensor from NCHW [batch, channels, H, W] to NHWC [batch, H, W, channels] format.\n+    /// </summary>\n+    private Tensor<T> ConvertNCHWtoNHWC(Tensor<T> nchw)\n+    {\n+        int batch = nchw.Shape[0];\n+        int channels = nchw.Shape[1];\n+        int height = nchw.Shape[2];\n+        int width = nchw.Shape[3];\n+\n+        var nhwc = new Tensor<T>([batch, height, width, channels]);\n+        for (int b = 0; b < batch; b++)\n+            for (int h = 0; h < height; h++)\n+                for (int w = 0; w < width; w++)\n+                    for (int c = 0; c < channels; c++)\n+                        nhwc[b, h, w, c] = nchw[b, c, h, w];\n+\n+        return nhwc;\n+    }\n+\n+    /// <summary>\n+    /// Applies activation function using autodiff operations.\n+    /// </summary>\n+    private Autodiff.ComputationNode<T> ApplyActivationAutodiff(Autodiff.ComputationNode<T> input)\n+    {\n+        // Apply the appropriate activation function\n+        if (UsingVectorActivation)\n+        {\n+            if (VectorActivation is IdentityActivation<T>)\n+                return input; // Identity: no operation needed\n+            else if (VectorActivation is ReLUActivation<T>)\n+                return Autodiff.TensorOperations<T>.ReLU(input);\n+            else if (VectorActivation is SigmoidActivation<T>)\n+                return Autodiff.TensorOperations<T>.Sigmoid(input);\n+            else if (VectorActivation is TanhActivation<T>)\n+                return Autodiff.TensorOperations<T>.Tanh(input);\n+            else\n+                throw new NotSupportedException($\"Activation {VectorActivation.GetType().Name} not yet supported in autodiff\");\n+        }\n+        else\n+        {\n+            if (ScalarActivation is IdentityActivation<T>)\n+                return input; // Identity: no operation needed\n+            else if (ScalarActivation is ReLUActivation<T>)\n+                return Autodiff.TensorOperations<T>.ReLU(input);\n+            else if (ScalarActivation is SigmoidActivation<T>)\n+                return Autodiff.TensorOperations<T>.Sigmoid(input);\n+            else if (ScalarActivation is TanhActivation<T>)\n+                return Autodiff.TensorOperations<T>.Tanh(input);\n+            else\n+                throw new NotSupportedException($\"Activation {ScalarActivation.GetType().Name} not yet supported in autodiff\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Gets the topological order of nodes in the computation graph.\n+    /// </summary>\n+    /// <param name=\"root\">The root node of the computation graph.</param>\n+    /// <returns>A list of nodes in topological order.</returns>\n+    private List<Autodiff.ComputationNode<T>> GetTopologicalOrder(Autodiff.ComputationNode<T> root)\n+    {\n+        var visited = new HashSet<Autodiff.ComputationNode<T>>();\n+        var result = new List<Autodiff.ComputationNode<T>>();\n+\n+        var stack = new Stack<(Autodiff.ComputationNode<T> node, bool processed)>();\n+        stack.Push((root, false));\n+\n+        while (stack.Count > 0)\n+        {\n+            var (node, processed) = stack.Pop();\n+\n+            if (visited.Contains(node))\n+                continue;\n+\n+            if (processed)\n+            {\n+                visited.Add(node);\n+                result.Add(node);\n+            }\n+            else\n+            {\n+                stack.Push((node, true));\n+                foreach (var parent in node.Parents)\n+                {\n+                    if (!visited.Contains(parent))\n+                        stack.Push((parent, false));\n+                }\n+            }\n+        }\n+\n+        return result;\n+    }"}]}},{"id":"PRRT_kwDOKSXUF85hlmp-","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/DepthwiseSeparableConvolutionalLayer.cs","line":1032,"originalLine":1032,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V_izl","author":{"login":"coderabbitai"},"body":"_⚠️ Potential issue_ | _🟡 Minor_\n\n**Remove redundant if-else block—both branches are identical.**\n\nBoth branches of this conditional execute `ApplyActivationAutodiff(pointwiseOutput)`, making the `UsingVectorActivation` check unnecessary. The comment \"For vector activations, we need to handle them differently\" suggests intent that wasn't implemented.\n\n\n\nApply this diff:\n\n```diff\n-        // Apply activation function\n-        Autodiff.ComputationNode<T> activatedOutput;\n-        if (UsingVectorActivation)\n-        {\n-            // For vector activations, we need to handle them differently\n-            // For now, approximate with element-wise operations\n-            activatedOutput = ApplyActivationAutodiff(pointwiseOutput);\n-        }\n-        else\n-        {\n-            activatedOutput = ApplyActivationAutodiff(pointwiseOutput);\n-        }\n+        // Apply activation function\n+        var activatedOutput = ApplyActivationAutodiff(pointwiseOutput);\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>🤖 Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/Layers/DepthwiseSeparableConvolutionalLayer.cs around\nlines 1023 to 1032, remove the redundant if-else that calls\nApplyActivationAutodiff(pointwiseOutput) in both branches; replace the entire\nconditional with a single call assigning activatedOutput =\nApplyActivationAutodiff(pointwiseOutput). Optionally keep or add a TODO comment\nnoting that vector activation handling still needs to be implemented if\nintended.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","createdAt":"2025-11-12T02:20:32Z","path":"src/NeuralNetworks/Layers/DepthwiseSeparableConvolutionalLayer.cs","line":1032,"originalLine":1032,"diffHunk":"@@ -952,6 +966,275 @@ public override Tensor<T> Backward(Tensor<T> outputGradient)\n         return inputGradient;\n     }\n \n+    /// <summary>\n+    /// Backward pass implementation using automatic differentiation.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">The gradient of the loss with respect to the layer's output.</param>\n+    /// <returns>The gradient of the loss with respect to the layer's input.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method uses automatic differentiation to compute gradients using DepthwiseConv2D and Conv2D operations.\n+    /// The layer performs two sequential operations:\n+    /// 1. Depthwise convolution (each channel filtered separately)\n+    /// 2. Pointwise convolution (1x1 conv to mix channels)\n+    /// </para>\n+    /// <para>\n+    /// This provides:\n+    /// - Automatic gradient computation through the computation graph\n+    /// - Verification of manual gradient implementations\n+    /// - Support for rapid prototyping with custom modifications\n+    /// </para>\n+    /// </remarks>\n+    private Tensor<T> BackwardViaAutodiff(Tensor<T> outputGradient)\n+    {\n+        if (_lastInput == null)\n+            throw new InvalidOperationException(\"Forward pass must be called before backward pass.\");\n+\n+        // Convert from NHWC [batch, H, W, channels] to NCHW [batch, channels, H, W]\n+        var inputNCHW = ConvertNHWCtoNCHW(_lastInput);\n+        var depthwiseKernelsNCHW = ConvertDepthwiseKernelsToNCHW(_depthwiseKernels);\n+        var pointwiseKernelsNCHW = ConvertPointwiseKernelsToNCHW(_pointwiseKernels);\n+\n+        // Create computation nodes\n+        var inputNode = Autodiff.TensorOperations<T>.Variable(inputNCHW, \"input\", requiresGradient: true);\n+        var depthwiseKernelNode = Autodiff.TensorOperations<T>.Variable(depthwiseKernelsNCHW, \"depthwise_kernel\", requiresGradient: true);\n+        var pointwiseKernelNode = Autodiff.TensorOperations<T>.Variable(pointwiseKernelsNCHW, \"pointwise_kernel\", requiresGradient: true);\n+        var biasNode = Autodiff.TensorOperations<T>.Variable(ConvertVectorToTensor(_biases), \"bias\", requiresGradient: true);\n+\n+        // Forward pass using autodiff operations\n+        // Step 1: Depthwise convolution\n+        var depthwiseOutput = Autodiff.TensorOperations<T>.DepthwiseConv2D(\n+            inputNode,\n+            depthwiseKernelNode,\n+            bias: null,\n+            stride: new int[] { _stride, _stride },\n+            padding: new int[] { _padding, _padding });\n+\n+        // Step 2: Pointwise convolution (1x1 conv)\n+        var pointwiseOutput = Autodiff.TensorOperations<T>.Conv2D(\n+            depthwiseOutput,\n+            pointwiseKernelNode,\n+            biasNode,\n+            stride: new int[] { 1, 1 },\n+            padding: new int[] { 0, 0 });\n+\n+        // Apply activation function\n+        Autodiff.ComputationNode<T> activatedOutput;\n+        if (UsingVectorActivation)\n+        {\n+            // For vector activations, we need to handle them differently\n+            // For now, approximate with element-wise operations\n+            activatedOutput = ApplyActivationAutodiff(pointwiseOutput);\n+        }\n+        else\n+        {\n+            activatedOutput = ApplyActivationAutodiff(pointwiseOutput);\n+        }"}]}},{"id":"PRRT_kwDOKSXUF85hlmqA","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/DepthwiseSeparableConvolutionalLayer.cs","line":1138,"originalLine":1138,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V_izn","author":{"login":"coderabbitai"},"body":"_🛠️ Refactor suggestion_ | _🟠 Major_\n\n**Remove no-op conversion methods.**\n\nFour methods (`ConvertDepthwiseKernelsToNCHW`, `ConvertDepthwiseKernelsFromNCHW`, `ConvertPointwiseKernelsToNCHW`, `ConvertPointwiseKernelsFromNCHW`) simply return their input unchanged. Since the kernels are already in the correct format, these methods add no value and should be removed.\n\n\n\nAt lines 995-996 and 1051-1054, replace:\n```diff\n-        var depthwiseKernelsNCHW = ConvertDepthwiseKernelsToNCHW(_depthwiseKernels);\n-        var pointwiseKernelsNCHW = ConvertPointwiseKernelsToNCHW(_pointwiseKernels);\n+        // Kernels are already in NCHW format [inputDepth, 1, kH, kW] and [outputDepth, inputDepth, 1, 1]\n+        var depthwiseKernelsNCHW = _depthwiseKernels;\n+        var pointwiseKernelsNCHW = _pointwiseKernels;\n```\n\nAnd:\n```diff\n         if (depthwiseKernelNode.Gradient != null)\n-            _depthwiseKernelsGradient = ConvertDepthwiseKernelsFromNCHW(depthwiseKernelNode.Gradient);\n+            _depthwiseKernelsGradient = depthwiseKernelNode.Gradient;\n\n         if (pointwiseKernelNode.Gradient != null)\n-            _pointwiseKernelsGradient = ConvertPointwiseKernelsFromNCHW(pointwiseKernelNode.Gradient);\n+            _pointwiseKernelsGradient = pointwiseKernelNode.Gradient;\n```\n\nThen delete lines 1104-1138.\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>🤖 Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/Layers/DepthwiseSeparableConvolutionalLayer.cs around\nlines 1107-1138, four methods (ConvertDepthwiseKernelsToNCHW,\nConvertDepthwiseKernelsFromNCHW, ConvertPointwiseKernelsToNCHW,\nConvertPointwiseKernelsFromNCHW) are no-ops that just return their input; remove\nthese method definitions and any related XML comments; then at the call sites\nreferenced (around lines 995-996 and 1051-1054) replace calls to these methods\nby using the kernel variable directly (pass the kernel Tensor<T> unchanged),\nremoving the wrapper calls so behavior is identical but without dead methods.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","createdAt":"2025-11-12T02:20:32Z","path":"src/NeuralNetworks/Layers/DepthwiseSeparableConvolutionalLayer.cs","line":1138,"originalLine":1138,"diffHunk":"@@ -952,6 +966,275 @@ public override Tensor<T> Backward(Tensor<T> outputGradient)\n         return inputGradient;\n     }\n \n+    /// <summary>\n+    /// Backward pass implementation using automatic differentiation.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">The gradient of the loss with respect to the layer's output.</param>\n+    /// <returns>The gradient of the loss with respect to the layer's input.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method uses automatic differentiation to compute gradients using DepthwiseConv2D and Conv2D operations.\n+    /// The layer performs two sequential operations:\n+    /// 1. Depthwise convolution (each channel filtered separately)\n+    /// 2. Pointwise convolution (1x1 conv to mix channels)\n+    /// </para>\n+    /// <para>\n+    /// This provides:\n+    /// - Automatic gradient computation through the computation graph\n+    /// - Verification of manual gradient implementations\n+    /// - Support for rapid prototyping with custom modifications\n+    /// </para>\n+    /// </remarks>\n+    private Tensor<T> BackwardViaAutodiff(Tensor<T> outputGradient)\n+    {\n+        if (_lastInput == null)\n+            throw new InvalidOperationException(\"Forward pass must be called before backward pass.\");\n+\n+        // Convert from NHWC [batch, H, W, channels] to NCHW [batch, channels, H, W]\n+        var inputNCHW = ConvertNHWCtoNCHW(_lastInput);\n+        var depthwiseKernelsNCHW = ConvertDepthwiseKernelsToNCHW(_depthwiseKernels);\n+        var pointwiseKernelsNCHW = ConvertPointwiseKernelsToNCHW(_pointwiseKernels);\n+\n+        // Create computation nodes\n+        var inputNode = Autodiff.TensorOperations<T>.Variable(inputNCHW, \"input\", requiresGradient: true);\n+        var depthwiseKernelNode = Autodiff.TensorOperations<T>.Variable(depthwiseKernelsNCHW, \"depthwise_kernel\", requiresGradient: true);\n+        var pointwiseKernelNode = Autodiff.TensorOperations<T>.Variable(pointwiseKernelsNCHW, \"pointwise_kernel\", requiresGradient: true);\n+        var biasNode = Autodiff.TensorOperations<T>.Variable(ConvertVectorToTensor(_biases), \"bias\", requiresGradient: true);\n+\n+        // Forward pass using autodiff operations\n+        // Step 1: Depthwise convolution\n+        var depthwiseOutput = Autodiff.TensorOperations<T>.DepthwiseConv2D(\n+            inputNode,\n+            depthwiseKernelNode,\n+            bias: null,\n+            stride: new int[] { _stride, _stride },\n+            padding: new int[] { _padding, _padding });\n+\n+        // Step 2: Pointwise convolution (1x1 conv)\n+        var pointwiseOutput = Autodiff.TensorOperations<T>.Conv2D(\n+            depthwiseOutput,\n+            pointwiseKernelNode,\n+            biasNode,\n+            stride: new int[] { 1, 1 },\n+            padding: new int[] { 0, 0 });\n+\n+        // Apply activation function\n+        Autodiff.ComputationNode<T> activatedOutput;\n+        if (UsingVectorActivation)\n+        {\n+            // For vector activations, we need to handle them differently\n+            // For now, approximate with element-wise operations\n+            activatedOutput = ApplyActivationAutodiff(pointwiseOutput);\n+        }\n+        else\n+        {\n+            activatedOutput = ApplyActivationAutodiff(pointwiseOutput);\n+        }\n+\n+        // Convert output gradient from NHWC to NCHW\n+        var outputGradientNCHW = ConvertNHWCtoNCHW(outputGradient);\n+\n+        // Perform backward pass\n+        activatedOutput.Gradient = outputGradientNCHW;\n+        var topoOrder = GetTopologicalOrder(activatedOutput);\n+        for (int i = topoOrder.Count - 1; i >= 0; i--)\n+        {\n+            var node = topoOrder[i];\n+            if (node.RequiresGradient && node.BackwardFunction != null && node.Gradient != null)\n+            {\n+                node.BackwardFunction(node.Gradient);\n+            }\n+        }\n+\n+        // Update parameter gradients\n+        if (depthwiseKernelNode.Gradient != null)\n+            _depthwiseKernelsGradient = ConvertDepthwiseKernelsFromNCHW(depthwiseKernelNode.Gradient);\n+\n+        if (pointwiseKernelNode.Gradient != null)\n+            _pointwiseKernelsGradient = ConvertPointwiseKernelsFromNCHW(pointwiseKernelNode.Gradient);\n+\n+        if (biasNode.Gradient != null)\n+            _biasesGradient = ConvertTensorToVector(biasNode.Gradient);\n+\n+        // Convert input gradient from NCHW back to NHWC\n+        var inputGradientNCHW = inputNode.Gradient ?? throw new InvalidOperationException(\"Gradient computation failed.\");\n+        return ConvertNCHWtoNHWC(inputGradientNCHW);\n+    }\n+\n+    /// <summary>\n+    /// Converts tensor from NHWC [batch, H, W, channels] to NCHW [batch, channels, H, W] format.\n+    /// </summary>\n+    private Tensor<T> ConvertNHWCtoNCHW(Tensor<T> nhwc)\n+    {\n+        int batch = nhwc.Shape[0];\n+        int height = nhwc.Shape[1];\n+        int width = nhwc.Shape[2];\n+        int channels = nhwc.Shape[3];\n+\n+        var nchw = new Tensor<T>([batch, channels, height, width]);\n+        for (int b = 0; b < batch; b++)\n+            for (int c = 0; c < channels; c++)\n+                for (int h = 0; h < height; h++)\n+                    for (int w = 0; w < width; w++)\n+                        nchw[b, c, h, w] = nhwc[b, h, w, c];\n+\n+        return nchw;\n+    }\n+\n+    /// <summary>\n+    /// Converts tensor from NCHW [batch, channels, H, W] to NHWC [batch, H, W, channels] format.\n+    /// </summary>\n+    private Tensor<T> ConvertNCHWtoNHWC(Tensor<T> nchw)\n+    {\n+        int batch = nchw.Shape[0];\n+        int channels = nchw.Shape[1];\n+        int height = nchw.Shape[2];\n+        int width = nchw.Shape[3];\n+\n+        var nhwc = new Tensor<T>([batch, height, width, channels]);\n+        for (int b = 0; b < batch; b++)\n+            for (int h = 0; h < height; h++)\n+                for (int w = 0; w < width; w++)\n+                    for (int c = 0; c < channels; c++)\n+                        nhwc[b, h, w, c] = nchw[b, c, h, w];\n+\n+        return nhwc;\n+    }\n+\n+    /// <summary>\n+    /// Converts depthwise kernels from [inputDepth, 1, kernelSize, kernelSize] to [inputDepth, 1, kernelSize, kernelSize] format.\n+    /// </summary>\n+    private Tensor<T> ConvertDepthwiseKernelsToNCHW(Tensor<T> kernels)\n+    {\n+        // Already in the correct format [inputDepth, multiplier, kH, kW]\n+        return kernels;\n+    }\n+\n+    /// <summary>\n+    /// Converts depthwise kernels from NCHW back to original format.\n+    /// </summary>\n+    private Tensor<T> ConvertDepthwiseKernelsFromNCHW(Tensor<T> kernels)\n+    {\n+        // Already in the correct format\n+        return kernels;\n+    }\n+\n+    /// <summary>\n+    /// Converts pointwise kernels from [outputDepth, inputDepth, 1, 1] to [outputDepth, inputDepth, 1, 1] format.\n+    /// </summary>\n+    private Tensor<T> ConvertPointwiseKernelsToNCHW(Tensor<T> kernels)\n+    {\n+        // Already in the correct format [outputDepth, inputDepth, 1, 1]\n+        return kernels;\n+    }\n+\n+    /// <summary>\n+    /// Converts pointwise kernels from NCHW back to original format.\n+    /// </summary>\n+    private Tensor<T> ConvertPointwiseKernelsFromNCHW(Tensor<T> kernels)\n+    {\n+        // Already in the correct format\n+        return kernels;\n+    }"}]}},{"id":"PRRT_kwDOKSXUF85hlmqC","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/RBFLayer.cs","line":357,"originalLine":357,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V_izo","author":{"login":"coderabbitai"},"body":"_⚠️ Potential issue_ | _🔴 Critical_\n\n<details>\n<summary>🧩 Analysis chain</summary>\n\n**Critical mismatch between forward pass and autodiff reconstruction.**\n\nThe forward pass (line 204) uses `_rbf.Compute(distance)` where `_rbf` can be any `IRadialBasisFunction` implementation (Gaussian, Multiquadric, Inverse Quadratic, etc.). However, `BackwardViaAutodiff` reconstructs the forward computation using `RBFKernel`, which specifically implements Gaussian RBF: `exp(-epsilon * distance^2)`.\n\n**Impact**: When `UseAutodiff` is true and a non-Gaussian RBF is used, the gradients will be incorrect because the forward and backward passes compute different functions.\n\n**Additional concern**: The semantic relationship between `_widths` and the `epsilons` parameter expected by `RBFKernel` is unclear. In Gaussian RBF, epsilon typically equals `1/(2*sigma^2)` or similar, not sigma directly. Verify whether `_widths` should be transformed before passing to `RBFKernel`.\n\n\n\n\nRun the following script to check if there's documentation or usage patterns that clarify the widths/epsilons relationship and RBF type constraints:\n\n---\n\n🏁 Script executed:\n\n```shell\n#!/bin/bash\n# Search for IRadialBasisFunction implementations and RBFKernel usage patterns\n\n# Find IRadialBasisFunction implementations\necho \"=== IRadialBasisFunction implementations ===\"\nrg -n \"class.*:.*IRadialBasisFunction\" --type=cs -A 10\n\n# Find RBFLayer constructor calls to see which RBF types are used\necho -e \"\\n=== RBFLayer instantiations ===\"\nrg -n \"new RBFLayer\" --type=cs -C 2\n\n# Check for any documentation about epsilon/width relationship\necho -e \"\\n=== RBFKernel documentation ===\"\nrg -n \"RBFKernel\" --type=cs -C 5 | rg -i \"epsilon|width|sigma\"\n\n# Check UseAutodiff usage with RBFLayer\necho -e \"\\n=== UseAutodiff with RBFLayer ===\"\nrg -n \"UseAutodiff.*RBF|RBF.*UseAutodiff\" --type=cs -C 3\n```\n\nLength of output: 15327\n\n---\n\nNeed to inspect the actual RBFLayer code directly. Let me run scripts to examine the codebase:\n\n\n\n```\n\n---\n\n🏁 Script executed:\n\n```shell\n#!/bin/bash\n# Examine RBFLayer.cs constructor, fields, and Forward/Backward methods\n\necho \"=== RBFLayer.cs field definitions ===\"\nrg -n \"private.*_widths|private.*_centers|private.*_rbf|UseAutodiff\" src/NeuralNetworks/Layers/RBFLayer.cs -A 2\n\necho -e \"\\n=== RBFLayer constructor ===\"\nrg -n \"public RBFLayer\" src/NeuralNetworks/Layers/RBFLayer.cs -A 20\n\necho -e \"\\n=== Forward method ===\"\nrg -n \"public.*Forward|private.*Forward\" src/NeuralNetworks/Layers/RBFLayer.cs -A 30 | head -60\n\necho -e \"\\n=== Backward method routing (UseAutodiff check) ===\"\nrg -n \"Backward.*|UseAutodiff\" src/NeuralNetworks/Layers/RBFLayer.cs -B 2 -A 5 | head -80\n```\n\nLength of output: 5910\n\n---\n\n```\n\n---\n\n🏁 Script executed:\n\n```shell\n#!/bin/bash\n# Examine RBFKernel implementation and parameter semantics\n\necho \"=== RBFKernel method signature and documentation ===\"\nrg -n \"RBFKernel\" src/Autodiff/TensorOperations.cs -A 30 | head -50\n\necho -e \"\\n=== Check if _widths relate to RBFKernel's epsilons ===\"\nrg -n \"Compute|epsilon|width|Gaussian\" src/RadialBasisFunctions/GaussianRBF.cs -A 3 | head -40\n\necho -e \"\\n=== How _widths are initialized in RBFLayer ===\"\nrg -n \"InitializeParameters\" src/NeuralNetworks/Layers/RBFLayer.cs -A 50 | head -80\n\necho -e \"\\n=== Check for UseAutodiff flag definition/documentation ===\"\nrg -n \"UseAutodiff\" src/NeuralNetworks/Layers/ -B 5 -A 2 | head -50\n```\n\nLength of output: 11582\n\n---\n\n\n\n</details>\n\n**Critical mismatch between forward and autodiff backward passes.**\n\nThe forward pass (line 204) uses `_rbf.Compute(distance)` where `_rbf` can be any `IRadialBasisFunction` implementation (Gaussian, Multiquadric, Linear, Cubic, ThinPlateSpline, etc.). However, `BackwardViaAutodiff` hardcodes the forward computation using `RBFKernel`, which specifically implements Gaussian RBF: `exp(-epsilon * distance²)`.\n\nWhen `UseAutodiff` is true and a non-Gaussian RBF is used, the gradients will be computed for the wrong function. For example, if `_rbf` is `LinearRBF` or `CubicRBF`, the backward pass still computes Gaussian RBF gradients, producing incorrect parameter updates.\n\n**Secondary concern**: The relationship between `_widths` (initialized to random [0, 1) values) and the `epsilons` parameter expected by `RBFKernel` is unclear. Verify whether `_widths` requires transformation before passing to `RBFKernel` and whether autodiff should only support Gaussian RBF or be extended to support all RBF types.\n\n<details>\n<summary>🤖 Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/Layers/RBFLayer.cs around lines 290 to 357, the autodiff\nbackward unconditionally calls the Gaussian RBFKernel which mismatches the\nforward pass when _rbf is non-Gaussian and may misinterpret _widths as epsilons;\nchange BackwardViaAutodiff to first check the concrete type of _rbf and either\n(a) if _rbf is Gaussian, map _widths to the RBFKernel epsilon parameter\nconsistently with the forward implementation (e.g., convert sigma/width to\nepsilon using the same formula used in forward: epsilon = ...), or (b) if _rbf\nis not Gaussian, do not call RBFKernel—either throw a clear\nNotSupportedException when UseAutodiff is requested for unsupported RBFs or\nimplement equivalent autodiff operations for each IRadialBasisFunction variant\nand use those so gradients match the forward; ensure any widths ↔ epsilon\ntransformation is documented and identical between forward and autodiff paths.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","createdAt":"2025-11-12T02:20:32Z","path":"src/NeuralNetworks/Layers/RBFLayer.cs","line":357,"originalLine":357,"diffHunk":"@@ -274,6 +287,161 @@ public override Tensor<T> Backward(Tensor<T> outputGradient)\n         return inputGradient;\n     }\n \n+    /// <summary>\n+    /// Backward pass implementation using automatic differentiation.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">The gradient of the loss with respect to the layer's output.</param>\n+    /// <returns>The gradient of the loss with respect to the layer's input.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method uses automatic differentiation via the RBFKernel operation to compute gradients.\n+    /// The operation handles Gaussian RBF computations with proper gradient flow.\n+    /// </para>\n+    /// </remarks>\n+    private Tensor<T> BackwardViaAutodiff(Tensor<T> outputGradient)\n+    {\n+        if (_lastInput == null)\n+            throw new InvalidOperationException(\"Forward pass must be called before backward pass.\");\n+\n+        // Convert centers matrix to tensor [numCenters, inputSize]\n+        var centersTensor = MatrixToTensor(_centers);\n+\n+        // Convert widths vector to tensor [numCenters]\n+        var widthsTensor = VectorToTensor(_widths);\n+\n+        // Create computation nodes\n+        var inputNode = Autodiff.TensorOperations<T>.Variable(\n+            _lastInput,\n+            \"input\",\n+            requiresGradient: true);\n+\n+        var centersNode = Autodiff.TensorOperations<T>.Variable(\n+            centersTensor,\n+            \"centers\",\n+            requiresGradient: true);\n+\n+        var widthsNode = Autodiff.TensorOperations<T>.Variable(\n+            widthsTensor,\n+            \"widths\",\n+            requiresGradient: true);\n+\n+        // Apply RBFKernel operation\n+        var outputNode = Autodiff.TensorOperations<T>.RBFKernel(\n+            inputNode,\n+            centersNode,\n+            widthsNode);\n+\n+        // Set the output gradient\n+        outputNode.Gradient = outputGradient;\n+\n+        // Perform backward pass\n+        var topoOrder = GetTopologicalOrder(outputNode);\n+        for (int i = topoOrder.Count - 1; i >= 0; i--)\n+        {\n+            var node = topoOrder[i];\n+            if (node.RequiresGradient && node.BackwardFunction != null && node.Gradient != null)\n+            {\n+                node.BackwardFunction(node.Gradient);\n+            }\n+        }\n+\n+        // Update parameter gradients\n+        if (centersNode.Gradient != null)\n+            _centersGradient = TensorToMatrix(centersNode.Gradient, _centers.Rows, _centers.Columns);\n+\n+        if (widthsNode.Gradient != null)\n+            _widthsGradient = TensorToVector(widthsNode.Gradient);\n+\n+        // Return input gradient\n+        return inputNode.Gradient ?? throw new InvalidOperationException(\"Gradient computation failed.\");\n+    }"}]}},{"id":"PRRT_kwDOKSXUF85hlmqF","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/RBFLayer.cs","line":442,"originalLine":442,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V_izs","author":{"login":"coderabbitai"},"body":"_⚠️ Potential issue_ | _🟠 Major_\n\n**Performance concern: element-by-element tensor conversions in training loop.**\n\nThe conversion helpers copy data element-by-element, which is called on every backward pass when using autodiff. For large layers (e.g., 1000 centers × 100 input dimensions), this results in 100K+ copy operations per backward pass.\n\nConsider optimizing these conversions using bulk memory operations or exploring shared memory representations between Matrix/Vector and Tensor types.\n\n\n\n\nExample optimization for `MatrixToTensor`:\n\n```csharp\nprivate Tensor<T> MatrixToTensor(Matrix<T> matrix)\n{\n    var tensor = new Tensor<T>([matrix.Rows, matrix.Columns]);\n    // If Matrix and Tensor use contiguous memory layouts that are compatible,\n    // consider using Buffer.BlockCopy or Span<T>.CopyTo for bulk copy\n    // Current implementation:\n    for (int i = 0; i < matrix.Rows; i++)\n    {\n        for (int j = 0; j < matrix.Columns; j++)\n        {\n            tensor[i, j] = matrix[i, j];\n        }\n    }\n    return tensor;\n}\n```\n\nIf bulk copy is feasible, it could provide significant speedup in training.\n\n<details>\n<summary>🤖 Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/Layers/RBFLayer.cs around lines 386–442 the\nMatrix/Vector↔Tensor helpers do element-by-element copies which are invoked\nevery backward pass; replace them with bulk memory operations or zero-copy\nviews: verify Matrix/Vector/Tensor use contiguous, compatible memory layouts,\nthen either (a) expose the underlying backing arrays and use\nArray.Copy/Buffer.BlockCopy or Span<T>.CopyTo to copy entire blocks, or (b) add\nTensor/Matrix/Vector constructors or factory methods that accept an existing\narray/memory/Span to create a view without copying; update these helper methods\nto use the chosen bulk-copy or view approach and fall back to the element loop\nonly when layouts are incompatible.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","createdAt":"2025-11-12T02:20:32Z","path":"src/NeuralNetworks/Layers/RBFLayer.cs","line":442,"originalLine":442,"diffHunk":"@@ -274,6 +287,161 @@ public override Tensor<T> Backward(Tensor<T> outputGradient)\n         return inputGradient;\n     }\n \n+    /// <summary>\n+    /// Backward pass implementation using automatic differentiation.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">The gradient of the loss with respect to the layer's output.</param>\n+    /// <returns>The gradient of the loss with respect to the layer's input.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method uses automatic differentiation via the RBFKernel operation to compute gradients.\n+    /// The operation handles Gaussian RBF computations with proper gradient flow.\n+    /// </para>\n+    /// </remarks>\n+    private Tensor<T> BackwardViaAutodiff(Tensor<T> outputGradient)\n+    {\n+        if (_lastInput == null)\n+            throw new InvalidOperationException(\"Forward pass must be called before backward pass.\");\n+\n+        // Convert centers matrix to tensor [numCenters, inputSize]\n+        var centersTensor = MatrixToTensor(_centers);\n+\n+        // Convert widths vector to tensor [numCenters]\n+        var widthsTensor = VectorToTensor(_widths);\n+\n+        // Create computation nodes\n+        var inputNode = Autodiff.TensorOperations<T>.Variable(\n+            _lastInput,\n+            \"input\",\n+            requiresGradient: true);\n+\n+        var centersNode = Autodiff.TensorOperations<T>.Variable(\n+            centersTensor,\n+            \"centers\",\n+            requiresGradient: true);\n+\n+        var widthsNode = Autodiff.TensorOperations<T>.Variable(\n+            widthsTensor,\n+            \"widths\",\n+            requiresGradient: true);\n+\n+        // Apply RBFKernel operation\n+        var outputNode = Autodiff.TensorOperations<T>.RBFKernel(\n+            inputNode,\n+            centersNode,\n+            widthsNode);\n+\n+        // Set the output gradient\n+        outputNode.Gradient = outputGradient;\n+\n+        // Perform backward pass\n+        var topoOrder = GetTopologicalOrder(outputNode);\n+        for (int i = topoOrder.Count - 1; i >= 0; i--)\n+        {\n+            var node = topoOrder[i];\n+            if (node.RequiresGradient && node.BackwardFunction != null && node.Gradient != null)\n+            {\n+                node.BackwardFunction(node.Gradient);\n+            }\n+        }\n+\n+        // Update parameter gradients\n+        if (centersNode.Gradient != null)\n+            _centersGradient = TensorToMatrix(centersNode.Gradient, _centers.Rows, _centers.Columns);\n+\n+        if (widthsNode.Gradient != null)\n+            _widthsGradient = TensorToVector(widthsNode.Gradient);\n+\n+        // Return input gradient\n+        return inputNode.Gradient ?? throw new InvalidOperationException(\"Gradient computation failed.\");\n+    }\n+\n+    /// <summary>\n+    /// Gets the computation nodes in topological order for backward pass.\n+    /// </summary>\n+    /// <param name=\"outputNode\">The output node to start from.</param>\n+    /// <returns>List of nodes in topological order.</returns>\n+    private List<Autodiff.ComputationNode<T>> GetTopologicalOrder(Autodiff.ComputationNode<T> outputNode)\n+    {\n+        var visited = new HashSet<Autodiff.ComputationNode<T>>();\n+        var order = new List<Autodiff.ComputationNode<T>>();\n+\n+        void Visit(Autodiff.ComputationNode<T> node)\n+        {\n+            if (visited.Contains(node)) return;\n+            visited.Add(node);\n+\n+            foreach (var parent in node.Parents)\n+            {\n+                Visit(parent);\n+            }\n+\n+            order.Add(node);\n+        }\n+\n+        Visit(outputNode);\n+        return order;\n+    }\n+\n+    /// <summary>\n+    /// Converts a Matrix to a Tensor.\n+    /// </summary>\n+    private Tensor<T> MatrixToTensor(Matrix<T> matrix)\n+    {\n+        var tensor = new Tensor<T>([matrix.Rows, matrix.Columns]);\n+        for (int i = 0; i < matrix.Rows; i++)\n+        {\n+            for (int j = 0; j < matrix.Columns; j++)\n+            {\n+                tensor[i, j] = matrix[i, j];\n+            }\n+        }\n+        return tensor;\n+    }\n+\n+    /// <summary>\n+    /// Converts a Tensor back to a Matrix.\n+    /// </summary>\n+    private Matrix<T> TensorToMatrix(Tensor<T> tensor, int rows, int cols)\n+    {\n+        var matrix = new Matrix<T>(rows, cols);\n+        for (int i = 0; i < rows; i++)\n+        {\n+            for (int j = 0; j < cols; j++)\n+            {\n+                matrix[i, j] = tensor[i, j];\n+            }\n+        }\n+        return matrix;\n+    }\n+\n+    /// <summary>\n+    /// Converts a Vector to a Tensor.\n+    /// </summary>\n+    private Tensor<T> VectorToTensor(Vector<T> vector)\n+    {\n+        var tensor = new Tensor<T>([vector.Length]);\n+        for (int i = 0; i < vector.Length; i++)\n+        {\n+            tensor[i] = vector[i];\n+        }\n+        return tensor;\n+    }\n+\n+    /// <summary>\n+    /// Converts a Tensor back to a Vector.\n+    /// </summary>\n+    private Vector<T> TensorToVector(Tensor<T> tensor)\n+    {\n+        var vector = new Vector<T>(tensor.Shape[0]);\n+        for (int i = 0; i < tensor.Shape[0]; i++)\n+        {\n+            vector[i] = tensor[i];\n+        }\n+        return vector;\n+    }"}]}},{"id":"PRRT_kwDOKSXUF85hlmqK","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/ReadoutLayer.cs","line":357,"originalLine":357,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V_izx","author":{"login":"coderabbitai"},"body":"_⚠️ Potential issue_ | _🔴 Critical_\n\n**Critical: Broken gradient flow due to manual reshape.**\n\nThe manual extraction and wrapping of `matmulNode.Value` into a `Constant` node at lines 351-354 breaks the computation graph. The `flatNode` created as a `Constant` does not have `matmulNode` as a parent, so gradients will not flow back through the matrix multiplication to `weightsNode` and `inputNode` during backpropagation. This will result in incorrect or zero gradients for weights and inputs.\n\n\n\nThe issue stems from trying to reshape `[outputSize, 1]` to `[outputSize]` by manually creating a new constant. You need to preserve the autodiff graph. Consider these solutions:\n\n**Solution 1: Keep 2D structure and handle bias addition with proper reshaping**\n\n```diff\n-// Forward pass: output = weights * input + bias\n-var matmulNode = Autodiff.TensorOperations<T>.MatMul(weightsNode, Autodiff.TensorOperations<T>.Transpose(inputNode));\n-\n-// Reshape matmul result from [outputSize, 1] to [outputSize]\n-var matmulFlat = new Tensor<T>([_weights.Rows]);\n-for (int i = 0; i < _weights.Rows; i++)\n-    matmulFlat[i] = matmulNode.Value[i, 0];\n-var flatNode = Autodiff.TensorOperations<T>.Constant(matmulFlat);\n-\n-// Add bias\n-var preActivationNode = Autodiff.TensorOperations<T>.Add(flatNode, biasNode);\n+// Forward pass: output = input * weights^T + bias\n+// Use input [1, inputSize] * weights^T [inputSize, outputSize] = [1, outputSize]\n+var weightsTransposed = Autodiff.TensorOperations<T>.Transpose(weightsNode);\n+var matmulNode = Autodiff.TensorOperations<T>.MatMul(inputNode, weightsTransposed);\n+\n+// Reshape matmul result from [1, outputSize] to [outputSize]\n+// Extract first row using autodiff operation (or implement Reshape if available)\n+var flatTensor = new Tensor<T>([_weights.Rows]);\n+for (int i = 0; i < _weights.Rows; i++)\n+    flatTensor[i] = matmulNode.Value[0, i];\n+    \n+// Create a Reshape operation that preserves gradient flow\n+// For now, work directly with [1, outputSize] and broadcast bias\n+var biasTensor2D = new Tensor<T>([1, _bias.Length]);\n+for (int i = 0; i < _bias.Length; i++)\n+    biasTensor2D[0, i] = _bias[i];\n+var biasNode2D = Autodiff.TensorOperations<T>.Variable(biasTensor2D, \"bias\", requiresGradient: true);\n+\n+var preActivationNode = Autodiff.TensorOperations<T>.Add(matmulNode, biasNode2D);\n```\n\n**Solution 2: Implement or use a Reshape autodiff operation** (if available in your TensorOperations) that properly tracks parents and implements backward pass.\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>🤖 Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/Layers/ReadoutLayer.cs around lines 348 to 357, the code\nbreaks gradient flow by materializing matmulNode.Value into a Constant (manually\nreshaping [outputSize,1] to [outputSize]); replace this with an autodiff-safe\nreshape or keep the 2D result and perform bias addition using an operation that\npreserves matmulNode as a parent. Concretely: remove the manual Tensor creation\nand Constant, use an existing TensorOperations.Reshape (or implement one) to\nconvert matmulNode from [outputSize,1] to [outputSize], or leave matmulNode as\n[outputSize,1] and add bias via broadcasting (ensuring biasNode has matching\nshape or is reshaped via autodiff) so the Add call has matmulNode as an input\nand gradients flow back to weightsNode and inputNode.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","createdAt":"2025-11-12T02:20:32Z","path":"src/NeuralNetworks/Layers/ReadoutLayer.cs","line":357,"originalLine":357,"diffHunk":"@@ -283,6 +296,173 @@ public override Tensor<T> Backward(Tensor<T> outputGradient)\n         return Tensor<T>.FromVector(_weights.Transpose() * gradient);\n     }\n \n+    /// <summary>\n+    /// Backward pass implementation using automatic differentiation.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">The gradient of the loss with respect to the layer's output.</param>\n+    /// <returns>The gradient of the loss with respect to the layer's input.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method uses automatic differentiation via MatMul and Add operations.\n+    /// The computation graph is: output = activation(weights * input + bias).\n+    /// </para>\n+    /// </remarks>\n+    private Tensor<T> BackwardViaAutodiff(Tensor<T> outputGradient)\n+    {\n+        if (_lastInput == null)\n+            throw new InvalidOperationException(\"Forward pass must be called before backward pass.\");\n+\n+        // Convert input to 2D tensor [1, inputSize]\n+        var inputTensor = new Tensor<T>([1, _lastInput.Length]);\n+        for (int i = 0; i < _lastInput.Length; i++)\n+            inputTensor[0, i] = _lastInput[i];\n+\n+        // Convert weights to tensor [outputSize, inputSize]\n+        var weightsTensor = new Tensor<T>([_weights.Rows, _weights.Columns]);\n+        for (int i = 0; i < _weights.Rows; i++)\n+            for (int j = 0; j < _weights.Columns; j++)\n+                weightsTensor[i, j] = _weights[i, j];\n+\n+        // Convert bias to tensor [outputSize]\n+        var biasTensor = new Tensor<T>([_bias.Length]);\n+        for (int i = 0; i < _bias.Length; i++)\n+            biasTensor[i] = _bias[i];\n+\n+        // Create computation nodes\n+        var inputNode = Autodiff.TensorOperations<T>.Variable(\n+            inputTensor,\n+            \"input\",\n+            requiresGradient: true);\n+\n+        var weightsNode = Autodiff.TensorOperations<T>.Variable(\n+            weightsTensor,\n+            \"weights\",\n+            requiresGradient: true);\n+\n+        var biasNode = Autodiff.TensorOperations<T>.Variable(\n+            biasTensor,\n+            \"bias\",\n+            requiresGradient: true);\n+\n+        // Forward pass: output = weights * input + bias\n+        var matmulNode = Autodiff.TensorOperations<T>.MatMul(weightsNode, Autodiff.TensorOperations<T>.Transpose(inputNode));\n+\n+        // Reshape matmul result from [outputSize, 1] to [outputSize]\n+        var matmulFlat = new Tensor<T>([_weights.Rows]);\n+        for (int i = 0; i < _weights.Rows; i++)\n+            matmulFlat[i] = matmulNode.Value[i, 0];\n+        var flatNode = Autodiff.TensorOperations<T>.Constant(matmulFlat);\n+\n+        // Add bias\n+        var preActivationNode = Autodiff.TensorOperations<T>.Add(flatNode, biasNode);"}]}},{"id":"PRRT_kwDOKSXUF85hlmqQ","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/ReconstructionLayer.cs","line":323,"originalLine":323,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V_iz4","author":{"login":"coderabbitai"},"body":"_⚠️ Potential issue_ | _🔴 Critical_\n\n<details>\n<summary>🧩 Analysis chain</summary>\n\n**Verify that `UseAutodiff` is propagated to sublayers.**\n\nThe `BackwardViaAutodiff` implementation is functionally identical to `BackwardManual`—both simply call `Backward()` on the sublayers in the same order. For autodiff to take effect, the sublayers (_fc1, _fc2, _fc3) must also have their `UseAutodiff` property enabled; otherwise, both code paths will execute the same manual backward pass.\n\nUnlike other layers in the codebase (e.g., ReadoutLayer, RepParameterizationLayer) that explicitly construct autodiff computation graphs using TensorOperations, this layer delegates to sublayers. Please confirm that the `UseAutodiff` flag is properly propagated to the constituent FullyConnectedLayer instances.\n\n---\n\n🏁 Script executed:\n\n```shell\n#!/bin/bash\n# Description: Search for UseAutodiff propagation in layer constructors and property setters\n\n# Check if ReconstructionLayer or LayerBase propagates UseAutodiff to sublayers\nrg -nP -C5 'UseAutodiff\\s*=' --type=cs\n\n# Also check if FullyConnectedLayer respects UseAutodiff in its Backward method\nast-grep --pattern $'class FullyConnectedLayer$$$$ {\n  $$$\n  Backward($$$) {\n    $$$\n  }\n  $$$\n}'\n```\n\nLength of output: 30100\n\n---\n\n</parameter>\n</invoke>\n\n---\n\n🏁 Script executed:\n\n```shell\n#!/bin/bash\n# Check ReconstructionLayer's UseAutodiff property definition and setter\nrg -nP -A10 'class ReconstructionLayer' --type=cs | head -50\n\n# Check if UseAutodiff is defined in LayerBase and how it's handled\nrg -nP -B5 -A10 'UseAutodiff.*{' --type=cs | head -100\n\n# Search for FullyConnectedLayer.Backward to see if it checks UseAutodiff\nrg -nP -B2 -A20 'class FullyConnectedLayer' --type=cs | head -80\n```\n\nLength of output: 6579\n\n---\n\n</parameter>\n</invoke>\n\n---\n\n🏁 Script executed:\n\n```shell\n#!/bin/bash\n# Get FullyConnectedLayer.Backward method implementation\nrg -nP -A30 'public.*Backward\\(' src/NeuralNetworks/Layers/FullyConnectedLayer.cs | head -80\n\n# Get ReconstructionLayer constructor to see if UseAutodiff is set on sublayers\nrg -nP -A50 'public ReconstructionLayer\\(' src/NeuralNetworks/Layers/ReconstructionLayer.cs | head -60\n\n# Get both BackwardManual and BackwardViaAutodiff from ReconstructionLayer\nrg -nP -B2 -A15 'BackwardManual|BackwardViaAutodiff' src/NeuralNetwork/Layers/ReconstructionLayer.cs\n```\n\nLength of output: 5212\n\n---\n\n</parameter>\n</invoke>\n\n---\n\n🏁 Script executed:\n\n```shell\n#!/bin/bash\n# Get complete FullyConnectedLayer.Backward method - check if it has UseAutodiff check\nrg -nP -A50 'public override Tensor<T> Backward' src/NeuralNetworks/Layers/FullyConnectedLayer.cs\n\n# Get BackwardManual and BackwardViaAutodiff from ReconstructionLayer with correct path\nrg -nP -B2 -A15 'private Tensor<T> Backward' src/NeuralNetworks/Layers/ReconstructionLayer.cs\n```\n\nLength of output: 3816\n\n---\n\n</parameter>\n</invoke>\n\n---\n\n🏁 Script executed:\n\n```shell\n#!/bin/bash\n# Find public Backward method in ReconstructionLayer that routes to BackwardManual/BackwardViaAutodiff\nrg -nP -B5 -A20 'public override Tensor<T> Backward' src/NeuralNetworks/Layers/ReconstructionLayer.cs\n```\n\nLength of output: 1141\n\n---\n\n\n\n</details>\n\n**Fix the `BackwardViaAutodiff` implementation or remove it if it's not functional.**\n\nThe `BackwardViaAutodiff` method is functionally identical to `BackwardManual`—both call `Backward()` on sublayers in the same sequence. The misleading comment claims to \"call Backward on each sublayer with UseAutodiff enabled,\" but this never occurs:\n\n1. **Sublayers are never configured**: The constructor doesn't set `UseAutodiff` on `_fc1`, `_fc2`, or `_fc3`; they default to `false`.\n2. **No propagation mechanism**: There's no code that propagates `ReconstructionLayer.UseAutodiff` to its sublayers.\n3. **FullyConnectedLayer ignores the flag**: `FullyConnectedLayer.Backward()` doesn't check `UseAutodiff`—it always executes manual backward.\n\nAs a result, setting `UseAutodiff = true` on `ReconstructionLayer` has no effect. Either implement proper autodiff support by propagating the flag to sublayers and ensuring they respect it, or remove the non-functional code path and update the comment.\n\n<details>\n<summary>🤖 Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/Layers/ReconstructionLayer.cs around lines 316 to 323,\nBackwardViaAutodiff is misleading and non-functional because sublayers never\nreceive UseAutodiff and FullyConnectedLayer.Backward ignores the flag; either\nremove BackwardViaAutodiff and its callers/comments or implement real autodiff\npropagation: add a property propagation step in the ReconstructionLayer\nconstructor or before calling sublayers that sets _fc1.UseAutodiff =\n_fc2.UseAutodiff = _fc3.UseAutodiff = this.UseAutodiff, and modify\nFullyConnectedLayer.Backward (and any other sublayers) to branch behavior based\non UseAutodiff (perform autodiff-based backward when true, manual otherwise);\nupdate or remove the misleading comment accordingly so UseAutodiff actually\ntakes effect or the alternate path is eliminated.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","createdAt":"2025-11-12T02:20:33Z","path":"src/NeuralNetworks/Layers/ReconstructionLayer.cs","line":323,"originalLine":323,"diffHunk":"@@ -282,13 +282,47 @@ public override Tensor<T> Forward(Tensor<T> input)\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);\n+    }\n+\n+    /// <summary>\n+    /// Manual backward pass implementation using optimized gradient calculations.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">The gradient of the loss with respect to the layer's output.</param>\n+    /// <returns>The gradient of the loss with respect to the layer's input.</returns>\n+    private Tensor<T> BackwardManual(Tensor<T> outputGradient)\n     {\n         var gradient = _fc3.Backward(outputGradient);\n         gradient = _fc2.Backward(gradient);\n \n         return _fc1.Backward(gradient);\n     }\n \n+    /// <summary>\n+    /// Backward pass implementation using automatic differentiation.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">The gradient of the loss with respect to the layer's output.</param>\n+    /// <returns>The gradient of the loss with respect to the layer's input.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method uses automatic differentiation by delegating to the autodiff implementations\n+    /// of the three constituent FullyConnectedLayers. Each sublayer will use its own autodiff implementation.\n+    /// </para>\n+    /// </remarks>\n+    private Tensor<T> BackwardViaAutodiff(Tensor<T> outputGradient)\n+    {\n+        // Composite layer: just call Backward on each sublayer with UseAutodiff enabled\n+        // The sublayers will handle their own autodiff if they support it\n+        var gradient = _fc3.Backward(outputGradient);\n+        gradient = _fc2.Backward(gradient);\n+        return _fc1.Backward(gradient);\n+    }"}]}},{"id":"PRRT_kwDOKSXUF85hlmqW","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/SeparableConvolutionalLayer.cs","line":918,"originalLine":918,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V_i0A","author":{"login":"coderabbitai"},"body":"_🛠️ Refactor suggestion_ | _🟠 Major_\n\n**Essential refactor: Extract duplicated conversion methods to shared utilities.**\n\nThese conversion methods (ConvertNHWCtoNCHW, ConvertNCHWtoNHWC, ConvertVectorToTensor, ConvertTensorToVector) are duplicated across multiple layers:\n- LocallyConnectedLayer has identical NHWC/NCHW converters (referenced snippets)\n- DepthwiseSeparableConvolutionalLayer has identical vector/tensor converters (referenced snippets)\n- This layer now duplicates all of them\n\n**Recommendation:** Extract these common utilities to a shared helper class, such as:\n\n```csharp\nnamespace AiDotNet.NeuralNetworks.Utilities\n{\n    internal static class TensorFormatConverter<T>\n    {\n        public static Tensor<T> ConvertNHWCtoNCHW(Tensor<T> nhwc) { ... }\n        public static Tensor<T> ConvertNCHWtoNHWC(Tensor<T> nchw) { ... }\n        public static Tensor<T> ConvertVectorToTensor(Vector<T> vector) { ... }\n        public static Vector<T> ConvertTensorToVector(Tensor<T> tensor) { ... }\n        // ... other common conversions\n    }\n}\n```\n\n**Benefits:**\n- Single source of truth for conversions reduces bugs\n- Easier to optimize all layers at once\n- Reduced code size and improved maintainability\n- Consistent behavior across all layers\n\n<details>\n<summary>🤖 Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/Layers/SeparableConvolutionalLayer.cs around lines 795 to\n918 the file duplicates multiple tensor/format conversion helpers\n(ConvertNHWCtoNCHW, ConvertNCHWtoNHWC, ConvertVectorToTensor,\nConvertTensorToVector); extract these into a single internal static generic\nhelper (e.g., AiDotNet.NeuralNetworks.Utilities.TensorFormatConverter<T>)\nimplementing the same methods, replace the local methods with calls to that\nhelper, remove the duplicated methods from this class, and update other layer\nclasses (LocallyConnectedLayer, DepthwiseSeparableConvolutionalLayer, etc.) to\nuse the new shared helper so there is one canonical implementation and no\nduplicated code.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","createdAt":"2025-11-12T02:20:33Z","path":"src/NeuralNetworks/Layers/SeparableConvolutionalLayer.cs","line":918,"originalLine":918,"diffHunk":"@@ -693,6 +706,291 @@ public override Tensor<T> Backward(Tensor<T> outputGradient)\n         return inputGradient;\n     }\n \n+    /// <summary>\n+    /// Backward pass implementation using automatic differentiation.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">The gradient of the loss with respect to the layer's output.</param>\n+    /// <returns>The gradient of the loss with respect to the layer's input.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method uses automatic differentiation to compute gradients. Currently, separable convolution operations\n+    /// are not yet available in TensorOperations, so this method falls back to the manual implementation.\n+    /// </para>\n+    /// <para>\n+    /// Once separable convolution operations are added to TensorOperations, this method will provide:\n+    /// - Automatic gradient computation through the computation graph\n+    /// - Verification of manual gradient implementations\n+    /// - Support for rapid prototyping with custom modifications\n+    /// </para>\n+    /// </remarks>\n+    private Tensor<T> BackwardViaAutodiff(Tensor<T> outputGradient)\n+    {\n+        if (_lastInput == null)\n+            throw new InvalidOperationException(\"Forward pass must be called before backward pass.\");\n+\n+        // Convert from NHWC [batch, H, W, channels] to NCHW [batch, channels, H, W]\n+        var inputNCHW = ConvertNHWCtoNCHW(_lastInput);\n+        var depthwiseKernelNCHW = ConvertDepthwiseKernelToNCHW(_depthwiseKernels);\n+        var pointwiseKernelNCHW = ConvertPointwiseKernelToNCHW(_pointwiseKernels);\n+\n+        // Create computation nodes\n+        var inputNode = Autodiff.TensorOperations<T>.Variable(inputNCHW, \"input\", requiresGradient: true);\n+        var depthwiseKernelNode = Autodiff.TensorOperations<T>.Variable(depthwiseKernelNCHW, \"depthwise_kernel\", requiresGradient: true);\n+        var pointwiseKernelNode = Autodiff.TensorOperations<T>.Variable(pointwiseKernelNCHW, \"pointwise_kernel\", requiresGradient: true);\n+        var biasNode = Autodiff.TensorOperations<T>.Variable(ConvertVectorToTensor(_biases), \"bias\", requiresGradient: true);\n+\n+        // Forward pass using autodiff operations\n+        // Step 1: Depthwise convolution (no bias)\n+        var depthwiseOutput = Autodiff.TensorOperations<T>.DepthwiseConv2D(\n+            inputNode,\n+            depthwiseKernelNode,\n+            bias: null,\n+            stride: new int[] { _stride, _stride },\n+            padding: new int[] { _padding, _padding });\n+\n+        // Step 2: Pointwise convolution (1x1 conv with bias)\n+        var pointwiseOutput = Autodiff.TensorOperations<T>.Conv2D(\n+            depthwiseOutput,\n+            pointwiseKernelNode,\n+            biasNode,\n+            stride: new int[] { 1, 1 },\n+            padding: new int[] { 0, 0 });\n+\n+        // Apply activation function\n+        var activatedOutput = ApplyActivationAutodiff(pointwiseOutput);\n+\n+        // Convert output gradient from NHWC to NCHW\n+        var outputGradientNCHW = ConvertNHWCtoNCHW(outputGradient);\n+\n+        // Perform backward pass\n+        activatedOutput.Gradient = outputGradientNCHW;\n+        var topoOrder = GetTopologicalOrder(activatedOutput);\n+        for (int i = topoOrder.Count - 1; i >= 0; i--)\n+        {\n+            var node = topoOrder[i];\n+            if (node.RequiresGradient && node.BackwardFunction != null && node.Gradient != null)\n+            {\n+                node.BackwardFunction(node.Gradient);\n+            }\n+        }\n+\n+        // Update parameter gradients\n+        if (depthwiseKernelNode.Gradient != null)\n+            _depthwiseKernelsGradient = ConvertDepthwiseKernelFromNCHW(depthwiseKernelNode.Gradient);\n+\n+        if (pointwiseKernelNode.Gradient != null)\n+            _pointwiseKernelsGradient = ConvertPointwiseKernelFromNCHW(pointwiseKernelNode.Gradient);\n+\n+        if (biasNode.Gradient != null)\n+            _biasesGradient = ConvertTensorToVector(biasNode.Gradient);\n+\n+        // Convert input gradient from NCHW back to NHWC\n+        var inputGradientNCHW = inputNode.Gradient ?? throw new InvalidOperationException(\"Gradient computation failed.\");\n+        return ConvertNCHWtoNHWC(inputGradientNCHW);\n+    }\n+\n+    /// <summary>\n+    /// Converts tensor from NHWC [batch, H, W, channels] to NCHW [batch, channels, H, W] format.\n+    /// </summary>\n+    private Tensor<T> ConvertNHWCtoNCHW(Tensor<T> nhwc)\n+    {\n+        int batch = nhwc.Shape[0];\n+        int height = nhwc.Shape[1];\n+        int width = nhwc.Shape[2];\n+        int channels = nhwc.Shape[3];\n+\n+        var nchw = new Tensor<T>([batch, channels, height, width]);\n+        for (int b = 0; b < batch; b++)\n+            for (int c = 0; c < channels; c++)\n+                for (int h = 0; h < height; h++)\n+                    for (int w = 0; w < width; w++)\n+                        nchw[b, c, h, w] = nhwc[b, h, w, c];\n+\n+        return nchw;\n+    }\n+\n+    /// <summary>\n+    /// Converts tensor from NCHW [batch, channels, H, W] to NHWC [batch, H, W, channels] format.\n+    /// </summary>\n+    private Tensor<T> ConvertNCHWtoNHWC(Tensor<T> nchw)\n+    {\n+        int batch = nchw.Shape[0];\n+        int channels = nchw.Shape[1];\n+        int height = nchw.Shape[2];\n+        int width = nchw.Shape[3];\n+\n+        var nhwc = new Tensor<T>([batch, height, width, channels]);\n+        for (int b = 0; b < batch; b++)\n+            for (int h = 0; h < height; h++)\n+                for (int w = 0; w < width; w++)\n+                    for (int c = 0; c < channels; c++)\n+                        nhwc[b, h, w, c] = nchw[b, c, h, w];\n+\n+        return nhwc;\n+    }\n+\n+    /// <summary>\n+    /// Converts depthwise kernel from [inputDepth, kernelSize, kernelSize, 1] to [inputDepth, 1, kernelSize, kernelSize] format.\n+    /// </summary>\n+    private Tensor<T> ConvertDepthwiseKernelToNCHW(Tensor<T> kernel)\n+    {\n+        int inputDepth = kernel.Shape[0];\n+        int kernelSize = kernel.Shape[1];\n+\n+        var nchw = new Tensor<T>([inputDepth, 1, kernelSize, kernelSize]);\n+        for (int d = 0; d < inputDepth; d++)\n+            for (int kh = 0; kh < kernelSize; kh++)\n+                for (int kw = 0; kw < kernelSize; kw++)\n+                    nchw[d, 0, kh, kw] = kernel[d, kh, kw, 0];\n+\n+        return nchw;\n+    }\n+\n+    /// <summary>\n+    /// Converts depthwise kernel from [inputDepth, 1, kernelSize, kernelSize] back to [inputDepth, kernelSize, kernelSize, 1] format.\n+    /// </summary>\n+    private Tensor<T> ConvertDepthwiseKernelFromNCHW(Tensor<T> kernel)\n+    {\n+        int inputDepth = kernel.Shape[0];\n+        int kernelSize = kernel.Shape[2];\n+\n+        var nhwc = new Tensor<T>([inputDepth, kernelSize, kernelSize, 1]);\n+        for (int d = 0; d < inputDepth; d++)\n+            for (int kh = 0; kh < kernelSize; kh++)\n+                for (int kw = 0; kw < kernelSize; kw++)\n+                    nhwc[d, kh, kw, 0] = kernel[d, 0, kh, kw];\n+\n+        return nhwc;\n+    }\n+\n+    /// <summary>\n+    /// Converts pointwise kernel from [inputDepth, 1, 1, outputDepth] to [outputDepth, inputDepth, 1, 1] format.\n+    /// </summary>\n+    private Tensor<T> ConvertPointwiseKernelToNCHW(Tensor<T> kernel)\n+    {\n+        int inputDepth = kernel.Shape[0];\n+        int outputDepth = kernel.Shape[3];\n+\n+        var nchw = new Tensor<T>([outputDepth, inputDepth, 1, 1]);\n+        for (int od = 0; od < outputDepth; od++)\n+            for (int id = 0; id < inputDepth; id++)\n+                nchw[od, id, 0, 0] = kernel[id, 0, 0, od];\n+\n+        return nchw;\n+    }\n+\n+    /// <summary>\n+    /// Converts pointwise kernel from [outputDepth, inputDepth, 1, 1] back to [inputDepth, 1, 1, outputDepth] format.\n+    /// </summary>\n+    private Tensor<T> ConvertPointwiseKernelFromNCHW(Tensor<T> kernel)\n+    {\n+        int outputDepth = kernel.Shape[0];\n+        int inputDepth = kernel.Shape[1];\n+\n+        var nhwc = new Tensor<T>([inputDepth, 1, 1, outputDepth]);\n+        for (int id = 0; id < inputDepth; id++)\n+            for (int od = 0; od < outputDepth; od++)\n+                nhwc[id, 0, 0, od] = kernel[od, id, 0, 0];\n+\n+        return nhwc;\n+    }\n+\n+    /// <summary>\n+    /// Converts vector to 1D tensor.\n+    /// </summary>\n+    private Tensor<T> ConvertVectorToTensor(Vector<T> vector)\n+    {\n+        var tensor = new Tensor<T>([vector.Length]);\n+        for (int i = 0; i < vector.Length; i++)\n+            tensor[i] = vector[i];\n+        return tensor;\n+    }\n+\n+    /// <summary>\n+    /// Converts 1D tensor to vector.\n+    /// </summary>\n+    private Vector<T> ConvertTensorToVector(Tensor<T> tensor)\n+    {\n+        var vector = new Vector<T>(tensor.Shape[0]);\n+        for (int i = 0; i < tensor.Shape[0]; i++)\n+            vector[i] = tensor[i];\n+        return vector;\n+    }"}]}},{"id":"PRRT_kwDOKSXUF85hlmqb","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/SeparableConvolutionalLayer.cs","line":948,"originalLine":948,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V_i0G","author":{"login":"coderabbitai"},"body":"_🛠️ Refactor suggestion_ | _🟠 Major_\n\n**Code duplication: ApplyActivationAutodiff is identical across multiple layers.**\n\nThis method is nearly identical to the version in DepthwiseSeparableConvolutionalLayer (see relevant snippets). Consider including activation handling in the shared utility class suggested for the conversion methods, or creating a base helper for autodiff-enabled layers.\n\n<details>\n<summary>🤖 Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/Layers/SeparableConvolutionalLayer.cs around lines 923-948\nthe ApplyActivationAutodiff method duplicates logic found in other layer\nclasses; extract this activation-selection logic into a shared helper (e.g., a\nstatic method in a common AutodiffUtils or a protected base method on an\nautodiff-enabled layer base) that accepts the generic T, the ComputationNode<T>\ninput and an IActivation instance (or separate vector/scalar activation params)\nand returns the appropriate Autodiff.TensorOperations<T> call\n(ReLU/Sigmoid/Tanh), including the unsupported-activation NotSupportedException;\nreplace the local method with a single call to that new helper from this class\n(preserve UsingVectorActivation branching by passing the proper activation\ninstance) so all layers reuse the same implementation.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","createdAt":"2025-11-12T02:20:33Z","path":"src/NeuralNetworks/Layers/SeparableConvolutionalLayer.cs","line":948,"originalLine":948,"diffHunk":"@@ -693,6 +706,291 @@ public override Tensor<T> Backward(Tensor<T> outputGradient)\n         return inputGradient;\n     }\n \n+    /// <summary>\n+    /// Backward pass implementation using automatic differentiation.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">The gradient of the loss with respect to the layer's output.</param>\n+    /// <returns>The gradient of the loss with respect to the layer's input.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method uses automatic differentiation to compute gradients. Currently, separable convolution operations\n+    /// are not yet available in TensorOperations, so this method falls back to the manual implementation.\n+    /// </para>\n+    /// <para>\n+    /// Once separable convolution operations are added to TensorOperations, this method will provide:\n+    /// - Automatic gradient computation through the computation graph\n+    /// - Verification of manual gradient implementations\n+    /// - Support for rapid prototyping with custom modifications\n+    /// </para>\n+    /// </remarks>\n+    private Tensor<T> BackwardViaAutodiff(Tensor<T> outputGradient)\n+    {\n+        if (_lastInput == null)\n+            throw new InvalidOperationException(\"Forward pass must be called before backward pass.\");\n+\n+        // Convert from NHWC [batch, H, W, channels] to NCHW [batch, channels, H, W]\n+        var inputNCHW = ConvertNHWCtoNCHW(_lastInput);\n+        var depthwiseKernelNCHW = ConvertDepthwiseKernelToNCHW(_depthwiseKernels);\n+        var pointwiseKernelNCHW = ConvertPointwiseKernelToNCHW(_pointwiseKernels);\n+\n+        // Create computation nodes\n+        var inputNode = Autodiff.TensorOperations<T>.Variable(inputNCHW, \"input\", requiresGradient: true);\n+        var depthwiseKernelNode = Autodiff.TensorOperations<T>.Variable(depthwiseKernelNCHW, \"depthwise_kernel\", requiresGradient: true);\n+        var pointwiseKernelNode = Autodiff.TensorOperations<T>.Variable(pointwiseKernelNCHW, \"pointwise_kernel\", requiresGradient: true);\n+        var biasNode = Autodiff.TensorOperations<T>.Variable(ConvertVectorToTensor(_biases), \"bias\", requiresGradient: true);\n+\n+        // Forward pass using autodiff operations\n+        // Step 1: Depthwise convolution (no bias)\n+        var depthwiseOutput = Autodiff.TensorOperations<T>.DepthwiseConv2D(\n+            inputNode,\n+            depthwiseKernelNode,\n+            bias: null,\n+            stride: new int[] { _stride, _stride },\n+            padding: new int[] { _padding, _padding });\n+\n+        // Step 2: Pointwise convolution (1x1 conv with bias)\n+        var pointwiseOutput = Autodiff.TensorOperations<T>.Conv2D(\n+            depthwiseOutput,\n+            pointwiseKernelNode,\n+            biasNode,\n+            stride: new int[] { 1, 1 },\n+            padding: new int[] { 0, 0 });\n+\n+        // Apply activation function\n+        var activatedOutput = ApplyActivationAutodiff(pointwiseOutput);\n+\n+        // Convert output gradient from NHWC to NCHW\n+        var outputGradientNCHW = ConvertNHWCtoNCHW(outputGradient);\n+\n+        // Perform backward pass\n+        activatedOutput.Gradient = outputGradientNCHW;\n+        var topoOrder = GetTopologicalOrder(activatedOutput);\n+        for (int i = topoOrder.Count - 1; i >= 0; i--)\n+        {\n+            var node = topoOrder[i];\n+            if (node.RequiresGradient && node.BackwardFunction != null && node.Gradient != null)\n+            {\n+                node.BackwardFunction(node.Gradient);\n+            }\n+        }\n+\n+        // Update parameter gradients\n+        if (depthwiseKernelNode.Gradient != null)\n+            _depthwiseKernelsGradient = ConvertDepthwiseKernelFromNCHW(depthwiseKernelNode.Gradient);\n+\n+        if (pointwiseKernelNode.Gradient != null)\n+            _pointwiseKernelsGradient = ConvertPointwiseKernelFromNCHW(pointwiseKernelNode.Gradient);\n+\n+        if (biasNode.Gradient != null)\n+            _biasesGradient = ConvertTensorToVector(biasNode.Gradient);\n+\n+        // Convert input gradient from NCHW back to NHWC\n+        var inputGradientNCHW = inputNode.Gradient ?? throw new InvalidOperationException(\"Gradient computation failed.\");\n+        return ConvertNCHWtoNHWC(inputGradientNCHW);\n+    }\n+\n+    /// <summary>\n+    /// Converts tensor from NHWC [batch, H, W, channels] to NCHW [batch, channels, H, W] format.\n+    /// </summary>\n+    private Tensor<T> ConvertNHWCtoNCHW(Tensor<T> nhwc)\n+    {\n+        int batch = nhwc.Shape[0];\n+        int height = nhwc.Shape[1];\n+        int width = nhwc.Shape[2];\n+        int channels = nhwc.Shape[3];\n+\n+        var nchw = new Tensor<T>([batch, channels, height, width]);\n+        for (int b = 0; b < batch; b++)\n+            for (int c = 0; c < channels; c++)\n+                for (int h = 0; h < height; h++)\n+                    for (int w = 0; w < width; w++)\n+                        nchw[b, c, h, w] = nhwc[b, h, w, c];\n+\n+        return nchw;\n+    }\n+\n+    /// <summary>\n+    /// Converts tensor from NCHW [batch, channels, H, W] to NHWC [batch, H, W, channels] format.\n+    /// </summary>\n+    private Tensor<T> ConvertNCHWtoNHWC(Tensor<T> nchw)\n+    {\n+        int batch = nchw.Shape[0];\n+        int channels = nchw.Shape[1];\n+        int height = nchw.Shape[2];\n+        int width = nchw.Shape[3];\n+\n+        var nhwc = new Tensor<T>([batch, height, width, channels]);\n+        for (int b = 0; b < batch; b++)\n+            for (int h = 0; h < height; h++)\n+                for (int w = 0; w < width; w++)\n+                    for (int c = 0; c < channels; c++)\n+                        nhwc[b, h, w, c] = nchw[b, c, h, w];\n+\n+        return nhwc;\n+    }\n+\n+    /// <summary>\n+    /// Converts depthwise kernel from [inputDepth, kernelSize, kernelSize, 1] to [inputDepth, 1, kernelSize, kernelSize] format.\n+    /// </summary>\n+    private Tensor<T> ConvertDepthwiseKernelToNCHW(Tensor<T> kernel)\n+    {\n+        int inputDepth = kernel.Shape[0];\n+        int kernelSize = kernel.Shape[1];\n+\n+        var nchw = new Tensor<T>([inputDepth, 1, kernelSize, kernelSize]);\n+        for (int d = 0; d < inputDepth; d++)\n+            for (int kh = 0; kh < kernelSize; kh++)\n+                for (int kw = 0; kw < kernelSize; kw++)\n+                    nchw[d, 0, kh, kw] = kernel[d, kh, kw, 0];\n+\n+        return nchw;\n+    }\n+\n+    /// <summary>\n+    /// Converts depthwise kernel from [inputDepth, 1, kernelSize, kernelSize] back to [inputDepth, kernelSize, kernelSize, 1] format.\n+    /// </summary>\n+    private Tensor<T> ConvertDepthwiseKernelFromNCHW(Tensor<T> kernel)\n+    {\n+        int inputDepth = kernel.Shape[0];\n+        int kernelSize = kernel.Shape[2];\n+\n+        var nhwc = new Tensor<T>([inputDepth, kernelSize, kernelSize, 1]);\n+        for (int d = 0; d < inputDepth; d++)\n+            for (int kh = 0; kh < kernelSize; kh++)\n+                for (int kw = 0; kw < kernelSize; kw++)\n+                    nhwc[d, kh, kw, 0] = kernel[d, 0, kh, kw];\n+\n+        return nhwc;\n+    }\n+\n+    /// <summary>\n+    /// Converts pointwise kernel from [inputDepth, 1, 1, outputDepth] to [outputDepth, inputDepth, 1, 1] format.\n+    /// </summary>\n+    private Tensor<T> ConvertPointwiseKernelToNCHW(Tensor<T> kernel)\n+    {\n+        int inputDepth = kernel.Shape[0];\n+        int outputDepth = kernel.Shape[3];\n+\n+        var nchw = new Tensor<T>([outputDepth, inputDepth, 1, 1]);\n+        for (int od = 0; od < outputDepth; od++)\n+            for (int id = 0; id < inputDepth; id++)\n+                nchw[od, id, 0, 0] = kernel[id, 0, 0, od];\n+\n+        return nchw;\n+    }\n+\n+    /// <summary>\n+    /// Converts pointwise kernel from [outputDepth, inputDepth, 1, 1] back to [inputDepth, 1, 1, outputDepth] format.\n+    /// </summary>\n+    private Tensor<T> ConvertPointwiseKernelFromNCHW(Tensor<T> kernel)\n+    {\n+        int outputDepth = kernel.Shape[0];\n+        int inputDepth = kernel.Shape[1];\n+\n+        var nhwc = new Tensor<T>([inputDepth, 1, 1, outputDepth]);\n+        for (int id = 0; id < inputDepth; id++)\n+            for (int od = 0; od < outputDepth; od++)\n+                nhwc[id, 0, 0, od] = kernel[od, id, 0, 0];\n+\n+        return nhwc;\n+    }\n+\n+    /// <summary>\n+    /// Converts vector to 1D tensor.\n+    /// </summary>\n+    private Tensor<T> ConvertVectorToTensor(Vector<T> vector)\n+    {\n+        var tensor = new Tensor<T>([vector.Length]);\n+        for (int i = 0; i < vector.Length; i++)\n+            tensor[i] = vector[i];\n+        return tensor;\n+    }\n+\n+    /// <summary>\n+    /// Converts 1D tensor to vector.\n+    /// </summary>\n+    private Vector<T> ConvertTensorToVector(Tensor<T> tensor)\n+    {\n+        var vector = new Vector<T>(tensor.Shape[0]);\n+        for (int i = 0; i < tensor.Shape[0]; i++)\n+            vector[i] = tensor[i];\n+        return vector;\n+    }\n+\n+    /// <summary>\n+    /// Applies activation function using autodiff operations.\n+    /// </summary>\n+    private Autodiff.ComputationNode<T> ApplyActivationAutodiff(Autodiff.ComputationNode<T> input)\n+    {\n+        // Apply the appropriate activation function\n+        if (UsingVectorActivation)\n+        {\n+            if (VectorActivation is ReLUActivation<T>)\n+                return Autodiff.TensorOperations<T>.ReLU(input);\n+            else if (VectorActivation is SigmoidActivation<T>)\n+                return Autodiff.TensorOperations<T>.Sigmoid(input);\n+            else if (VectorActivation is TanhActivation<T>)\n+                return Autodiff.TensorOperations<T>.Tanh(input);\n+            else\n+                throw new NotSupportedException($\"Activation {VectorActivation.GetType().Name} not yet supported in autodiff\");\n+        }\n+        else\n+        {\n+            if (ScalarActivation is ReLUActivation<T>)\n+                return Autodiff.TensorOperations<T>.ReLU(input);\n+            else if (ScalarActivation is SigmoidActivation<T>)\n+                return Autodiff.TensorOperations<T>.Sigmoid(input);\n+            else if (ScalarActivation is TanhActivation<T>)\n+                return Autodiff.TensorOperations<T>.Tanh(input);\n+            else\n+                throw new NotSupportedException($\"Activation {ScalarActivation.GetType().Name} not yet supported in autodiff\");\n+        }\n+    }"}]}},{"id":"PRRT_kwDOKSXUF85hlmqf","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/SeparableConvolutionalLayer.cs","line":992,"originalLine":992,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V_i0K","author":{"login":"coderabbitai"},"body":"_🛠️ Refactor suggestion_ | _🟠 Major_\n\n**Code duplication: GetTopologicalOrder is identical to LocallyConnectedLayer.**\n\nThis topological sort implementation is duplicated verbatim from LocallyConnectedLayer (see relevant snippets). This should also be extracted to the shared utility class alongside the conversion methods.\n\n<details>\n<summary>🤖 Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/Layers/SeparableConvolutionalLayer.cs around lines 955 to\n992 the GetTopologicalOrder method is duplicated from LocallyConnectedLayer;\nextract this method into a shared utility (for example a static GraphUtils or\nAutodiffUtilities class) with the same generic signature (e.g., static\nList<ComputationNode<T>> GetTopologicalOrder<T>(ComputationNode<T> root)),\nreplace the copy in SeparableConvolutionalLayer and the implementation in\nLocallyConnectedLayer with calls to that shared utility, update\naccessibility/usings as needed, and remove the duplicate code; also run/build\ntests and update any references or namespace imports to ensure both layers use\nthe new utility method.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","createdAt":"2025-11-12T02:20:33Z","path":"src/NeuralNetworks/Layers/SeparableConvolutionalLayer.cs","line":992,"originalLine":992,"diffHunk":"@@ -693,6 +706,291 @@ public override Tensor<T> Backward(Tensor<T> outputGradient)\n         return inputGradient;\n     }\n \n+    /// <summary>\n+    /// Backward pass implementation using automatic differentiation.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">The gradient of the loss with respect to the layer's output.</param>\n+    /// <returns>The gradient of the loss with respect to the layer's input.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method uses automatic differentiation to compute gradients. Currently, separable convolution operations\n+    /// are not yet available in TensorOperations, so this method falls back to the manual implementation.\n+    /// </para>\n+    /// <para>\n+    /// Once separable convolution operations are added to TensorOperations, this method will provide:\n+    /// - Automatic gradient computation through the computation graph\n+    /// - Verification of manual gradient implementations\n+    /// - Support for rapid prototyping with custom modifications\n+    /// </para>\n+    /// </remarks>\n+    private Tensor<T> BackwardViaAutodiff(Tensor<T> outputGradient)\n+    {\n+        if (_lastInput == null)\n+            throw new InvalidOperationException(\"Forward pass must be called before backward pass.\");\n+\n+        // Convert from NHWC [batch, H, W, channels] to NCHW [batch, channels, H, W]\n+        var inputNCHW = ConvertNHWCtoNCHW(_lastInput);\n+        var depthwiseKernelNCHW = ConvertDepthwiseKernelToNCHW(_depthwiseKernels);\n+        var pointwiseKernelNCHW = ConvertPointwiseKernelToNCHW(_pointwiseKernels);\n+\n+        // Create computation nodes\n+        var inputNode = Autodiff.TensorOperations<T>.Variable(inputNCHW, \"input\", requiresGradient: true);\n+        var depthwiseKernelNode = Autodiff.TensorOperations<T>.Variable(depthwiseKernelNCHW, \"depthwise_kernel\", requiresGradient: true);\n+        var pointwiseKernelNode = Autodiff.TensorOperations<T>.Variable(pointwiseKernelNCHW, \"pointwise_kernel\", requiresGradient: true);\n+        var biasNode = Autodiff.TensorOperations<T>.Variable(ConvertVectorToTensor(_biases), \"bias\", requiresGradient: true);\n+\n+        // Forward pass using autodiff operations\n+        // Step 1: Depthwise convolution (no bias)\n+        var depthwiseOutput = Autodiff.TensorOperations<T>.DepthwiseConv2D(\n+            inputNode,\n+            depthwiseKernelNode,\n+            bias: null,\n+            stride: new int[] { _stride, _stride },\n+            padding: new int[] { _padding, _padding });\n+\n+        // Step 2: Pointwise convolution (1x1 conv with bias)\n+        var pointwiseOutput = Autodiff.TensorOperations<T>.Conv2D(\n+            depthwiseOutput,\n+            pointwiseKernelNode,\n+            biasNode,\n+            stride: new int[] { 1, 1 },\n+            padding: new int[] { 0, 0 });\n+\n+        // Apply activation function\n+        var activatedOutput = ApplyActivationAutodiff(pointwiseOutput);\n+\n+        // Convert output gradient from NHWC to NCHW\n+        var outputGradientNCHW = ConvertNHWCtoNCHW(outputGradient);\n+\n+        // Perform backward pass\n+        activatedOutput.Gradient = outputGradientNCHW;\n+        var topoOrder = GetTopologicalOrder(activatedOutput);\n+        for (int i = topoOrder.Count - 1; i >= 0; i--)\n+        {\n+            var node = topoOrder[i];\n+            if (node.RequiresGradient && node.BackwardFunction != null && node.Gradient != null)\n+            {\n+                node.BackwardFunction(node.Gradient);\n+            }\n+        }\n+\n+        // Update parameter gradients\n+        if (depthwiseKernelNode.Gradient != null)\n+            _depthwiseKernelsGradient = ConvertDepthwiseKernelFromNCHW(depthwiseKernelNode.Gradient);\n+\n+        if (pointwiseKernelNode.Gradient != null)\n+            _pointwiseKernelsGradient = ConvertPointwiseKernelFromNCHW(pointwiseKernelNode.Gradient);\n+\n+        if (biasNode.Gradient != null)\n+            _biasesGradient = ConvertTensorToVector(biasNode.Gradient);\n+\n+        // Convert input gradient from NCHW back to NHWC\n+        var inputGradientNCHW = inputNode.Gradient ?? throw new InvalidOperationException(\"Gradient computation failed.\");\n+        return ConvertNCHWtoNHWC(inputGradientNCHW);\n+    }\n+\n+    /// <summary>\n+    /// Converts tensor from NHWC [batch, H, W, channels] to NCHW [batch, channels, H, W] format.\n+    /// </summary>\n+    private Tensor<T> ConvertNHWCtoNCHW(Tensor<T> nhwc)\n+    {\n+        int batch = nhwc.Shape[0];\n+        int height = nhwc.Shape[1];\n+        int width = nhwc.Shape[2];\n+        int channels = nhwc.Shape[3];\n+\n+        var nchw = new Tensor<T>([batch, channels, height, width]);\n+        for (int b = 0; b < batch; b++)\n+            for (int c = 0; c < channels; c++)\n+                for (int h = 0; h < height; h++)\n+                    for (int w = 0; w < width; w++)\n+                        nchw[b, c, h, w] = nhwc[b, h, w, c];\n+\n+        return nchw;\n+    }\n+\n+    /// <summary>\n+    /// Converts tensor from NCHW [batch, channels, H, W] to NHWC [batch, H, W, channels] format.\n+    /// </summary>\n+    private Tensor<T> ConvertNCHWtoNHWC(Tensor<T> nchw)\n+    {\n+        int batch = nchw.Shape[0];\n+        int channels = nchw.Shape[1];\n+        int height = nchw.Shape[2];\n+        int width = nchw.Shape[3];\n+\n+        var nhwc = new Tensor<T>([batch, height, width, channels]);\n+        for (int b = 0; b < batch; b++)\n+            for (int h = 0; h < height; h++)\n+                for (int w = 0; w < width; w++)\n+                    for (int c = 0; c < channels; c++)\n+                        nhwc[b, h, w, c] = nchw[b, c, h, w];\n+\n+        return nhwc;\n+    }\n+\n+    /// <summary>\n+    /// Converts depthwise kernel from [inputDepth, kernelSize, kernelSize, 1] to [inputDepth, 1, kernelSize, kernelSize] format.\n+    /// </summary>\n+    private Tensor<T> ConvertDepthwiseKernelToNCHW(Tensor<T> kernel)\n+    {\n+        int inputDepth = kernel.Shape[0];\n+        int kernelSize = kernel.Shape[1];\n+\n+        var nchw = new Tensor<T>([inputDepth, 1, kernelSize, kernelSize]);\n+        for (int d = 0; d < inputDepth; d++)\n+            for (int kh = 0; kh < kernelSize; kh++)\n+                for (int kw = 0; kw < kernelSize; kw++)\n+                    nchw[d, 0, kh, kw] = kernel[d, kh, kw, 0];\n+\n+        return nchw;\n+    }\n+\n+    /// <summary>\n+    /// Converts depthwise kernel from [inputDepth, 1, kernelSize, kernelSize] back to [inputDepth, kernelSize, kernelSize, 1] format.\n+    /// </summary>\n+    private Tensor<T> ConvertDepthwiseKernelFromNCHW(Tensor<T> kernel)\n+    {\n+        int inputDepth = kernel.Shape[0];\n+        int kernelSize = kernel.Shape[2];\n+\n+        var nhwc = new Tensor<T>([inputDepth, kernelSize, kernelSize, 1]);\n+        for (int d = 0; d < inputDepth; d++)\n+            for (int kh = 0; kh < kernelSize; kh++)\n+                for (int kw = 0; kw < kernelSize; kw++)\n+                    nhwc[d, kh, kw, 0] = kernel[d, 0, kh, kw];\n+\n+        return nhwc;\n+    }\n+\n+    /// <summary>\n+    /// Converts pointwise kernel from [inputDepth, 1, 1, outputDepth] to [outputDepth, inputDepth, 1, 1] format.\n+    /// </summary>\n+    private Tensor<T> ConvertPointwiseKernelToNCHW(Tensor<T> kernel)\n+    {\n+        int inputDepth = kernel.Shape[0];\n+        int outputDepth = kernel.Shape[3];\n+\n+        var nchw = new Tensor<T>([outputDepth, inputDepth, 1, 1]);\n+        for (int od = 0; od < outputDepth; od++)\n+            for (int id = 0; id < inputDepth; id++)\n+                nchw[od, id, 0, 0] = kernel[id, 0, 0, od];\n+\n+        return nchw;\n+    }\n+\n+    /// <summary>\n+    /// Converts pointwise kernel from [outputDepth, inputDepth, 1, 1] back to [inputDepth, 1, 1, outputDepth] format.\n+    /// </summary>\n+    private Tensor<T> ConvertPointwiseKernelFromNCHW(Tensor<T> kernel)\n+    {\n+        int outputDepth = kernel.Shape[0];\n+        int inputDepth = kernel.Shape[1];\n+\n+        var nhwc = new Tensor<T>([inputDepth, 1, 1, outputDepth]);\n+        for (int id = 0; id < inputDepth; id++)\n+            for (int od = 0; od < outputDepth; od++)\n+                nhwc[id, 0, 0, od] = kernel[od, id, 0, 0];\n+\n+        return nhwc;\n+    }\n+\n+    /// <summary>\n+    /// Converts vector to 1D tensor.\n+    /// </summary>\n+    private Tensor<T> ConvertVectorToTensor(Vector<T> vector)\n+    {\n+        var tensor = new Tensor<T>([vector.Length]);\n+        for (int i = 0; i < vector.Length; i++)\n+            tensor[i] = vector[i];\n+        return tensor;\n+    }\n+\n+    /// <summary>\n+    /// Converts 1D tensor to vector.\n+    /// </summary>\n+    private Vector<T> ConvertTensorToVector(Tensor<T> tensor)\n+    {\n+        var vector = new Vector<T>(tensor.Shape[0]);\n+        for (int i = 0; i < tensor.Shape[0]; i++)\n+            vector[i] = tensor[i];\n+        return vector;\n+    }\n+\n+    /// <summary>\n+    /// Applies activation function using autodiff operations.\n+    /// </summary>\n+    private Autodiff.ComputationNode<T> ApplyActivationAutodiff(Autodiff.ComputationNode<T> input)\n+    {\n+        // Apply the appropriate activation function\n+        if (UsingVectorActivation)\n+        {\n+            if (VectorActivation is ReLUActivation<T>)\n+                return Autodiff.TensorOperations<T>.ReLU(input);\n+            else if (VectorActivation is SigmoidActivation<T>)\n+                return Autodiff.TensorOperations<T>.Sigmoid(input);\n+            else if (VectorActivation is TanhActivation<T>)\n+                return Autodiff.TensorOperations<T>.Tanh(input);\n+            else\n+                throw new NotSupportedException($\"Activation {VectorActivation.GetType().Name} not yet supported in autodiff\");\n+        }\n+        else\n+        {\n+            if (ScalarActivation is ReLUActivation<T>)\n+                return Autodiff.TensorOperations<T>.ReLU(input);\n+            else if (ScalarActivation is SigmoidActivation<T>)\n+                return Autodiff.TensorOperations<T>.Sigmoid(input);\n+            else if (ScalarActivation is TanhActivation<T>)\n+                return Autodiff.TensorOperations<T>.Tanh(input);\n+            else\n+                throw new NotSupportedException($\"Activation {ScalarActivation.GetType().Name} not yet supported in autodiff\");\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Gets the topological order of nodes in the computation graph.\n+    /// </summary>\n+    /// <param name=\"root\">The root node of the computation graph.</param>\n+    /// <returns>A list of nodes in topological order.</returns>\n+    private List<Autodiff.ComputationNode<T>> GetTopologicalOrder(Autodiff.ComputationNode<T> root)\n+    {\n+        var visited = new HashSet<Autodiff.ComputationNode<T>>();\n+        var result = new List<Autodiff.ComputationNode<T>>();\n+\n+        var stack = new Stack<(Autodiff.ComputationNode<T> node, bool processed)>();\n+        stack.Push((root, false));\n+\n+        while (stack.Count > 0)\n+        {\n+            var (node, processed) = stack.Pop();\n+\n+            if (visited.Contains(node))\n+            {\n+                continue;\n+            }\n+\n+            if (processed)\n+            {\n+                visited.Add(node);\n+                result.Add(node);\n+            }\n+            else\n+            {\n+                stack.Push((node, true));\n+\n+                foreach (var parent in node.Parents)\n+                {\n+                    if (!visited.Contains(parent))\n+                    {\n+                        stack.Push((parent, false));\n+                    }\n+                }\n+            }\n+        }\n+\n+        return result;\n+    }"}]}},{"id":"PRRT_kwDOKSXUF85hlmqh","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/SpatialTransformerLayer.cs","line":953,"originalLine":953,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V_i0M","author":{"login":"coderabbitai"},"body":"_⚠️ Potential issue_ | _🔴 Critical_\n\n**Critical: Incomplete batch handling and gradient computation in autodiff backward path.**\n\nThe autodiff backward implementation has several correctness issues that will produce incorrect gradients during training:\n\n1. **Lines 937-948**: Only the first batch item is processed for `_localizationWeights2Gradient` due to `Math.Min(1, batchSize)`. Gradients should accumulate contributions from **all** batch items, not just the first.\n\n2. **Lines 910-923**: Averaging theta gradients across the batch dimension is a simplification. The proper approach is to process each batch item's contribution to the localization network gradients.\n\n3. **Lines 925-948**: The localization network backpropagation is incomplete:\n   - Missing activation function gradient application (compare with `BackwardManual` line 1155 which calls `ApplyActivationGradient`)\n   - The `localization1` computation at line 928 isn't used correctly for gradient flow through the activation\n   - `_localizationWeights1Gradient` and `_localizationBias1Gradient` are never computed\n\n4. **Lines 850-862**: The comment \"Current implementation uses single transformation matrix for simplicity\" is misleading. The forward pass computes a single transformation matrix (line 520), but here it's replicated across batch items. This inconsistency needs clarification or correction.\n\nThese issues mean that training with `UseAutodiff = true` will produce incorrect parameter updates and the network won't learn properly.\n\n\n\n**Recommendation**: Either complete the implementation to properly handle all batch items and the full localization network backpropagation, or clearly document this as a work-in-progress with a TODO/FIXME comment and disable autodiff for this layer until it's complete.\n\n**Example fix skeleton for proper batch handling:**\n\n```csharp\n// Process each batch item's contribution\nfor (int b = 0; b < batchSize; b++)\n{\n    // Extract theta gradient for this batch item\n    var thetaGradBatch = new Tensor<T>([1, 6]);\n    for (int i = 0; i < 2; i++)\n    {\n        for (int j = 0; j < 3; j++)\n        {\n            thetaGradBatch[0, i * 3 + j] = thetaNode.Gradient[b, i, j];\n        }\n    }\n    \n    // Compute localization network gradients for this batch item\n    // ... (proper gradient computation through activation)\n    \n    // Accumulate into parameter gradients\n    // ...\n}\n```\n\n<details>\n<summary>🤖 Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/Layers/SpatialTransformerLayer.cs around lines 839-953,\nthe autodiff backward pass incorrectly handles batching and localization-network\ngradients: it averages theta across the batch and only uses a single batch item\nto compute _localizationWeights2Gradient, never computes\n_localizationWeights1Gradient/_localizationBias1Gradient, and omits applying the\nactivation gradient; fix this by iterating over all batch items, extracting\nper-batch theta gradients from thetaNode.Gradient, backpropagating each\nthetaGrad through the localization network (apply the same activation derivative\nused in BackwardManual), compute per-batch gradients for weights2, bias2,\nweights1 and bias1, and accumulate (sum) these into the layer parameter gradient\nmatrices/vectors; additionally, remove or correct the misleading comment about a\nsingle shared transformation matrix (or explicitly document replication), and\nensure input gradients are accumulated across the batch before returning.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","createdAt":"2025-11-12T02:20:33Z","path":"src/NeuralNetworks/Layers/SpatialTransformerLayer.cs","line":953,"originalLine":953,"diffHunk":"@@ -811,6 +824,162 @@ public override Tensor<T> Backward(Tensor<T> outputGradient)\n         return inputGradient;\n     }\n \n+    /// <summary>\n+    /// Backward pass implementation using automatic differentiation.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">The gradient of the loss with respect to the layer's output.</param>\n+    /// <returns>The gradient of the loss with respect to the layer's input.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method uses automatic differentiation via AffineGrid and GridSample operations.\n+    /// The localization network gradients are computed using standard matrix operations,\n+    /// while the spatial transformation uses the specialized autodiff operations.\n+    /// </para>\n+    /// </remarks>\n+    private Tensor<T> BackwardViaAutodiff(Tensor<T> outputGradient)\n+    {\n+        if (_lastInput == null || _lastTransformationMatrix == null)\n+            throw new InvalidOperationException(\"Forward pass must be called before backward pass.\");\n+\n+        int batchSize = _lastInput.Shape[0];\n+\n+        // For now, SpatialTransformer with full localization network autodiff is complex\n+        // We use the specialized AffineGrid and GridSample operations for the transformation part\n+        // but handle the localization network manually to avoid excessive complexity\n+\n+        // Convert transformation matrix to tensor [batch, 2, 3]\n+        // Note: Current implementation uses single transformation matrix for simplicity\n+        var thetaTensor = new Tensor<T>([batchSize, 2, 3]);\n+        for (int b = 0; b < batchSize; b++)\n+        {\n+            for (int i = 0; i < 2; i++)\n+            {\n+                for (int j = 0; j < 3; j++)\n+                {\n+                    thetaTensor[b, i, j] = _lastTransformationMatrix[i, j];\n+                }\n+            }\n+        }\n+\n+        // Create computation nodes\n+        var inputNode = Autodiff.TensorOperations<T>.Variable(\n+            _lastInput,\n+            \"input\",\n+            requiresGradient: true);\n+\n+        var thetaNode = Autodiff.TensorOperations<T>.Variable(\n+            thetaTensor,\n+            \"theta\",\n+            requiresGradient: true);\n+\n+        // Apply AffineGrid to generate sampling grid\n+        var gridNode = Autodiff.TensorOperations<T>.AffineGrid(\n+            thetaNode,\n+            _outputHeight,\n+            _outputWidth);\n+\n+        // Apply GridSample to sample from input\n+        var outputNode = Autodiff.TensorOperations<T>.GridSample(\n+            inputNode,\n+            gridNode);\n+\n+        // Set the output gradient\n+        outputNode.Gradient = outputGradient;\n+\n+        // Perform backward pass\n+        var topoOrder = GetTopologicalOrder(outputNode);\n+        for (int i = topoOrder.Count - 1; i >= 0; i--)\n+        {\n+            var node = topoOrder[i];\n+            if (node.RequiresGradient && node.BackwardFunction != null && node.Gradient != null)\n+            {\n+                node.BackwardFunction(node.Gradient);\n+            }\n+        }\n+\n+        // For the localization network parameters, we need to backpropagate from theta gradients\n+        // This requires converting theta gradients back and computing localization network gradients\n+        if (thetaNode.Gradient != null)\n+        {\n+            // Initialize parameter gradients\n+            _localizationWeights1Gradient = new Matrix<T>(_localizationWeights1.Rows, _localizationWeights1.Columns);\n+            _localizationBias1Gradient = new Vector<T>(_localizationBias1.Length);\n+            _localizationWeights2Gradient = new Matrix<T>(_localizationWeights2.Rows, _localizationWeights2.Columns);\n+            _localizationBias2Gradient = new Vector<T>(_localizationBias2.Length);\n+\n+            // Extract theta gradient (averaging across batch for simplicity in this implementation)\n+            var thetaGrad = new Tensor<T>([1, 6]);\n+            for (int i = 0; i < 2; i++)\n+            {\n+                for (int j = 0; j < 3; j++)\n+                {\n+                    T avgGrad = NumOps.Zero;\n+                    for (int b = 0; b < batchSize; b++)\n+                    {\n+                        avgGrad = NumOps.Add(avgGrad, thetaNode.Gradient[b, i, j]);\n+                    }\n+                    thetaGrad[0, i * 3 + j] = NumOps.Divide(avgGrad, NumOps.FromInt(batchSize));\n+                }\n+            }\n+\n+            // Backpropagate through localization network\n+            // This is a simplified version - full implementation would process each batch item\n+            var flattenedInput = _lastInput.Reshape(batchSize, _inputHeight * _inputWidth);\n+            var localization1 = flattenedInput.Multiply(_localizationWeights1).Add(_localizationBias1);\n+\n+            // Gradient for localization bias2\n+            for (int i = 0; i < 6; i++)\n+            {\n+                _localizationBias2Gradient[i] = thetaGrad[0, i];\n+            }\n+\n+            // Gradient for localization weights2\n+            for (int b = 0; b < Math.Min(1, batchSize); b++)  // Simplified: single batch item\n+            {\n+                for (int i = 0; i < 32; i++)\n+                {\n+                    for (int j = 0; j < 6; j++)\n+                    {\n+                        _localizationWeights2Gradient[i, j] = NumOps.Multiply(\n+                            localization1[b, i],\n+                            thetaGrad[0, j]);\n+                    }\n+                }\n+            }\n+        }\n+\n+        // Return input gradient\n+        return inputNode.Gradient ?? throw new InvalidOperationException(\"Gradient computation failed.\");\n+    }"}]}},{"id":"PRRT_kwDOKSXUF85hlmqr","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/SubpixelConvolutionalLayer.cs","line":756,"originalLine":756,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V_i0Y","author":{"login":"coderabbitai"},"body":"_⚠️ Potential issue_ | _🔴 Critical_\n\n**Critical: Incomplete autodiff backward pass that doesn't match the forward pass.**\n\nThis autodiff implementation has several critical issues:\n\n1. **Missing convolution step**: The forward pass performs convolution (lines 529-561) before pixel shuffling, but the autodiff path (line 741) only calls `PixelShuffle` directly on the input without any convolution operation.\n\n2. **Missing activation step**: The forward pass applies activation (line 586), but the autodiff path doesn't include activation in the computation graph.\n\n3. **Parameter gradients not computed**: The manual backward path sets `_kernelGradients` and `_biasGradients` (lines 673-674), which are required by `UpdateParameters` (line 937). The autodiff path doesn't compute or set these gradients, so calling `UpdateParameters` after an autodiff backward pass would throw an `InvalidOperationException`.\n\n4. **Inconsistent documentation**: The documentation (lines 722-723) claims this method \"falls back to the manual implementation,\" but the code doesn't fall back—it attempts to execute an incomplete autodiff path.\n\n5. **Missing weights and biases**: The forward convolution uses `_kernels` and `_biases`, but these aren't incorporated into the autodiff computation graph.\n\n\n\nTo properly implement this method, you need to:\n\n```diff\n private Tensor<T> BackwardViaAutodiff(Tensor<T> outputGradient)\n {\n     if (_lastInput == null)\n         throw new InvalidOperationException(\"Forward pass must be called before backward pass.\");\n\n-    // Convert input to computation node\n-    var inputNode = Autodiff.TensorOperations<T>.Variable(_lastInput, \"input\", requiresGradient: true);\n-\n-    // Apply pixel shuffle operation\n-    var outputNode = Autodiff.TensorOperations<T>.PixelShuffle(inputNode, UpscaleFactor);\n+    // Create computation nodes for input and parameters\n+    var inputNode = Autodiff.TensorOperations<T>.Variable(_lastInput, \"input\", requiresGradient: true);\n+    var kernelsNode = Autodiff.TensorOperations<T>.Variable(_kernels, \"kernels\", requiresGradient: true);\n+    var biasesNode = Autodiff.TensorOperations<T>.Variable(ConvertVectorToTensor(_biases), \"biases\", requiresGradient: true);\n+\n+    // TODO: Implement or use Conv2D operation in TensorOperations\n+    // var convNode = Autodiff.TensorOperations<T>.Conv2D(inputNode, kernelsNode, biasesNode, ...);\n+    \n+    // Apply pixel shuffle operation\n+    // var shuffleNode = Autodiff.TensorOperations<T>.PixelShuffle(convNode, _upscaleFactor);\n+    \n+    // Apply activation\n+    // var outputNode = ApplyActivationAutodiff(shuffleNode);\n\n     // Perform backward pass\n     outputNode.Gradient = outputGradient;\n     var topoOrder = GetTopologicalOrder(outputNode);\n     for (int i = topoOrder.Count - 1; i >= 0; i--)\n     {\n         var node = topoOrder[i];\n         if (node.RequiresGradient && node.BackwardFunction != null && node.Gradient != null)\n         {\n             node.BackwardFunction(node.Gradient);\n         }\n     }\n+    \n+    // Extract parameter gradients\n+    if (kernelsNode.Gradient != null)\n+        _kernelGradients = kernelsNode.Gradient;\n+    \n+    if (biasesNode.Gradient != null)\n+        _biasGradients = ConvertTensorToVector(biasesNode.Gradient);\n\n     return inputNode.Gradient ?? throw new InvalidOperationException(\"Gradient computation failed.\");\n }\n```\n\nAlternatively, if subpixel convolution operations are not yet available in TensorOperations (as the documentation suggests), the method should explicitly fall back to the manual implementation:\n\n```diff\n private Tensor<T> BackwardViaAutodiff(Tensor<T> outputGradient)\n {\n-    if (_lastInput == null)\n-        throw new InvalidOperationException(\"Forward pass must be called before backward pass.\");\n-\n-    // Convert input to computation node\n-    var inputNode = Autodiff.TensorOperations<T>.Variable(_lastInput, \"input\", requiresGradient: true);\n-\n-    // Apply pixel shuffle operation\n-    var outputNode = Autodiff.TensorOperations<T>.PixelShuffle(inputNode, UpscaleFactor);\n-\n-    // Perform backward pass\n-    outputNode.Gradient = outputGradient;\n-    var topoOrder = GetTopologicalOrder(outputNode);\n-    for (int i = topoOrder.Count - 1; i >= 0; i--)\n-    {\n-        var node = topoOrder[i];\n-        if (node.RequiresGradient && node.BackwardFunction != null && node.Gradient != null)\n-        {\n-            node.BackwardFunction(node.Gradient);\n-        }\n-    }\n-\n-    return inputNode.Gradient ?? throw new InvalidOperationException(\"Gradient computation failed.\");\n+    // Subpixel convolution operations are not yet available in TensorOperations.\n+    // Fall back to manual implementation until autodiff support is complete.\n+    return BackwardManual(outputGradient);\n }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>📝 Committable suggestion</summary>\n\n> ‼️ **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    private Tensor<T> BackwardViaAutodiff(Tensor<T> outputGradient)\n    {\n        // Subpixel convolution operations are not yet available in TensorOperations.\n        // Fall back to manual implementation until autodiff support is complete.\n        return BackwardManual(outputGradient);\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>🤖 Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/Layers/SubpixelConvolutionalLayer.cs around lines 732 to\n756, the autodiff backward path is incomplete: it only applies PixelShuffle to\nthe input and omits the convolution, activation and parameter nodes used in the\nforward pass, does not include _kernels/_biases in the computation graph, does\nnot produce _kernelGradients/_biasGradients required by UpdateParameters, and\nthe docstring incorrectly claims a fallback to the manual implementation. Fix by\nconstructing the same computation graph as the forward pass (create variables\nfor _kernels and _biases, perform convolution on the inputNode, add bias and\nactivation, then PixelShuffle), wire gradients through that graph and after\nbackward pass extract and assign the parameter gradients to _kernelGradients and\n_biasGradients, and update the docstring; alternatively, if TensorOperations\nlacks subpixel convolution primitives, explicitly call the existing manual\nBackward implementation (and document that fallback) instead of the current\nincomplete autodiff path.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","createdAt":"2025-11-12T02:20:33Z","path":"src/NeuralNetworks/Layers/SubpixelConvolutionalLayer.cs","line":756,"originalLine":756,"diffHunk":"@@ -698,6 +712,93 @@ public override Tensor<T> Backward(Tensor<T> outputGradient)\n         return inputGradient;\n     }\n \n+    /// <summary>\n+    /// Backward pass implementation using automatic differentiation.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">The gradient of the loss with respect to the layer's output.</param>\n+    /// <returns>The gradient of the loss with respect to the layer's input.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method uses automatic differentiation to compute gradients. Currently, subpixel convolution operations\n+    /// are not yet available in TensorOperations, so this method falls back to the manual implementation.\n+    /// </para>\n+    /// <para>\n+    /// Once subpixel convolution operations are added to TensorOperations, this method will provide:\n+    /// - Automatic gradient computation through the computation graph\n+    /// - Verification of manual gradient implementations\n+    /// - Support for rapid prototyping with custom modifications\n+    /// </para>\n+    /// </remarks>\n+    private Tensor<T> BackwardViaAutodiff(Tensor<T> outputGradient)\n+    {\n+        if (_lastInput == null)\n+            throw new InvalidOperationException(\"Forward pass must be called before backward pass.\");\n+\n+        // Convert input to computation node\n+        var inputNode = Autodiff.TensorOperations<T>.Variable(_lastInput, \"input\", requiresGradient: true);\n+\n+        // Apply pixel shuffle operation\n+        var outputNode = Autodiff.TensorOperations<T>.PixelShuffle(inputNode, UpscaleFactor);\n+\n+        // Perform backward pass\n+        outputNode.Gradient = outputGradient;\n+        var topoOrder = GetTopologicalOrder(outputNode);\n+        for (int i = topoOrder.Count - 1; i >= 0; i--)\n+        {\n+            var node = topoOrder[i];\n+            if (node.RequiresGradient && node.BackwardFunction != null && node.Gradient != null)\n+            {\n+                node.BackwardFunction(node.Gradient);\n+            }\n+        }\n+\n+        return inputNode.Gradient ?? throw new InvalidOperationException(\"Gradient computation failed.\");\n+    }"}]}},{"id":"PRRT_kwDOKSXUF85hl4Ft","isResolved":false,"isOutdated":false,"path":"src/NeuralNetworks/Layers/AnomalyDetectorLayer.cs","line":530,"originalLine":530,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V_8kR","author":{"login":"coderabbitai"},"body":"_⚠️ Potential issue_ | _🟠 Major_\n\n**Misleading documentation: BackwardViaAutodiff doesn't actually use autodiff.**\n\nThe documentation at lines 514-518 claims this method \"uses automatic differentiation to compute gradients,\" but the implementation manually creates zero gradients identical to `BackwardManual`. Unlike the autodiff pattern shown in `GraphConvolutionalLayer.cs` (which uses `ComputationNode`, `TensorOperations`, and gradient propagation), this method doesn't use any autodiff infrastructure.\n\nSince this layer:\n- Has no trainable parameters (`SupportsTraining = false`)\n- Uses a manual forward pass that doesn't create a computation graph\n- Legitimately returns zero gradients\n\nConsider one of these approaches:\n\n1. **Update documentation to reflect actual behavior** (simpler, recommended):\n\n```diff\n /// <summary>\n-/// Backward pass implementation using automatic differentiation.\n+/// Backward pass stub that returns zero gradients (autodiff variant).\n /// </summary>\n /// <param name=\"outputGradient\">The gradient of the loss with respect to the layer's output.</param>\n /// <returns>The gradient of the loss with respect to the layer's input.</returns>\n /// <remarks>\n /// <para>\n-/// This method uses automatic differentiation to compute gradients. Since this layer\n-/// has no trainable parameters and serves as a monitoring layer, it returns zero gradients.\n-/// This matches the manual implementation behavior.\n+/// Since this layer has no trainable parameters and serves as a monitoring layer,\n+/// it returns zero gradients. This matches the manual implementation behavior.\n+/// A full autodiff implementation is not needed because the forward pass doesn't\n+/// produce a differentiable computation graph.\n /// </para>\n /// </remarks>\n```\n\n2. **Remove BackwardViaAutodiff entirely** and have `Backward` always call `BackwardManual`, since both implementations are identical.\n\n<details>\n<summary>🤖 Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/Layers/AnomalyDetectorLayer.cs around lines 508 to 530,\nthe XML doc incorrectly states that BackwardViaAutodiff \"uses automatic\ndifferentiation\" while the implementation manually constructs and returns zero\ngradients; update the documentation to accurately state that this method does\nnot use autodiff and simply returns zero gradients (or alternatively delete the\nBackwardViaAutodiff method and route Backward to BackwardManual if you prefer to\nremove duplication); ensure the summary and remarks reflect that the layer has\nno trainable parameters, no computation graph is built, and zero gradients are\nreturned to match BackwardManual.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","createdAt":"2025-11-12T03:16:53Z","path":"src/NeuralNetworks/Layers/AnomalyDetectorLayer.cs","line":530,"originalLine":530,"diffHunk":"@@ -464,31 +464,68 @@ public Dictionary<string, double> GetAnomalyStatistics()\n     /// trainable parameters, it simply passes the gradient through to the previous layer.\n     /// </para>\n     /// <para><b>For Beginners:</b> This method passes error information backward during training.\n-    /// \n+    ///\n     /// The backward pass:\n     /// - Takes an error gradient from the next layer\n     /// - Propagates it back to the previous layer\n     /// - Doesn't modify any parameters since this layer doesn't learn\n-    /// \n+    ///\n     /// This method exists to maintain compatibility with the neural network\n     /// backpropagation mechanism, but it doesn't do much in this layer\n     /// since there are no weights to adjust.\n     /// </para>\n     /// </remarks>\n     public override Tensor<T> Backward(Tensor<T> outputGradient)\n+    {\n+        if (UseAutodiff)\n+            return BackwardViaAutodiff(outputGradient);\n+        else\n+            return BackwardManual(outputGradient);\n+    }\n+\n+    /// <summary>\n+    /// Manual backward pass implementation using optimized gradient calculations.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">The gradient of the loss with respect to the layer's output.</param>\n+    /// <returns>The gradient of the loss with respect to the layer's input.</returns>\n+    private Tensor<T> BackwardManual(Tensor<T> outputGradient)\n     {\n         // Since this layer doesn't have trainable parameters, we just propagate the gradient\n         // back to the input. For anomaly detection, this is primarily a pass-through operation.\n-        \n+\n         // Create an input gradient of the same size as the input\n         var inputGradient = new Vector<T>(InputShape[0]);\n-        \n+\n         // Set all gradients to zero since we don't directly optimize for anomaly detection\n         for (int i = 0; i < inputGradient.Length; i++)\n         {\n             inputGradient[i] = NumOps.Zero;\n         }\n-        \n+\n+        return Tensor<T>.FromVector(inputGradient);\n+    }\n+\n+    /// <summary>\n+    /// Backward pass implementation using automatic differentiation.\n+    /// </summary>\n+    /// <param name=\"outputGradient\">The gradient of the loss with respect to the layer's output.</param>\n+    /// <returns>The gradient of the loss with respect to the layer's input.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method uses automatic differentiation to compute gradients. Since this layer\n+    /// has no trainable parameters and serves as a monitoring layer, it returns zero gradients.\n+    /// This matches the manual implementation behavior.\n+    /// </para>\n+    /// </remarks>\n+    private Tensor<T> BackwardViaAutodiff(Tensor<T> outputGradient)\n+    {\n+        // AnomalyDetectorLayer has no trainable parameters and is typically used for monitoring.\n+        // Return zero gradients to match manual implementation.\n+        var inputGradient = new Vector<T>(InputShape[0]);\n+        for (int i = 0; i < inputGradient.Length; i++)\n+        {\n+            inputGradient[i] = NumOps.Zero;\n+        }\n         return Tensor<T>.FromVector(inputGradient);\n     }"}]}}]}}}}}