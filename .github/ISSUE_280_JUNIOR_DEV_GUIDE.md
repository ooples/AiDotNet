# Issue #280: ONNX Export and ONNX Runtime Execution
## Junior Developer Implementation Guide

**For**: Developers enabling ONNX interoperability for production deployment
**Difficulty**: Intermediate to Advanced
**Estimated Time**: 30-40 hours
**Prerequisites**: Understanding of neural networks, ONNX format basics

---

## Understanding ONNX

**For Beginners**: ONNX (Open Neural Network Exchange) is like a universal translator for AI models. Train in PyTorch, deploy in TensorFlow, run in C# - ONNX makes it all possible!

**Why ONNX Matters**:
- **Interoperability**: Use models from any framework (PyTorch, TensorFlow, etc.)
- **Performance**: ONNX Runtime is highly optimized (often faster than original framework!)
- **Production**: Run models in C++, C#, JavaScript, Python - anywhere
- **Ecosystem**: Access to thousands of pre-trained models on Hugging Face

**Real Example**:
```
Train: PyTorch (Python) â†’ Export: .onnx file â†’ Deploy: ONNX Runtime (C#)
Model size: 500 MB
Inference: 10ms (vs 25ms in PyTorch!)
No Python required in production!
```

---

## Architecture Overview

```
src/
â”œâ”€â”€ Onnx/
â”‚   â”œâ”€â”€ OnnxModel.cs                    [NEW - AC 1.2]
â”‚   â”œâ”€â”€ OnnxExporter.cs                 [NEW - AC 2.1]
â”‚   â””â”€â”€ OnnxHelper.cs                   [NEW - helper methods]
â””â”€â”€ Interfaces/
    â””â”€â”€ IOnnxExportable.cs              [NEW - optional]

tests/
â””â”€â”€ IntegrationTests/
    â”œâ”€â”€ OnnxModelTests.cs               [NEW - AC 3.1]
    â””â”€â”€ OnnxRoundTripTests.cs           [NEW - AC 3.2]
```

---

## Phase 1: ONNX Runtime Integration

### AC 1.1: Add ONNX Runtime Dependency (1 point)

**Modify**: `C:\Users\cheat\source\repos\AiDotNet\src\AiDotNet.csproj`

```xml
<ItemGroup>
  <PackageReference Include="Microsoft.ML.OnnxRuntime" Version="1.19.0" />
  <PackageReference Include="Microsoft.ML.OnnxRuntime.DirectML" Version="1.19.0" Condition="'$(OS)' == 'Windows_NT'" />
  <PackageReference Include="Microsoft.ML.OnnxRuntime.Gpu" Version="1.19.0" Condition="'$(OS)' != 'Windows_NT'" />
</ItemGroup>
```

**Explanation**:
- **OnnxRuntime**: Core library (CPU)
- **DirectML**: Windows GPU acceleration (AMD/NVIDIA/Intel)
- **Gpu**: Linux/Mac GPU (NVIDIA CUDA only)

### AC 1.2: Implement OnnxModel Wrapper (8 points)

**File**: `C:\Users\cheat\source\repos\AiDotNet\src\Onnx\OnnxModel.cs`

```csharp
using AiDotNet.Interfaces;
using AiDotNet.LinearAlgebra;
using Microsoft.ML.OnnxRuntime;
using Microsoft.ML.OnnxRuntime.Tensors;
using System;
using System.Linq;

namespace AiDotNet.Onnx;

/// <summary>
/// Wraps an ONNX model for execution in AiDotNet.
/// </summary>
/// <typeparam name="T">Numeric type (float or double).</typeparam>
/// <remarks>
/// <para>
/// <b>For Beginners:</b> This class lets you run any ONNX model (from PyTorch, TensorFlow, etc.)
/// as if it were a native AiDotNet model. Just load the .onnx file and call Forward()!
/// </para>
/// <para>
/// <b>Example:</b>
/// <code>
/// var model = new OnnxModel<float>("bert-base-uncased.onnx");
/// var input = CreateInputTensor();
/// var output = model.Forward(input);
/// </code>
/// </para>
/// </remarks>
public class OnnxModel<T> : IModel<T>
{
    private readonly InferenceSession _session;
    private readonly string _inputName;
    private readonly string _outputName;

    /// <summary>
    /// Loads an ONNX model from file.
    /// </summary>
    /// <param name="modelPath">Path to .onnx file.</param>
    /// <param name="options">Optional session options for GPU acceleration.</param>
    public OnnxModel(string modelPath, SessionOptions? options = null)
    {
        if (!File.Exists(modelPath))
            throw new FileNotFoundException($"ONNX model not found: {modelPath}");

        // Create session with options
        options ??= CreateDefaultOptions();
        _session = new InferenceSession(modelPath, options);

        // Get input/output names
        _inputName = _session.InputMetadata.Keys.First();
        _outputName = _session.OutputMetadata.Keys.First();

        Console.WriteLine($"ONNX model loaded: {modelPath}");
        Console.WriteLine($"  Input: {_inputName} {string.Join("Ã—", _session.InputMetadata[_inputName].Dimensions)}");
        Console.WriteLine($"  Output: {_outputName} {string.Join("Ã—", _session.OutputMetadata[_outputName].Dimensions)}");
    }

    /// <summary>
    /// Creates default session options with GPU if available.
    /// </summary>
    private static SessionOptions CreateDefaultOptions()
    {
        var options = new SessionOptions();

        try
        {
            // Try DirectML (Windows GPU)
            options.AppendExecutionProvider_DML(0);
            Console.WriteLine("Using DirectML GPU acceleration");
        }
        catch
        {
            try
            {
                // Try CUDA (Linux/Mac GPU)
                options.AppendExecutionProvider_CUDA(0);
                Console.WriteLine("Using CUDA GPU acceleration");
            }
            catch
            {
                // Fall back to CPU
                Console.WriteLine("Using CPU execution");
            }
        }

        return options;
    }

    /// <summary>
    /// Runs forward pass through ONNX model.
    /// </summary>
    public Tensor<T> Forward(Tensor<T> input)
    {
        // Convert AiDotNet tensor â†’ ONNX tensor
        var onnxInput = ConvertToOnnx(input);

        // Run inference
        var inputs = new[] { NamedOnnxValue.CreateFromTensor(_inputName, onnxInput) };
        using var results = _session.Run(inputs);

        // Convert ONNX output â†’ AiDotNet tensor
        var onnxOutput = results.First().AsTensor<T>();
        var output = ConvertFromOnnx(onnxOutput);

        return output;
    }

    /// <summary>
    /// Converts AiDotNet tensor to ONNX DenseTensor.
    /// </summary>
    private DenseTensor<T> ConvertToOnnx(Tensor<T> tensor)
    {
        var dims = tensor.Shape;
        var onnxTensor = new DenseTensor<T>(dims);

        // Copy data
        var data = tensor.GetData();
        var span = onnxTensor.Buffer.Span;

        for (int i = 0; i < data.Length; i++)
            span[i] = data[i];

        return onnxTensor;
    }

    /// <summary>
    /// Converts ONNX DenseTensor to AiDotNet tensor.
    /// </summary>
    private Tensor<T> ConvertFromOnnx(Tensor<T> onnxTensor)
    {
        var dims = onnxTensor.Dimensions.ToArray();
        var result = new Tensor<T>(dims);

        var data = new T[onnxTensor.Length];
        onnxTensor.CopyTo(data);
        result.SetData(data);

        return result;
    }

    /// <summary>
    /// Not supported for ONNX models (inference only).
    /// </summary>
    public Tensor<T> Backward(Tensor<T> gradient)
    {
        throw new NotSupportedException("ONNX models are inference-only. For training, use native AiDotNet models.");
    }

    public Vector<T> GetParameters() => throw new NotSupportedException();
    public void SetParameters(Vector<T> parameters) => throw new NotSupportedException();
    public void UpdateParameters(T learningRate) => throw new NotSupportedException();

    /// <summary>
    /// Disposes ONNX session.
    /// </summary>
    public void Dispose()
    {
        _session?.Dispose();
    }
}
```

---

## Phase 2: ONNX Exporter

### AC 2.1: Implement Basic OnnxExporter (13 points)

**File**: `C:\Users\cheat\source\repos\AiDotNet\src\Onnx\OnnxExporter.cs`

```csharp
using AiDotNet.Interfaces;
using AiDotNet.NeuralNetworks.Layers;
using Onnx;
using System;
using System.Collections.Generic;
using System.IO;

namespace AiDotNet.Onnx;

/// <summary>
/// Exports AiDotNet models to ONNX format.
/// </summary>
/// <remarks>
/// <para>
/// <b>Limitations:</b> This proof-of-concept exporter supports:
/// - Linear (fully-connected) layers
/// - ReLU, Sigmoid, Tanh activations
/// - Sequential models only (no branching)
///
/// For advanced architectures (transformers, ResNets, etc.), use PyTorch export instead.
/// </para>
/// </remarks>
public static class OnnxExporter
{
    /// <summary>
    /// Exports a simple sequential model to ONNX format.
    /// </summary>
    /// <param name="model">AiDotNet model to export.</param>
    /// <param name="outputPath">Path to save .onnx file.</param>
    /// <param name="inputShape">Shape of model input (e.g., [1, 784] for MNIST).</param>
    public static void Export<T>(IModel<T> model, string outputPath, int[] inputShape)
    {
        Console.WriteLine($"Exporting model to ONNX: {outputPath}");

        // Create ONNX graph
        var graph = CreateGraph(model, inputShape);

        // Create ONNX model
        var onnxModel = new ModelProto
        {
            IrVersion = 8,
            OpsetImport = { new OperatorSetIdProto { Domain = "", Version = 14 } },
            Graph = graph,
            Producer = "AiDotNet",
            ModelVersion = 1
        };

        // Save to file
        using var stream = File.Create(outputPath);
        onnxModel.WriteTo(stream);

        Console.WriteLine($"Export complete. File size: {new FileInfo(outputPath).Length / 1024} KB");
    }

    /// <summary>
    /// Creates ONNX graph from model.
    /// </summary>
    private static GraphProto CreateGraph<T>(IModel<T> model, int[] inputShape)
    {
        var graph = new GraphProto { Name = "AiDotNetModel" };
        var nodes = new List<NodeProto>();
        var initializers = new List<TensorProto>();

        // Input
        var inputName = "input";
        graph.Input.Add(CreateValueInfo(inputName, inputShape));

        // Convert layers to ONNX nodes
        string currentOutput = inputName;
        int nodeIndex = 0;

        foreach (var layer in GetLayers(model))
        {
            if (layer is Linear<T> linear)
            {
                // Create MatMul + Add nodes for Linear layer
                var (matmulNode, addNode, weight, bias) = ConvertLinearLayer(linear, currentOutput, nodeIndex);

                nodes.Add(matmulNode);
                nodes.Add(addNode);
                initializers.Add(weight);
                initializers.Add(bias);

                currentOutput = addNode.Output[0];
                nodeIndex += 2;
            }
            else if (layer is ReLU<T>)
            {
                var reluNode = new NodeProto
                {
                    OpType = "Relu",
                    Input = { currentOutput },
                    Output = { $"relu_{nodeIndex}" },
                    Name = $"relu_{nodeIndex}"
                };

                nodes.Add(reluNode);
                currentOutput = reluNode.Output[0];
                nodeIndex++;
            }
            // Add other activation types as needed
        }

        // Output
        graph.Output.Add(CreateValueInfo(currentOutput, new[] { -1, GetOutputSize(model) }));

        // Add all nodes and initializers to graph
        graph.Node.AddRange(nodes);
        graph.Initializer.AddRange(initializers);

        return graph;
    }

    /// <summary>
    /// Converts Linear layer to ONNX MatMul + Add nodes.
    /// </summary>
    private static (NodeProto matmul, NodeProto add, TensorProto weight, TensorProto bias) ConvertLinearLayer<T>(
        Linear<T> layer,
        string input,
        int index)
    {
        var weightName = $"weight_{index}";
        var biasName = $"bias_{index}";
        var matmulOutput = $"matmul_{index}";
        var addOutput = $"add_{index}";

        // MatMul node
        var matmul = new NodeProto
        {
            OpType = "MatMul",
            Input = { input, weightName },
            Output = { matmulOutput },
            Name = $"matmul_{index}"
        };

        // Add node (for bias)
        var add = new NodeProto
        {
            OpType = "Add",
            Input = { matmulOutput, biasName },
            Output = { addOutput },
            Name = $"add_{index}"
        };

        // Extract weights and bias
        var parameters = layer.GetParameters();
        var weightTensor = CreateTensorProto(weightName, ExtractWeights(parameters, layer.InputSize, layer.OutputSize));
        var biasTensor = CreateTensorProto(biasName, ExtractBias(parameters, layer.OutputSize));

        return (matmul, add, weightTensor, biasTensor);
    }

    /// <summary>
    /// Creates ONNX ValueInfoProto for input/output.
    /// </summary>
    private static ValueInfoProto CreateValueInfo(string name, int[] shape)
    {
        var valueInfo = new ValueInfoProto { Name = name };
        var tensorType = new TypeProto.Types.Tensor
        {
            ElemType = (int)TensorProto.Types.DataType.Float,
            Shape = new TensorShapeProto()
        };

        foreach (var dim in shape)
        {
            tensorType.Shape.Dim.Add(new TensorShapeProto.Types.Dimension
            {
                DimValue = dim
            });
        }

        valueInfo.Type = new TypeProto { TensorType = tensorType };
        return valueInfo;
    }

    /// <summary>
    /// Creates ONNX TensorProto from data.
    /// </summary>
    private static TensorProto CreateTensorProto(string name, float[] data, int[] shape = null)
    {
        shape ??= new[] { data.Length };

        var tensor = new TensorProto
        {
            Name = name,
            DataType = (int)TensorProto.Types.DataType.Float
        };

        tensor.Dims.AddRange(shape);
        tensor.FloatData.AddRange(data);

        return tensor;
    }

    // Helper methods (extract weights, get layers, etc.)
    private static IEnumerable<ILayer<T>> GetLayers<T>(IModel<T> model)
    {
        // Reflection to get all ILayer<T> properties
        var props = model.GetType().GetProperties();
        foreach (var prop in props)
        {
            if (typeof(ILayer<T>).IsAssignableFrom(prop.PropertyType))
            {
                yield return (ILayer<T>)prop.GetValue(model);
            }
        }
    }

    private static float[] ExtractWeights<T>(Vector<T> parameters, int inputSize, int outputSize)
    {
        // Implementation depends on parameter layout
        // This is a simplified version
        var weights = new float[inputSize * outputSize];
        // ... extract and convert to float[]
        return weights;
    }

    private static float[] ExtractBias<T>(Vector<T> parameters, int outputSize)
    {
        var bias = new float[outputSize];
        // ... extract from parameters
        return bias;
    }

    private static int GetOutputSize<T>(IModel<T> model)
    {
        // Get output dimension from model
        return 10; // Placeholder
    }
}
```

---

## Phase 3: Testing

### AC 3.1: OnnxModel Integration Test (5 points)

```csharp
[Fact]
public void OnnxModel_LoadsAndRunsInference()
{
    // Use a simple pre-existing ONNX model
    var modelPath = "models/simple_linear.onnx";

    var model = new OnnxModel<float>(modelPath);

    // Create input
    var input = new Tensor<float>(new[] { 1, 784 });
    // ... fill with test data

    // Run inference
    var output = model.Forward(input);

    // Verify output shape
    Assert.Equal(new[] { 1, 10 }, output.Shape);
}
```

### AC 3.2: Round-Trip Test (8 points)

```csharp
[Fact]
public void OnnxExport_RoundTrip_ProducesSameOutput()
{
    // Create simple AiDotNet model
    var originalModel = new SequentialModel<float>
    {
        new Linear<float>(784, 128),
        new ReLU<float>(),
        new Linear<float>(128, 10)
    };

    // Create test input
    var input = CreateRandomTensor(1, 784);

    // Get original output
    var originalOutput = originalModel.Forward(input);

    // Export to ONNX
    var onnxPath = Path.GetTempFileName() + ".onnx";
    OnnxExporter.Export(originalModel, onnxPath, new[] { 1, 784 });

    // Load ONNX model
    var onnxModel = new OnnxModel<float>(onnxPath);

    // Get ONNX output
    var onnxOutput = onnxModel.Forward(input);

    // Compare outputs (should be very close)
    AssertTensorsEqual(originalOutput, onnxOutput, tolerance: 1e-5);

    // Cleanup
    File.Delete(onnxPath);
}
```

---

## Common Pitfalls

1. **Dynamic Shapes**: ONNX requires fixed shapes. Use batch_size = -1 for dynamic batching.
2. **Missing Ops**: Not all AiDotNet operations have ONNX equivalents. Start simple!
3. **Type Mismatches**: Ensure input types match (float vs double).
4. **Provider Not Found**: Install DirectML or CUDA libraries for GPU support.

---

## Performance Benchmarks

| Model | Framework | Inference Time | Notes |
|-------|-----------|---------------|-------|
| ResNet-50 | PyTorch (Python) | 25 ms | Baseline |
| ResNet-50 | ONNX Runtime (CPU) | 18 ms | 1.4x faster |
| ResNet-50 | ONNX Runtime (GPU) | 5 ms | 5x faster! |

---

## Conclusion

ONNX integration provides:
- Interoperability with PyTorch/TensorFlow models
- 1.5-2x faster inference than original frameworks
- Deploy anywhere (C++, C#, mobile)

Ready for production! ðŸš€
