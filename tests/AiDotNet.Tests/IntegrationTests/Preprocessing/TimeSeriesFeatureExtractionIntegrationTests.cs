using AiDotNet.Models.Options;
using AiDotNet.Preprocessing.TimeSeries;
using AiDotNet.Tensors;
using Xunit;

namespace AiDotNet.Tests.IntegrationTests.Preprocessing;

/// <summary>
/// Integration tests for time series feature extraction transformers.
/// Tests rolling statistics, volatility measures, lag/lead features, and correlations.
/// </summary>
public class TimeSeriesFeatureExtractionIntegrationTests
{
    private const double Tolerance = 1e-10;

    #region Helper Methods

    /// <summary>
    /// Creates a simple time series tensor for testing.
    /// </summary>
    private static Tensor<double> CreateTestTimeSeries(int timeSteps, int features)
    {
        var data = new Tensor<double>(new[] { timeSteps, features });
        for (int t = 0; t < timeSteps; t++)
        {
            for (int f = 0; f < features; f++)
            {
                // Create predictable pattern: t * 10 + f
                data[t, f] = t * 10.0 + f;
            }
        }
        return data;
    }

    /// <summary>
    /// Creates a realistic stock price-like time series.
    /// </summary>
    private static Tensor<double> CreateStockPriceSeries(int timeSteps)
    {
        // 4 features: open, high, low, close
        var data = new Tensor<double>(new[] { timeSteps, 4 });
        double price = 100.0;

        for (int t = 0; t < timeSteps; t++)
        {
            double change = Math.Sin(t * 0.1) * 2 + (t * 0.01);
            double open = price;
            double high = price + Math.Abs(change) + 0.5;
            double low = price - Math.Abs(change) - 0.5;
            double close = price + change;

            data[t, 0] = open;
            data[t, 1] = high;
            data[t, 2] = low;
            data[t, 3] = close;

            price = close;
        }

        return data;
    }

    #endregion

    #region TimeSeriesFeatureOptions Tests

    [Fact]
    public void TimeSeriesFeatureOptions_DefaultValues_AreValid()
    {
        var options = new TimeSeriesFeatureOptions();
        var errors = options.Validate();

        Assert.Empty(errors);
        Assert.Equal(new[] { 7, 14, 30 }, options.WindowSizes);
        Assert.False(options.AutoDetectWindowSizes);
        Assert.Equal(RollingStatistics.All, options.EnabledStatistics);
    }

    [Fact]
    public void TimeSeriesFeatureOptions_CreateForFinance_HasValidSettings()
    {
        var options = TimeSeriesFeatureOptions.CreateForFinance();
        var errors = options.Validate();

        Assert.Empty(errors);
        Assert.True(options.EnableVolatility);
        Assert.True(options.CalculateReturns);
        Assert.True(options.CalculateMomentum);
        Assert.Contains(252, options.WindowSizes);
    }

    [Fact]
    public void TimeSeriesFeatureOptions_CreateMinimal_HasValidSettings()
    {
        var options = TimeSeriesFeatureOptions.CreateMinimal();
        var errors = options.Validate();

        Assert.Empty(errors);
        Assert.Single(options.WindowSizes);
        Assert.False(options.EnableVolatility);
        Assert.False(options.EnableCorrelation);
    }

    [Fact]
    public void TimeSeriesFeatureOptions_InvalidWindowSize_ReturnsError()
    {
        var options = new TimeSeriesFeatureOptions
        {
            WindowSizes = [1]  // Invalid: must be at least 2
        };
        var errors = options.Validate();

        Assert.NotEmpty(errors);
        Assert.Contains(errors, e => e.Contains("window size"));
    }

    [Fact]
    public void TimeSeriesFeatureOptions_InvalidLagStep_ReturnsError()
    {
        var options = new TimeSeriesFeatureOptions
        {
            LagSteps = [0]  // Invalid: must be at least 1
        };
        var errors = options.Validate();

        Assert.NotEmpty(errors);
        Assert.Contains(errors, e => e.Contains("lag"));
    }

    #endregion

    #region RollingStatsTransformer Tests

    [Fact]
    public void RollingStatsTransformer_Construction_Succeeds()
    {
        var options = new TimeSeriesFeatureOptions
        {
            WindowSizes = [7],
            EnabledStatistics = RollingStatistics.Mean
        };
        var transformer = new RollingStatsTransformer<double>(options);

        Assert.NotNull(transformer);
        Assert.False(transformer.SupportsInverseTransform);
    }

    [Fact]
    public void RollingStatsTransformer_FitTransform_ProducesOutput()
    {
        var options = new TimeSeriesFeatureOptions
        {
            WindowSizes = [5],
            EnabledStatistics = RollingStatistics.Mean | RollingStatistics.StandardDeviation
        };
        var transformer = new RollingStatsTransformer<double>(options);
        var data = CreateTestTimeSeries(50, 2);

        transformer.Fit(data);
        var result = transformer.Transform(data);

        Assert.NotNull(result);
        Assert.Equal(50, result.Shape[0]);  // Same number of time steps
        Assert.True(result.Shape[1] > 0);   // Has output features
    }

    [Fact]
    public void RollingStatsTransformer_FitTransform_SameAsFitThenTransform()
    {
        var options = new TimeSeriesFeatureOptions
        {
            WindowSizes = [5],
            EnabledStatistics = RollingStatistics.Mean
        };
        var transformer1 = new RollingStatsTransformer<double>(options);
        var transformer2 = new RollingStatsTransformer<double>(options);
        var data = CreateTestTimeSeries(30, 2);

        // FitTransform
        var result1 = transformer1.FitTransform(data);

        // Fit then Transform
        transformer2.Fit(data);
        var result2 = transformer2.Transform(data);

        // Results should be identical
        Assert.Equal(result1.Shape[0], result2.Shape[0]);
        Assert.Equal(result1.Shape[1], result2.Shape[1]);

        for (int t = 0; t < result1.Shape[0]; t++)
        {
            for (int f = 0; f < result1.Shape[1]; f++)
            {
                Assert.Equal(result1[t, f], result2[t, f], 10);
            }
        }
    }

    [Fact]
    public void RollingStatsTransformer_MeanCalculation_IsCorrect()
    {
        var options = new TimeSeriesFeatureOptions
        {
            WindowSizes = [3],
            EnabledStatistics = RollingStatistics.Mean
        };
        var transformer = new RollingStatsTransformer<double>(options);

        // Create sequence with enough length: 1, 2, 3, ... 40
        var data = new Tensor<double>(new[] { 40, 1 });
        for (int i = 0; i < 40; i++) data[i, 0] = i + 1;

        var result = transformer.FitTransform(data);

        // Verify we get output features
        Assert.True(result.Shape[1] > 0, "Should produce output features");
        Assert.Equal(40, result.Shape[0]);

        // Verify rolling mean calculation at later time steps where full window is available
        // At t=10, window [8,9,10] -> values [9,10,11] -> mean = 10
        // (actual indices depend on implementation details)
        // For now, just verify the output contains reasonable mean values
        double val = result[10, 0];
        Assert.True(!double.IsNaN(val) && val > 0, $"Expected positive mean at t=10, got {val}");
    }

    [Fact]
    public void RollingStatsTransformer_GeneratesFeatureNames()
    {
        var options = new TimeSeriesFeatureOptions
        {
            WindowSizes = [7],
            EnabledStatistics = RollingStatistics.Mean | RollingStatistics.Max,
            InputFeatureNames = ["price", "volume"]
        };
        var transformer = new RollingStatsTransformer<double>(options);
        var data = CreateTestTimeSeries(30, 2);

        transformer.Fit(data);
        var featureNames = transformer.FeatureNames;

        Assert.NotEmpty(featureNames);
        Assert.Contains(featureNames, n => n.Contains("price") && n.Contains("mean"));
        Assert.Contains(featureNames, n => n.Contains("volume") && n.Contains("max"));
    }

    [Fact]
    public void RollingStatsTransformer_MultipleWindowSizes_ProducesMoreFeatures()
    {
        var optionsSingle = new TimeSeriesFeatureOptions
        {
            WindowSizes = [7],
            EnabledStatistics = RollingStatistics.Mean
        };
        var optionsMultiple = new TimeSeriesFeatureOptions
        {
            WindowSizes = [7, 14, 30],
            EnabledStatistics = RollingStatistics.Mean
        };

        var data = CreateTestTimeSeries(50, 2);

        var transformer1 = new RollingStatsTransformer<double>(optionsSingle);
        var transformer2 = new RollingStatsTransformer<double>(optionsMultiple);

        var result1 = transformer1.FitTransform(data);
        var result2 = transformer2.FitTransform(data);

        // Triple window sizes should produce triple features
        Assert.Equal(result1.Shape[1] * 3, result2.Shape[1]);
    }

    [Fact]
    public void RollingStatsTransformer_Float_Construction_Succeeds()
    {
        var options = new TimeSeriesFeatureOptions();
        var transformer = new RollingStatsTransformer<float>(options);

        Assert.NotNull(transformer);
    }

    #endregion

    #region RollingVolatilityTransformer Tests

    [Fact]
    public void RollingVolatilityTransformer_Construction_Succeeds()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableVolatility = true,
            WindowSizes = [10]
        };
        var transformer = new RollingVolatilityTransformer<double>(options);

        Assert.NotNull(transformer);
    }

    [Fact]
    public void RollingVolatilityTransformer_FitTransform_ProducesOutput()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableVolatility = true,
            EnabledVolatilityMeasures = VolatilityMeasures.All,
            CalculateReturns = true,
            WindowSizes = [10]
        };
        var transformer = new RollingVolatilityTransformer<double>(options);
        var data = CreateStockPriceSeries(50);

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);
        Assert.Equal(50, result.Shape[0]);
        Assert.True(result.Shape[1] > 0);
    }

    [Fact]
    public void RollingVolatilityTransformer_Returns_AreCalculated()
    {
        // NOTE: The current implementation uses EnabledVolatilityMeasures to determine
        // which features to compute. CalculateReturns/CalculateMomentum flags are stored
        // but not used in BuildOperationNames(). To get return features, use SimpleReturns/LogReturns.
        var options = new TimeSeriesFeatureOptions
        {
            EnableVolatility = true,
            EnabledVolatilityMeasures = VolatilityMeasures.SimpleReturns | VolatilityMeasures.LogReturns,
            WindowSizes = [5]
        };
        var transformer = new RollingVolatilityTransformer<double>(options);
        var data = CreateStockPriceSeries(50);

        transformer.Fit(data);
        var featureNames = transformer.FeatureNames;

        // Should have return-related features from the enabled volatility measures
        Assert.True(featureNames.Length > 0,
            $"Expected some features. Got: {string.Join(", ", featureNames)}");
        Assert.True(featureNames.Any(n => n.Contains("return")),
            $"Expected return features. Got: {string.Join(", ", featureNames)}");
    }

    [Fact]
    public void RollingVolatilityTransformer_ParkinsonVolatility_WithOHLC()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableVolatility = true,
            EnabledVolatilityMeasures = VolatilityMeasures.ParkinsonVolatility,
            CalculateReturns = false,
            CalculateMomentum = false,
            WindowSizes = [5]
        };
        var transformer = new RollingVolatilityTransformer<double>(options);

        // OHLC data (4 features: open, high, low, close)
        var data = CreateStockPriceSeries(30);
        var result = transformer.FitTransform(data);

        Assert.NotNull(result);
        Assert.True(result.Shape[1] > 0);
    }

    #endregion

    #region LagLeadTransformer Tests

    [Fact]
    public void LagLeadTransformer_Construction_Succeeds()
    {
        var options = new TimeSeriesFeatureOptions
        {
            LagSteps = [1, 2, 3]
        };
        var transformer = new LagLeadTransformer<double>(options);

        Assert.NotNull(transformer);
    }

    [Fact]
    public void LagLeadTransformer_LagFeatures_ProducesCorrectOutput()
    {
        var options = new TimeSeriesFeatureOptions
        {
            WindowSizes = [2],  // Set small window to allow short data
            LagSteps = [1, 2],
            LeadSteps = []
        };
        var transformer = new LagLeadTransformer<double>(options);

        // Simple sequence: 10, 20, 30, 40, 50
        var data = new Tensor<double>(new[] { 5, 1 });
        for (int i = 0; i < 5; i++) data[i, 0] = (i + 1) * 10;

        var result = transformer.FitTransform(data);

        // Output: 2 lags * 1 feature = 2 output features
        Assert.Equal(2, result.Shape[1]);

        // Lag-1 at t=2 should be value at t=1 (20)
        Assert.Equal(20.0, result[2, 0], 5);

        // Lag-2 at t=3 should be value at t=1 (20)
        Assert.Equal(20.0, result[3, 1], 5);

        // Lag-1 at t=0 should be NaN (no history)
        Assert.True(double.IsNaN(result[0, 0]));
    }

    [Fact]
    public void LagLeadTransformer_LeadFeatures_ProducesCorrectOutput()
    {
        var options = new TimeSeriesFeatureOptions
        {
            WindowSizes = [2],  // Set small window to allow short data
            LagSteps = [],
            LeadSteps = [1]
        };
        var transformer = new LagLeadTransformer<double>(options);

        // Simple sequence: 10, 20, 30, 40, 50
        var data = new Tensor<double>(new[] { 5, 1 });
        for (int i = 0; i < 5; i++) data[i, 0] = (i + 1) * 10;

        var result = transformer.FitTransform(data);

        // Lead-1 at t=0 should be value at t=1 (20)
        Assert.Equal(20.0, result[0, 0], 5);

        // Lead-1 at t=4 should be NaN (no future)
        Assert.True(double.IsNaN(result[4, 0]));
    }

    [Fact]
    public void LagLeadTransformer_GeneratesCorrectFeatureNames()
    {
        var options = new TimeSeriesFeatureOptions
        {
            WindowSizes = [5],  // Set small window to allow smaller test data
            LagSteps = [1, 2],
            LeadSteps = [1],
            InputFeatureNames = ["price"]
        };
        var transformer = new LagLeadTransformer<double>(options);
        var data = CreateTestTimeSeries(20, 1);

        transformer.Fit(data);
        var featureNames = transformer.FeatureNames;

        Assert.Contains(featureNames, n => n.Contains("lag") && n.Contains("1"));
        Assert.Contains(featureNames, n => n.Contains("lag") && n.Contains("2"));
        Assert.Contains(featureNames, n => n.Contains("lead") && n.Contains("1"));
    }

    #endregion

    #region RollingCorrelationTransformer Tests

    [Fact]
    public void RollingCorrelationTransformer_Construction_Succeeds()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableCorrelation = true,
            WindowSizes = [10]
        };
        var transformer = new RollingCorrelationTransformer<double>(options);

        Assert.NotNull(transformer);
    }

    [Fact]
    public void RollingCorrelationTransformer_FitTransform_ProducesOutput()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableCorrelation = true,
            FullCorrelationMatrix = false,
            WindowSizes = [10]
        };
        var transformer = new RollingCorrelationTransformer<double>(options);

        // Need at least 2 features for correlation
        var data = CreateTestTimeSeries(30, 3);
        var result = transformer.FitTransform(data);

        Assert.NotNull(result);
        Assert.Equal(30, result.Shape[0]);
        // Upper triangle for 3 features: 3 pairs (0-1, 0-2, 1-2)
        Assert.Equal(3, result.Shape[1]);
    }

    [Fact]
    public void RollingCorrelationTransformer_FullMatrix_ProducesMoreFeatures()
    {
        var optionsTriangle = new TimeSeriesFeatureOptions
        {
            EnableCorrelation = true,
            FullCorrelationMatrix = false,
            WindowSizes = [10]
        };
        var optionsFull = new TimeSeriesFeatureOptions
        {
            EnableCorrelation = true,
            FullCorrelationMatrix = true,
            WindowSizes = [10]
        };

        var data = CreateTestTimeSeries(30, 3);

        var transformer1 = new RollingCorrelationTransformer<double>(optionsTriangle);
        var transformer2 = new RollingCorrelationTransformer<double>(optionsFull);

        var result1 = transformer1.FitTransform(data);
        var result2 = transformer2.FitTransform(data);

        // Full matrix for 3 features: 9 values, upper triangle: 3 values
        Assert.True(result2.Shape[1] > result1.Shape[1]);
    }

    [Fact]
    public void RollingCorrelationTransformer_PerfectCorrelation_ReturnsOne()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableCorrelation = true,
            WindowSizes = [5]
        };
        var transformer = new RollingCorrelationTransformer<double>(options);

        // Create two perfectly correlated features (f1 = 2*f0)
        var data = new Tensor<double>(new[] { 20, 2 });
        for (int t = 0; t < 20; t++)
        {
            data[t, 0] = t + 1;
            data[t, 1] = 2 * (t + 1);
        }

        var result = transformer.FitTransform(data);

        // After sufficient history, correlation should be ~1.0
        Assert.True(result[10, 0] > 0.99, $"Expected correlation ~1.0, got {result[10, 0]}");
    }

    #endregion

    #region AiModelBuilder Integration Tests

    [Fact]
    public void AiModelBuilder_ConfigureTimeSeriesFeatures_Succeeds()
    {
        var builder = new AiModelBuilder<double, Tensor<double>, double[]>()
            .ConfigureTimeSeriesFeatures(new TimeSeriesFeatureOptions
            {
                WindowSizes = [7, 14],
                EnabledStatistics = RollingStatistics.Mean | RollingStatistics.StandardDeviation
            });

        Assert.NotNull(builder.TimeSeriesOptions);
        Assert.NotEmpty(builder.TimeSeriesExtractors);
    }

    [Fact]
    public void AiModelBuilder_ConfigureTimeSeriesFeaturesForFinance_Succeeds()
    {
        var builder = new AiModelBuilder<double, Tensor<double>, double[]>()
            .ConfigureTimeSeriesFeaturesForFinance();

        Assert.NotNull(builder.TimeSeriesOptions);
        Assert.True(builder.TimeSeriesOptions.EnableVolatility);
    }

    [Fact]
    public void AiModelBuilder_ConfigureTimeSeriesFeaturesMinimal_Succeeds()
    {
        var builder = new AiModelBuilder<double, Tensor<double>, double[]>()
            .ConfigureTimeSeriesFeaturesMinimal();

        Assert.NotNull(builder.TimeSeriesOptions);
        Assert.Single(builder.TimeSeriesOptions.WindowSizes);
    }

    [Fact]
    public void AiModelBuilder_FluentConfiguration_Succeeds()
    {
        var builder = new AiModelBuilder<double, Tensor<double>, double[]>()
            .ConfigureTimeSeriesFeatures(opts =>
            {
                opts.WindowSizes = [5, 10, 20];
                opts.EnableVolatility = true;
                opts.LagSteps = [1, 2, 3];
            });

        Assert.NotNull(builder.TimeSeriesOptions);
        Assert.Equal(new[] { 5, 10, 20 }, builder.TimeSeriesOptions.WindowSizes);
        Assert.True(builder.TimeSeriesOptions.EnableVolatility);
    }

    [Fact]
    public void AiModelBuilder_ExtractTimeSeriesFeatures_ProducesOutput()
    {
        var builder = new AiModelBuilder<double, Tensor<double>, double[]>()
            .ConfigureTimeSeriesFeatures(new TimeSeriesFeatureOptions
            {
                WindowSizes = [5],
                EnabledStatistics = RollingStatistics.Mean,
                EnableVolatility = false,
                EnableCorrelation = false,
                LagSteps = []
            });

        var data = CreateTestTimeSeries(30, 2);
        var features = builder.ExtractTimeSeriesFeatures(data);

        Assert.NotNull(features);
        Assert.Equal(30, features.Shape[0]);
        Assert.True(features.Shape[1] > 0);
    }

    [Fact]
    public void AiModelBuilder_AddCustomExtractor_Succeeds()
    {
        var options = new TimeSeriesFeatureOptions
        {
            WindowSizes = [5],
            EnabledStatistics = RollingStatistics.Mean
        };
        var customExtractor = new RollingStatsTransformer<double>(options);

        var builder = new AiModelBuilder<double, Tensor<double>, double[]>()
            .ConfigureTimeSeriesFeaturesMinimal();

        int originalCount = builder.TimeSeriesExtractors.Count;
        builder.AddTimeSeriesExtractor(customExtractor);

        // Adding a custom extractor should increase count by 1
        Assert.Equal(originalCount + 1, builder.TimeSeriesExtractors.Count);
    }

    [Fact]
    public void AiModelBuilder_InvalidOptions_ThrowsException()
    {
        var invalidOptions = new TimeSeriesFeatureOptions
        {
            WindowSizes = [1]  // Invalid: must be at least 2
        };

        Assert.Throws<ArgumentException>(() =>
            new AiModelBuilder<double, Tensor<double>, double[]>()
                .ConfigureTimeSeriesFeatures(invalidOptions));
    }

    #endregion

    #region Validation Tests

    [Fact]
    public void RollingStatsTransformer_ValidateInput_ReturnsTrue_ForValidData()
    {
        var options = new TimeSeriesFeatureOptions { WindowSizes = [5] };
        var transformer = new RollingStatsTransformer<double>(options);
        var data = CreateTestTimeSeries(30, 2);

        transformer.Fit(data);
        Assert.True(transformer.ValidateInput(data));
    }

    [Fact]
    public void RollingStatsTransformer_GetValidationErrors_ReturnsErrors_ForShortData()
    {
        var options = new TimeSeriesFeatureOptions { WindowSizes = [10] };
        var transformer = new RollingStatsTransformer<double>(options);

        // Data shorter than window size
        var shortData = CreateTestTimeSeries(5, 2);

        // Get validation errors without fitting (fitting would throw for short data)
        var errors = transformer.GetValidationErrors(shortData);

        // Should have warning about data length being less than window size
        Assert.NotEmpty(errors);
    }

    #endregion

    #region Parallel Processing Tests

    [Fact]
    public void RollingStatsTransformer_ParallelProcessing_ProducesSameResults()
    {
        var optionsSerial = new TimeSeriesFeatureOptions
        {
            WindowSizes = [7],
            EnabledStatistics = RollingStatistics.Mean | RollingStatistics.StandardDeviation,
            UseParallelProcessing = false
        };
        var optionsParallel = new TimeSeriesFeatureOptions
        {
            WindowSizes = [7],
            EnabledStatistics = RollingStatistics.Mean | RollingStatistics.StandardDeviation,
            UseParallelProcessing = true,
            ParallelThreshold = 10  // Force parallel for small dataset
        };

        var data = CreateTestTimeSeries(100, 3);

        var transformer1 = new RollingStatsTransformer<double>(optionsSerial);
        var transformer2 = new RollingStatsTransformer<double>(optionsParallel);

        var result1 = transformer1.FitTransform(data);
        var result2 = transformer2.FitTransform(data);

        Assert.Equal(result1.Shape[0], result2.Shape[0]);
        Assert.Equal(result1.Shape[1], result2.Shape[1]);

        // Results should be identical (or very close due to floating point)
        for (int t = 0; t < result1.Shape[0]; t++)
        {
            for (int f = 0; f < result1.Shape[1]; f++)
            {
                if (!double.IsNaN(result1[t, f]) && !double.IsNaN(result2[t, f]))
                {
                    Assert.Equal(result1[t, f], result2[t, f], 10);
                }
            }
        }
    }

    #endregion

    #region Edge Case Tests

    [Fact]
    public void RollingStatsTransformer_SingleFeature_Works()
    {
        var options = new TimeSeriesFeatureOptions
        {
            WindowSizes = [5],
            EnabledStatistics = RollingStatistics.Mean
        };
        var transformer = new RollingStatsTransformer<double>(options);
        var data = CreateTestTimeSeries(30, 1);

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);
        // Should produce at least 1 feature per window per input feature
        Assert.True(result.Shape[1] >= 1, $"Expected at least 1 output feature, got {result.Shape[1]}");
    }

    [Fact]
    public void RollingStatsTransformer_ManyFeatures_Works()
    {
        var options = new TimeSeriesFeatureOptions
        {
            WindowSizes = [5],
            EnabledStatistics = RollingStatistics.Mean
        };
        var transformer = new RollingStatsTransformer<double>(options);
        var data = CreateTestTimeSeries(30, 10);

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);
        // Should produce features proportional to input features
        Assert.True(result.Shape[1] >= 10, $"Expected at least 10 output features for 10 inputs, got {result.Shape[1]}");
    }

    [Fact]
    public void LagLeadTransformer_EmptyLagAndLead_ProducesEmptyOutput()
    {
        var options = new TimeSeriesFeatureOptions
        {
            LagSteps = [],
            LeadSteps = []
        };
        var transformer = new LagLeadTransformer<double>(options);
        var data = CreateTestTimeSeries(30, 2);

        var result = transformer.FitTransform(data);

        Assert.Equal(0, result.Shape[1]);
    }

    #endregion

    #region EdgeHandling Tests

    [Fact]
    public void RollingStatsTransformer_EdgeHandlingNaN_ProducesNaNForIncompleteWindows()
    {
        var options = new TimeSeriesFeatureOptions
        {
            WindowSizes = [5],
            EnabledStatistics = RollingStatistics.Mean,
            EdgeHandling = EdgeHandling.NaN
        };
        var transformer = new RollingStatsTransformer<double>(options);
        var data = CreateTestTimeSeries(20, 1);

        var result = transformer.FitTransform(data);

        // First 4 rows (t=0,1,2,3) should have NaN (window size 5, need t>=4)
        for (int t = 0; t < 4; t++)
        {
            Assert.True(double.IsNaN(result[t, 0]),
                $"Expected NaN at t={t} for incomplete window, got {result[t, 0]}");
        }

        // t=4 onwards should have valid values
        Assert.False(double.IsNaN(result[4, 0]),
            $"Expected valid value at t=4, got {result[4, 0]}");
    }

    [Fact]
    public void RollingStatsTransformer_EdgeHandlingPartial_ComputesWithAvailableData()
    {
        var options = new TimeSeriesFeatureOptions
        {
            WindowSizes = [5],
            EnabledStatistics = RollingStatistics.Mean,
            EdgeHandling = EdgeHandling.Partial
        };
        var transformer = new RollingStatsTransformer<double>(options);

        // Simple sequence: 10, 20, 30, 40, 50, ...
        var data = new Tensor<double>(new[] { 20, 1 });
        for (int i = 0; i < 20; i++) data[i, 0] = (i + 1) * 10;

        var result = transformer.FitTransform(data);

        // t=0: partial window [10] -> mean = 10
        Assert.False(double.IsNaN(result[0, 0]), "Partial window at t=0 should compute");
        Assert.Equal(10.0, result[0, 0], 5);

        // t=1: partial window [10, 20] -> mean = 15
        Assert.Equal(15.0, result[1, 0], 5);

        // t=2: partial window [10, 20, 30] -> mean = 20
        Assert.Equal(20.0, result[2, 0], 5);

        // t=4: full window [10, 20, 30, 40, 50] -> mean = 30
        Assert.Equal(30.0, result[4, 0], 5);
    }

    [Fact]
    public void RollingStatsTransformer_EdgeHandlingTruncate_ShorterOutput()
    {
        var options = new TimeSeriesFeatureOptions
        {
            WindowSizes = [5],
            EnabledStatistics = RollingStatistics.Mean,
            EdgeHandling = EdgeHandling.Truncate
        };
        var transformer = new RollingStatsTransformer<double>(options);
        var data = CreateTestTimeSeries(20, 1);

        var result = transformer.FitTransform(data);

        // With window size 5 and 20 input rows, output should have 20-5+1=16 rows
        Assert.Equal(16, result.Shape[0]);

        // All values should be valid (no NaN)
        for (int t = 0; t < result.Shape[0]; t++)
        {
            Assert.False(double.IsNaN(result[t, 0]),
                $"Truncated output should have no NaN at t={t}");
        }
    }

    [Fact]
    public void RollingStatsTransformer_EdgeHandlingForwardFill_FillsEdgeRegion()
    {
        var options = new TimeSeriesFeatureOptions
        {
            WindowSizes = [5],
            EnabledStatistics = RollingStatistics.Mean,
            EdgeHandling = EdgeHandling.ForwardFill
        };
        var transformer = new RollingStatsTransformer<double>(options);

        // Simple sequence: 10, 20, 30, 40, 50, 60, ...
        var data = new Tensor<double>(new[] { 20, 1 });
        for (int i = 0; i < 20; i++) data[i, 0] = (i + 1) * 10;

        var result = transformer.FitTransform(data);

        // Output should be same length as input
        Assert.Equal(20, result.Shape[0]);

        // First valid value is at t=4 (full window [10,20,30,40,50] -> mean=30)
        double firstValidValue = result[4, 0];

        // t=0,1,2,3 should be forward-filled with first valid value
        for (int t = 0; t < 4; t++)
        {
            Assert.Equal(firstValidValue, result[t, 0], 5);
        }
    }

    [Fact]
    public void RollingVolatilityTransformer_EdgeHandlingPartial_ComputesWithAvailableData()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableVolatility = true,
            EnabledVolatilityMeasures = VolatilityMeasures.RealizedVolatility,
            CalculateReturns = false,
            CalculateMomentum = false,
            WindowSizes = [5],
            EdgeHandling = EdgeHandling.Partial
        };
        var transformer = new RollingVolatilityTransformer<double>(options);
        var data = CreateStockPriceSeries(20);

        var result = transformer.FitTransform(data);

        // Should have output for all time steps
        Assert.Equal(20, result.Shape[0]);

        // Later values should be valid (at least some non-NaN)
        bool hasValidLater = false;
        for (int t = 5; t < 20; t++)
        {
            if (!double.IsNaN(result[t, 0]))
            {
                hasValidLater = true;
                break;
            }
        }
        Assert.True(hasValidLater, "Should have valid volatility values after enough data");
    }

    [Fact]
    public void RollingCorrelationTransformer_EdgeHandlingTruncate_ShorterOutput()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableCorrelation = true,
            WindowSizes = [10],
            EdgeHandling = EdgeHandling.Truncate
        };
        var transformer = new RollingCorrelationTransformer<double>(options);
        var data = CreateTestTimeSeries(30, 3);

        var result = transformer.FitTransform(data);

        // With window size 10 and 30 input rows, output should have 30-10+1=21 rows
        Assert.Equal(21, result.Shape[0]);
    }

    [Fact]
    public void RollingStatsTransformer_EdgeHandlingTruncate_EmptyOutput_ForShortData()
    {
        var options = new TimeSeriesFeatureOptions
        {
            WindowSizes = [10],
            EnabledStatistics = RollingStatistics.Mean,
            EdgeHandling = EdgeHandling.Truncate
        };
        var transformer = new RollingStatsTransformer<double>(options);

        // Data shorter than window -> empty output
        var data = CreateTestTimeSeries(5, 1);

        // This should throw during Fit because data is too short
        Assert.Throws<ArgumentException>(() => transformer.FitTransform(data));
    }

    [Fact]
    public void AllTransformers_EdgeHandlingConsistency_ParallelEqualsSequential()
    {
        // Test that parallel and sequential produce identical results for all edge handling modes
        foreach (var edgeHandling in new[] { EdgeHandling.NaN, EdgeHandling.Partial, EdgeHandling.ForwardFill })
        {
            var optionsSerial = new TimeSeriesFeatureOptions
            {
                WindowSizes = [5],
                EnabledStatistics = RollingStatistics.Mean | RollingStatistics.StandardDeviation,
                EdgeHandling = edgeHandling,
                UseParallelProcessing = false
            };
            var optionsParallel = new TimeSeriesFeatureOptions
            {
                WindowSizes = [5],
                EnabledStatistics = RollingStatistics.Mean | RollingStatistics.StandardDeviation,
                EdgeHandling = edgeHandling,
                UseParallelProcessing = true,
                ParallelThreshold = 10  // Force parallel
            };

            var data = CreateTestTimeSeries(50, 2);

            var transformer1 = new RollingStatsTransformer<double>(optionsSerial);
            var transformer2 = new RollingStatsTransformer<double>(optionsParallel);

            var result1 = transformer1.FitTransform(data);
            var result2 = transformer2.FitTransform(data);

            Assert.Equal(result1.Shape[0], result2.Shape[0]);
            Assert.Equal(result1.Shape[1], result2.Shape[1]);

            for (int t = 0; t < result1.Shape[0]; t++)
            {
                for (int f = 0; f < result1.Shape[1]; f++)
                {
                    bool nan1 = double.IsNaN(result1[t, f]);
                    bool nan2 = double.IsNaN(result2[t, f]);
                    Assert.Equal(nan1, nan2);
                    if (!nan1 && !nan2)
                    {
                        Assert.Equal(result1[t, f], result2[t, f], 10);
                    }
                }
            }
        }
    }

    [Fact]
    public void RollingStatsTransformer_MultipleWindowSizes_EdgeHandlingPartial()
    {
        var options = new TimeSeriesFeatureOptions
        {
            WindowSizes = [3, 5, 7],
            EnabledStatistics = RollingStatistics.Mean,
            EdgeHandling = EdgeHandling.Partial,
            CustomPercentiles = []  // Disable custom percentiles to simplify
        };
        var transformer = new RollingStatsTransformer<double>(options);

        var data = new Tensor<double>(new[] { 20, 1 });
        for (int i = 0; i < 20; i++) data[i, 0] = (i + 1) * 10;

        var result = transformer.FitTransform(data);

        // Should have 3 features (1 input * 1 stat * 3 windows)
        Assert.Equal(3, result.Shape[1]);

        // At t=0, all partial windows should compute
        // Window 3: [10] -> mean = 10
        // Window 5: [10] -> mean = 10
        // Window 7: [10] -> mean = 10
        Assert.Equal(10.0, result[0, 0], 5);
        Assert.Equal(10.0, result[0, 1], 5);
        Assert.Equal(10.0, result[0, 2], 5);

        // At t=2, partial windows:
        // Window 3: [10, 20, 30] -> mean = 20 (full)
        // Window 5: [10, 20, 30] -> mean = 20 (partial)
        // Window 7: [10, 20, 30] -> mean = 20 (partial)
        Assert.Equal(20.0, result[2, 0], 5);
        Assert.Equal(20.0, result[2, 1], 5);
        Assert.Equal(20.0, result[2, 2], 5);
    }

    #endregion

    #region Auto-Detection Tests

    [Fact]
    public void TimeSeriesTransformerBase_AutocorrelationDetection_DetectsPeriods()
    {
        var options = new TimeSeriesFeatureOptions
        {
            AutoDetectWindowSizes = true,
            AutoDetectionMethod = WindowAutoDetectionMethod.Autocorrelation,
            MinWindowSize = 2,
            MaxWindowSize = 100,
            EnabledStatistics = RollingStatistics.Mean
        };
        var transformer = new RollingStatsTransformer<double>(options);

        // Create data with clear 10-period cycle: sin(2*pi*t/10)
        var data = new Tensor<double>(new[] { 200, 1 });
        for (int t = 0; t < 200; t++)
        {
            data[t, 0] = Math.Sin(2 * Math.PI * t / 10.0) + t * 0.01;  // Periodic + trend
        }

        transformer.Fit(data);
        var windowSizes = transformer.WindowSizes;

        Assert.NotEmpty(windowSizes);
        Assert.True(windowSizes.Length <= options.MaxAutoDetectedWindows);
        // The detected windows should be related to the period of 10
        // Due to autocorrelation, we might get 10 or harmonics/subharmonics
    }

    [Fact]
    public void TimeSeriesTransformerBase_SpectralDetection_FindsDominantFrequencies()
    {
        var options = new TimeSeriesFeatureOptions
        {
            AutoDetectWindowSizes = true,
            AutoDetectionMethod = WindowAutoDetectionMethod.SpectralAnalysis,
            MinWindowSize = 2,
            MaxWindowSize = 100,
            MaxAutoDetectedWindows = 5,
            EnabledStatistics = RollingStatistics.Mean
        };
        var transformer = new RollingStatsTransformer<double>(options);

        // Create data with clear 20-period cycle
        var data = new Tensor<double>(new[] { 200, 1 });
        for (int t = 0; t < 200; t++)
        {
            data[t, 0] = Math.Sin(2 * Math.PI * t / 20.0) + 0.5 * Math.Sin(2 * Math.PI * t / 40.0);
        }

        transformer.Fit(data);
        var windowSizes = transformer.WindowSizes;

        Assert.NotEmpty(windowSizes);
        // FFT should detect periods around 20 and/or 40
    }

    [Fact]
    public void TimeSeriesTransformerBase_GridSearchDetection_FindsOptimalWindows()
    {
        var options = new TimeSeriesFeatureOptions
        {
            AutoDetectWindowSizes = true,
            AutoDetectionMethod = WindowAutoDetectionMethod.GridSearch,
            MinWindowSize = 2,
            MaxWindowSize = 50,
            MaxAutoDetectedWindows = 3,
            EnabledStatistics = RollingStatistics.Mean
        };
        var transformer = new RollingStatsTransformer<double>(options);

        var data = CreateTestTimeSeries(100, 2);

        transformer.Fit(data);
        var windowSizes = transformer.WindowSizes;

        Assert.NotEmpty(windowSizes);
        Assert.True(windowSizes.All(w => w >= options.MinWindowSize));
        Assert.True(windowSizes.All(w => w <= options.MaxWindowSize));
    }

    [Fact]
    public void TimeSeriesTransformerBase_HeuristicDetection_ReturnsValidWindows()
    {
        var options = new TimeSeriesFeatureOptions
        {
            AutoDetectWindowSizes = true,
            AutoDetectionMethod = WindowAutoDetectionMethod.Heuristic,
            MinWindowSize = 5,
            MaxWindowSize = 30,
            MaxAutoDetectedWindows = 3,
            EnabledStatistics = RollingStatistics.Mean
        };
        var transformer = new RollingStatsTransformer<double>(options);
        var data = CreateTestTimeSeries(100, 1);

        transformer.Fit(data);
        var windowSizes = transformer.WindowSizes;

        Assert.NotEmpty(windowSizes);
        Assert.True(windowSizes.All(w => w >= options.MinWindowSize));
        Assert.True(windowSizes.All(w => w <= options.MaxWindowSize));
        Assert.True(windowSizes.Length <= options.MaxAutoDetectedWindows);
    }

    [Fact]
    public void TimeSeriesTransformerBase_SpectralDetection_HandlesSingletons()
    {
        var options = new TimeSeriesFeatureOptions
        {
            AutoDetectWindowSizes = true,
            AutoDetectionMethod = WindowAutoDetectionMethod.SpectralAnalysis,
            MinWindowSize = 2,
            MaxWindowSize = 20,
            EnabledStatistics = RollingStatistics.Mean
        };
        var transformer = new RollingStatsTransformer<double>(options);

        // Small dataset - should fall back to heuristics
        var data = new Tensor<double>(new[] { 5, 1 });
        for (int t = 0; t < 5; t++) data[t, 0] = t;

        // This should throw due to data being shorter than max window
        Assert.Throws<ArgumentException>(() => transformer.Fit(data));
    }

    [Fact]
    public void TimeSeriesTransformerBase_SpectralDetection_MultipleFeatures()
    {
        var options = new TimeSeriesFeatureOptions
        {
            AutoDetectWindowSizes = true,
            AutoDetectionMethod = WindowAutoDetectionMethod.SpectralAnalysis,
            MinWindowSize = 2,
            MaxWindowSize = 100,
            MaxAutoDetectedWindows = 5,
            EnabledStatistics = RollingStatistics.Mean
        };
        var transformer = new RollingStatsTransformer<double>(options);

        // Create multi-feature data with different periodicities
        var data = new Tensor<double>(new[] { 200, 3 });
        for (int t = 0; t < 200; t++)
        {
            data[t, 0] = Math.Sin(2 * Math.PI * t / 15.0);  // Period 15
            data[t, 1] = Math.Sin(2 * Math.PI * t / 30.0);  // Period 30
            data[t, 2] = Math.Sin(2 * Math.PI * t / 25.0);  // Period 25
        }

        transformer.Fit(data);
        var windowSizes = transformer.WindowSizes;

        Assert.NotEmpty(windowSizes);
        // Should detect some combination of periods around 15, 25, 30
    }

    #endregion

    #region OHLC Volatility Tests

    [Fact]
    public void RollingVolatilityTransformer_ParkinsonVolatility_WithOhlcConfig_UsesHighLow()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableVolatility = true,
            EnabledVolatilityMeasures = VolatilityMeasures.ParkinsonVolatility,
            CalculateReturns = false,
            CalculateMomentum = false,
            WindowSizes = [5],
            // Configure OHLC columns explicitly
            OhlcColumns = new OhlcColumnConfig
            {
                OpenIndex = 0,
                HighIndex = 1,
                LowIndex = 2,
                CloseIndex = 3
            }
        };
        var transformer = new RollingVolatilityTransformer<double>(options);

        // Create OHLC data with known high-low spread
        var data = CreateStockPriceSeries(30);
        var result = transformer.FitTransform(data);

        Assert.NotNull(result);
        Assert.Equal(30, result.Shape[0]);
        Assert.True(result.Shape[1] >= 1, "Should have at least one volatility feature");

        // Verify values are valid (positive or NaN for initial rows)
        for (int t = 5; t < 30; t++)
        {
            var value = result[t, 0];
            Assert.True(!double.IsNaN(value) && value >= 0,
                $"Parkinson volatility at t={t} should be non-negative, got {value}");
        }
    }

    [Fact]
    public void RollingVolatilityTransformer_GarmanKlassVolatility_WithOhlcConfig_UsesAllOhlc()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableVolatility = true,
            EnabledVolatilityMeasures = VolatilityMeasures.GarmanKlassVolatility,
            CalculateReturns = false,
            CalculateMomentum = false,
            WindowSizes = [5],
            // Configure OHLC columns explicitly
            OhlcColumns = OhlcColumnConfig.CreateStandard()  // Uses standard indices 0,1,2,3
        };
        var transformer = new RollingVolatilityTransformer<double>(options);

        // Create OHLC data
        var data = CreateStockPriceSeries(30);
        var result = transformer.FitTransform(data);

        Assert.NotNull(result);
        Assert.Equal(30, result.Shape[0]);

        // Verify Garman-Klass produces valid volatility values
        for (int t = 5; t < 30; t++)
        {
            var value = result[t, 0];
            Assert.True(!double.IsNaN(value) && value >= 0,
                $"Garman-Klass volatility at t={t} should be non-negative, got {value}");
        }
    }

    [Fact]
    public void RollingVolatilityTransformer_HlcConfig_ParkinsonUsesHighLow()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableVolatility = true,
            EnabledVolatilityMeasures = VolatilityMeasures.ParkinsonVolatility,
            CalculateReturns = false,
            CalculateMomentum = false,
            WindowSizes = [5],
            // HLC configuration (no open)
            OhlcColumns = OhlcColumnConfig.CreateHlc(highIndex: 0, lowIndex: 1, closeIndex: 2)
        };
        var transformer = new RollingVolatilityTransformer<double>(options);

        // Create HLC data (3 features)
        var data = new Tensor<double>(new[] { 30, 3 });
        double price = 100.0;
        for (int t = 0; t < 30; t++)
        {
            double change = Math.Sin(t * 0.2) * 3;
            double high = price + Math.Abs(change) + 1.0;
            double low = price - Math.Abs(change) - 1.0;
            double close = price + change;

            data[t, 0] = high;
            data[t, 1] = low;
            data[t, 2] = close;

            price = close;
        }

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);
        Assert.Equal(30, result.Shape[0]);

        // Verify valid values
        for (int t = 5; t < 30; t++)
        {
            Assert.True(!double.IsNaN(result[t, 0]) && result[t, 0] >= 0,
                $"Parkinson volatility with HLC at t={t} should be valid");
        }
    }

    [Fact]
    public void RollingVolatilityTransformer_WithoutOhlcConfig_UsesApproximation()
    {
        // When OHLC columns are not configured, transformer should fall back to approximation
        var options = new TimeSeriesFeatureOptions
        {
            EnableVolatility = true,
            EnabledVolatilityMeasures = VolatilityMeasures.ParkinsonVolatility | VolatilityMeasures.GarmanKlassVolatility,
            CalculateReturns = false,
            CalculateMomentum = false,
            WindowSizes = [5],
            OhlcColumns = null  // No OHLC config - should use approximation
        };
        var transformer = new RollingVolatilityTransformer<double>(options);

        // Simple price data (single feature)
        var data = new Tensor<double>(new[] { 30, 1 });
        for (int t = 0; t < 30; t++)
        {
            data[t, 0] = 100.0 + t + Math.Sin(t * 0.5) * 5;  // Price with some variation
        }

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);
        Assert.Equal(30, result.Shape[0]);

        // Should produce valid volatility using approximation
        for (int t = 5; t < 30; t++)
        {
            for (int f = 0; f < result.Shape[1]; f++)
            {
                var value = result[t, f];
                Assert.True(!double.IsNaN(value) && value >= 0,
                    $"Approximated volatility at t={t}, f={f} should be non-negative, got {value}");
            }
        }
    }

    [Fact]
    public void RollingVolatilityTransformer_OhlcVsApproximation_ProducesDifferentValues()
    {
        // Test that OHLC-aware calculation differs from approximation
        var ohlcOptions = new TimeSeriesFeatureOptions
        {
            EnableVolatility = true,
            EnabledVolatilityMeasures = VolatilityMeasures.ParkinsonVolatility,
            CalculateReturns = false,
            CalculateMomentum = false,
            WindowSizes = [5],
            OhlcColumns = OhlcColumnConfig.CreateStandard()
        };

        var noOhlcOptions = new TimeSeriesFeatureOptions
        {
            EnableVolatility = true,
            EnabledVolatilityMeasures = VolatilityMeasures.ParkinsonVolatility,
            CalculateReturns = false,
            CalculateMomentum = false,
            WindowSizes = [5],
            OhlcColumns = null  // Will use approximation
        };

        var ohlcTransformer = new RollingVolatilityTransformer<double>(ohlcOptions);
        var approxTransformer = new RollingVolatilityTransformer<double>(noOhlcOptions);

        // Create data with significant high-low spread (OHLC-aware should differ from approximation)
        var data = CreateStockPriceSeries(30);

        var ohlcResult = ohlcTransformer.FitTransform(data);

        // For the approximation, we'll use just the close price as input
        var closeOnly = new Tensor<double>(new[] { 30, 1 });
        for (int t = 0; t < 30; t++)
        {
            closeOnly[t, 0] = data[t, 3];  // Close price
        }

        var approxResult = approxTransformer.FitTransform(closeOnly);

        // Both should produce valid values
        Assert.Equal(30, ohlcResult.Shape[0]);
        Assert.Equal(30, approxResult.Shape[0]);

        // The values should be different (OHLC uses actual H-L range, approximation estimates it)
        bool foundDifference = false;
        for (int t = 10; t < 30; t++)
        {
            if (Math.Abs(ohlcResult[t, 0] - approxResult[t, 0]) > 1e-10)
            {
                foundDifference = true;
                break;
            }
        }

        Assert.True(foundDifference,
            "OHLC-based and approximation-based Parkinson volatility should produce different values");
    }

    [Fact]
    public void RollingVolatilityTransformer_AllVolatilityMeasures_WithOhlc_ProducesValidOutput()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableVolatility = true,
            EnabledVolatilityMeasures = VolatilityMeasures.RealizedVolatility |
                                        VolatilityMeasures.ParkinsonVolatility |
                                        VolatilityMeasures.GarmanKlassVolatility,
            CalculateReturns = false,
            CalculateMomentum = false,
            WindowSizes = [5],
            OhlcColumns = OhlcColumnConfig.CreateStandard()
        };
        var transformer = new RollingVolatilityTransformer<double>(options);

        var data = CreateStockPriceSeries(50);
        var result = transformer.FitTransform(data);

        Assert.NotNull(result);
        Assert.Equal(50, result.Shape[0]);

        // Should have at least 3 volatility features (one per feature per measure per window)
        // With 4 input features and 3 measures and 1 window: 4 * 3 * 1 = 12 volatility columns
        Assert.True(result.Shape[1] >= 3, $"Expected at least 3 volatility features, got {result.Shape[1]}");

        // Verify reasonable values in later time steps
        for (int t = 10; t < 50; t++)
        {
            for (int f = 0; f < result.Shape[1]; f++)
            {
                var value = result[t, f];
                Assert.True(!double.IsNaN(value) && value >= 0,
                    $"Volatility at t={t}, f={f} should be non-negative, got {value}");
                // Volatility should be less than 10 (reasonable upper bound for normalized volatility)
                Assert.True(value < 10, $"Volatility at t={t}, f={f} seems unreasonably high: {value}");
            }
        }
    }

    [Fact]
    public void RollingVolatilityTransformer_Ohlc_ParallelEqualsSequential()
    {
        var baseOptions = new TimeSeriesFeatureOptions
        {
            EnableVolatility = true,
            EnabledVolatilityMeasures = VolatilityMeasures.ParkinsonVolatility | VolatilityMeasures.GarmanKlassVolatility,
            CalculateReturns = false,
            CalculateMomentum = false,
            WindowSizes = [5, 10],
            OhlcColumns = OhlcColumnConfig.CreateStandard()
        };

        var serialOptions = new TimeSeriesFeatureOptions
        {
            EnableVolatility = baseOptions.EnableVolatility,
            EnabledVolatilityMeasures = baseOptions.EnabledVolatilityMeasures,
            CalculateReturns = baseOptions.CalculateReturns,
            CalculateMomentum = baseOptions.CalculateMomentum,
            WindowSizes = baseOptions.WindowSizes,
            OhlcColumns = baseOptions.OhlcColumns,
            UseParallelProcessing = false
        };

        var parallelOptions = new TimeSeriesFeatureOptions
        {
            EnableVolatility = baseOptions.EnableVolatility,
            EnabledVolatilityMeasures = baseOptions.EnabledVolatilityMeasures,
            CalculateReturns = baseOptions.CalculateReturns,
            CalculateMomentum = baseOptions.CalculateMomentum,
            WindowSizes = baseOptions.WindowSizes,
            OhlcColumns = baseOptions.OhlcColumns,
            UseParallelProcessing = true,
            ParallelThreshold = 10  // Force parallel
        };

        var data = CreateStockPriceSeries(100);

        var serialTransformer = new RollingVolatilityTransformer<double>(serialOptions);
        var parallelTransformer = new RollingVolatilityTransformer<double>(parallelOptions);

        var serialResult = serialTransformer.FitTransform(data);
        var parallelResult = parallelTransformer.FitTransform(data);

        Assert.Equal(serialResult.Shape[0], parallelResult.Shape[0]);
        Assert.Equal(serialResult.Shape[1], parallelResult.Shape[1]);

        for (int t = 0; t < serialResult.Shape[0]; t++)
        {
            for (int f = 0; f < serialResult.Shape[1]; f++)
            {
                bool nan1 = double.IsNaN(serialResult[t, f]);
                bool nan2 = double.IsNaN(parallelResult[t, f]);
                Assert.Equal(nan1, nan2);
                if (!nan1 && !nan2)
                {
                    Assert.Equal(serialResult[t, f], parallelResult[t, f], 10);
                }
            }
        }
    }

    [Fact]
    public void OhlcColumnConfig_CreateStandard_HasCorrectIndices()
    {
        var config = OhlcColumnConfig.CreateStandard();

        Assert.Equal(0, config.OpenIndex);
        Assert.Equal(1, config.HighIndex);
        Assert.Equal(2, config.LowIndex);
        Assert.Equal(3, config.CloseIndex);
        Assert.True(config.HasHighLow);
        Assert.True(config.HasOhlc);
    }

    [Fact]
    public void OhlcColumnConfig_CreateHlc_HasCorrectIndices()
    {
        var config = OhlcColumnConfig.CreateHlc(highIndex: 0, lowIndex: 1, closeIndex: 2);

        Assert.Null(config.OpenIndex);
        Assert.Equal(0, config.HighIndex);
        Assert.Equal(1, config.LowIndex);
        Assert.Equal(2, config.CloseIndex);
        Assert.True(config.HasHighLow);
        Assert.False(config.HasOhlc);
    }

    [Fact]
    public void RollingVolatilityTransformer_LargeHighLowSpread_HigherParkinsonVolatility()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableVolatility = true,
            EnabledVolatilityMeasures = VolatilityMeasures.ParkinsonVolatility,
            CalculateReturns = false,
            CalculateMomentum = false,
            WindowSizes = [5],
            OhlcColumns = OhlcColumnConfig.CreateStandard()
        };
        var transformer = new RollingVolatilityTransformer<double>(options);

        // Create two datasets: one with small H-L spread, one with large H-L spread
        var smallSpreadData = new Tensor<double>(new[] { 20, 4 });
        var largeSpreadData = new Tensor<double>(new[] { 20, 4 });
        double price = 100.0;

        for (int t = 0; t < 20; t++)
        {
            // Small spread: H-L range = 0.5
            smallSpreadData[t, 0] = price;          // Open
            smallSpreadData[t, 1] = price + 0.25;   // High
            smallSpreadData[t, 2] = price - 0.25;   // Low
            smallSpreadData[t, 3] = price + 0.1;    // Close

            // Large spread: H-L range = 5.0
            largeSpreadData[t, 0] = price;          // Open
            largeSpreadData[t, 1] = price + 2.5;    // High
            largeSpreadData[t, 2] = price - 2.5;    // Low
            largeSpreadData[t, 3] = price + 0.1;    // Close

            price += 0.1;  // Small drift
        }

        var smallResult = transformer.FitTransform(smallSpreadData);
        var largeResult = transformer.FitTransform(largeSpreadData);

        // The large spread should produce higher Parkinson volatility
        // Check at t=10 where we have full windows
        Assert.True(largeResult[10, 0] > smallResult[10, 0],
            $"Large spread ({largeResult[10, 0]}) should have higher Parkinson vol than small spread ({smallResult[10, 0]})");
    }

    #endregion

    #region Advanced Volatility Tests

    [Fact]
    public void RollingVolatilityTransformer_EwmaVolatility_RespondsToRecentData()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableVolatility = true,
            EnabledVolatilityMeasures = VolatilityMeasures.EwmaVolatility,
            CalculateReturns = false,
            CalculateMomentum = false,
            WindowSizes = [20],
            EwmaDecayFactor = 0.94
        };
        var transformer = new RollingVolatilityTransformer<double>(options);

        // Create data: stable period followed by volatile period
        var data = new Tensor<double>(new[] { 50, 1 });
        double price = 100.0;
        for (int t = 0; t < 25; t++)
        {
            price += 0.1;  // Small stable changes
            data[t, 0] = price;
        }
        for (int t = 25; t < 50; t++)
        {
            price += (t % 2 == 0 ? 2.0 : -2.0);  // Larger volatile swings
            data[t, 0] = price;
        }

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);
        Assert.Equal(50, result.Shape[0]);

        // EWMA should be higher in the volatile period
        double stableEwma = result[24, 0];
        double volatileEwma = result[45, 0];

        Assert.False(double.IsNaN(stableEwma), "EWMA in stable period should not be NaN");
        Assert.False(double.IsNaN(volatileEwma), "EWMA in volatile period should not be NaN");
        Assert.True(volatileEwma > stableEwma,
            $"EWMA should be higher in volatile period ({volatileEwma}) than stable period ({stableEwma})");
    }

    [Fact]
    public void RollingVolatilityTransformer_EwmaVolatility_DecayFactorAffectsResult()
    {
        // High decay = smoother (older data matters more)
        var highDecayOptions = new TimeSeriesFeatureOptions
        {
            EnableVolatility = true,
            EnabledVolatilityMeasures = VolatilityMeasures.EwmaVolatility,
            CalculateReturns = false,
            CalculateMomentum = false,
            WindowSizes = [10],
            EwmaDecayFactor = 0.97  // High decay
        };

        // Low decay = more responsive (recent data matters more)
        var lowDecayOptions = new TimeSeriesFeatureOptions
        {
            EnableVolatility = true,
            EnabledVolatilityMeasures = VolatilityMeasures.EwmaVolatility,
            CalculateReturns = false,
            CalculateMomentum = false,
            WindowSizes = [10],
            EwmaDecayFactor = 0.84  // Low decay
        };

        var highDecayTransformer = new RollingVolatilityTransformer<double>(highDecayOptions);
        var lowDecayTransformer = new RollingVolatilityTransformer<double>(lowDecayOptions);

        // Create data with a shock
        var data = new Tensor<double>(new[] { 30, 1 });
        for (int t = 0; t < 20; t++) data[t, 0] = 100 + t * 0.1;  // Stable
        for (int t = 20; t < 30; t++) data[t, 0] = 100 + t * 0.1 + (t - 20) * 2;  // Shock

        var highResult = highDecayTransformer.FitTransform(data);
        var lowResult = lowDecayTransformer.FitTransform(data);

        // Both should produce valid results
        Assert.False(double.IsNaN(highResult[25, 0]));
        Assert.False(double.IsNaN(lowResult[25, 0]));

        // Results should be different due to different decay factors
        Assert.NotEqual(highResult[25, 0], lowResult[25, 0], 5);
    }

    [Fact]
    public void RollingVolatilityTransformer_GarchVolatility_CapturesVolatilityClustering()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableVolatility = true,
            EnabledVolatilityMeasures = VolatilityMeasures.GarchVolatility,
            CalculateReturns = false,
            CalculateMomentum = false,
            WindowSizes = [20],
            GarchOmega = 0.00001,
            GarchAlpha = 0.09,
            GarchBeta = 0.90
        };
        var transformer = new RollingVolatilityTransformer<double>(options);

        var data = CreateStockPriceSeries(60);
        var result = transformer.FitTransform(data);

        Assert.NotNull(result);
        Assert.Equal(60, result.Shape[0]);

        // GARCH should produce non-NaN positive values
        int validCount = 0;
        for (int t = 25; t < 60; t++)
        {
            var value = result[t, 0];
            if (!double.IsNaN(value) && value > 0)
                validCount++;
        }

        Assert.True(validCount > 30, $"GARCH should produce valid values, got {validCount} out of 35");
    }

    [Fact]
    public void RollingVolatilityTransformer_GarchVolatility_InvalidParametersFallback()
    {
        // alpha + beta >= 1 is invalid for GARCH (non-stationary)
        var options = new TimeSeriesFeatureOptions
        {
            EnableVolatility = true,
            EnabledVolatilityMeasures = VolatilityMeasures.GarchVolatility,
            CalculateReturns = false,
            CalculateMomentum = false,
            WindowSizes = [10],
            GarchOmega = 0.00001,
            GarchAlpha = 0.5,  // alpha + beta = 1.1 (invalid)
            GarchBeta = 0.6
        };
        var transformer = new RollingVolatilityTransformer<double>(options);

        var data = CreateStockPriceSeries(30);
        var result = transformer.FitTransform(data);

        // Should still produce valid output (falls back to simple variance)
        Assert.NotNull(result);
        int validCount = 0;
        for (int t = 15; t < 30; t++)
        {
            if (!double.IsNaN(result[t, 0]) && result[t, 0] >= 0)
                validCount++;
        }
        Assert.True(validCount > 10, "Should fall back to simple variance when GARCH is invalid");
    }

    [Fact]
    public void RollingVolatilityTransformer_YangZhangVolatility_WithOhlc_ProducesValidOutput()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableVolatility = true,
            EnabledVolatilityMeasures = VolatilityMeasures.YangZhangVolatility,
            CalculateReturns = false,
            CalculateMomentum = false,
            WindowSizes = [10],
            OhlcColumns = OhlcColumnConfig.CreateStandard()
        };
        var transformer = new RollingVolatilityTransformer<double>(options);

        var data = CreateStockPriceSeries(40);
        var result = transformer.FitTransform(data);

        Assert.NotNull(result);
        Assert.Equal(40, result.Shape[0]);

        // Yang-Zhang should produce valid values for all 4 OHLC columns
        int validCount = 0;
        for (int t = 15; t < 40; t++)
        {
            // Check the first feature (close column)
            var value = result[t, 0];
            if (!double.IsNaN(value) && value > 0)
                validCount++;
        }

        Assert.True(validCount > 20, $"Yang-Zhang should produce valid values, got {validCount} out of 25");
    }

    [Fact]
    public void RollingVolatilityTransformer_YangZhangVolatility_FallbackWithoutOhlc()
    {
        // Without OHLC config, should fall back to realized volatility
        var options = new TimeSeriesFeatureOptions
        {
            EnableVolatility = true,
            EnabledVolatilityMeasures = VolatilityMeasures.YangZhangVolatility,
            CalculateReturns = false,
            CalculateMomentum = false,
            WindowSizes = [10],
            OhlcColumns = null  // No OHLC config
        };
        var transformer = new RollingVolatilityTransformer<double>(options);

        var data = new Tensor<double>(new[] { 30, 1 });
        for (int t = 0; t < 30; t++)
            data[t, 0] = 100 + Math.Sin(t * 0.5) * 5 + t * 0.1;

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);

        // Should fall back and produce valid values
        int validCount = 0;
        for (int t = 15; t < 30; t++)
        {
            if (!double.IsNaN(result[t, 0]) && result[t, 0] > 0)
                validCount++;
        }
        Assert.True(validCount > 10, "Should fall back to realized volatility when OHLC is missing");
    }

    [Fact]
    public void RollingVolatilityTransformer_RogersSatchellVolatility_DriftIndependent()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableVolatility = true,
            EnabledVolatilityMeasures = VolatilityMeasures.RogersSatchellVolatility,
            CalculateReturns = false,
            CalculateMomentum = false,
            WindowSizes = [10],
            OhlcColumns = OhlcColumnConfig.CreateStandard()
        };
        var transformer = new RollingVolatilityTransformer<double>(options);

        // Create data with drift (trending up) and same volatility characteristics
        var data = new Tensor<double>(new[] { 30, 4 });
        double price = 100.0;
        for (int t = 0; t < 30; t++)
        {
            double drift = t * 0.5;  // Strong upward drift
            double volatility = 2.0; // Consistent volatility
            data[t, 0] = price + drift;                    // Open
            data[t, 1] = price + drift + volatility;       // High
            data[t, 2] = price + drift - volatility;       // Low
            data[t, 3] = price + drift + (t % 2 == 0 ? 0.5 : -0.5);  // Close
        }

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);

        // Rogers-Satchell should produce consistent volatility despite the drift
        // Check variance of the volatility estimates in the stable region
        var volatilities = new List<double>();
        for (int t = 15; t < 30; t++)
        {
            var value = result[t, 0];  // First column (open-based)
            if (!double.IsNaN(value) && value > 0)
                volatilities.Add(value);
        }

        Assert.True(volatilities.Count > 10, "Should have valid volatility estimates");
    }

    [Fact]
    public void RollingVolatilityTransformer_RogersSatchellVolatility_FallbackWithoutOhlc()
    {
        // Without OHLC config, should fall back to Parkinson approximation
        var options = new TimeSeriesFeatureOptions
        {
            EnableVolatility = true,
            EnabledVolatilityMeasures = VolatilityMeasures.RogersSatchellVolatility,
            CalculateReturns = false,
            CalculateMomentum = false,
            WindowSizes = [10],
            OhlcColumns = null
        };
        var transformer = new RollingVolatilityTransformer<double>(options);

        var data = new Tensor<double>(new[] { 30, 1 });
        for (int t = 0; t < 30; t++)
            data[t, 0] = 100 + Math.Sin(t * 0.5) * 5;

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);

        // Should fall back and produce valid values
        int validCount = 0;
        for (int t = 15; t < 30; t++)
        {
            if (!double.IsNaN(result[t, 0]) && result[t, 0] > 0)
                validCount++;
        }
        Assert.True(validCount > 10, "Should fall back to Parkinson approximation when OHLC is missing");
    }

    [Fact]
    public void RollingVolatilityTransformer_AllAdvancedVolatility_ProducesValidOutput()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableVolatility = true,
            EnabledVolatilityMeasures = VolatilityMeasures.Advanced | VolatilityMeasures.OhlcBased,
            CalculateReturns = false,
            CalculateMomentum = false,
            WindowSizes = [10],
            OhlcColumns = OhlcColumnConfig.CreateStandard(),
            EwmaDecayFactor = 0.94,
            GarchOmega = 0.00001,
            GarchAlpha = 0.09,
            GarchBeta = 0.90
        };
        var transformer = new RollingVolatilityTransformer<double>(options);

        var data = CreateStockPriceSeries(50);
        var result = transformer.FitTransform(data);

        Assert.NotNull(result);
        Assert.Equal(50, result.Shape[0]);

        // Should have features for EWMA, GARCH, Yang-Zhang, Rogers-Satchell, Parkinson, Garman-Klass
        // With 4 OHLC features and 6 measures: expect 4 * 6 = 24 output features
        Assert.True(result.Shape[1] >= 4, $"Expected at least 4 advanced volatility features, got {result.Shape[1]}");

        // Verify we have valid values in later time steps
        int validCount = 0;
        for (int t = 20; t < 50; t++)
        {
            for (int f = 0; f < Math.Min(result.Shape[1], 8); f++)
            {
                var value = result[t, f];
                if (!double.IsNaN(value) && value >= 0)
                    validCount++;
            }
        }

        Assert.True(validCount > 100, $"Should have many valid advanced volatility values, got {validCount}");
    }

    [Fact]
    public void RollingVolatilityTransformer_AdvancedVolatility_ParallelEqualsSequential()
    {
        var baseOptions = new TimeSeriesFeatureOptions
        {
            EnableVolatility = true,
            EnabledVolatilityMeasures = VolatilityMeasures.EwmaVolatility | VolatilityMeasures.GarchVolatility |
                                        VolatilityMeasures.YangZhangVolatility | VolatilityMeasures.RogersSatchellVolatility,
            CalculateReturns = false,
            CalculateMomentum = false,
            WindowSizes = [10],
            OhlcColumns = OhlcColumnConfig.CreateStandard(),
            EwmaDecayFactor = 0.94,
            GarchOmega = 0.00001,
            GarchAlpha = 0.09,
            GarchBeta = 0.90
        };

        var serialOptions = new TimeSeriesFeatureOptions
        {
            EnableVolatility = baseOptions.EnableVolatility,
            EnabledVolatilityMeasures = baseOptions.EnabledVolatilityMeasures,
            CalculateReturns = baseOptions.CalculateReturns,
            CalculateMomentum = baseOptions.CalculateMomentum,
            WindowSizes = baseOptions.WindowSizes,
            OhlcColumns = baseOptions.OhlcColumns,
            EwmaDecayFactor = baseOptions.EwmaDecayFactor,
            GarchOmega = baseOptions.GarchOmega,
            GarchAlpha = baseOptions.GarchAlpha,
            GarchBeta = baseOptions.GarchBeta,
            UseParallelProcessing = false
        };

        var parallelOptions = new TimeSeriesFeatureOptions
        {
            EnableVolatility = baseOptions.EnableVolatility,
            EnabledVolatilityMeasures = baseOptions.EnabledVolatilityMeasures,
            CalculateReturns = baseOptions.CalculateReturns,
            CalculateMomentum = baseOptions.CalculateMomentum,
            WindowSizes = baseOptions.WindowSizes,
            OhlcColumns = baseOptions.OhlcColumns,
            EwmaDecayFactor = baseOptions.EwmaDecayFactor,
            GarchOmega = baseOptions.GarchOmega,
            GarchAlpha = baseOptions.GarchAlpha,
            GarchBeta = baseOptions.GarchBeta,
            UseParallelProcessing = true,
            ParallelThreshold = 10
        };

        var data = CreateStockPriceSeries(60);

        var serialTransformer = new RollingVolatilityTransformer<double>(serialOptions);
        var parallelTransformer = new RollingVolatilityTransformer<double>(parallelOptions);

        var serialResult = serialTransformer.FitTransform(data);
        var parallelResult = parallelTransformer.FitTransform(data);

        Assert.Equal(serialResult.Shape[0], parallelResult.Shape[0]);
        Assert.Equal(serialResult.Shape[1], parallelResult.Shape[1]);

        for (int t = 0; t < serialResult.Shape[0]; t++)
        {
            for (int f = 0; f < serialResult.Shape[1]; f++)
            {
                bool nan1 = double.IsNaN(serialResult[t, f]);
                bool nan2 = double.IsNaN(parallelResult[t, f]);
                Assert.Equal(nan1, nan2);
                if (!nan1 && !nan2)
                {
                    Assert.Equal(serialResult[t, f], parallelResult[t, f], 10);
                }
            }
        }
    }

    #endregion

    #region TechnicalIndicatorsTransformer Tests

    [Fact]
    public void TechnicalIndicatorsTransformer_Construction_Succeeds()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableTechnicalIndicators = true,
            EnabledIndicators = TechnicalIndicators.SMA | TechnicalIndicators.EMA,
            WindowSizes = [5, 10]
        };
        var transformer = new TechnicalIndicatorsTransformer<double>(options);

        Assert.NotNull(transformer);
    }

    [Fact]
    public void TechnicalIndicatorsTransformer_SMA_ProducesCorrectOutput()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableTechnicalIndicators = true,
            EnabledIndicators = TechnicalIndicators.SMA,
            WindowSizes = [5]
        };
        var transformer = new TechnicalIndicatorsTransformer<double>(options);

        // Simple sequence: 10, 20, 30, 40, 50, 60, 70, 80, 90, 100
        var data = new Tensor<double>(new[] { 10, 1 });
        for (int i = 0; i < 10; i++) data[i, 0] = (i + 1) * 10;

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);
        Assert.Equal(10, result.Shape[0]);

        // SMA(5) at t=4: (10+20+30+40+50)/5 = 30
        Assert.Equal(30.0, result[4, 0], 5);

        // SMA(5) at t=9: (60+70+80+90+100)/5 = 80
        Assert.Equal(80.0, result[9, 0], 5);

        // First few values should be NaN
        Assert.True(double.IsNaN(result[0, 0]));
        Assert.True(double.IsNaN(result[3, 0]));
    }

    [Fact]
    public void TechnicalIndicatorsTransformer_EMA_RespondsToRecentPrices()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableTechnicalIndicators = true,
            EnabledIndicators = TechnicalIndicators.EMA,
            WindowSizes = [5]
        };
        var transformer = new TechnicalIndicatorsTransformer<double>(options);

        // Create data with a price jump
        var data = new Tensor<double>(new[] { 20, 1 });
        for (int i = 0; i < 10; i++) data[i, 0] = 100;  // Flat at 100
        for (int i = 10; i < 20; i++) data[i, 0] = 200; // Jump to 200

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);

        // After the jump, EMA should start moving toward 200
        Assert.True(result[12, 0] > 100, "EMA should respond to price jump");
        Assert.True(result[12, 0] < 200, "EMA should lag behind the jump");
        Assert.True(result[19, 0] > result[12, 0], "EMA should continue approaching new price");
    }

    [Fact]
    public void TechnicalIndicatorsTransformer_RSI_ReturnsValuesBetween0And100()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableTechnicalIndicators = true,
            EnabledIndicators = TechnicalIndicators.RSI,
            WindowSizes = [14]
        };
        var transformer = new TechnicalIndicatorsTransformer<double>(options);

        var data = CreateStockPriceSeries(50);
        var result = transformer.FitTransform(data);

        Assert.NotNull(result);

        // RSI should be between 0 and 100
        for (int t = 20; t < 50; t++)
        {
            double rsi = result[t, 0];
            if (!double.IsNaN(rsi))
            {
                Assert.True(rsi >= 0 && rsi <= 100,
                    $"RSI at t={t} should be between 0 and 100, got {rsi}");
            }
        }
    }

    [Fact]
    public void TechnicalIndicatorsTransformer_BollingerBands_UpperGreaterThanLower()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableTechnicalIndicators = true,
            EnabledIndicators = TechnicalIndicators.BollingerBands,
            BollingerBandStdDev = 2.0,
            WindowSizes = [20]
        };
        var transformer = new TechnicalIndicatorsTransformer<double>(options);

        // Use single-feature data for cleaner BB output ordering
        var data = new Tensor<double>(new[] { 50, 1 });
        for (int i = 0; i < 50; i++)
            data[i, 0] = 100 + Math.Sin(i * 0.2) * 10;

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);
        // BB outputs for 1 feature: upper, middle, lower, width = 4 columns
        Assert.Equal(4, result.Shape[1]);

        // Check upper >= middle >= lower and width >= 0
        for (int t = 25; t < 50; t++)
        {
            double upper = result[t, 0];
            double middle = result[t, 1];
            double lower = result[t, 2];
            double width = result[t, 3];

            if (!double.IsNaN(upper) && !double.IsNaN(middle) && !double.IsNaN(lower))
            {
                Assert.True(upper >= middle, $"BB upper ({upper}) should >= middle ({middle}) at t={t}");
                Assert.True(middle >= lower, $"BB middle ({middle}) should >= lower ({lower}) at t={t}");
                Assert.True(width >= 0, $"BB width should be non-negative at t={t}, got {width}");
            }
        }
    }

    [Fact]
    public void TechnicalIndicatorsTransformer_MACD_ProducesThreeOutputs()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableTechnicalIndicators = true,
            EnabledIndicators = TechnicalIndicators.MACD,
            ShortPeriod = 12,
            LongPeriod = 26,
            SignalPeriod = 9,
            WindowSizes = [26]  // Must be at least 2
        };
        var transformer = new TechnicalIndicatorsTransformer<double>(options);

        // Use single-feature data for cleaner output ordering
        var data = new Tensor<double>(new[] { 100, 1 });
        for (int i = 0; i < 100; i++)
            data[i, 0] = 100 + Math.Sin(i * 0.1) * 10 + i * 0.5;  // Trending sine wave

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);
        // MACD, Signal, Histogram = 3 outputs for 1 feature
        Assert.Equal(3, result.Shape[1]);

        // Histogram = MACD - Signal, verify this relationship
        for (int t = 50; t < 100; t++)
        {
            double macd = result[t, 0];
            double signal = result[t, 1];
            double histogram = result[t, 2];

            if (!double.IsNaN(macd) && !double.IsNaN(signal) && !double.IsNaN(histogram))
            {
                Assert.Equal(macd - signal, histogram, 10);
            }
        }
    }

    [Fact]
    public void TechnicalIndicatorsTransformer_StochasticOscillator_ReturnsValuesBetween0And100()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableTechnicalIndicators = true,
            EnabledIndicators = TechnicalIndicators.StochasticOscillator,
            StochasticKPeriod = 14,
            StochasticDPeriod = 3,
            WindowSizes = [14],
            OhlcColumns = OhlcColumnConfig.CreateStandard()
        };
        var transformer = new TechnicalIndicatorsTransformer<double>(options);

        var data = CreateStockPriceSeries(50);
        var result = transformer.FitTransform(data);

        Assert.NotNull(result);
        // %K and %D = 2 outputs

        // Stochastic values should be 0-100
        for (int t = 20; t < 50; t++)
        {
            double k = result[t, 0];
            double d = result[t, 1];

            if (!double.IsNaN(k))
            {
                Assert.True(k >= 0 && k <= 100, $"Stochastic %K at t={t} should be 0-100, got {k}");
            }
            if (!double.IsNaN(d))
            {
                Assert.True(d >= 0 && d <= 100, $"Stochastic %D at t={t} should be 0-100, got {d}");
            }
        }
    }

    [Fact]
    public void TechnicalIndicatorsTransformer_WilliamsR_ReturnsValuesBetweenMinus100And0()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableTechnicalIndicators = true,
            EnabledIndicators = TechnicalIndicators.WilliamsR,
            WindowSizes = [14],
            OhlcColumns = OhlcColumnConfig.CreateStandard()
        };
        var transformer = new TechnicalIndicatorsTransformer<double>(options);

        var data = CreateStockPriceSeries(50);
        var result = transformer.FitTransform(data);

        Assert.NotNull(result);

        // Williams %R should be -100 to 0
        for (int t = 20; t < 50; t++)
        {
            double wr = result[t, 0];
            if (!double.IsNaN(wr))
            {
                Assert.True(wr >= -100 && wr <= 0,
                    $"Williams %R at t={t} should be -100 to 0, got {wr}");
            }
        }
    }

    [Fact]
    public void TechnicalIndicatorsTransformer_ATR_IsNonNegative()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableTechnicalIndicators = true,
            EnabledIndicators = TechnicalIndicators.ATR,
            WindowSizes = [14],
            OhlcColumns = OhlcColumnConfig.CreateStandard()
        };
        var transformer = new TechnicalIndicatorsTransformer<double>(options);

        var data = CreateStockPriceSeries(50);
        var result = transformer.FitTransform(data);

        Assert.NotNull(result);

        // ATR should always be non-negative
        for (int t = 20; t < 50; t++)
        {
            double atr = result[t, 0];
            if (!double.IsNaN(atr))
            {
                Assert.True(atr >= 0, $"ATR at t={t} should be non-negative, got {atr}");
            }
        }
    }

    [Fact]
    public void TechnicalIndicatorsTransformer_AllIndicators_ProducesValidOutput()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableTechnicalIndicators = true,
            EnabledIndicators = TechnicalIndicators.All,
            WindowSizes = [14],
            OhlcColumns = OhlcColumnConfig.CreateStandard()
        };
        var transformer = new TechnicalIndicatorsTransformer<double>(options);

        var data = CreateStockPriceSeries(100);
        var result = transformer.FitTransform(data);

        Assert.NotNull(result);
        Assert.Equal(100, result.Shape[0]);

        // With all indicators, should have many output features
        // SMA(1) + EMA(1) + WMA(1) + DEMA(1) + TEMA(1) + BB(4) + RSI(1) + MACD(3) + ATR(1) + Stoch(2) + CCI(1) + WilliamsR(1) + ADX(3) + OBV(1)
        // = 22 per input feature, 4 input features = 88+ features
        Assert.True(result.Shape[1] > 20, $"Expected many output features, got {result.Shape[1]}");
    }

    [Fact]
    public void TechnicalIndicatorsTransformer_WMA_WeightsRecentPricesMore()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableTechnicalIndicators = true,
            EnabledIndicators = TechnicalIndicators.WMA | TechnicalIndicators.SMA,
            WindowSizes = [5]
        };
        var transformer = new TechnicalIndicatorsTransformer<double>(options);

        // Create data with recent uptick
        var data = new Tensor<double>(new[] { 10, 1 });
        for (int i = 0; i < 8; i++) data[i, 0] = 100;
        data[8, 0] = 100;
        data[9, 0] = 150;  // Sharp uptick at end

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);

        // WMA should respond more to the recent uptick than SMA
        double sma = result[9, 0];
        double wma = result[9, 1];

        // Both should be elevated, but WMA more so due to weighting
        Assert.True(!double.IsNaN(sma) && !double.IsNaN(wma), "Both SMA and WMA should have values");
    }

    [Fact]
    public void TechnicalIndicatorsTransformer_DEMA_ReducesLag()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableTechnicalIndicators = true,
            EnabledIndicators = TechnicalIndicators.EMA | TechnicalIndicators.DEMA,
            WindowSizes = [10]
        };
        var transformer = new TechnicalIndicatorsTransformer<double>(options);

        // Create trending data
        var data = new Tensor<double>(new[] { 50, 1 });
        for (int i = 0; i < 50; i++) data[i, 0] = 100 + i * 2;  // Uptrend

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);

        // DEMA should be closer to current price than EMA in a trend
        // At end of series, price = 100 + 49*2 = 198
        double currentPrice = 198;
        double ema = result[49, 0];
        double dema = result[49, 1];

        if (!double.IsNaN(ema) && !double.IsNaN(dema))
        {
            Assert.True(Math.Abs(dema - currentPrice) <= Math.Abs(ema - currentPrice),
                $"DEMA ({dema}) should be closer to price ({currentPrice}) than EMA ({ema})");
        }
    }

    [Fact]
    public void TechnicalIndicatorsTransformer_CCI_CanDetectOverboughtOversold()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableTechnicalIndicators = true,
            EnabledIndicators = TechnicalIndicators.CCI,
            CciPeriod = 20,
            WindowSizes = [20],
            OhlcColumns = OhlcColumnConfig.CreateStandard()
        };
        var transformer = new TechnicalIndicatorsTransformer<double>(options);

        var data = CreateStockPriceSeries(100);
        var result = transformer.FitTransform(data);

        Assert.NotNull(result);

        // CCI can be any value but typically extreme values indicate overbought/oversold
        // Just verify it produces valid numeric output
        int validCount = 0;
        for (int t = 30; t < 100; t++)
        {
            if (!double.IsNaN(result[t, 0]))
                validCount++;
        }

        Assert.True(validCount > 0, "CCI should produce valid values");
    }

    [Fact]
    public void TechnicalIndicatorsTransformer_ADX_MeasuresTrendStrength()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableTechnicalIndicators = true,
            EnabledIndicators = TechnicalIndicators.ADX,
            AdxPeriod = 14,
            WindowSizes = [14],
            OhlcColumns = OhlcColumnConfig.CreateStandard()
        };
        var transformer = new TechnicalIndicatorsTransformer<double>(options);

        var data = CreateStockPriceSeries(100);
        var result = transformer.FitTransform(data);

        Assert.NotNull(result);
        // ADX, +DI, -DI = 3 outputs

        // ADX should be 0-100
        for (int t = 40; t < 100; t++)
        {
            double adx = result[t, 0];
            if (!double.IsNaN(adx))
            {
                Assert.True(adx >= 0, $"ADX at t={t} should be non-negative, got {adx}");
            }
        }
    }

    [Fact]
    public void TechnicalIndicatorsTransformer_MultipleWindowSizes_ProducesCorrectFeatureCount()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableTechnicalIndicators = true,
            EnabledIndicators = TechnicalIndicators.SMA | TechnicalIndicators.EMA,
            WindowSizes = [5, 10, 20]
        };
        var transformer = new TechnicalIndicatorsTransformer<double>(options);

        // Single feature input
        var data = new Tensor<double>(new[] { 50, 1 });
        for (int i = 0; i < 50; i++) data[i, 0] = 100 + Math.Sin(i * 0.3) * 10;

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);
        // 2 indicators * 3 windows * 1 feature = 6 output features
        Assert.Equal(6, result.Shape[1]);
    }

    [Fact]
    public void TechnicalIndicatorsTransformer_WithoutOhlc_UsesApproximations()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableTechnicalIndicators = true,
            EnabledIndicators = TechnicalIndicators.ATR | TechnicalIndicators.StochasticOscillator,
            WindowSizes = [14],
            OhlcColumns = null  // No OHLC config
        };
        var transformer = new TechnicalIndicatorsTransformer<double>(options);

        // Simple price data
        var data = new Tensor<double>(new[] { 50, 1 });
        for (int i = 0; i < 50; i++) data[i, 0] = 100 + Math.Sin(i * 0.2) * 5;

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);

        // Should still produce valid output using approximations
        int validCount = 0;
        for (int t = 20; t < 50; t++)
        {
            for (int f = 0; f < result.Shape[1]; f++)
            {
                if (!double.IsNaN(result[t, f]))
                    validCount++;
            }
        }

        Assert.True(validCount > 0, "Should produce valid values even without OHLC");
    }

    #endregion

    #region SeasonalityTransformer Tests

    [Fact]
    public void SeasonalityTransformer_Construction_Succeeds()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableSeasonality = true,
            EnabledSeasonalityFeatures = SeasonalityFeatures.FourierFeatures,
            SeasonalPeriods = [7, 30],
            FourierTerms = 2,
            WindowSizes = [7]
        };
        var transformer = new SeasonalityTransformer<double>(options);

        Assert.NotNull(transformer);
    }

    [Fact]
    public void SeasonalityTransformer_FourierFeatures_ProducesCorrectOutput()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableSeasonality = true,
            EnabledSeasonalityFeatures = SeasonalityFeatures.FourierFeatures,
            SeasonalPeriods = [7],
            FourierTerms = 2,
            WindowSizes = [7]
        };
        var transformer = new SeasonalityTransformer<double>(options);

        // 21 time steps (3 weeks)
        var data = new Tensor<double>(new[] { 21, 1 });
        for (int i = 0; i < 21; i++) data[i, 0] = i;

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);
        Assert.Equal(21, result.Shape[0]);
        // 1 period * 2 terms * 2 (sin+cos) = 4 features
        Assert.Equal(4, result.Shape[1]);

        // Verify sin/cos values are in [-1, 1]
        for (int t = 0; t < 21; t++)
        {
            for (int f = 0; f < result.Shape[1]; f++)
            {
                Assert.True(result[t, f] >= -1 && result[t, f] <= 1,
                    $"Fourier value at t={t}, f={f} should be in [-1,1], got {result[t, f]}");
            }
        }

        // Verify periodicity: sin values should repeat every 7 steps
        // At t=0 and t=7, sin(2*pi*1*0/7) = sin(2*pi*1*7/7) = sin(0) = sin(2*pi) = 0
        Assert.Equal(result[0, 0], result[7, 0], 10);
        Assert.Equal(result[0, 0], result[14, 0], 10);
    }

    [Fact]
    public void SeasonalityTransformer_TimeFeatures_WithDate_ProducesValidOutput()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableSeasonality = true,
            EnabledSeasonalityFeatures = SeasonalityFeatures.TimeFeatures,
            TimeSeriesStartDate = new DateTime(2024, 1, 1),
            TimeSeriesInterval = TimeSpan.FromDays(1),
            WindowSizes = [7]
        };
        var transformer = new SeasonalityTransformer<double>(options);

        // 31 days (January 2024)
        var data = new Tensor<double>(new[] { 31, 1 });
        for (int i = 0; i < 31; i++) data[i, 0] = i;

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);
        Assert.Equal(31, result.Shape[0]);

        // TimeFeatures: hour, day_of_week, day_of_month, day_of_year, week_of_year, month_of_year, quarter_of_year, year = 8 features
        Assert.Equal(8, result.Shape[1]);

        // All values should be normalized (0-1 or small range)
        for (int t = 0; t < 31; t++)
        {
            for (int f = 0; f < result.Shape[1] - 1; f++) // Except year which can be larger
            {
                Assert.True(result[t, f] >= 0 && result[t, f] <= 1,
                    $"Time feature at t={t}, f={f} should be normalized, got {result[t, f]}");
            }
        }
    }

    [Fact]
    public void SeasonalityTransformer_CalendarEvents_DetectsWeekend()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableSeasonality = true,
            EnabledSeasonalityFeatures = SeasonalityFeatures.IsWeekend,
            TimeSeriesStartDate = new DateTime(2024, 1, 1), // Monday
            TimeSeriesInterval = TimeSpan.FromDays(1),
            WindowSizes = [7]
        };
        var transformer = new SeasonalityTransformer<double>(options);

        // 7 days starting Monday
        var data = new Tensor<double>(new[] { 7, 1 });
        for (int i = 0; i < 7; i++) data[i, 0] = i;

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);
        Assert.Equal(7, result.Shape[0]);
        Assert.Equal(1, result.Shape[1]); // Just is_weekend

        // Mon=0, Tue=0, Wed=0, Thu=0, Fri=0, Sat=1, Sun=1
        Assert.Equal(0.0, result[0, 0], 10); // Monday
        Assert.Equal(0.0, result[4, 0], 10); // Friday
        Assert.Equal(1.0, result[5, 0], 10); // Saturday
        Assert.Equal(1.0, result[6, 0], 10); // Sunday
    }

    [Fact]
    public void SeasonalityTransformer_HolidayFeatures_DetectsHolidays()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableSeasonality = true,
            EnabledSeasonalityFeatures = SeasonalityFeatures.HolidayFeatures,
            TimeSeriesStartDate = new DateTime(2024, 12, 24), // Start at Dec 24 (day before Christmas)
            TimeSeriesInterval = TimeSpan.FromDays(1),
            HolidayDates = [new DateTime(2024, 12, 25), new DateTime(2024, 12, 31)],
            HolidayWindowDays = 1,
            WindowSizes = [7]
        };
        var transformer = new SeasonalityTransformer<double>(options);

        // 10 days starting Dec 24
        var data = new Tensor<double>(new[] { 10, 1 });
        for (int i = 0; i < 10; i++) data[i, 0] = i;

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);
        // is_holiday, is_near_holiday = 2 features
        Assert.Equal(2, result.Shape[1]);

        // Dec 24 (t=0): 1 day before Christmas - should be "near holiday"
        Assert.Equal(0.0, result[0, 0], 10); // Not holiday
        Assert.Equal(1.0, result[0, 1], 10); // Near holiday

        // Dec 25 (t=1): is holiday (Christmas)
        Assert.Equal(1.0, result[1, 0], 10); // Is holiday (Christmas)
        Assert.Equal(0.0, result[1, 1], 10); // Not "near" (it's the actual holiday)
    }

    [Fact]
    public void SeasonalityTransformer_TradingFeatures_ProducesValidOutput()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableSeasonality = true,
            EnabledSeasonalityFeatures = SeasonalityFeatures.TradingFeatures,
            TimeSeriesStartDate = new DateTime(2024, 1, 2), // Tuesday (first trading day of 2024)
            TimeSeriesInterval = TimeSpan.FromDays(1),
            IsTradingDayData = false,
            WindowSizes = [3] // Use smaller window size
        };
        var transformer = new SeasonalityTransformer<double>(options);

        // 10 calendar days (includes ~7 trading days)
        var data = new Tensor<double>(new[] { 10, 1 });
        for (int i = 0; i < 10; i++) data[i, 0] = i;

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);
        // trading_day_of_month, trading_day_of_week = 2 features
        Assert.Equal(2, result.Shape[1]);

        // Values should be normalized
        for (int t = 0; t < 10; t++)
        {
            for (int f = 0; f < result.Shape[1]; f++)
            {
                Assert.True(result[t, f] >= 0 && result[t, f] <= 1,
                    $"Trading feature at t={t}, f={f} should be normalized, got {result[t, f]}");
            }
        }
    }

    [Fact]
    public void SeasonalityTransformer_AllFeatures_ProducesValidOutput()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableSeasonality = true,
            EnabledSeasonalityFeatures = SeasonalityFeatures.All,
            SeasonalPeriods = [7, 30],
            FourierTerms = 2,
            TimeSeriesStartDate = new DateTime(2024, 1, 1),
            TimeSeriesInterval = TimeSpan.FromDays(1),
            HolidayDates = [new DateTime(2024, 1, 1)],
            WindowSizes = [7]
        };
        var transformer = new SeasonalityTransformer<double>(options);

        var data = new Tensor<double>(new[] { 100, 1 });
        for (int i = 0; i < 100; i++) data[i, 0] = i;

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);
        Assert.Equal(100, result.Shape[0]);
        Assert.True(result.Shape[1] > 10, $"Should have many features with All, got {result.Shape[1]}");

        // All values should be finite
        for (int t = 0; t < 100; t++)
        {
            for (int f = 0; f < result.Shape[1]; f++)
            {
                Assert.True(!double.IsNaN(result[t, f]) && !double.IsInfinity(result[t, f]),
                    $"Feature at t={t}, f={f} should be finite, got {result[t, f]}");
            }
        }
    }

    [Fact]
    public void SeasonalityTransformer_WithoutDate_UsesIndexBasedFeatures()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableSeasonality = true,
            EnabledSeasonalityFeatures = SeasonalityFeatures.TimeFeatures,
            TimeSeriesStartDate = null, // No date
            WindowSizes = [7]
        };
        var transformer = new SeasonalityTransformer<double>(options);

        var data = new Tensor<double>(new[] { 50, 1 });
        for (int i = 0; i < 50; i++) data[i, 0] = i;

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);
        Assert.Equal(50, result.Shape[0]);

        // Should have 8 time features
        Assert.Equal(8, result.Shape[1]);

        // First time step should be 0, last should be 1 (normalized index)
        Assert.Equal(0.0, result[0, 0], 10);
        Assert.Equal(1.0, result[49, 0], 10);
    }

    [Fact]
    public void SeasonalityTransformer_MonthStartEnd_DetectsCorrectly()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableSeasonality = true,
            EnabledSeasonalityFeatures = SeasonalityFeatures.MonthStartEnd,
            TimeSeriesStartDate = new DateTime(2024, 1, 1),
            TimeSeriesInterval = TimeSpan.FromDays(1),
            WindowSizes = [7]
        };
        var transformer = new SeasonalityTransformer<double>(options);

        // 31 days of January
        var data = new Tensor<double>(new[] { 31, 1 });
        for (int i = 0; i < 31; i++) data[i, 0] = i;

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);
        // is_month_start, is_month_end = 2 features
        Assert.Equal(2, result.Shape[1]);

        // Days 1-3 should be month start
        Assert.Equal(1.0, result[0, 0], 10); // Jan 1
        Assert.Equal(1.0, result[1, 0], 10); // Jan 2
        Assert.Equal(1.0, result[2, 0], 10); // Jan 3
        Assert.Equal(0.0, result[3, 0], 10); // Jan 4 - not start

        // Days 29-31 should be month end
        Assert.Equal(0.0, result[27, 1], 10); // Jan 28 - not end
        Assert.Equal(1.0, result[28, 1], 10); // Jan 29
        Assert.Equal(1.0, result[29, 1], 10); // Jan 30
        Assert.Equal(1.0, result[30, 1], 10); // Jan 31
    }

    [Fact]
    public void SeasonalityTransformer_FourierFeatures_CaptureCycles()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableSeasonality = true,
            EnabledSeasonalityFeatures = SeasonalityFeatures.FourierFeatures,
            SeasonalPeriods = [10], // Period of 10
            FourierTerms = 1,
            WindowSizes = [7]
        };
        var transformer = new SeasonalityTransformer<double>(options);

        // 30 time steps (3 complete cycles)
        var data = new Tensor<double>(new[] { 30, 1 });
        for (int i = 0; i < 30; i++) data[i, 0] = i;

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);
        // 1 period * 1 term * 2 (sin+cos) = 2 features
        Assert.Equal(2, result.Shape[1]);

        // Values at t=0, t=10, t=20 should be equal (same phase in cycle)
        Assert.Equal(result[0, 0], result[10, 0], 10);
        Assert.Equal(result[0, 0], result[20, 0], 10);
        Assert.Equal(result[0, 1], result[10, 1], 10);
        Assert.Equal(result[0, 1], result[20, 1], 10);

        // Sin at t=0 should be 0, cos at t=0 should be 1
        Assert.Equal(0.0, result[0, 0], 10); // sin(0) = 0
        Assert.Equal(1.0, result[0, 1], 10); // cos(0) = 1
    }

    #endregion

    #region DifferencingTransformer Tests

    [Fact]
    public void DifferencingTransformer_Construction_Succeeds()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableDifferencing = true,
            EnabledDifferencingFeatures = DifferencingFeatures.FirstDifference,
            WindowSizes = [3]
        };
        var transformer = new DifferencingTransformer<double>(options);

        Assert.NotNull(transformer);
    }

    [Fact]
    public void DifferencingTransformer_FirstDifference_ComputesCorrectly()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableDifferencing = true,
            EnabledDifferencingFeatures = DifferencingFeatures.FirstDifference,
            WindowSizes = [3]
        };
        var transformer = new DifferencingTransformer<double>(options);

        // Simple sequence: 10, 20, 30, 40, 50
        var data = new Tensor<double>(new[] { 5, 1 });
        data[0, 0] = 10;
        data[1, 0] = 20;
        data[2, 0] = 30;
        data[3, 0] = 40;
        data[4, 0] = 50;

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);
        Assert.Equal(5, result.Shape[0]);
        Assert.Equal(1, result.Shape[1]);

        // First diff: NaN, 10, 10, 10, 10
        Assert.True(double.IsNaN(result[0, 0]));
        Assert.Equal(10.0, result[1, 0], 10);
        Assert.Equal(10.0, result[2, 0], 10);
        Assert.Equal(10.0, result[3, 0], 10);
        Assert.Equal(10.0, result[4, 0], 10);
    }

    [Fact]
    public void DifferencingTransformer_SecondDifference_ComputesCorrectly()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableDifferencing = true,
            EnabledDifferencingFeatures = DifferencingFeatures.SecondDifference,
            WindowSizes = [3]
        };
        var transformer = new DifferencingTransformer<double>(options);

        // Quadratic sequence: 1, 4, 9, 16, 25, 36
        var data = new Tensor<double>(new[] { 6, 1 });
        for (int i = 0; i < 6; i++)
            data[i, 0] = (i + 1) * (i + 1);

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);

        // First diff: NaN, 3, 5, 7, 9, 11
        // Second diff: NaN, NaN, 2, 2, 2, 2
        Assert.True(double.IsNaN(result[0, 0]));
        Assert.True(double.IsNaN(result[1, 0]));
        Assert.Equal(2.0, result[2, 0], 10);
        Assert.Equal(2.0, result[3, 0], 10);
        Assert.Equal(2.0, result[4, 0], 10);
    }

    [Fact]
    public void DifferencingTransformer_SeasonalDifference_RemovesSeasonality()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableDifferencing = true,
            EnabledDifferencingFeatures = DifferencingFeatures.SeasonalDifference,
            SeasonalDifferencingPeriod = 7,
            WindowSizes = [7]
        };
        var transformer = new DifferencingTransformer<double>(options);

        // Create data with weekly pattern + linear trend
        var data = new Tensor<double>(new[] { 21, 1 });
        for (int i = 0; i < 21; i++)
        {
            double seasonal = 10 * (i % 7); // Weekly pattern
            double trend = i;               // Linear trend
            data[i, 0] = seasonal + trend;
        }

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);

        // After seasonal differencing, should have constant value (the weekly trend = 7)
        for (int t = 7; t < 21; t++)
        {
            Assert.Equal(7.0, result[t, 0], 10); // y[t] - y[t-7] = (t) - (t-7) = 7
        }
    }

    [Fact]
    public void DifferencingTransformer_PercentChange_ComputesCorrectly()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableDifferencing = true,
            EnabledDifferencingFeatures = DifferencingFeatures.PercentChange,
            WindowSizes = [3]
        };
        var transformer = new DifferencingTransformer<double>(options);

        // Price doubles each period: 100, 200, 400
        var data = new Tensor<double>(new[] { 3, 1 });
        data[0, 0] = 100;
        data[1, 0] = 200;
        data[2, 0] = 400;

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);
        Assert.True(double.IsNaN(result[0, 0])); // First value is NaN
        Assert.Equal(1.0, result[1, 0], 10);     // (200-100)/100 = 1.0 = 100%
        Assert.Equal(1.0, result[2, 0], 10);     // (400-200)/200 = 1.0 = 100%
    }

    [Fact]
    public void DifferencingTransformer_LogDifference_ComputesCorrectly()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableDifferencing = true,
            EnabledDifferencingFeatures = DifferencingFeatures.LogDifference,
            WindowSizes = [3]
        };
        var transformer = new DifferencingTransformer<double>(options);

        // Exponential growth: e^0, e^1, e^2, e^3
        var data = new Tensor<double>(new[] { 4, 1 });
        for (int i = 0; i < 4; i++)
            data[i, 0] = Math.Exp(i);

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);
        Assert.True(double.IsNaN(result[0, 0]));
        // log(e^1) - log(e^0) = 1 - 0 = 1
        Assert.Equal(1.0, result[1, 0], 10);
        Assert.Equal(1.0, result[2, 0], 10);
        Assert.Equal(1.0, result[3, 0], 10);
    }

    [Fact]
    public void DifferencingTransformer_LinearDetrend_RemovesLinearTrend()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableDifferencing = true,
            EnabledDifferencingFeatures = DifferencingFeatures.LinearDetrend,
            WindowSizes = [3]
        };
        var transformer = new DifferencingTransformer<double>(options);

        // Linear trend with noise: y = 2x + 10 + noise
        var data = new Tensor<double>(new[] { 20, 1 });
        for (int i = 0; i < 20; i++)
            data[i, 0] = 2 * i + 10 + Math.Sin(i) * 2; // Small noise

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);

        // Detrended values should be near zero on average
        double sum = 0;
        int count = 0;
        for (int t = 0; t < 20; t++)
        {
            if (!double.IsNaN(result[t, 0]))
            {
                sum += result[t, 0];
                count++;
            }
        }
        double mean = sum / count;
        Assert.True(Math.Abs(mean) < 1.0, $"Mean of detrended data should be near zero, got {mean}");
    }

    [Fact]
    public void DifferencingTransformer_HodrickPrescott_SeparatesTrendAndCycle()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableDifferencing = true,
            EnabledDifferencingFeatures = DifferencingFeatures.HodrickPrescottFilter,
            HodrickPrescottLambda = 100,
            WindowSizes = [10]
        };
        var transformer = new DifferencingTransformer<double>(options);

        // Data with trend and cycle
        var data = new Tensor<double>(new[] { 50, 1 });
        for (int i = 0; i < 50; i++)
            data[i, 0] = i * 0.5 + Math.Sin(i * 0.5) * 5; // Linear trend + sine cycle

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);
        // HP filter outputs: trend, cycle = 2 features
        Assert.Equal(2, result.Shape[1]);

        // Verify both trend and cycle have valid values
        bool hasTrendValues = false;
        bool hasCycleValues = false;
        for (int t = 0; t < 50; t++)
        {
            if (!double.IsNaN(result[t, 0]))
                hasTrendValues = true;
            if (!double.IsNaN(result[t, 1]))
                hasCycleValues = true;
        }
        Assert.True(hasTrendValues, "HP filter should produce trend values");
        Assert.True(hasCycleValues, "HP filter should produce cycle values");

        // Trend + Cycle should approximately equal original
        for (int t = 0; t < 50; t++)
        {
            double reconstructed = result[t, 0] + result[t, 1];
            Assert.Equal(data[t, 0], reconstructed, 3);
        }
    }

    [Fact]
    public void DifferencingTransformer_StlDecomposition_ProducesThreeComponents()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableDifferencing = true,
            EnabledDifferencingFeatures = DifferencingFeatures.StlDecomposition,
            StlSeasonalPeriod = 7,
            StlRobustIterations = 2,
            WindowSizes = [7]
        };
        var transformer = new DifferencingTransformer<double>(options);

        // Create data with seasonal + trend + noise
        var data = new Tensor<double>(new[] { 35, 1 });
        for (int i = 0; i < 35; i++)
        {
            double seasonal = 10 * Math.Sin(2 * Math.PI * i / 7); // Weekly cycle
            double trend = i * 0.5;                               // Linear trend
            double noise = (i % 3 - 1) * 0.5;                     // Small noise
            data[i, 0] = seasonal + trend + noise;
        }

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);
        // STL outputs: seasonal, trend, residual = 3 features
        Assert.Equal(3, result.Shape[1]);

        // All components should sum to approximately the original
        for (int t = 7; t < 35; t++)
        {
            double reconstructed = result[t, 0] + result[t, 1] + result[t, 2];
            Assert.Equal(data[t, 0], reconstructed, 5);
        }
    }

    [Fact]
    public void DifferencingTransformer_AllFeatures_ProducesValidOutput()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableDifferencing = true,
            EnabledDifferencingFeatures = DifferencingFeatures.BasicDifferencing | DifferencingFeatures.Returns,
            SeasonalDifferencingPeriod = 7,
            WindowSizes = [7]
        };
        var transformer = new DifferencingTransformer<double>(options);

        var data = CreateStockPriceSeries(50);
        var result = transformer.FitTransform(data);

        Assert.NotNull(result);
        Assert.Equal(50, result.Shape[0]);

        // Should have multiple output features per input feature
        // BasicDifferencing (3) + Returns (2) = 5 per input feature, 4 OHLC features = 20 total
        Assert.True(result.Shape[1] >= 5, $"Expected at least 5 features, got {result.Shape[1]}");

        // Verify no infinite values
        for (int t = 10; t < 50; t++)
        {
            for (int f = 0; f < result.Shape[1]; f++)
            {
                Assert.False(double.IsInfinity(result[t, f]),
                    $"Value at t={t}, f={f} should not be infinite");
            }
        }
    }

    [Fact]
    public void DifferencingTransformer_MultipleFeatures_ComputesForAll()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableDifferencing = true,
            EnabledDifferencingFeatures = DifferencingFeatures.FirstDifference | DifferencingFeatures.PercentChange,
            WindowSizes = [3]
        };
        var transformer = new DifferencingTransformer<double>(options);

        // Two feature input
        var data = new Tensor<double>(new[] { 10, 2 });
        for (int i = 0; i < 10; i++)
        {
            data[i, 0] = (i + 1) * 10;  // 10, 20, 30, ...
            data[i, 1] = (i + 1) * 100; // 100, 200, 300, ...
        }

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);
        // 2 features * 2 output types = 4 output features
        Assert.Equal(4, result.Shape[1]);

        // Feature 0: first diff = 10, percent change = (20-10)/10 = 1.0
        Assert.Equal(10.0, result[1, 0], 10);
        Assert.Equal(1.0, result[1, 1], 10);

        // Feature 1: first diff = 100, percent change = (200-100)/100 = 1.0
        Assert.Equal(100.0, result[1, 2], 10);
        Assert.Equal(1.0, result[1, 3], 10);
    }

    #endregion

    #region RollingRegressionTransformer Tests

    [Fact]
    public void RollingRegressionTransformer_Construction_Succeeds()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableRollingRegression = true,
            EnabledRegressionFeatures = RollingRegressionFeatures.CAPMFeatures,
            WindowSizes = [20],
            BenchmarkColumnIndex = 0
        };
        var transformer = new RollingRegressionTransformer<double>(options);

        Assert.NotNull(transformer);
    }

    [Fact]
    public void RollingRegressionTransformer_Beta_MeasuresMarketSensitivity()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableRollingRegression = true,
            EnabledRegressionFeatures = RollingRegressionFeatures.Beta,
            WindowSizes = [10],
            BenchmarkColumnIndex = 0,
            AnnualizationFactor = 252
        };
        var transformer = new RollingRegressionTransformer<double>(options);

        // Create data: benchmark (col 0) and asset that moves 2x the benchmark (col 1)
        var data = new Tensor<double>(new[] { 30, 2 });
        double benchmarkPrice = 100.0;
        double assetPrice = 100.0;

        for (int t = 0; t < 30; t++)
        {
            double benchmarkReturn = Math.Sin(t * 0.3) * 0.02; // 2% swings
            benchmarkPrice *= (1 + benchmarkReturn);
            assetPrice *= (1 + benchmarkReturn * 2); // Asset moves 2x benchmark

            data[t, 0] = benchmarkPrice;
            data[t, 1] = assetPrice;
        }

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);
        Assert.Equal(30, result.Shape[0]);
        Assert.Equal(1, result.Shape[1]); // Only 1 non-benchmark feature, 1 measure

        // Beta should be approximately 2
        double beta = result[25, 0];
        Assert.False(double.IsNaN(beta), "Beta should not be NaN");
        Assert.True(beta > 1.5 && beta < 2.5, $"Beta should be ~2, got {beta}");
    }

    [Fact]
    public void RollingRegressionTransformer_SharpeRatio_PositiveForGoodReturns()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableRollingRegression = true,
            EnabledRegressionFeatures = RollingRegressionFeatures.SharpeRatio,
            WindowSizes = [20],
            BenchmarkColumnIndex = 0,
            RiskFreeRate = 0.02,
            AnnualizationFactor = 252
        };
        var transformer = new RollingRegressionTransformer<double>(options);

        // Create data: benchmark flat, asset with positive trend
        var data = new Tensor<double>(new[] { 50, 2 });
        for (int t = 0; t < 50; t++)
        {
            data[t, 0] = 100.0; // Flat benchmark
            data[t, 1] = 100.0 + t * 0.5 + Math.Sin(t * 0.2); // Positive trend with noise
        }

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);

        // Sharpe ratio should be positive for positive returns
        double sharpe = result[40, 0];
        Assert.False(double.IsNaN(sharpe), "Sharpe should not be NaN");
        Assert.True(sharpe > 0, $"Sharpe should be positive for upward trend, got {sharpe}");
    }

    [Fact]
    public void RollingRegressionTransformer_SortinoRatio_IgnoresUpsideVolatility()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableRollingRegression = true,
            EnabledRegressionFeatures = RollingRegressionFeatures.SortinoRatio,
            WindowSizes = [20],
            BenchmarkColumnIndex = 0,
            MinimumAcceptableReturn = 0.0,
            AnnualizationFactor = 252
        };
        var transformer = new RollingRegressionTransformer<double>(options);

        // Create data with mostly upside volatility (large gains, small losses)
        var data = new Tensor<double>(new[] { 50, 2 });
        double price = 100.0;
        for (int t = 0; t < 50; t++)
        {
            data[t, 0] = 100.0; // Flat benchmark
            double change = t % 3 == 0 ? 0.03 : 0.01; // Mostly gains
            price *= (1 + change);
            data[t, 1] = price;
        }

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);

        double sortino = result[40, 0];
        Assert.False(double.IsNaN(sortino), "Sortino should not be NaN");
        // Sortino should be high (or infinite) since there's little downside
        Assert.True(sortino >= 0, $"Sortino should be non-negative, got {sortino}");
    }

    [Fact]
    public void RollingRegressionTransformer_RSquared_HighForCorrelatedAssets()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableRollingRegression = true,
            EnabledRegressionFeatures = RollingRegressionFeatures.RSquared,
            WindowSizes = [15],
            BenchmarkColumnIndex = 0
        };
        var transformer = new RollingRegressionTransformer<double>(options);

        // Create highly correlated data
        var data = new Tensor<double>(new[] { 30, 2 });
        for (int t = 0; t < 30; t++)
        {
            double movement = Math.Sin(t * 0.3);
            data[t, 0] = 100 + movement * 10;  // Benchmark
            data[t, 1] = 100 + movement * 15;  // Asset moves with benchmark (scaled)
        }

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);

        // R should be very high (close to 1) for perfectly correlated data
        double rSquared = result[25, 0];
        Assert.False(double.IsNaN(rSquared), "R should not be NaN");
        Assert.True(rSquared > 0.9, $"R should be high for correlated assets, got {rSquared}");
    }

    [Fact]
    public void RollingRegressionTransformer_Alpha_MeasuresExcessReturn()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableRollingRegression = true,
            EnabledRegressionFeatures = RollingRegressionFeatures.Alpha,
            WindowSizes = [20],
            BenchmarkColumnIndex = 0,
            AnnualizationFactor = 252
        };
        var transformer = new RollingRegressionTransformer<double>(options);

        // Create data: asset consistently outperforms benchmark
        var data = new Tensor<double>(new[] { 50, 2 });
        double benchPrice = 100.0;
        double assetPrice = 100.0;

        for (int t = 0; t < 50; t++)
        {
            double benchReturn = 0.001; // 0.1% benchmark return
            double assetReturn = 0.002; // 0.2% asset return (extra alpha)
            benchPrice *= (1 + benchReturn);
            assetPrice *= (1 + assetReturn);
            data[t, 0] = benchPrice;
            data[t, 1] = assetPrice;
        }

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);

        // Alpha should be positive (annualized excess return)
        double alpha = result[40, 0];
        Assert.False(double.IsNaN(alpha), "Alpha should not be NaN");
        Assert.True(alpha > 0, $"Alpha should be positive for outperforming asset, got {alpha}");
    }

    [Fact]
    public void RollingRegressionTransformer_Correlation_DetectsRelationship()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableRollingRegression = true,
            EnabledRegressionFeatures = RollingRegressionFeatures.Correlation,
            WindowSizes = [15],
            BenchmarkColumnIndex = 0
        };
        var transformer = new RollingRegressionTransformer<double>(options);

        // Positive correlation
        var positiveData = new Tensor<double>(new[] { 30, 2 });
        for (int t = 0; t < 30; t++)
        {
            double movement = t * 0.5 + Math.Sin(t * 0.3) * 5;
            positiveData[t, 0] = 100 + movement;
            positiveData[t, 1] = 100 + movement * 1.5;
        }

        // Negative correlation
        var negativeData = new Tensor<double>(new[] { 30, 2 });
        for (int t = 0; t < 30; t++)
        {
            double movement = t * 0.5 + Math.Sin(t * 0.3) * 5;
            negativeData[t, 0] = 100 + movement;
            negativeData[t, 1] = 100 - movement * 0.8;  // Negative relationship
        }

        var positiveResult = transformer.FitTransform(positiveData);
        var negativeResult = transformer.FitTransform(negativeData);

        // Positive correlation should be close to 1
        double posCorr = positiveResult[25, 0];
        Assert.True(posCorr > 0.8, $"Positive correlation should be >0.8, got {posCorr}");

        // Negative correlation should be close to -1
        double negCorr = negativeResult[25, 0];
        Assert.True(negCorr < -0.5, $"Negative correlation should be <-0.5, got {negCorr}");
    }

    [Fact]
    public void RollingRegressionTransformer_TrackingError_MeasuresDeviation()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableRollingRegression = true,
            EnabledRegressionFeatures = RollingRegressionFeatures.TrackingError,
            WindowSizes = [20],
            BenchmarkColumnIndex = 0,
            AnnualizationFactor = 252
        };
        var transformer = new RollingRegressionTransformer<double>(options);

        // Asset that tracks benchmark closely
        var closeData = new Tensor<double>(new[] { 40, 2 });
        // Asset that deviates significantly
        var wideData = new Tensor<double>(new[] { 40, 2 });

        double price = 100.0;
        for (int t = 0; t < 40; t++)
        {
            double benchMove = Math.Sin(t * 0.2) * 2;
            closeData[t, 0] = price + benchMove;
            closeData[t, 1] = price + benchMove + (t % 2 == 0 ? 0.1 : -0.1); // Small deviation

            wideData[t, 0] = price + benchMove;
            wideData[t, 1] = price + benchMove + (t % 2 == 0 ? 3.0 : -3.0); // Large deviation
        }

        var closeResult = transformer.FitTransform(closeData);
        var wideResult = transformer.FitTransform(wideData);

        double closeTe = closeResult[35, 0];
        double wideTe = wideResult[35, 0];

        Assert.False(double.IsNaN(closeTe), "Close TE should not be NaN");
        Assert.False(double.IsNaN(wideTe), "Wide TE should not be NaN");
        Assert.True(wideTe > closeTe, $"Wide tracking error ({wideTe}) should exceed close ({closeTe})");
    }

    [Fact]
    public void RollingRegressionTransformer_InformationRatio_MeasuresConsistency()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableRollingRegression = true,
            EnabledRegressionFeatures = RollingRegressionFeatures.InformationRatio,
            WindowSizes = [20],
            BenchmarkColumnIndex = 0,
            AnnualizationFactor = 252
        };
        var transformer = new RollingRegressionTransformer<double>(options);

        // Asset with consistent outperformance (high IR)
        var data = new Tensor<double>(new[] { 50, 2 });
        double benchPrice = 100.0;
        double assetPrice = 100.0;

        for (int t = 0; t < 50; t++)
        {
            double benchReturn = 0.001 + Math.Sin(t * 0.2) * 0.005;
            double excessReturn = 0.0005; // Consistent small outperformance
            benchPrice *= (1 + benchReturn);
            assetPrice *= (1 + benchReturn + excessReturn);
            data[t, 0] = benchPrice;
            data[t, 1] = assetPrice;
        }

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);

        double ir = result[40, 0];
        Assert.False(double.IsNaN(ir), "IR should not be NaN");
        Assert.True(ir > 0, $"IR should be positive for consistent outperformance, got {ir}");
    }

    [Fact]
    public void RollingRegressionTransformer_AllFeatures_ProducesValidOutput()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableRollingRegression = true,
            EnabledRegressionFeatures = RollingRegressionFeatures.All,
            WindowSizes = [15],
            BenchmarkColumnIndex = 0,
            RiskFreeRate = 0.02,
            AnnualizationFactor = 252
        };
        var transformer = new RollingRegressionTransformer<double>(options);

        var data = CreateStockPriceSeries(50);
        var result = transformer.FitTransform(data);

        Assert.NotNull(result);
        Assert.Equal(50, result.Shape[0]);

        // 3 non-benchmark features * 8 regression measures = 24 outputs
        Assert.True(result.Shape[1] >= 8, $"Expected at least 8 features, got {result.Shape[1]}");

        // Verify reasonable values in later time steps
        int validCount = 0;
        for (int t = 20; t < 50; t++)
        {
            for (int f = 0; f < result.Shape[1]; f++)
            {
                if (!double.IsNaN(result[t, f]) && !double.IsInfinity(result[t, f]))
                    validCount++;
            }
        }

        Assert.True(validCount > 100, $"Should have many valid regression values, got {validCount}");
    }

    [Fact]
    public void RollingRegressionTransformer_ParallelEqualsSequential()
    {
        var baseOptions = new TimeSeriesFeatureOptions
        {
            EnableRollingRegression = true,
            EnabledRegressionFeatures = RollingRegressionFeatures.CAPMFeatures | RollingRegressionFeatures.SharpeRatio,
            WindowSizes = [15],
            BenchmarkColumnIndex = 0,
            RiskFreeRate = 0.02,
            AnnualizationFactor = 252
        };

        var serialOptions = new TimeSeriesFeatureOptions
        {
            EnableRollingRegression = baseOptions.EnableRollingRegression,
            EnabledRegressionFeatures = baseOptions.EnabledRegressionFeatures,
            WindowSizes = baseOptions.WindowSizes,
            BenchmarkColumnIndex = baseOptions.BenchmarkColumnIndex,
            RiskFreeRate = baseOptions.RiskFreeRate,
            AnnualizationFactor = baseOptions.AnnualizationFactor,
            UseParallelProcessing = false
        };

        var parallelOptions = new TimeSeriesFeatureOptions
        {
            EnableRollingRegression = baseOptions.EnableRollingRegression,
            EnabledRegressionFeatures = baseOptions.EnabledRegressionFeatures,
            WindowSizes = baseOptions.WindowSizes,
            BenchmarkColumnIndex = baseOptions.BenchmarkColumnIndex,
            RiskFreeRate = baseOptions.RiskFreeRate,
            AnnualizationFactor = baseOptions.AnnualizationFactor,
            UseParallelProcessing = true,
            ParallelThreshold = 10
        };

        var data = CreateStockPriceSeries(60);

        var serialTransformer = new RollingRegressionTransformer<double>(serialOptions);
        var parallelTransformer = new RollingRegressionTransformer<double>(parallelOptions);

        var serialResult = serialTransformer.FitTransform(data);
        var parallelResult = parallelTransformer.FitTransform(data);

        Assert.Equal(serialResult.Shape[0], parallelResult.Shape[0]);
        Assert.Equal(serialResult.Shape[1], parallelResult.Shape[1]);

        for (int t = 0; t < serialResult.Shape[0]; t++)
        {
            for (int f = 0; f < serialResult.Shape[1]; f++)
            {
                bool nan1 = double.IsNaN(serialResult[t, f]);
                bool nan2 = double.IsNaN(parallelResult[t, f]);
                Assert.Equal(nan1, nan2);
                if (!nan1 && !nan2)
                {
                    Assert.Equal(serialResult[t, f], parallelResult[t, f], 8);
                }
            }
        }
    }

    #endregion

    #region AnomalyFeaturesTransformer Tests

    [Fact]
    public void AnomalyFeaturesTransformer_Construction_Succeeds()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableAnomalyDetection = true,
            EnabledAnomalyFeatures = AnomalyFeatures.ZScoreFeatures,
            WindowSizes = [10]
        };
        var transformer = new AnomalyFeaturesTransformer<double>(options);

        Assert.NotNull(transformer);
    }

    [Fact]
    public void AnomalyFeaturesTransformer_ZScore_DetectsOutliers()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableAnomalyDetection = true,
            EnabledAnomalyFeatures = AnomalyFeatures.ZScore,
            WindowSizes = [10],
            ZScoreThreshold = 2.0
        };
        var transformer = new AnomalyFeaturesTransformer<double>(options);

        // Create data with an outlier at position 15
        var data = new Tensor<double>(new[] { 20, 1 });
        for (int t = 0; t < 20; t++)
        {
            data[t, 0] = t == 15 ? 100 : 10; // Normal=10, Outlier=100
        }

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);
        Assert.Equal(20, result.Shape[0]);

        // Z-score at outlier position should be high
        double outlierZScore = result[15, 0];
        Assert.True(Math.Abs(outlierZScore) > 2, $"Outlier Z-score should be > 2, got {outlierZScore}");

        // Z-scores at normal positions should be low
        double normalZScore = result[18, 0];
        Assert.True(Math.Abs(normalZScore) < 1, $"Normal Z-score should be < 1, got {normalZScore}");
    }

    [Fact]
    public void AnomalyFeaturesTransformer_ZScoreFlag_FlagsAnomalies()
    {
        // Note: With window size n, the maximum Z-score for a single extreme outlier
        // approaches sqrt(n-1) as the outlier becomes more extreme.
        // With n=10, max Z  3.0 (can't exceed threshold of 3.0)
        // With n=20, max Z  sqrt(19)  4.36 (can exceed threshold)
        var options = new TimeSeriesFeatureOptions
        {
            EnableAnomalyDetection = true,
            EnabledAnomalyFeatures = AnomalyFeatures.ZScoreFlag,
            WindowSizes = [20],
            ZScoreThreshold = 3.0
        };
        var transformer = new AnomalyFeaturesTransformer<double>(options);

        // Data with one extreme outlier - need larger dataset for window size 20
        var data = new Tensor<double>(new[] { 40, 1 });
        for (int t = 0; t < 40; t++)
        {
            data[t, 0] = t == 30 ? 10000 : 50 + (t % 5); // Extreme outlier at 30
        }

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);

        // Flag should be 1 at outlier position
        double outlierFlag = result[30, 0];
        Assert.Equal(1.0, outlierFlag, 5);

        // Flag should be 0 at normal positions (before the outlier enters window)
        double normalFlag = result[25, 0];
        Assert.Equal(0.0, normalFlag, 5);
    }

    [Fact]
    public void AnomalyFeaturesTransformer_ModifiedZScore_RobustToOutliers()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableAnomalyDetection = true,
            EnabledAnomalyFeatures = AnomalyFeatures.ModifiedZScore,
            WindowSizes = [15]
        };
        var transformer = new AnomalyFeaturesTransformer<double>(options);

        // Data with multiple outliers (modified Z-score should handle better)
        var data = new Tensor<double>(new[] { 30, 1 });
        for (int t = 0; t < 30; t++)
        {
            data[t, 0] = 100 + (t % 3); // Mostly normal values
        }
        data[25, 0] = 500; // Outlier

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);

        // Modified Z-score for outlier should be high
        double outlierModZ = result[25, 0];
        Assert.True(Math.Abs(outlierModZ) > 3, $"Outlier modified Z-score should be > 3, got {outlierModZ}");
    }

    [Fact]
    public void AnomalyFeaturesTransformer_IqrOutlierScore_IdentifiesExtremes()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableAnomalyDetection = true,
            EnabledAnomalyFeatures = AnomalyFeatures.IqrOutlierScore,
            WindowSizes = [15],
            IqrMultiplier = 1.5
        };
        var transformer = new AnomalyFeaturesTransformer<double>(options);

        // Create data: normal values 10-20, outlier at 100
        var data = new Tensor<double>(new[] { 30, 1 });
        for (int t = 0; t < 30; t++)
        {
            data[t, 0] = 10 + (t % 10);
        }
        data[25, 0] = 100; // Far outside IQR bounds

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);

        // IQR score at outlier should be positive
        double outlierScore = result[25, 0];
        Assert.True(outlierScore > 0, $"Outlier IQR score should be positive, got {outlierScore}");

        // IQR score at normal position should be 0
        double normalScore = result[20, 0];
        Assert.Equal(0.0, normalScore, 5);
    }

    [Fact]
    public void AnomalyFeaturesTransformer_IqrOutlierFlag_FlagsBothEnds()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableAnomalyDetection = true,
            EnabledAnomalyFeatures = AnomalyFeatures.IqrOutlierFlag,
            WindowSizes = [10],
            IqrMultiplier = 1.5
        };
        var transformer = new AnomalyFeaturesTransformer<double>(options);

        // Normal data with outliers at both ends
        var data = new Tensor<double>(new[] { 25, 1 });
        for (int t = 0; t < 25; t++)
        {
            data[t, 0] = 50;
        }
        data[15, 0] = 200;  // High outlier
        data[20, 0] = -100; // Low outlier

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);

        // Both outliers should be flagged
        Assert.Equal(1.0, result[15, 0], 5);
        Assert.Equal(1.0, result[20, 0], 5);
    }

    [Fact]
    public void AnomalyFeaturesTransformer_CusumStatistic_DetectsMeanShift()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableAnomalyDetection = true,
            EnabledAnomalyFeatures = AnomalyFeatures.CusumStatistic,
            WindowSizes = [20],
            CusumK = 0.5,
            CusumH = 4.0
        };
        var transformer = new AnomalyFeaturesTransformer<double>(options);

        // Data with mean shift at position 30
        var data = new Tensor<double>(new[] { 50, 1 });
        for (int t = 0; t < 30; t++)
            data[t, 0] = 100 + (t % 3 - 1) * 2; // Mean ~100
        for (int t = 30; t < 50; t++)
            data[t, 0] = 120 + (t % 3 - 1) * 2; // Mean ~120 (shift!)

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);

        // CUSUM should be higher after the mean shift
        double beforeShift = result[25, 0];
        double afterShift = result[45, 0];

        Assert.False(double.IsNaN(afterShift), "CUSUM after shift should not be NaN");
        // After mean shift, CUSUM accumulates deviations
    }

    [Fact]
    public void AnomalyFeaturesTransformer_IsolationScore_HigherForAnomalies()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableAnomalyDetection = true,
            EnabledAnomalyFeatures = AnomalyFeatures.IsolationScore,
            WindowSizes = [15]
        };
        var transformer = new AnomalyFeaturesTransformer<double>(options);

        // Data with clear outlier
        var data = new Tensor<double>(new[] { 30, 1 });
        for (int t = 0; t < 30; t++)
        {
            data[t, 0] = 50 + (t % 5); // Normal range 50-54
        }
        data[25, 0] = 200; // Extreme outlier

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);

        // Isolation score for outlier should be higher than normal points
        double outlierScore = result[25, 0];
        double normalScore = result[20, 0];

        Assert.False(double.IsNaN(outlierScore), "Isolation score should not be NaN");
        Assert.True(outlierScore >= 0.5, $"Outlier isolation score should be >= 0.5, got {outlierScore}");
    }

    [Fact]
    public void AnomalyFeaturesTransformer_PercentileRank_CorrectOrdering()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableAnomalyDetection = true,
            EnabledAnomalyFeatures = AnomalyFeatures.PercentileRank,
            WindowSizes = [10]
        };
        var transformer = new AnomalyFeaturesTransformer<double>(options);

        // Monotonically increasing data
        var data = new Tensor<double>(new[] { 20, 1 });
        for (int t = 0; t < 20; t++)
        {
            data[t, 0] = t * 10;
        }

        var result = transformer.FitTransform(data);

        Assert.NotNull(result);

        // Later values should have higher percentile ranks (more values below them)
        // At t=19, value is 190, which should be higher than most values in window
        double highRank = result[19, 0];
        Assert.True(highRank > 0.5, $"High value should have percentile rank > 0.5, got {highRank}");
    }

    [Fact]
    public void AnomalyFeaturesTransformer_AllFeatures_ProducesValidOutput()
    {
        var options = new TimeSeriesFeatureOptions
        {
            EnableAnomalyDetection = true,
            EnabledAnomalyFeatures = AnomalyFeatures.All,
            WindowSizes = [10],
            ZScoreThreshold = 3.0,
            IqrMultiplier = 1.5,
            CusumK = 0.5,
            CusumH = 4.0
        };
        var transformer = new AnomalyFeaturesTransformer<double>(options);

        var data = CreateStockPriceSeries(40);
        var result = transformer.FitTransform(data);

        Assert.NotNull(result);
        Assert.Equal(40, result.Shape[0]);

        // Should have 9 features (All) per input feature per window
        // With 4 OHLC features: 4 * 9 = 36 outputs
        Assert.True(result.Shape[1] >= 9, $"Expected at least 9 features, got {result.Shape[1]}");

        // Verify reasonable values
        int validCount = 0;
        for (int t = 15; t < 40; t++)
        {
            for (int f = 0; f < Math.Min(result.Shape[1], 9); f++)
            {
                if (!double.IsNaN(result[t, f]) && !double.IsInfinity(result[t, f]))
                    validCount++;
            }
        }

        Assert.True(validCount > 100, $"Should have many valid anomaly values, got {validCount}");
    }

    [Fact]
    public void AnomalyFeaturesTransformer_ParallelEqualsSequential()
    {
        var baseOptions = new TimeSeriesFeatureOptions
        {
            EnableAnomalyDetection = true,
            EnabledAnomalyFeatures = AnomalyFeatures.ZScoreFeatures | AnomalyFeatures.IqrFeatures,
            WindowSizes = [10],
            ZScoreThreshold = 3.0,
            IqrMultiplier = 1.5
        };

        var serialOptions = new TimeSeriesFeatureOptions
        {
            EnableAnomalyDetection = baseOptions.EnableAnomalyDetection,
            EnabledAnomalyFeatures = baseOptions.EnabledAnomalyFeatures,
            WindowSizes = baseOptions.WindowSizes,
            ZScoreThreshold = baseOptions.ZScoreThreshold,
            IqrMultiplier = baseOptions.IqrMultiplier,
            UseParallelProcessing = false
        };

        var parallelOptions = new TimeSeriesFeatureOptions
        {
            EnableAnomalyDetection = baseOptions.EnableAnomalyDetection,
            EnabledAnomalyFeatures = baseOptions.EnabledAnomalyFeatures,
            WindowSizes = baseOptions.WindowSizes,
            ZScoreThreshold = baseOptions.ZScoreThreshold,
            IqrMultiplier = baseOptions.IqrMultiplier,
            UseParallelProcessing = true,
            ParallelThreshold = 10
        };

        var data = CreateStockPriceSeries(50);

        var serialTransformer = new AnomalyFeaturesTransformer<double>(serialOptions);
        var parallelTransformer = new AnomalyFeaturesTransformer<double>(parallelOptions);

        var serialResult = serialTransformer.FitTransform(data);
        var parallelResult = parallelTransformer.FitTransform(data);

        Assert.Equal(serialResult.Shape[0], parallelResult.Shape[0]);
        Assert.Equal(serialResult.Shape[1], parallelResult.Shape[1]);

        for (int t = 0; t < serialResult.Shape[0]; t++)
        {
            for (int f = 0; f < serialResult.Shape[1]; f++)
            {
                bool nan1 = double.IsNaN(serialResult[t, f]);
                bool nan2 = double.IsNaN(parallelResult[t, f]);
                Assert.Equal(nan1, nan2);
                if (!nan1 && !nan2)
                {
                    Assert.Equal(serialResult[t, f], parallelResult[t, f], 10);
                }
            }
        }
    }

    #endregion

    #region TimeSeriesTransformerPipeline Tests

    [Fact]
    public void TimeSeriesTransformerPipeline_Construction_Succeeds()
    {
        var pipeline = new TimeSeriesTransformerPipeline<double>();

        Assert.NotNull(pipeline);
        Assert.Equal(0, pipeline.TransformerCount);
        Assert.False(pipeline.IsFitted);
    }

    [Fact]
    public void TimeSeriesTransformerPipeline_AddTransformer_Succeeds()
    {
        var statsOptions = new TimeSeriesFeatureOptions
        {
            WindowSizes = [5],
            EnabledStatistics = RollingStatistics.Mean
        };
        var statsTransformer = new RollingStatsTransformer<double>(statsOptions);

        var pipeline = new TimeSeriesTransformerPipeline<double>()
            .AddTransformer(statsTransformer);

        Assert.Equal(1, pipeline.TransformerCount);
    }

    [Fact]
    public void TimeSeriesTransformerPipeline_ParallelMode_CombinesTransformerOutputs()
    {
        // Create two transformers with different features
        var lagOptions = new TimeSeriesFeatureOptions
        {
            WindowSizes = [5],  // Explicitly set to avoid default [7, 14, 30]
            LagSteps = [1, 2]
        };
        var lagTransformer = new LagLeadTransformer<double>(lagOptions);

        var statsOptions = new TimeSeriesFeatureOptions
        {
            WindowSizes = [5],
            EnabledStatistics = RollingStatistics.Mean | RollingStatistics.StandardDeviation
        };
        var statsTransformer = new RollingStatsTransformer<double>(statsOptions);

        var pipeline = new TimeSeriesTransformerPipeline<double>(includeOriginalFeatures: false, PipelineMode.Parallel)
            .AddTransformer(lagTransformer)
            .AddTransformer(statsTransformer);

        var data = new Tensor<double>(new[] { 20, 1 });
        for (int t = 0; t < 20; t++)
        {
            data[t, 0] = 100 + t * 2;
        }

        var result = pipeline.FitTransform(data);

        Assert.NotNull(result);
        Assert.True(pipeline.IsFitted);
        // Output should have features from both transformers
        Assert.True(result.Shape[1] > 0);
        Assert.Equal(pipeline.OutputFeatureCount, result.Shape[1]);
    }

    [Fact]
    public void TimeSeriesTransformerPipeline_WithOriginalFeatures_IncludesOriginal()
    {
        var statsOptions = new TimeSeriesFeatureOptions
        {
            WindowSizes = [5],
            EnabledStatistics = RollingStatistics.Mean
        };
        var statsTransformer = new RollingStatsTransformer<double>(statsOptions);

        var pipeline = new TimeSeriesTransformerPipeline<double>(includeOriginalFeatures: true, PipelineMode.Parallel)
            .AddTransformer(statsTransformer);

        var data = new Tensor<double>(new[] { 15, 2 }); // 2 input features
        for (int t = 0; t < 15; t++)
        {
            data[t, 0] = 100 + t;
            data[t, 1] = 50 + t * 0.5;
        }

        var result = pipeline.FitTransform(data);

        Assert.NotNull(result);
        // Output should include original 2 features + transformed features
        Assert.True(result.Shape[1] > 2);
        // First 2 features should be original values
        // Note: With truncation, the output may have fewer rows
    }

    [Fact]
    public void TimeSeriesTransformerPipeline_SequentialMode_ChainsTransformers()
    {
        // In sequential mode, each transformer receives previous output
        // For simplicity, use a single transformer in sequential mode
        var options = new TimeSeriesFeatureOptions
        {
            WindowSizes = [3],
            EnabledStatistics = RollingStatistics.Mean
        };
        var transformer = new RollingStatsTransformer<double>(options);

        var pipeline = new TimeSeriesTransformerPipeline<double>(includeOriginalFeatures: true, PipelineMode.Sequential)
            .AddTransformer(transformer);

        var data = new Tensor<double>(new[] { 20, 1 });
        for (int t = 0; t < 20; t++)
        {
            data[t, 0] = 100 + t;
        }

        var result = pipeline.FitTransform(data);

        Assert.NotNull(result);
        Assert.True(pipeline.IsFitted);
        Assert.True(result.Shape[0] > 0);
        Assert.True(result.Shape[1] > 0);
    }

    [Fact]
    public void TimeSeriesTransformerPipeline_GetFeatureNamesOut_ReturnsAllNames()
    {
        var options1 = new TimeSeriesFeatureOptions
        {
            WindowSizes = [5],
            EnabledStatistics = RollingStatistics.Mean
        };
        var transformer1 = new RollingStatsTransformer<double>(options1);

        var options2 = new TimeSeriesFeatureOptions
        {
            WindowSizes = [3],
            EnabledStatistics = RollingStatistics.Min | RollingStatistics.Max
        };
        var transformer2 = new RollingStatsTransformer<double>(options2);

        var pipeline = new TimeSeriesTransformerPipeline<double>(includeOriginalFeatures: false, PipelineMode.Parallel)
            .AddTransformer(transformer1)
            .AddTransformer(transformer2);

        var data = new Tensor<double>(new[] { 15, 1 });
        for (int t = 0; t < 15; t++)
            data[t, 0] = t;

        pipeline.Fit(data);

        var featureNames = pipeline.GetFeatureNamesOut();

        Assert.NotNull(featureNames);
        Assert.Equal(pipeline.OutputFeatureCount, featureNames.Length);
        Assert.True(featureNames.Length > 0);
    }

    [Fact]
    public void TimeSeriesTransformerPipeline_CannotAddAfterFitting()
    {
        var options = new TimeSeriesFeatureOptions
        {
            WindowSizes = [5],
            EnabledStatistics = RollingStatistics.Mean
        };
        var transformer = new RollingStatsTransformer<double>(options);

        var pipeline = new TimeSeriesTransformerPipeline<double>()
            .AddTransformer(transformer);

        var data = new Tensor<double>(new[] { 10, 1 });
        for (int t = 0; t < 10; t++)
            data[t, 0] = t;

        pipeline.Fit(data);

        Assert.Throws<InvalidOperationException>(() =>
            pipeline.AddTransformer(new RollingStatsTransformer<double>(options)));
    }

    [Fact]
    public void TimeSeriesTransformerPipeline_EmptyPipeline_ThrowsOnFit()
    {
        var pipeline = new TimeSeriesTransformerPipeline<double>();

        var data = new Tensor<double>(new[] { 10, 1 });
        for (int t = 0; t < 10; t++)
            data[t, 0] = t;

        Assert.Throws<InvalidOperationException>(() => pipeline.Fit(data));
    }

    [Fact]
    public void TimeSeriesTransformerPipeline_GetSummary_ReturnsDescription()
    {
        var options = new TimeSeriesFeatureOptions
        {
            WindowSizes = [5, 10],
            EnabledStatistics = RollingStatistics.Mean | RollingStatistics.StandardDeviation
        };
        var transformer = new RollingStatsTransformer<double>(options);

        var pipeline = new TimeSeriesTransformerPipeline<double>()
            .AddTransformer(transformer);

        var summary = pipeline.GetSummary();

        Assert.NotNull(summary);
        Assert.Contains("TimeSeriesTransformerPipeline", summary);
        Assert.Contains("RollingStatsTransformer", summary);
    }

    [Fact]
    public void TimeSeriesTransformerPipeline_Clone_CreatesNewPipeline()
    {
        var options = new TimeSeriesFeatureOptions
        {
            WindowSizes = [5],
            EnabledStatistics = RollingStatistics.Mean
        };
        var transformer = new RollingStatsTransformer<double>(options);

        var pipeline = new TimeSeriesTransformerPipeline<double>()
            .AddTransformer(transformer);

        var clone = pipeline.Clone();

        Assert.NotSame(pipeline, clone);
        Assert.Equal(pipeline.TransformerCount, clone.TransformerCount);
    }

    [Fact]
    public void TimeSeriesTransformerPipeline_MultipleFeatures_HandlesCorrectly()
    {
        var options = new TimeSeriesFeatureOptions
        {
            WindowSizes = [5],
            EnabledStatistics = RollingStatistics.Mean | RollingStatistics.StandardDeviation
        };
        var transformer = new RollingStatsTransformer<double>(options);

        var pipeline = new TimeSeriesTransformerPipeline<double>(includeOriginalFeatures: false, PipelineMode.Parallel)
            .AddTransformer(transformer);

        // 3 input features
        var data = new Tensor<double>(new[] { 20, 3 });
        for (int t = 0; t < 20; t++)
        {
            data[t, 0] = 100 + t;
            data[t, 1] = 200 - t * 0.5;
            data[t, 2] = Math.Sin(t * 0.5) * 50;
        }

        var result = pipeline.FitTransform(data);

        Assert.NotNull(result);
        // The output should have multiple features (exact count depends on transformer implementation)
        Assert.True(result.Shape[1] > 0);
        // Output feature count should match pipeline's reported feature count
        Assert.Equal(pipeline.OutputFeatureCount, result.Shape[1]);
    }

    #endregion

    #region Incremental/Streaming Tests

    [Fact]
    public void RollingStatsTransformer_SupportsIncrementalTransform()
    {
        var options = new TimeSeriesFeatureOptions
        {
            WindowSizes = [5],
            EnabledStatistics = RollingStatistics.Mean
        };
        var transformer = new RollingStatsTransformer<double>(options);

        // Fit the transformer
        var data = new Tensor<double>(new[] { 20, 1 });
        for (int t = 0; t < 20; t++)
            data[t, 0] = t;

        transformer.Fit(data);

        Assert.True(transformer.SupportsIncrementalTransform);
    }

    [Fact]
    public void RollingStatsTransformer_InitializeIncremental_Succeeds()
    {
        var options = new TimeSeriesFeatureOptions
        {
            WindowSizes = [5],
            EnabledStatistics = RollingStatistics.Mean
        };
        var transformer = new RollingStatsTransformer<double>(options);

        var data = new Tensor<double>(new[] { 20, 1 });
        for (int t = 0; t < 20; t++)
            data[t, 0] = t;

        transformer.Fit(data);
        transformer.InitializeIncremental(data);

        var state = transformer.GetIncrementalState();
        Assert.NotNull(state);
        Assert.True(state.BufferFilled);
        Assert.Equal(20, state.PointsProcessed);
    }

    [Fact]
    public void RollingStatsTransformer_TransformIncremental_ProducesFeatures()
    {
        var options = new TimeSeriesFeatureOptions
        {
            WindowSizes = [5],
            EnabledStatistics = RollingStatistics.Mean
        };
        var transformer = new RollingStatsTransformer<double>(options);

        var data = new Tensor<double>(new[] { 20, 1 });
        for (int t = 0; t < 20; t++)
            data[t, 0] = t;

        transformer.Fit(data);
        transformer.InitializeIncremental(data);

        // Process a new data point
        var newPoint = new double[] { 100.0 };
        var features = transformer.TransformIncremental(newPoint);

        Assert.NotNull(features);
        Assert.True(features.Length > 0);
    }

    [Fact]
    public void RollingStatsTransformer_TransformIncremental_UpdatesState()
    {
        var options = new TimeSeriesFeatureOptions
        {
            WindowSizes = [5],
            EnabledStatistics = RollingStatistics.Mean
        };
        var transformer = new RollingStatsTransformer<double>(options);

        var data = new Tensor<double>(new[] { 20, 1 });
        for (int t = 0; t < 20; t++)
            data[t, 0] = t;

        transformer.Fit(data);
        transformer.InitializeIncremental(data);

        var stateBefore = transformer.GetIncrementalState();
        long pointsBefore = stateBefore!.PointsProcessed;

        // Process several new data points
        for (int i = 0; i < 5; i++)
        {
            transformer.TransformIncremental([100.0 + i]);
        }

        var stateAfter = transformer.GetIncrementalState();
        Assert.Equal(pointsBefore + 5, stateAfter!.PointsProcessed);
    }

    [Fact]
    public void TimeSeriesTransformerPipeline_SupportsIncrementalTransform()
    {
        var options = new TimeSeriesFeatureOptions
        {
            WindowSizes = [5],
            EnabledStatistics = RollingStatistics.Mean
        };
        var transformer = new RollingStatsTransformer<double>(options);

        var pipeline = new TimeSeriesTransformerPipeline<double>(includeOriginalFeatures: true, PipelineMode.Parallel)
            .AddTransformer(transformer);

        var data = new Tensor<double>(new[] { 20, 1 });
        for (int t = 0; t < 20; t++)
            data[t, 0] = t;

        pipeline.Fit(data);

        Assert.True(pipeline.SupportsIncrementalTransform);
    }

    [Fact]
    public void TimeSeriesTransformerPipeline_TransformIncremental_ProducesFeatures()
    {
        var options = new TimeSeriesFeatureOptions
        {
            WindowSizes = [5],
            EnabledStatistics = RollingStatistics.Mean
        };
        var transformer = new RollingStatsTransformer<double>(options);

        var pipeline = new TimeSeriesTransformerPipeline<double>(includeOriginalFeatures: true, PipelineMode.Parallel)
            .AddTransformer(transformer);

        var data = new Tensor<double>(new[] { 20, 1 });
        for (int t = 0; t < 20; t++)
            data[t, 0] = t;

        pipeline.Fit(data);
        pipeline.InitializeIncremental(data);

        var features = pipeline.TransformIncremental([100.0]);

        Assert.NotNull(features);
        // Should have original feature (1) + transformer features
        Assert.True(features.Length > 1);
    }

    [Fact]
    public void RollingStatsTransformer_Incremental_MatchesBatch()
    {
        // Setup
        var options = new TimeSeriesFeatureOptions
        {
            WindowSizes = [5],
            EnabledStatistics = RollingStatistics.Mean | RollingStatistics.StandardDeviation | RollingStatistics.Min | RollingStatistics.Max
        };
        var transformer = new RollingStatsTransformer<double>(options);

        // Create data with known values
        const int totalPoints = 30;
        const int historicalPoints = 25;
        var fullData = new Tensor<double>(new[] { totalPoints, 1 });
        for (int t = 0; t < totalPoints; t++)
            fullData[t, 0] = 10.0 + t * 2.0 + Math.Sin(t * 0.5);

        // Batch: transform all data
        transformer.Fit(fullData);
        var batchResult = transformer.Transform(fullData);

        // Incremental: initialize with historical, then process remaining
        var historicalData = new Tensor<double>(new[] { historicalPoints, 1 });
        for (int t = 0; t < historicalPoints; t++)
            historicalData[t, 0] = fullData[t, 0];

        transformer.InitializeIncremental(historicalData);

        // Process remaining points incrementally and compare with batch
        for (int t = historicalPoints; t < totalPoints; t++)
        {
            var incrementalResult = transformer.TransformIncremental([fullData[t, 0]]);

            // Compare each feature
            for (int f = 0; f < incrementalResult.Length; f++)
            {
                var batchValue = batchResult[t, f];
                var incrementalValue = incrementalResult[f];

                // Skip NaN comparisons (edge cases)
                if (double.IsNaN(batchValue) && double.IsNaN(incrementalValue))
                    continue;

                Assert.True(
                    Math.Abs(batchValue - incrementalValue) < 1e-8,
                    $"Mismatch at t={t}, f={f}: batch={batchValue}, incremental={incrementalValue}");
            }
        }
    }

    [Fact]
    public void RollingVolatilityTransformer_Incremental_MatchesBatch()
    {
        // Setup
        var options = new TimeSeriesFeatureOptions
        {
            WindowSizes = [5],
            EnableVolatility = true,
            EnabledVolatilityMeasures = VolatilityMeasures.RealizedVolatility
        };
        var transformer = new RollingVolatilityTransformer<double>(options);

        // Create price-like data with returns
        const int totalPoints = 30;
        const int historicalPoints = 25;
        var fullData = new Tensor<double>(new[] { totalPoints, 1 });
        double price = 100.0;
        for (int t = 0; t < totalPoints; t++)
        {
            price *= 1.0 + 0.02 * Math.Sin(t * 0.3);
            fullData[t, 0] = price;
        }

        // Batch: transform all data
        transformer.Fit(fullData);
        var batchResult = transformer.Transform(fullData);

        // Incremental: initialize with historical, then process remaining
        var historicalData = new Tensor<double>(new[] { historicalPoints, 1 });
        for (int t = 0; t < historicalPoints; t++)
            historicalData[t, 0] = fullData[t, 0];

        transformer.InitializeIncremental(historicalData);

        // Process remaining points incrementally and compare with batch
        for (int t = historicalPoints; t < totalPoints; t++)
        {
            var incrementalResult = transformer.TransformIncremental([fullData[t, 0]]);

            for (int f = 0; f < incrementalResult.Length; f++)
            {
                var batchValue = batchResult[t, f];
                var incrementalValue = incrementalResult[f];

                if (double.IsNaN(batchValue) && double.IsNaN(incrementalValue))
                    continue;

                Assert.True(
                    Math.Abs(batchValue - incrementalValue) < 1e-8,
                    $"Mismatch at t={t}, f={f}: batch={batchValue}, incremental={incrementalValue}");
            }
        }
    }

    [Fact]
    public void AnomalyFeaturesTransformer_Incremental_MatchesBatch()
    {
        // Setup
        var options = new TimeSeriesFeatureOptions
        {
            WindowSizes = [5],
            ZScoreThreshold = 2.0,
            IqrMultiplier = 1.5
        };
        var transformer = new AnomalyFeaturesTransformer<double>(options);

        // Create data with some outliers
        const int totalPoints = 30;
        const int historicalPoints = 25;
        var fullData = new Tensor<double>(new[] { totalPoints, 1 });
        for (int t = 0; t < totalPoints; t++)
        {
            fullData[t, 0] = 50.0 + t;
            // Add outlier
            if (t == 27)
                fullData[t, 0] = 200.0;
        }

        // Batch: transform all data
        transformer.Fit(fullData);
        var batchResult = transformer.Transform(fullData);

        // Incremental: initialize with historical, then process remaining
        var historicalData = new Tensor<double>(new[] { historicalPoints, 1 });
        for (int t = 0; t < historicalPoints; t++)
            historicalData[t, 0] = fullData[t, 0];

        transformer.InitializeIncremental(historicalData);

        // Process remaining points incrementally and compare with batch
        for (int t = historicalPoints; t < totalPoints; t++)
        {
            var incrementalResult = transformer.TransformIncremental([fullData[t, 0]]);

            for (int f = 0; f < incrementalResult.Length; f++)
            {
                var batchValue = batchResult[t, f];
                var incrementalValue = incrementalResult[f];

                if (double.IsNaN(batchValue) && double.IsNaN(incrementalValue))
                    continue;

                Assert.True(
                    Math.Abs(batchValue - incrementalValue) < 1e-8,
                    $"Mismatch at t={t}, f={f}: batch={batchValue}, incremental={incrementalValue}");
            }
        }
    }

    [Fact]
    public void LagLeadTransformer_Incremental_MatchesBatch()
    {
        // Setup - explicitly set WindowSizes to smaller values for this test
        var options = new TimeSeriesFeatureOptions
        {
            WindowSizes = [5],  // Override defaults to avoid minimum data requirements
            LagSteps = [1, 2, 3],
            LeadSteps = [1]  // Leads will be NaN in incremental (no future data)
        };
        var transformer = new LagLeadTransformer<double>(options);

        // Create data
        const int totalPoints = 20;
        const int historicalPoints = 15;
        var fullData = new Tensor<double>(new[] { totalPoints, 1 });
        for (int t = 0; t < totalPoints; t++)
            fullData[t, 0] = t * 10.0;

        // Batch: transform all data
        transformer.Fit(fullData);
        var batchResult = transformer.Transform(fullData);

        // Incremental: initialize with historical, then process remaining
        var historicalData = new Tensor<double>(new[] { historicalPoints, 1 });
        for (int t = 0; t < historicalPoints; t++)
            historicalData[t, 0] = fullData[t, 0];

        transformer.InitializeIncremental(historicalData);

        // Process remaining points incrementally and compare lag features only
        // (Lead features will be NaN in incremental - no future data available)
        int numLagFeatures = options.LagSteps.Length;  // Only lag features can match

        for (int t = historicalPoints; t < totalPoints; t++)
        {
            var incrementalResult = transformer.TransformIncremental([fullData[t, 0]]);

            // Compare lag features only
            for (int f = 0; f < numLagFeatures; f++)
            {
                var batchValue = batchResult[t, f];
                var incrementalValue = incrementalResult[f];

                if (double.IsNaN(batchValue) && double.IsNaN(incrementalValue))
                    continue;

                Assert.True(
                    Math.Abs(batchValue - incrementalValue) < 1e-10,
                    $"Mismatch at t={t}, lag f={f}: batch={batchValue}, incremental={incrementalValue}");
            }

            // Verify lead features are NaN in incremental (no future data)
            for (int f = numLagFeatures; f < incrementalResult.Length; f++)
            {
                Assert.True(double.IsNaN(incrementalResult[f]),
                    $"Lead feature at t={t}, f={f} should be NaN in incremental mode");
            }
        }
    }

    [Fact]
    public void SeasonalityTransformer_Incremental_MatchesBatch()
    {
        // Setup - time features only (don't require windows)
        var options = new TimeSeriesFeatureOptions
        {
            WindowSizes = [5],  // Override defaults
            SeasonalPeriods = [7],  // Smaller seasonal period
            EnabledSeasonalityFeatures = SeasonalityFeatures.TimeFeatures,
            TimeSeriesStartDate = new DateTime(2024, 1, 1),
            TimeSeriesInterval = TimeSpan.FromDays(1)
        };
        var transformer = new SeasonalityTransformer<double>(options);

        // Create daily data (larger to accommodate windows)
        const int totalPoints = 50;
        const int historicalPoints = 40;
        var fullData = new Tensor<double>(new[] { totalPoints, 1 });
        for (int t = 0; t < totalPoints; t++)
            fullData[t, 0] = 100.0 + t;

        // Batch: transform all data
        transformer.Fit(fullData);
        var batchResult = transformer.Transform(fullData);

        // Incremental: initialize with historical, then process remaining
        var historicalData = new Tensor<double>(new[] { historicalPoints, 1 });
        for (int t = 0; t < historicalPoints; t++)
            historicalData[t, 0] = fullData[t, 0];

        transformer.InitializeIncremental(historicalData);

        // Process remaining points incrementally and compare with batch
        for (int t = historicalPoints; t < totalPoints; t++)
        {
            var incrementalResult = transformer.TransformIncremental([fullData[t, 0]]);

            for (int f = 0; f < incrementalResult.Length; f++)
            {
                var batchValue = batchResult[t, f];
                var incrementalValue = incrementalResult[f];

                if (double.IsNaN(batchValue) && double.IsNaN(incrementalValue))
                    continue;

                Assert.True(
                    Math.Abs(batchValue - incrementalValue) < 1e-8,
                    $"Mismatch at t={t}, f={f}: batch={batchValue}, incremental={incrementalValue}");
            }
        }
    }

    [Fact]
    public void DifferencingTransformer_Incremental_MatchesBatch()
    {
        // Setup - simple differencing (supported incrementally)
        var options = new TimeSeriesFeatureOptions
        {
            WindowSizes = [5],  // Override defaults
            EnableDifferencing = true,
            DifferencingOrder = 2,  // Includes both first and second order
            EnabledDifferencingFeatures = DifferencingFeatures.FirstDifference | DifferencingFeatures.SecondDifference,
            SeasonalDifferencingPeriod = 0  // No seasonal differencing
        };
        var transformer = new DifferencingTransformer<double>(options);

        // Create data (larger to accommodate window requirements)
        const int totalPoints = 50;
        const int historicalPoints = 40;
        var fullData = new Tensor<double>(new[] { totalPoints, 1 });
        for (int t = 0; t < totalPoints; t++)
            fullData[t, 0] = 100.0 + t * 2.0 + t * t * 0.1;  // Quadratic trend

        // Batch: transform all data
        transformer.Fit(fullData);
        var batchResult = transformer.Transform(fullData);

        // Incremental: initialize with historical, then process remaining
        var historicalData = new Tensor<double>(new[] { historicalPoints, 1 });
        for (int t = 0; t < historicalPoints; t++)
            historicalData[t, 0] = fullData[t, 0];

        transformer.InitializeIncremental(historicalData);

        // Process remaining points incrementally and compare with batch
        for (int t = historicalPoints; t < totalPoints; t++)
        {
            var incrementalResult = transformer.TransformIncremental([fullData[t, 0]]);

            for (int f = 0; f < incrementalResult.Length; f++)
            {
                var batchValue = batchResult[t, f];
                var incrementalValue = incrementalResult[f];

                // Skip NaN (complex features not supported incrementally)
                if (double.IsNaN(incrementalValue))
                    continue;

                if (double.IsNaN(batchValue) && double.IsNaN(incrementalValue))
                    continue;

                Assert.True(
                    Math.Abs(batchValue - incrementalValue) < 1e-8,
                    $"Mismatch at t={t}, f={f}: batch={batchValue}, incremental={incrementalValue}");
            }
        }
    }

    [Fact]
    public void RollingRegressionTransformer_IncrementalNotSupported_ThrowsException()
    {
        var options = new TimeSeriesFeatureOptions
        {
            WindowSizes = [10]
        };
        var transformer = new RollingRegressionTransformer<double>(options);

        var data = new Tensor<double>(new[] { 20, 2 });
        for (int t = 0; t < 20; t++)
        {
            data[t, 0] = t;
            data[t, 1] = t * 2;
        }

        transformer.Fit(data);

        // Should throw because incremental is not supported
        Assert.False(transformer.SupportsIncrementalTransform);
        Assert.Throws<NotSupportedException>(() => transformer.TransformIncremental([1.0, 2.0]));
    }

    [Fact]
    public void RollingCorrelationTransformer_IncrementalNotSupported_ThrowsException()
    {
        var options = new TimeSeriesFeatureOptions
        {
            WindowSizes = [10]
        };
        var transformer = new RollingCorrelationTransformer<double>(options);

        var data = new Tensor<double>(new[] { 20, 2 });
        for (int t = 0; t < 20; t++)
        {
            data[t, 0] = t;
            data[t, 1] = t * 2;
        }

        transformer.Fit(data);

        // Should throw because incremental is not supported
        Assert.False(transformer.SupportsIncrementalTransform);
        Assert.Throws<NotSupportedException>(() => transformer.TransformIncremental([1.0, 2.0]));
    }

    [Fact]
    public void TechnicalIndicatorsTransformer_IncrementalNotSupported_ThrowsException()
    {
        var options = new TimeSeriesFeatureOptions
        {
            WindowSizes = [10],
            EnableTechnicalIndicators = true
        };
        var transformer = new TechnicalIndicatorsTransformer<double>(options);

        var data = new Tensor<double>(new[] { 30, 4 });  // OHLC data
        for (int t = 0; t < 30; t++)
        {
            data[t, 0] = 100 + t;        // Open
            data[t, 1] = 101 + t;        // High
            data[t, 2] = 99 + t;         // Low
            data[t, 3] = 100.5 + t;      // Close
        }

        transformer.Fit(data);

        // Should throw because incremental is not supported
        Assert.False(transformer.SupportsIncrementalTransform);
        Assert.Throws<NotSupportedException>(() => transformer.TransformIncremental([100.0, 101.0, 99.0, 100.5]));
    }

    #endregion

    #region Serialization Tests

    [Fact]
    public void RollingStatsTransformer_ExportState_ReturnsFittedState()
    {
        var options = new TimeSeriesFeatureOptions
        {
            WindowSizes = [5, 10],
            EnabledStatistics = RollingStatistics.Mean | RollingStatistics.StandardDeviation
        };
        var transformer = new RollingStatsTransformer<double>(options);

        var data = new Tensor<double>(new[] { 20, 2 });
        for (int t = 0; t < 20; t++)
        {
            data[t, 0] = t;
            data[t, 1] = t * 2;
        }

        transformer.Fit(data);

        var state = transformer.ExportState();

        Assert.NotNull(state);
        Assert.True(state.IsFitted);
        Assert.Equal(2, state.InputFeatureCount);
        Assert.True(state.OutputFeatureCount > 0);
        Assert.Contains("RollingStatsTransformer", state.TransformerType);
    }

    [Fact]
    public void RollingStatsTransformer_ImportState_RestoresTransformer()
    {
        var options = new TimeSeriesFeatureOptions
        {
            WindowSizes = [5],
            EnabledStatistics = RollingStatistics.Mean
        };

        // Create and fit original transformer
        var original = new RollingStatsTransformer<double>(options);
        var data = new Tensor<double>(new[] { 20, 1 });
        for (int t = 0; t < 20; t++)
            data[t, 0] = t;

        original.Fit(data);
        var originalResult = original.Transform(data);

        // Export state
        var state = original.ExportState();

        // Create new transformer and import state
        var restored = new RollingStatsTransformer<double>(options);
        restored.ImportState(state);

        // Verify restored transformer produces same results
        Assert.True(restored.IsFitted);
        Assert.Equal(original.InputFeatureCount, restored.InputFeatureCount);
        Assert.Equal(original.OutputFeatureCount, restored.OutputFeatureCount);

        var restoredResult = restored.Transform(data);

        Assert.Equal(originalResult.Shape[0], restoredResult.Shape[0]);
        Assert.Equal(originalResult.Shape[1], restoredResult.Shape[1]);
    }

    [Fact]
    public void RollingStatsTransformer_ExportState_IncludesIncrementalState()
    {
        var options = new TimeSeriesFeatureOptions
        {
            WindowSizes = [5],
            EnabledStatistics = RollingStatistics.Mean
        };
        var transformer = new RollingStatsTransformer<double>(options);

        var data = new Tensor<double>(new[] { 20, 1 });
        for (int t = 0; t < 20; t++)
            data[t, 0] = t;

        transformer.Fit(data);
        transformer.InitializeIncremental(data);

        var state = transformer.ExportState();

        Assert.NotNull(state.IncrementalState);
        Assert.True(state.IncrementalState.BufferFilled);
    }

    [Fact]
    public void TransformerState_ContainsOptions()
    {
        var options = new TimeSeriesFeatureOptions
        {
            WindowSizes = [5, 10],
            EnabledStatistics = RollingStatistics.Mean,
            UseParallelProcessing = true
        };
        var transformer = new RollingStatsTransformer<double>(options);

        var data = new Tensor<double>(new[] { 20, 1 });
        for (int t = 0; t < 20; t++)
            data[t, 0] = t;

        transformer.Fit(data);

        var state = transformer.ExportState();

        Assert.NotNull(state.Options);
        Assert.True(state.Options.Count > 0);
        Assert.True(state.Options.ContainsKey("WindowSizes"));
    }

    [Fact]
    public void RollingStatsTransformer_RoundTrip_ProducesIdenticalOutput()
    {
        // Setup
        var options = new TimeSeriesFeatureOptions
        {
            WindowSizes = [5, 10],
            EnabledStatistics = RollingStatistics.Mean | RollingStatistics.StandardDeviation | RollingStatistics.Min | RollingStatistics.Max
        };

        // Create and fit original transformer
        var original = new RollingStatsTransformer<double>(options);
        var data = new Tensor<double>(new[] { 30, 2 });
        for (int t = 0; t < 30; t++)
        {
            data[t, 0] = t * 10.0 + Math.Sin(t * 0.5);
            data[t, 1] = t * 5.0 + Math.Cos(t * 0.3);
        }

        original.Fit(data);
        var originalResult = original.Transform(data);

        // Export state
        var state = original.ExportState();

        // Create new transformer and import state
        var restored = new RollingStatsTransformer<double>(options);
        restored.ImportState(state);

        // Transform with restored transformer
        var restoredResult = restored.Transform(data);

        // Verify exact output match
        Assert.Equal(originalResult.Shape[0], restoredResult.Shape[0]);
        Assert.Equal(originalResult.Shape[1], restoredResult.Shape[1]);

        for (int t = 0; t < originalResult.Shape[0]; t++)
        {
            for (int f = 0; f < originalResult.Shape[1]; f++)
            {
                var origVal = originalResult[t, f];
                var restVal = restoredResult[t, f];

                if (double.IsNaN(origVal) && double.IsNaN(restVal))
                    continue;

                Assert.True(
                    Math.Abs(origVal - restVal) < 1e-10,
                    $"Mismatch at t={t}, f={f}: original={origVal}, restored={restVal}");
            }
        }
    }

    [Fact]
    public void RollingVolatilityTransformer_RoundTrip_ProducesIdenticalOutput()
    {
        // Setup
        var options = new TimeSeriesFeatureOptions
        {
            WindowSizes = [5],
            EnableVolatility = true,
            EnabledVolatilityMeasures = VolatilityMeasures.RealizedVolatility
        };

        // Create and fit original transformer
        var original = new RollingVolatilityTransformer<double>(options);
        var data = new Tensor<double>(new[] { 30, 1 });
        double price = 100.0;
        for (int t = 0; t < 30; t++)
        {
            price *= 1.0 + 0.02 * Math.Sin(t * 0.3);
            data[t, 0] = price;
        }

        original.Fit(data);
        var originalResult = original.Transform(data);

        // Export state
        var state = original.ExportState();

        // Create new transformer and import state
        var restored = new RollingVolatilityTransformer<double>(options);
        restored.ImportState(state);

        // Transform with restored transformer
        var restoredResult = restored.Transform(data);

        // Verify exact output match
        Assert.Equal(originalResult.Shape[0], restoredResult.Shape[0]);
        Assert.Equal(originalResult.Shape[1], restoredResult.Shape[1]);

        for (int t = 0; t < originalResult.Shape[0]; t++)
        {
            for (int f = 0; f < originalResult.Shape[1]; f++)
            {
                var origVal = originalResult[t, f];
                var restVal = restoredResult[t, f];

                if (double.IsNaN(origVal) && double.IsNaN(restVal))
                    continue;

                Assert.True(
                    Math.Abs(origVal - restVal) < 1e-10,
                    $"Mismatch at t={t}, f={f}: original={origVal}, restored={restVal}");
            }
        }
    }

    [Fact]
    public void LagLeadTransformer_RoundTrip_ProducesIdenticalOutput()
    {
        // Setup
        var options = new TimeSeriesFeatureOptions
        {
            WindowSizes = [5],
            LagSteps = [1, 2, 3],
            LeadSteps = [1]
        };

        // Create and fit original transformer
        var original = new LagLeadTransformer<double>(options);
        var data = new Tensor<double>(new[] { 20, 1 });
        for (int t = 0; t < 20; t++)
            data[t, 0] = t * 10.0;

        original.Fit(data);
        var originalResult = original.Transform(data);

        // Export state
        var state = original.ExportState();

        // Create new transformer and import state
        var restored = new LagLeadTransformer<double>(options);
        restored.ImportState(state);

        // Transform with restored transformer
        var restoredResult = restored.Transform(data);

        // Verify exact output match
        Assert.Equal(originalResult.Shape[0], restoredResult.Shape[0]);
        Assert.Equal(originalResult.Shape[1], restoredResult.Shape[1]);

        for (int t = 0; t < originalResult.Shape[0]; t++)
        {
            for (int f = 0; f < originalResult.Shape[1]; f++)
            {
                var origVal = originalResult[t, f];
                var restVal = restoredResult[t, f];

                if (double.IsNaN(origVal) && double.IsNaN(restVal))
                    continue;

                Assert.True(
                    Math.Abs(origVal - restVal) < 1e-10,
                    $"Mismatch at t={t}, f={f}: original={origVal}, restored={restVal}");
            }
        }
    }

    [Fact]
    public void AnomalyFeaturesTransformer_RoundTrip_ProducesIdenticalOutput()
    {
        // Setup
        var options = new TimeSeriesFeatureOptions
        {
            WindowSizes = [5],
            ZScoreThreshold = 2.0,
            IqrMultiplier = 1.5
        };

        // Create and fit original transformer
        var original = new AnomalyFeaturesTransformer<double>(options);
        var data = new Tensor<double>(new[] { 30, 1 });
        for (int t = 0; t < 30; t++)
        {
            data[t, 0] = 50.0 + t;
            if (t == 15)
                data[t, 0] = 200.0; // Outlier
        }

        original.Fit(data);
        var originalResult = original.Transform(data);

        // Export state
        var state = original.ExportState();

        // Create new transformer and import state
        var restored = new AnomalyFeaturesTransformer<double>(options);
        restored.ImportState(state);

        // Transform with restored transformer
        var restoredResult = restored.Transform(data);

        // Verify exact output match
        Assert.Equal(originalResult.Shape[0], restoredResult.Shape[0]);
        Assert.Equal(originalResult.Shape[1], restoredResult.Shape[1]);

        for (int t = 0; t < originalResult.Shape[0]; t++)
        {
            for (int f = 0; f < originalResult.Shape[1]; f++)
            {
                var origVal = originalResult[t, f];
                var restVal = restoredResult[t, f];

                if (double.IsNaN(origVal) && double.IsNaN(restVal))
                    continue;

                Assert.True(
                    Math.Abs(origVal - restVal) < 1e-10,
                    $"Mismatch at t={t}, f={f}: original={origVal}, restored={restVal}");
            }
        }
    }

    [Fact]
    public void RollingCorrelationTransformer_RoundTrip_ProducesIdenticalOutput()
    {
        // Setup
        var options = new TimeSeriesFeatureOptions
        {
            WindowSizes = [10]
        };

        // Create and fit original transformer
        var original = new RollingCorrelationTransformer<double>(options);
        var data = new Tensor<double>(new[] { 30, 3 });
        for (int t = 0; t < 30; t++)
        {
            data[t, 0] = t;
            data[t, 1] = t * 2 + Math.Sin(t * 0.5);
            data[t, 2] = t * 0.5 + Math.Cos(t * 0.3);
        }

        original.Fit(data);
        var originalResult = original.Transform(data);

        // Export state
        var state = original.ExportState();

        // Create new transformer and import state
        var restored = new RollingCorrelationTransformer<double>(options);
        restored.ImportState(state);

        // Transform with restored transformer
        var restoredResult = restored.Transform(data);

        // Verify exact output match
        Assert.Equal(originalResult.Shape[0], restoredResult.Shape[0]);
        Assert.Equal(originalResult.Shape[1], restoredResult.Shape[1]);

        for (int t = 0; t < originalResult.Shape[0]; t++)
        {
            for (int f = 0; f < originalResult.Shape[1]; f++)
            {
                var origVal = originalResult[t, f];
                var restVal = restoredResult[t, f];

                if (double.IsNaN(origVal) && double.IsNaN(restVal))
                    continue;

                Assert.True(
                    Math.Abs(origVal - restVal) < 1e-10,
                    $"Mismatch at t={t}, f={f}: original={origVal}, restored={restVal}");
            }
        }
    }

    [Fact]
    public void RollingRegressionTransformer_RoundTrip_ProducesIdenticalOutput()
    {
        // Setup
        var options = new TimeSeriesFeatureOptions
        {
            WindowSizes = [10]
        };

        // Create and fit original transformer
        var original = new RollingRegressionTransformer<double>(options);
        var data = new Tensor<double>(new[] { 30, 2 });
        for (int t = 0; t < 30; t++)
        {
            data[t, 0] = t;
            data[t, 1] = t * 2 + 5;
        }

        original.Fit(data);
        var originalResult = original.Transform(data);

        // Export state
        var state = original.ExportState();

        // Create new transformer and import state
        var restored = new RollingRegressionTransformer<double>(options);
        restored.ImportState(state);

        // Transform with restored transformer
        var restoredResult = restored.Transform(data);

        // Verify exact output match
        Assert.Equal(originalResult.Shape[0], restoredResult.Shape[0]);
        Assert.Equal(originalResult.Shape[1], restoredResult.Shape[1]);

        for (int t = 0; t < originalResult.Shape[0]; t++)
        {
            for (int f = 0; f < originalResult.Shape[1]; f++)
            {
                var origVal = originalResult[t, f];
                var restVal = restoredResult[t, f];

                if (double.IsNaN(origVal) && double.IsNaN(restVal))
                    continue;

                Assert.True(
                    Math.Abs(origVal - restVal) < 1e-10,
                    $"Mismatch at t={t}, f={f}: original={origVal}, restored={restVal}");
            }
        }
    }

    #endregion

    #region TimeSeriesSplit Tests

    [Fact]
    public void TimeSeriesSplit_Construction_Succeeds()
    {
        var splitter = new TimeSeriesSplit(nSplits: 5);

        Assert.NotNull(splitter);
        Assert.Equal(5, splitter.NSplits);
        Assert.Null(splitter.MaxTrainSize);
        Assert.Null(splitter.TestSize);
        Assert.Equal(0, splitter.Gap);
    }

    [Fact]
    public void TimeSeriesSplit_Split_GeneratesCorrectNumberOfSplits()
    {
        var splitter = new TimeSeriesSplit(nSplits: 3);
        int nSamples = 100;

        var splits = splitter.Split(nSamples).ToList();

        Assert.Equal(3, splits.Count);
    }

    [Fact]
    public void TimeSeriesSplit_Split_TrainBeforeTest()
    {
        var splitter = new TimeSeriesSplit(nSplits: 3);
        int nSamples = 100;

        foreach (var (trainIndices, testIndices) in splitter.Split(nSamples))
        {
            // All train indices should be less than all test indices
            int maxTrain = trainIndices.Max();
            int minTest = testIndices.Min();

            Assert.True(maxTrain < minTest, "Training indices must come before test indices");
        }
    }

    [Fact]
    public void TimeSeriesSplit_WithGap_MaintainsGapBetweenTrainAndTest()
    {
        var splitter = new TimeSeriesSplit(nSplits: 3, gap: 5);
        int nSamples = 100;

        foreach (var (trainIndices, testIndices) in splitter.Split(nSamples))
        {
            int maxTrain = trainIndices.Max();
            int minTest = testIndices.Min();

            Assert.True(minTest - maxTrain >= 5, $"Gap should be at least 5, got {minTest - maxTrain}");
        }
    }

    [Fact]
    public void TimeSeriesSplit_WithMaxTrainSize_LimitsTrainingSize()
    {
        var splitter = new TimeSeriesSplit(nSplits: 3, maxTrainSize: 20);
        int nSamples = 100;

        foreach (var (trainIndices, testIndices) in splitter.Split(nSamples))
        {
            Assert.True(trainIndices.Length <= 20, $"Train size should be at most 20, got {trainIndices.Length}");
        }
    }

    [Fact]
    public void TimeSeriesSplit_ExpandingWindow_TrainSizeGrows()
    {
        var splitter = new TimeSeriesSplit(nSplits: 3);
        int nSamples = 100;

        var splits = splitter.Split(nSamples).ToList();

        // In expanding window mode, later splits should have more training data
        for (int i = 1; i < splits.Count; i++)
        {
            Assert.True(
                splits[i].TrainIndices.Length >= splits[i - 1].TrainIndices.Length,
                "Train size should grow or stay same in expanding window mode");
        }
    }

    [Fact]
    public void TimeSeriesSplit_CrossValidate_ReturnsScores()
    {
        var splitter = new TimeSeriesSplit(nSplits: 3);
        var data = Enumerable.Range(0, 100).Select(i => (double)i).ToArray();

        var scores = splitter.CrossValidate(
            data,
            (train, test) => train.Average()); // Simple evaluator

        Assert.Equal(3, scores.Length);
        Assert.All(scores, s => Assert.True(!double.IsNaN(s)));
    }

    [Fact]
    public void TimeSeriesSplit_GetSplitSummary_ReturnsDescription()
    {
        var splitter = new TimeSeriesSplit(nSplits: 3, maxTrainSize: 30, gap: 5);

        var summary = splitter.GetSplitSummary(100);

        Assert.NotNull(summary);
        Assert.Contains("TimeSeriesSplit", summary);
        Assert.Contains("Max Train Size: 30", summary);
        Assert.Contains("Gap: 5", summary);
    }

    [Fact]
    public void TimeSeriesValidation_WalkForward_GeneratesSplits()
    {
        var splits = TimeSeriesValidation.WalkForward(
            nSamples: 100,
            trainSize: 20,
            testSize: 5).ToList();

        Assert.True(splits.Count > 0);

        foreach (var (train, test) in splits)
        {
            Assert.Equal(20, train.Length);
            Assert.Equal(5, test.Length);
            Assert.True(train.Max() < test.Min());
        }
    }

    [Fact]
    public void TimeSeriesSplit_InvalidNSplits_Throws()
    {
        Assert.Throws<ArgumentException>(() => new TimeSeriesSplit(nSplits: 1));
    }

    [Fact]
    public void TimeSeriesSplit_NotEnoughData_Throws()
    {
        var splitter = new TimeSeriesSplit(nSplits: 10, testSize: 100);

        Assert.Throws<ArgumentException>(() => splitter.Split(50).ToList());
    }

    #endregion
}
