{
  "data": {
    "repository": {
      "pullRequest": {
        "reviewThreads": {
          "pageInfo": {
            "hasNextPage": false,
            "endCursor": "Y3Vyc29yOnYyOpK0MjAyNS0xMS0xM1QwMTozMzoxNFrOYcSePg=="
          },
          "nodes": [
            {
              "id": "PRRT_kwDOKSXUF85huXSV",
              "isResolved": true,
              "comments": {
                "nodes": [
                  {
                    "id": "PRRC_kwDOKSXUF86WLjSy",
                    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Guard teacher/student feature alignment.**\n\n`ComputeFlowLoss` assumes `studentFeatures` and `teacherFeatures` have identical lengths; if they differ (common when architectures donΓÇÖt line up), youΓÇÖll index past the end and crash. Please validate the lengths up front and throw a clear exception (or handle mismatched counts).\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/Strategies/FlowBasedDistillationStrategy.cs around\nlines 172 to 189, add an upfront validation that studentFeatures and\nteacherFeatures have the same Length and that both have at least 2 layers; if\nlengths differ or either has fewer than 2 elements, throw an ArgumentException\nwith a clear message (e.g. \"studentFeatures and teacherFeatures must have the\nsame number of layers and at least 2 layers to compute flow matrices\"). This\nprevents indexing past the end and makes the failure explicit.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 5935afc to 8aeee73",
                    "path": "src/KnowledgeDistillation/Strategies/FlowBasedDistillationStrategy.cs",
                    "createdAt": "2025-11-12T19:55:34Z",
                    "author": {
                      "login": "coderabbitai"
                    }
                  }
                ]
              }
            },
            {
              "id": "PRRT_kwDOKSXUF85huXSc",
              "isResolved": true,
              "comments": {
                "nodes": [
                  {
                    "id": "PRRC_kwDOKSXUF86WLjS9",
                    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Selectivity term not integrated; base loss is reduced regardless**\n\nComputeLoss scales standard distillation by (1 ΓêÆ selectivityWeight) but never adds the selectivity loss, so selectivity never influences training and the base objective is weakened.\n\nOptions:\n- Preferred: integrate selectivity into the optimized loss: final = (1 ΓêÆ w)┬╖KD + w┬╖SelectivityLoss(batch activations); mirror this in gradients.\n- Interim safe fix: remove the (1 ΓêÆ w) scaling until selectivity is properly added.\n\nDiff for the interim fix:\n\n```diff\n-        // Apply selectivity weight reduction exactly once\n-        return NumOps.Multiply(finalLoss, NumOps.FromDouble(1.0 - _selectivityWeight));\n+        return finalLoss;\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    public override T ComputeLoss(Vector<T> studentOutput, Vector<T> teacherOutput, Vector<T>? trueLabels = null)\n    {\n        ValidateOutputDimensions(studentOutput, teacherOutput, v => v.Length);\n\n        // Standard distillation loss\n        var studentSoft = Softmax(studentOutput, Temperature);\n        var teacherSoft = Softmax(teacherOutput, Temperature);\n        var softLoss = KLDivergence(teacherSoft, studentSoft);\n        softLoss = NumOps.Multiply(softLoss, NumOps.FromDouble(Temperature * Temperature));\n\n        T finalLoss;\n        if (trueLabels != null)\n        {\n            ValidateLabelDimensions(studentOutput, trueLabels, v => v.Length);\n            var studentProbs = Softmax(studentOutput, 1.0);\n            var hardLoss = CrossEntropy(studentProbs, trueLabels);\n            finalLoss = NumOps.Add(\n                NumOps.Multiply(NumOps.FromDouble(Alpha), hardLoss),\n                NumOps.Multiply(NumOps.FromDouble(1.0 - Alpha), softLoss));\n        }\n        else\n        {\n            finalLoss = softLoss;\n        }\n\n        return finalLoss;\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<!-- fingerprinting:phantom:poseidon:chinchilla -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 285fc01 to 93d5f68",
                    "path": "src/KnowledgeDistillation/Strategies/NeuronSelectivityDistillationStrategy.cs",
                    "createdAt": "2025-11-12T19:55:34Z",
                    "author": {
                      "login": "coderabbitai"
                    }
                  }
                ]
              }
            },
            {
              "id": "PRRT_kwDOKSXUF85huXSq",
              "isResolved": false,
              "comments": {
                "nodes": [
                  {
                    "id": "PRRC_kwDOKSXUF86WLjTO",
                    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Gradient scaling mirrors the same issue**\n\nYou scale the gradient by (1 ΓêÆ selectivityWeight) without adding any selectivity gradient, further weakening optimization.\n\nInterim fix consistent with the loss change:\n\n```diff\n-                // Apply selectivity weight reduction exactly once\n-                gradient[i] = NumOps.Multiply(combined, NumOps.FromDouble(1.0 - _selectivityWeight));\n+                gradient[i] = combined;\n@@\n-                // Apply selectivity weight reduction exactly once\n-                gradient[i] = NumOps.Multiply(softGrad, NumOps.FromDouble(1.0 - _selectivityWeight));\n+                gradient[i] = softGrad;\n```\n\n<!-- fingerprinting:phantom:poseidon:chinchilla -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                    "path": "src/KnowledgeDistillation/Strategies/NeuronSelectivityDistillationStrategy.cs",
                    "createdAt": "2025-11-12T19:55:34Z",
                    "author": {
                      "login": "coderabbitai"
                    }
                  }
                ]
              }
            },
            {
              "id": "PRRT_kwDOKSXUF85huXSu",
              "isResolved": false,
              "comments": {
                "nodes": [
                  {
                    "id": "PRRC_kwDOKSXUF86WLjTW",
                    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n<details>\n<summary>≡ƒº⌐ Analysis chain</summary>\n\n**Relational loss/gradients are still applied to the wrong samples**\n\nYou compute batch relational terms when the batch fills, then amortize them over the next N per-sample calls. Earlier samples that formed the batch receive only ΓÇ£in-batchΓÇ¥ partial gradients, not the finalized amortized contribution; subsequent samples (from the next batch) receive the cached average. This misaligns loss and gradient with contributing samples and weakens RKD.\n\nRecommendation:\n- Switch trainer/strategy API to batch-level for RKD (return per-sample losses/gradients for the same batch), or\n- Cache per-sample RKD gradients for the completed batch (array of Vector<T>, not a single average), then apply them to the next N calls strictly corresponding to those batch slots (and clear afterward). Document the approximation if batch API changes are not possible.\n\nThis is a correctness concern for RKD signal strength. \n\n\n\n\nAlso applies to: 295-366\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Examine the full RelationalDistillationStrategy.cs file to understand amortization logic\nfd -t f \"RelationalDistillationStrategy.cs\" -x cat {}\n```\n\nLength of output: 38351\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Search for usages of RelationalDistillationStrategy and calls to ComputeLoss/ComputeGradient\n# to understand the execution order and trainer integration\nrg -n \"RelationalDistillationStrategy|ComputeLoss|ComputeGradient\" --type cs -B2 -A2 | head -100\n```\n\nLength of output: 9504\n\n---\n\n</parameter>\n</invoke>\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Search for trainer integration and Reset() calls\nfd -t f \"*.cs\" -x rg -l \"Reset\\(\\)\" {} \\; | xargs -I {} sh -c 'echo \"=== {} ===\" && rg -n \"Reset|ComputeLoss|ComputeGradient\" -B3 -A3 {}'\n```\n\nLength of output: 283\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Search for Reset() calls and usage patterns\nrg -n \"\\.Reset\\(\\)\" --type cs -B5 -A5\n```\n\nLength of output: 13009\n\n---\n\n\n\n</details>\n\n**Relational loss and gradients are indeed misaligned between batch computation and sample application**\n\nThe review concern is valid. Here's the verified issue:\n\n**Execution Timeline (batch_size=32):**\n- **Samples 0-31:** Accumulated in buffers. When 31st sample added, relational loss computed on batch[0..31], then buffers cleared.\n- **Sample 31's gradient:** Cannot use Path 1 in `ComputeGradient` (buffers empty after `ComputeLoss`), so it gets zero relational gradient or stale cached gradient.\n- **Samples 32-63:** Receive amortized relational gradient computed from batch[0..31]ΓÇönot from samples that contribute to *their* relationships.\n\nThis causes:\n1. **Temporal misalignment:** Samples forming a batch don't uniformly receive their own relational loss signal.\n2. **Cross-batch leakage:** Relational knowledge from batch N applied to batch N+1.\n3. **Weakened RKD signal:** Loss-gradient consistency is broken; the gradient doesn't match the sample that incurred the loss.\n\nThe `Reset()` call at epoch boundaries (verified in `KnowledgeDistillationTrainerBase.cs:570`) flushes partial batches but doesn't address the within-epoch misalignment.\n\n**Recommendation from original review remains sound:** Cache per-sample relational gradients (array, not average) and apply them to the exact samples that computed the batch, or refactor to batch-level API.\n\n<!-- fingerprinting:phantom:poseidon:chinchilla -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                    "path": "src/KnowledgeDistillation/Strategies/RelationalDistillationStrategy.cs",
                    "createdAt": "2025-11-12T19:55:34Z",
                    "author": {
                      "login": "coderabbitai"
                    }
                  }
                ]
              }
            },
            {
              "id": "PRRT_kwDOKSXUF85huXS1",
              "isResolved": true,
              "comments": {
                "nodes": [
                  {
                    "id": "PRRC_kwDOKSXUF86WLjTe",
                    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Stop wiring the online teacher with a no-op updater.**\n\n`CreateOnlineTeacher` still passes `(pred, target) => { }` into `OnlineTeacherModel`. That teacher will never adapt, so every ΓÇ£onlineΓÇ¥ training session silently degenerates into a static teacher. This violates the contract of the online variant and repeats the functional bug we previously called out. Please surface a real update delegate (e.g., supplied by the caller or exposed from the model) or refuse to build this teacher until one is provided.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/TeacherModelFactory.cs around lines 146 to 164, the\nOnline teacher is being constructed with a no-op updater which prevents any\nonline adaptation; replace the empty update delegate with a real updater by\neither (a) obtaining the update function from the provided IFullModel (e.g.,\nmodel.Update or a similar method exposed by the model) and pass it into\nOnlineTeacherModel, or (b) change the factory signature to require an\nAction<Vector<T>, Vector<T>> updateDelegate parameter (or Func/appropriate\ndelegate) and throw an ArgumentException if it is null so the factory refuses to\nbuild an online teacher without a valid updater; ensure the chosen updater\nmatches the OnlineTeacherModel delegate signature and wire through\nupdateMode/updateRate unchanged.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                    "path": "src/KnowledgeDistillation/TeacherModelFactory.cs",
                    "createdAt": "2025-11-12T19:55:34Z",
                    "author": {
                      "login": "coderabbitai"
                    }
                  }
                ]
              }
            },
            {
              "id": "PRRT_kwDOKSXUF85huXS9",
              "isResolved": false,
              "comments": {
                "nodes": [
                  {
                    "id": "PRRC_kwDOKSXUF86WLjTo",
                    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**DonΓÇÖt rely on metadata reflection for output size.**\n\n`GetOutputDimensionFromModel` now throws for any `IFullModel` that doesnΓÇÖt implement a `GetMetadata()` method returning a specific `Dictionary<string, object>`. The factory (`TeacherModelFactory.CreateNeuralNetworkTeacher`, `CreateAdaptiveTeacher`, etc.) still builds wrappers via `new TeacherModelWrapper<T>(model)`, so common models that previously worked will now fail immediately. Please accept an explicit `outputDimension` (and update the factory to pass it) or keep a robust fallbackΓÇöotherwise the default instantiation path is broken.\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                    "path": "src/KnowledgeDistillation/TeacherModelWrapper.cs",
                    "createdAt": "2025-11-12T19:55:35Z",
                    "author": {
                      "login": "coderabbitai"
                    }
                  }
                ]
              }
            },
            {
              "id": "PRRT_kwDOKSXUF85huXTD",
              "isResolved": false,
              "comments": {
                "nodes": [
                  {
                    "id": "PRRC_kwDOKSXUF86WLjTt",
                    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Fix geometric-mean aggregation ΓÇö itΓÇÖs still wrong.**\n\nThis branch continues to apply `Math.Log(Math.Abs(logit))` on raw logits and multiplies by an overall sign. Taking logs of logits destroys their scale (and flips negatives to positives), so the ensemble invents confidence patterns that no teacher produced. Please convert logits to probabilities (or log-probabilities via softmax) before computing a geometric mean, as previously requested.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/Teachers/EnsembleTeacherModel.cs around lines 205\nto 221, the geometric-mean branch is wrong because it takes logs of raw logits\nand tracks a sign; instead convert each teacher's logits to probabilities (or\nlog-probabilities via softmax) first, then compute the weighted mean of\nlog-probabilities (i.e. sum(weights * log(p_i))) to implement the geometric mean\nin probability space, use a small epsilon to avoid log(0), and finally\nexponentiate (or normalize/convert back to logits) as needed for result storage;\nremove the sign-tracking and direct Math.Log(Math.Abs(logit)) usage and ensure\nweights are applied to the per-teacher log-probabilities.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                    "path": "src/KnowledgeDistillation/Teachers/EnsembleTeacherModel.cs",
                    "createdAt": "2025-11-12T19:55:35Z",
                    "author": {
                      "login": "coderabbitai"
                    }
                  }
                ]
              }
            },
            {
              "id": "PRRT_kwDOKSXUF85huXTM",
              "isResolved": false,
              "comments": {
                "nodes": [
                  {
                    "id": "PRRC_kwDOKSXUF86WLjT4",
                    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Validate cached vector dimensions before storing.**\n\n`CachePredictions` only checks for nulls/empties. If any cached vectorΓÇÖs length differs from `_outputDim`, later consumers (and KD trainers) will index past the end or combine mismatched logits, yielding runtime exceptions or garbage gradients. Please enforce `predictions[i].Length == _outputDim` (and guard against negative indices in `GetCachedPrediction`).\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/Teachers/SelfTeacherModel.cs around lines 27 to 44,\nthe CachePredictions method currently only checks for null/empty vectors but\ndoes not validate that each prediction vector length matches the expected\n_outputDim (and callers like GetCachedPrediction should also guard against\nnegative indices); update CachePredictions to throw an ArgumentException if any\npredictions[i].Length != _outputDim with a clear message including the index and\nexpected dimension, and also add a guard in GetCachedPrediction to validate the\nrequested index is within [0, _cachedPredictions.Length) (throw\nArgumentOutOfRangeException for negative or out-of-range indices) before\nindexing into the array.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                    "path": "src/KnowledgeDistillation/Teachers/SelfTeacherModel.cs",
                    "createdAt": "2025-11-12T19:55:35Z",
                    "author": {
                      "login": "coderabbitai"
                    }
                  }
                ]
              }
            },
            {
              "id": "PRRT_kwDOKSXUF85huXTW",
              "isResolved": false,
              "comments": {
                "nodes": [
                  {
                    "id": "PRRC_kwDOKSXUF86WLjUE",
                    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Do not call options.Validate() here (throws with default settings)**\n\nValidate() requires ValidationInputs when ValidateAfterEpoch=true (default). At this point options has no validation data, so this will throw on typical usage.\n\nApply one of:\n- Minimal: remove validation here and perform targeted checks (ranges, positivity) locally.\n\n```diff\n-        options.Validate(); // Validate all options before training\n+        // Defer full options.Validate(); trainer already validates shapes at runtime.\n+        // TODO: Optionally validate only numeric ranges here.\n```\n\n- Or populate options.ValidationInputs/Labels from valInputs/valLabels before calling Validate (move Validate below where those are available).\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/PredictionModelBuilder.cs around lines 1016 to 1018, do not call\noptions.Validate() there because Validate() requires ValidationInputs when\nValidateAfterEpoch=true (the default) and options currently has no validation\ndata which will throw; either remove this Validate() call and instead perform\nminimal local checks (e.g., numeric ranges, non-negative values, required\nfields) at this location, or move the Validate() call to after\nvalInputs/valLabels have been assigned to options.ValidationInputs/Labels so\nfull validation runs only when validation data exists.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:chinchilla -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                    "path": "src/PredictionModelBuilder.cs",
                    "createdAt": "2025-11-12T19:55:35Z",
                    "author": {
                      "login": "coderabbitai"
                    }
                  }
                ]
              }
            },
            {
              "id": "PRRT_kwDOKSXUF85huXTg",
              "isResolved": false,
              "comments": {
                "nodes": [
                  {
                    "id": "PRRC_kwDOKSXUF86WLjUR",
                    "body": "_≡ƒ¢á∩╕Å Refactor suggestion_ | _≡ƒƒá Major_\n\n**KD hardΓÇærequires Vector<T> model types; clashes with builderΓÇÖs Matrix path**\n\nThe cast to IFullModel<T, Vector<T>, Vector<T>> forbids MatrixΓÇæbased students. Combined with BuildAsyncΓÇÖs early ConvertToMatrix, users canΓÇÖt satisfy both. This blocks valid scenarios (e.g., Matrix<double> inputs).\n\nOptions:\n- Support Matrix students: adapt studentForward via ConversionsHelper.ConvertVectorToInputWithoutReference<T,TInput>() and call studentModel.Predict(TInput) without the hard cast; update backward to use model training APIs.\n- Or route to KD path before ConvertToMatrix and document TInput=Vector<T> requirement (breaking for Matrix users, but consistent).\nWhich direction do you prefer?\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/PredictionModelBuilder.cs around lines 1021 to 1026, the code hard-casts\nthe student model to IFullModel<T, Vector<T>, Vector<T>> which prevents\nMatrix-based student models and conflicts with the earlier ConvertToMatrix path;\ninstead, remove the hard cast and implement a studentForward that accepts the\nalready-constructed Vector<T> and converts it to the student model's TInput via\nConversionsHelper.ConvertVectorToInputWithoutReference<T,TInput>(), call\nstudentModel.Predict(TInput) (or the appropriate Predict method) and ensure the\nbackward/training path uses the model's training API with TInput/TOutput types;\nupdate types and null checks accordingly so both Vector and Matrix student\nmodels are supported without changing the outer ConvertToMatrix ordering.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:chinchilla -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                    "path": "src/PredictionModelBuilder.cs",
                    "createdAt": "2025-11-12T19:55:35Z",
                    "author": {
                      "login": "coderabbitai"
                    }
                  }
                ]
              }
            },
            {
              "id": "PRRT_kwDOKSXUF85huXTj",
              "isResolved": true,
              "comments": {
                "nodes": [
                  {
                    "id": "PRRC_kwDOKSXUF86WLjUV",
                    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Backprop likely lacks forward memory; avoid Predict() for training**\n\nstudentBackward calls Network.Backpropagate expecting a prior forward-with-memory. studentForward uses vectorStudentModel.Predict, which typically runs inference without caching activations.\n\nRefactor studentForward to use a training forward that records activations (e.g., nnModel.Network.ForwardWithMemory or equivalent), and base studentBackward on that same pass; or switch to the modelΓÇÖs ComputeGradients/ApplyGradients API for correctness.\n\n<!-- fingerprinting:phantom:poseidon:chinchilla -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 26005d1 to f09b882",
                    "path": "src/PredictionModelBuilder.cs",
                    "createdAt": "2025-11-12T19:55:35Z",
                    "author": {
                      "login": "coderabbitai"
                    }
                  }
                ]
              }
            },
            {
              "id": "PRRT_kwDOKSXUF85huXTo",
              "isResolved": false,
              "comments": {
                "nodes": [
                  {
                    "id": "PRRC_kwDOKSXUF86WLjUd",
                    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Early stopping is ineffective and uses the wrong metric**\n\n- It compares training loss (avgLoss) but labels it ΓÇ£val lossΓÇ¥.\n- It never stops training (only logs).\n\nWire early stopping to actual validation loss and enforce stopping.\n\nSuggested approach:\n- Compute validation loss inside the callback by averaging DistillationStrategy.ComputeLoss over (valInputs,valLabels) with teacher logits.\n- Add a stop flag the trainer honors, or refactor to loop epochs externally (call Train with epochs:1 in a for-loop) and break when patience exhausted.\n\n<!-- fingerprinting:phantom:poseidon:chinchilla -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                    "path": "src/PredictionModelBuilder.cs",
                    "createdAt": "2025-11-12T19:55:35Z",
                    "author": {
                      "login": "coderabbitai"
                    }
                  }
                ]
              }
            },
            {
              "id": "PRRT_kwDOKSXUF85huXTt",
              "isResolved": true,
              "comments": {
                "nodes": [
                  {
                    "id": "PRRC_kwDOKSXUF86WLjUm",
                    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Mock model exposes inconsistent parameter shapes.**\n\n`ParameterCount` advertises 100/1000 parameters, yet `GetParameters()` and `ApplyGradients()` operate on zero-length vectors. The KD builder relies on these APIs during training; this mismatch will either throw (shape checks) or silently skip updates. Please return a vector whose length matches `ParameterCount` (and honour it in `ApplyGradients`).\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn testconsole/Examples/KnowledgeDistillationExample.cs around lines 295-369,\nthe MockModel advertises ParameterCount of 100/1000 but GetParameters,\nComputeGradients and ApplyGradients use/return zero-length vectors causing shape\nmismatches; fix by adding an internal parameters array/Vector<double> sized to\nParameterCount set in the constructor, have GetParameters return a copy of that\nvector, SetParameters validate length and copy values, ComputeGradients return a\nvector of length ParameterCount (e.g., zeros or mock gradients of same length),\nand implement ApplyGradients to update the internal parameters element-wise\nusing the provided gradients and learningRate; also ensure DeepCopy/Clone create\nnew MockModel instances that copy the parameter values.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                    "path": "testconsole/Examples/KnowledgeDistillationExample.cs",
                    "createdAt": "2025-11-12T19:55:36Z",
                    "author": {
                      "login": "coderabbitai"
                    }
                  }
                ]
              }
            },
            {
              "id": "PRRT_kwDOKSXUF85huXTy",
              "isResolved": true,
              "comments": {
                "nodes": [
                  {
                    "id": "PRRC_kwDOKSXUF86WLjUs",
                    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Example wonΓÇÖt run with current KD path (Matrix student vs VectorΓÇæonly KD)**\n\nKD helper casts the student to IFullModel<ΓÇª, Vector<T>, Vector<T>>. This example uses Matrix<double> inputs, so ConfigureKnowledgeDistillation ΓåÆ BuildAsync will throw.\n\nEither:\n- Update example to Vector<T> student/builder (shown below), or\n- Extend KD to support Matrix students (preferred for usability).\n\nVectorΓÇæbased example adjustments:\n\n```diff\n-IFullModel<double, Matrix<double>, Vector<double>> teacherModel = GetTeacherModel();\n-IFullModel<double, Matrix<double>, Vector<double>> studentModel = CreateStudentModel();\n-var kdOptions = new KnowledgeDistillationOptions<double, Matrix<double>, Vector<double>>\n+IFullModel<double, Vector<double>, Vector<double>> teacherModel = GetTeacherModel();\n+IFullModel<double, Vector<double>, Vector<double>> studentModel = CreateStudentModel();\n+var kdOptions = new KnowledgeDistillationOptions<double, Vector<double>, Vector<double>>\n@@\n-var result = await new PredictionModelBuilder<double, Matrix<double>, Vector<double>>()\n+var result = await new PredictionModelBuilder<double, Vector<double>, Vector<double>>()\n```\n\nYouΓÇÖll also need LoadYourData/GetTeacherModel/CreateStudentModel to use Vector<double> inputs (or switch the KD implementation to accept Matrix).\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\ntestconsole/Examples/SimpleKnowledgeDistillationExample.cs lines 25-46: the\nexample configures knowledge distillation but uses Matrix<double> for model\ninputs while the KD helper currently casts the student to IFullModel<ΓÇª,\nVector<T>, Vector<T>>, causing BuildAsync to throw; fix by either (A) updating\nthis example and its helper data/model factory methods (LoadYourData,\nGetTeacherModel, CreateStudentModel) to use Vector<double> everywhere so the\nstudent/teacher generics match the KD API, or (B) extend the KD implementation\nto accept Matrix<T> students by changing the KD helper/generic signatures and\ninternal cast to support IFullModel<T, Matrix<T>, Vector<T>> (and update\nloss/forward code that assumes Vector inputs accordingly) ΓÇö pick one approach\nand make the corresponding changes consistently across the example and KD code.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:chinchilla -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 5935afc to 8aeee73",
                    "path": "testconsole/Examples/SimpleKnowledgeDistillationExample.cs",
                    "createdAt": "2025-11-12T19:55:36Z",
                    "author": {
                      "login": "coderabbitai"
                    }
                  }
                ]
              }
            },
            {
              "id": "PRRT_kwDOKSXUF85huXT6",
              "isResolved": false,
              "comments": {
                "nodes": [
                  {
                    "id": "PRRC_kwDOKSXUF86WLjU3",
                    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Remove FeatureBased expectations or assert the thrown exception.**\n\n`DistillationStrategyFactory.CreateStrategy(DistillationStrategyType.FeatureBased, ΓÇª)` still throws `NotSupportedException`. These tests now call it in `CreateStrategy_WithAllStrategyTypes` and `CreateStrategy_FeatureBased_ReturnsFeatureDistillationStrategy`, so the suite fails before any assertions run. Please either skip FeatureBased here or assert the expected exception instead of `Assert.IsType`.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn\ntests/AiDotNet.Tests/UnitTests/KnowledgeDistillation/DistillationStrategyFactoryTests.cs\naround lines 14 to 85, the FeatureBased strategy is not supported and\nCreateStrategy throws NotSupportedException, so remove FeatureBased from the\nTheory InlineData list (so CreateStrategy_WithAllStrategyTypes won't invoke it)\nand modify the FeatureBased-specific test to expect the exception (replace the\nAssert.IsType assertion with Assert.Throws<NotSupportedException>(() =>\nDistillationStrategyFactory<double>.CreateStrategy(DistillationStrategyType.FeatureBased,\nfeatureWeight: 0.7))); this keeps the rest of the parameterized test intact and\ndocuments the unsupported behavior.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                    "path": "tests/AiDotNet.Tests/UnitTests/KnowledgeDistillation/DistillationStrategyFactoryTests.cs",
                    "createdAt": "2025-11-12T19:55:36Z",
                    "author": {
                      "login": "coderabbitai"
                    }
                  }
                ]
              }
            },
            {
              "id": "PRRT_kwDOKSXUF85huXUB",
              "isResolved": true,
              "comments": {
                "nodes": [
                  {
                    "id": "PRRC_kwDOKSXUF86WLjVA",
                    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n<details>\n<summary>≡ƒº⌐ Analysis chain</summary>\n\n**Evaluate() scale mismatch? Tests assume 0..1, impl may return 0..100**\n\nThe test asserts 0.5 and 0ΓëñaccΓëñ1. The trainer baseΓÇÖs Evaluate in snippets returns percentage├ù100. Please verify and align either the test or implementation.\n\n\nIf it returns 0..100, update tests to expect 50.0 and adjust the range to 0..100; otherwise keep current tests and change Evaluate to normalize 0..1.\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Inspect Evaluate implementation to confirm scale\nrg -nP --type=cs -C2 '\\bdouble\\s+Evaluate\\s*\\(.*Vector<.*>\\s+inputs.*\\)\\s*\\{' src/KnowledgeDistillation/KnowledgeDistillationTrainerBase.cs\nrg -nP --type=cs -C3 'return\\s*\\(double\\)\\s*correct\\s*/\\s*inputs\\.Length' src/KnowledgeDistillation/KnowledgeDistillationTrainerBase.cs\n```\n\nLength of output: 146\n\n---\n\n\n\n</details>\n\n**Evaluate() returns 0..100 but test expects 0..1ΓÇöfix required**\n\nVerification confirmed: line 356 in KnowledgeDistillationTrainerBase.cs returns `(double)correct / inputs.Length * 100.0`, which scales to 0..100. The test expects 0..1 range and will fail. Either update the test to expect 50.0 and range 0..100, or normalize Evaluate() to return 0..1.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn\ntests/AiDotNet.Tests/UnitTests/KnowledgeDistillation/KnowledgeDistillationTrainerTests.cs\naround lines 214-244, the test expects Evaluate() to return accuracy in 0..1 but\nKnowledgeDistillationTrainerBase.cs line 356 currently returns a percentage\n(correct/inputs.Length * 100.0); change Evaluate() to return a normalized\nfraction by removing the multiplication by 100 (i.e., return (double)correct /\ninputs.Length) so the method returns 0..1 and the existing test assertions\nremain valid.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:chinchilla -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 26005d1 to f09b882",
                    "path": "tests/AiDotNet.Tests/UnitTests/KnowledgeDistillation/KnowledgeDistillationTrainerTests.cs",
                    "createdAt": "2025-11-12T19:55:36Z",
                    "author": {
                      "login": "coderabbitai"
                    }
                  }
                ]
              }
            },
            {
              "id": "PRRT_kwDOKSXUF85hwOI9",
              "isResolved": true,
              "comments": {
                "nodes": [
                  {
                    "id": "PRRC_kwDOKSXUF86WOK3O",
                    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**≡ƒö┤ CRITICAL: Past review issue NOT fixed ΓÇô gradient scaling breaks mathematical correctness.**\n\nThis exact issue was flagged in a previous review (lines 166-186) and marked as \"Γ£à Addressed in commit 285fc01\", **but the problematic code is still present**.\n\n**The bug:** When `trueLabels` is null:\n- `ComputeLoss` (line 139) returns: `softLoss` (no scaling)\n- `ComputeGradient` (lines 182-184) returns: `softGradient ├ù (1 - Alpha)`\n\nThis breaks the fundamental relationship `gradient = d(loss)/d(logits)`.\n\n**Impact:** Optimizer will apply incorrect weight updates, degrading training convergence and final model quality.\n\n**This same bug exists in CurriculumDistillationStrategyBase.cs** (lines 208-211 in snippets), indicating a systematic issue across multiple strategy classes.\n\n\n\nApply the fix from the original review:\n\n```diff\n         }\n         else\n         {\n-            // Scale by (1 - alpha) if no hard loss\n-            for (int i = 0; i < n; i++)\n-            {\n-                gradient[i] = NumOps.Multiply(gradient[i], NumOps.FromDouble(1.0 - Alpha));\n-            }\n+            // No extra scaling when no hard loss; gradient matches loss = softLoss\n         }\n\n         return gradient;\n```\n\nAlso verify and fix the same issue in:\n- `CurriculumDistillationStrategyBase.cs` (lines 208-211)\n- Any other strategy classes with similar patterns\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        else\n        {\n            // No extra scaling when no hard loss; gradient matches loss = softLoss\n        }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                    "path": "src/KnowledgeDistillation/Strategies/AdaptiveDistillationStrategyBase.cs",
                    "createdAt": "2025-11-13T00:00:02Z",
                    "author": {
                      "login": "coderabbitai"
                    }
                  }
                ]
              }
            },
            {
              "id": "PRRT_kwDOKSXUF85hwOJF",
              "isResolved": true,
              "comments": {
                "nodes": [
                  {
                    "id": "PRRC_kwDOKSXUF86WOK3a",
                    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Fix Alpha weighting for hard vs. soft loss.**\n\n`ComputeLoss` and `ComputeGradient` currently multiply the KL term by `Alpha` and the hard term by `(1 - Alpha)`, which is the opposite of every other strategy (including the base docs). This flips the intended trade-off and causes the gradient to optimize a different objective than callers expect. Align the weighting with the rest of the framework so `Alpha` scales the hard loss/gradient.\n\n```diff\n-            return NumOps.Add(\n-                NumOps.Multiply(alphaT, klLoss),\n-                NumOps.Multiply(oneMinusAlpha, hardLoss));\n+            return NumOps.Add(\n+                NumOps.Multiply(alphaT, hardLoss),\n+                NumOps.Multiply(oneMinusAlpha, klLoss));\nΓÇª\n-            var alphaT = NumOps.FromDouble(Alpha);\n-            var oneMinusAlpha = NumOps.FromDouble(1.0 - Alpha);\n-            \n-            for (int i = 0; i < gradKL.Length; i++)\n-            {\n-                gradKL[i] = NumOps.Add(\n-                    NumOps.Multiply(alphaT, gradKL[i]),\n-                    NumOps.Multiply(oneMinusAlpha, gradHard[i]));\n-            }\n+            var alphaT = NumOps.FromDouble(Alpha);\n+            var oneMinusAlpha = NumOps.FromDouble(1.0 - Alpha);\n+\n+            for (int i = 0; i < gradKL.Length; i++)\n+            {\n+                gradKL[i] = NumOps.Add(\n+                    NumOps.Multiply(alphaT, gradHard[i]),\n+                    NumOps.Multiply(oneMinusAlpha, gradKL[i]));\n+            }\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/Strategies/FlowBasedDistillationStrategy.cs around\nlines 90-147, the code currently multiplies the KL/soft term by Alpha and the\nhard/cross-entropy term by (1-Alpha); change the weighting so Alpha scales the\nhard loss/gradient and (1-Alpha) scales the KL/soft term to match the rest of\nthe framework: in ComputeLoss return NumOps.Add(NumOps.Multiply(alphaT,\nhardLoss), NumOps.Multiply(oneMinusAlpha, klLoss)) instead of the current order,\nand in ComputeGradient combine terms so grad = Alpha * gradHard + (1-Alpha) *\ngradKL (i.e., use alphaT on gradHard and oneMinusAlpha on gradKL), making sure\nto create alphaT and oneMinusAlpha once via NumOps.FromDouble and apply them\nconsistently.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 5935afc to 8aeee73",
                    "path": "src/KnowledgeDistillation/Strategies/FlowBasedDistillationStrategy.cs",
                    "createdAt": "2025-11-13T00:00:02Z",
                    "author": {
                      "login": "coderabbitai"
                    }
                  }
                ]
              }
            },
            {
              "id": "PRRT_kwDOKSXUF85hwOJJ",
              "isResolved": true,
              "comments": {
                "nodes": [
                  {
                    "id": "PRRC_kwDOKSXUF86WOK3f",
                    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Remove unintended `(1 - selectivityWeight)` scaling from the gradient.**\n\n`ComputeLoss` now returns the full KD loss, but `ComputeGradient` still multiplies every component by `(1 - _selectivityWeight)`. That means the gradient no longer matches the derivative of the loss that backprop consumes, so updates are biased toward zero. Drop this scaling (or reintroduce the same factor in `ComputeLoss`) so the gradient aligns with the objective.\n\n```diff\n-                // Apply selectivity weight reduction exactly once\n-                gradient[i] = NumOps.Multiply(combined, NumOps.FromDouble(1.0 - _selectivityWeight));\n+                gradient[i] = combined;\nΓÇª\n-                // Apply selectivity weight reduction exactly once\n-                gradient[i] = NumOps.Multiply(softGrad, NumOps.FromDouble(1.0 - _selectivityWeight));\n+                gradient[i] = softGrad;\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/Strategies/NeuronSelectivityDistillationStrategy.cs\naround lines 85 to 113, remove the unintended (1 - _selectivityWeight) scaling\napplied to the gradient so the gradient matches the returned loss: stop\nmultiplying combined and softGrad by NumOps.FromDouble(1.0 - _selectivityWeight)\nin both branches of the if/else and instead assign the combined (in the first\nbranch) or softGrad (in the else branch) directly to gradient[i]; do not change\nComputeLoss here ΓÇö this ensures ComputeGradient is the true derivative of the\ncurrent loss.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                    "path": "src/KnowledgeDistillation/Strategies/NeuronSelectivityDistillationStrategy.cs",
                    "createdAt": "2025-11-13T00:00:02Z",
                    "author": {
                      "login": "coderabbitai"
                    }
                  }
                ]
              }
            },
            {
              "id": "PRRT_kwDOKSXUF85hwOJM",
              "isResolved": true,
              "comments": {
                "nodes": [
                  {
                    "id": "PRRC_kwDOKSXUF86WOK3i",
                    "body": "_≡ƒ¢á∩╕Å Refactor suggestion_ | _≡ƒƒá Major_\n\n**Consolidate duplicate softmax implementations.**\n\n`SoftmaxVector` duplicates the functionality of the `Softmax` method (lines 97-138) with subtle but important differences:\n\n1. **Missing validation**: No temperature check (could allow temperature Γëñ 0)\n2. **Inconsistent comparison**: Uses `GreaterThan` (line 167) vs `GreaterThanOrEquals` (line 114 in Softmax) when finding max logitΓÇöthis can yield different results when logits contain equal maximum values\n3. **Implementation divergence**: Uses `Convert.ToDouble` + `Math.Exp` (lines 163, 175-176) vs `NumOps` operations, potentially causing precision loss for certain numeric types\n\nSince `ApplyTemperatureSoftmax` now delegates to `SoftmaxVector`, consider:\n1. Removing the `Softmax` method if it's unused, or\n2. Making `SoftmaxVector` call `Softmax` internally to eliminate duplication and ensure consistent behavior\n\n\n\nApply this diff to consolidate by having `SoftmaxVector` delegate to `Softmax`:\n\n```diff\n private Vector<T> SoftmaxVector(Vector<T> logits, double temperature)\n {\n-    int n = logits.Length;\n-    var result = new Vector<T>(n);\n-    var scaled = new T[n];\n-\n-    for (int i = 0; i < n; i++)\n-        scaled[i] = NumOps.FromDouble(Convert.ToDouble(logits[i]) / temperature);\n-\n-    T maxLogit = scaled[0];\n-    for (int i = 1; i < n; i++)\n-        if (NumOps.GreaterThan(scaled[i], maxLogit))\n-            maxLogit = scaled[i];\n-\n-    T sum = NumOps.Zero;\n-    var expValues = new T[n];\n-\n-    for (int i = 0; i < n; i++)\n-    {\n-        double val = Convert.ToDouble(NumOps.Subtract(scaled[i], maxLogit));\n-        expValues[i] = NumOps.FromDouble(Math.Exp(val));\n-        sum = NumOps.Add(sum, expValues[i]);\n-    }\n-\n-    for (int i = 0; i < n; i++)\n-        result[i] = NumOps.Divide(expValues[i], sum);\n-\n-    return result;\n+    return Softmax(logits, temperature);\n }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    private Vector<T> SoftmaxVector(Vector<T> logits, double temperature)\n    {\n        return Softmax(logits, temperature);\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/TeacherModelBase.cs around lines 156 to 184,\nSoftmaxVector duplicates Softmax with subtle differences (no temperature\nvalidation, different max comparison, and using Convert.ToDouble/Math.Exp\ninstead of NumOps) causing inconsistent behavior and potential precision loss;\nupdate SoftmaxVector to simply call the existing Softmax method (passing logits\nand temperature) so all validation, numeric operations, and comparison logic are\ncentralized in Softmax, or if Softmax has a different signature adapt it to\naccept the Vector<T>/temperature and forward to it, then remove the duplicated\nimplementation body.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 5935afc to 8aeee73",
                    "path": "src/KnowledgeDistillation/TeacherModelBase.cs",
                    "createdAt": "2025-11-13T00:00:02Z",
                    "author": {
                      "login": "coderabbitai"
                    }
                  }
                ]
              }
            },
            {
              "id": "PRRT_kwDOKSXUF85hxJ4D",
              "isResolved": false,
              "comments": {
                "nodes": [
                  {
                    "id": "PRRC_kwDOKSXUF86WPuJ5",
                    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Documentation mismatch for Evaluate return value**\n\nComment says ΓÇ£Accuracy as a percentage (0ΓÇô100)ΓÇ¥ but the method returns a fraction (0ΓÇô1). Either multiply by 100 in the return or adjust the docstring.\n\n```diff\n-        return (double)correct / inputs.Length;\n+        return (double)correct / inputs.Length; // adjust XML doc to indicate 0ΓÇô1\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/KnowledgeDistillationTrainerBase.cs around lines\n336 to 361, the XML/docstring claims Evaluate returns accuracy as a percentage\n(0ΓÇô100) but the method currently returns a fraction (0.0ΓÇô1.0); update the\nimplementation to return percentage by multiplying the fraction by 100 before\nreturning (i.e., return ((double)correct / inputs.Length) * 100) or\nalternatively update the docstring to state it returns a fraction ΓÇö pick the\npercentage behavior and adjust the return accordingly so the code matches the\ndocumentation.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:chinchilla -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                    "path": "src/KnowledgeDistillation/KnowledgeDistillationTrainerBase.cs",
                    "createdAt": "2025-11-13T01:33:12Z",
                    "author": {
                      "login": "coderabbitai"
                    }
                  }
                ]
              }
            },
            {
              "id": "PRRT_kwDOKSXUF85hxJ4M",
              "isResolved": false,
              "comments": {
                "nodes": [
                  {
                    "id": "PRRC_kwDOKSXUF86WPuKI",
                    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Incomplete implementation creates confusing API.**\n\nThe constructor accepts `selectivityWeight` (lines 30-42) and validates it, but the TODOs at lines 48-51 and 79-82 acknowledge that selectivity is not integrated into the loss or gradient computations. The `_selectivityWeight` parameter has no effect on training, and `ComputeSelectivityLoss` exists but is never called by the strategy itself.\n\nThis creates a misleading API where users can configure selectivity behavior that won't actually work. Consider:\n1. **Complete the implementation**: Integrate `ComputeSelectivityLoss` into `ComputeLoss` and `ComputeGradient` as the TODOs suggest.\n2. **Mark as work-in-progress**: Add `[Obsolete]` or internal visibility with clear documentation.\n3. **Throw `NotImplementedException`**: Make it explicit in the constructor or methods that selectivity isn't yet supported.\n\n\n\nAlso applies to: 44-127\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/Strategies/NeuronSelectivityDistillationStrategy.cs\naround lines 30-42 and extending to lines 44-127, the constructor validates the\nselectivityWeight parameter but this parameter is never actually used in the\nloss or gradient computations, making the API misleading. Choose one of the\nfollowing approaches to fix: either complete the implementation by integrating\nComputeSelectivityLoss into both ComputeLoss and ComputeGradient methods as\nindicated by the existing TODOs, or alternatively mark the constructor and\nrelated methods with [Obsolete] attribute and clear documentation stating that\nselectivity integration is not yet implemented, or throw NotImplementedException\nin the constructor to explicitly signal that this feature is incomplete.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                    "path": "src/KnowledgeDistillation/Strategies/NeuronSelectivityDistillationStrategy.cs",
                    "createdAt": "2025-11-13T01:33:13Z",
                    "author": {
                      "login": "coderabbitai"
                    }
                  }
                ]
              }
            },
            {
              "id": "PRRT_kwDOKSXUF85hxJ4T",
              "isResolved": false,
              "comments": {
                "nodes": [
                  {
                    "id": "PRRC_kwDOKSXUF86WPuKV",
                    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Add dimension validation for activation vectors.**\n\n`ComputeSelectivityLoss` validates batch size (line 141) but doesn't verify that all activation vectors have consistent dimensions. If vectors have varying lengths, `ComputeSelectivityScores` will use an incorrect `numNeurons` value or cause index-out-of-range errors.\n\nPrevious review comments flagged this and it was marked as addressed, but the validation is still missing.\n\n\nAdd validation after line 148:\n\n```diff\n         int batchSize = studentActivations.Length;\n         int numNeurons = studentActivations[0].Length;\n+        \n+        // Validate all activation vectors have consistent dimensions\n+        for (int i = 0; i < batchSize; i++)\n+        {\n+            if (studentActivations[i].Length != numNeurons)\n+                throw new ArgumentException(\n+                    $\"All student activation vectors must have the same length. Expected {numNeurons}, got {studentActivations[i].Length} at index {i}\");\n+            if (teacherActivations[i].Length != numNeurons)\n+                throw new ArgumentException(\n+                    $\"All teacher activation vectors must have the same length. Expected {numNeurons}, got {teacherActivations[i].Length} at index {i}\");\n+            if (studentActivations[i].Length != teacherActivations[i].Length)\n+                throw new ArgumentException(\n+                    $\"Student and teacher activation dimensions must match at index {i}. Student: {studentActivations[i].Length}, Teacher: {teacherActivations[i].Length}\");\n+            if (studentActivations[i].Length == 0)\n+                throw new ArgumentException($\"Activation vector at index {i} cannot be empty\");\n+        }\n \n         // Compute selectivity for each neuron\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    public T ComputeSelectivityLoss(Vector<T>[] studentActivations, Vector<T>[] teacherActivations)\n    {\n        if (studentActivations.Length != teacherActivations.Length)\n            throw new ArgumentException(\"Student and teacher must have same batch size\");\n\n        if (studentActivations.Length == 0)\n            return NumOps.Zero;\n\n        int batchSize = studentActivations.Length;\n        int numNeurons = studentActivations[0].Length;\n        \n        // Validate all activation vectors have consistent dimensions\n        for (int i = 0; i < batchSize; i++)\n        {\n            if (studentActivations[i].Length != numNeurons)\n                throw new ArgumentException(\n                    $\"All student activation vectors must have the same length. Expected {numNeurons}, got {studentActivations[i].Length} at index {i}\");\n            if (teacherActivations[i].Length != numNeurons)\n                throw new ArgumentException(\n                    $\"All teacher activation vectors must have the same length. Expected {numNeurons}, got {teacherActivations[i].Length} at index {i}\");\n            if (studentActivations[i].Length != teacherActivations[i].Length)\n                throw new ArgumentException(\n                    $\"Student and teacher activation dimensions must match at index {i}. Student: {studentActivations[i].Length}, Teacher: {teacherActivations[i].Length}\");\n            if (studentActivations[i].Length == 0)\n                throw new ArgumentException($\"Activation vector at index {i} cannot be empty\");\n        }\n\n        // Compute selectivity for each neuron\n        var studentSelectivity = ComputeSelectivityScores(studentActivations, numNeurons);\n        var teacherSelectivity = ComputeSelectivityScores(teacherActivations, numNeurons);\n\n        // MSE between selectivity scores\n        T loss = NumOps.Zero;\n        for (int i = 0; i < numNeurons; i++)\n        {\n            var diff = studentSelectivity[i] - teacherSelectivity[i];\n            loss = NumOps.Add(loss, NumOps.FromDouble(diff * diff));\n        }\n\n        loss = NumOps.Divide(loss, NumOps.FromDouble(numNeurons));\n        return NumOps.Multiply(loss, NumOps.FromDouble(_selectivityWeight));\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/Strategies/NeuronSelectivityDistillationStrategy.cs\naround lines 139 to 164, the ComputeSelectivityLoss method is missing validation\nto ensure all activation vectors have consistent dimensions. After line 148\nwhere numNeurons is determined from studentActivations[0].Length, add a loop or\nLINQ check to validate that all vectors in both studentActivations and\nteacherActivations have the same length as numNeurons. If any vector has a\ndifferent length, throw an ArgumentException with a descriptive message to\nprevent index-out-of-range errors and ensure ComputeSelectivityScores receives\ncorrect dimensions.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                    "path": "src/KnowledgeDistillation/Strategies/NeuronSelectivityDistillationStrategy.cs",
                    "createdAt": "2025-11-13T01:33:13Z",
                    "author": {
                      "login": "coderabbitai"
                    }
                  }
                ]
              }
            },
            {
              "id": "PRRT_kwDOKSXUF85hxJ4g",
              "isResolved": false,
              "comments": {
                "nodes": [
                  {
                    "id": "PRRC_kwDOKSXUF86WPuKw",
                    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**CRITICAL: Developer acknowledges the bug flagged in past reviews remains unfixed.**\n\nThe TODO comment at lines 274-287 explicitly states the same critical issue raised in all previous review comments: relational loss is computed on samples 1-N but applied to samples N+1 to 2N. This breaks the fundamental contract of distillation and renders RKD ineffective.\n\n**Timeline for batch_size=32:**\n- Samples 1-31: Accumulate in buffers, return with NO relational loss\n- Sample 32: Triggers relational computation, receives 1/32 of loss, buffers cleared\n- Samples 33-64: Receive remaining 31/32 of loss computed from samples 1-32\n\nThis misalignment was flagged in every past review comment (all marked \"Γ£à Addressed\") yet the TODO proves it remains unresolved. Issue #408 requires functional RKD; this implementation cannot satisfy that requirement.\n\n**Required fix (as TODO notes):** Architectural change to apply relational loss to the samples that generated it, either by:\n- Switching to true batch-level API (return losses for entire batch simultaneously)\n- Buffering individual sample losses and retroactively adding relational contribution once batch completes\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                    "path": "src/KnowledgeDistillation/Strategies/RelationalDistillationStrategy.cs",
                    "createdAt": "2025-11-13T01:33:13Z",
                    "author": {
                      "login": "coderabbitai"
                    }
                  }
                ]
              }
            },
            {
              "id": "PRRT_kwDOKSXUF85hxJ4p",
              "isResolved": false,
              "comments": {
                "nodes": [
                  {
                    "id": "PRRC_kwDOKSXUF86WPuK_",
                    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Relational gradient suffers from same sample misalignment and uses improper averaging.**\n\nTwo issues compound the architectural problem:\n\n1. **Temporal misalignment:** Path 1 (lines 354-367) computes gradient while the batch is accumulating. When sample 32 arrives and `ComputeLoss` clears the buffers, `ComputeGradient` for sample 32 cannot use Path 1 (buffers empty) and falls back to Path 2, applying the cached gradient from the *previous* batch. Samples 33-64 also use Path 2, receiving gradients computed from batch 1-32.\n\n2. **Averaging approximation:** `_cachedRelationalGradient` (line 237) is an average across all batch samples (from `ComputeAverageRelationalGradientForBatch`). RKD gradients are sample-specificΓÇöeach sample's gradient should reflect its unique position in the relational graph (which pairs/triplets it participates in). Applying the same averaged gradient to all samples discards this structural information and weakens the RKD signal.\n\n**Correct approach:** Cache per-sample relational gradients (array of `Vector<T>`, not a single average) and apply each to its corresponding sample.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                    "path": "src/KnowledgeDistillation/Strategies/RelationalDistillationStrategy.cs",
                    "createdAt": "2025-11-13T01:33:13Z",
                    "author": {
                      "login": "coderabbitai"
                    }
                  }
                ]
              }
            },
            {
              "id": "PRRT_kwDOKSXUF85hxJ4r",
              "isResolved": false,
              "comments": {
                "nodes": [
                  {
                    "id": "PRRC_kwDOKSXUF86WPuLE",
                    "body": "_≡ƒ¢á∩╕Å Refactor suggestion_ | _≡ƒƒá Major_\n\n**Numerical gradient approximation is computationally expensive and less accurate.**\n\nComputing angle gradients via finite differences (lines 532-551) requires O(d) angle computations per triplet, where d is the embedding dimension. For d=512 and a batch of 32 samples with 10 triplets per sample (line 438), this results in ~163,000 angle computations per batch.\n\nAnalytical gradients for cosine-based angles can be derived using the chain rule on `acos(dot(u,v) / (||u|| ||v||))` and would be:\n- Faster: single pass instead of d passes\n- More accurate: no epsilon approximation error\n- Numerically stable: can handle edge cases explicitly\n\n\n\nConsider implementing analytical angle gradients:\n\n```csharp\n// Analytical gradient of angle(i,j,k) w.r.t. point i\nprivate Vector<T> ComputeTripletAngleGradientAnalytical(...)\n{\n    var ji = Subtract(vi, vj);\n    var jk = Subtract(vk, vj);\n    T dot = DotProduct(ji, jk);\n    T normJi = Norm(ji);\n    T normJk = Norm(jk);\n    \n    // Γêéangle/Γêévi = -1/sqrt(1-cos┬▓) * Γêécos/Γêévi\n    // where Γêécos/Γêévi = jk/(||ji||||jk||) - cos*ji/||ji||┬▓\n    // ... (expand chain rule)\n}\n```\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                    "path": "src/KnowledgeDistillation/Strategies/RelationalDistillationStrategy.cs",
                    "createdAt": "2025-11-13T01:33:13Z",
                    "author": {
                      "login": "coderabbitai"
                    }
                  }
                ]
              }
            },
            {
              "id": "PRRT_kwDOKSXUF85hxJ4u",
              "isResolved": false,
              "comments": {
                "nodes": [
                  {
                    "id": "PRRC_kwDOKSXUF86WPuLT",
                    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Avoid defaulting Self teacher to 10 outputs**\n\nTeacherModelType.Self => CreateSelfTeacher(outputDimension ?? 10) silently guesses class count. Require outputDimension and throw if null to prevent misconfigured training.\n\n```diff\n-            TeacherModelType.Self => CreateSelfTeacher(outputDimension ?? 10),\n+            TeacherModelType.Self => outputDimension.HasValue\n+                ? CreateSelfTeacher(outputDimension.Value)\n+                : throw new ArgumentException(\"Output dimension is required for Self teacher type\"),\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/TeacherModelFactory.cs at lines 44 to 59, the Self\nteacher type handler silently defaults outputDimension to 10 if null is passed,\nwhich can lead to misconfigured training with an incorrect class count\nassumption. Instead of using the null-coalescing operator with a hardcoded\ndefault value, add validation to check if outputDimension is null and throw an\nArgumentException with a descriptive message indicating that outputDimension is\nrequired for the Self teacher type. This ensures the caller explicitly provides\nthe correct output dimension rather than silently accepting an incorrect\ndefault.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:chinchilla -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                    "path": "src/KnowledgeDistillation/TeacherModelFactory.cs",
                    "createdAt": "2025-11-13T01:33:14Z",
                    "author": {
                      "login": "coderabbitai"
                    }
                  }
                ]
              }
            },
            {
              "id": "PRRT_kwDOKSXUF85hxJ4z",
              "isResolved": false,
              "comments": {
                "nodes": [
                  {
                    "id": "PRRC_kwDOKSXUF86WPuLn",
                    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Pass an explicit output dimension (donΓÇÖt rely on wrapper inference)**\n\nIf metadata is missing, TeacherModelWrapper(model) will throw. Require outputDimension and pass it via the forwardFunc overload to decouple from metadata.\n\n```diff\n-    if (model == null)\n-        throw new ArgumentException(\"Model is required for NeuralNetwork teacher type\");\n-\n-    return new TeacherModelWrapper<T>(model);\n+    if (model == null)\n+        throw new ArgumentException(\"Model is required for NeuralNetwork teacher type\");\n+    if (!outputDimension.HasValue)\n+        throw new ArgumentException(\"Output dimension is required for NeuralNetwork teacher type\");\n+    return new TeacherModelWrapper<T>(model.Predict, outputDimension.Value);\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/TeacherModelFactory.cs around lines 61 to 69: the\ncurrent CreateNeuralNetworkTeacher relies on TeacherModelWrapper to infer output\ndimension from model metadata which may be missing; require a non-null\noutputDimension (throw ArgumentException if null) and construct the\nTeacherModelWrapper using the overload that accepts an explicit forward function\nand output dimension (i.e. provide a forwardFunc that delegates to model.Forward\nand pass the provided outputDimension) so the wrapper no longer depends on\nmetadata.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:chinchilla -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                    "path": "src/KnowledgeDistillation/TeacherModelFactory.cs",
                    "createdAt": "2025-11-13T01:33:14Z",
                    "author": {
                      "login": "coderabbitai"
                    }
                  }
                ]
              }
            },
            {
              "id": "PRRT_kwDOKSXUF85hxJ44",
              "isResolved": false,
              "comments": {
                "nodes": [
                  {
                    "id": "PRRC_kwDOKSXUF86WPuLz",
                    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Do not default OutputDimension to 10 for TeacherForward**\n\nDefaulting to 10 risks mis-sized one-hot labels and corrupted training. Require options.OutputDimension when using TeacherForward and fail fast if missing.\n\n```diff\n-                int outputDim = options.OutputDimension ?? 10;\n+                if (!options.OutputDimension.HasValue)\n+                    throw new InvalidOperationException(\n+                        \"OutputDimension is required when using TeacherForward.\");\n+                int outputDim = options.OutputDimension.Value;\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/PredictionModelBuilder.cs around lines 1064 to 1076, the code currently\ndefaults options.OutputDimension to 10 when TeacherForward is used which can\ncreate mis-sized one-hot labels; instead, validate that options.OutputDimension\nis set and throw a clear InvalidOperationException if it is null. Remove the\ndefault assignment to 10, check options.OutputDimension.HasValue before\nconstructing the TeacherModelWrapper, and include the actual missing-field\nmessage (e.g., \"OutputDimension must be provided when using TeacherForward.\") so\nthe build fails fast with a descriptive error.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:chinchilla -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                    "path": "src/PredictionModelBuilder.cs",
                    "createdAt": "2025-11-13T01:33:14Z",
                    "author": {
                      "login": "coderabbitai"
                    }
                  }
                ]
              }
            },
            {
              "id": "PRRT_kwDOKSXUF85hxJ4-",
              "isResolved": false,
              "comments": {
                "nodes": [
                  {
                    "id": "PRRC_kwDOKSXUF86WPuL8",
                    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Avoid re-running Optimize() after manual KD training**\n\nCalling optimizer.Optimize here can overwrite KD-updated weights or retrain the model a second time. Return an OptimizationResult that reflects the current (KDΓÇætrained) model state instead of invoking Optimize again.\n\nSuggestion: construct and return an OptimizationResult<T,TInput,TOutput> from the current model parameters and any tracked metrics, or add a helper like OptimizerHelper.CreateResultFromTrainedModel(...) and use that.\n\n<!-- fingerprinting:phantom:poseidon:chinchilla -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
                    "path": "src/PredictionModelBuilder.cs",
                    "createdAt": "2025-11-13T01:33:14Z",
                    "author": {
                      "login": "coderabbitai"
                    }
                  }
                ]
              }
            }
          ]
        }
      }
    }
  }
}
