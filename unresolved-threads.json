[
  {
    "id": "PRRT_kwDOKSXUF85hcSfJ",
    "isResolved": false,
    "isOutdated": true,
    "path": "src/KnowledgeDistillation/TeacherModelBase.cs",
    "line": null,
    "originalLine": 117,
    "comments": {
      "nodes": [
        {
          "id": "PRRC_kwDOKSXUF86VyrhR",
          "author": {
            "login": "coderabbitai"
          },
          "body": "_≡ƒ¢á∩╕Å Refactor suggestion_ | _≡ƒƒá Major_\n\n**Provide a default Vector\\<T\\> softmax implementation to reduce duplication.**\n\n`ApplyTemperatureSoftmax` is declared abstract, forcing every subclass to implement temperature-scaled softmax. As a result, `CurriculumTeacherModel`, `QuantizedTeacherModel`, `TransformerTeacherModel`, `OnlineTeacherModel`, and strategy classes all duplicate the identical 30-line softmax logic. Since all current implementations use `Vector<T>` for `TOutput`, consider making this method virtual with a concrete default implementation.\n\n\n\nChange the abstract method to a virtual protected helper:\n\n```diff\n-    protected abstract TOutput ApplyTemperatureSoftmax(TOutput logits, double temperature);\n+    protected virtual TOutput ApplyTemperatureSoftmax(TOutput logits, double temperature)\n+    {\n+        // Default implementation for Vector<T>\n+        if (logits is not Vector<T> vectorLogits)\n+            throw new NotSupportedException(\n+                $\"Default softmax implementation only supports Vector<T>. Override ApplyTemperatureSoftmax for {typeof(TOutput).Name}.\");\n+\n+        return (TOutput)(object)SoftmaxVector(vectorLogits, temperature);\n+    }\n+\n+    private Vector<T> SoftmaxVector(Vector<T> logits, double temperature)\n+    {\n+        int n = logits.Length;\n+        var result = new Vector<T>(n);\n+        var scaled = new T[n];\n+\n+        for (int i = 0; i < n; i++)\n+            scaled[i] = NumOps.FromDouble(Convert.ToDouble(logits[i]) / temperature);\n+\n+        T maxLogit = scaled[0];\n+        for (int i = 1; i < n; i++)\n+            if (NumOps.GreaterThan(scaled[i], maxLogit))\n+                maxLogit = scaled[i];\n+\n+        T sum = NumOps.Zero;\n+        var expValues = new T[n];\n+\n+        for (int i = 0; i < n; i++)\n+        {\n+            double val = Convert.ToDouble(NumOps.Subtract(scaled[i], maxLogit));\n+            expValues[i] = NumOps.FromDouble(Math.Exp(val));\n+            sum = NumOps.Add(sum, expValues[i]);\n+        }\n+\n+        for (int i = 0; i < n; i++)\n+            result[i] = NumOps.Divide(expValues[i], sum);\n+\n+        return result;\n+    }\n```\n\nThen remove the duplicated softmax methods from all subclasses.\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    /// <summary>\n    /// Applies temperature-scaled softmax to logits. Must be implemented by subclasses\n    /// based on their output type (Vector, Matrix, etc.).\n    /// </summary>\n    /// <param name=\"logits\">Raw model outputs.</param>\n    /// <param name=\"temperature\">Temperature for scaling.</param>\n    /// <returns>Probability distribution.</returns>\n    protected virtual TOutput ApplyTemperatureSoftmax(TOutput logits, double temperature)\n    {\n        // Default implementation for Vector<T>\n        if (logits is not Vector<T> vectorLogits)\n            throw new NotSupportedException(\n                $\"Default softmax implementation only supports Vector<T>. Override ApplyTemperatureSoftmax for {typeof(TOutput).Name}.\");\n\n        return (TOutput)(object)SoftmaxVector(vectorLogits, temperature);\n    }\n\n    private Vector<T> SoftmaxVector(Vector<T> logits, double temperature)\n    {\n        int n = logits.Length;\n        var result = new Vector<T>(n);\n        var scaled = new T[n];\n\n        for (int i = 0; i < n; i++)\n            scaled[i] = NumOps.FromDouble(Convert.ToDouble(logits[i]) / temperature);\n\n        T maxLogit = scaled[0];\n        for (int i = 1; i < n; i++)\n            if (NumOps.GreaterThan(scaled[i], maxLogit))\n                maxLogit = scaled[i];\n\n        T sum = NumOps.Zero;\n        var expValues = new T[n];\n\n        for (int i = 0; i < n; i++)\n        {\n            double val = Convert.ToDouble(NumOps.Subtract(scaled[i], maxLogit));\n            expValues[i] = NumOps.FromDouble(Math.Exp(val));\n            sum = NumOps.Add(sum, expValues[i]);\n        }\n\n        for (int i = 0; i < n; i++)\n            result[i] = NumOps.Divide(expValues[i], sum);\n\n        return result;\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/TeacherModelBase.cs around lines 110 to 117, change\nthe abstract ApplyTemperatureSoftmax signature to a protected virtual method and\nprovide a default implementation that handles the common Vector<T> case: cast\nlogits to Vector<T>, convert to doubles, divide by temperature, apply\nnumerically-stable softmax (subtract max, exp, normalize), convert results back\nto T and wrap in the same Vector<T> return type; if logits are not a Vector<T>\nthrow/return a clear NotSupported/invalid-operation to preserve behavior. After\nadding this default, remove the duplicated softmax implementations from\nCurriculumTeacherModel, QuantizedTeacherModel, TransformerTeacherModel,\nOnlineTeacherModel and strategy classes so they inherit the base implementation.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
          "createdAt": "2025-11-11T07:10:59Z",
          "path": "src/KnowledgeDistillation/TeacherModelBase.cs",
          "line": null,
          "diffHunk": "@@ -0,0 +1,137 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.KnowledgeDistillation;\n+\n+/// <summary>\n+/// Abstract base class for teacher models used in knowledge distillation.\n+/// Provides common functionality for extracting logits, soft predictions, and features from trained models.\n+/// </summary>\n+/// <typeparam name=\"TInput\">The input data type (e.g., Vector, Matrix, Tensor).</typeparam>\n+/// <typeparam name=\"TOutput\">The output data type (typically logits or probabilities as Vector or Matrix).</typeparam>\n+/// <typeparam name=\"T\">The numeric type for calculations (e.g., double, float).</typeparam>\n+/// <remarks>\n+/// <para><b>For Beginners:</b> This base class provides common functionality that all teacher models need.\n+/// Instead of each teacher implementation writing the same code for softmax and temperature scaling,\n+/// they inherit these capabilities from this base class.</para>\n+///\n+/// <para><b>Why use a base class?</b>\n+/// - **Code Reuse**: Common operations like softmax are implemented once\n+/// - **Consistency**: All teachers behave the same way for standard operations\n+/// - **Extensibility**: New teacher types only need to implement model-specific logic\n+/// - **Maintainability**: Bug fixes in base class benefit all implementations</para>\n+///\n+/// <para><b>Design Pattern:</b> This implements the Template Method pattern, where the base class\n+/// defines the algorithm structure (e.g., GetSoftPredictions), and subclasses fill in specific steps\n+/// (e.g., GetLogits implementation).</para>\n+/// </remarks>\n+public abstract class TeacherModelBase<TInput, TOutput, T> : ITeacherModel<TInput, TOutput>\n+{\n+    /// <summary>\n+    /// Numeric operations for the specified type T.\n+    /// </summary>\n+    protected readonly INumericOperations<T> NumOps;\n+\n+    /// <summary>\n+    /// Gets the number of output dimensions (e.g., number of classes for classification).\n+    /// </summary>\n+    public abstract int OutputDimension { get; }\n+\n+    /// <summary>\n+    /// Initializes the base teacher model.\n+    /// </summary>\n+    protected TeacherModelBase()\n+    {\n+        NumOps = MathHelper.GetNumericOperations<T>();\n+    }\n+\n+    /// <summary>\n+    /// Gets the teacher's raw logits (pre-softmax outputs) for the given input.\n+    /// </summary>\n+    /// <param name=\"input\">The input data to process.</param>\n+    /// <returns>Raw logits before applying softmax.</returns>\n+    /// <remarks>\n+    /// <para><b>For Implementers:</b> Override this method to extract logits from your specific model type.\n+    /// Ensure you return pre-activation outputs, not probabilities.</para>\n+    /// </remarks>\n+    public abstract TOutput GetLogits(TInput input);\n+\n+    /// <summary>\n+    /// Gets the teacher's soft predictions (probabilities) with temperature scaling.\n+    /// </summary>\n+    /// <param name=\"input\">The input data to process.</param>\n+    /// <param name=\"temperature\">Softmax temperature (default 1.0). Higher values produce softer distributions.</param>\n+    /// <returns>Probability distribution with temperature scaling applied.</returns>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This method applies temperature scaling to logits and converts them\n+    /// to probabilities. The base class handles this automatically - subclasses just need to provide logits.</para>\n+    /// </remarks>\n+    public virtual TOutput GetSoftPredictions(TInput input, double temperature = 1.0)\n+    {\n+        if (temperature <= 0)\n+            throw new ArgumentException(\"Temperature must be positive\", nameof(temperature));\n+\n+        var logits = GetLogits(input);\n+        return ApplyTemperatureSoftmax(logits, temperature);\n+    }\n+\n+    /// <summary>\n+    /// Gets intermediate layer features from the teacher model.\n+    /// </summary>\n+    /// <param name=\"input\">The input data to process.</param>\n+    /// <param name=\"layerName\">The name of the layer to extract features from.</param>\n+    /// <returns>Feature map from the specified layer, or null if not supported.</returns>\n+    /// <remarks>\n+    /// <para><b>For Implementers:</b> Override this method if your model supports feature extraction.\n+    /// Return null if feature extraction is not applicable for your model type.</para>\n+    /// </remarks>\n+    public virtual object? GetFeatures(TInput input, string layerName)\n+    {\n+        // Default implementation: feature extraction not supported\n+        return null;\n+    }\n+\n+    /// <summary>\n+    /// Gets attention weights from the teacher model (for transformer architectures).\n+    /// </summary>\n+    /// <param name=\"input\">The input data to process.</param>\n+    /// <param name=\"layerName\">The name of the attention layer to extract weights from.</param>\n+    /// <returns>Attention weight matrix from the specified layer, or null if not supported.</returns>\n+    /// <remarks>\n+    /// <para><b>For Implementers:</b> Override this method if your model has attention mechanisms.\n+    /// This is primarily for transformer-based models like BERT, GPT, etc.</para>\n+    /// </remarks>\n+    public virtual object? GetAttentionWeights(TInput input, string layerName)\n+    {\n+        // Default implementation: attention extraction not supported\n+        return null;\n+    }\n+\n+    /// <summary>\n+    /// Applies temperature-scaled softmax to logits. Must be implemented by subclasses\n+    /// based on their output type (Vector, Matrix, etc.).\n+    /// </summary>\n+    /// <param name=\"logits\">Raw model outputs.</param>\n+    /// <param name=\"temperature\">Temperature for scaling.</param>\n+    /// <returns>Probability distribution.</returns>\n+    protected abstract TOutput ApplyTemperatureSoftmax(TOutput logits, double temperature);"
        }
      ]
    }
  },
  {
    "id": "PRRT_kwDOKSXUF85hcSfV",
    "isResolved": false,
    "isOutdated": true,
    "path": "src/KnowledgeDistillation/Teachers/AdaptiveTeacherModel.cs",
    "line": null,
    "originalLine": 58,
    "comments": {
      "nodes": [
        {
          "id": "PRRC_kwDOKSXUF86Vyrhe",
          "author": {
            "login": "coderabbitai"
          },
          "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Consider thread safety for StudentPerformance dictionary.**\n\nThe `StudentPerformance` dictionary is publicly mutable and modified in `UpdateStudentPerformance`. In multi-threaded training scenarios (parallel batch processing), concurrent access could cause race conditions.\n\n\n\nConsider using `ConcurrentDictionary<int, double>` if parallel training is anticipated:\n\n```diff\n-    public Dictionary<int, double> StudentPerformance { get; set; } = new();\n+    public System.Collections.Concurrent.ConcurrentDictionary<int, double> StudentPerformance { get; set; } = new();\n```\n\nAnd update the EMA logic to use `AddOrUpdate`:\n\n```diff\n-        // Exponential moving average\n-        if (StudentPerformance.ContainsKey(sampleIndex))\n-        {\n-            StudentPerformance[sampleIndex] =\n-                _adaptationRate * performance + (1 - _adaptationRate) * StudentPerformance[sampleIndex];\n-        }\n-        else\n-        {\n-            StudentPerformance[sampleIndex] = performance;\n-        }\n+        // Exponential moving average\n+        StudentPerformance.AddOrUpdate(\n+            sampleIndex,\n+            performance,\n+            (key, oldValue) => _adaptationRate * performance + (1 - _adaptationRate) * oldValue);\n```\n\n\nAlso applies to: 165-173\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/Teachers/AdaptiveTeacherModel.cs around lines 58\nand 165-173, the publicly mutable StudentPerformance Dictionary is not\nthread-safe; replace it with a ConcurrentDictionary<int,double> (update the\nproperty type and initializer) and change UpdateStudentPerformance to use\nConcurrentDictionary.AddOrUpdate with a valueFactory and updateFactory that\nimplements the EMA update (new = alpha * observed + (1 - alpha) * existing) so\nupdates are atomic and race-free; ensure any reads use the\nConcurrentDictionary's TryGetValue or indexer as appropriate.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
          "createdAt": "2025-11-11T07:11:00Z",
          "path": "src/KnowledgeDistillation/Teachers/AdaptiveTeacherModel.cs",
          "line": null,
          "diffHunk": "@@ -0,0 +1,325 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+\n+namespace AiDotNet.KnowledgeDistillation.Teachers;\n+\n+/// <summary>\n+/// Adaptive teacher model that adjusts its teaching strategy based on student performance.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type for calculations (e.g., double, float).</typeparam>\n+/// <remarks>\n+/// <para><b>For Beginners:</b> Adaptive distillation is like a tutor who adjusts their teaching\n+/// style based on how well the student is doing. If the student is struggling, the teacher provides\n+/// more detailed guidance. If the student is doing well, the teacher can provide more advanced knowledge.</para>\n+///\n+/// <para><b>How It Works:</b>\n+/// - Monitor student's accuracy/confidence on different samples\n+/// - For hard samples (student struggles): Use lower temperature, focus on correct class\n+/// - For easy samples (student confident): Use higher temperature, reveal more class relationships\n+/// - Dynamically adjust temperature per sample based on student performance</para>\n+///\n+/// <para><b>Real-world Analogy:</b>\n+/// A math tutor notices a student struggling with algebra but excelling at geometry.\n+/// The tutor spends more time on algebra fundamentals while letting the student explore\n+/// advanced geometry concepts independently.</para>\n+///\n+/// <para><b>Benefits:</b>\n+/// - **Curriculum Learning**: Automatically adjusts difficulty\n+/// - **Efficient Training**: Focus teaching effort where needed\n+/// - **Better Convergence**: Avoids overwhelming student early\n+/// - **Personalized**: Adapts to this specific student</para>\n+///\n+/// <para><b>Adaptation Strategies:</b>\n+/// - **Sample-wise**: Different temperature per sample\n+/// - **Class-wise**: Different approach per class\n+/// - **Epoch-wise**: Adjust over training progress\n+/// - **Confidence-based**: Based on student's prediction confidence</para>\n+///\n+/// <para><b>References:</b>\n+/// - Mirzadeh et al. (2020). Improved Knowledge Distillation via Teacher Assistant. AAAI.\n+/// - Zhou et al. (2021). Rethinking Soft Labels for Knowledge Distillation: A Bias-Variance Tradeoff Perspective.</para>\n+/// </remarks>\n+public class AdaptiveTeacherModel<T> : TeacherModelBase<Vector<T>, Vector<T>, T>\n+{\n+    private readonly ITeacherModel<Vector<T>, Vector<T>> _baseTeacher;\n+    private readonly AdaptiveStrategy _strategy;\n+    private readonly double _minTemperature;\n+    private readonly double _maxTemperature;\n+    private readonly double _adaptationRate;\n+\n+    /// <summary>\n+    /// Gets the output dimension.\n+    /// </summary>\n+    public override int OutputDimension => _baseTeacher.OutputDimension;\n+\n+    /// <summary>\n+    /// Gets or sets the student performance tracker (confidence/accuracy per sample).\n+    /// </summary>\n+    public Dictionary<int, double> StudentPerformance { get; set; } = new();"
        }
      ]
    }
  },
  {
    "id": "PRRT_kwDOKSXUF85hcSfb",
    "isResolved": false,
    "isOutdated": true,
    "path": "src/KnowledgeDistillation/Teachers/AdaptiveTeacherModel.cs",
    "line": null,
    "originalLine": 156,
    "comments": {
      "nodes": [
        {
          "id": "PRRC_kwDOKSXUF86Vyrhm",
          "author": {
            "login": "coderabbitai"
          },
          "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Silent failure when trueLabel is missing for AccuracyBased strategy.**\n\nWhen `_strategy` is `AccuracyBased` and `trueLabel` is null, the performance update is silently skipped (performance remains 0). This could lead to confusing behavior where the adaptive mechanism doesn't work as expected.\n\n\n\nConsider warning or throwing when required data is missing:\n\n```diff\n             case AdaptiveStrategy.AccuracyBased:\n-                if (trueLabel != null)\n-                {\n-                    // Correctness as performance measure\n-                    performance = IsCorrect(studentPrediction, trueLabel) ? 1.0 : 0.0;\n-                }\n+                if (trueLabel == null)\n+                    throw new ArgumentNullException(nameof(trueLabel), \n+                        \"True label required for AccuracyBased strategy\");\n+                \n+                // Correctness as performance measure\n+                performance = IsCorrect(studentPrediction, trueLabel) ? 1.0 : 0.0;\n                 break;\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n            case AdaptiveStrategy.AccuracyBased:\n                if (trueLabel == null)\n                    throw new ArgumentNullException(nameof(trueLabel), \n                        \"True label required for AccuracyBased strategy\");\n                \n                // Correctness as performance measure\n                performance = IsCorrect(studentPrediction, trueLabel) ? 1.0 : 0.0;\n                break;\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/Teachers/AdaptiveTeacherModel.cs around lines 150\nto 156, the AccuracyBased branch silently skips updating performance when\ntrueLabel is null; change this to explicitly handle the missing label by either\nlogging a warning (with context: strategy name, studentPrediction) or throwing\nan ArgumentNullException so callers know required data is missing; implement the\nchosen approach (prefer a warning to avoid breaking runtime) and ensure\nperformance is set to a defined fallback only after logging/throwing.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
          "createdAt": "2025-11-11T07:11:00Z",
          "path": "src/KnowledgeDistillation/Teachers/AdaptiveTeacherModel.cs",
          "line": null,
          "diffHunk": "@@ -0,0 +1,325 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+\n+namespace AiDotNet.KnowledgeDistillation.Teachers;\n+\n+/// <summary>\n+/// Adaptive teacher model that adjusts its teaching strategy based on student performance.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type for calculations (e.g., double, float).</typeparam>\n+/// <remarks>\n+/// <para><b>For Beginners:</b> Adaptive distillation is like a tutor who adjusts their teaching\n+/// style based on how well the student is doing. If the student is struggling, the teacher provides\n+/// more detailed guidance. If the student is doing well, the teacher can provide more advanced knowledge.</para>\n+///\n+/// <para><b>How It Works:</b>\n+/// - Monitor student's accuracy/confidence on different samples\n+/// - For hard samples (student struggles): Use lower temperature, focus on correct class\n+/// - For easy samples (student confident): Use higher temperature, reveal more class relationships\n+/// - Dynamically adjust temperature per sample based on student performance</para>\n+///\n+/// <para><b>Real-world Analogy:</b>\n+/// A math tutor notices a student struggling with algebra but excelling at geometry.\n+/// The tutor spends more time on algebra fundamentals while letting the student explore\n+/// advanced geometry concepts independently.</para>\n+///\n+/// <para><b>Benefits:</b>\n+/// - **Curriculum Learning**: Automatically adjusts difficulty\n+/// - **Efficient Training**: Focus teaching effort where needed\n+/// - **Better Convergence**: Avoids overwhelming student early\n+/// - **Personalized**: Adapts to this specific student</para>\n+///\n+/// <para><b>Adaptation Strategies:</b>\n+/// - **Sample-wise**: Different temperature per sample\n+/// - **Class-wise**: Different approach per class\n+/// - **Epoch-wise**: Adjust over training progress\n+/// - **Confidence-based**: Based on student's prediction confidence</para>\n+///\n+/// <para><b>References:</b>\n+/// - Mirzadeh et al. (2020). Improved Knowledge Distillation via Teacher Assistant. AAAI.\n+/// - Zhou et al. (2021). Rethinking Soft Labels for Knowledge Distillation: A Bias-Variance Tradeoff Perspective.</para>\n+/// </remarks>\n+public class AdaptiveTeacherModel<T> : TeacherModelBase<Vector<T>, Vector<T>, T>\n+{\n+    private readonly ITeacherModel<Vector<T>, Vector<T>> _baseTeacher;\n+    private readonly AdaptiveStrategy _strategy;\n+    private readonly double _minTemperature;\n+    private readonly double _maxTemperature;\n+    private readonly double _adaptationRate;\n+\n+    /// <summary>\n+    /// Gets the output dimension.\n+    /// </summary>\n+    public override int OutputDimension => _baseTeacher.OutputDimension;\n+\n+    /// <summary>\n+    /// Gets or sets the student performance tracker (confidence/accuracy per sample).\n+    /// </summary>\n+    public Dictionary<int, double> StudentPerformance { get; set; } = new();\n+\n+    /// <summary>\n+    /// Initializes a new instance of the AdaptiveTeacherModel class.\n+    /// </summary>\n+    /// <param name=\"baseTeacher\">The underlying teacher model.</param>\n+    /// <param name=\"strategy\">Adaptation strategy to use.</param>\n+    /// <param name=\"minTemperature\">Minimum temperature for hard samples (default: 1.0).</param>\n+    /// <param name=\"maxTemperature\">Maximum temperature for easy samples (default: 5.0).</param>\n+    /// <param name=\"adaptationRate\">How quickly to adapt (default: 0.1).</param>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> Create an adaptive teacher by providing:\n+    /// - Base teacher: The underlying expert model\n+    /// - Strategy: How to adapt (confidence-based recommended)\n+    /// - Temperature range: Control softness (1-5 typical)</para>\n+    ///\n+    /// <para>Example:\n+    /// <code>\n+    /// var baseTeacher = new TeacherModelWrapper&lt;double&gt;(trainedModel);\n+    /// var adaptiveTeacher = new AdaptiveTeacherModel&lt;double&gt;(\n+    ///     baseTeacher,\n+    ///     AdaptiveStrategy.ConfidenceBased,\n+    ///     minTemperature: 1.0,  // Sharp for hard samples\n+    ///     maxTemperature: 5.0   // Soft for easy samples\n+    /// );\n+    /// </code>\n+    /// </para>\n+    /// </remarks>\n+    public AdaptiveTeacherModel(\n+        ITeacherModel<Vector<T>, Vector<T>> baseTeacher,\n+        AdaptiveStrategy strategy = AdaptiveStrategy.ConfidenceBased,\n+        double minTemperature = 1.0,\n+        double maxTemperature = 5.0,\n+        double adaptationRate = 0.1)\n+    {\n+        _baseTeacher = baseTeacher ?? throw new ArgumentNullException(nameof(baseTeacher));\n+        _strategy = strategy;\n+        _minTemperature = minTemperature;\n+        _maxTemperature = maxTemperature;\n+        _adaptationRate = adaptationRate;\n+\n+        if (minTemperature <= 0 || maxTemperature <= minTemperature)\n+            throw new ArgumentException(\"Invalid temperature range\");\n+        if (adaptationRate <= 0 || adaptationRate > 1)\n+            throw new ArgumentException(\"Adaptation rate must be in (0, 1]\");\n+    }\n+\n+    /// <summary>\n+    /// Gets logits from the base teacher.\n+    /// </summary>\n+    public override Vector<T> GetLogits(Vector<T> input)\n+    {\n+        return _baseTeacher.GetLogits(input);\n+    }\n+\n+    /// <summary>\n+    /// Gets soft predictions with adaptive temperature based on sample difficulty.\n+    /// </summary>\n+    /// <param name=\"input\">Input data.</param>\n+    /// <param name=\"temperature\">Base temperature (will be adapted).</param>\n+    /// <returns>Soft predictions with adaptive temperature.</returns>\n+    public override Vector<T> GetSoftPredictions(Vector<T> input, double temperature = 1.0)\n+    {\n+        var logits = GetLogits(input);\n+\n+        // Compute adaptive temperature based on strategy\n+        double adaptiveTemp = ComputeAdaptiveTemperature(logits, temperature);\n+\n+        return ApplyTemperatureSoftmax(logits, adaptiveTemp);\n+    }\n+\n+    /// <summary>\n+    /// Updates student performance tracking for adaptive adjustment.\n+    /// </summary>\n+    /// <param name=\"sampleIndex\">Index of the sample in batch.</param>\n+    /// <param name=\"studentPrediction\">Student's prediction.</param>\n+    /// <param name=\"trueLabel\">True label (optional).</param>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> Call this after each prediction to help the teacher\n+    /// learn which samples the student finds difficult.</para>\n+    /// </remarks>\n+    public void UpdateStudentPerformance(int sampleIndex, Vector<T> studentPrediction, Vector<T>? trueLabel = null)\n+    {\n+        double performance = 0;\n+\n+        switch (_strategy)\n+        {\n+            case AdaptiveStrategy.ConfidenceBased:\n+                // Max confidence as performance measure\n+                performance = GetMaxConfidence(studentPrediction);\n+                break;\n+\n+            case AdaptiveStrategy.AccuracyBased:\n+                if (trueLabel != null)\n+                {\n+                    // Correctness as performance measure\n+                    performance = IsCorrect(studentPrediction, trueLabel) ? 1.0 : 0.0;\n+                }\n+                break;"
        }
      ]
    }
  },
  {
    "id": "PRRT_kwDOKSXUF85hcSfs",
    "isResolved": false,
    "isOutdated": true,
    "path": "src/KnowledgeDistillation/Teachers/CurriculumTeacherModel.cs",
    "line": null,
    "originalLine": 67,
    "comments": {
      "nodes": [
        {
          "id": "PRRC_kwDOKSXUF86Vyrh5",
          "author": {
            "login": "coderabbitai"
          },
          "body": "_≡ƒ¢á∩╕Å Refactor suggestion_ | _≡ƒƒá Major_\n\n**Extract duplicated Softmax to a shared helper.**\n\nThe `Softmax(Vector<T> logits, double temperature)` implementation at lines 39-67 is duplicated across `CurriculumTeacherModel`, `QuantizedTeacherModel`, `TransformerTeacherModel`, `OnlineTeacherModel`, and `ProbabilisticDistillationStrategy`. This violates DRY and means bug fixes (e.g., numerical stability improvements) must be applied in five places.\n\n\n\nMove the softmax logic to `TeacherModelBase<TInput, TOutput, T>` as a protected helper or to a static utility class:\n\n```diff\n+// In TeacherModelBase.cs or a new MathUtilities.cs\n+protected Vector<T> Softmax(Vector<T> logits, double temperature)\n+{\n+    int n = logits.Length;\n+    var result = new Vector<T>(n);\n+    var scaled = new T[n];\n+\n+    for (int i = 0; i < n; i++)\n+        scaled[i] = NumOps.FromDouble(Convert.ToDouble(logits[i]) / temperature);\n+\n+    T maxLogit = scaled[0];\n+    for (int i = 1; i < n; i++)\n+        if (NumOps.GreaterThan(scaled[i], maxLogit))\n+            maxLogit = scaled[i];\n+\n+    T sum = NumOps.Zero;\n+    var expValues = new T[n];\n+\n+    for (int i = 0; i < n; i++)\n+    {\n+        double val = Convert.ToDouble(NumOps.Subtract(scaled[i], maxLogit));\n+        expValues[i] = NumOps.FromDouble(Math.Exp(val));\n+        sum = NumOps.Add(sum, expValues[i]);\n+    }\n+\n+    for (int i = 0; i < n; i++)\n+        result[i] = NumOps.Divide(expValues[i], sum);\n+\n+    return result;\n+}\n```\n\nThen replace all five copies with calls to the shared implementation.\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
          "createdAt": "2025-11-11T07:11:00Z",
          "path": "src/KnowledgeDistillation/Teachers/CurriculumTeacherModel.cs",
          "line": null,
          "diffHunk": "@@ -0,0 +1,74 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+\n+namespace AiDotNet.KnowledgeDistillation.Teachers;\n+\n+/// <summary>\n+/// Curriculum teacher that gradually increases task difficulty during training.\n+/// </summary>\n+public class CurriculumTeacherModel<T> : TeacherModelBase<Vector<T>, Vector<T>, T>\n+{\n+    private readonly ITeacherModel<Vector<T>, Vector<T>> _baseTeacher;\n+    private readonly CurriculumStrategy _strategy;\n+    private double _currentDifficulty;\n+\n+    public override int OutputDimension => _baseTeacher.OutputDimension;\n+\n+    public CurriculumTeacherModel(\n+        ITeacherModel<Vector<T>, Vector<T>> baseTeacher,\n+        CurriculumStrategy strategy = CurriculumStrategy.EasyToHard)\n+    {\n+        _baseTeacher = baseTeacher ?? throw new ArgumentNullException(nameof(baseTeacher));\n+        _strategy = strategy;\n+        _currentDifficulty = 0.0;\n+    }\n+\n+    public void UpdateDifficulty(double difficulty) => _currentDifficulty = MathHelper.Clamp(difficulty, 0.0, 1.0);\n+\n+    public override Vector<T> GetLogits(Vector<T> input) => _baseTeacher.GetLogits(input);\n+\n+    protected override Vector<T> ApplyTemperatureSoftmax(Vector<T> logits, double temperature)\n+    {\n+        double adjustedTemp = _strategy == CurriculumStrategy.EasyToHard\n+            ? temperature * (1.0 + _currentDifficulty)\n+            : temperature * (2.0 - _currentDifficulty);\n+\n+        return Softmax(logits, adjustedTemp);\n+    }\n+\n+    private Vector<T> Softmax(Vector<T> logits, double temperature)\n+    {\n+        int n = logits.Length;\n+        var result = new Vector<T>(n);\n+        var scaled = new T[n];\n+\n+        for (int i = 0; i < n; i++)\n+            scaled[i] = NumOps.FromDouble(Convert.ToDouble(logits[i]) / temperature);\n+\n+        T maxLogit = scaled[0];\n+        for (int i = 1; i < n; i++)\n+            if (NumOps.GreaterThan(scaled[i], maxLogit))\n+                maxLogit = scaled[i];\n+\n+        T sum = NumOps.Zero;\n+        var expValues = new T[n];\n+\n+        for (int i = 0; i < n; i++)\n+        {\n+            double val = Convert.ToDouble(NumOps.Subtract(scaled[i], maxLogit));\n+            expValues[i] = NumOps.FromDouble(Math.Exp(val));\n+            sum = NumOps.Add(sum, expValues[i]);\n+        }\n+\n+        for (int i = 0; i < n; i++)\n+            result[i] = NumOps.Divide(expValues[i], sum);\n+\n+        return result;\n+    }"
        }
      ]
    }
  },
  {
    "id": "PRRT_kwDOKSXUF85hcSgM",
    "isResolved": false,
    "isOutdated": true,
    "path": "src/Models/NeuralNetworkModel.cs",
    "line": 979,
    "originalLine": 979,
    "comments": {
      "nodes": [
        {
          "id": "PRRC_kwDOKSXUF86Vyrif",
          "author": {
            "login": "coderabbitai"
          },
          "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Preserve configured loss when cloning or re-parameterizing**\n\n`WithParameters` (Line 858) and `DeepCopy` (Line 967) rebuild new models without supplying the existing `_defaultLossFunction`, resetting them to helper defaults and discarding any caller-provided loss. ThatΓÇÖs a functional regression for consumers who set a custom loss and expect clones/variants to behave identically.\n\nApply this diff so both paths retain the configured loss:\n\n```diff\n-        var newModel = new NeuralNetworkModel<T>(Architecture);\n+        var newModel = new NeuralNetworkModel<T>(Architecture, _defaultLossFunction);\n@@\n-        var copy = new NeuralNetworkModel<T>(Architecture);\n+        var copy = new NeuralNetworkModel<T>(Architecture, _defaultLossFunction);\n```\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
          "createdAt": "2025-11-11T07:11:01Z",
          "path": "src/Models/NeuralNetworkModel.cs",
          "line": 979,
          "diffHunk": "@@ -1,1043 +1,1043 @@\n-namespace AiDotNet.Models;\r\n-\r\n-/// <summary>\r\n-/// Represents a neural network model that implements the IFullModel interface.\r\n-/// </summary>\r\n-/// <remarks>\r\n-/// <para>\r\n-/// This class wraps a neural network implementation to provide a consistent interface with other model types.\r\n-/// It handles training, prediction, serialization, and other operations required by the IFullModel interface,\r\n-/// delegating to the underlying neural network. This allows neural networks to be used interchangeably with\r\n-/// other model types in optimization and model selection processes.\r\n-/// </para>\r\n-/// <para><b>For Beginners:</b> This is a wrapper that makes neural networks work with the same interface as simpler models.\r\n-/// \r\n-/// Neural networks are powerful machine learning models that can:\r\n-/// - Learn complex patterns in data that simpler models might miss\r\n-/// - Process different types of data like images, text, or tabular data\r\n-/// - Automatically extract useful features from raw data\r\n-/// \r\n-/// This class allows you to use neural networks anywhere you would use simpler models,\r\n-/// making it easy to compare them or use them in the same optimization processes.\r\n-/// </para>\r\n-/// </remarks>\r\n-/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\r\n-public class NeuralNetworkModel<T> : IFullModel<T, Tensor<T>, Tensor<T>>\r\n-{\r\n-    /// <summary>\r\n-    /// Gets the underlying neural network.\r\n-    /// </summary>\r\n-    /// <value>A NeuralNetworkBase&lt;T&gt; instance containing the actual neural network.</value>\r\n-    /// <remarks>\r\n-    /// <para>\r\n-    /// This property provides access to the underlying neural network implementation. The network is responsible for\r\n-    /// the actual computations, while this class serves as an adapter to the IFullModel interface. This property\r\n-    /// can be used to access network-specific features not exposed through the IFullModel interface.\r\n-    /// </para>\r\n-    /// <para><b>For Beginners:</b> This property gives you direct access to the actual neural network.\r\n-    /// \r\n-    /// The network:\r\n-    /// - Contains all the layers and connections of the neural network\r\n-    /// - Handles the actual calculations and learning\r\n-    /// - Stores all the learned weights and parameters\r\n-    /// \r\n-    /// You can use this property to access neural network-specific features\r\n-    /// that aren't available through the standard model interface.\r\n-    /// </para>\r\n-    /// </remarks>\r\n-    public NeuralNetworkBase<T> Network { get; }\r\n-    \r\n-    /// <summary>\r\n-    /// Gets the architecture of the neural network.\r\n-    /// </summary>\r\n-    /// <value>A NeuralNetworkArchitecture&lt;T&gt; instance defining the structure of the network.</value>\r\n-    /// <remarks>\r\n-    /// <para>\r\n-    /// This property provides access to the architecture that defines the structure of the neural network, including\r\n-    /// its layers, input/output dimensions, and task-specific properties. The architecture serves as a blueprint for\r\n-    /// the network and contains information about the network's topology and configuration.\r\n-    /// </para>\r\n-    /// <para><b>For Beginners:</b> This property gives you access to the blueprint of the neural network.\r\n-    /// \r\n-    /// The architecture:\r\n-    /// - Defines how many layers the network has\r\n-    /// - Specifies how many neurons are in each layer\r\n-    /// - Determines what kind of data the network can process\r\n-    /// - Configures how the network learns and makes predictions\r\n-    /// \r\n-    /// Think of it like the plans for a building - it defines the structure\r\n-    /// but doesn't contain the actual building materials.\r\n-    /// </para>\r\n-    /// </remarks>\r\n-    public NeuralNetworkArchitecture<T> Architecture { get; }\r\n-    \r\n-    /// <summary>\r\n-    /// The numeric operations provider used for mathematical operations on type T.\r\n-    /// </summary>\r\n-    /// <remarks>\r\n-    /// <para>\r\n-    /// This field provides access to basic mathematical operations for the generic type T,\r\n-    /// allowing the class to perform calculations regardless of the specific numeric type.\r\n-    /// </para>\r\n-    /// <para><b>For Beginners:</b> This provides a way to do math with different number types.\r\n-    /// \r\n-    /// Since neural networks can work with different types of numbers (float, double, etc.),\r\n-    /// we need a way to perform math operations like addition and multiplication\r\n-    /// without knowing exactly what number type we're using. This helper provides\r\n-    /// those operations in a consistent way regardless of the number type.\r\n-    /// </para>\r\n-    /// </remarks>\r\n-    private static readonly INumericOperations<T> _numOps = MathHelper.GetNumericOperations<T>();\r\n-    \r\n-    /// <summary>\r\n-    /// The learning rate used during training to control the size of weight updates.\r\n-    /// </summary>\r\n-    /// <remarks>\r\n-    /// <para>\r\n-    /// The learning rate determines how quickly the model adapts to the problem.\r\n-    /// Smaller values mean slower learning but potentially more precision, while \r\n-    /// larger values mean faster learning but risk overshooting the optimal solution.\r\n-    /// </para>\r\n-    /// <para><b>For Beginners:</b> This controls how big each learning step is during training.\r\n-    /// \r\n-    /// Think of it like adjusting the size of steps when walking:\r\n-    /// - Small learning rate = small steps (slow progress but less risk of going too far)\r\n-    /// - Large learning rate = large steps (faster progress but might overshoot the target)\r\n-    /// \r\n-    /// Finding the right learning rate is important - too small and training takes forever,\r\n-    /// too large and the model might never find the best solution.\r\n-    /// </para>\r\n-    /// </remarks>\r\n-    private T _learningRate;\r\n-    \r\n-    /// <summary>\r\n-    /// Indicates whether the model is currently in training mode.\r\n-    /// </summary>\r\n-    /// <remarks>\r\n-    /// <para>\r\n-    /// Some neural network components behave differently during training versus inference.\r\n-    /// This flag enables those components to adjust their behavior accordingly.\r\n-    /// </para>\r\n-    /// <para><b>For Beginners:</b> This tells the network whether it's learning or making predictions.\r\n-    ///\r\n-    /// Some parts of neural networks work differently depending on whether the network is:\r\n-    /// - Training (learning from examples)\r\n-    /// - Making predictions (using what it learned)\r\n-    ///\r\n-    /// For example, a technique called \"dropout\" randomly turns off some neurons during\r\n-    /// training to prevent overfitting, but doesn't do this during prediction.\r\n-    /// </para>\r\n-    /// </remarks>\r\n-    private bool _isTrainingMode = true;\r\n-\r\n-    /// <summary>\r\n-    /// The default loss function used by this model for gradient computation.\r\n-    /// </summary>\r\n-    private ILossFunction<T> _defaultLossFunction;\r\n-\r\n-    /// <summary>\r\n-    /// Initializes a new instance of the NeuralNetworkModel class with the specified architecture.\r\n-    /// </summary>\r\n-    /// <param name=\"architecture\">The architecture defining the structure of the neural network.</param>\r\n-    /// <param name=\"lossFunction\">Optional loss function to use for training. If null, uses a default based on task type (CrossEntropy for classification, MSE for regression).</param>\r\n-    /// <remarks>\r\n-    /// <para>\r\n-    /// This constructor creates a new NeuralNetworkModel instance with the specified architecture. It initializes\r\n-    /// the underlying neural network based on the architecture provided. The architecture determines the network's\r\n-    /// structure, including the number and type of layers, the input and output dimensions, and the type of task\r\n-    /// the network is designed to perform.\r\n-    /// </para>\r\n-    /// <para><b>For Beginners:</b> This constructor creates a new neural network model with the specified design.\r\n-    ///\r\n-    /// When creating a NeuralNetworkModel:\r\n-    /// - You provide an architecture that defines the network's structure\r\n-    /// - The constructor creates the actual neural network based on this design\r\n-    /// - The model is ready to be trained or to make predictions\r\n-    ///\r\n-    /// The architecture is crucial as it determines what kind of data the network can process\r\n-    /// and what kind of problems it can solve. Different architectures work better for\r\n-    /// different types of problems.\r\n-    /// </para>\r\n-    /// </remarks>\r\n-    public NeuralNetworkModel(NeuralNetworkArchitecture<T> architecture, ILossFunction<T>? lossFunction = null)\r\n-    {\r\n-        Architecture = architecture ?? throw new ArgumentNullException(nameof(architecture));\r\n-        Network = new NeuralNetwork<T>(architecture);\r\n-        _learningRate = _numOps.FromDouble(0.01); // Default learning rate\r\n-        _defaultLossFunction = lossFunction ?? NeuralNetworkHelper<T>.GetDefaultLossFunction(architecture.TaskType);\r\n-    }\r\n-\r\n-    /// <summary>\r\n-    /// Gets the default loss function used by this model for gradient computation.\r\n-    /// </summary>\r\n-    /// <remarks>\r\n-    /// <para>\r\n-    /// The default loss function is determined by the network's task type:\r\n-    /// - Classification tasks use CrossEntropyLoss\r\n-    /// - Regression tasks use MeanSquaredErrorLoss\r\n-    /// - Custom loss functions can be provided via the constructor\r\n-    /// </para>\r\n-    /// </remarks>\r\n-    public ILossFunction<T> DefaultLossFunction => _defaultLossFunction;\r\n-\r\n-    /// <summary>\r\n-    /// Gets the number of features used by the model.\r\n-    /// </summary>\r\n-    /// <value>An integer representing the number of input features.</value>\r\n-    /// <remarks>\r\n-    /// <para>\r\n-    /// This property returns the number of features that the model uses, which is determined by the input size\r\n-    /// of the neural network. For one-dimensional inputs, this is simply the input size. For multi-dimensional\r\n-    /// inputs, this is the total number of input elements (calculated as InputHeight * InputWidth * InputDepth).\r\n-    /// </para>\r\n-    /// <para><b>For Beginners:</b> This tells you how many input variables the neural network uses.\r\n-    /// \r\n-    /// The feature count:\r\n-    /// - For simple data, it's the number of input values (like age, height, weight)\r\n-    /// - For image data, it's the total number of pixels times the number of color channels\r\n-    /// - For text data, it might be the vocabulary size or embedding dimension\r\n-    /// \r\n-    /// This helps you understand how much input information the network is considering,\r\n-    /// and it's important for ensuring your input data has the right dimensions.\r\n-    /// </para>\r\n-    /// </remarks>\r\n-    public int FeatureCount => Architecture.CalculatedInputSize;\r\n-\r\n-    /// <summary>\r\n-    /// Gets the complexity of the model.\r\n-    /// </summary>\r\n-    /// <value>An integer representing the model's complexity.</value>\r\n-    /// <remarks>\r\n-    /// <para>\r\n-    /// This property returns a measure of the model's complexity, which is calculated as the total number of\r\n-    /// trainable parameters (weights and biases) in the neural network. The complexity of a neural network is\r\n-    /// an important factor in understanding its capacity to learn, its potential for overfitting, and its\r\n-    /// computational requirements.\r\n-    /// </para>\r\n-    /// <para><b>For Beginners:</b> This tells you how complex the neural network is.\r\n-    /// \r\n-    /// The complexity:\r\n-    /// - Is measured by the total number of adjustable parameters in the network\r\n-    /// - Higher complexity means the network can learn more complex patterns\r\n-    /// - But higher complexity also means more training data is needed\r\n-    /// - And higher complexity increases the risk of overfitting\r\n-    /// \r\n-    /// A simple network might have hundreds of parameters,\r\n-    /// while deep networks can have millions or billions.\r\n-    /// </para>\r\n-    /// </remarks>\r\n-    public int Complexity => Network.GetParameterCount();\r\n-\r\n-    /// <summary>\r\n-    /// Sets the learning rate for training the model.\r\n-    /// </summary>\r\n-    /// <param name=\"learningRate\">The learning rate to use during training.</param>\r\n-    /// <returns>This model instance for method chaining.</returns>\r\n-    /// <remarks>\r\n-    /// <para>\r\n-    /// This method sets the learning rate used during training. The learning rate controls how quickly the model\r\n-    /// adapts to the training data. A higher learning rate means faster learning but may cause instability, while\r\n-    /// a lower learning rate means slower but more stable learning.\r\n-    /// </para>\r\n-    /// <para><b>For Beginners:</b> This lets you control how big each learning step is during training.\r\n-    /// \r\n-    /// The learning rate:\r\n-    /// - Controls how quickly the network adjusts its weights\r\n-    /// - Smaller values (like 0.001) make training more stable but slower\r\n-    /// - Larger values (like 0.1) make training faster but potentially unstable\r\n-    /// \r\n-    /// Finding the right learning rate is often a process of trial and error.\r\n-    /// This method lets you set it to the value you want to try.\r\n-    /// </para>\r\n-    /// </remarks>\r\n-    public NeuralNetworkModel<T> SetLearningRate(T learningRate)\r\n-    {\r\n-        _learningRate = learningRate;\r\n-        return this;\r\n-    }\r\n-\r\n-    /// <summary>\r\n-    /// Sets whether the model is in training mode or prediction mode.\r\n-    /// </summary>\r\n-    /// <param name=\"isTraining\">True for training mode, false for prediction mode.</param>\r\n-    /// <returns>This model instance for method chaining.</returns>\r\n-    /// <remarks>\r\n-    /// <para>\r\n-    /// This method sets whether the model is in training mode or prediction mode. Some components of neural networks\r\n-    /// behave differently during training versus prediction, such as dropout layers, which randomly disable neurons\r\n-    /// during training but not during prediction.\r\n-    /// </para>\r\n-    /// <para><b>For Beginners:</b> This switches the network between learning mode and prediction mode.\r\n-    /// \r\n-    /// The two modes are:\r\n-    /// - Training mode: The network is learning and updating its weights\r\n-    /// - Prediction mode: The network is using what it learned to make predictions\r\n-    /// \r\n-    /// Some special layers like Dropout and BatchNormalization work differently\r\n-    /// depending on which mode the network is in. This method lets you switch between them.\r\n-    /// </para>\r\n-    /// </remarks>\r\n-    public NeuralNetworkModel<T> SetTrainingMode(bool isTraining)\r\n-    {\r\n-        _isTrainingMode = isTraining;\r\n-        Network.SetTrainingMode(isTraining);\r\n-        return this;\r\n-    }\r\n-\r\n-    /// <summary>\r\n-    /// Determines whether a specific feature is used by the model.\r\n-    /// </summary>\r\n-    /// <param name=\"featureIndex\">The index of the feature to check.</param>\r\n-    /// <returns>Always returns true for neural networks, as they typically use all input features.</returns>\r\n-    /// <remarks>\r\n-    /// <para>\r\n-    /// This method determines whether a specific feature is used by the model. For neural networks, all features\r\n-    /// are typically used in some capacity, so this method always returns true. Unlike some linear models where\r\n-    /// features can have zero coefficients and therefore no impact, neural networks generally incorporate all\r\n-    /// input features, though they may learn to assign different importance to different features during training.\r\n-    /// </para>\r\n-    /// <para><b>For Beginners:</b> This method checks if a particular input variable affects the model's predictions.\r\n-    /// \r\n-    /// For neural networks:\r\n-    /// - This method always returns true\r\n-    /// - Neural networks typically use all input features in some way\r\n-    /// - The network learns which features are important during training\r\n-    /// - Even if a feature isn't useful, the network will learn to assign it less weight\r\n-    /// \r\n-    /// This differs from simpler models like linear regression,\r\n-    /// where features can be explicitly excluded with zero coefficients.\r\n-    /// </para>\r\n-    /// </remarks>\r\n-    public bool IsFeatureUsed(int featureIndex)\r\n-    {\r\n-        if (featureIndex < 0 || featureIndex >= FeatureCount)\r\n-        {\r\n-            throw new ArgumentOutOfRangeException(nameof(featureIndex), \r\n-                $\"Feature index must be between 0 and {FeatureCount - 1}\");\r\n-        }\r\n-        \r\n-        // Neural networks typically use all input features in some capacity\r\n-        return true;\r\n-    }\r\n-\r\n-    /// <summary>\r\n-    /// Computes gradients of the loss function with respect to model parameters WITHOUT updating parameters.\r\n-    /// </summary>\r\n-    /// <param name=\"input\">The input tensor.</param>\r\n-    /// <param name=\"target\">The target/expected output tensor.</param>\r\n-    /// <param name=\"lossFunction\">The loss function to use. If null, uses the model's default loss function.</param>\r\n-    /// <returns>A vector containing gradients with respect to all model parameters.</returns>\r\n-    /// <exception cref=\"InvalidOperationException\">If the network doesn't support training or loss function is null and no default is configured.</exception>\r\n-    /// <remarks>\r\n-    /// <para>\r\n-    /// This method performs a forward pass, computes the loss, and back-propagates to compute gradients,\r\n-    /// but does NOT update the model's parameters. The parameters remain unchanged after this call.\r\n-    /// </para>\r\n-    /// <para><b>For Beginners:</b>\r\n-    /// This method calculates which direction to move the model's parameters to reduce error,\r\n-    /// but it doesn't actually move them. This is useful for:\r\n-    /// - Distributed training: compute gradients on different machines and average them\r\n-    /// - Custom optimization: apply your own learning algorithm to the gradients\r\n-    /// - Analysis: inspect gradient values to understand what the model is learning\r\n-    /// </para>\r\n-    /// </remarks>\r\n-    public Vector<T> ComputeGradients(Tensor<T> input, Tensor<T> target, ILossFunction<T>? lossFunction = null)\r\n-    {\r\n-        if (!Network.SupportsTraining)\r\n-        {\r\n-            throw new InvalidOperationException(\"This neural network does not support training.\");\r\n-        }\r\n-\r\n-        var loss = lossFunction ?? DefaultLossFunction;\r\n-\r\n-        // Ensure the network is in training mode\r\n-        Network.SetTrainingMode(true);\r\n-\r\n-        // Convert tensors to the format expected by the network\r\n-        Vector<T> inputVector = input.ToVector();\r\n-        Vector<T> targetVector = target.ToVector();\r\n-\r\n-        // Forward pass with memory to store intermediate values for backpropagation\r\n-        Tensor<T> outputTensor = Network.ForwardWithMemory(Tensor<T>.FromVector(inputVector));\r\n-        Vector<T> outputVector = outputTensor.ToVector();\r\n-\r\n-        // Calculate error gradient using the loss function\r\n-        Vector<T> error = loss.CalculateDerivative(outputVector, targetVector);\r\n-\r\n-        // Backpropagate error through the network\r\n-        Network.Backpropagate(Tensor<T>.FromVector(error));\r\n-\r\n-        // Get and return gradients from the network\r\n-        Vector<T> gradients = Network.GetParameterGradients();\r\n-        return gradients;\r\n-    }\r\n-\r\n-    /// <summary>\r\n-    /// Applies pre-computed gradients to update the model parameters.\r\n-    /// </summary>\r\n-    /// <param name=\"gradients\">The gradient vector to apply.</param>\r\n-    /// <param name=\"learningRate\">The learning rate for the update.</param>\r\n-    /// <exception cref=\"ArgumentNullException\">If gradients is null.</exception>\r\n-    /// <exception cref=\"ArgumentException\">If gradient vector length doesn't match parameter count.</exception>\r\n-    /// <remarks>\r\n-    /// <para>\r\n-    /// Updates parameters using: ╬╕ = ╬╕ - learningRate * gradients\r\n-    /// </para>\r\n-    /// <para><b>For Beginners:</b>\r\n-    /// After computing gradients (seeing which direction to move),\r\n-    /// this method actually moves the model in that direction.\r\n-    /// The learning rate controls how big of a step to take.\r\n-    ///\r\n-    /// In distributed training, this applies the synchronized (averaged) gradients after\r\n-    /// communication across workers. Each worker applies the same averaged gradients\r\n-    /// to keep parameters consistent.\r\n-    /// </para>\r\n-    /// </remarks>\r\n-    public void ApplyGradients(Vector<T> gradients, T learningRate)\r\n-    {\r\n-        if (gradients == null)\r\n-            throw new ArgumentNullException(nameof(gradients));\r\n-\r\n-        var currentParams = Network.GetParameters();\r\n-\r\n-        if (gradients.Length != currentParams.Length)\r\n-        {\r\n-            throw new ArgumentException(\r\n-                $\"Gradient vector length ({gradients.Length}) must match parameter count ({currentParams.Length})\",\r\n-                nameof(gradients));\r\n-        }\r\n-\r\n-        var newParams = new Vector<T>(currentParams.Length);\r\n-\r\n-        // Apply gradient descent: params = params - learningRate * gradients\r\n-        for (int i = 0; i < currentParams.Length; i++)\r\n-        {\r\n-            T update = _numOps.Multiply(learningRate, gradients[i]);\r\n-            newParams[i] = _numOps.Subtract(currentParams[i], update);\r\n-        }\r\n-\r\n-        Network.UpdateParameters(newParams);\r\n-    }\r\n-\r\n-    /// <summary>\r\n-    /// Trains the model with the provided input and expected output.\r\n-    /// </summary>\r\n-    /// <param name=\"input\">The input tensor to train with.</param>\r\n-    /// <param name=\"expectedOutput\">The expected output tensor.</param>\r\n-    /// <remarks>\r\n-    /// <para>\r\n-    /// This method trains the neural network with the provided input and expected output tensors.\r\n-    /// It sets the network to training mode, performs a forward pass through the network, calculates\r\n-    /// the error between the predicted output and the expected output, and backpropagates the error\r\n-    /// to update the network's weights.\r\n-    /// </para>\r\n-    /// <para><b>For Beginners:</b> This method teaches the neural network using an example.\r\n-    /// \r\n-    /// During training:\r\n-    /// 1. The input data is sent through the network (forward pass)\r\n-    /// 2. The network makes a prediction\r\n-    /// 3. The prediction is compared to the expected output\r\n-    /// 4. The error is calculated\r\n-    /// 5. The network adjusts its weights to reduce the error\r\n-    /// \r\n-    /// This process is repeated with many examples to gradually improve the network's performance.\r\n-    /// Each example helps the network learn a little more about the patterns in your data.\r\n-    /// </para>\r\n-    /// </remarks>\r\n-    public void Train(Tensor<T> input, Tensor<T> expectedOutput)\r\n-    {\r\n-        if (!Network.SupportsTraining)\r\n-        {\r\n-            throw new InvalidOperationException(\"This neural network does not support training.\");\r\n-        }\r\n-\r\n-        // Save the current training mode to restore it after training\r\n-        bool previousTrainingMode = _isTrainingMode;\r\n-\r\n-        try\r\n-        {\r\n-            // Ensure the network is in training mode\r\n-            Network.SetTrainingMode(true);\r\n-\r\n-            // Convert tensors to the format expected by the network\r\n-            Vector<T> inputVector = input.ToVector();\r\n-            Vector<T> expectedOutputVector = expectedOutput.ToVector();\r\n-\r\n-            // Forward pass with memory to store intermediate values for backpropagation\r\n-            Tensor<T> outputTensor = Network.ForwardWithMemory(Tensor<T>.FromVector(inputVector));\r\n-            Vector<T> outputVector = outputTensor.ToVector();\r\n-\r\n-            // Calculate error gradient\r\n-            Vector<T> error = CalculateError(outputVector, expectedOutputVector);\r\n-\r\n-            // Backpropagate error\r\n-            Network.Backpropagate(Tensor<T>.FromVector(error));\r\n-\r\n-            // Update weights using the calculated gradients\r\n-            Vector<T> gradients = Network.GetParameterGradients();\r\n-            Vector<T> currentParams = Network.GetParameters();\r\n-            Vector<T> newParams = new Vector<T>(currentParams.Length);\r\n-\r\n-            for (int i = 0; i < currentParams.Length; i++)\r\n-            {\r\n-                // Simple gradient descent: param = param - learningRate * gradient\r\n-                T update = _numOps.Multiply(_learningRate, gradients[i]);\r\n-                newParams[i] = _numOps.Subtract(currentParams[i], update);\r\n-            }\r\n-\r\n-            Network.UpdateParameters(newParams);\r\n-        }\r\n-        finally\r\n-        {\r\n-            // Restore the original training mode\r\n-            // This ensures that if the model was in inference mode before,\r\n-            // it returns to inference mode after training, preventing\r\n-            // dropout and batch normalization from being in the wrong state\r\n-            SetTrainingMode(previousTrainingMode);\r\n-        }\r\n-    }\r\n-\r\n-    /// <summary>\r\n-    /// Uses the model to make a prediction for the given input.\r\n-    /// </summary>\r\n-    /// <param name=\"input\">The input tensor to make a prediction for.</param>\r\n-    /// <returns>The predicted output tensor.</returns>\r\n-    /// <remarks>\r\n-    /// <para>\r\n-    /// This method uses the trained neural network to make a prediction for the given input tensor.\r\n-    /// It sets the network to prediction mode (not training mode), performs a forward pass through\r\n-    /// the network, and returns the output as a tensor with the appropriate shape.\r\n-    /// </para>\r\n-    /// <para><b>For Beginners:</b> This method makes predictions using what the neural network has learned.\r\n-    /// \r\n-    /// When making a prediction:\r\n-    /// 1. The input data is sent through the network\r\n-    /// 2. Each layer processes the data based on its learned weights\r\n-    /// 3. The final layer produces the output (prediction)\r\n-    /// \r\n-    /// Unlike training, no weights are updated during prediction - the network\r\n-    /// is simply using what it already knows to make its best guess.\r\n-    /// </para>\r\n-    /// </remarks>\r\n-    public Tensor<T> Predict(Tensor<T> input)\r\n-    {\r\n-        // Set to prediction mode (not training)\r\n-        Network.SetTrainingMode(false);\r\n-    \r\n-        // Forward pass through the network\r\n-        return Network.Predict(input);\r\n-    }\r\n-\r\n-    /// <summary>\r\n-    /// Trains the network with the provided input and expected output vectors.\r\n-    /// </summary>\r\n-    /// <param name=\"input\">The input vector.</param>\r\n-    /// <param name=\"expectedOutput\">The expected output vector.</param>\r\n-    /// <remarks>\r\n-    /// <para>\r\n-    /// This method implements the actual training of the neural network. It performs forward propagation to compute\r\n-    /// the network's output, calculates the error gradient, and then performs backpropagation to update the network's\r\n-    /// parameters. This is the core of the learning process for neural networks. The specific implementation may vary\r\n-    /// depending on the type of neural network and the training algorithm being used.\r\n-    /// </para>\r\n-    /// <para><b>For Beginners:</b> This method handles the details of teaching the neural network.\r\n-    /// \r\n-    /// During training:\r\n-    /// 1. The input data is sent through the network (forward propagation)\r\n-    /// 2. The error between the network's output and the expected output is calculated\r\n-    /// 3. This error is sent backward through the network (backpropagation)\r\n-    /// 4. The network adjusts its weights to reduce the error\r\n-    /// \r\n-    /// This process is repeated many times over different examples,\r\n-    /// gradually improving the network's accuracy.\r\n-    /// </para>\r\n-    /// </remarks>\r\n-    private void TrainNetwork(Tensor<T> input, Tensor<T> expectedOutput)\r\n-    {\r\n-        // Implementation depends on the specific neural network type\r\n-        if (!Network.SupportsTraining)\r\n-        {\r\n-            throw new InvalidOperationException(\"This neural network does not support training.\");\r\n-        }\r\n-        \r\n-        // Forward pass with memory to store intermediate values\r\n-        Tensor<T> outputTensor = Network.ForwardWithMemory(input);\r\n-        Vector<T> output = outputTensor.ToVector();\r\n-\r\n-        // Calculate error gradient\r\n-        Vector<T> error = CalculateError(output, expectedOutput.ToVector());\r\n-\r\n-        // Backpropagate error\r\n-        Network.Backpropagate(Tensor<T>.FromVector(error));\r\n-        \r\n-        // Update weights using the calculated gradients\r\n-        Vector<T> gradients = Network.GetParameterGradients();\r\n-        Vector<T> currentParams = Network.GetParameters();\r\n-        Vector<T> newParams = new Vector<T>(currentParams.Length);\r\n-        \r\n-        for (int i = 0; i < currentParams.Length; i++)\r\n-        {\r\n-            // Simple gradient descent: param = param - learningRate * gradient\r\n-            T update = _numOps.Multiply(_learningRate, gradients[i]);\r\n-            newParams[i] = _numOps.Subtract(currentParams[i], update);\r\n-        }\r\n-        \r\n-        Network.UpdateParameters(newParams);\r\n-    }\r\n-\r\n-    /// <summary>\r\n-    /// Calculates the error between predicted and expected outputs.\r\n-    /// </summary>\r\n-    /// <param name=\"predicted\">The predicted output values.</param>\r\n-    /// <param name=\"expected\">The expected output values.</param>\r\n-    /// <returns>A vector containing the error for each output.</returns>\r\n-    /// <remarks>\r\n-    /// <para>\r\n-    /// This method calculates the error between the predicted output values and the expected output values.\r\n-    /// The error is calculated using a loss function appropriate for the network's task type (e.g., mean squared error\r\n-    /// for regression tasks, cross-entropy for classification tasks). The resulting error vector is used during\r\n-    /// backpropagation to update the network's weights.\r\n-    /// </para>\r\n-    /// <para><b>For Beginners:</b> This method measures how wrong each prediction is compared to \r\n-    /// the expected value. These error values are used to adjust the network's weights during training.\r\n-    /// \r\n-    /// Different types of problems use different ways to measure error:\r\n-    /// - For predicting numeric values (regression), we often use squared differences\r\n-    /// - For classifying into categories, we often use cross-entropy\r\n-    /// \r\n-    /// This method automatically chooses the right error measure based on what\r\n-    /// kind of problem your network is solving.\r\n-    /// </para>\r\n-    /// </remarks>\r\n-    private Vector<T> CalculateError(Vector<T> predicted, Vector<T> expected)\r\n-    {\r\n-        // Check if vectors have the same length\r\n-        if (predicted.Length != expected.Length)\r\n-        {\r\n-            throw new ArgumentException(\"Predicted and expected vectors must have the same length.\");\r\n-        }\r\n-\r\n-        // Get appropriate loss function based on the task type\r\n-        var lossFunction = NeuralNetworkHelper<T>.GetDefaultLossFunction(Architecture.TaskType);\r\n-    \r\n-        // Calculate gradients based on the loss function\r\n-        Vector<T> error = lossFunction.CalculateDerivative(predicted, expected);\r\n-    \r\n-        return error;\r\n-    }\r\n-\r\n-    /// <summary>\r\n-    /// Gets metadata about the model.\r\n-    /// </summary>\r\n-    /// <returns>A ModelMetadata object containing information about the model.</returns>\r\n-    /// <remarks>\r\n-    /// <para>\r\n-    /// This method returns metadata about the model, including its type, feature count, complexity, and additional\r\n-    /// information about the neural network. The metadata includes the model type (Neural Network), the number of\r\n-    /// features, the complexity (total parameter count), a description, and additional information such as the\r\n-    /// architecture details, layer counts, and activation functions used. This metadata is useful for model selection,\r\n-    /// analysis, and visualization.\r\n-    /// </para>\r\n-    /// <para><b>For Beginners:</b> This method returns detailed information about the neural network model.\r\n-    /// \r\n-    /// The metadata includes:\r\n-    /// - Basic properties like model type, feature count, and complexity\r\n-    /// - Architecture details like layer counts and types\r\n-    /// - Statistics about the model's parameters\r\n-    /// \r\n-    /// This information is useful for:\r\n-    /// - Understanding the model's structure\r\n-    /// - Comparing different models\r\n-    /// - Analyzing the model's capabilities\r\n-    /// - Documenting the model for future reference\r\n-    /// </para>\r\n-    /// </remarks>\r\n-    public ModelMetadata<T> GetModelMetadata()\r\n-    {\r\n-        int[] layerSizes = Architecture.GetLayerSizes();\r\n-        \r\n-        return new ModelMetadata<T>\r\n-        {\r\n-            FeatureCount = FeatureCount,\r\n-            Complexity = Complexity,\r\n-            Description = $\"Neural Network model with {layerSizes.Length} layers\",\r\n-            AdditionalInfo = new Dictionary<string, object>\r\n-            {\r\n-                { \"LayerSizes\", layerSizes },\r\n-                { \"InputShape\", Architecture.GetInputShape() },\r\n-                { \"OutputShape\", Architecture.GetOutputShape() },\r\n-                { \"TaskType\", Architecture.TaskType.ToString() },\r\n-                { \"InputType\", Architecture.InputType.ToString() },\r\n-                { \"HiddenLayerCount\", Architecture.GetHiddenLayerSizes().Length },\r\n-                { \"ParameterCount\", Network.GetParameterCount() },\r\n-                { \"SupportsTraining\", Network.SupportsTraining }\r\n-            }\r\n-        };\r\n-    }\r\n-\r\n-    /// <summary>\r\n-    /// Serializes the model to a byte array.\r\n-    /// </summary>\r\n-    /// <returns>A byte array containing the serialized model.</returns>\r\n-    /// <remarks>\r\n-    /// <para>\r\n-    /// This method serializes the model to a byte array by writing the architecture details and the network parameters.\r\n-    /// The serialization format includes the architecture information followed by the network parameters. This allows\r\n-    /// the model to be stored or transmitted and later reconstructed using the Deserialize method.\r\n-    /// </para>\r\n-    /// <para><b>For Beginners:</b> This method converts the neural network model to a byte array that can be saved or transmitted.\r\n-    /// \r\n-    /// When serializing the model:\r\n-    /// - Both the architecture (structure) and parameters (weights) are saved\r\n-    /// - The data is formatted in a way that can be efficiently stored\r\n-    /// - The resulting byte array contains everything needed to reconstruct the model\r\n-    /// \r\n-    /// This is useful for:\r\n-    /// - Saving trained models to disk\r\n-    /// - Sharing models with others\r\n-    /// - Deploying models to production systems\r\n-    /// - Creating model checkpoints during long training processes\r\n-    /// </para>\r\n-    /// </remarks>\r\n-    public byte[] Serialize()\r\n-    {\r\n-        using MemoryStream ms = new MemoryStream();\r\n-        using BinaryWriter writer = new BinaryWriter(ms);\r\n-        \r\n-        // Write a version number for forward compatibility\r\n-        writer.Write(1); // Version 1\r\n-        \r\n-        // Write the architecture type\r\n-        writer.Write(Architecture.GetType().FullName ?? \"Unknown\");\r\n-        \r\n-        // Serialize the architecture\r\n-        // In a real implementation, we would need a more sophisticated approach\r\n-        // Here we just write key architecture properties\r\n-        writer.Write((int)Architecture.InputType);\r\n-        writer.Write((int)Architecture.TaskType);\r\n-        writer.Write((int)Architecture.Complexity);\r\n-        writer.Write(Architecture.InputSize);\r\n-        writer.Write(Architecture.OutputSize);\r\n-        writer.Write(Architecture.InputHeight);\r\n-        writer.Write(Architecture.InputWidth);\r\n-        writer.Write(Architecture.InputDepth);\r\n-        \r\n-        // Serialize the network parameters\r\n-        var serializedNetwork = Network.Serialize();\r\n-        writer.Write(serializedNetwork.Length);\r\n-        writer.Write(serializedNetwork);\r\n-        \r\n-        return ms.ToArray();\r\n-    }\r\n-\r\n-    /// <summary>\r\n-    /// Deserializes the model from a byte array.\r\n-    /// </summary>\r\n-    /// <param name=\"data\">The byte array containing the serialized model.</param>\r\n-    /// <remarks>\r\n-    /// <para>\r\n-    /// This method deserializes the model from a byte array by reading the architecture details and the network parameters.\r\n-    /// It expects the same format as produced by the Serialize method: the architecture information followed by the network\r\n-    /// parameters. This allows a model that was previously serialized to be reconstructed.\r\n-    /// </para>\r\n-    /// <para><b>For Beginners:</b> This method reconstructs a neural network model from a byte array created by Serialize.\r\n-    /// \r\n-    /// When deserializing the model:\r\n-    /// - The architecture is read first to recreate the structure\r\n-    /// - Then the parameters (weights) are loaded into that structure\r\n-    /// - The resulting model is identical to the one that was serialized\r\n-    /// \r\n-    /// This is used when:\r\n-    /// - Loading a previously saved model\r\n-    /// - Receiving a model from another system\r\n-    /// - Resuming training from a checkpoint\r\n-    /// \r\n-    /// After deserialization, the model can be used for predictions or further training\r\n-    /// just as if it had never been serialized.\r\n-    /// </para>\r\n-    /// </remarks>\r\n-    public void Deserialize(byte[] data)\r\n-    {\r\n-        if (data == null || data.Length == 0)\r\n-        {\r\n-            throw new ArgumentException(\"Serialized data cannot be null or empty.\", nameof(data));\r\n-        }\r\n-        \r\n-        using MemoryStream ms = new MemoryStream(data);\r\n-        using BinaryReader reader = new BinaryReader(ms);\r\n-            \r\n-        // Read version number\r\n-        int version = reader.ReadInt32();\r\n-            \r\n-        // Read architecture type\r\n-        string architectureType = reader.ReadString();\r\n-            \r\n-        // Read architecture properties\r\n-        InputType inputType = (InputType)reader.ReadInt32();\r\n-        NeuralNetworkTaskType taskType = (NeuralNetworkTaskType)reader.ReadInt32();\r\n-        NetworkComplexity complexity = (NetworkComplexity)reader.ReadInt32();\r\n-        int inputSize = reader.ReadInt32();\r\n-        int outputSize = reader.ReadInt32();\r\n-        int inputHeight = reader.ReadInt32();\r\n-        int inputWidth = reader.ReadInt32();\r\n-        int inputDepth = reader.ReadInt32();\r\n-            \r\n-        // Check if the architecture matches\r\n-        if (Architecture.InputType != inputType ||\r\n-            Architecture.TaskType != taskType ||\r\n-            Architecture.InputSize != inputSize ||\r\n-            Architecture.OutputSize != outputSize)\r\n-        {\r\n-            throw new InvalidOperationException(\r\n-                \"Serialized network architecture doesn't match this model's architecture.\");\r\n-        }\r\n-        \r\n-        var length = reader.ReadInt32();\r\n-        var bytes = reader.ReadBytes(length);\r\n-        // Deserialize the network parameters\r\n-        Network.Deserialize(bytes);\r\n-    }\r\n-\r\n-    /// <summary>\r\n-    /// Gets all trainable parameters of the neural network as a single vector.\r\n-    /// </summary>\r\n-    /// <returns>A vector containing all trainable parameters.</returns>\r\n-    /// <remarks>\r\n-    /// <para>\r\n-    /// This method returns all trainable parameters of the neural network as a single vector.\r\n-    /// These parameters include weights and biases from all layers that support training.\r\n-    /// The vector can be used to save the model's state, apply optimization techniques,\r\n-    /// or transfer learning between models.\r\n-    /// </para>\r\n-    /// <para><b>For Beginners:</b> This method collects all the learned weights and biases from the neural network\r\n-    /// into a single list. This is useful for saving the model, optimizing it, or transferring its knowledge.\r\n-    /// \r\n-    /// The parameters:\r\n-    /// - Are the numbers that the neural network has learned during training\r\n-    /// - Include weights (how strongly neurons connect to each other)\r\n-    /// - Include biases (baseline activation levels for neurons)\r\n-    /// \r\n-    /// A simple network might have hundreds of parameters, while modern deep networks\r\n-    /// often have millions or billions of parameters.\r\n-    /// </para>\r\n-    /// </remarks>\r\n-    public Vector<T> GetParameters()\r\n-    {\r\n-        return Network.GetParameters();\r\n-    }\r\n-\r\n-    /// <summary>\r\n-    /// Updates the model with new parameter values.\r\n-    /// </summary>\r\n-    /// <param name=\"parameters\">The new parameter values to use.</param>\r\n-    /// <returns>The updated model.</returns>\r\n-    /// <remarks>\r\n-    /// <para>\r\n-    /// This method creates a new model with the same architecture as the current model but with the provided\r\n-    /// parameter values. This allows creating a modified version of the model without altering the original.\r\n-    /// The new parameters must match the number of parameters in the original model.\r\n-    /// </para>\r\n-    /// <para><b>For Beginners:</b> This method lets you change all the weights and biases in the neural network\r\n-    /// at once by providing a list of new values. It's useful when optimizing the model or loading saved weights.\r\n-    /// \r\n-    /// When updating parameters:\r\n-    /// - A new model is created with the same structure as this one\r\n-    /// - The new model's weights and biases are set to the values you provide\r\n-    /// - The original model remains unchanged\r\n-    /// \r\n-    /// This is useful for:\r\n-    /// - Loading pre-trained weights\r\n-    /// - Testing different parameter values\r\n-    /// - Implementing evolutionary algorithms\r\n-    /// - Creating ensemble models with different parameter sets\r\n-    /// </para>\r\n-    /// </remarks>\r\n-    public IFullModel<T, Tensor<T>, Tensor<T>> WithParameters(Vector<T> parameters)\r\n-    {\r\n-        // Create a new model with the same architecture\r\n-        var newModel = new NeuralNetworkModel<T>(Architecture);\r\n-    \r\n-        // Update the parameters of the new model\r\n-        newModel.Network.UpdateParameters(parameters);\r\n-    \r\n-        return newModel;\r\n-    }\r\n-\r\n-    /// <summary>\r\n-    /// Gets the indices of all features used by this model.\r\n-    /// </summary>\r\n-    /// <returns>A collection of feature indices.</returns>\r\n-    /// <remarks>\r\n-    /// <para>\r\n-    /// This method returns the indices of all features that are used by the model. For neural networks,\r\n-    /// this typically includes all features from 0 to FeatureCount-1, as neural networks generally use\r\n-    /// all input features to some extent.\r\n-    /// </para>\r\n-    /// <para><b>For Beginners:</b> This method returns a list of which input features the model actually uses.\r\n-    /// For neural networks, this typically includes all available features unless specific feature selection has been applied.\r\n-    /// \r\n-    /// Unlike some simpler models (like linear regression with feature selection) where\r\n-    /// certain inputs might be completely ignored, neural networks typically process\r\n-    /// all input features and learn which ones are important during training.\r\n-    /// \r\n-    /// This method returns all feature indices from 0 to (FeatureCount-1).\r\n-    /// </para>\r\n-    /// </remarks>\r\n-    public IEnumerable<int> GetActiveFeatureIndices()\r\n-    {\r\n-        // Neural networks typically use all input features\r\n-        // Return indices for all features from 0 to FeatureCount-1\r\n-        return Enumerable.Range(0, FeatureCount);\r\n-    }\r\n-\r\n-    /// <summary>\r\n-    /// Sets the parameters for this model.\r\n-    /// </summary>\r\n-    /// <param name=\"parameters\">A vector containing the model parameters.</param>\r\n-    public void SetParameters(Vector<T> parameters)\r\n-    {\r\n-        if (Network == null)\r\n-        {\r\n-            throw new InvalidOperationException(\"Network has not been initialized.\");\r\n-        }\r\n-\r\n-        Network.SetParameters(parameters);\r\n-    }\r\n-\r\n-    /// <summary>\r\n-    /// Sets the active feature indices for this model.\r\n-    /// </summary>\r\n-    /// <param name=\"featureIndices\">The indices of features to activate.</param>\r\n-    public void SetActiveFeatureIndices(IEnumerable<int> featureIndices)\r\n-    {\r\n-        // Neural networks typically don't support feature masking after training\r\n-        throw new NotSupportedException(\"Neural networks do not support setting active features after network construction.\");\r\n-    }\r\n-\r\n-    /// <summary>\r\n-    /// Gets the feature importance scores as a dictionary.\r\n-    /// </summary>\r\n-    /// <returns>A dictionary mapping feature names to their importance scores.</returns>\r\n-    /// <exception cref=\"NotSupportedException\">\r\n-    /// This method is not supported for neural networks. Feature importance in neural networks\r\n-    /// requires specialized techniques like gradient-based attribution or permutation importance.\r\n-    /// </exception>\r\n-    public Dictionary<string, T> GetFeatureImportance()\r\n-    {\r\n-        // Neural network feature importance requires specialized techniques like:\r\n-        // - Gradient-based attribution methods (e.g., Integrated Gradients, SHAP)\r\n-        // - Permutation importance\r\n-        // - Layer-wise relevance propagation\r\n-        // These are complex to implement correctly and beyond the scope of this basic method.\r\n-        throw new NotSupportedException(\r\n-            \"Feature importance is not supported for neural networks through this method. \" +\r\n-            \"Neural networks require specialized techniques like gradient-based attribution, \" +\r\n-            \"permutation importance, or SHAP values to properly assess feature importance.\");\r\n-    }\r\n-\r\n-    /// <summary>\r\n-    /// Creates a deep copy of this model.\r\n-    /// </summary>\r\n-    /// <returns>A new instance with the same architecture and parameters.</returns>\r\n-    /// <remarks>\r\n-    /// <para>\r\n-    /// This method creates a deep copy of the neural network model, including both its architecture and\r\n-    /// learned parameters. The new model is independent of the original, so changes to one will not affect\r\n-    /// the other. This is useful for creating variations of a model while preserving the original.\r\n-    /// </para>\r\n-    /// <para><b>For Beginners:</b> This method creates an exact duplicate of the neural network,\r\n-    /// with the same structure and the same learned weights. This is useful when you need to\r\n-    /// make changes to a model without affecting the original.\r\n-    /// \r\n-    /// The deep copy:\r\n-    /// - Has identical architecture (same layers, neurons, connections)\r\n-    /// - Has identical parameters (same weights and biases)\r\n-    /// - Is completely independent of the original\r\n-    /// \r\n-    /// This is useful for:\r\n-    /// - Creating model variants for experimentation\r\n-    /// - Saving a checkpoint before making changes\r\n-    /// - Creating ensemble models\r\n-    /// - Implementing techniques like dropout ensemble\r\n-    /// </para>\r\n-    /// </remarks>\r\n-    public IFullModel<T, Tensor<T>, Tensor<T>> DeepCopy()\r\n-    {\r\n-        // Create a new model with the same architecture\r\n-        var copy = new NeuralNetworkModel<T>(Architecture);\r\n-        \r\n-        // Copy the network parameters\r\n-        var parameters = Network.GetParameters();\r\n-        copy.Network.UpdateParameters(parameters);\r\n-        \r\n-        // Copy additional properties\r\n-        copy._learningRate = _learningRate;\r\n-        copy._isTrainingMode = _isTrainingMode;\r\n-        copy.Network.SetTrainingMode(_isTrainingMode);\r\n-        \r\n-        return copy;\r\n-    }\r\n-\r\n-    /// <summary>\r\n-    /// Creates a shallow copy of this model.\r\n-    /// </summary>\r\n-    /// <returns>A new instance with the same architecture and parameters.</returns>\r\n-    /// <remarks>\r\n-    /// <para>\r\n-    /// This method creates a copy of the model that shares the same architecture but has its own set\r\n-    /// of parameters. It is equivalent to DeepCopy for this implementation but is provided for compatibility\r\n-    /// with the IFullModel interface.\r\n-    /// </para>\r\n-    /// <para><b>For Beginners:</b> This method creates a copy of the neural network model.\r\n-    ///\r\n-    /// In this implementation, Clone and DeepCopy do the same thing - they\r\n-    /// both create a completely independent copy of the model with the same\r\n-    /// architecture and parameters. Both methods are provided for compatibility\r\n-    /// with the IFullModel interface.\r\n-    /// </para>\r\n-    /// </remarks>\r\n-    public IFullModel<T, Tensor<T>, Tensor<T>> Clone()\r\n-    {\r\n-        return DeepCopy();\r\n-    }\r\n-\r\n-    public virtual int ParameterCount\r\n-    {\r\n-        get { return Network.GetParameterCount(); }\r\n-    }\r\n-\r\n-    public virtual void SaveModel(string filePath)\r\n-    {\r\n-        if (string.IsNullOrWhiteSpace(filePath))\r\n-            throw new ArgumentException(\"File path must not be null or empty.\", nameof(filePath));\r\n-\r\n-        try\r\n-        {\r\n-            var data = Serialize();\r\n-            var directory = Path.GetDirectoryName(filePath);\r\n-            if (!string.IsNullOrEmpty(directory) && !Directory.Exists(directory))\r\n-                Directory.CreateDirectory(directory);\r\n-            File.WriteAllBytes(filePath, data);\r\n-        }\r\n-        catch (IOException ex) { throw new InvalidOperationException($\"Failed to save model to '{filePath}': {ex.Message}\", ex); }\r\n-        catch (UnauthorizedAccessException ex) { throw new InvalidOperationException($\"Access denied when saving model to '{filePath}': {ex.Message}\", ex); }\r\n-        catch (System.Security.SecurityException ex) { throw new InvalidOperationException($\"Security error when saving model to '{filePath}': {ex.Message}\", ex); }\r\n-    }\r\n-\r\n-    public virtual void LoadModel(string filePath)\r\n-    {\r\n-        if (string.IsNullOrWhiteSpace(filePath))\r\n-            throw new ArgumentException(\"File path must not be null or empty.\", nameof(filePath));\r\n-\r\n-        try\r\n-        {\r\n-            var data = File.ReadAllBytes(filePath);\r\n-            Deserialize(data);\r\n-        }\r\n-        catch (FileNotFoundException ex) { throw new FileNotFoundException($\"The specified model file does not exist: {filePath}\", filePath, ex); }\r\n-        catch (IOException ex) { throw new InvalidOperationException($\"File I/O error while loading model from '{filePath}': {ex.Message}\", ex); }\r\n-        catch (UnauthorizedAccessException ex) { throw new InvalidOperationException($\"Access denied when loading model from '{filePath}': {ex.Message}\", ex); }\r\n-        catch (System.Security.SecurityException ex) { throw new InvalidOperationException($\"Security error when loading model from '{filePath}': {ex.Message}\", ex); }\r\n-        catch (Exception ex) { throw new InvalidOperationException($\"Failed to deserialize model from file '{filePath}'. The file may be corrupted or incompatible: {ex.Message}\", ex); }\r\n-    }\r\n-}\r\n+namespace AiDotNet.Models;\n+\n+/// <summary>\n+/// Represents a neural network model that implements the IFullModel interface.\n+/// </summary>\n+/// <remarks>\n+/// <para>\n+/// This class wraps a neural network implementation to provide a consistent interface with other model types.\n+/// It handles training, prediction, serialization, and other operations required by the IFullModel interface,\n+/// delegating to the underlying neural network. This allows neural networks to be used interchangeably with\n+/// other model types in optimization and model selection processes.\n+/// </para>\n+/// <para><b>For Beginners:</b> This is a wrapper that makes neural networks work with the same interface as simpler models.\n+/// \n+/// Neural networks are powerful machine learning models that can:\n+/// - Learn complex patterns in data that simpler models might miss\n+/// - Process different types of data like images, text, or tabular data\n+/// - Automatically extract useful features from raw data\n+/// \n+/// This class allows you to use neural networks anywhere you would use simpler models,\n+/// making it easy to compare them or use them in the same optimization processes.\n+/// </para>\n+/// </remarks>\n+/// <typeparam name=\"T\">The numeric type used for calculations, typically float or double.</typeparam>\n+public class NeuralNetworkModel<T> : IFullModel<T, Tensor<T>, Tensor<T>>\n+{\n+    /// <summary>\n+    /// Gets the underlying neural network.\n+    /// </summary>\n+    /// <value>A NeuralNetworkBase&lt;T&gt; instance containing the actual neural network.</value>\n+    /// <remarks>\n+    /// <para>\n+    /// This property provides access to the underlying neural network implementation. The network is responsible for\n+    /// the actual computations, while this class serves as an adapter to the IFullModel interface. This property\n+    /// can be used to access network-specific features not exposed through the IFullModel interface.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This property gives you direct access to the actual neural network.\n+    /// \n+    /// The network:\n+    /// - Contains all the layers and connections of the neural network\n+    /// - Handles the actual calculations and learning\n+    /// - Stores all the learned weights and parameters\n+    /// \n+    /// You can use this property to access neural network-specific features\n+    /// that aren't available through the standard model interface.\n+    /// </para>\n+    /// </remarks>\n+    public NeuralNetworkBase<T> Network { get; }\n+    \n+    /// <summary>\n+    /// Gets the architecture of the neural network.\n+    /// </summary>\n+    /// <value>A NeuralNetworkArchitecture&lt;T&gt; instance defining the structure of the network.</value>\n+    /// <remarks>\n+    /// <para>\n+    /// This property provides access to the architecture that defines the structure of the neural network, including\n+    /// its layers, input/output dimensions, and task-specific properties. The architecture serves as a blueprint for\n+    /// the network and contains information about the network's topology and configuration.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This property gives you access to the blueprint of the neural network.\n+    /// \n+    /// The architecture:\n+    /// - Defines how many layers the network has\n+    /// - Specifies how many neurons are in each layer\n+    /// - Determines what kind of data the network can process\n+    /// - Configures how the network learns and makes predictions\n+    /// \n+    /// Think of it like the plans for a building - it defines the structure\n+    /// but doesn't contain the actual building materials.\n+    /// </para>\n+    /// </remarks>\n+    public NeuralNetworkArchitecture<T> Architecture { get; }\n+    \n+    /// <summary>\n+    /// The numeric operations provider used for mathematical operations on type T.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This field provides access to basic mathematical operations for the generic type T,\n+    /// allowing the class to perform calculations regardless of the specific numeric type.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This provides a way to do math with different number types.\n+    /// \n+    /// Since neural networks can work with different types of numbers (float, double, etc.),\n+    /// we need a way to perform math operations like addition and multiplication\n+    /// without knowing exactly what number type we're using. This helper provides\n+    /// those operations in a consistent way regardless of the number type.\n+    /// </para>\n+    /// </remarks>\n+    private static readonly INumericOperations<T> _numOps = MathHelper.GetNumericOperations<T>();\n+    \n+    /// <summary>\n+    /// The learning rate used during training to control the size of weight updates.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// The learning rate determines how quickly the model adapts to the problem.\n+    /// Smaller values mean slower learning but potentially more precision, while \n+    /// larger values mean faster learning but risk overshooting the optimal solution.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This controls how big each learning step is during training.\n+    /// \n+    /// Think of it like adjusting the size of steps when walking:\n+    /// - Small learning rate = small steps (slow progress but less risk of going too far)\n+    /// - Large learning rate = large steps (faster progress but might overshoot the target)\n+    /// \n+    /// Finding the right learning rate is important - too small and training takes forever,\n+    /// too large and the model might never find the best solution.\n+    /// </para>\n+    /// </remarks>\n+    private T _learningRate;\n+    \n+    /// <summary>\n+    /// Indicates whether the model is currently in training mode.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Some neural network components behave differently during training versus inference.\n+    /// This flag enables those components to adjust their behavior accordingly.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This tells the network whether it's learning or making predictions.\n+    ///\n+    /// Some parts of neural networks work differently depending on whether the network is:\n+    /// - Training (learning from examples)\n+    /// - Making predictions (using what it learned)\n+    ///\n+    /// For example, a technique called \"dropout\" randomly turns off some neurons during\n+    /// training to prevent overfitting, but doesn't do this during prediction.\n+    /// </para>\n+    /// </remarks>\n+    private bool _isTrainingMode = true;\n+\n+    /// <summary>\n+    /// The default loss function used by this model for gradient computation.\n+    /// </summary>\n+    private ILossFunction<T> _defaultLossFunction;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the NeuralNetworkModel class with the specified architecture.\n+    /// </summary>\n+    /// <param name=\"architecture\">The architecture defining the structure of the neural network.</param>\n+    /// <param name=\"lossFunction\">Optional loss function to use for training. If null, uses a default based on task type (CrossEntropy for classification, MSE for regression).</param>\n+    /// <remarks>\n+    /// <para>\n+    /// This constructor creates a new NeuralNetworkModel instance with the specified architecture. It initializes\n+    /// the underlying neural network based on the architecture provided. The architecture determines the network's\n+    /// structure, including the number and type of layers, the input and output dimensions, and the type of task\n+    /// the network is designed to perform.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This constructor creates a new neural network model with the specified design.\n+    ///\n+    /// When creating a NeuralNetworkModel:\n+    /// - You provide an architecture that defines the network's structure\n+    /// - The constructor creates the actual neural network based on this design\n+    /// - The model is ready to be trained or to make predictions\n+    ///\n+    /// The architecture is crucial as it determines what kind of data the network can process\n+    /// and what kind of problems it can solve. Different architectures work better for\n+    /// different types of problems.\n+    /// </para>\n+    /// </remarks>\n+    public NeuralNetworkModel(NeuralNetworkArchitecture<T> architecture, ILossFunction<T>? lossFunction = null)\n+    {\n+        Architecture = architecture ?? throw new ArgumentNullException(nameof(architecture));\n+        Network = new NeuralNetwork<T>(architecture);\n+        _learningRate = _numOps.FromDouble(0.01); // Default learning rate\n+        _defaultLossFunction = lossFunction ?? NeuralNetworkHelper<T>.GetDefaultLossFunction(architecture.TaskType);\n+    }\n+\n+    /// <summary>\n+    /// Gets the default loss function used by this model for gradient computation.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// The default loss function is determined by the network's task type:\n+    /// - Classification tasks use CrossEntropyLoss\n+    /// - Regression tasks use MeanSquaredErrorLoss\n+    /// - Custom loss functions can be provided via the constructor\n+    /// </para>\n+    /// </remarks>\n+    public ILossFunction<T> DefaultLossFunction => _defaultLossFunction;\n+\n+    /// <summary>\n+    /// Gets the number of features used by the model.\n+    /// </summary>\n+    /// <value>An integer representing the number of input features.</value>\n+    /// <remarks>\n+    /// <para>\n+    /// This property returns the number of features that the model uses, which is determined by the input size\n+    /// of the neural network. For one-dimensional inputs, this is simply the input size. For multi-dimensional\n+    /// inputs, this is the total number of input elements (calculated as InputHeight * InputWidth * InputDepth).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This tells you how many input variables the neural network uses.\n+    /// \n+    /// The feature count:\n+    /// - For simple data, it's the number of input values (like age, height, weight)\n+    /// - For image data, it's the total number of pixels times the number of color channels\n+    /// - For text data, it might be the vocabulary size or embedding dimension\n+    /// \n+    /// This helps you understand how much input information the network is considering,\n+    /// and it's important for ensuring your input data has the right dimensions.\n+    /// </para>\n+    /// </remarks>\n+    public int FeatureCount => Architecture.CalculatedInputSize;\n+\n+    /// <summary>\n+    /// Gets the complexity of the model.\n+    /// </summary>\n+    /// <value>An integer representing the model's complexity.</value>\n+    /// <remarks>\n+    /// <para>\n+    /// This property returns a measure of the model's complexity, which is calculated as the total number of\n+    /// trainable parameters (weights and biases) in the neural network. The complexity of a neural network is\n+    /// an important factor in understanding its capacity to learn, its potential for overfitting, and its\n+    /// computational requirements.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This tells you how complex the neural network is.\n+    /// \n+    /// The complexity:\n+    /// - Is measured by the total number of adjustable parameters in the network\n+    /// - Higher complexity means the network can learn more complex patterns\n+    /// - But higher complexity also means more training data is needed\n+    /// - And higher complexity increases the risk of overfitting\n+    /// \n+    /// A simple network might have hundreds of parameters,\n+    /// while deep networks can have millions or billions.\n+    /// </para>\n+    /// </remarks>\n+    public int Complexity => Network.GetParameterCount();\n+\n+    /// <summary>\n+    /// Sets the learning rate for training the model.\n+    /// </summary>\n+    /// <param name=\"learningRate\">The learning rate to use during training.</param>\n+    /// <returns>This model instance for method chaining.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method sets the learning rate used during training. The learning rate controls how quickly the model\n+    /// adapts to the training data. A higher learning rate means faster learning but may cause instability, while\n+    /// a lower learning rate means slower but more stable learning.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This lets you control how big each learning step is during training.\n+    /// \n+    /// The learning rate:\n+    /// - Controls how quickly the network adjusts its weights\n+    /// - Smaller values (like 0.001) make training more stable but slower\n+    /// - Larger values (like 0.1) make training faster but potentially unstable\n+    /// \n+    /// Finding the right learning rate is often a process of trial and error.\n+    /// This method lets you set it to the value you want to try.\n+    /// </para>\n+    /// </remarks>\n+    public NeuralNetworkModel<T> SetLearningRate(T learningRate)\n+    {\n+        _learningRate = learningRate;\n+        return this;\n+    }\n+\n+    /// <summary>\n+    /// Sets whether the model is in training mode or prediction mode.\n+    /// </summary>\n+    /// <param name=\"isTraining\">True for training mode, false for prediction mode.</param>\n+    /// <returns>This model instance for method chaining.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method sets whether the model is in training mode or prediction mode. Some components of neural networks\n+    /// behave differently during training versus prediction, such as dropout layers, which randomly disable neurons\n+    /// during training but not during prediction.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This switches the network between learning mode and prediction mode.\n+    /// \n+    /// The two modes are:\n+    /// - Training mode: The network is learning and updating its weights\n+    /// - Prediction mode: The network is using what it learned to make predictions\n+    /// \n+    /// Some special layers like Dropout and BatchNormalization work differently\n+    /// depending on which mode the network is in. This method lets you switch between them.\n+    /// </para>\n+    /// </remarks>\n+    public NeuralNetworkModel<T> SetTrainingMode(bool isTraining)\n+    {\n+        _isTrainingMode = isTraining;\n+        Network.SetTrainingMode(isTraining);\n+        return this;\n+    }\n+\n+    /// <summary>\n+    /// Determines whether a specific feature is used by the model.\n+    /// </summary>\n+    /// <param name=\"featureIndex\">The index of the feature to check.</param>\n+    /// <returns>Always returns true for neural networks, as they typically use all input features.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method determines whether a specific feature is used by the model. For neural networks, all features\n+    /// are typically used in some capacity, so this method always returns true. Unlike some linear models where\n+    /// features can have zero coefficients and therefore no impact, neural networks generally incorporate all\n+    /// input features, though they may learn to assign different importance to different features during training.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This method checks if a particular input variable affects the model's predictions.\n+    /// \n+    /// For neural networks:\n+    /// - This method always returns true\n+    /// - Neural networks typically use all input features in some way\n+    /// - The network learns which features are important during training\n+    /// - Even if a feature isn't useful, the network will learn to assign it less weight\n+    /// \n+    /// This differs from simpler models like linear regression,\n+    /// where features can be explicitly excluded with zero coefficients.\n+    /// </para>\n+    /// </remarks>\n+    public bool IsFeatureUsed(int featureIndex)\n+    {\n+        if (featureIndex < 0 || featureIndex >= FeatureCount)\n+        {\n+            throw new ArgumentOutOfRangeException(nameof(featureIndex), \n+                $\"Feature index must be between 0 and {FeatureCount - 1}\");\n+        }\n+        \n+        // Neural networks typically use all input features in some capacity\n+        return true;\n+    }\n+\n+    /// <summary>\n+    /// Computes gradients of the loss function with respect to model parameters WITHOUT updating parameters.\n+    /// </summary>\n+    /// <param name=\"input\">The input tensor.</param>\n+    /// <param name=\"target\">The target/expected output tensor.</param>\n+    /// <param name=\"lossFunction\">The loss function to use. If null, uses the model's default loss function.</param>\n+    /// <returns>A vector containing gradients with respect to all model parameters.</returns>\n+    /// <exception cref=\"InvalidOperationException\">If the network doesn't support training or loss function is null and no default is configured.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// This method performs a forward pass, computes the loss, and back-propagates to compute gradients,\n+    /// but does NOT update the model's parameters. The parameters remain unchanged after this call.\n+    /// </para>\n+    /// <para><b>For Beginners:</b>\n+    /// This method calculates which direction to move the model's parameters to reduce error,\n+    /// but it doesn't actually move them. This is useful for:\n+    /// - Distributed training: compute gradients on different machines and average them\n+    /// - Custom optimization: apply your own learning algorithm to the gradients\n+    /// - Analysis: inspect gradient values to understand what the model is learning\n+    /// </para>\n+    /// </remarks>\n+    public Vector<T> ComputeGradients(Tensor<T> input, Tensor<T> target, ILossFunction<T>? lossFunction = null)\n+    {\n+        if (!Network.SupportsTraining)\n+        {\n+            throw new InvalidOperationException(\"This neural network does not support training.\");\n+        }\n+\n+        var loss = lossFunction ?? DefaultLossFunction;\n+\n+        // Ensure the network is in training mode\n+        Network.SetTrainingMode(true);\n+\n+        // Convert tensors to the format expected by the network\n+        Vector<T> inputVector = input.ToVector();\n+        Vector<T> targetVector = target.ToVector();\n+\n+        // Forward pass with memory to store intermediate values for backpropagation\n+        Tensor<T> outputTensor = Network.ForwardWithMemory(Tensor<T>.FromVector(inputVector));\n+        Vector<T> outputVector = outputTensor.ToVector();\n+\n+        // Calculate error gradient using the loss function\n+        Vector<T> error = loss.CalculateDerivative(outputVector, targetVector);\n+\n+        // Backpropagate error through the network\n+        Network.Backpropagate(Tensor<T>.FromVector(error));\n+\n+        // Get and return gradients from the network\n+        Vector<T> gradients = Network.GetParameterGradients();\n+        return gradients;\n+    }\n+\n+    /// <summary>\n+    /// Applies pre-computed gradients to update the model parameters.\n+    /// </summary>\n+    /// <param name=\"gradients\">The gradient vector to apply.</param>\n+    /// <param name=\"learningRate\">The learning rate for the update.</param>\n+    /// <exception cref=\"ArgumentNullException\">If gradients is null.</exception>\n+    /// <exception cref=\"ArgumentException\">If gradient vector length doesn't match parameter count.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// Updates parameters using: ╬╕ = ╬╕ - learningRate * gradients\n+    /// </para>\n+    /// <para><b>For Beginners:</b>\n+    /// After computing gradients (seeing which direction to move),\n+    /// this method actually moves the model in that direction.\n+    /// The learning rate controls how big of a step to take.\n+    ///\n+    /// In distributed training, this applies the synchronized (averaged) gradients after\n+    /// communication across workers. Each worker applies the same averaged gradients\n+    /// to keep parameters consistent.\n+    /// </para>\n+    /// </remarks>\n+    public void ApplyGradients(Vector<T> gradients, T learningRate)\n+    {\n+        if (gradients == null)\n+            throw new ArgumentNullException(nameof(gradients));\n+\n+        var currentParams = Network.GetParameters();\n+\n+        if (gradients.Length != currentParams.Length)\n+        {\n+            throw new ArgumentException(\n+                $\"Gradient vector length ({gradients.Length}) must match parameter count ({currentParams.Length})\",\n+                nameof(gradients));\n+        }\n+\n+        var newParams = new Vector<T>(currentParams.Length);\n+\n+        // Apply gradient descent: params = params - learningRate * gradients\n+        for (int i = 0; i < currentParams.Length; i++)\n+        {\n+            T update = _numOps.Multiply(learningRate, gradients[i]);\n+            newParams[i] = _numOps.Subtract(currentParams[i], update);\n+        }\n+\n+        Network.UpdateParameters(newParams);\n+    }\n+\n+    /// <summary>\n+    /// Trains the model with the provided input and expected output.\n+    /// </summary>\n+    /// <param name=\"input\">The input tensor to train with.</param>\n+    /// <param name=\"expectedOutput\">The expected output tensor.</param>\n+    /// <remarks>\n+    /// <para>\n+    /// This method trains the neural network with the provided input and expected output tensors.\n+    /// It sets the network to training mode, performs a forward pass through the network, calculates\n+    /// the error between the predicted output and the expected output, and backpropagates the error\n+    /// to update the network's weights.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This method teaches the neural network using an example.\n+    /// \n+    /// During training:\n+    /// 1. The input data is sent through the network (forward pass)\n+    /// 2. The network makes a prediction\n+    /// 3. The prediction is compared to the expected output\n+    /// 4. The error is calculated\n+    /// 5. The network adjusts its weights to reduce the error\n+    /// \n+    /// This process is repeated with many examples to gradually improve the network's performance.\n+    /// Each example helps the network learn a little more about the patterns in your data.\n+    /// </para>\n+    /// </remarks>\n+    public void Train(Tensor<T> input, Tensor<T> expectedOutput)\n+    {\n+        if (!Network.SupportsTraining)\n+        {\n+            throw new InvalidOperationException(\"This neural network does not support training.\");\n+        }\n+\n+        // Save the current training mode to restore it after training\n+        bool previousTrainingMode = _isTrainingMode;\n+\n+        try\n+        {\n+            // Ensure the network is in training mode\n+            Network.SetTrainingMode(true);\n+\n+            // Convert tensors to the format expected by the network\n+            Vector<T> inputVector = input.ToVector();\n+            Vector<T> expectedOutputVector = expectedOutput.ToVector();\n+\n+            // Forward pass with memory to store intermediate values for backpropagation\n+            Tensor<T> outputTensor = Network.ForwardWithMemory(Tensor<T>.FromVector(inputVector));\n+            Vector<T> outputVector = outputTensor.ToVector();\n+\n+            // Calculate error gradient\n+            Vector<T> error = CalculateError(outputVector, expectedOutputVector);\n+\n+            // Backpropagate error\n+            Network.Backpropagate(Tensor<T>.FromVector(error));\n+\n+            // Update weights using the calculated gradients\n+            Vector<T> gradients = Network.GetParameterGradients();\n+            Vector<T> currentParams = Network.GetParameters();\n+            Vector<T> newParams = new Vector<T>(currentParams.Length);\n+\n+            for (int i = 0; i < currentParams.Length; i++)\n+            {\n+                // Simple gradient descent: param = param - learningRate * gradient\n+                T update = _numOps.Multiply(_learningRate, gradients[i]);\n+                newParams[i] = _numOps.Subtract(currentParams[i], update);\n+            }\n+\n+            Network.UpdateParameters(newParams);\n+        }\n+        finally\n+        {\n+            // Restore the original training mode\n+            // This ensures that if the model was in inference mode before,\n+            // it returns to inference mode after training, preventing\n+            // dropout and batch normalization from being in the wrong state\n+            SetTrainingMode(previousTrainingMode);\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Uses the model to make a prediction for the given input.\n+    /// </summary>\n+    /// <param name=\"input\">The input tensor to make a prediction for.</param>\n+    /// <returns>The predicted output tensor.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method uses the trained neural network to make a prediction for the given input tensor.\n+    /// It sets the network to prediction mode (not training mode), performs a forward pass through\n+    /// the network, and returns the output as a tensor with the appropriate shape.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This method makes predictions using what the neural network has learned.\n+    /// \n+    /// When making a prediction:\n+    /// 1. The input data is sent through the network\n+    /// 2. Each layer processes the data based on its learned weights\n+    /// 3. The final layer produces the output (prediction)\n+    /// \n+    /// Unlike training, no weights are updated during prediction - the network\n+    /// is simply using what it already knows to make its best guess.\n+    /// </para>\n+    /// </remarks>\n+    public Tensor<T> Predict(Tensor<T> input)\n+    {\n+        // Set to prediction mode (not training)\n+        Network.SetTrainingMode(false);\n+    \n+        // Forward pass through the network\n+        return Network.Predict(input);\n+    }\n+\n+    /// <summary>\n+    /// Trains the network with the provided input and expected output vectors.\n+    /// </summary>\n+    /// <param name=\"input\">The input vector.</param>\n+    /// <param name=\"expectedOutput\">The expected output vector.</param>\n+    /// <remarks>\n+    /// <para>\n+    /// This method implements the actual training of the neural network. It performs forward propagation to compute\n+    /// the network's output, calculates the error gradient, and then performs backpropagation to update the network's\n+    /// parameters. This is the core of the learning process for neural networks. The specific implementation may vary\n+    /// depending on the type of neural network and the training algorithm being used.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This method handles the details of teaching the neural network.\n+    /// \n+    /// During training:\n+    /// 1. The input data is sent through the network (forward propagation)\n+    /// 2. The error between the network's output and the expected output is calculated\n+    /// 3. This error is sent backward through the network (backpropagation)\n+    /// 4. The network adjusts its weights to reduce the error\n+    /// \n+    /// This process is repeated many times over different examples,\n+    /// gradually improving the network's accuracy.\n+    /// </para>\n+    /// </remarks>\n+    private void TrainNetwork(Tensor<T> input, Tensor<T> expectedOutput)\n+    {\n+        // Implementation depends on the specific neural network type\n+        if (!Network.SupportsTraining)\n+        {\n+            throw new InvalidOperationException(\"This neural network does not support training.\");\n+        }\n+        \n+        // Forward pass with memory to store intermediate values\n+        Tensor<T> outputTensor = Network.ForwardWithMemory(input);\n+        Vector<T> output = outputTensor.ToVector();\n+\n+        // Calculate error gradient\n+        Vector<T> error = CalculateError(output, expectedOutput.ToVector());\n+\n+        // Backpropagate error\n+        Network.Backpropagate(Tensor<T>.FromVector(error));\n+        \n+        // Update weights using the calculated gradients\n+        Vector<T> gradients = Network.GetParameterGradients();\n+        Vector<T> currentParams = Network.GetParameters();\n+        Vector<T> newParams = new Vector<T>(currentParams.Length);\n+        \n+        for (int i = 0; i < currentParams.Length; i++)\n+        {\n+            // Simple gradient descent: param = param - learningRate * gradient\n+            T update = _numOps.Multiply(_learningRate, gradients[i]);\n+            newParams[i] = _numOps.Subtract(currentParams[i], update);\n+        }\n+        \n+        Network.UpdateParameters(newParams);\n+    }\n+\n+    /// <summary>\n+    /// Calculates the error between predicted and expected outputs.\n+    /// </summary>\n+    /// <param name=\"predicted\">The predicted output values.</param>\n+    /// <param name=\"expected\">The expected output values.</param>\n+    /// <returns>A vector containing the error for each output.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method calculates the error between the predicted output values and the expected output values.\n+    /// The error is calculated using a loss function appropriate for the network's task type (e.g., mean squared error\n+    /// for regression tasks, cross-entropy for classification tasks). The resulting error vector is used during\n+    /// backpropagation to update the network's weights.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This method measures how wrong each prediction is compared to \n+    /// the expected value. These error values are used to adjust the network's weights during training.\n+    /// \n+    /// Different types of problems use different ways to measure error:\n+    /// - For predicting numeric values (regression), we often use squared differences\n+    /// - For classifying into categories, we often use cross-entropy\n+    /// \n+    /// This method automatically chooses the right error measure based on what\n+    /// kind of problem your network is solving.\n+    /// </para>\n+    /// </remarks>\n+    private Vector<T> CalculateError(Vector<T> predicted, Vector<T> expected)\n+    {\n+        // Check if vectors have the same length\n+        if (predicted.Length != expected.Length)\n+        {\n+            throw new ArgumentException(\"Predicted and expected vectors must have the same length.\");\n+        }\n+\n+        // Get appropriate loss function based on the task type\n+        var lossFunction = NeuralNetworkHelper<T>.GetDefaultLossFunction(Architecture.TaskType);\n+    \n+        // Calculate gradients based on the loss function\n+        Vector<T> error = lossFunction.CalculateDerivative(predicted, expected);\n+    \n+        return error;\n+    }\n+\n+    /// <summary>\n+    /// Gets metadata about the model.\n+    /// </summary>\n+    /// <returns>A ModelMetadata object containing information about the model.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method returns metadata about the model, including its type, feature count, complexity, and additional\n+    /// information about the neural network. The metadata includes the model type (Neural Network), the number of\n+    /// features, the complexity (total parameter count), a description, and additional information such as the\n+    /// architecture details, layer counts, and activation functions used. This metadata is useful for model selection,\n+    /// analysis, and visualization.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This method returns detailed information about the neural network model.\n+    /// \n+    /// The metadata includes:\n+    /// - Basic properties like model type, feature count, and complexity\n+    /// - Architecture details like layer counts and types\n+    /// - Statistics about the model's parameters\n+    /// \n+    /// This information is useful for:\n+    /// - Understanding the model's structure\n+    /// - Comparing different models\n+    /// - Analyzing the model's capabilities\n+    /// - Documenting the model for future reference\n+    /// </para>\n+    /// </remarks>\n+    public ModelMetadata<T> GetModelMetadata()\n+    {\n+        int[] layerSizes = Architecture.GetLayerSizes();\n+        \n+        return new ModelMetadata<T>\n+        {\n+            FeatureCount = FeatureCount,\n+            Complexity = Complexity,\n+            Description = $\"Neural Network model with {layerSizes.Length} layers\",\n+            AdditionalInfo = new Dictionary<string, object>\n+            {\n+                { \"LayerSizes\", layerSizes },\n+                { \"InputShape\", Architecture.GetInputShape() },\n+                { \"OutputShape\", Architecture.GetOutputShape() },\n+                { \"TaskType\", Architecture.TaskType.ToString() },\n+                { \"InputType\", Architecture.InputType.ToString() },\n+                { \"HiddenLayerCount\", Architecture.GetHiddenLayerSizes().Length },\n+                { \"ParameterCount\", Network.GetParameterCount() },\n+                { \"SupportsTraining\", Network.SupportsTraining }\n+            }\n+        };\n+    }\n+\n+    /// <summary>\n+    /// Serializes the model to a byte array.\n+    /// </summary>\n+    /// <returns>A byte array containing the serialized model.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method serializes the model to a byte array by writing the architecture details and the network parameters.\n+    /// The serialization format includes the architecture information followed by the network parameters. This allows\n+    /// the model to be stored or transmitted and later reconstructed using the Deserialize method.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This method converts the neural network model to a byte array that can be saved or transmitted.\n+    /// \n+    /// When serializing the model:\n+    /// - Both the architecture (structure) and parameters (weights) are saved\n+    /// - The data is formatted in a way that can be efficiently stored\n+    /// - The resulting byte array contains everything needed to reconstruct the model\n+    /// \n+    /// This is useful for:\n+    /// - Saving trained models to disk\n+    /// - Sharing models with others\n+    /// - Deploying models to production systems\n+    /// - Creating model checkpoints during long training processes\n+    /// </para>\n+    /// </remarks>\n+    public byte[] Serialize()\n+    {\n+        using MemoryStream ms = new MemoryStream();\n+        using BinaryWriter writer = new BinaryWriter(ms);\n+        \n+        // Write a version number for forward compatibility\n+        writer.Write(1); // Version 1\n+        \n+        // Write the architecture type\n+        writer.Write(Architecture.GetType().FullName ?? \"Unknown\");\n+        \n+        // Serialize the architecture\n+        // In a real implementation, we would need a more sophisticated approach\n+        // Here we just write key architecture properties\n+        writer.Write((int)Architecture.InputType);\n+        writer.Write((int)Architecture.TaskType);\n+        writer.Write((int)Architecture.Complexity);\n+        writer.Write(Architecture.InputSize);\n+        writer.Write(Architecture.OutputSize);\n+        writer.Write(Architecture.InputHeight);\n+        writer.Write(Architecture.InputWidth);\n+        writer.Write(Architecture.InputDepth);\n+        \n+        // Serialize the network parameters\n+        var serializedNetwork = Network.Serialize();\n+        writer.Write(serializedNetwork.Length);\n+        writer.Write(serializedNetwork);\n+        \n+        return ms.ToArray();\n+    }\n+\n+    /// <summary>\n+    /// Deserializes the model from a byte array.\n+    /// </summary>\n+    /// <param name=\"data\">The byte array containing the serialized model.</param>\n+    /// <remarks>\n+    /// <para>\n+    /// This method deserializes the model from a byte array by reading the architecture details and the network parameters.\n+    /// It expects the same format as produced by the Serialize method: the architecture information followed by the network\n+    /// parameters. This allows a model that was previously serialized to be reconstructed.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This method reconstructs a neural network model from a byte array created by Serialize.\n+    /// \n+    /// When deserializing the model:\n+    /// - The architecture is read first to recreate the structure\n+    /// - Then the parameters (weights) are loaded into that structure\n+    /// - The resulting model is identical to the one that was serialized\n+    /// \n+    /// This is used when:\n+    /// - Loading a previously saved model\n+    /// - Receiving a model from another system\n+    /// - Resuming training from a checkpoint\n+    /// \n+    /// After deserialization, the model can be used for predictions or further training\n+    /// just as if it had never been serialized.\n+    /// </para>\n+    /// </remarks>\n+    public void Deserialize(byte[] data)\n+    {\n+        if (data == null || data.Length == 0)\n+        {\n+            throw new ArgumentException(\"Serialized data cannot be null or empty.\", nameof(data));\n+        }\n+        \n+        using MemoryStream ms = new MemoryStream(data);\n+        using BinaryReader reader = new BinaryReader(ms);\n+            \n+        // Read version number\n+        int version = reader.ReadInt32();\n+            \n+        // Read architecture type\n+        string architectureType = reader.ReadString();\n+            \n+        // Read architecture properties\n+        InputType inputType = (InputType)reader.ReadInt32();\n+        NeuralNetworkTaskType taskType = (NeuralNetworkTaskType)reader.ReadInt32();\n+        NetworkComplexity complexity = (NetworkComplexity)reader.ReadInt32();\n+        int inputSize = reader.ReadInt32();\n+        int outputSize = reader.ReadInt32();\n+        int inputHeight = reader.ReadInt32();\n+        int inputWidth = reader.ReadInt32();\n+        int inputDepth = reader.ReadInt32();\n+            \n+        // Check if the architecture matches\n+        if (Architecture.InputType != inputType ||\n+            Architecture.TaskType != taskType ||\n+            Architecture.InputSize != inputSize ||\n+            Architecture.OutputSize != outputSize)\n+        {\n+            throw new InvalidOperationException(\n+                \"Serialized network architecture doesn't match this model's architecture.\");\n+        }\n+        \n+        var length = reader.ReadInt32();\n+        var bytes = reader.ReadBytes(length);\n+        // Deserialize the network parameters\n+        Network.Deserialize(bytes);\n+    }\n+\n+    /// <summary>\n+    /// Gets all trainable parameters of the neural network as a single vector.\n+    /// </summary>\n+    /// <returns>A vector containing all trainable parameters.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method returns all trainable parameters of the neural network as a single vector.\n+    /// These parameters include weights and biases from all layers that support training.\n+    /// The vector can be used to save the model's state, apply optimization techniques,\n+    /// or transfer learning between models.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This method collects all the learned weights and biases from the neural network\n+    /// into a single list. This is useful for saving the model, optimizing it, or transferring its knowledge.\n+    /// \n+    /// The parameters:\n+    /// - Are the numbers that the neural network has learned during training\n+    /// - Include weights (how strongly neurons connect to each other)\n+    /// - Include biases (baseline activation levels for neurons)\n+    /// \n+    /// A simple network might have hundreds of parameters, while modern deep networks\n+    /// often have millions or billions of parameters.\n+    /// </para>\n+    /// </remarks>\n+    public Vector<T> GetParameters()\n+    {\n+        return Network.GetParameters();\n+    }\n+\n+    /// <summary>\n+    /// Updates the model with new parameter values.\n+    /// </summary>\n+    /// <param name=\"parameters\">The new parameter values to use.</param>\n+    /// <returns>The updated model.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method creates a new model with the same architecture as the current model but with the provided\n+    /// parameter values. This allows creating a modified version of the model without altering the original.\n+    /// The new parameters must match the number of parameters in the original model.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This method lets you change all the weights and biases in the neural network\n+    /// at once by providing a list of new values. It's useful when optimizing the model or loading saved weights.\n+    /// \n+    /// When updating parameters:\n+    /// - A new model is created with the same structure as this one\n+    /// - The new model's weights and biases are set to the values you provide\n+    /// - The original model remains unchanged\n+    /// \n+    /// This is useful for:\n+    /// - Loading pre-trained weights\n+    /// - Testing different parameter values\n+    /// - Implementing evolutionary algorithms\n+    /// - Creating ensemble models with different parameter sets\n+    /// </para>\n+    /// </remarks>\n+    public IFullModel<T, Tensor<T>, Tensor<T>> WithParameters(Vector<T> parameters)\n+    {\n+        // Create a new model with the same architecture\n+        var newModel = new NeuralNetworkModel<T>(Architecture);\n+    \n+        // Update the parameters of the new model\n+        newModel.Network.UpdateParameters(parameters);\n+    \n+        return newModel;\n+    }\n+\n+    /// <summary>\n+    /// Gets the indices of all features used by this model.\n+    /// </summary>\n+    /// <returns>A collection of feature indices.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method returns the indices of all features that are used by the model. For neural networks,\n+    /// this typically includes all features from 0 to FeatureCount-1, as neural networks generally use\n+    /// all input features to some extent.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This method returns a list of which input features the model actually uses.\n+    /// For neural networks, this typically includes all available features unless specific feature selection has been applied.\n+    /// \n+    /// Unlike some simpler models (like linear regression with feature selection) where\n+    /// certain inputs might be completely ignored, neural networks typically process\n+    /// all input features and learn which ones are important during training.\n+    /// \n+    /// This method returns all feature indices from 0 to (FeatureCount-1).\n+    /// </para>\n+    /// </remarks>\n+    public IEnumerable<int> GetActiveFeatureIndices()\n+    {\n+        // Neural networks typically use all input features\n+        // Return indices for all features from 0 to FeatureCount-1\n+        return Enumerable.Range(0, FeatureCount);\n+    }\n+\n+    /// <summary>\n+    /// Sets the parameters for this model.\n+    /// </summary>\n+    /// <param name=\"parameters\">A vector containing the model parameters.</param>\n+    public void SetParameters(Vector<T> parameters)\n+    {\n+        if (Network == null)\n+        {\n+            throw new InvalidOperationException(\"Network has not been initialized.\");\n+        }\n+\n+        Network.SetParameters(parameters);\n+    }\n+\n+    /// <summary>\n+    /// Sets the active feature indices for this model.\n+    /// </summary>\n+    /// <param name=\"featureIndices\">The indices of features to activate.</param>\n+    public void SetActiveFeatureIndices(IEnumerable<int> featureIndices)\n+    {\n+        // Neural networks typically don't support feature masking after training\n+        throw new NotSupportedException(\"Neural networks do not support setting active features after network construction.\");\n+    }\n+\n+    /// <summary>\n+    /// Gets the feature importance scores as a dictionary.\n+    /// </summary>\n+    /// <returns>A dictionary mapping feature names to their importance scores.</returns>\n+    /// <exception cref=\"NotSupportedException\">\n+    /// This method is not supported for neural networks. Feature importance in neural networks\n+    /// requires specialized techniques like gradient-based attribution or permutation importance.\n+    /// </exception>\n+    public Dictionary<string, T> GetFeatureImportance()\n+    {\n+        // Neural network feature importance requires specialized techniques like:\n+        // - Gradient-based attribution methods (e.g., Integrated Gradients, SHAP)\n+        // - Permutation importance\n+        // - Layer-wise relevance propagation\n+        // These are complex to implement correctly and beyond the scope of this basic method.\n+        throw new NotSupportedException(\n+            \"Feature importance is not supported for neural networks through this method. \" +\n+            \"Neural networks require specialized techniques like gradient-based attribution, \" +\n+            \"permutation importance, or SHAP values to properly assess feature importance.\");\n+    }\n+\n+    /// <summary>\n+    /// Creates a deep copy of this model.\n+    /// </summary>\n+    /// <returns>A new instance with the same architecture and parameters.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method creates a deep copy of the neural network model, including both its architecture and\n+    /// learned parameters. The new model is independent of the original, so changes to one will not affect\n+    /// the other. This is useful for creating variations of a model while preserving the original.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This method creates an exact duplicate of the neural network,\n+    /// with the same structure and the same learned weights. This is useful when you need to\n+    /// make changes to a model without affecting the original.\n+    /// \n+    /// The deep copy:\n+    /// - Has identical architecture (same layers, neurons, connections)\n+    /// - Has identical parameters (same weights and biases)\n+    /// - Is completely independent of the original\n+    /// \n+    /// This is useful for:\n+    /// - Creating model variants for experimentation\n+    /// - Saving a checkpoint before making changes\n+    /// - Creating ensemble models\n+    /// - Implementing techniques like dropout ensemble\n+    /// </para>\n+    /// </remarks>\n+    public IFullModel<T, Tensor<T>, Tensor<T>> DeepCopy()\n+    {\n+        // Create a new model with the same architecture\n+        var copy = new NeuralNetworkModel<T>(Architecture);\n+        \n+        // Copy the network parameters\n+        var parameters = Network.GetParameters();\n+        copy.Network.UpdateParameters(parameters);\n+        \n+        // Copy additional properties\n+        copy._learningRate = _learningRate;\n+        copy._isTrainingMode = _isTrainingMode;\n+        copy.Network.SetTrainingMode(_isTrainingMode);\n+        \n+        return copy;\n+    }"
        }
      ]
    }
  },
  {
    "id": "PRRT_kwDOKSXUF85hg5_t",
    "isResolved": false,
    "isOutdated": false,
    "path": "src/Interfaces/IPredictionModelBuilder.cs",
    "line": 595,
    "originalLine": 593,
    "comments": {
      "nodes": [
        {
          "id": "PRRC_kwDOKSXUF86V46CV",
          "author": {
            "login": "coderabbitai"
          },
          "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Doc: correct generic arguments in the Quick Start sample.**\n\nOptions generic order is <T, TInput, TOutput>. The sample shows them mis-ordered.\n\n\nApply:\n\n```diff\n-    var distillationOptions = new KnowledgeDistillationOptions<Vector<double>, Vector<double>, double>\n+    var distillationOptions = new KnowledgeDistillationOptions<double, Vector<double>, Vector<double>>\n```\n\nAlso add a note that the student model must use Vector<T> input/output for the current KD path (BuildAsync will throw otherwise). [Based on learnings]\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/Interfaces/IPredictionModelBuilder.cs around lines 565 to 593, the Quick\nStart sample uses the KnowledgeDistillationOptions generics in the wrong order;\nupdate any sample or XML docs so the options generic order is\nKnowledgeDistillationOptions<T, TInput, TOutput> (matching the method\nsignature). Also append a short note in the remarks that for the current KD\nimplementation the student model must use Vector<T> for both input and output\n(BuildAsync will throw otherwise), so callers know this constraint.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:chinchilla -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
          "createdAt": "2025-11-11T15:42:47Z",
          "path": "src/Interfaces/IPredictionModelBuilder.cs",
          "line": 595,
          "diffHunk": "@@ -562,6 +562,35 @@ IPredictionModelBuilder<T, TInput, TOutput> ConfigureDistributedTraining(\n     /// <returns>The builder instance for method chaining.</returns>\n     IPredictionModelBuilder<T, TInput, TOutput> ConfigureCrossValidation(ICrossValidator<T, TInput, TOutput> crossValidator);\n \n+    /// <summary>\n+    /// Configures knowledge distillation for training a smaller student model from a larger teacher model.\n+    /// </summary>\n+    /// <remarks>\n+    /// Knowledge distillation enables model compression by transferring knowledge from a large,\n+    /// accurate teacher model to a smaller, faster student model. The student learns to mimic\n+    /// the teacher's predictions and internal representations.\n+    ///\n+    /// <b>For Beginners:</b> Knowledge distillation is like having an expert teacher help train\n+    /// a smaller, faster student. The student model learns not just from the training labels,\n+    /// but also from the teacher's \"soft\" predictions which contain richer information about\n+    /// relationships between classes.\n+    ///\n+    /// Benefits:\n+    /// - Model compression: Deploy 10x smaller models with 90%+ of original accuracy\n+    /// - Faster inference: Smaller models run significantly faster\n+    /// - Lower memory: Fits on edge devices and mobile platforms\n+    /// - Better generalization: Learning from soft labels often improves accuracy\n+    ///\n+    /// Common use cases:\n+    /// - DistilBERT: 40% smaller than BERT, 97% performance, 60% faster\n+    /// - MobileNet: Distilled from ResNet for mobile deployment\n+    /// - Edge AI: Deploy powerful models on resource-constrained devices\n+    /// </remarks>\n+    /// <param name=\"options\">The knowledge distillation configuration options.</param>\n+    /// <returns>The builder instance for method chaining.</returns>\n+    IPredictionModelBuilder<T, TInput, TOutput> ConfigureKnowledgeDistillation(\n+        KnowledgeDistillationOptions<T, TInput, TOutput> options);\n+"
        }
      ]
    }
  },
  {
    "id": "PRRT_kwDOKSXUF85hg6Ah",
    "isResolved": false,
    "isOutdated": true,
    "path": "src/KnowledgeDistillation/Teachers/AdaptiveTeacherModel.cs",
    "line": null,
    "originalLine": 235,
    "comments": {
      "nodes": [
        {
          "id": "PRRC_kwDOKSXUF86V46De",
          "author": {
            "login": "coderabbitai"
          },
          "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Comment inverts ΓÇ£difficultyΓÇ¥ mapping.**\n\nCode maps higher difficulty to lower temperature (correct), but comment says ΓÇ£High difficulty (0)ΓÇ¥.\n\nApply:\n\n```diff\n-        // High difficulty (0) -> min temp (sharper)\n-        // Low difficulty (1) -> max temp (softer)\n+        // High difficulty (1) -> min temp (sharper)\n+        // Low difficulty (0) -> max temp (softer)\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/Teachers/AdaptiveTeacherModel.cs around lines 233\nto 235, the inline comment inverts the difficulty mapping; update the comment to\naccurately reflect the code logic where difficulty is 1.0 for high difficulty\nand 0.0 for low difficulty (so High difficulty (1) -> min temp (sharper) and Low\ndifficulty (0) -> max temp (softer)), or reword to describe the formula\ndirectly: adaptiveTemp = min + (1 - difficulty) * (max - min) meaning higher\ndifficulty yields lower temperature.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:chinchilla -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
          "createdAt": "2025-11-11T15:42:48Z",
          "path": "src/KnowledgeDistillation/Teachers/AdaptiveTeacherModel.cs",
          "line": null,
          "diffHunk": "@@ -0,0 +1,356 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+\n+namespace AiDotNet.KnowledgeDistillation.Teachers;\n+\n+/// <summary>\n+/// Adaptive teacher model that adjusts its teaching strategy based on student performance.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type for calculations (e.g., double, float).</typeparam>\n+/// <remarks>\n+/// <para><b>For Beginners:</b> Adaptive distillation is like a tutor who adjusts their teaching\n+/// style based on how well the student is doing. If the student is struggling, the teacher provides\n+/// more detailed guidance. If the student is doing well, the teacher can provide more advanced knowledge.</para>\n+///\n+/// <para><b>How It Works:</b>\n+/// - Monitor student's accuracy/confidence on different samples\n+/// - For hard samples (student struggles): Use lower temperature, focus on correct class\n+/// - For easy samples (student confident): Use higher temperature, reveal more class relationships\n+/// - Dynamically adjust temperature per sample based on student performance</para>\n+///\n+/// <para><b>Real-world Analogy:</b>\n+/// A math tutor notices a student struggling with algebra but excelling at geometry.\n+/// The tutor spends more time on algebra fundamentals while letting the student explore\n+/// advanced geometry concepts independently.</para>\n+///\n+/// <para><b>Benefits:</b>\n+/// - **Curriculum Learning**: Automatically adjusts difficulty\n+/// - **Efficient Training**: Focus teaching effort where needed\n+/// - **Better Convergence**: Avoids overwhelming student early\n+/// - **Personalized**: Adapts to this specific student</para>\n+///\n+/// <para><b>Adaptation Strategies:</b>\n+/// - **Sample-wise**: Different temperature per sample\n+/// - **Class-wise**: Different approach per class\n+/// - **Epoch-wise**: Adjust over training progress\n+/// - **Confidence-based**: Based on student's prediction confidence</para>\n+///\n+/// <para><b>References:</b>\n+/// - Mirzadeh et al. (2020). Improved Knowledge Distillation via Teacher Assistant. AAAI.\n+/// - Zhou et al. (2021). Rethinking Soft Labels for Knowledge Distillation: A Bias-Variance Tradeoff Perspective.</para>\n+/// </remarks>\n+public class AdaptiveTeacherModel<T> : TeacherModelBase<Vector<T>, Vector<T>, T>\n+{\n+    private readonly ITeacherModel<Vector<T>, Vector<T>> _baseTeacher;\n+    private readonly AdaptiveStrategy _strategy;\n+    private readonly double _minTemperature;\n+    private readonly double _maxTemperature;\n+    private readonly double _adaptationRate;\n+\n+    /// <summary>\n+    /// Gets the output dimension.\n+    /// </summary>\n+    public override int OutputDimension => _baseTeacher.OutputDimension;\n+\n+    /// <summary>\n+    /// Gets or sets the student performance tracker (confidence/accuracy per sample).\n+    /// </summary>\n+    public Dictionary<int, double> StudentPerformance { get; set; } = new();\n+\n+    /// <summary>\n+    /// Initializes a new instance of the AdaptiveTeacherModel class.\n+    /// </summary>\n+    /// <param name=\"baseTeacher\">The underlying teacher model.</param>\n+    /// <param name=\"strategy\">Adaptation strategy to use.</param>\n+    /// <param name=\"minTemperature\">Minimum temperature for hard samples (default: 1.0).</param>\n+    /// <param name=\"maxTemperature\">Maximum temperature for easy samples (default: 5.0).</param>\n+    /// <param name=\"adaptationRate\">How quickly to adapt (default: 0.1).</param>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> Create an adaptive teacher by providing:\n+    /// - Base teacher: The underlying expert model\n+    /// - Strategy: How to adapt (confidence-based recommended)\n+    /// - Temperature range: Control softness (1-5 typical)</para>\n+    ///\n+    /// <para>Example:\n+    /// <code>\n+    /// var baseTeacher = new TeacherModelWrapper&lt;double&gt;(trainedModel);\n+    /// var adaptiveTeacher = new AdaptiveTeacherModel&lt;double&gt;(\n+    ///     baseTeacher,\n+    ///     AdaptiveStrategy.ConfidenceBased,\n+    ///     minTemperature: 1.0,  // Sharp for hard samples\n+    ///     maxTemperature: 5.0   // Soft for easy samples\n+    /// );\n+    /// </code>\n+    /// </para>\n+    /// </remarks>\n+    public AdaptiveTeacherModel(\n+        ITeacherModel<Vector<T>, Vector<T>> baseTeacher,\n+        AdaptiveStrategy strategy = AdaptiveStrategy.ConfidenceBased,\n+        double minTemperature = 1.0,\n+        double maxTemperature = 5.0,\n+        double adaptationRate = 0.1)\n+    {\n+        _baseTeacher = baseTeacher ?? throw new ArgumentNullException(nameof(baseTeacher));\n+        _strategy = strategy;\n+        _minTemperature = minTemperature;\n+        _maxTemperature = maxTemperature;\n+        _adaptationRate = adaptationRate;\n+\n+        if (minTemperature <= 0 || maxTemperature <= minTemperature)\n+            throw new ArgumentException(\"Invalid temperature range\");\n+        if (adaptationRate <= 0 || adaptationRate > 1)\n+            throw new ArgumentException(\"Adaptation rate must be in (0, 1]\");\n+    }\n+\n+    /// <summary>\n+    /// Gets logits from the base teacher.\n+    /// </summary>\n+    public override Vector<T> GetLogits(Vector<T> input)\n+    {\n+        return _baseTeacher.GetLogits(input);\n+    }\n+\n+    /// <summary>\n+    /// Gets soft predictions with adaptive temperature based on sample difficulty.\n+    /// </summary>\n+    /// <param name=\"input\">Input data.</param>\n+    /// <param name=\"temperature\">Base temperature (will be adapted).</param>\n+    /// <returns>Soft predictions with adaptive temperature.</returns>\n+    public override Vector<T> GetSoftPredictions(Vector<T> input, double temperature = 1.0)\n+    {\n+        return GetSoftPredictions(input, temperature, sampleIndex: null);\n+    }\n+\n+    /// <summary>\n+    /// Gets soft predictions with adaptive temperature based on sample difficulty.\n+    /// </summary>\n+    /// <param name=\"input\">Input data.</param>\n+    /// <param name=\"temperature\">Base temperature (will be adapted).</param>\n+    /// <param name=\"sampleIndex\">Optional sample index to look up recorded performance (for AccuracyBased strategy).</param>\n+    /// <returns>Soft predictions with adaptive temperature.</returns>\n+    public Vector<T> GetSoftPredictions(Vector<T> input, double temperature, int? sampleIndex)\n+    {\n+        var logits = GetLogits(input);\n+\n+        // Compute adaptive temperature based on strategy\n+        double adaptiveTemp = ComputeAdaptiveTemperature(logits, temperature, sampleIndex);\n+\n+        return ApplyTemperatureSoftmax(logits, adaptiveTemp);\n+    }\n+\n+    /// <summary>\n+    /// Updates student performance tracking for adaptive adjustment.\n+    /// </summary>\n+    /// <param name=\"sampleIndex\">Index of the sample in batch.</param>\n+    /// <param name=\"studentPrediction\">Student's prediction.</param>\n+    /// <param name=\"trueLabel\">True label (optional).</param>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> Call this after each prediction to help the teacher\n+    /// learn which samples the student finds difficult.</para>\n+    /// </remarks>\n+    public void UpdateStudentPerformance(int sampleIndex, Vector<T> studentPrediction, Vector<T>? trueLabel = null)\n+    {\n+        double performance = 0;\n+\n+        switch (_strategy)\n+        {\n+            case AdaptiveStrategy.ConfidenceBased:\n+                // Max confidence as performance measure\n+                performance = GetMaxConfidence(studentPrediction);\n+                break;\n+\n+            case AdaptiveStrategy.AccuracyBased:\n+                if (trueLabel != null)\n+                {\n+                    // Correctness as performance measure\n+                    performance = IsCorrect(studentPrediction, trueLabel) ? 1.0 : 0.0;\n+                }\n+                break;\n+\n+            case AdaptiveStrategy.EntropyBased:\n+                // Low entropy = confident (high performance)\n+                performance = 1.0 - ComputeEntropy(studentPrediction);\n+                break;\n+        }\n+\n+        // Exponential moving average\n+        if (StudentPerformance.ContainsKey(sampleIndex))\n+        {\n+            StudentPerformance[sampleIndex] =\n+                _adaptationRate * performance + (1 - _adaptationRate) * StudentPerformance[sampleIndex];\n+        }\n+        else\n+        {\n+            StudentPerformance[sampleIndex] = performance;\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Computes adaptive temperature based on sample difficulty.\n+    /// </summary>\n+    /// <param name=\"logits\">Raw model outputs.</param>\n+    /// <param name=\"baseTemperature\">Base temperature to scale.</param>\n+    /// <param name=\"sampleIndex\">Optional sample index to look up recorded performance.</param>\n+    private double ComputeAdaptiveTemperature(Vector<T> logits, double baseTemperature, int? sampleIndex)\n+    {\n+        // Convert logits to normalized probabilities\n+        var probs = ApplyTemperatureSoftmax(logits, 1.0);\n+\n+        double difficulty = 0;\n+\n+        switch (_strategy)\n+        {\n+            case AdaptiveStrategy.ConfidenceBased:\n+                // Lower confidence = harder sample = lower temperature\n+                difficulty = 1.0 - GetMaxConfidence(probs);\n+                break;\n+\n+            case AdaptiveStrategy.EntropyBased:\n+                // Higher entropy = harder sample = lower temperature\n+                difficulty = ComputeEntropy(probs);\n+                break;\n+\n+            case AdaptiveStrategy.AccuracyBased:\n+                // Use stored performance for this sample if available\n+                // High performance (1.0) -> low difficulty (0.0) -> min temp (sharper)\n+                // Low performance (0.0) -> high difficulty (1.0) -> max temp (softer)\n+                if (sampleIndex.HasValue && StudentPerformance.ContainsKey(sampleIndex.Value))\n+                {\n+                    difficulty = 1.0 - StudentPerformance[sampleIndex.Value];\n+                }\n+                else\n+                {\n+                    // Fallback to medium difficulty if no performance data\n+                    difficulty = 0.5;\n+                }\n+                break;\n+        }\n+\n+        // Clamp difficulty to [0,1] to ensure temperature stays within bounds\n+        difficulty = Math.Max(0.0, Math.Min(1.0, difficulty));\n+\n+        // Map difficulty [0,1] to temperature [min, max]\n+        // High difficulty (0) -> min temp (sharper)\n+        // Low difficulty (1) -> max temp (softer)\n+        double adaptiveTemp = _minTemperature + (1.0 - difficulty) * (_maxTemperature - _minTemperature);"
        }
      ]
    }
  }
]
