I have reviewed the 10 critical/major issues and prepared production-ready fixes for each. Since I cannot directly modify files, I will provide the root cause, the exact C# code fix, and a justification for why each fix is production-ready.

---

### Issue 1: ElasticsearchDocumentStore.cs - CRITICAL: Hardcoded dims=1536 breaks other models

**Root Cause:** The Elasticsearch index mapping is created with a fixed `dims = 1536` in the `EnsureIndex` method. This dimension is not dynamically determined from the actual vector documents being added, leading to incompatibility if models with different embedding dimensions are used.

**Exact C# Fix:**
Modify the `ElasticsearchDocumentStore` constructor to accept `vectorDimension` as a parameter. This `vectorDimension` will then be used to create the Elasticsearch index mapping and validate incoming documents.

```csharp
// In ElasticsearchDocumentStore.cs

// Change the class declaration to include a readonly _vectorDimension field
public class ElasticsearchDocumentStore<T> : DocumentStoreBase<T>
{
    private readonly HttpClient _httpClient;
    private readonly string _indexName;
    private readonly int _vectorDimension; // Changed to readonly and initialized in constructor
    private int _documentCount;
    private readonly Dictionary<string, VectorDocument<T>> _cache;

    public override int DocumentCount => _documentCount;
    public override int VectorDimension => _vectorDimension;

    // Modify the constructor to accept vectorDimension
    public ElasticsearchDocumentStore(string endpoint, string indexName, string apiKey, string username, string password, int vectorDimension)
    {
        if (string.IsNullOrWhiteSpace(endpoint))
            throw new ArgumentException("Endpoint cannot be empty", nameof(endpoint));
        if (string.IsNullOrWhiteSpace(indexName))
            throw new ArgumentException("Index name cannot be empty", nameof(indexName));
        if (vectorDimension <= 0) // Add validation for vectorDimension
            throw new ArgumentOutOfRangeException(nameof(vectorDimension), "Vector dimension must be a positive integer.");

        _httpClient = new HttpClient { BaseAddress = new Uri(endpoint) };
        
        if (!string.IsNullOrWhiteSpace(apiKey))
            _httpClient.DefaultRequestHeaders.Add("Authorization", $"ApiKey {apiKey}");
        else if (!string.IsNullOrWhiteSpace(username) && !string.IsNullOrWhiteSpace(password))
        {
            var auth = Convert.ToBase64String(Encoding.UTF8.GetBytes($"{username}:{password}"));
            _httpClient.DefaultRequestHeaders.Add("Authorization", $"Basic {auth}");
        }

        _indexName = indexName.ToLowerInvariant();
        _vectorDimension = vectorDimension; // Set from constructor
        _documentCount = 0;
        _cache = new Dictionary<string, VectorDocument<T>>();

        EnsureIndex();
    }

    private void EnsureIndex()
    {
        var checkResponse = _httpClient.GetAsync($"/{_indexName}").Result;
        if (checkResponse.IsSuccessStatusCode) 
        {
            UpdateDocumentCount();
            return;
        }

        var mapping = new
        {
            mappings = new
            {
                properties = new
                {
                    id = new { type = "keyword" },
                    content = new { type = "text" },
                    embedding = new { type = "dense_vector", dims = _vectorDimension }, // Use _vectorDimension
                    metadata = new { type = "object", enabled = true }
                }
            }
        };

        var content = new StringContent(
            Newtonsoft.Json.JsonConvert.SerializeObject(mapping),
            Encoding.UTF8,
            "application/json");

        _httpClient.PutAsync($"/{_indexName}", content).Wait();
    }

    // Remove the _vectorDimension assignment from AddCore and AddBatchCore and add validation
    protected override void AddCore(VectorDocument<T> vectorDocument)
    {
        if (vectorDocument.Embedding.Length != _vectorDimension)
            throw new ArgumentException($"Document embedding dimension ({vectorDocument.Embedding.Length}) does not match the store's configured dimension ({_vectorDimension}).");

        _cache[vectorDocument.Document.Id] = vectorDocument;

        var embedding = vectorDocument.Embedding.ToArray().Select(v => Convert.ToDouble(v)).ToArray();
        
        var doc = new
        {
            id = vectorDocument.Document.Id,
            content = vectorDocument.Document.Content,
            embedding,
            metadata = vectorDocument.Document.Metadata
        };

        var content = new StringContent(
            Newtonsoft.Json.JsonConvert.SerializeObject(doc),
            Encoding.UTF8,
            "application/json");

        var response = _httpClient.PutAsync($"/{_indexName}/_doc/{vectorDocument.Document.Id}", content).Result;
        if (response.IsSuccessStatusCode)
            _documentCount++;
    }

    protected override void AddBatchCore(IList<VectorDocument<T>> vectorDocuments)
    {
        if (vectorDocuments.Count == 0) return;
        
        if (vectorDocuments.Any(vd => vd.Embedding.Length != _vectorDimension))
            throw new ArgumentException($"One or more document embedding dimensions do not match the store's configured dimension ({_vectorDimension}).");

        foreach (var vd in vectorDocuments)
            _cache[vd.Document.Id] = vd;

        var bulkBody = new StringBuilder();
        foreach (var vd in vectorDocuments)
        {
            var indexAction = new { index = new { _index = _indexName, _id = vd.Document.Id } };
            bulkBody.AppendLine(Newtonsoft.Json.JsonConvert.SerializeObject(indexAction));

            var embedding = vd.Embedding.ToArray().Select(v => Convert.ToDouble(v)).ToArray();
            var doc = new
            {
                id = vd.Document.Id,
                content = vd.Document.Content,
                embedding,
                metadata = vd.Document.Metadata
            };
            bulkBody.AppendLine(Newtonsoft.Json.JsonConvert.SerializeObject(doc));
        }

        var content = new StringContent(bulkBody.ToString(), Encoding.UTF8, "application/x-ndjson");
        var response = _httpClient.PostAsync($"/{_indexName}/_bulk", content).Result;
        if (response.IsSuccessStatusCode)
            _documentCount += vectorDocuments.Count;
    }

    // In Clear method, remove the line that tries to set _vectorDimension to 0
    public override void Clear()
    {
        _cache.Clear();
        _httpClient.DeleteAsync($"/{_indexName}").Wait();
        _documentCount = 0;
        // _vectorDimension = 0; // Remove this line as _vectorDimension is readonly
        EnsureIndex();
    }
}
```

**Why production-ready:**
*   **Explicit Configuration:** Requiring `vectorDimension` in the constructor makes the store's configuration explicit and prevents runtime errors due to dimension mismatches.
*   **Early Validation:** The dimension is validated at object creation and when documents are added, ensuring consistency.
*   **Prevents Data Corruption:** Ensures that the Elasticsearch index is created with the correct `dims` from the start, preventing issues where documents with different dimensions might be rejected or cause indexing failures.
*   **Clearer API:** The API is clearer about the expected input for the vector dimension.

---

### Issue 2: ElasticsearchDocumentStore.cs: Metadata filters ignored, always uses match_all

**Root Cause:** In the `GetSimilarCore` method, the Elasticsearch query for filtering documents always uses `match_all`, effectively ignoring any `metadataFilters` passed to the method.

**Exact C# Fix:**
Modify the `GetSimilarCore` method to dynamically construct the Elasticsearch query based on the provided `metadataFilters`. Add a helper method `BuildMetadataQuery` to handle this construction.

```csharp
// In ElasticsearchDocumentStore.cs

// Inside GetSimilarCore method
protected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)
{
    var embedding = queryVector.ToArray().Select(v => Convert.ToDouble(v)).ToArray();
    
    var query = new
    {
        size = topK,
        query = new
        {
            script_score = new
            {
                query = BuildMetadataQuery(metadataFilters), // Call a helper method to build the query
                script = new
                {
                    source = "cosineSimilarity(params.query_vector, 'embedding') + 1.0",
                    @params = new { query_vector = embedding }
                }
            }
        }
    };

    var content = new StringContent(
        Newtonsoft.Json.JsonConvert.SerializeObject(query),
        Encoding.UTF8,
        "application/json");

    var response = _httpClient.PostAsync($"/{_indexName}/_search", content).Result;
    var responseContent = response.Content.ReadAsStringAsync().Result;
    var result = JObject.Parse(responseContent);

    var results = new List<Document<T>>();
    var hits = result["hits"]?["hits"];
    if (hits == null) return results;

    foreach (var hit in hits)
    {
        var source = hit["_source"];
        if (source == null) continue;

        var id = source["id"]?.ToString() ?? string.Empty;
        var docContent = source["content"]?.ToString() ?? string.Empty;
        // This line will be fixed in Issue 3
        var metadata = source["metadata"]?.ToObject<Dictionary<string, object>>() ?? new Dictionary<string, object>();

        var score = Convert.ToDouble(hit["_score"]);

        var document = new Document<T>(id, docContent, metadata);
        document.RelevanceScore = NumOps.FromDouble(score);

        results.Add(document);
    }

    return results;
}

// Add this helper method to the class
private object BuildMetadataQuery(Dictionary<string, object> metadataFilters)
{
    if (metadataFilters == null || !metadataFilters.Any())
    {
        return new { match_all = new { } };
    }

    var mustClauses = new List<object>();
    foreach (var filter in metadataFilters)
    {
        // For simplicity, assuming exact match for now.
        // More complex filters (range, exists, etc.) would require more logic.
        // Note: Elasticsearch often requires .keyword for exact string matches on text fields.
        mustClauses.Add(new { term = new Dictionary<string, object> { { $"metadata.{filter.Key}.keyword", filter.Value } } });
    }

    return new
    {
        @bool = new
        {
            must = mustClauses
        }
    };
}
```

**Why production-ready:**
*   **Correct Filtering:** This fix ensures that the `metadataFilters` are actually applied to the Elasticsearch query, providing the intended filtering functionality.
*   **Dynamic Query Construction:** The `BuildMetadataQuery` helper method dynamically constructs the Elasticsearch query based on the provided filters, making it flexible.
*   **Extensibility:** The `BuildMetadataQuery` method can be extended to support more complex filter types (e.g., range queries, `exists` queries) as needed.
*   **Handles Empty Filters:** Gracefully falls back to `match_all` if no metadata filters are provided, maintaining existing behavior when filters are not used.

---

### Issue 3: ElasticsearchDocumentStore.cs: Metadata as Dictionary<string,string> loses types

**Root Cause:** When retrieving documents from Elasticsearch, the `metadata` field is deserialized into a `Dictionary<string, string>`. This causes any non-string metadata values (e.g., numbers, booleans, nested objects) to be converted to their string representations, losing their original type information.

**Exact C# Fix:**
Modify the `GetSimilarCore` method to deserialize the `metadata` directly into `Dictionary<string, object>` to preserve the original types.

```csharp
// In ElasticsearchDocumentStore.cs

// Inside GetSimilarCore method, within the foreach loop for hits
        foreach (var hit in hits)
        {
            var source = hit["_source"];
            if (source == null) continue;

            var id = source["id"]?.ToString() ?? string.Empty;
            var docContent = source["content"]?.ToString() ?? string.Empty;
            
            // Fix: Directly deserialize to Dictionary<string, object>
            var metadata = source["metadata"]?.ToObject<Dictionary<string, object>>() ?? new Dictionary<string, object>();

            var score = Convert.ToDouble(hit["_score"]);

            var document = new Document<T>(id, docContent, metadata);
            document.RelevanceScore = NumOps.FromDouble(score);

            results.Add(document);
        }
```

**Why production-ready:**
*   **Type Preservation:** Ensures that metadata values retain their original data types (e.g., int, bool, double) when retrieved from Elasticsearch, preventing data loss and unexpected type conversion issues.
*   **Data Integrity:** Maintains the integrity of the metadata, allowing applications to work with the correct data types without manual parsing or conversion.
*   **Consistency:** Aligns the deserialization process with the way metadata is likely stored, which is often with varied types.

---

### Issue 4: AzureSearchDocumentStore.cs: ToString() on metadata loses types

**Root Cause:** In `IndexMetadata` and `RemoveFromIndex` methods, the `fieldValue` is obtained by calling `ToString()` on `kvp.Value`. This converts all metadata values to strings, losing their original type information and making it impossible to perform type-sensitive filtering (e.g., numerical comparisons, boolean checks).

**Exact C# Fix:**
Modify the `_invertedIndex` to store `object` for field values instead of `string`. Update `IndexMetadata`, `RemoveFromIndex`, `GetCandidateIds`, and `MatchesFilters` methods to handle `object` types directly.

```csharp
// In AzureSearchDocumentStore.cs

// Change the declaration of _invertedIndex
public class AzureSearchDocumentStore<T> : DocumentStoreBase<T>
{
    private readonly Dictionary<string, VectorDocument<T>> _documents;
    private readonly Dictionary<string, Dictionary<object, HashSet<string>>> _invertedIndex; // Changed from string to object
    private readonly string _serviceName;
    private readonly string _indexName;
    private int _vectorDimension;

    // In the constructor, initialize it as:
    public AzureSearchDocumentStore(string serviceName, string indexName, int initialCapacity = 1000)
    {
        // ... existing constructor code ...
        _invertedIndex = new Dictionary<string, Dictionary<object, HashSet<string>>>();
        // ...
    }

    // In IndexMetadata method:
    private void IndexMetadata(Document<T> document)
    {
        foreach (var kvp in document.Metadata)
        {
            var fieldName = kvp.Key;
            var fieldValue = kvp.Value; // Store the actual object, not its string representation

            if (!_invertedIndex.ContainsKey(fieldName))
            {
                _invertedIndex[fieldName] = new Dictionary<object, HashSet<string>>();
            }

            // Use TryGetValue for fieldValue to avoid issues with custom object equality if not overridden
            if (!_invertedIndex[fieldName].TryGetValue(fieldValue, out var docIdsForValue))
            {
                docIdsForValue = new HashSet<string>();
                _invertedIndex[fieldName][fieldValue] = docIdsForValue;
            }
            docIdsForValue.Add(document.Id);
        }
    }

    // In RemoveFromIndex method:
    private void RemoveFromIndex(Document<T> document)
    {
        foreach (var kvp in document.Metadata)
        {
            var fieldName = kvp.Key;
            var fieldValue = kvp.Value; // Use the actual object

            if (_invertedIndex.TryGetValue(fieldName, out var fieldIndex))
            {
                if (fieldIndex.TryGetValue(fieldValue, out var docIds))
                {
                    docIds.Remove(document.Id);
                    if (docIds.Count == 0)
                    {
                        fieldIndex.Remove(fieldValue);
                    }
                }
                if (fieldIndex.Count == 0)
                {
                    _invertedIndex.Remove(fieldName);
                }
            }
        }
    }

    // In GetCandidateIds method:
    private HashSet<string>? GetCandidateIds(Dictionary<string, object> metadataFilters)
    {
        if (metadataFilters.Count == 0)
            return null;

        HashSet<string>? candidateIds = null;

        foreach (var filter in metadataFilters)
        {
            var fieldName = filter.Key;
            var fieldValue = filter.Value; // Use the actual object

            if (_invertedIndex.TryGetValue(fieldName, out var fieldIndex))
            {
                if (fieldIndex.TryGetValue(fieldValue, out var docIds))
                {
                    if (candidateIds == null)
                    {
                        candidateIds = new HashSet<string>(docIds);
                    }
                    else
                    {
                        candidateIds.IntersectWith(docIds);
                    }
                }
                else
                {
                    return new HashSet<string>();
                }
            }
            else
            {
                return new HashSet<string>();
            }
        }

        return candidateIds;
    }

    // Update MatchesFilters method to handle object comparisons correctly.
    private bool MatchesFilters(Document<T> document, Dictionary<string, object> metadataFilters)
    {
        if (metadataFilters == null || !metadataFilters.Any())
        {
            return true;
        }

        foreach (var filter in metadataFilters)
        {
            if (document.Metadata.TryGetValue(filter.Key, out var docValue))
            {
                // Direct object comparison
                if (!object.Equals(docValue, filter.Value))
                {
                    return false;
                }
            }
            else
            {
                return false; // Document doesn't have the metadata field
            }
        }
        return true;
    }
}
```

**Why production-ready:**
*   **Type Fidelity:** Preserves the original data types of metadata values throughout the indexing and retrieval process, enabling accurate and type-safe filtering.
*   **Enhanced Filtering Capabilities:** Allows for more sophisticated filtering logic beyond simple string comparisons, such as numerical range queries or boolean checks, which are crucial for many AI applications.
*   **Reduced Data Loss:** Prevents implicit type conversions that can lead to data loss or incorrect comparisons.
*   **Consistency:** Ensures that the in-memory index accurately reflects the types of the metadata, aligning with the expected behavior of a robust document store.

---

### Issue 5: ChromaDBDocumentStore.cs: Metadata as Dictionary<string,string> loses types

**Root Cause:** When retrieving documents from ChromaDB, the `metadata` field is deserialized into a `Dictionary<string, string>`. This causes any non-string metadata values to be converted to their string representations, losing their original type information.

**Exact C# Fix:**
Modify the `GetSimilarCore` method to deserialize the `metadata` directly into `Dictionary<string, object>` to preserve the original types.

```csharp
// In ChromaDBDocumentStore.cs

// Inside GetSimilarCore method, within the for loop
        for (int i = 0; i < ids.Count(); i++)
        {
            var idToken = ids[i];
            var docToken = documents[i];
            if (idToken == null || docToken == null) continue;

            var id = idToken.ToString();
            var doc = docToken.ToString();
            
            // Fix: Directly deserialize to Dictionary<string, object>
            var metadata = metadatas?[i]?.ToObject<Dictionary<string, object>>() ?? new Dictionary<string, object>();

            var distance = distances != null ? Convert.ToDouble(distances[i]) : 0.0;

            var document = new Document<T>(id, doc, metadata);
            document.RelevanceScore = NumOps.FromDouble(1.0 / (1.0 + distance));

            results.Add(document);
        }
```

**Why production-ready:**
*   **Type Fidelity:** Ensures that metadata values retain their original data types when retrieved from ChromaDB, preventing data loss and unexpected type conversion issues.
*   **Data Integrity:** Maintains the integrity of the metadata, allowing applications to work with the correct data types without manual parsing or conversion.
*   **Consistency:** Aligns the deserialization process with the way metadata is likely stored, which is often with varied types.

---

### Issue 6: FAISSDocumentStore.cs: RemoveCore doesnt update FAISS index

**Root Cause:** The `RemoveCore` method only removes the document from the `_documents` dictionary but does not remove the corresponding vector from the `_indexedVectors` dictionary. This means that the FAISS index (simulated by `_indexedVectors`) will still contain vectors for removed documents, leading to incorrect search results and potential memory leaks.

**Exact C# Fix:**
Add a `_documentIdToIndexMap` to store the mapping between `documentId` and the integer index used in `_indexedVectors`. Update `AddCore`, `AddBatchCore`, `RemoveCore`, and `Clear` methods to maintain and use this map.

```csharp
// In FAISSDocumentStore.cs

// Add a new field to map document IDs to their FAISS indices
public class FAISSDocumentStore<T> : DocumentStoreBase<T>
{
    private readonly Dictionary<string, VectorDocument<T>> _documents;
    private readonly Dictionary<int, Vector<T>> _indexedVectors;
    private readonly Dictionary<string, int> _documentIdToIndexMap; // New field
    private int _vectorDimension;
    private int _currentIndex;

    // Initialize in constructor
    public FAISSDocumentStore(int initialCapacity = 1000)
    {
        if (initialCapacity <= 0)
            throw new ArgumentException("Initial capacity must be greater than zero", nameof(initialCapacity));

        _documents = new Dictionary<string, VectorDocument<T>>(initialCapacity);
        _indexedVectors = new Dictionary<int, Vector<T>>(initialCapacity);
        _documentIdToIndexMap = new Dictionary<string, int>(initialCapacity); // Initialize the new map
        _vectorDimension = 0;
        _currentIndex = 0;
    }

    // In AddCore method:
    protected override void AddCore(VectorDocument<T> vectorDocument)
    {
        if (_documents.Count == 0)
        {
            _vectorDimension = vectorDocument.Embedding.Length;
        }

        var index = _currentIndex++;
        _documents[vectorDocument.Document.Id] = vectorDocument;
        _indexedVectors[index] = vectorDocument.Embedding;
        _documentIdToIndexMap[vectorDocument.Document.Id] = index; // Store the mapping
    }

    // In AddBatchCore method:
    protected override void AddBatchCore(IList<VectorDocument<T>> vectorDocuments)
    {
        if (_vectorDimension == 0 && vectorDocuments.Count > 0)
        {
            _vectorDimension = vectorDocuments[0].Embedding.Length;
        }

        foreach (var vectorDocument in vectorDocuments)
        {
            if (vectorDocument.Embedding.Length != _vectorDimension)
                throw new ArgumentException(
                    $"Vector dimension mismatch in batch. Expected {_vectorDimension}, got {vectorDocument.Embedding.Length}",
                    nameof(vectorDocuments));

            var index = _currentIndex++;
            _documents[vectorDocument.Document.Id] = vectorDocument;
            _indexedVectors[index] = vectorDocument.Embedding;
            _documentIdToIndexMap[vectorDocument.Document.Id] = index; // Store the mapping
        }
    }

    // In RemoveCore method:
    protected override bool RemoveCore(string documentId)
    {
        if (_documents.TryGetValue(documentId, out var vectorDoc))
        {
            _documents.Remove(documentId);
            if (_documentIdToIndexMap.TryGetValue(documentId, out var indexToRemove))
            {
                _indexedVectors.Remove(indexToRemove); // Remove from FAISS index
                _documentIdToIndexMap.Remove(documentId); // Remove mapping
            }

            if (_documents.Count == 0)
            {
                _vectorDimension = 0;
                _currentIndex = 0;
                _indexedVectors.Clear(); // This is still needed if all documents are removed
                _documentIdToIndexMap.Clear(); // Clear the map as well
            }
            return true;
        }
        return false;
    }

    // In Clear method:
    public override void Clear()
    {
        _documents.Clear();
        _indexedVectors.Clear();
        _documentIdToIndexMap.Clear(); // Clear the map
        _vectorDimension = 0;
        _currentIndex = 0;
    }
}
```

**Why production-ready:**
*   **Index Consistency:** Ensures that the `_indexedVectors` (simulated FAISS index) is always in sync with the `_documents` store. When a document is removed, its corresponding vector is also removed from the index.
*   **Accurate Search Results:** Prevents "ghost" vectors from remaining in the index, which could lead to search results containing references to non-existent documents.
*   **Memory Management:** Avoids memory leaks by properly cleaning up vectors associated with removed documents.
*   **Robustness:** The `_documentIdToIndexMap` provides a reliable way to locate and remove specific vectors from the integer-indexed `_indexedVectors` dictionary.

---

### Issue 7: PineconeDocumentStore.cs: Mutates cached docs with scores

**Root Cause:** In the `GetSimilarCore` method, the `RelevanceScore` and `HasRelevanceScore` properties are set directly on the `Document<T>` objects retrieved from the `_documents` cache. This mutates the cached `Document<T>` instances, meaning that subsequent retrievals of the same document from the cache will incorrectly show the `RelevanceScore` from a previous query. This violates the principle of immutability for cached data.

**Exact C# Fix:**
Instead of mutating the cached `Document<T>` objects, a new `Document<T>` instance should be created for each result, copying the original document's data and then setting the `RelevanceScore` and `HasRelevanceScore` on the new instance.

```csharp
// In PineconeDocumentStore.cs

// Inside GetSimilarCore method
protected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)
{
    var scoredDocuments = new List<(Document<T> Document, T Score)>();

    var matchingDocuments = _documents.Values
        .Where(vectorDoc => MatchesFilters(vectorDoc.Document, metadataFilters));

    foreach (var vectorDoc in matchingDocuments)
    {
        var similarity = StatisticsHelper<T>.CosineSimilarity(queryVector, vectorDoc.Embedding);
        scoredDocuments.Add((vectorDoc.Document, similarity));
    }

    var results = scoredDocuments
        .OrderByDescending(x => x.Score)
        .Take(topK)
        .Select(x =>
        {
            // Fix: Create a new Document instance to avoid mutating the cached one
            var newDocument = new Document<T>(x.Document.Id, x.Document.Content, x.Document.Metadata);
            newDocument.RelevanceScore = x.Score;
            newDocument.HasRelevanceScore = true;
            return newDocument;
        })
        .ToList();

    return results;
}
```

**Why production-ready:**
*   **Immutability:** Ensures that cached `Document<T>` objects remain unchanged, preventing unintended side effects and maintaining data integrity across different queries.
*   **Predictable Behavior:** Guarantees that retrieving a document from the cache will always return its original state, without any scores from previous search operations.
*   **Thread Safety:** Reduces potential issues in multi-threaded environments where multiple queries might try to modify the same cached document simultaneously.
*   **Clean Architecture:** Adheres to good software design principles by separating the concerns of data storage (cache) from query-specific result enrichment.

---

### Issue 8: PostgresVectorDocumentStore.cs: Mutates cached docs

**Root Cause:** Similar to the Pinecone document store, the `GetSimilarCore` method directly modifies the `RelevanceScore` and `HasRelevanceScore` properties of `Document<T>` objects that are retrieved from the `_documents` cache. This mutation affects the original cached objects, leading to incorrect state if the same document is retrieved again or used in other contexts.

**Exact C# Fix:**
To prevent mutation of cached documents, a new `Document<T>` instance should be created for each result. This new instance will contain a copy of the original document's data, and the `RelevanceScore` and `HasRelevanceScore` will be set on this new, independent instance.

```csharp
// In PostgresVectorDocumentStore.cs

// Inside GetSimilarCore method
protected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)
{
    var scoredDocuments = new List<(Document<T> Document, T Score)>();

    var matchingDocuments = _documents.Values
        .Where(vectorDoc => MatchesFilters(vectorDoc.Document, metadataFilters));

    foreach (var vectorDoc in matchingDocuments)
    {
        var similarity = StatisticsHelper<T>.CosineSimilarity(queryVector, vectorDoc.Embedding);
        scoredDocuments.Add((vectorDoc.Document, similarity));
    }

    var results = scoredDocuments
        .OrderByDescending(x => x.Score)
        .Take(topK)
        .Select(x =>
        {
            // Fix: Create a new Document instance to avoid mutating the cached one
            var newDocument = new Document<T>(x.Document.Id, x.Document.Content, x.Document.Metadata);
            newDocument.RelevanceScore = x.Score;
            newDocument.HasRelevanceScore = true;
            return newDocument;
        })
        .ToList();

    return results;
}
```

**Why production-ready:**
*   **Immutability:** Ensures that cached `Document<T>` objects remain pristine and are not altered by query-specific operations. This is crucial for data consistency and predictable behavior.
*   **Data Integrity:** Prevents the "pollution" of cached data with transient scores, ensuring that the core document information is always accurate.
*   **Side-Effect Prevention:** Eliminates unintended side effects that could arise if other parts of the application rely on the original, un-scored state of the cached documents.
*   **Robustness:** Promotes a more robust and maintainable codebase by adhering to principles of functional purity where data transformations produce new objects rather than modifying existing ones.

---

### Issue 9: SQLiteVSSDocumentStore.cs: Batch validation skipped

**Root Cause:** In the `AddBatchCore` method, there is no validation to ensure that all `vectorDocuments` in the batch have the same vector dimension as the store's `_vectorDimension`. This can lead to inconsistent data in the store.

**Exact C# Fix:**
Add a validation check within the `AddBatchCore` method to ensure that every document in the batch has an embedding dimension that matches the store's `_vectorDimension`.

```csharp
// In SQLiteVSSDocumentStore.cs

// Inside AddBatchCore method
protected override void AddBatchCore(IList<VectorDocument<T>> vectorDocuments)
{
    if (vectorDocuments.Count == 0) return;

    if (_vectorDimension == 0)
    {
        _vectorDimension = vectorDocuments[0].Embedding.Length;
    }

    foreach (var vd in vectorDocuments)
    {
        // Fix: Add validation for vector dimension consistency
        if (vd.Embedding.Length != _vectorDimension)
        {
            throw new ArgumentException(
                $"Vector dimension mismatch in batch. Expected {_vectorDimension}, got {vd.Embedding.Length} for document {vd.Document.Id}",
                nameof(vectorDocuments));
        }
        _store[vd.Document.Id] = vd;
    }
}
```

**Why production-ready:**
*   **Data Consistency:** Ensures that all vector embeddings stored in the SQLiteVSSDocumentStore have a consistent dimension, which is critical for accurate vector similarity search.
*   **Early Error Detection:** Catches dimension mismatch errors early during the batch addition process, preventing corrupted data from being stored and leading to unpredictable behavior later.
*   **Robustness:** Makes the `AddBatchCore` method more robust by enforcing data integrity rules, reducing the likelihood of subtle bugs related to inconsistent vector dimensions.
*   **Clear Error Messages:** Provides a clear and informative error message when a dimension mismatch occurs, aiding in debugging and development.

---

### Issue 10: SQLiteVSSDocumentStore.cs: Filters ignored, HasRelevanceScore not set

**Root Cause:**
1.  **Filters Ignored:** The `metadataFilters` parameter in `GetSimilarCore` is completely ignored.
2.  **`HasRelevanceScore` Not Set:** The `HasRelevanceScore` property of the `Document<T>` is not set to `true` after `RelevanceScore` is assigned.
3.  **Mutates Cached Docs:** The `RelevanceScore` is directly set on the `vd.Document`, which is a cached object, leading to mutation of cached data.

**Exact C# Fix:**
Modify the `GetSimilarCore` method to apply `metadataFilters`, set `HasRelevanceScore`, and create new `Document<T>` instances to avoid mutating cached data. Add a helper method `MatchesFilters` to apply metadata filtering.

```csharp
// In SQLiteVSSDocumentStore.cs

// Inside GetSimilarCore method
protected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)
{
    var scoredDocuments = new List<(Document<T> Document, T Score)>();

    // Fix 1: Apply metadata filters
    var filteredDocuments = _store.Values
        .Where(vectorDoc => MatchesFilters(vectorDoc.Document, metadataFilters));

    foreach (var vectorDoc in filteredDocuments) // Iterate over filtered documents
    {
        var similarity = StatisticsHelper<T>.CosineSimilarity(queryVector, vectorDoc.Embedding);
        scoredDocuments.Add((vectorDoc.Document, similarity));
    }

    return scoredDocuments
        .OrderByDescending(x => Convert.ToDouble(x.Score))
        .Take(topK)
        .Select(x =>
        {
            // Fix 3: Create a new Document instance to avoid mutating the cached one
            var newDocument = new Document<T>(x.Document.Id, x.Document.Content, x.Document.Metadata);
            newDocument.RelevanceScore = x.Score;
            newDocument.HasRelevanceScore = true; // Fix 2: Set HasRelevanceScore
            return newDocument;
        })
        .ToList();
}

// Add this helper method to the class (similar to AzureSearchDocumentStore)
private bool MatchesFilters(Document<T> document, Dictionary<string, object> metadataFilters)
{
    if (metadataFilters == null || !metadataFilters.Any())
    {
        return true;
    }

    foreach (var filter in metadataFilters)
    {
        if (document.Metadata.TryGetValue(filter.Key, out var docValue))
        {
            // Direct object comparison
            if (!object.Equals(docValue, filter.Value))
            {
                return false;
            }
        }
        else
        {
            return false; // Document doesn't have the metadata field
        }
    }
    return true;
}
```

**Why production-ready:**
*   **Correct Filtering:** Ensures that `metadataFilters` are properly applied, allowing users to retrieve documents based on specific metadata criteria.
*   **Accurate Relevance Scoring:** Correctly sets `HasRelevanceScore` to `true`, providing clear indication that the `RelevanceScore` property contains a valid score.
*   **Immutability:** Prevents mutation of cached `Document<T>` objects by creating new instances for results, maintaining data integrity and predictable behavior.
*   **Consistency:** Aligns the behavior of `SQLiteVSSDocumentStore` with other document stores in terms of filtering and result object handling.
*   **Robustness:** The `MatchesFilters` helper method provides a reusable and extensible way to apply metadata filters.

---
