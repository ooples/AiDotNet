{"data":{"repository":{"pullRequest":{"reviewThreads":{"nodes":[{"id":"PRRT_kwDOKSXUF85hLZvj","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VboI-","databaseId":2507047486,"body":"The teacher predictions are shuffled with a different random permutation than the inputs and labels, causing data misalignment. The `ShuffleData` method on line 163 uses its own random seed (via `Guid.NewGuid()`), and line 169 creates a completely different random permutation. This means `shuffledInputs[i]`, `shuffledLabels[i]`, and `shuffledTeacher[i]` will not correspond to the same training example. The fix is to reuse the same indices from `ShuffleData` or shuffle all three arrays together using a single permutation.\n```suggestion\n                // Shuffle data (inputs, labels, and teacher predictions in sync)\n                Vector<T>[] shuffledInputs;\n                Vector<T>[] shuffledLabels;\n                Vector<T>[]? shuffledTeacher = null;\n\n                int[] indices = Enumerable.Range(0, trainInputs.Length).OrderBy(_ => Guid.NewGuid()).ToArray();\n                shuffledInputs = indices.Select(i => trainInputs[i]).ToArray();\n                shuffledLabels = indices.Select(i => trainLabels[i]).ToArray();\n                if (teacherPredictions != null)\n                {\n```","path":"src/KnowledgeDistillation/SelfDistillationTrainer.cs","position":1,"line":null,"createdAt":"2025-11-08T18:03:24Z","author":{"login":"copilot-pull-request-reviewer"}}]}},{"id":"PRRT_kwDOKSXUF85hLZvm","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VboJD","databaseId":2507047491,"body":"Creating a new `Random()` instance on every call to `ShuffleData` can lead to performance issues and potential non-randomness. When called in rapid succession, multiple instances may be seeded with the same system time, producing identical sequences. Consider creating a single static `Random` instance (with thread-safe access if needed) or accepting a `Random` instance as a parameter. For better thread safety in concurrent scenarios, use `Random.Shared` (available in .NET 6+) or `ThreadLocal<Random>`.\n```suggestion\n        var random = Random.Shared;\n```","path":"src/KnowledgeDistillation/KnowledgeDistillationTrainer.cs","position":1,"line":null,"createdAt":"2025-11-08T18:03:24Z","author":{"login":"copilot-pull-request-reviewer"}}]}},{"id":"PRRT_kwDOKSXUF85hLZvn","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VboJE","databaseId":2507047492,"body":"Using `Guid.NewGuid()` for shuffling is inefficient compared to Fisher-Yates shuffle. This approach has O(n log n) complexity due to sorting, generates many GUIDs, and creates additional allocations. The `KnowledgeDistillationTrainer` class uses the more efficient Fisher-Yates algorithm (O(n)) in its `ShuffleData` method. Consider using the same approach here for consistency and better performance.","path":"src/KnowledgeDistillation/SelfDistillationTrainer.cs","position":1,"line":null,"createdAt":"2025-11-08T18:03:25Z","author":{"login":"copilot-pull-request-reviewer"}}]}},{"id":"PRRT_kwDOKSXUF85hLZvo","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VboJF","databaseId":2507047493,"body":"This foreach loop [implicitly filters its target sequence](1) - consider filtering the sequence explicitly using '.Where(...)'.","path":"src/KnowledgeDistillation/FeatureDistillationStrategy.cs","position":1,"line":null,"createdAt":"2025-11-08T18:03:25Z","author":{"login":"copilot-pull-request-reviewer"}}]}},{"id":"PRRT_kwDOKSXUF85hLZvr","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VboJI","databaseId":2507047496,"body":"This foreach loop immediately [maps its iteration variable to another variable](1) - consider mapping the sequence explicitly using '.Select(...)'.","path":"src/KnowledgeDistillation/FeatureDistillationStrategy.cs","position":1,"line":null,"createdAt":"2025-11-08T18:03:25Z","author":{"login":"copilot-pull-request-reviewer"}}]}},{"id":"PRRT_kwDOKSXUF85hOiwA","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86Vfynz","databaseId":2508138995,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Fix the generic variance that breaks the build.**\n\n`IDistillationStrategy<in TOutput, T>` returns `TOutput` from `ComputeGradient`, so marking `TOutput` contravariant violates C# variance rules and causes CS1961 (see pipeline). Drop the `in` or otherwise stop emitting `TOutput`. Removing the variance keeps compatibility with all current callers and restores the build.\n\n\nApply this diff to unblock compilation:\n\n```diff\n-public interface IDistillationStrategy<in TOutput, T>\n+public interface IDistillationStrategy<TOutput, T>\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\npublic interface IDistillationStrategy<TOutput, T>\n{\n    /// <summary>\n    /// Computes the distillation loss between student and teacher outputs.\n    /// </summary>\n    /// <param name=\"studentOutput\">The student model's output (logits).</param>\n    /// <param name=\"teacherOutput\">The teacher model's output (logits).</param>\n    /// <param name=\"trueLabels\">Ground truth labels (optional, can be null for unsupervised distillation).</param>\n    /// <returns>The computed distillation loss value.</returns>\n    /// <remarks>\n    /// <para><b>For Beginners:</b> This calculates how different the student's predictions are\n    /// from the teacher's predictions. A lower loss means the student is learning well from the teacher.</para>\n    ///\n    /// <para>The formula typically used is:\n    /// Total Loss = ╬▒ ├ù Hard Loss + (1 - ╬▒) ├ù Soft Loss\n    ///\n    /// Where:\n    /// - Hard Loss: Cross-entropy between student predictions and true labels\n    /// - Soft Loss: KL divergence between student and teacher (with temperature scaling)\n    /// - ╬▒ (alpha): Balance parameter (typically 0.3-0.5)</para>\n    /// </remarks>\n    T ComputeLoss(TOutput studentOutput, TOutput teacherOutput, TOutput? trueLabels = default);\n\n    /// <summary>\n    /// Computes the gradient of the distillation loss for backpropagation.\n    /// </summary>\n    /// <param name=\"studentOutput\">The student model's output (logits).</param>\n    /// <param name=\"teacherOutput\">The teacher model's output (logits).</param>\n    /// <param name=\"trueLabels\">Ground truth labels (optional).</param>\n    /// <returns>The gradient of the loss with respect to student outputs.</returns>\n    /// <remarks>\n    /// <para><b>For Beginners:</b> Gradients tell us how to adjust the student model's parameters\n    /// to reduce the loss. They point in the direction of steepest increase in loss, so we move\n    /// in the opposite direction during training.</para>\n    ///\n    /// <para>The gradient combines information from both the teacher (soft targets) and the\n    /// true labels (hard targets), helping the student learn from both sources.</para>\n    /// </remarks>\n    TOutput ComputeGradient(TOutput studentOutput, TOutput teacherOutput, TOutput? trueLabels = default);\n\n    /// <summary>\n    /// Gets or sets the temperature parameter for softening probability distributions.\n    /// </summary>\n    /// <remarks>\n    /// <para><b>For Beginners:</b> Temperature controls how \"soft\" the predictions become:\n    /// - T = 1: Normal predictions (standard softmax)\n    /// - T = 2-10: Softer predictions that reveal more about class relationships\n    /// - Higher T: Even softer, but gradients become smaller</para>\n    ///\n    /// <para>Typical values: 3-5 for most applications, 2-3 for easier tasks, 5-10 for harder tasks.</para>\n    /// </remarks>\n    double Temperature { get; set; }\n\n    /// <summary>\n    /// Gets or sets the balance parameter (alpha) between hard loss and soft loss.\n    /// </summary>\n    /// <remarks>\n    /// <para><b>For Beginners:</b> Alpha controls the trade-off between learning from true labels\n    /// and learning from the teacher:\n    /// - ╬▒ = 0: Only learn from teacher (pure distillation)\n    /// - ╬▒ = 0.3-0.5: Balanced (recommended for most cases)\n    /// - ╬▒ = 1: Only learn from true labels (standard training, no distillation)</para>\n    ///\n    /// <para>When true labels are noisy or scarce, lower alpha (more weight on teacher) helps.\n    /// When labels are clean and abundant, higher alpha (more weight on labels) works better.</para>\n    /// </remarks>\n    double Alpha { get; set; }\n}\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒº░ Tools</summary>\n\n<details>\n<summary>≡ƒ¬¢ GitHub Actions: Build</summary>\n\n[error] 61-61: Invalid variance: The type parameter 'TOutput' must be covariantly valid on 'IDistillationStrategy<TOutput, T>.ComputeGradient(TOutput, TOutput, TOutput?)'. 'TOutput' is contravariant.\n\n</details>\n<details>\n<summary>≡ƒ¬¢ GitHub Actions: Quality Gates (.NET)</summary>\n\n[error] 61-61: CS1961: Invalid variance: The type parameter 'TOutput' must be covariantly valid on 'IDistillationStrategy<TOutput, T>.ComputeGradient(TOutput, TOutput, TOutput?)'. 'TOutput' is contravariant.\n\n</details>\n<details>\n<summary>≡ƒ¬¢ GitHub Check: Build All Frameworks</summary>\n\n[failure] 61-61: \nInvalid variance: The type parameter 'TOutput' must be covariantly valid on 'IDistillationStrategy<TOutput, T>.ComputeGradient(TOutput, TOutput, TOutput?)'. 'TOutput' is contravariant.\n\n---\n\n[failure] 61-61: \nInvalid variance: The type parameter 'TOutput' must be covariantly valid on 'IDistillationStrategy<TOutput, T>.ComputeGradient(TOutput, TOutput, TOutput?)'. 'TOutput' is contravariant.\n\n---\n\n[failure] 61-61: \nInvalid variance: The type parameter 'TOutput' must be covariantly valid on 'IDistillationStrategy<TOutput, T>.ComputeGradient(TOutput, TOutput, TOutput?)'. 'TOutput' is contravariant.\n\n---\n\n[failure] 61-61: \nInvalid variance: The type parameter 'TOutput' must be covariantly valid on 'IDistillationStrategy<TOutput, T>.ComputeGradient(TOutput, TOutput, TOutput?)'. 'TOutput' is contravariant.\n\n</details>\n<details>\n<summary>≡ƒ¬¢ GitHub Check: Publish Size Analysis</summary>\n\n[failure] 61-61: \nInvalid variance: The type parameter 'TOutput' must be covariantly valid on 'IDistillationStrategy<TOutput, T>.ComputeGradient(TOutput, TOutput, TOutput?)'. 'TOutput' is contravariant.\n\n</details>\n\n</details>\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/Interfaces/IDistillationStrategy.cs around lines 23 to 90, the generic\ndeclaration uses contravariant 'in TOutput' but TOutput is returned by\nComputeGradient, causing CS1961; remove the 'in' variance modifier from TOutput\n(i.e., change IDistillationStrategy<in TOutput, T> to\nIDistillationStrategy<TOutput, T>) so TOutput is invariant and the build will\nsucceed; update any XML/summary if needed to match the new signature.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits a2059ca to e88ed58","path":"src/Interfaces/IDistillationStrategy.cs","position":1,"line":null,"createdAt":"2025-11-09T16:00:41Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hOiwD","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86Vfyn2","databaseId":2508138998,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Seal invalid `Temperature`/`Alpha` values before they slip in.**\n\n`Temperature` and `Alpha` can be reassigned after construction without any guardrails, so a caller can inadvertently set `Temperature <= 0` (division by zero in `Softmax`) or push `Alpha` outside `[0,1]`, giving negative/oversized weights in loss and gradient mix. Please carry the constructor validation over to the setters.\n\n\nApply this diff to harden the properties:\n\n```diff\n-    public double Temperature { get; set; }\n-\n-    public double Alpha { get; set; }\n+    private double _temperature;\n+    public double Temperature\n+    {\n+        get => _temperature;\n+        set\n+        {\n+            if (value <= 0)\n+                throw new ArgumentException(\"Temperature must be positive\", nameof(value));\n+            _temperature = value;\n+        }\n+    }\n+\n+    private double _alpha;\n+    public double Alpha\n+    {\n+        get => _alpha;\n+        set\n+        {\n+            if (value < 0 || value > 1)\n+                throw new ArgumentException(\"Alpha must be between 0 and 1\", nameof(value));\n+            _alpha = value;\n+        }\n+    }\n@@\n-        Temperature = temperature;\n-        Alpha = alpha;\n+        _temperature = temperature;\n+        _alpha = alpha;\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    private double _temperature;\n    public double Temperature\n    {\n        get => _temperature;\n        set\n        {\n            if (value <= 0)\n                throw new ArgumentException(\"Temperature must be positive\", nameof(value));\n            _temperature = value;\n        }\n    }\n\n    /// <summary>\n    /// Gets or sets the balance parameter between hard loss and soft loss.\n    /// </summary>\n    /// <remarks>\n    /// <para><b>For Beginners:</b> Controls how much the student learns from true labels vs. teacher:\n    /// - ╬▒ = 0: Only learn from teacher (useful when labels are noisy)\n    /// - ╬▒ = 0.3: 30% from labels, 70% from teacher (common default)\n    /// - ╬▒ = 0.5: Equal weight to both sources\n    /// - ╬▒ = 1: Only learn from labels (no distillation)</para>\n    /// </remarks>\n    private double _alpha;\n    public double Alpha\n    {\n        get => _alpha;\n        set\n        {\n            if (value < 0 || value > 1)\n                throw new ArgumentException(\"Alpha must be between 0 and 1\", nameof(value));\n            _alpha = value;\n        }\n    }\n\n    /// <summary>\n    /// Initializes a new instance of the DistillationLoss class.\n    /// </summary>\n    /// <param name=\"temperature\">Softmax temperature for distillation (default: 3.0). Higher values (2-10)\n    /// produce softer probability distributions that transfer more knowledge.</param>\n    /// <param name=\"alpha\">Balance between hard loss and soft loss (default: 0.3). Lower values give\n    /// more weight to the teacher's knowledge.</param>\n    /// <remarks>\n    /// <para><b>For Beginners:</b> The default values (temperature=3.0, alpha=0.3) work well for most\n    /// classification tasks. You may want to adjust them based on your specific problem:\n    /// - Increase temperature if the teacher's uncertainty is important (complex tasks)\n    /// - Decrease alpha if you have noisy labels or want to rely more on the teacher\n    /// - Increase alpha if you have very clean labels and a smaller capacity gap</para>\n    /// </remarks>\n    public DistillationLoss(double temperature = 3.0, double alpha = 0.3)\n    {\n        if (temperature <= 0)\n            throw new ArgumentException(\"Temperature must be positive\", nameof(temperature));\n        if (alpha < 0 || alpha > 1)\n            throw new ArgumentException(\"Alpha must be between 0 and 1\", nameof(alpha));\n\n        _numOps = MathHelper.GetNumericOperations<T>();\n        _temperature = temperature;\n        _alpha = alpha;\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/DistillationLoss.cs around lines 60 to 98, the\nconstructor validates Temperature and Alpha but the public auto-properties allow\nlater invalid assignments; add private backing fields (e.g. _temperature,\n_alpha) and replace the auto-properties with properties whose setters validate\n(Temperature > 0, Alpha in [0,1]) and throw ArgumentException (using same\nmessages/named parameter) so invalid values cannot be assigned after\nconstruction; keep existing constructor logic but set the backing fields through\nthe validated properties.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","path":"src/KnowledgeDistillation/DistillationLoss.cs","position":1,"line":null,"createdAt":"2025-11-09T16:00:41Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hOiwF","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86Vfyn4","databaseId":2508139000,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**`UseEMA` is a no-op**  \nThe public `UseEMA`/`EMADecay` knobs never influence training, so enabling EMA teacher smoothing silently does nothing. Either wire them into the teacher-prediction update or drop the surface area, but leaving a useless toggle is a functional bug.  \n\n\n```diff\n-        // Store teacher predictions for current generation\n-        Vector<T>[]? teacherPredictions = null;\n+        // Store teacher predictions for current generation\n+        Vector<T>[]? teacherPredictions = null;\n+        Vector<T>[]? emaTeacherPredictions = null;\n@@\n-            // After training this generation, save predictions as teacher for next generation\n+            // After training this generation, save predictions as teacher for next generation\n             if (generation < _generations - 1)\n             {\n-                teacherPredictions = new Vector<T>[trainInputs.Length];\n-                for (int i = 0; i < trainInputs.Length; i++)\n-                {\n-                    teacherPredictions[i] = modelForward(trainInputs[i]);\n-                }\n+                var currentPredictions = new Vector<T>[trainInputs.Length];\n+                for (int i = 0; i < trainInputs.Length; i++)\n+                {\n+                    currentPredictions[i] = modelForward(trainInputs[i]);\n+                }\n+\n+                if (UseEMA)\n+                {\n+                    var decay = _numOps.FromDouble(EMADecay);\n+                    var oneMinusDecay = _numOps.FromDouble(1.0 - EMADecay);\n+\n+                    if (emaTeacherPredictions == null)\n+                    {\n+                        emaTeacherPredictions = currentPredictions;\n+                    }\n+                    else\n+                    {\n+                        for (int i = 0; i < emaTeacherPredictions.Length; i++)\n+                        {\n+                            var emaVector = emaTeacherPredictions[i];\n+                            var currentVector = currentPredictions[i];\n+\n+                            for (int j = 0; j < emaVector.Length; j++)\n+                            {\n+                                emaVector[j] = _numOps.Add(\n+                                    _numOps.Multiply(decay, emaVector[j]),\n+                                    _numOps.Multiply(oneMinusDecay, currentVector[j]));\n+                            }\n+                        }\n+                    }\n+\n+                    teacherPredictions = emaTeacherPredictions;\n+                }\n+                else\n+                {\n+                    teacherPredictions = currentPredictions;\n+                }\n             }\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/SelfDistillationTrainer.cs around lines 150 to 218,\nthe public UseEMA/EMADecay settings are not applied when updating\nteacherPredictions so enabling EMA does nothing; fix by wiring EMA into the\nteacher update: keep a persistent teacherPredictions array (or\nemaTeacherPredictions) and after computing current model predictions at\ngeneration end, if UseEMA==true update each teacher vector as teacher = EMADecay\n* oldTeacher + (1 - EMADecay) * newPrediction (handling null/first-generation by\nassigning newPrediction), otherwise assign teacherPredictions = newPrediction\ndirectly; ensure EMADecay is validated (0..1) and types/ops use _numOps for\nscalar and vector arithmetic so the smoothing uses the same numeric ops as the\nrest of the trainer.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits a2059ca to db0b301","path":"src/KnowledgeDistillation/SelfDistillationTrainer.cs","position":1,"line":null,"createdAt":"2025-11-09T16:00:41Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hOiwH","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86Vfyn6","databaseId":2508139002,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Average loss is inflated by `numBatches`**  \n`epochLoss` accumulates the *mean* loss of every batch, so `avgGenLoss` ends up being `numBatches` times larger than the true average. Anything consuming the callback (progress bars, early stopping, logging) will read nonsense. Normalise the epoch loss before folding it into the generation loss.  \n\n\n```diff\n-                generationLoss = _numOps.Add(generationLoss, epochLoss);\n+                var avgEpochLoss = _numOps.Divide(epochLoss, _numOps.FromDouble(numBatches));\n+                generationLoss = _numOps.Add(generationLoss, avgEpochLoss);\n             }\n \n-            var avgGenLoss = _numOps.Divide(generationLoss, _numOps.FromDouble(epochs));\n+            var avgGenLoss = _numOps.Divide(generationLoss, _numOps.FromDouble(epochs));\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n            for (int epoch = 0; epoch < epochs; epoch++)\n            {\n                T epochLoss = _numOps.Zero;\n\n                // Shuffle data\n                var (shuffledInputs, shuffledLabels) = ShuffleData(trainInputs, trainLabels);\n                Vector<T>[]? shuffledTeacher = null;\n\n                if (teacherPredictions != null)\n                {\n                    // Shuffle teacher predictions in sync\n                    var indices = Enumerable.Range(0, trainInputs.Length).OrderBy(_ => Guid.NewGuid()).ToArray();\n                    shuffledTeacher = indices.Select(i => teacherPredictions[i]).ToArray();\n                }\n\n                // Train on batches\n                for (int b = 0; b < numBatches; b++)\n                {\n                    int start = b * batchSize;\n                    int end = Math.Min(start + batchSize, trainInputs.Length);\n                    int currentBatchSize = end - start;\n\n                    var batchInputs = new Vector<T>[currentBatchSize];\n                    var batchLabels = new Vector<T>[currentBatchSize];\n                    Array.Copy(shuffledInputs, start, batchInputs, 0, currentBatchSize);\n                    Array.Copy(shuffledLabels, start, batchLabels, 0, currentBatchSize);\n\n                    Vector<T>[]? batchTeacher = null;\n                    if (shuffledTeacher != null)\n                    {\n                        batchTeacher = new Vector<T>[currentBatchSize];\n                        Array.Copy(shuffledTeacher, start, batchTeacher, 0, currentBatchSize);\n                    }\n\n                    var batchLoss = TrainBatch(\n                        modelForward,\n                        modelBackward,\n                        batchInputs,\n                        batchLabels,\n                        batchTeacher);\n                    epochLoss = _numOps.Add(epochLoss, batchLoss);\n                }\n\n                var avgEpochLoss = _numOps.Divide(epochLoss, _numOps.FromDouble(numBatches));\n                generationLoss = _numOps.Add(generationLoss, avgEpochLoss);\n            }\n\n            var avgGenLoss = _numOps.Divide(generationLoss, _numOps.FromDouble(epochs));\n\n            // Invoke callback if provided\n            onGenerationComplete?.Invoke(generation, avgGenLoss);\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/SelfDistillationTrainer.cs around lines 158 to 208,\nepochLoss is built by summing per-batch mean losses so folding it directly into\ngenerationLoss inflates avgGenLoss by ~numBatches; normalize epochLoss before\nadding it to generationLoss. After the inner batch loop and before\ngenerationLoss = _numOps.Add(...), divide epochLoss by the number of batches (or\nbetter, compute a weighted epoch mean by dividing the summed batch losses by the\ntotal number of training samples) so you add the true epoch average to\ngenerationLoss; then compute avgGenLoss as now-normalized generationLoss divided\nby epochs.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits a2059ca to db0b301","path":"src/KnowledgeDistillation/SelfDistillationTrainer.cs","position":1,"line":null,"createdAt":"2025-11-09T16:00:42Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hbeVE","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86Vxl1y","databaseId":2512805234,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Fix Evaluate return range in docs.**\n\n`KnowledgeDistillationTrainerBase.Evaluate` returns a percentage in `[0, 100]`, but the XML comment promises a value between `0` and `1`. This mismatched contract will mislead consumers. Please update the documentation (or adjust the implementation) so they agree.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/Interfaces/IKnowledgeDistillationTrainer.cs around lines 72 to 86, the\nXML comment for Evaluate incorrectly states the return is a fraction in [0,1]\nwhile the implementation returns a percentage in [0,100]; update the\ndocumentation to reflect the actual contract (return is a percentage between 0\nand 100) by changing the summary/returns/remarks to state percentage semantics\nand range, and adjust the beginner note accordingly (or, if you prefer changing\nbehavior instead, modify the implementation to return a 0ΓÇô1 fraction and update\ncallers/tests to match) so the doc and implementation agree.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits c962fd4 to a523b32","path":"src/Interfaces/IKnowledgeDistillationTrainer.cs","position":1,"line":null,"createdAt":"2025-11-11T04:35:49Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hbeVK","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86Vxl18","databaseId":2512805244,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Restore the interface-compliant `Train` signature.**\n\n`IKnowledgeDistillationTrainer` declares `Train(..., int epochs, int batchSize = 32, Action<int, T>? onEpochComplete = null)`. This class only exposes an overload with extra validation parameters, so the build fails with CS0535. Reintroduce the exact interface signature (e.g., thin wrapper calling the extended overload) or adjust parameter list to match the interface contract while keeping validation supported through a separate method.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/KnowledgeDistillationTrainerBase.cs around lines\n160 to 239, the class removed the interface-required Train signature so the type\nno longer implements IKnowledgeDistillationTrainer (CS0535). Restore the exact\ninterface method signature (matching parameter names, types and defaults) by\nadding a thin public virtual Train method that matches the interface and\nforwards to the existing extended overload (or vice versa), performing any\nvalidation inside the extended overload only; ensure parameter order, defaults\nand nullability exactly match the interface contract so the class compiles\nagainst IKnowledgeDistillationTrainer.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits acece9d to 72aeece","path":"src/KnowledgeDistillation/KnowledgeDistillationTrainerBase.cs","position":290,"line":290,"createdAt":"2025-11-11T04:35:49Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hbeVV","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86Vxl2N","databaseId":2512805261,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Fix KL divergence argument order to match distillation math.**\n\n`ComputeLoss` feeds `studentSoft` as the first argument to `KLDivergence`, so you are minimizing D(studentΓÇûteacher) while `ComputeGradient` assumes D(teacherΓÇûstudent). This mismatch produces a systematically wrong loss scalar and gradient direction, breaking distillation convergence. Flip the arguments so both loss and gradient optimize the same objective.\n\n```diff\n-        var softLoss = KLDivergence(studentSoft, teacherSoft);\n+        var softLoss = KLDivergence(teacherSoft, studentSoft);\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/Strategies/AttentionDistillationStrategy.cs around\nlines 145 to 172, the KL divergence call currently computes\nKLDivergence(studentSoft, teacherSoft) which is the wrong direction; change it\nto KLDivergence(teacherSoft, studentSoft) so loss matches the distillation math\nand gradients; make this swap for the softLoss computation and verify\nComputeGradient (and any other places using KLDivergence) uses the same argument\norder to ensure loss and gradient optimize the same objective.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits c962fd4 to d0f53e7","path":"src/KnowledgeDistillation/Strategies/AttentionDistillationStrategy.cs","position":172,"line":172,"createdAt":"2025-11-11T04:35:50Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hbeVb","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86Vxl2W","databaseId":2512805270,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Relational loss never influences training.**\n\n`KnowledgeDistillationTrainerBase.TrainBatch` calls `ComputeLoss/ComputeGradient` per sample. Here those overrides only reproduce standard response distillation; the relational terms live in `ComputeRelationalLoss`, but that method is never invoked, and no gradient contribution is propagated. As a result this strategy collapses to ordinary distillation and cannot satisfy Issue┬á#408ΓÇÖs relational KD requirement. Please integrate the relational objectives into `ComputeLoss`/`ComputeGradient` (e.g., by accumulating batch embeddings inside the strategy or by adapting the trainer to pass them) so the returned loss/gradient actually reflect distance/angle penalties.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/Strategies/RelationalDistillationStrategy.cs around\nlines 145ΓÇô214, the overrides for ComputeLoss and ComputeGradient only compute\nstandard response distillation and never include the relational terms\n(ComputeRelationalLoss is unused), so relational KD has no effect; fix this by\nintegrating relational objectives into these methods: either (A) change\nComputeLoss/ComputeGradient signatures (and update the trainer call sites) to\naccept batch-level student/teacher embeddings (or a batch context) and then call\nComputeRelationalLoss to compute the relational loss/grad for the batch and add\nit (with its weighting hyperparameter) to the per-sample soft/hard losses and\ngradients, or (B) keep the current signatures but add internal per-batch buffers\nin the strategy that accumulate embeddings across TrainBatch calls, compute the\nrelational loss/gradient once per batch using those accumulated embeddings,\ndistribute/accumulate the resulting relational loss into each sampleΓÇÖs returned\nloss and add the relational gradient contribution to each sampleΓÇÖs returned\ngradient (ensuring gradients are computed w.r.t. student outputs via the\nembedding mapping), and update/clear the buffers at batch boundaries; ensure\nloss scaling and alpha/weight factors are applied consistently and that\nValidate* dimension checks still hold.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits c962fd4 to bb72f8b","path":"src/KnowledgeDistillation/Strategies/RelationalDistillationStrategy.cs","position":537,"line":537,"createdAt":"2025-11-11T04:35:50Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hbeVf","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86Vxl2e","databaseId":2512805278,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Guard `LabelSmoothingFactor` within [0,ΓÇ»1].**\n\n`Validate()` leaves `LabelSmoothingFactor` unconstrained, so callers can pass values ΓëÑ1 or negative. Typical smoothing mixes `(1 - ╬╡)` with `╬╡/(K-1)`; invalid ╬╡ leads to negative or >1 probabilities and destabilizes training. Add the same bound check you already apply for other weights.\n\n```diff\n+        if (LabelSmoothingFactor < 0 || LabelSmoothingFactor > 1)\n+            throw new ArgumentException(\"LabelSmoothingFactor must be between 0 and 1\", nameof(LabelSmoothingFactor));\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    {\n        if (Temperature <= 0)\n            throw new ArgumentException(\"Temperature must be positive\", nameof(Temperature));\n        if (Alpha < 0 || Alpha > 1)\n            throw new ArgumentException(\"Alpha must be between 0 and 1\", nameof(Alpha));\n        if (Epochs <= 0)\n            throw new ArgumentException(\"Epochs must be positive\", nameof(Epochs));\n        if (BatchSize <= 0)\n            throw new ArgumentException(\"BatchSize must be positive\", nameof(BatchSize));\n        if (LearningRate <= 0)\n            throw new ArgumentException(\"LearningRate must be positive\", nameof(LearningRate));\n        if (FeatureWeight < 0 || FeatureWeight > 1)\n            throw new ArgumentException(\"FeatureWeight must be between 0 and 1\", nameof(FeatureWeight));\n        if (AttentionWeight < 0 || AttentionWeight > 1)\n            throw new ArgumentException(\"AttentionWeight must be between 0 and 1\", nameof(AttentionWeight));\n        if (EMADecay < 0 || EMADecay > 1)\n            throw new ArgumentException(\"EMADecay must be between 0 and 1\", nameof(EMADecay));\n        if (LabelSmoothingFactor < 0 || LabelSmoothingFactor > 1)\n            throw new ArgumentException(\"LabelSmoothingFactor must be between 0 and 1\", nameof(LabelSmoothingFactor));\n        if (SelfDistillationGenerations < 1)\n            throw new ArgumentException(\"SelfDistillationGenerations must be at least 1\", nameof(SelfDistillationGenerations));\n\n        if (ValidateAfterEpoch && (ValidationInputs == null || ValidationLabels == null))\n            throw new ArgumentException(\"Validation data must be provided when ValidateAfterEpoch is true\");\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/Models/Options/KnowledgeDistillationOptions.cs around lines 260 to 282,\nadd a guard that ensures LabelSmoothingFactor is within [0, 1]; specifically,\nvalidate LabelSmoothingFactor >= 0 && LabelSmoothingFactor <= 1 and throw an\nArgumentException with a clear message and nameof(LabelSmoothingFactor) (place\nthis check alongside the other weight/bound checks in the Validate() method).\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits c962fd4 to 8beeed2","path":"src/Models/Options/KnowledgeDistillationOptions.cs","position":426,"line":426,"createdAt":"2025-11-11T04:35:50Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hboue","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86Vxz_z","databaseId":2512863219,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Swap KL arguments to keep the loss/gradient consistent.**\n\n`ComputeLoss` currently calls `KLDivergence(studentSoft, teacherSoft)`, so it reports KL(studentΓÇûteacher) while `ComputeGradient` implements the gradient of KL(teacherΓÇûstudent) (the `studentSoft - teacherSoft` term). Because of this mismatch, the optimizer follows gradients that are not the derivative of the returned scalar, breaking convergence guarantees. Swap the arguments so the value and gradient describe the same objective.\n\n```diff\n-        var softLoss = KLDivergence(studentSoft, teacherSoft);\n+        var softLoss = KLDivergence(teacherSoft, studentSoft);\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        var studentSoft = Softmax(studentOutput, Temperature);\n        var teacherSoft = Softmax(teacherOutput, Temperature);\n        var softLoss = KLDivergence(teacherSoft, studentSoft);\n        softLoss = NumOps.Multiply(softLoss, NumOps.FromDouble(Temperature * Temperature));\n\n        if (trueLabels != null)\n        {\n            ValidateLabelDimensions(studentOutput, trueLabels, v => v.Length);\n            var studentProbs = Softmax(studentOutput, 1.0);\n            var hardLoss = CrossEntropy(studentProbs, trueLabels);\n\n            var alphaT = NumOps.FromDouble(Alpha);\n            var oneMinusAlpha = NumOps.FromDouble(1.0 - Alpha);\n\n            var combinedLoss = NumOps.Add(\n                NumOps.Multiply(alphaT, hardLoss),\n                NumOps.Multiply(oneMinusAlpha, softLoss));\n\n            // Scale by (1 - contrastiveWeight) to make room for contrastive loss\n            return NumOps.Multiply(combinedLoss, NumOps.FromDouble(1.0 - _contrastiveWeight));\n        }\n\n        return NumOps.Multiply(softLoss, NumOps.FromDouble(1.0 - _contrastiveWeight));\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/Strategies/ContrastiveDistillationStrategy.cs\naround lines 124 to 147, the KL call currently computes\nKLDivergence(studentSoft, teacherSoft) which returns KL(studentΓÇûteacher) while\nthe gradient implementation expects KL(teacherΓÇûstudent); change the call to\nKLDivergence(teacherSoft, studentSoft) so the scalar loss matches the\nimplemented gradient (keep the subsequent temperature-squared scaling and the\nrest of the loss composition unchanged).\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits c962fd4 to d0f53e7","path":"src/KnowledgeDistillation/Strategies/ContrastiveDistillationStrategy.cs","position":147,"line":147,"createdAt":"2025-11-11T05:13:11Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hbouj","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86Vxz_7","databaseId":2512863227,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Fix adaptive temperature difficulty to use probabilities**\n\n`GetMaxConfidence` and `ComputeEntropy` assume normalized probabilities, but this switch feeds raw logits. With logits like `[-4, 4]`, `difficulty` becomes `-3`, so the derived temperature can exceed the configured max and wreck the scaling. Convert logits to probabilities before computing difficulty.\n\n\n\n```diff\n         switch (_strategy)\n         {\n             case AdaptiveStrategy.ConfidenceBased:\n-                // Lower confidence = harder sample = lower temperature\n-                difficulty = 1.0 - GetMaxConfidence(logits);\n-                break;\n+            {\n+                // Lower confidence = harder sample = lower temperature\n+                var confidenceProbs = ApplyTemperatureSoftmax(logits, baseTemperature);\n+                difficulty = 1.0 - GetMaxConfidence(confidenceProbs);\n+                break;\n+            }\n \n             case AdaptiveStrategy.EntropyBased:\n-                // Higher entropy = harder sample = lower temperature\n-                difficulty = ComputeEntropy(logits);\n-                break;\n+            {\n+                // Higher entropy = harder sample = lower temperature\n+                var entropyProbs = ApplyTemperatureSoftmax(logits, baseTemperature);\n+                difficulty = ComputeEntropy(entropyProbs);\n+                break;\n+            }\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/Teachers/AdaptiveTeacherModel.cs around lines 185\nto 204, the switch computes difficulty from raw logits which breaks bounds;\nconvert the logits to normalized probabilities (softmax) before calling\nGetMaxConfidence or ComputeEntropy, then clamp the resulting difficulty to\n[0,1]; leave AccuracyBased as the stored performance default but also clamp it;\nfinally use the clamped difficulty to map to adaptiveTemp so temperature remains\nwithin [_minTemperature, _maxTemperature].\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits c962fd4 to b0b3266","path":"src/KnowledgeDistillation/Teachers/AdaptiveTeacherModel.cs","position":1,"line":null,"createdAt":"2025-11-11T05:13:12Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hboum","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86Vxz_-","databaseId":2512863230,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Accuracy-based adaptation ignores recorded performance**\n\nThe accuracy branch hard-codes `difficulty = 0.5`, so temperatures never adapt no matter what `UpdateStudentPerformance` records. Please plumb the relevant sampleΓÇÖs performance (or an aggregate) into this branchΓÇöe.g., by passing the sample index into `GetSoftPredictions`/`ComputeAdaptiveTemperature` and deriving `difficulty = 1.0 - StudentPerformance[sampleIndex]` with a sensible fallback.\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits c962fd4 to 6b30c8c","path":"src/KnowledgeDistillation/Teachers/AdaptiveTeacherModel.cs","position":1,"line":null,"createdAt":"2025-11-11T05:13:12Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hbouo","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86Vxz__","databaseId":2512863231,"body":"_≡ƒ¢á∩╕Å Refactor suggestion_ | _≡ƒƒá Major_\n\n**Validate `EnsembleWeights` constraints when provided.**\n\nThe documentation states weights \"Must sum to 1.0,\" but `Validate()` doesn't enforce this. Invalid weights would produce incorrect ensemble predictions.\n\n\n\nAdd validation in the `Validate()` method:\n\n```diff\n         if (SelfDistillationGenerations < 1)\n             throw new ArgumentException(\"SelfDistillationGenerations must be at least 1\", nameof(SelfDistillationGenerations));\n+        \n+        if (EnsembleWeights != null)\n+        {\n+            if (Teachers != null && EnsembleWeights.Length != Teachers.Length)\n+                throw new ArgumentException(\"EnsembleWeights length must match Teachers length\", nameof(EnsembleWeights));\n+            \n+            double sum = EnsembleWeights.Sum();\n+            if (Math.Abs(sum - 1.0) > 1e-6)\n+                throw new ArgumentException($\"EnsembleWeights must sum to 1.0 (got {sum})\", nameof(EnsembleWeights));\n+        }\n\n         if (ValidateAfterEpoch && (ValidationInputs == null || ValidationLabels == null))\n```\n\n\nAlso applies to: 309-332\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/Models/Options/KnowledgeDistillationOptions.cs around line 112 (and also\napply same checks to the other Validate() area at lines 309-332), add validation\nfor EnsembleWeights when non-null: ensure the array length matches the number of\nmodels/ensemble members (use the existing property that holds model count),\nensure every weight is >= 0, and ensure the weights sum to 1.0 within a small\nepsilon (e.g. 1e-6); on failure, add/throw the same type of validation error\nused elsewhere in Validate() with a clear message like \"EnsembleWeights must be\nnon-negative, match model count and sum to 1.0\".\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","path":"src/Models/Options/KnowledgeDistillationOptions.cs","position":1,"line":null,"createdAt":"2025-11-11T05:13:12Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hbout","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86Vx0AI","databaseId":2512863240,"body":"_≡ƒ¢á∩╕Å Refactor suggestion_ | _≡ƒƒá Major_\n\n**Validate that validation arrays are non-empty and have matching lengths.**\n\nThe current check at lines 330-331 only guards against null, but allows empty arrays or length mismatches that would cause runtime errors during validation.\n\n\n\nEnhance the validation:\n\n```diff\n-        if (ValidateAfterEpoch && (ValidationInputs == null || ValidationLabels == null))\n-            throw new ArgumentException(\"Validation data must be provided when ValidateAfterEpoch is true\");\n+        if (ValidateAfterEpoch)\n+        {\n+            if (ValidationInputs == null || ValidationLabels == null)\n+                throw new ArgumentException(\"Validation data must be provided when ValidateAfterEpoch is true\");\n+            \n+            if (ValidationInputs.Length == 0)\n+                throw new ArgumentException(\"ValidationInputs cannot be empty\", nameof(ValidationInputs));\n+            \n+            if (ValidationLabels.Length == 0)\n+                throw new ArgumentException(\"ValidationLabels cannot be empty\", nameof(ValidationLabels));\n+            \n+            if (ValidationInputs.Length != ValidationLabels.Length)\n+                throw new ArgumentException(\n+                    $\"ValidationInputs and ValidationLabels must have the same length (got {ValidationInputs.Length} and {ValidationLabels.Length})\");\n+        }\n```\n\n\nAlso applies to: 330-331\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/Models/Options/KnowledgeDistillationOptions.cs around lines 236 and\nespecially the validation logic at lines 330-331, the current guard only checks\nfor null which permits empty arrays or mismatched lengths; update the validation\nso that if either ValidationInputs or ValidationLabels is non-null you require\nboth to be non-null, both to have Length > 0, and both to have the same Length,\notherwise throw an ArgumentException (or ArgumentNullException where\nappropriate) with a clear message indicating which array is missing/empty or\nthat lengths do not match; apply the same stricter checks at the other\noccurrence referenced (330-331).\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commit 8fa5328","path":"src/Models/Options/KnowledgeDistillationOptions.cs","position":242,"line":242,"createdAt":"2025-11-11T05:13:12Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hcGMS","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VybaB","databaseId":2513024641,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Fix compile failure: missing `ContrastiveLossType`**\n\n`CreateStrategy` references `ContrastiveLossType`, but that type is not defined anywhere in the solution (see the Quality Gates failure pointing at these lines). As a result the project no longer builds. Please add the missing enum under `AiDotNet.Enums` (e.g., values such as `InfoNCE`, `TripletLoss`, etc.) or update the factory to use an existing enum that already represents the contrastive loss modes.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/DistillationStrategyFactory.cs around lines 42 to\n43, the factory references ContrastiveLossType which does not exist; either add\na new enum ContrastiveLossType under AiDotNet.Enums (e.g., values InfoNCE,\nTripletLoss, NTXent) and update project references/imports, or change the\nfactory to use an existing enum that represents contrastive loss modes; ensure\nthe chosen enum is properly namespaced and compiled across the solution so\nCreateContrastiveStrategy(ΓÇª, contrastiveLossType ?? ContrastiveLossType.InfoNCE)\ncompiles.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits c962fd4 to 8f3cc9b","path":"src/KnowledgeDistillation/DistillationStrategyFactory.cs","position":1,"line":null,"createdAt":"2025-11-11T06:39:54Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hcGMX","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VybaH","databaseId":2513024647,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Avoid double-scaling the standard loss.**\n\n`softLoss` is already multiplied by `(1 - _factorWeight)` before the hard/soft blend, and then the blended result is multiplied by `(1 - _factorWeight)` again. With hard labels present this pushes the soft component to `(1 - _factorWeight)^2`, so even modest factor weights throttle the response loss. Please apply the scaling exactly once.\n\n\n```diff\n-        var softLoss = KLDivergence(studentSoft, teacherSoft);\n-        softLoss = NumOps.Multiply(softLoss, NumOps.FromDouble(Temperature * Temperature * (1.0 - _factorWeight)));\n+        var softLoss = KLDivergence(studentSoft, teacherSoft);\n+        softLoss = NumOps.Multiply(softLoss, NumOps.FromDouble(Temperature * Temperature));\n...\n-            return NumOps.Multiply(combinedLoss, NumOps.FromDouble(1.0 - _factorWeight));\n+            return NumOps.Multiply(combinedLoss, NumOps.FromDouble(1.0 - _factorWeight));\n         }\n \n-        return softLoss;\n+        return NumOps.Multiply(softLoss, NumOps.FromDouble(1.0 - _factorWeight));\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        // Standard distillation loss (weighted)\n        var studentSoft = Softmax(studentOutput, Temperature);\n        var teacherSoft = Softmax(teacherOutput, Temperature);\n        var softLoss = KLDivergence(studentSoft, teacherSoft);\n        softLoss = NumOps.Multiply(softLoss, NumOps.FromDouble(Temperature * Temperature));\n\n        if (trueLabels != null)\n        {\n            ValidateLabelDimensions(studentOutput, trueLabels, v => v.Length);\n            var studentProbs = Softmax(studentOutput, 1.0);\n            var hardLoss = CrossEntropy(studentProbs, trueLabels);\n            var combinedLoss = NumOps.Add(\n                NumOps.Multiply(NumOps.FromDouble(Alpha), hardLoss),\n                NumOps.Multiply(NumOps.FromDouble(1.0 - Alpha), softLoss));\n            return NumOps.Multiply(combinedLoss, NumOps.FromDouble(1.0 - _factorWeight));\n        }\n\n        return NumOps.Multiply(softLoss, NumOps.FromDouble(1.0 - _factorWeight));\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/Strategies/FactorTransferDistillationStrategy.cs\naround lines 78-96, the standard distillation term is being scaled by (1 -\n_factorWeight) twice (once when computing softLoss and again when multiplying\nthe combined loss), which double-scales the soft component when trueLabels are\npresent; fix this by removing the first scaling on softLoss and instead apply\nNumOps.Multiply(..., NumOps.FromDouble(1.0 - _factorWeight)) exactly once after\ncomputing either the blended combinedLoss (when trueLabels != null) or the plain\nsoftLoss (when trueLabels == null), so the factorWeight reduction is applied\nonly once to the final returned loss.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commit 5402416","path":"src/KnowledgeDistillation/Strategies/FactorTransferDistillationStrategy.cs","position":1,"line":null,"createdAt":"2025-11-11T06:39:54Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hcGMl","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VybaZ","databaseId":2513024665,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Correct gradient weighting for the factor mix.**\n\nThe gradient path mirrors the loss issue: each soft-term gradient already carries `(1 - _factorWeight)` and then gets multiplied by `(1 - _factorWeight)` again in the hard-label branch, shrinking the response gradient to `(1 - _factorWeight)^2`. Apply the factor weight once after combining hard and soft components.\n\n\n```diff\n-        for (int i = 0; i < n; i++)\n-        {\n-            var diff = NumOps.Subtract(studentSoft[i], teacherSoft[i]);\n-            gradient[i] = NumOps.Multiply(diff, NumOps.FromDouble(Temperature * Temperature * (1.0 - _factorWeight)));\n-        }\n+        for (int i = 0; i < n; i++)\n+        {\n+            var diff = NumOps.Subtract(studentSoft[i], teacherSoft[i]);\n+            gradient[i] = NumOps.Multiply(diff, NumOps.FromDouble(Temperature * Temperature));\n+        }\n...\n-                gradient[i] = NumOps.Add(\n-                    NumOps.Multiply(NumOps.FromDouble(Alpha * (1.0 - _factorWeight)), hardGrad),\n-                    NumOps.Multiply(NumOps.FromDouble((1.0 - Alpha) * (1.0 - _factorWeight)), gradient[i]));\n+                gradient[i] = NumOps.Add(\n+                    NumOps.Multiply(NumOps.FromDouble(Alpha), hardGrad),\n+                    NumOps.Multiply(NumOps.FromDouble(1.0 - Alpha), gradient[i]));\n         }\n \n-        return gradient;\n+        var scaling = NumOps.FromDouble(1.0 - _factorWeight);\n+        for (int i = 0; i < n; i++)\n+            gradient[i] = NumOps.Multiply(gradient[i], scaling);\n+\n+        return gradient;\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/Strategies/FactorTransferDistillationStrategy.cs\naround lines 105 to 125, the soft-term gradient is being multiplied by (1 -\n_factorWeight) in the soft loop and then the hard-branch also multiplies\ncomponents by (1 - _factorWeight), causing the factor to be applied twice.\nRemove the (1 - _factorWeight) scaling from the per-term soft gradient\ncalculation and from the per-term hard/soft combination; instead compute\nsoftGrad = temperature-scaled soft difference, compute hardGrad = studentProbs -\ntrueLabels, form combined = Alpha * hardGrad + (1 - Alpha) * softGrad, then\nmultiply the final combined gradient by NumOps.FromDouble(1.0 - _factorWeight)\n(and by any temperature^2 factor as needed) before assigning to gradient[i].\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commit 5402416","path":"src/KnowledgeDistillation/Strategies/FactorTransferDistillationStrategy.cs","position":134,"line":134,"createdAt":"2025-11-11T06:39:54Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hcGMt","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86Vybai","databaseId":2513024674,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Align KL loss with gradient and remove duplicated scaling**\n\nThe soft loss again computes KL(studentΓÇûteacher) while the gradient (Line 80) assumes KL(teacherΓÇûstudent). That inconsistency destabilizes training. Additionally, the `(1 - _selectivityWeight)` factor is applied once in Line 52 and then twice in Lines 93-94, so the gradient magnitude no longer represents the derivative of the loss the trainer optimizes. Please mirror the fix from the probabilistic strategy: swap the KL arguments and defer the `(1 - _selectivityWeight)` scaling until after the soft gradient is formed, e.g.\n\n```diff\n-        var softLoss = KLDivergence(studentSoft, teacherSoft);\n+        var softLoss = KLDivergence(teacherSoft, studentSoft);\n ΓÇª\n-        for (int i = 0; i < n; i++)\n-        {\n-            var diff = NumOps.Subtract(studentSoft[i], teacherSoft[i]);\n-            gradient[i] = NumOps.Multiply(diff, NumOps.FromDouble(Temperature * Temperature * (1.0 - _selectivityWeight)));\n-        }\n+        for (int i = 0; i < n; i++)\n+        {\n+            var diff = NumOps.Subtract(studentSoft[i], teacherSoft[i]);\n+            gradient[i] = NumOps.Multiply(diff, NumOps.FromDouble(Temperature * Temperature));\n+        }\n ΓÇª\n-                gradient[i] = NumOps.Add(\n-                    NumOps.Multiply(NumOps.FromDouble(Alpha * (1.0 - _selectivityWeight)), hardGrad),\n-                    NumOps.Multiply(NumOps.FromDouble((1.0 - Alpha) * (1.0 - _selectivityWeight)), gradient[i]));\n+                var combined = NumOps.Add(\n+                    NumOps.Multiply(NumOps.FromDouble(Alpha), hardGrad),\n+                    NumOps.Multiply(NumOps.FromDouble(1.0 - Alpha), gradient[i]));\n+                gradient[i] = NumOps.Multiply(combined, NumOps.FromDouble(1.0 - _selectivityWeight));\n             }\n         }\n-\n-        return gradient;\n+        if (trueLabels == null)\n+        {\n+            var scale = NumOps.FromDouble(1.0 - _selectivityWeight);\n+            for (int i = 0; i < n; i++)\n+                gradient[i] = NumOps.Multiply(gradient[i], scale);\n+        }\n+\n+        return gradient;\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/Strategies/NeuronSelectivityDistillationStrategy.cs\naround lines 48 to 99, the KL loss is computed as KL(studentΓÇûteacher) while the\ngradient assumes KL(teacherΓÇûstudent) and the (1 - _selectivityWeight) scaling is\napplied twice; to fix this, change the KL call to KLDivergence(teacherSoft,\nstudentSoft) so it matches the gradient, remove the (1 - _selectivityWeight)\nfactor from the softLoss multiplication (leave only Temperature*Temperature\nscaling), and likewise remove any (1 - _selectivityWeight) factors used inside\nthe gradient combination for the hard/soft parts; finally, after forming the\nfinal loss or the final combined gradient (depending on whether trueLabels is\npresent), multiply the whole loss or whole gradient once by\nNumOps.FromDouble(1.0 - _selectivityWeight) before returning.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 5402416 to c87817d","path":"src/KnowledgeDistillation/Strategies/NeuronSelectivityDistillationStrategy.cs","position":1,"line":null,"createdAt":"2025-11-11T06:39:54Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hcGMz","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86Vyban","databaseId":2513024679,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Fix inconsistent KL direction and gradient scaling**\n\nLine 62 is computing KL(studentΓÇûteacher), but Line 91 derives gradients as if the loss were KL(teacherΓÇûstudent). The mismatch means the optimizer is following the wrong objective (especially obvious when distributions differ) and will lead to divergence despite passing current tests. In the same block, once hard labels are present you scale by `(1 - _distributionWeight)` twice, so the gradient no longer matches the loss derivative. Please swap the KL arguments and refactor the gradient blend so the `(1 - _distributionWeight)` factor is applied exactly once, like this:\n\n```diff\n-        var softLoss = KLDivergence(studentSoft, teacherSoft);\n+        var softLoss = KLDivergence(teacherSoft, studentSoft);\n         softLoss = NumOps.Multiply(softLoss, NumOps.FromDouble(Temperature * Temperature * (1.0 - _distributionWeight)));\n ΓÇª\n-        for (int i = 0; i < n; i++)\n-        {\n-            var diff = NumOps.Subtract(studentSoft[i], teacherSoft[i]);\n-            gradient[i] = NumOps.Multiply(diff, NumOps.FromDouble(Temperature * Temperature * (1.0 - _distributionWeight)));\n-        }\n+        for (int i = 0; i < n; i++)\n+        {\n+            var diff = NumOps.Subtract(studentSoft[i], teacherSoft[i]);\n+            gradient[i] = NumOps.Multiply(diff, NumOps.FromDouble(Temperature * Temperature));\n+        }\n ΓÇª\n-                gradient[i] = NumOps.Add(\n-                    NumOps.Multiply(NumOps.FromDouble(Alpha * (1.0 - _distributionWeight)), hardGrad),\n-                    NumOps.Multiply(NumOps.FromDouble((1.0 - Alpha) * (1.0 - _distributionWeight)), gradient[i]));\n+                var combined = NumOps.Add(\n+                    NumOps.Multiply(NumOps.FromDouble(Alpha), hardGrad),\n+                    NumOps.Multiply(NumOps.FromDouble(1.0 - Alpha), gradient[i]));\n+                gradient[i] = NumOps.Multiply(combined, NumOps.FromDouble(1.0 - _distributionWeight));\n             }\n         }\n-\n-        return gradient;\n+        if (trueLabels == null)\n+        {\n+            var scale = NumOps.FromDouble(1.0 - _distributionWeight);\n+            for (int i = 0; i < n; i++)\n+            {\n+                gradient[i] = NumOps.Multiply(gradient[i], scale);\n+            }\n+        }\n+\n+        return gradient;\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        // Standard distillation loss (weighted)\n        var studentSoft = Softmax(studentOutput, Temperature);\n        var teacherSoft = Softmax(teacherOutput, Temperature);\n        var softLoss = KLDivergence(teacherSoft, studentSoft);\n        softLoss = NumOps.Multiply(softLoss, NumOps.FromDouble(Temperature * Temperature * (1.0 - _distributionWeight)));\n\n        if (trueLabels != null)\n        {\n            ValidateLabelDimensions(studentOutput, trueLabels, v => v.Length);\n            var studentProbs = Softmax(studentOutput, 1.0);\n            var hardLoss = CrossEntropy(studentProbs, trueLabels);\n            var combinedLoss = NumOps.Add(\n                NumOps.Multiply(NumOps.FromDouble(Alpha), hardLoss),\n                NumOps.Multiply(NumOps.FromDouble(1.0 - Alpha), softLoss));\n            return NumOps.Multiply(combinedLoss, NumOps.FromDouble(1.0 - _distributionWeight));\n        }\n\n        return softLoss;\n    }\n\n    public override Vector<T> ComputeGradient(Vector<T> studentOutput, Vector<T> teacherOutput, Vector<T>? trueLabels = null)\n    {\n        ValidateOutputDimensions(studentOutput, teacherOutput, v => v.Length);\n\n        int n = studentOutput.Length;\n        var gradient = new Vector<T>(n);\n\n        var studentSoft = Softmax(studentOutput, Temperature);\n        var teacherSoft = Softmax(teacherOutput, Temperature);\n\n        for (int i = 0; i < n; i++)\n        {\n            var diff = NumOps.Subtract(studentSoft[i], teacherSoft[i]);\n            gradient[i] = NumOps.Multiply(diff, NumOps.FromDouble(Temperature * Temperature));\n        }\n\n        if (trueLabels != null)\n        {\n            ValidateLabelDimensions(studentOutput, trueLabels, v => v.Length);\n            var studentProbs = Softmax(studentOutput, 1.0);\n\n            for (int i = 0; i < n; i++)\n            {\n                var hardGrad = NumOps.Subtract(studentProbs[i], trueLabels[i]);\n                var combined = NumOps.Add(\n                    NumOps.Multiply(NumOps.FromDouble(Alpha), hardGrad),\n                    NumOps.Multiply(NumOps.FromDouble(1.0 - Alpha), gradient[i]));\n                gradient[i] = NumOps.Multiply(combined, NumOps.FromDouble(1.0 - _distributionWeight));\n            }\n        }\n\n        if (trueLabels == null)\n        {\n            var scale = NumOps.FromDouble(1.0 - _distributionWeight);\n            for (int i = 0; i < n; i++)\n            {\n                gradient[i] = NumOps.Multiply(gradient[i], scale);\n            }\n        }\n\n        return gradient;\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/Strategies/ProbabilisticDistillationStrategy.cs\naround lines 59-110, the KL is computed as KL(studentΓÇûteacher) but the gradient\nis derived as if KL(teacherΓÇûstudent); also (1 - _distributionWeight) is applied\ntwice to the gradient when true labels are present. Fix by swapping the KL\ninputs (compute KLDivergence(teacherSoft, studentSoft)) and in ComputeGradient\ncompute the soft-gradient as the derivative consistent with KL(teacherΓÇûstudent)\n(use teacherSoft - studentSoft) scaled by Temperature*Temperature*(1 -\n_distributionWeight) exactly once; when blending with hard-label gradient,\ncombine as: Alpha*(1 - _distributionWeight)*hardGrad + (1 - Alpha)*(1 -\n_distributionWeight)*softGrad so the (1 - _distributionWeight) factor is applied\nonly once to the combined gradient.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 5402416 to 2bc248f","path":"src/KnowledgeDistillation/Strategies/ProbabilisticDistillationStrategy.cs","position":1,"line":null,"createdAt":"2025-11-11T06:39:54Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hcGNA","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86Vyba1","databaseId":2513024693,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Defend against mismatched embedding batches**\n\n`ComputeSimilarityLoss` assumes both embedding arrays are non-null, have identical batch sizes, and that every student/teacher pair shares the same dimensionality. Any mismatch will currently surface as an `IndexOutOfRangeException` inside the nested loops. Please add the usual validation before iterating.\n\n```diff\n-    public T ComputeSimilarityLoss(Vector<T>[] studentEmbeddings, Vector<T>[] teacherEmbeddings)\n-    {\n-        int n = studentEmbeddings.Length;\n-        T totalLoss = NumOps.Zero;\n-        int pairCount = 0;\n+    public T ComputeSimilarityLoss(Vector<T>[] studentEmbeddings, Vector<T>[] teacherEmbeddings)\n+    {\n+        ArgumentNullException.ThrowIfNull(studentEmbeddings);\n+        ArgumentNullException.ThrowIfNull(teacherEmbeddings);\n+\n+        if (studentEmbeddings.Length != teacherEmbeddings.Length)\n+        {\n+            throw new ArgumentException(\n+                $\"Student and teacher embedding batches must match. Student: {studentEmbeddings.Length}, Teacher: {teacherEmbeddings.Length}\");\n+        }\n+\n+        int n = studentEmbeddings.Length;\n+        if (n == 0)\n+        {\n+            return NumOps.Zero;\n+        }\n+\n+        int expectedDim = studentEmbeddings[0].Length;\n+        for (int i = 0; i < n; i++)\n+        {\n+            if (studentEmbeddings[i].Length != expectedDim || teacherEmbeddings[i].Length != expectedDim)\n+            {\n+                throw new ArgumentException(\"All embeddings must share the same dimensionality.\");\n+            }\n+        }\n+\n+        T totalLoss = NumOps.Zero;\n+        int pairCount = 0;\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    public T ComputeSimilarityLoss(Vector<T>[] studentEmbeddings, Vector<T>[] teacherEmbeddings)\n    {\n        ArgumentNullException.ThrowIfNull(studentEmbeddings);\n        ArgumentNullException.ThrowIfNull(teacherEmbeddings);\n\n        if (studentEmbeddings.Length != teacherEmbeddings.Length)\n        {\n            throw new ArgumentException(\n                $\"Student and teacher embedding batches must match. Student: {studentEmbeddings.Length}, Teacher: {teacherEmbeddings.Length}\");\n        }\n\n        int n = studentEmbeddings.Length;\n        if (n == 0)\n        {\n            return NumOps.Zero;\n        }\n\n        int expectedDim = studentEmbeddings[0].Length;\n        for (int i = 0; i < n; i++)\n        {\n            if (studentEmbeddings[i].Length != expectedDim || teacherEmbeddings[i].Length != expectedDim)\n            {\n                throw new ArgumentException(\"All embeddings must share the same dimensionality.\");\n            }\n        }\n\n        T totalLoss = NumOps.Zero;\n        int pairCount = 0;\n\n        for (int i = 0; i < n; i++)\n        {\n            for (int j = i + 1; j < n; j++)\n            {\n                double teacherSim = CosineSimilarity(teacherEmbeddings[i], teacherEmbeddings[j]);\n                double studentSim = CosineSimilarity(studentEmbeddings[i], studentEmbeddings[j]);\n                double diff = teacherSim - studentSim;\n                totalLoss = NumOps.Add(totalLoss, NumOps.FromDouble(diff * diff));\n                pairCount++;\n            }\n        }\n\n        var loss = pairCount > 0 ? NumOps.Divide(totalLoss, NumOps.FromDouble(pairCount)) : NumOps.Zero;\n        return NumOps.Multiply(loss, NumOps.FromDouble(_similarityWeight));\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits f5a28ea to c2be9b8","path":"src/KnowledgeDistillation/Strategies/SimilarityPreservingStrategy.cs","position":120,"line":120,"createdAt":"2025-11-11T06:39:55Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hcGNK","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VybbA","databaseId":2513024704,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Variational weight zeroes the loss and never adds the variational term**\n\n`ComputeLoss` never calls `ComputeVariationalLoss`, so `_mode`/`_betaIB` are ignored and the strategy collapses to a softened KL/CrossEntropy. Worse, `_variationalWeight` is only used to scale the classical KD loss, meaning `_variationalWeight = 1` returns zero loss and any positive value just shrinks the base loss without ever adding the variational component. This makes the strategy functionally incorrect and unusable. Please incorporate the variational loss (e.g., compute and add `_variationalWeight * L_variational`) and leave the classical part with its standard `(╬▒, (1-╬▒)T┬▓)` scaling instead of suppressing it.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/Strategies/VariationalDistillationStrategy.cs\naround lines 63 to 85, the method currently never calls ComputeVariationalLoss\nand wrongly uses _variationalWeight to scale down the classical KD loss (causing\nloss to become zero when _variationalWeight == 1); change the logic to compute\nvarLoss = ComputeVariationalLoss(studentOutput[, teacherOutput or needed\ninputs]) and combine losses as: classical hard/soft terms keep their standard\nscaling (hardLoss * Alpha + softLoss * (1-Alpha) * Temperature^2), then add the\nvariational term weighted by _variationalWeight (totalLoss = classicalLoss +\n_variationalWeight * varLoss); apply the same combination whether trueLabels is\npresent or not (when no trueLabels classicalLoss == softLoss), and do not\nmultiply the classicalLoss by (1 - _variationalWeight).\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","path":"src/KnowledgeDistillation/Strategies/VariationalDistillationStrategy.cs","position":85,"line":85,"createdAt":"2025-11-11T06:39:55Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hcGNR","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VybbJ","databaseId":2513024713,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Gradient omits variational contribution and improperly scales classical term**\n\n`ComputeGradient` mirrors the loss issue: it never differentiates any variational objective, yet it multiplies the entire classical gradient by `(1 - _variationalWeight)`. Setting `_variationalWeight` near 1 drives the gradient to zero even though no variational gradient is supplied, so training stalls. Please add the gradient of the chosen variational objective and keep the response-based gradient unscaled, combining them as `(1 - w)ΓêçL_classic + wΓêçL_variational`.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/Strategies/VariationalDistillationStrategy.cs\naround lines 87 to 114, the gradient implementation currently only uses the\nteacher-student softmax term scaled by (1 - _variationalWeight) and never\ncomputes the variational objective gradient, which causes gradients to vanish\nwhen _variationalWeight Γëê 1. Compute the variational gradient for the selected\nvariational objective (e.g., gradient of the variational loss term with respect\nto studentOutput) and do not scale the classical response-based gradient itself;\ninstead combine them as finalGradient = (1 - _variationalWeight) * grad_classic\n+ _variationalWeight * grad_variational (keeping any temperature-dependent\nfactors correct for each term), and when true labels are present apply the\nhard-label contribution to grad_classic before combination so the final\nassignment replaces gradient[i] with that weighted sum for each element.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","path":"src/KnowledgeDistillation/Strategies/VariationalDistillationStrategy.cs","position":114,"line":114,"createdAt":"2025-11-11T06:39:55Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hcGNX","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VybbR","databaseId":2513024721,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Use plain average when no weights are supplied**\n\n`CreateEnsembleTeacher` always picks `EnsembleAggregation.WeightedAverage`, so calling it without weights will push a null weight array into the weighted path (very likely leading to a runtime failure). Switch to the plain average when weights are absent and validate the provided weights length.  \n\n\n```diff\n         if (ensembleModels == null || ensembleModels.Length == 0)\n             throw new ArgumentException(\"Ensemble models are required for Ensemble teacher type\");\n \n-        var aggregation = ensembleWeights != null\n-            ? EnsembleAggregation.WeightedAverage\n-            : EnsembleAggregation.WeightedAverage;\n+        var aggregation = ensembleWeights != null\n+            ? EnsembleAggregation.WeightedAverage\n+            : EnsembleAggregation.Average;\n+\n+        if (ensembleWeights != null && ensembleWeights.Length != ensembleModels.Length)\n+            throw new ArgumentException(\"Ensemble weights must match number of models\", nameof(ensembleWeights));\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        if (ensembleModels == null || ensembleModels.Length == 0)\n            throw new ArgumentException(\"Ensemble models are required for Ensemble teacher type\");\n\n        var aggregation = ensembleWeights != null\n            ? EnsembleAggregation.WeightedAverage\n            : EnsembleAggregation.Average;\n\n        if (ensembleWeights != null && ensembleWeights.Length != ensembleModels.Length)\n            throw new ArgumentException(\"Ensemble weights must match number of models\", nameof(ensembleWeights));\n\n        return new EnsembleTeacherModel<T>(ensembleModels, aggregation, ensembleWeights);\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/TeacherModelFactory.cs around lines 61 to 69, the\nmethod always selects EnsembleAggregation.WeightedAverage which passes a null\nweight array when no weights are supplied; change the aggregation selection to\nuse EnsembleAggregation.Average when ensembleWeights is null, and when\nensembleWeights is provided validate that its Length equals\nensembleModels.Length (throw ArgumentException if not) to prevent runtime\nfailures.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits e713a2b to a770473","path":"src/KnowledgeDistillation/TeacherModelFactory.cs","position":84,"line":84,"createdAt":"2025-11-11T06:39:55Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hcGNf","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86Vybbf","databaseId":2513024735,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Do not guess output dimension via dummy prediction or silently default to 10**\n\n`GetOutputDimensionFromModel` fabricates a lengthΓÇæ1 input and swallows any failure by defaulting to 10 outputs. Real models typically expect their true input dimensionality, so the dummy call will throw, after which the method returns 10 even if the model actually has 2 or 512 classes. Downstream, the trainer will build one-hot labels of the wrong size and either crash or corrupt training. Please remove the dummy inference/default and require a reliable source (metadata or explicit constructor parameter); if you cannot determine the dimension, throw instead of guessing.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/TeacherModelWrapper.cs around lines 134 to 148,\nremove the logic that fabricates a lengthΓÇæ1 dummy input, calls model.Predict,\nand falls back to returning 10; instead require the output dimension to be\nprovided reliably (e.g., as an explicit constructor parameter or read from\nvalidated model metadata) and use that value; if the output dimension cannot be\ndetermined from metadata or an explicit argument, throw a clear exception rather\nthan guessing so downstream code cannot continue with an incorrect label size.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits a0cfdab to 87c8373","path":"src/KnowledgeDistillation/TeacherModelWrapper.cs","position":1,"line":null,"createdAt":"2025-11-11T06:39:55Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hcGNm","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86Vybbk","databaseId":2513024740,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Validate worker set before aggregation**\n\nAccessing `_workers[0]` assumes the input array is non-empty and that every worker shares the same output dimensionality. If the caller passes an empty array, a null slot, or a worker with a different `OutputDimension`, this will throw immediately (IndexOutOfRangeException) when `OutputDimension` or `GetLogits` is invoked. Please fail fast in the constructor.\n\n```diff\n     public DistributedTeacherModel(\n         ITeacherModel<Vector<T>, Vector<T>>[] workers,\n         AggregationMode aggregation = AggregationMode.Average)\n     {\n-        _workers = workers ?? throw new ArgumentNullException(nameof(workers));\n-        _aggregation = aggregation;\n+        if (workers == null)\n+            throw new ArgumentNullException(nameof(workers));\n+        if (workers.Length == 0)\n+            throw new ArgumentException(\"At least one worker must be provided.\", nameof(workers));\n+\n+        var first = workers[0] ?? throw new ArgumentException(\"Workers cannot contain null entries.\", nameof(workers));\n+        int expectedDim = first.OutputDimension;\n+\n+        for (int i = 0; i < workers.Length; i++)\n+        {\n+            if (workers[i] == null)\n+                throw new ArgumentException($\"Worker index {i} is null.\", nameof(workers));\n+            if (workers[i].OutputDimension != expectedDim)\n+                throw new ArgumentException(\"All workers must share the same output dimension.\", nameof(workers));\n+        }\n+\n+        _workers = workers;\n+        _aggregation = aggregation;\n     }\n```\n\n\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 5402416 to 38b49eb","path":"src/KnowledgeDistillation/Teachers/DistributedTeacherModel.cs","position":46,"line":46,"createdAt":"2025-11-11T06:39:56Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hcGNs","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86Vybbr","databaseId":2513024747,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Empty modality list crashes construction**\n\nIf `modalityTeachers` is empty we divide by zero in `1.0 / modalityTeachers.Length` and then dereference `_modalityTeachers[0]`, so the class throws immediately. We should also reject teachers with mismatched `OutputDimension`, because the nested loop will index past the end of a shorter logits vector. Please guard for both cases up front.  \n\n\n```diff\n-        _modalityTeachers = modalityTeachers ?? throw new ArgumentNullException(nameof(modalityTeachers));\n-\n-        if (modalityWeights == null)\n+        if (modalityTeachers is null)\n+            throw new ArgumentNullException(nameof(modalityTeachers));\n+\n+        if (modalityTeachers.Length == 0)\n+            throw new ArgumentException(\"At least one modality teacher is required\", nameof(modalityTeachers));\n+\n+        for (int i = 1; i < modalityTeachers.Length; i++)\n+        {\n+            if (modalityTeachers[i].OutputDimension != modalityTeachers[0].OutputDimension)\n+                throw new ArgumentException(\"All modality teachers must share the same output dimension\", nameof(modalityTeachers));\n+        }\n+\n+        _modalityTeachers = modalityTeachers;\n+\n+        if (modalityWeights == null)\n         {\n             _modalityWeights = Enumerable.Repeat(1.0 / modalityTeachers.Length, modalityTeachers.Length).ToArray();\n         }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        if (modalityTeachers is null)\n            throw new ArgumentNullException(nameof(modalityTeachers));\n\n        if (modalityTeachers.Length == 0)\n            throw new ArgumentException(\"At least one modality teacher is required\", nameof(modalityTeachers));\n\n        for (int i = 1; i < modalityTeachers.Length; i++)\n        {\n            if (modalityTeachers[i].OutputDimension != modalityTeachers[0].OutputDimension)\n                throw new ArgumentException(\"All modality teachers must share the same output dimension\", nameof(modalityTeachers));\n        }\n\n        _modalityTeachers = modalityTeachers;\n\n        if (modalityWeights == null)\n        {\n            _modalityWeights = Enumerable.Repeat(1.0 / modalityTeachers.Length, modalityTeachers.Length).ToArray();\n        }\n        else\n        {\n            if (modalityWeights.Length != modalityTeachers.Length)\n                throw new ArgumentException(\"Modality weights must match number of teachers\");\n            _modalityWeights = modalityWeights;\n        }\n    }\n\n    public override Vector<T> GetLogits(Vector<T> input)\n    {\n        int n = _modalityTeachers[0].OutputDimension;\n        var combined = new Vector<T>(n);\n\n        for (int j = 0; j < n; j++)\n        {\n            T sum = NumOps.Zero;\n            for (int i = 0; i < _modalityTeachers.Length; i++)\n            {\n                var logits = _modalityTeachers[i].GetLogits(input);\n                var weighted = NumOps.Multiply(logits[j], NumOps.FromDouble(_modalityWeights[i]));\n                sum = NumOps.Add(sum, weighted);\n            }\n            combined[j] = sum;\n        }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nsrc/KnowledgeDistillation/Teachers/MultiModalTeacherModel.cs around lines 20 to\n49: constructor currently divides by modalityTeachers.Length and accesses\n_modalityTeachers[0] without checking for empty input and doesn't validate that\nevery teacher has the same OutputDimension; update the constructor to first\nthrow an ArgumentException if modalityTeachers is null or has zero length, then\nset _modalityTeachers = modalityTeachers, compute default modalityWeights only\nafter confirming non-zero length (or validate provided modalityWeights length\nmatches teachers), and iterate through the teachers to ensure each\nteacher.OutputDimension equals the first teacher's OutputDimension throwing an\nArgumentException if any mismatch is found so later GetLogits won't index past\nshorter logits vectors.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 5402416 to 06dad2d","path":"src/KnowledgeDistillation/Teachers/MultiModalTeacherModel.cs","position":91,"line":91,"createdAt":"2025-11-11T06:39:56Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hcGN5","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86Vybb4","databaseId":2513024760,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Fix quantization-bit overflow.**\n\n`(1 << _quantizationBits)` overflows for `_quantizationBits` ΓëÑ 31 (e.g., 1 << 31 becomes negative, 1 << 32 wraps to 1), so `scale` collapses to zero or a negative number and every logits vector quantizes incorrectly. Please switch to a non-overflowing computation.\n\n\n```diff\n-        double scale = (1 << _quantizationBits) - 1;\n+        double scale = Math.Pow(2.0, _quantizationBits) - 1.0;\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        double scale = Math.Pow(2.0, _quantizationBits) - 1.0;\n\n        T minVal = vector[0], maxVal = vector[0];\n        for (int i = 1; i < n; i++)\n        {\n            if (NumOps.LessThan(vector[i], minVal)) minVal = vector[i];\n            if (NumOps.GreaterThan(vector[i], maxVal)) maxVal = vector[i];\n        }\n\n        double range = NumOps.ToDouble(NumOps.Subtract(maxVal, minVal));\n        if (range < 1e-10) return vector;\n\n        for (int i = 0; i < n; i++)\n        {\n            double normalized = (NumOps.ToDouble(vector[i]) - NumOps.ToDouble(minVal)) / range;\n            int quantized = (int)(normalized * scale);\n            double dequantized = NumOps.ToDouble(minVal) + (quantized / scale) * range;\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/Teachers/QuantizedTeacherModel.cs around lines\n42-58, the computation double scale = (1 << _quantizationBits) - 1 overflows for\n_quantizationBits ΓëÑ 31; replace it with a non-overflowing calculation (e.g.,\ndouble scale = Math.Pow(2.0, _quantizationBits) - 1.0) or use a 64-bit shift\nwith validation of the bit count, ensure you validate/clamp _quantizationBits to\na safe range so scale>0, and clamp the computed quantized value to [0, scale]\nbefore dequantizing.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","path":"src/KnowledgeDistillation/Teachers/QuantizedTeacherModel.cs","position":1,"line":null,"createdAt":"2025-11-11T06:39:56Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hcGOB","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VybcB","databaseId":2513024769,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Fix return type to match the optimizer result**\n\nThe build is currently failing (`OptimizerResult<,,>` type/namespace not found) because we introduced a type that doesnΓÇÖt exist in the project. The optimizers return `OptimizationResult<T, TInput, TOutput>` (the type used elsewhere), so we need to align both the local variable and the helper signature with that concrete type.  \n\n\n```diff\n-        OptimizerResult<T, TInput, TOutput> optimizationResult;\n+        OptimizationResult<T, TInput, TOutput> optimizationResult;\n@@\n-    private async Task<OptimizerResult<T, TInput, TOutput>> PerformKnowledgeDistillationAsync(\n+    private async Task<OptimizationResult<T, TInput, TOutput>> PerformKnowledgeDistillationAsync(\n         IFullModel<T, TInput, TOutput> studentModel,\n         IOptimizer<T, TInput, TOutput> optimizer,\n         TInput XTrain,\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/PredictionModelBuilder.cs around lines 425 to 446, the local variable\ntype and helper signature use a non-existent OptimizerResult<,,>; change them to\nuse the existing OptimizationResult<T, TInput, TOutput> type instead. Update the\nvariable declaration and any method/ helper signatures or calls in this block\n(e.g., PerformKnowledgeDistillationAsync return type and finalOptimizer.Optimize\ncall expectation) to use OptimizationResult<T, TInput, TOutput> so the types\nalign with the rest of the project and the build succeeds.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits acece9d to 72aeece","path":"src/PredictionModelBuilder.cs","position":1,"line":null,"createdAt":"2025-11-11T06:39:56Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hcGOG","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VybcH","databaseId":2513024775,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**DonΓÇÖt validate options before the builder supplies validation data**\n\nCalling `options.Validate()` here throws for the default configuration because `ValidateAfterEpoch` defaults to true while `ValidationInputs`/`ValidationLabels` stay null; the builder only produces validation splits later. The result is an `ArgumentException` as soon as users call `ConfigureKnowledgeDistillation` with the documented defaults. Defer validation until after you prepare the data (or populate the option fields first).  \n\n\n```diff\n-        options.Validate(); // Validate options immediately\n         _knowledgeDistillationOptions = options;\n         return this;\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    public IPredictionModelBuilder<T, TInput, TOutput> ConfigureKnowledgeDistillation(\n        KnowledgeDistillationOptions<TInput, TOutput, T> options)\n    {\n        ArgumentNullException.ThrowIfNull(options);\n        _knowledgeDistillationOptions = options;\n        return this;\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits c962fd4 to 8f3cc9b","path":"src/PredictionModelBuilder.cs","position":116,"line":993,"createdAt":"2025-11-11T06:39:56Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hcSed","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VyrgS","databaseId":2513090578,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Reject malformed layer pair strings**\n\nThe constructor currently treats any string containing `:` as valid, so inputs like `\"teacher:\"` or `\"teacher:student:extra\"` slip through. Those produce empty or ambiguous layer names, and later in `ComputeFeatureLoss` we call the extractors with those invalid names, which will either hit the wrong layer or throw at runtime. Tighten the validation so we only accept exactly two non-empty names per pair.\n\n\n\n```diff\n-        var invalidPairs = _layerPairs.Where(pair => string.IsNullOrWhiteSpace(pair) || !pair.Contains(':')).ToArray();\n+        var invalidPairs = _layerPairs\n+            .Where(pair =>\n+            {\n+                if (string.IsNullOrWhiteSpace(pair))\n+                    return true;\n+\n+                var parts = pair.Split(':');\n+                if (parts.Length != 2)\n+                    return true;\n+\n+                return string.IsNullOrWhiteSpace(parts[0]) || string.IsNullOrWhiteSpace(parts[1]);\n+            })\n+            .ToArray();\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        // Validate layer pair format using explicit Where\n        var invalidPairs = _layerPairs\n            .Where(pair =>\n            {\n                if (string.IsNullOrWhiteSpace(pair))\n                    return true;\n\n                var parts = pair.Split(':');\n                if (parts.Length != 2)\n                    return true;\n\n                return string.IsNullOrWhiteSpace(parts[0]) || string.IsNullOrWhiteSpace(parts[1]);\n            })\n            .ToArray();\n        if (invalidPairs.Length > 0)\n        {\n            var invalidList = string.Join(\", \", invalidPairs.Select(p => $\"'{p}'\"));\n            throw new ArgumentException(\n                $\"Invalid layer pair format: {invalidList}. Expected 'teacher_layer:student_layer'\",\n                nameof(layerPairs));\n        }\n    }\n\n    /// <summary>\n    /// Computes the feature matching loss between student and teacher intermediate representations.\n    /// </summary>\n    /// <param name=\"teacherFeatureExtractor\">Function to extract teacher features for a layer name.</param>\n    /// <param name=\"studentFeatureExtractor\">Function to extract student features for a layer name.</param>\n    /// <param name=\"input\">Input data for forward pass.</param>\n    /// <returns>Mean squared error between matched feature pairs.</returns>\n    /// <remarks>\n    /// <para><b>For Beginners:</b> This computes how different the student's internal features\n    /// are from the teacher's. Lower loss means the student is learning to think like the teacher.</para>\n    ///\n    /// <para>The loss is computed as:\n    /// L_feature = (1/N) ├ù ╬ú MSE(teacher_features_i, student_features_i)\n    /// where N is the number of layer pairs.</para>\n    ///\n    /// <para>If feature dimensions don't match, consider adding a projection layer\n    /// (simple linear transformation) to the student.</para>\n    /// </remarks>\n    public T ComputeFeatureLoss(\n        Func<string, Vector<T>> teacherFeatureExtractor,\n        Func<string, Vector<T>> studentFeatureExtractor,\n        Vector<T> input)\n    {\n        if (teacherFeatureExtractor == null) throw new ArgumentNullException(nameof(teacherFeatureExtractor));\n        if (studentFeatureExtractor == null) throw new ArgumentNullException(nameof(studentFeatureExtractor));\n        if (input == null) throw new ArgumentNullException(nameof(input));\n\n        T totalLoss = _numOps.Zero;\n\n        // Parse layer pairs into structured format using explicit Select\n        var parsedPairs = _layerPairs.Select(pair =>\n        {\n            var parts = pair.Split(':');\n            return (TeacherLayer: parts[0].Trim(), StudentLayer: parts[1].Trim());\n        });\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 5402416 to ee5e081","path":"src/KnowledgeDistillation/FeatureDistillationStrategy.cs","position":1,"line":null,"createdAt":"2025-11-11T07:10:58Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hcSep","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86Vyrgg","databaseId":2513090592,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Fix nullability mismatch to satisfy interface contract.**\n\nThe parameter `trueLabels` is declared as non-nullable `TOutput[]` with a null default, but `IKnowledgeDistillationTrainer<TInput, TOutput, T>.TrainBatch` expects `TOutput[]?`. This causes compilation error CS8767.\n\n\n\nApply this diff to fix the nullability:\n\n```diff\n     public virtual T TrainBatch(\n         Func<TInput, TOutput> studentForward,\n         Action<TOutput> studentBackward,\n         TInput[] inputs,\n-        TOutput[] trueLabels = null)\n+        TOutput[]? trueLabels = null)\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    public virtual T TrainBatch(\n        Func<TInput, TOutput> studentForward,\n        Action<TOutput> studentBackward,\n        TInput[] inputs,\n        TOutput[]? trueLabels = null)\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒº░ Tools</summary>\n\n<details>\n<summary>≡ƒ¬¢ GitHub Actions: Build</summary>\n\n[error] 97-97: CS8767: Nullability of reference types in type of parameter 'trueLabels' of 'T KnowledgeDistillationTrainerBase<TInput, TOutput, T>.TrainBatch(Func<TInput, TOutput> studentForward, Action<TOutput> studentBackward, TInput[] inputs, TOutput[] trueLabels = null)' doesn't match implicitly implemented member 'T IKnowledgeDistillationTrainer<TInput, TOutput, T>.TrainBatch(Func<TInput, TOutput> studentForward, Action<TOutput> studentBackward, TInput[] inputs, TOutput[]? trueLabels = null)' (possibly because of nullability attributes).\n\n</details>\n<details>\n<summary>≡ƒ¬¢ GitHub Actions: Quality Gates (.NET)</summary>\n\n[error] 97-97: CS8767: Nullability of reference types in type of parameter 'trueLabels' of 'T KnowledgeDistillationTrainerBase<TInput, TOutput, T>.TrainBatch(Func<TInput, TOutput> studentForward, Action<TOutput> studentBackward, TInput[] inputs, TOutput[] trueLabels = null)' doesn't match implicitly implemented member 'T IKnowledgeDistillationTrainer<TInput, TOutput, T>.TrainBatch(Func<TInput, TOutput> studentForward, Action<TOutput> studentBackward, TInput[] inputs, TOutput[]? trueLabels = null)' (possibly because of nullability attributes).\n\n</details>\n<details>\n<summary>≡ƒ¬¢ GitHub Check: Build All Frameworks</summary>\n\n[failure] 101-101: \nCannot convert null literal to non-nullable reference type.\n\n---\n\n[failure] 97-97: \nNullability of reference types in type of parameter 'trueLabels' of 'T KnowledgeDistillationTrainerBase<TInput, TOutput, T>.TrainBatch(Func<TInput, TOutput> studentForward, Action<TOutput> studentBackward, TInput[] inputs, TOutput[] trueLabels = null)' doesn't match implicitly implemented member 'T IKnowledgeDistillationTrainer<TInput, TOutput, T>.TrainBatch(Func<TInput, TOutput> studentForward, Action<TOutput> studentBackward, TInput[] inputs, TOutput[]? trueLabels = null)' (possibly because of nullability attributes).\n\n</details>\n<details>\n<summary>≡ƒ¬¢ GitHub Check: Publish Size Analysis</summary>\n\n[failure] 101-101: \nCannot convert null literal to non-nullable reference type.\n\n---\n\n[failure] 97-97: \nNullability of reference types in type of parameter 'trueLabels' of 'T KnowledgeDistillationTrainerBase<TInput, TOutput, T>.TrainBatch(Func<TInput, TOutput> studentForward, Action<TOutput> studentBackward, TInput[] inputs, TOutput[] trueLabels = null)' doesn't match implicitly implemented member 'T IKnowledgeDistillationTrainer<TInput, TOutput, T>.TrainBatch(Func<TInput, TOutput> studentForward, Action<TOutput> studentBackward, TInput[] inputs, TOutput[]? trueLabels = null)' (possibly because of nullability attributes).\n\n</details>\n\n</details>\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/KnowledgeDistillationTrainerBase.cs around lines 97\nto 101, the TrainBatch signature declares the parameter trueLabels as\nnon-nullable TOutput[] with a null default causing a nullability mismatch with\nIKnowledgeDistillationTrainer<TInput, TOutput, T>.TrainBatch which expects\nTOutput[]?; change the parameter type to TOutput[]? (retain the = null default)\nso the signature matches the interface and resolves CS8767, and ensure any\nusages inside the method handle the nullable array (null checks) as needed.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 5402416 to bfb3263","path":"src/KnowledgeDistillation/KnowledgeDistillationTrainerBase.cs","position":1,"line":null,"createdAt":"2025-11-11T07:10:58Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hcSe4","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86Vyrg6","databaseId":2513090618,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Include the positive pair in the NT-Xent denominator**\n\nThe current denominator skips the positive pair, so the loss becomes `-s_pos + log ╬ú_neg`, which is not NT-Xent and produces over-penalized gradients. Add the positive exponential back into the normalizer.\n\n```diff\n-            double positiveSim = CosineSimilarity(studentEmbs[i], teacherEmbs[i]) / Temperature;\n-            double denominator = 0;\n+            double positiveLogit = CosineSimilarity(studentEmbs[i], teacherEmbs[i]) / Temperature;\n+            double expPositive = Math.Exp(positiveLogit);\n+            double denominator = expPositive;\n             for (int j = 0; j < batchSize; j++)\n             {\n                 if (i != j)\n                 {\n                     double sim = CosineSimilarity(studentEmbs[i], teacherEmbs[j]) / Temperature;\n                     denominator += Math.Exp(sim);\n                 }\n             }\n-\n-            double loss = -positiveSim + Math.Log(denominator + Epsilon);\n+            double loss = -Math.Log(expPositive / (denominator + Epsilon));\n             totalLoss = NumOps.Add(totalLoss, NumOps.FromDouble(loss));\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n            // Positive: corresponding teacher embedding\n            double positiveLogit = CosineSimilarity(studentEmbs[i], teacherEmbs[i]) / Temperature;\n            double expPositive = Math.Exp(positiveLogit);\n            double denominator = expPositive;\n            for (int j = 0; j < batchSize; j++)\n            {\n                if (i != j)\n                {\n                    double sim = CosineSimilarity(studentEmbs[i], teacherEmbs[j]) / Temperature;\n                    denominator += Math.Exp(sim);\n                }\n            }\n\n            double loss = -Math.Log(expPositive / (denominator + Epsilon));\n            totalLoss = NumOps.Add(totalLoss, NumOps.FromDouble(loss));\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/Strategies/ContrastiveDistillationStrategy.cs\naround lines 343 to 358, the NT-Xent denominator currently omits the positive\npair which yields -s_pos + log(sum_neg) instead of log(sum_all); fix by\nincluding the positive term in the denominatorΓÇöeither loop over all j including\ni or add Math.Exp(positiveSim) to the denominator before taking the log (ensure\nyou use the same temperature-scaled positiveSim and still add Epsilon to avoid\nlog(0)).\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 5402416 to 8c8296b","path":"src/KnowledgeDistillation/Strategies/ContrastiveDistillationStrategy.cs","position":359,"line":359,"createdAt":"2025-11-11T07:10:59Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hcSfA","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VyrhF","databaseId":2513090629,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Match KL orientation with gradient**\n\nAs in the contrastive strategy, the scalar uses KL(studentΓÇûteacher) while the gradient is for KL(teacherΓÇûstudent). Swap the arguments so the optimization target is consistent.\n\n```diff\n-        var softLoss = KLDivergence(studentSoft, teacherSoft);\n+        var softLoss = KLDivergence(teacherSoft, studentSoft);\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        var studentSoft = Softmax(studentOutput, Temperature);\n        var teacherSoft = Softmax(teacherOutput, Temperature);\n        var softLoss = KLDivergence(teacherSoft, studentSoft);\n        softLoss = NumOps.Multiply(softLoss, NumOps.FromDouble(Temperature * Temperature));\n\n        if (trueLabels != null)\n        {\n            ValidateLabelDimensions(studentOutput, trueLabels, v => v.Length);\n            var studentProbs = Softmax(studentOutput, 1.0);\n            var hardLoss = CrossEntropy(studentProbs, trueLabels);\n            return NumOps.Add(\n                NumOps.Multiply(NumOps.FromDouble(Alpha), hardLoss),\n                NumOps.Multiply(NumOps.FromDouble(1.0 - Alpha), softLoss));\n        }\n\n        return softLoss;\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/Strategies/SimilarityPreservingStrategy.cs around\nlines 27 to 43, the KL divergence is computed as KLDivergence(studentSoft,\nteacherSoft) which produces KL(studentΓÇûteacher) while the gradient logic expects\nKL(teacherΓÇûstudent); swap the arguments to KLDivergence(teacherSoft,\nstudentSoft) so the optimization target and gradient orientation match, keeping\nthe temperature scaling and subsequent weighting of soft and hard losses\nunchanged.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits c962fd4 to d0f53e7","path":"src/KnowledgeDistillation/Strategies/SimilarityPreservingStrategy.cs","position":43,"line":43,"createdAt":"2025-11-11T07:10:59Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hcSfE","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VyrhM","databaseId":2513090636,"body":"_≡ƒ¢á∩╕Å Refactor suggestion_ | _≡ƒƒá Major_\n\n**Unused public API - clarify integration or usage.**\n\n`ComputeVariationalLoss` is never called by `ComputeLoss` or `ComputeGradient`, making it a disconnected public API. Users would need to:\n1. Manually extract or encode latent representations (mean, logVar) from their models\n2. Call this method separately\n3. Combine the result with the classical loss themselves\n\nThis fragmentation is confusing and error-prone.\n\n\n\n**Recommendation:**\n\nEither integrate this method into the main loss computation (see comments on lines 63-85), or document that this is an **advanced/manual API** for users who have separate encoder models producing Gaussian latent representations, with a clear example showing:\n- How to obtain `studentMean`, `studentLogVar`, `teacherMean`, `teacherLogVar`\n- How to combine the result with classical distillation loss\n- When to use this vs. the standard `ComputeLoss` method\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/Strategies/VariationalDistillationStrategy.cs\naround lines 132 to 150, ComputeVariationalLoss is a public API that is never\nused by ComputeLoss or ComputeGradient, producing a confusing/disconnected\nsurface; either (A) integrate it into the main loss path by invoking\nComputeVariationalLoss from ComputeLoss and/or ComputeGradient (extract latent\nmean/logVar from the student/teacher encoders or accept them from existing\ninternal latent tensors, then add the returned value scaled by\n_variationalWeight to the final loss) or (B) keep it as an advanced/manual API\nbut mark it clearly with XML documentation and examples showing how to obtain\nstudentMean/studentLogVar/teacherMean/teacherLogVar from encoder outputs, how to\ncall this method, and how to combine its return with the classical distillation\nloss; implement one of these fixes and remove the unused public exposure if you\nchoose integration (make method private/internal) or keep it public only with\nthe added documentation example.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits e713a2b to 48e902f","path":"src/KnowledgeDistillation/Strategies/VariationalDistillationStrategy.cs","position":184,"line":184,"createdAt":"2025-11-11T07:10:59Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hcSfJ","isResolved":false,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VyrhR","databaseId":2513090641,"body":"_≡ƒ¢á∩╕Å Refactor suggestion_ | _≡ƒƒá Major_\n\n**Provide a default Vector\\<T\\> softmax implementation to reduce duplication.**\n\n`ApplyTemperatureSoftmax` is declared abstract, forcing every subclass to implement temperature-scaled softmax. As a result, `CurriculumTeacherModel`, `QuantizedTeacherModel`, `TransformerTeacherModel`, `OnlineTeacherModel`, and strategy classes all duplicate the identical 30-line softmax logic. Since all current implementations use `Vector<T>` for `TOutput`, consider making this method virtual with a concrete default implementation.\n\n\n\nChange the abstract method to a virtual protected helper:\n\n```diff\n-    protected abstract TOutput ApplyTemperatureSoftmax(TOutput logits, double temperature);\n+    protected virtual TOutput ApplyTemperatureSoftmax(TOutput logits, double temperature)\n+    {\n+        // Default implementation for Vector<T>\n+        if (logits is not Vector<T> vectorLogits)\n+            throw new NotSupportedException(\n+                $\"Default softmax implementation only supports Vector<T>. Override ApplyTemperatureSoftmax for {typeof(TOutput).Name}.\");\n+\n+        return (TOutput)(object)SoftmaxVector(vectorLogits, temperature);\n+    }\n+\n+    private Vector<T> SoftmaxVector(Vector<T> logits, double temperature)\n+    {\n+        int n = logits.Length;\n+        var result = new Vector<T>(n);\n+        var scaled = new T[n];\n+\n+        for (int i = 0; i < n; i++)\n+            scaled[i] = NumOps.FromDouble(Convert.ToDouble(logits[i]) / temperature);\n+\n+        T maxLogit = scaled[0];\n+        for (int i = 1; i < n; i++)\n+            if (NumOps.GreaterThan(scaled[i], maxLogit))\n+                maxLogit = scaled[i];\n+\n+        T sum = NumOps.Zero;\n+        var expValues = new T[n];\n+\n+        for (int i = 0; i < n; i++)\n+        {\n+            double val = Convert.ToDouble(NumOps.Subtract(scaled[i], maxLogit));\n+            expValues[i] = NumOps.FromDouble(Math.Exp(val));\n+            sum = NumOps.Add(sum, expValues[i]);\n+        }\n+\n+        for (int i = 0; i < n; i++)\n+            result[i] = NumOps.Divide(expValues[i], sum);\n+\n+        return result;\n+    }\n```\n\nThen remove the duplicated softmax methods from all subclasses.\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    /// <summary>\n    /// Applies temperature-scaled softmax to logits. Must be implemented by subclasses\n    /// based on their output type (Vector, Matrix, etc.).\n    /// </summary>\n    /// <param name=\"logits\">Raw model outputs.</param>\n    /// <param name=\"temperature\">Temperature for scaling.</param>\n    /// <returns>Probability distribution.</returns>\n    protected virtual TOutput ApplyTemperatureSoftmax(TOutput logits, double temperature)\n    {\n        // Default implementation for Vector<T>\n        if (logits is not Vector<T> vectorLogits)\n            throw new NotSupportedException(\n                $\"Default softmax implementation only supports Vector<T>. Override ApplyTemperatureSoftmax for {typeof(TOutput).Name}.\");\n\n        return (TOutput)(object)SoftmaxVector(vectorLogits, temperature);\n    }\n\n    private Vector<T> SoftmaxVector(Vector<T> logits, double temperature)\n    {\n        int n = logits.Length;\n        var result = new Vector<T>(n);\n        var scaled = new T[n];\n\n        for (int i = 0; i < n; i++)\n            scaled[i] = NumOps.FromDouble(Convert.ToDouble(logits[i]) / temperature);\n\n        T maxLogit = scaled[0];\n        for (int i = 1; i < n; i++)\n            if (NumOps.GreaterThan(scaled[i], maxLogit))\n                maxLogit = scaled[i];\n\n        T sum = NumOps.Zero;\n        var expValues = new T[n];\n\n        for (int i = 0; i < n; i++)\n        {\n            double val = Convert.ToDouble(NumOps.Subtract(scaled[i], maxLogit));\n            expValues[i] = NumOps.FromDouble(Math.Exp(val));\n            sum = NumOps.Add(sum, expValues[i]);\n        }\n\n        for (int i = 0; i < n; i++)\n            result[i] = NumOps.Divide(expValues[i], sum);\n\n        return result;\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/TeacherModelBase.cs around lines 110 to 117, change\nthe abstract ApplyTemperatureSoftmax signature to a protected virtual method and\nprovide a default implementation that handles the common Vector<T> case: cast\nlogits to Vector<T>, convert to doubles, divide by temperature, apply\nnumerically-stable softmax (subtract max, exp, normalize), convert results back\nto T and wrap in the same Vector<T> return type; if logits are not a Vector<T>\nthrow/return a clear NotSupported/invalid-operation to preserve behavior. After\nadding this default, remove the duplicated softmax implementations from\nCurriculumTeacherModel, QuantizedTeacherModel, TransformerTeacherModel,\nOnlineTeacherModel and strategy classes so they inherit the base implementation.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","path":"src/KnowledgeDistillation/TeacherModelBase.cs","position":1,"line":null,"createdAt":"2025-11-11T07:10:59Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hcSfP","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VyrhY","databaseId":2513090648,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Fix compilation errors and remove unreliable dimension inference.**\n\nThis method has multiple critical issues:\n\n1. **Line 121**: `model.GetMetadata()` doesn't exist on `IFullModel<T, Vector<T>, Vector<T>>` interface (CS error).\n2. **Line 124**: Variable `outputDim` is used in the condition before being assigned, causing a compilation error.\n3. **Lines 134-142**: The dummy prediction with a length-1 vector will fail for models expecting their true input dimensionality, then silently defaults to 10, which will corrupt training if incorrect.\n\n\n\nRemove the unreliable inference logic and require an explicit `outputDimension` constructor parameter:\n\n```diff\n     public TeacherModelWrapper(\n         IFullModel<T, Vector<T>, Vector<T>> model,\n+        int outputDimension,\n         Func<Vector<T>, string, object?>? featureExtractor = null,\n         Func<Vector<T>, string, object?>? attentionExtractor = null)\n         : this(\n             forwardFunc: input => model.Predict(input),\n-            outputDimension: GetOutputDimensionFromModel(model),\n+            outputDimension: outputDimension,\n             featureExtractor: featureExtractor,\n             attentionExtractor: attentionExtractor)\n     {\n     }\n-\n-    private static int GetOutputDimensionFromModel(IFullModel<T, Vector<T>, Vector<T>> model)\n-    {\n-        // Try to infer output dimension from model metadata\n-        try\n-        {\n-            var metadata = model.GetMetadata();\n-\n-            // Check if metadata contains output dimension/class count\n-            if (metadata.TryGetValue(\"OutputDimension\", out var outputDimValue) && outputDimValue is int outputDim && outputDim > 0)\n-                return outputDim;\n-\n-            if (metadata.TryGetValue(\"NumClasses\", out var numClassesValue) && numClassesValue is int numClasses && numClasses > 0)\n-                return numClasses;\n-\n-            if (metadata.TryGetValue(\"ClassCount\", out var classCountValue) && classCountValue is int classCount && classCount > 0)\n-                return classCount;\n-\n-            // If metadata doesn't contain dimension info, try inferring from a dummy prediction\n-            // Create a minimal dummy input vector (size 1) to get output shape\n-            var dummyInput = new Vector<T>(1);\n-            var dummyOutput = model.Predict(dummyInput);\n-\n-            if (dummyOutput != null && dummyOutput.Length > 0)\n-                return dummyOutput.Length;\n-\n-            // Ultimate fallback if all else fails\n-            return 10; // Common default for classification (e.g., CIFAR-10)\n-        }\n-        catch\n-        {\n-            // If any error occurs during inference, use reasonable default\n-            return 10; // Common default for classification tasks\n-        }\n-    }\n```\n\nUpdate the factory method in `TeacherModelFactory.cs` to pass the required parameter:\n\n```csharp\nprivate static ITeacherModel<Vector<T>, Vector<T>> CreateNeuralNetworkTeacher(\n    IFullModel<T, Vector<T>, Vector<T>>? model,\n    int? outputDimension)\n{\n    if (model == null)\n        throw new ArgumentException(\"Model is required for NeuralNetwork teacher type\");\n    \n    if (outputDimension == null)\n        throw new ArgumentException(\"Output dimension is required for NeuralNetwork teacher type\");\n\n    return new TeacherModelWrapper<T>(model, outputDimension.Value);\n}\n```\n\n<details>\n<summary>≡ƒº░ Tools</summary>\n\n<details>\n<summary>≡ƒ¬¢ GitHub Check: Build All Frameworks</summary>\n\n[failure] 124-124: \nUse of unassigned local variable 'outputDim'\n\n---\n\n[failure] 121-121: \n'IFullModel<T, Vector<T>, Vector<T>>' does not contain a definition for 'GetMetadata' and no accessible extension method 'GetMetadata' accepting a first argument of type 'IFullModel<T, Vector<T>, Vector<T>>' could be found (are you missing a using directive or an assembly reference?)\n\n</details>\n<details>\n<summary>≡ƒ¬¢ GitHub Check: Publish Size Analysis</summary>\n\n[failure] 124-124: \nUse of unassigned local variable 'outputDim'\n\n---\n\n[failure] 121-121: \n'IFullModel<T, Vector<T>, Vector<T>>' does not contain a definition for 'GetMetadata' and no accessible extension method 'GetMetadata' accepting a first argument of type 'IFullModel<T, Vector<T>, Vector<T>>' could be found (are you missing a using directive or an assembly reference?)\n\n</details>\n\n</details>\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/TeacherModelWrapper.cs lines 116-149: remove the\nunreliable metadata/dummy-prediction inference (calls to model.GetMetadata(),\nthe uninitialized outputDim variable, and the length-1 dummy input) and instead\nrequire an explicit outputDimension passed into the TeacherModelWrapper\nconstructor; update the class to store that outputDimension and return it\ndirectly from GetOutputDimensionFromModel (or remove the method and replace\ncallers to use the stored value), and ensure any exceptions are not silently\nswallowed but surface as argument validation errors; also update the factory in\nTeacherModelFactory.cs to validate and pass the non-null outputDimension into\nthe TeacherModelWrapper (throw ArgumentException when outputDimension is null)\nas described in the review.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits cdea7d7 to 7a5d0dd","path":"src/KnowledgeDistillation/TeacherModelWrapper.cs","position":135,"line":135,"createdAt":"2025-11-11T07:10:59Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hcSfV","isResolved":false,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86Vyrhe","databaseId":2513090654,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Consider thread safety for StudentPerformance dictionary.**\n\nThe `StudentPerformance` dictionary is publicly mutable and modified in `UpdateStudentPerformance`. In multi-threaded training scenarios (parallel batch processing), concurrent access could cause race conditions.\n\n\n\nConsider using `ConcurrentDictionary<int, double>` if parallel training is anticipated:\n\n```diff\n-    public Dictionary<int, double> StudentPerformance { get; set; } = new();\n+    public System.Collections.Concurrent.ConcurrentDictionary<int, double> StudentPerformance { get; set; } = new();\n```\n\nAnd update the EMA logic to use `AddOrUpdate`:\n\n```diff\n-        // Exponential moving average\n-        if (StudentPerformance.ContainsKey(sampleIndex))\n-        {\n-            StudentPerformance[sampleIndex] =\n-                _adaptationRate * performance + (1 - _adaptationRate) * StudentPerformance[sampleIndex];\n-        }\n-        else\n-        {\n-            StudentPerformance[sampleIndex] = performance;\n-        }\n+        // Exponential moving average\n+        StudentPerformance.AddOrUpdate(\n+            sampleIndex,\n+            performance,\n+            (key, oldValue) => _adaptationRate * performance + (1 - _adaptationRate) * oldValue);\n```\n\n\nAlso applies to: 165-173\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/Teachers/AdaptiveTeacherModel.cs around lines 58\nand 165-173, the publicly mutable StudentPerformance Dictionary is not\nthread-safe; replace it with a ConcurrentDictionary<int,double> (update the\nproperty type and initializer) and change UpdateStudentPerformance to use\nConcurrentDictionary.AddOrUpdate with a valueFactory and updateFactory that\nimplements the EMA update (new = alpha * observed + (1 - alpha) * existing) so\nupdates are atomic and race-free; ensure any reads use the\nConcurrentDictionary's TryGetValue or indexer as appropriate.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","path":"src/KnowledgeDistillation/Teachers/AdaptiveTeacherModel.cs","position":1,"line":null,"createdAt":"2025-11-11T07:11:00Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hcSfb","isResolved":false,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86Vyrhm","databaseId":2513090662,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Silent failure when trueLabel is missing for AccuracyBased strategy.**\n\nWhen `_strategy` is `AccuracyBased` and `trueLabel` is null, the performance update is silently skipped (performance remains 0). This could lead to confusing behavior where the adaptive mechanism doesn't work as expected.\n\n\n\nConsider warning or throwing when required data is missing:\n\n```diff\n             case AdaptiveStrategy.AccuracyBased:\n-                if (trueLabel != null)\n-                {\n-                    // Correctness as performance measure\n-                    performance = IsCorrect(studentPrediction, trueLabel) ? 1.0 : 0.0;\n-                }\n+                if (trueLabel == null)\n+                    throw new ArgumentNullException(nameof(trueLabel), \n+                        \"True label required for AccuracyBased strategy\");\n+                \n+                // Correctness as performance measure\n+                performance = IsCorrect(studentPrediction, trueLabel) ? 1.0 : 0.0;\n                 break;\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n            case AdaptiveStrategy.AccuracyBased:\n                if (trueLabel == null)\n                    throw new ArgumentNullException(nameof(trueLabel), \n                        \"True label required for AccuracyBased strategy\");\n                \n                // Correctness as performance measure\n                performance = IsCorrect(studentPrediction, trueLabel) ? 1.0 : 0.0;\n                break;\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/Teachers/AdaptiveTeacherModel.cs around lines 150\nto 156, the AccuracyBased branch silently skips updating performance when\ntrueLabel is null; change this to explicitly handle the missing label by either\nlogging a warning (with context: strategy name, studentPrediction) or throwing\nan ArgumentNullException so callers know required data is missing; implement the\nchosen approach (prefer a warning to avoid breaking runtime) and ensure\nperformance is set to a defined fallback only after logging/throwing.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","path":"src/KnowledgeDistillation/Teachers/AdaptiveTeacherModel.cs","position":1,"line":null,"createdAt":"2025-11-11T07:11:00Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hcSfj","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86Vyrhw","databaseId":2513090672,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Final temperature can exceed configured bounds when baseTemperature Γëá 1.0.**\n\nThe code computes `adaptiveTemp` in `[_minTemperature, _maxTemperature]` (line 204), then multiplies by `baseTemperature` (line 206). If `baseTemperature = 2.0` and `_maxTemperature = 5.0`, the returned temperature becomes `10.0`, exceeding the documented maximum of 5.0.\n\nThis contradicts the constructor documentation: \"Maximum temperature for easy samples (default: 5.0)\" implies an absolute bound. Either:\n1. The implementation should not multiply by `baseTemperature` (treat min/max as absolute bounds), or\n2. The documentation should clarify that min/max are relative to the base temperature.\n\n\n\n**Option 1 (recommended):** Remove the base temperature scaling to respect absolute bounds:\n\n```diff\n-        return adaptiveTemp * baseTemperature;\n+        return adaptiveTemp;\n```\n\n**Option 2:** Update documentation to clarify the scaling behavior:\n\n```diff\n     /// <param name=\"minTemperature\">Minimum temperature for hard samples (default: 1.0).</param>\n-    /// <param name=\"maxTemperature\">Maximum temperature for easy samples (default: 5.0).</param>\n+    /// <param name=\"maxTemperature\">Maximum temperature multiplier for easy samples (default: 5.0). Final temperature is multiplied by the base temperature passed to GetSoftPredictions.</param>\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        return adaptiveTemp;\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/Teachers/AdaptiveTeacherModel.cs around line 206,\nthe method multiplies the computed adaptiveTemp (already clamped to\n_minTemperature/_maxTemperature) by baseTemperature which allows the final\ntemperature to exceed the documented absolute bounds; remove the multiplication\nby baseTemperature so the method returns the clamped adaptiveTemp directly\n(treat _minTemperature/_maxTemperature as absolute bounds), and ensure any\ncallers that expect scaling are adjusted or document the change.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits cdea7d7 to 7a5d0dd","path":"src/KnowledgeDistillation/Teachers/AdaptiveTeacherModel.cs","position":1,"line":null,"createdAt":"2025-11-11T07:11:00Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hcSfs","isResolved":false,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86Vyrh5","databaseId":2513090681,"body":"_≡ƒ¢á∩╕Å Refactor suggestion_ | _≡ƒƒá Major_\n\n**Extract duplicated Softmax to a shared helper.**\n\nThe `Softmax(Vector<T> logits, double temperature)` implementation at lines 39-67 is duplicated across `CurriculumTeacherModel`, `QuantizedTeacherModel`, `TransformerTeacherModel`, `OnlineTeacherModel`, and `ProbabilisticDistillationStrategy`. This violates DRY and means bug fixes (e.g., numerical stability improvements) must be applied in five places.\n\n\n\nMove the softmax logic to `TeacherModelBase<TInput, TOutput, T>` as a protected helper or to a static utility class:\n\n```diff\n+// In TeacherModelBase.cs or a new MathUtilities.cs\n+protected Vector<T> Softmax(Vector<T> logits, double temperature)\n+{\n+    int n = logits.Length;\n+    var result = new Vector<T>(n);\n+    var scaled = new T[n];\n+\n+    for (int i = 0; i < n; i++)\n+        scaled[i] = NumOps.FromDouble(Convert.ToDouble(logits[i]) / temperature);\n+\n+    T maxLogit = scaled[0];\n+    for (int i = 1; i < n; i++)\n+        if (NumOps.GreaterThan(scaled[i], maxLogit))\n+            maxLogit = scaled[i];\n+\n+    T sum = NumOps.Zero;\n+    var expValues = new T[n];\n+\n+    for (int i = 0; i < n; i++)\n+    {\n+        double val = Convert.ToDouble(NumOps.Subtract(scaled[i], maxLogit));\n+        expValues[i] = NumOps.FromDouble(Math.Exp(val));\n+        sum = NumOps.Add(sum, expValues[i]);\n+    }\n+\n+    for (int i = 0; i < n; i++)\n+        result[i] = NumOps.Divide(expValues[i], sum);\n+\n+    return result;\n+}\n```\n\nThen replace all five copies with calls to the shared implementation.\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<!-- This is an auto-generated comment by CodeRabbit -->","path":"src/KnowledgeDistillation/Teachers/CurriculumTeacherModel.cs","position":1,"line":null,"createdAt":"2025-11-11T07:11:00Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hcSf5","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VyriK","databaseId":2513090698,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Validate cached predictions before storing**\n\n`CachePredictions` accepts `null`, zero-length arrays, or vectors whose length differs from `_outputDim`. Later consumers rely on `OutputDimension` to reflect each cached vector, so a bad cache silently violates that contract and breaks consumers at use time. Guard the setter so only well-formed predictions are stored.\n\n```diff\n     public void CachePredictions(Vector<T>[] predictions)\n     {\n-        _cachedPredictions = predictions;\n+        if (predictions is null)\n+            throw new ArgumentNullException(nameof(predictions));\n+        if (predictions.Length == 0)\n+            throw new ArgumentException(\"At least one prediction must be provided.\", nameof(predictions));\n+\n+        for (int i = 0; i < predictions.Length; i++)\n+        {\n+            if (predictions[i] is null)\n+                throw new ArgumentException($\"Prediction at index {i} is null.\", nameof(predictions));\n+            if (predictions[i].Length != _outputDim)\n+                throw new ArgumentException($\"Prediction at index {i} has dimension {predictions[i].Length}, expected {_outputDim}.\", nameof(predictions));\n+        }\n+\n+        _cachedPredictions = predictions;\n     }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    public void CachePredictions(Vector<T>[] predictions)\n    {\n        if (predictions is null)\n            throw new ArgumentNullException(nameof(predictions));\n        if (predictions.Length == 0)\n            throw new ArgumentException(\"At least one prediction must be provided.\", nameof(predictions));\n\n        for (int i = 0; i < predictions.Length; i++)\n        {\n            if (predictions[i] is null)\n                throw new ArgumentException($\"Prediction at index {i} is null.\", nameof(predictions));\n            if (predictions[i].Length != _outputDim)\n                throw new ArgumentException($\"Prediction at index {i} has dimension {predictions[i].Length}, expected {_outputDim}.\", nameof(predictions));\n        }\n\n        _cachedPredictions = predictions;\n    }\n\n    public override Vector<T> GetLogits(Vector<T> input)\n    {\n        throw new InvalidOperationException(\"Self teacher uses cached predictions, not direct input\");\n    }\n\n    public Vector<T> GetCachedPrediction(int index)\n    {\n        if (_cachedPredictions == null || index >= _cachedPredictions.Length)\n            throw new InvalidOperationException(\"Predictions not cached or index out of range\");\n        return _cachedPredictions[index];\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/Teachers/SelfTeacherModel.cs around lines 21 to 36,\nCachePredictions currently accepts null, empty arrays, or vectors with\nmismatched lengths which breaks consumers that assume each cached Vector<T> has\nlength _outputDim; validate the input before assignment by checking the array is\nnot null, has Length > 0, and every Vector<T>.Length equals _outputDim, and if\nany check fails throw an ArgumentException (or ArgumentNullException) with a\nclear message; only assign _cachedPredictions when all validations pass.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","path":"src/KnowledgeDistillation/Teachers/SelfTeacherModel.cs","position":1,"line":65,"createdAt":"2025-11-11T07:11:00Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hcSgE","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VyriT","databaseId":2513090707,"body":"_≡ƒ¢á∩╕Å Refactor suggestion_ | _≡ƒƒá Major_\n\n**Validate outputDimension is positive.**\n\nThe constructor accepts `outputDimension` but doesn't validate it's positive. A zero or negative dimension would cause runtime errors in softmax and other operations. Other teacher constructors (e.g., `TeacherModelWrapper` at line 68-70 in the relevant snippets) validate `outputDimension > 0`.\n\n\n\nAdd validation:\n\n```diff\n public TransformerTeacherModel(\n     Func<Vector<T>, Vector<T>> forwardFunc,\n     int outputDimension,\n     Func<Vector<T>, string, object?>? attentionExtractor = null)\n {\n     _forwardFunc = forwardFunc ?? throw new ArgumentNullException(nameof(forwardFunc));\n+    if (outputDimension <= 0)\n+        throw new ArgumentException(\"Output dimension must be positive\", nameof(outputDimension));\n     _outputDim = outputDimension;\n     _attentionExtractor = attentionExtractor;\n }\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/Teachers/TransformerTeacherModel.cs around lines 17\nto 25, the constructor does not validate that outputDimension is positive; add a\nguard that throws an ArgumentOutOfRangeException (or ArgumentException) when\noutputDimension <= 0, include the parameter name in the exception, and only\nassign _outputDim after the validation so invalid values are rejected\nimmediately.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits f5a28ea to d83d239","path":"src/KnowledgeDistillation/Teachers/TransformerTeacherModel.cs","position":1,"line":44,"createdAt":"2025-11-11T07:11:01Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hcSgJ","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86Vyric","databaseId":2513090716,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Custom loss functions are silently ignored**\n\nLine 621 always rebuilds a fresh default loss via `NeuralNetworkHelper<T>.GetDefaultLossFunction(...)`, so any custom loss passed into the constructor (or exposed through `DefaultLossFunction`) is never used during `Train`, `TrainNetwork`, or other paths that rely on `CalculateError`. This regresses the documented behavior and breaks scenarios that rely on bespoke losses. Please reuse `_defaultLossFunction` here.\n\nApply this diff to honor the configured loss:\n\n```diff\n-        var lossFunction = NeuralNetworkHelper<T>.GetDefaultLossFunction(Architecture.TaskType);\n+        var lossFunction = _defaultLossFunction ?? NeuralNetworkHelper<T>.GetDefaultLossFunction(Architecture.TaskType);\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        // Get appropriate loss function based on the task type\n        var lossFunction = _defaultLossFunction ?? NeuralNetworkHelper<T>.GetDefaultLossFunction(Architecture.TaskType);\n    \n        // Calculate gradients based on the loss function\n        Vector<T> error = lossFunction.CalculateDerivative(predicted, expected);\n    \n        return error;\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/Models/NeuralNetworkModel.cs around lines 620 to 626, the code always\nrecreates a default loss via\nNeuralNetworkHelper<T>.GetDefaultLossFunction(Architecture.TaskType) which\nignores any custom loss stored in _defaultLossFunction; replace that call with\nusing the configured _defaultLossFunction (and only call\nNeuralNetworkHelper.GetDefaultLossFunction(...) as a null fallback) so\nCalculateError uses the injected/custom loss function instead of always\nrebuilding a default one.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 5402416 to 8e1d67a","path":"src/Models/NeuralNetworkModel.cs","position":1,"line":null,"createdAt":"2025-11-11T07:11:01Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hcSgM","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86Vyrif","databaseId":2513090719,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Preserve configured loss when cloning or re-parameterizing**\n\n`WithParameters` (Line 858) and `DeepCopy` (Line 967) rebuild new models without supplying the existing `_defaultLossFunction`, resetting them to helper defaults and discarding any caller-provided loss. ThatΓÇÖs a functional regression for consumers who set a custom loss and expect clones/variants to behave identically.\n\nApply this diff so both paths retain the configured loss:\n\n```diff\n-        var newModel = new NeuralNetworkModel<T>(Architecture);\n+        var newModel = new NeuralNetworkModel<T>(Architecture, _defaultLossFunction);\n@@\n-        var copy = new NeuralNetworkModel<T>(Architecture);\n+        var copy = new NeuralNetworkModel<T>(Architecture, _defaultLossFunction);\n```\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commit 285fc01","path":"src/Models/NeuralNetworkModel.cs","position":1,"line":975,"createdAt":"2025-11-11T07:11:01Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hcSgQ","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86Vyrin","databaseId":2513090727,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Fix compilation blockers: async signature, missing interface methods, and type mismatches.**\n\nMultiple critical issues prevent compilation:\n\n1. **Async without await (line 901)**: The method signature is `async Task<...>` but contains no `await` expressions. Remove the `async` modifier.\n\n2. **Missing interface methods (lines 1083, 1091, 1114)**: `IFullModel<T, Vector<T>, Vector<T>>` does not define `SaveState(Stream)` or `LoadState(Stream)`. Either add these methods to the interface or cast to a concrete type that supports serialization.\n\n3. **Type conversion errors**:\n   - **Line 938**: `options.Teachers` has type `ITeacherModel<TInput, TOutput>[]`, but `CreateTeacher` expects `ITeacherModel<Vector<T>, Vector<T>>[]`. The constraint at lines 918-921 already validates the model is `IFullModel<T, Vector<T>, Vector<T>>`, but the options' teacher array remains generic. Cast or constrain the options at the call site.\n   - **Line 946**: `options.TeacherForward` has type `Func<TInput, TOutput>`, but the wrapper constructor expects `Func<Vector<T>, Vector<T>>`. Cast the delegate or validate compatibility earlier.\n\n\n\nApply this diff to remove the spurious `async`:\n\n```diff\n-    private async Task<OptimizationResult<T, TInput, TOutput>> PerformKnowledgeDistillationAsync(\n+    private Task<OptimizationResult<T, TInput, TOutput>> PerformKnowledgeDistillationAsync(\n         IFullModel<T, TInput, TOutput> studentModel,\n```\n\nFor SaveState/LoadState, either add to `IFullModel<T, TInput, TOutput>`:\n\n```csharp\n// In IFullModel.cs\nvoid SaveState(Stream stream);\nvoid LoadState(Stream stream);\n```\n\nor cast to a concrete model type that supports persistence. For the type-conversion issues at lines 938 and 946, validate and cast the generic options fields:\n\n```diff\n else if (options.Teachers != null && options.Teachers.Length > 0)\n {\n     // Use ensemble of teachers\n+    if (options.Teachers is not ITeacherModel<Vector<T>, Vector<T>>[])\n+        throw new InvalidOperationException(\"Teachers must be ITeacherModel<Vector<T>, Vector<T>>[] for distillation.\");\n     teacher = KnowledgeDistillation.TeacherModelFactory<T>.CreateTeacher(\n         TeacherModelType.Ensemble,\n-        ensembleModels: options.Teachers,\n+        ensembleModels: (ITeacherModel<Vector<T>, Vector<T>>[])options.Teachers,\n         ensembleWeights: options.EnsembleWeights);\n }\n else if (options.TeacherForward != null)\n {\n     // Wrap forward function as teacher\n     int outputDim = options.OutputDimension ?? 10;\n+    if (options.TeacherForward is not Func<Vector<T>, Vector<T>>)\n+        throw new InvalidOperationException(\"TeacherForward must be Func<Vector<T>, Vector<T>> for distillation.\");\n     teacher = new KnowledgeDistillation.TeacherModelWrapper<T>(\n-        options.TeacherForward,\n+        (Func<Vector<T>, Vector<T>>)options.TeacherForward,\n         outputDim);\n }\n```\n\n<details>\n<summary>≡ƒº░ Tools</summary>\n\n<details>\n<summary>≡ƒ¬¢ GitHub Check: Build All Frameworks</summary>\n\n[failure] 901-901: \nThis async method lacks 'await' operators and will run synchronously. Consider using the 'await' operator to await non-blocking API calls, or 'await Task.Run(...)' to do CPU-bound work on a background thread.\n\n---\n\n[failure] 1114-1114: \n'IFullModel<T, Vector<T>, Vector<T>>' does not contain a definition for 'LoadState' and no accessible extension method 'LoadState' accepting a first argument of type 'IFullModel<T, Vector<T>, Vector<T>>' could be found (are you missing a using directive or an assembly reference?)\n\n---\n\n[failure] 1091-1091: \n'IFullModel<T, Vector<T>, Vector<T>>' does not contain a definition for 'SaveState' and no accessible extension method 'SaveState' accepting a first argument of type 'IFullModel<T, Vector<T>, Vector<T>>' could be found (are you missing a using directive or an assembly reference?)\n\n---\n\n[failure] 1083-1083: \n'IFullModel<T, Vector<T>, Vector<T>>' does not contain a definition for 'SaveState' and no accessible extension method 'SaveState' accepting a first argument of type 'IFullModel<T, Vector<T>, Vector<T>>' could be found (are you missing a using directive or an assembly reference?)\n\n---\n\n[failure] 946-946: \nArgument 1: cannot convert from 'System.Func<TInput, TOutput>' to 'System.Func<AiDotNet.LinearAlgebra.Vector<T>, AiDotNet.LinearAlgebra.Vector<T>>'\n\n---\n\n[failure] 938-938: \nArgument 2: cannot convert from 'AiDotNet.Interfaces.ITeacherModel<TInput, TOutput>[]' to 'AiDotNet.Interfaces.ITeacherModel<AiDotNet.LinearAlgebra.Vector<T>, AiDotNet.LinearAlgebra.Vector<T>>[]?'\n\n</details>\n<details>\n<summary>≡ƒ¬¢ GitHub Check: Publish Size Analysis</summary>\n\n[failure] 901-901: \nThis async method lacks 'await' operators and will run synchronously. Consider using the 'await' operator to await non-blocking API calls, or 'await Task.Run(...)' to do CPU-bound work on a background thread.\n\n---\n\n[failure] 1114-1114: \n'IFullModel<T, Vector<T>, Vector<T>>' does not contain a definition for 'LoadState' and no accessible extension method 'LoadState' accepting a first argument of type 'IFullModel<T, Vector<T>, Vector<T>>' could be found (are you missing a using directive or an assembly reference?)\n\n---\n\n[failure] 1091-1091: \n'IFullModel<T, Vector<T>, Vector<T>>' does not contain a definition for 'SaveState' and no accessible extension method 'SaveState' accepting a first argument of type 'IFullModel<T, Vector<T>, Vector<T>>' could be found (are you missing a using directive or an assembly reference?)\n\n---\n\n[failure] 1083-1083: \n'IFullModel<T, Vector<T>, Vector<T>>' does not contain a definition for 'SaveState' and no accessible extension method 'SaveState' accepting a first argument of type 'IFullModel<T, Vector<T>, Vector<T>>' could be found (are you missing a using directive or an assembly reference?)\n\n---\n\n[failure] 946-946: \nArgument 1: cannot convert from 'System.Func<TInput, TOutput>' to 'System.Func<AiDotNet.LinearAlgebra.Vector<T>, AiDotNet.LinearAlgebra.Vector<T>>'\n\n---\n\n[failure] 938-938: \nArgument 2: cannot convert from 'AiDotNet.Interfaces.ITeacherModel<TInput, TOutput>[]' to 'AiDotNet.Interfaces.ITeacherModel<AiDotNet.LinearAlgebra.Vector<T>, AiDotNet.LinearAlgebra.Vector<T>>[]?'\n\n</details>\n\n</details>\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/PredictionModelBuilder.cs around lines 901 to 1135: the method is marked\nasync but contains no awaits (remove the async modifier and return Task<...>\ndirectly); the code calls SaveState/LoadState on IFullModel which the interface\ndoes not define (either add void SaveState(Stream)/void LoadState(Stream) to\nIFullModel or cast the model to a concrete serializable type before calling\nthose methods); and the teacher-related types mismatch (options.Teachers is\nITeacherModel<TInput,TOutput>[] but you need\nITeacherModel<Vector<T>,Vector<T>>[] ΓÇö validate and cast the entries or change\nthe options type to the vector-specialized teacher array, and similarly ensure\noptions.TeacherForward is a Func<Vector<T>,Vector<T>> by validating/casting or\nwrapping the provided Func<TInput,TOutput> into a Vector<T> -> Vector<T> adapter\nbefore passing to TeacherModelWrapper).\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits e713a2b to 83525de","path":"src/PredictionModelBuilder.cs","position":1,"line":null,"createdAt":"2025-11-11T07:11:01Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hg5_t","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V46CV","databaseId":2514722965,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Doc: correct generic arguments in the Quick Start sample.**\n\nOptions generic order is <T, TInput, TOutput>. The sample shows them mis-ordered.\n\n\nApply:\n\n```diff\n-    var distillationOptions = new KnowledgeDistillationOptions<Vector<double>, Vector<double>, double>\n+    var distillationOptions = new KnowledgeDistillationOptions<double, Vector<double>, Vector<double>>\n```\n\nAlso add a note that the student model must use Vector<T> input/output for the current KD path (BuildAsync will throw otherwise). [Based on learnings]\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/Interfaces/IPredictionModelBuilder.cs around lines 565 to 593, the Quick\nStart sample uses the KnowledgeDistillationOptions generics in the wrong order;\nupdate any sample or XML docs so the options generic order is\nKnowledgeDistillationOptions<T, TInput, TOutput> (matching the method\nsignature). Also append a short note in the remarks that for the current KD\nimplementation the student model must use Vector<T> for both input and output\n(BuildAsync will throw otherwise), so callers know this constraint.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:chinchilla -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commit 285fc01","path":"src/Interfaces/IPredictionModelBuilder.cs","position":1,"line":611,"createdAt":"2025-11-11T15:42:47Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hg5_5","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V46Cm","databaseId":2514722982,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Default relational weights are 50├ù smaller than recommended values**\n\nThe factory uses `distanceWeight: 0.5` and `angleWeight: 0.5`, but `RelationalDistillationStrategy.cs` documentation (lines 117-118 in that file) states: \"Park et al. used 25\" for distance and \"50, typically 2├ù distance\" for angle. With weights 100├ù smaller than the published recommendation, the relational component will have negligible impact on training.\n\n\n\nUse recommended defaults:\n\n```diff\n private static IDistillationStrategy<Vector<T>, T> CreateRelationBasedStrategy(\n         double temperature,\n         double alpha)\n     {\n         return new RelationalDistillationStrategy<T>(\n-            distanceWeight: 0.5,\n-            angleWeight: 0.5,\n+            distanceWeight: 25.0,\n+            angleWeight: 50.0,\n             temperature: temperature,\n             alpha: alpha);\n     }\n```\n\nOr document why the factory intentionally uses much weaker relational supervision than the literature.\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    private static IDistillationStrategy<Vector<T>, T> CreateRelationBasedStrategy(\n        double temperature,\n        double alpha)\n    {\n        return new RelationalDistillationStrategy<T>(\n            distanceWeight: 25.0,\n            angleWeight: 50.0,\n            temperature: temperature,\n            alpha: alpha);\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/DistillationStrategyFactory.cs around lines 99 to\n108, the factory sets relational weights to distanceWeight: 0.5 and angleWeight:\n0.5, which is ~100├ù smaller than the literature-recommended values; update the\nfactory to use the recommended defaults (distanceWeight: 25, angleWeight: 50) or\ncompute angleWeight as 2 * distanceWeight, and if you intentionally keep smaller\nvalues add a clear comment explaining the deviation and rationale so the weaker\nsupervision isn't accidental.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 7d2dfa8 to 9f61865","path":"src/KnowledgeDistillation/DistillationStrategyFactory.cs","position":1,"line":null,"createdAt":"2025-11-11T15:42:47Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hg6AI","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V46C8","databaseId":2514723004,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n<details>\n<summary>≡ƒº⌐ Analysis chain</summary>\n\n**Verify state management across epochs and incomplete batches**\n\nThe batch accumulation buffers (`_batchStudentOutputs`, `_batchTeacherOutputs`, `_samplesSinceRelationalCompute`) persist as instance fields but may not be cleared between epochs or when batch sizes don't divide evenly:\n\n1. **Incomplete batches**: If the dataset size isn't a multiple of `_relationalBatchSize`, the last partial batch never triggers relational loss computation (line 168), and those samples are carried over to the next epoch.\n2. **Amortization mismatch**: Line 208 divides by `_relationalBatchSize` even if the actual batch was smaller.\n3. **Cross-epoch contamination**: Buffers aren't cleared between epochs, so the first batch of epoch N+1 might mix with the last batch of epoch N.\n\n\n\nCheck how the trainer invokes this strategy across epochs and batches:\n\n\n\nIf the trainer doesn't reset strategy state between epochs, consider adding a public `Reset()` or `BeginEpoch()` method to clear the buffers, or document that the strategy is single-use per training run.\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Search for where ComputeLoss/ComputeGradient are called and if state is reset\nrg -n -C5 -g '*.cs' 'ComputeLoss.*RelationalDistillationStrategy|new RelationalDistillationStrategy'\n```\n\nLength of output: 2351\n\n---\n\nI need to search more specifically for where `ComputeLoss` is actually called during training to understand the state management:\n\nLet me search the codebase for where `ComputeLoss` is actually called in the training loop:\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Find all files that reference ComputeLoss and trainer patterns\nrg -n -g '*.cs' --type-list\nfd -g '*.cs' | head -30 | xargs grep -l 'ComputeLoss\\|Trainer\\|training.*loop\\|epoch' 2>/dev/null | head -20\n```\n\nLength of output: 5010\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Search more specifically for trainer/training patterns and epoch handling\nrg -n -g '*.cs' -A10 'for.*epoch|foreach.*epoch|while.*epoch' \n```\n\nLength of output: 63326\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Look for where _batchStudentOutputs, _batchTeacherOutputs or state reset happens\nrg -n -g '*.cs' '_batchStudentOutputs|_batchTeacherOutputs|Clear\\(\\)|Reset\\(\\)' | grep -i 'RelationalDistillationStrategy\\|trainer\\|training'\n```\n\nLength of output: 1990\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Find all files in the KnowledgeDistillation directory to understand structure\nfd -g '*.cs' 'src/KnowledgeDistillation' | head -30\n```\n\nLength of output: 1775\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Get the complete RelationalDistillationStrategy class to find any Reset/OnEpochEnd methods\ncat src/KnowledgeDistillation/Strategies/RelationalDistillationStrategy.cs | head -300\n```\n\nLength of output: 13386\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Look at the area around line 267 to understand the second buffer reference\nsed -n '260,280p' src/KnowledgeDistillation/Strategies/RelationalDistillationStrategy.cs\n```\n\nLength of output: 768\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check how the strategy is used in KnowledgeDistillationTrainer to understand instance lifecycle\nrg -n -A20 'Train.*Func.*TInput.*TOutput' src/KnowledgeDistillation/KnowledgeDistillationTrainerBase.cs | head -80\n```\n\nLength of output: 0\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Search for the Train method implementation in KnowledgeDistillationTrainerBase\nsed -n '134,250p' src/KnowledgeDistillation/KnowledgeDistillationTrainerBase.cs\n```\n\nLength of output: 5523\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Look for strategy instantiation and how many times it's created during training\nrg -n -B5 -A30 'IDistillationStrategy.*_strategy|_strategy\\s*=' src/KnowledgeDistillation/KnowledgeDistillationTrainerBase.cs\n```\n\nLength of output: 0\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check if there's any cleanup or reset between epochs in the trainer\nrg -n -B3 -A3 'OnEpochEnd|OnEpochStart' src/KnowledgeDistillation/KnowledgeDistillationTrainerBase.cs\n```\n\nLength of output: 1420\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check if the strategy instance is stored as field or passed around\nrg -n 'private.*IDistillationStrategy|_distillationStrategy' src/KnowledgeDistillation/KnowledgeDistillationTrainer.cs\n```\n\nLength of output: 0\n\n---\n\n\n\n</details>\n\n**Add state reset mechanism between epochs or document single-use requirement**\n\nThe batch accumulation buffers (`_batchStudentOutputs`, `_batchTeacherOutputs`, `_samplesSinceRelationalCompute`) persist as instance fields and are not cleared between training epochs, causing three interconnected issues:\n\n1. **Incomplete batches leak across epochs** (line 168): When the dataset size isn't a multiple of `_relationalBatchSize`, the last partial batch never triggers relational loss computation and its outputs remain in buffers.\n\n2. **State contamination on epoch boundaries**: These leftover buffers mix with the first batch of the next epoch, corrupting gradient/loss computations.\n\n3. **Amortization divides by wrong count** (line 208): The division by `_relationalBatchSize` assumes full batches, but incomplete final batches should divide by their actual accumulated count.\n\n**Recommended fix**: Override `OnEpochEnd()` in `RelationalDistillationStrategy` to finalize any incomplete batches and clear buffers before epoch transitions, or add a public `Reset()` method invoked by trainer implementations.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/Strategies/RelationalDistillationStrategy.cs around\nlines 159-215, the instance buffers and counters used to accumulate relational\nbatches are never reset between epochs which allows partial batches to leak into\nthe next epoch and causes amortization to divide by the configured\n_relationalBatchSize instead of the actual accumulated count; fix by adding a\npublic Reset() method or overriding OnEpochEnd() that: if there are any buffered\nstudent/teacher outputs, compute a final relational loss using the actual buffer\ncount (use that count for the amortization/division), add or store that final\ncontribution, then clear _batchStudentOutputs and _batchTeacherOutputs and reset\n_samplesSinceRelationalCompute and _accumulatedRelationalLoss to initial values;\nensure trainer code calls Reset()/OnEpochEnd at epoch boundaries or document\nthat requirement.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits cdea7d7 to 9ee890f","path":"src/KnowledgeDistillation/Strategies/RelationalDistillationStrategy.cs","position":286,"line":286,"createdAt":"2025-11-11T15:42:47Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hg6AV","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V46DL","databaseId":2514723019,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Relational gradient only applied during accumulation, not after batch completes**\n\nLine 267 adds relational gradient only when `_batchStudentOutputs.Count > 1 && _batchStudentOutputs.Count <= _relationalBatchSize`. However, `ComputeLoss` clears the buffers at line 175-176 once the batch is full, so subsequent samples in the same batch won't receive relational gradients even though they contributed to the amortized relational loss.\n\nThis creates an inconsistency: relational loss is distributed across all `_relationalBatchSize` samples (lines 206-212), but relational gradient is only applied to samples processed *before* the batch completes.\n\n\n\nConsider computing and caching the relational gradient when the batch is full (similar to caching `_accumulatedRelationalLoss`), then distributing it across subsequent samples. Alternatively, document this limitation clearly.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/Strategies/RelationalDistillationStrategy.cs around\nlines 264 to 279, the relational gradient is only applied to samples processed\nbefore the relational batch fills, so later samples that contributed to the\namortized relational loss never receive their share; change the flow so that\nwhen the relational batch becomes full you compute and cache the per-sample\nrelational gradient (similar to how _accumulatedRelationalLoss is cached) and\nthen apply that cached gradient to subsequent samples: add a private field to\nstore the computed relational gradients for the whole batch, compute and\npopulate it at the point where buffers are cleared (when batch is full), then in\nthis block check for a cached relational gradient and add it into gradient[i]\nfor the current sample, decrement or clear the cache once all stored gradients\nhave been applied, and ensure the cache is cleared/reset whenever the buffers\nare reset.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits cdea7d7 to 9ee890f","path":"src/KnowledgeDistillation/Strategies/RelationalDistillationStrategy.cs","position":352,"line":352,"createdAt":"2025-11-11T15:42:48Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hg6Ah","isResolved":false,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V46De","databaseId":2514723038,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Comment inverts ΓÇ£difficultyΓÇ¥ mapping.**\n\nCode maps higher difficulty to lower temperature (correct), but comment says ΓÇ£High difficulty (0)ΓÇ¥.\n\nApply:\n\n```diff\n-        // High difficulty (0) -> min temp (sharper)\n-        // Low difficulty (1) -> max temp (softer)\n+        // High difficulty (1) -> min temp (sharper)\n+        // Low difficulty (0) -> max temp (softer)\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/Teachers/AdaptiveTeacherModel.cs around lines 233\nto 235, the inline comment inverts the difficulty mapping; update the comment to\naccurately reflect the code logic where difficulty is 1.0 for high difficulty\nand 0.0 for low difficulty (so High difficulty (1) -> min temp (sharper) and Low\ndifficulty (0) -> max temp (softer)), or reword to describe the formula\ndirectly: adaptiveTemp = min + (1 - difficulty) * (max - min) meaning higher\ndifficulty yields lower temperature.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:chinchilla -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","path":"src/KnowledgeDistillation/Teachers/AdaptiveTeacherModel.cs","position":1,"line":null,"createdAt":"2025-11-11T15:42:48Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hg6As","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V46Dr","databaseId":2514723051,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Avoid ArgumentNullException.ThrowIfNull for broad TFMs.**\n\nThis extension may be unavailable on some targets.\n\n\n```diff\n-        ArgumentNullException.ThrowIfNull(options);\n+        if (options == null) throw new ArgumentNullException(nameof(options));\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    public IPredictionModelBuilder<T, TInput, TOutput> ConfigureKnowledgeDistillation(\n        KnowledgeDistillationOptions<T, TInput, TOutput> options)\n    {\n        if (options == null) throw new ArgumentNullException(nameof(options));\n        _knowledgeDistillationOptions = options;\n        return this;\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒº░ Tools</summary>\n\n<details>\n<summary>≡ƒ¬¢ GitHub Check: Build All Frameworks</summary>\n\n[failure] 888-888: \n'ArgumentNullException' does not contain a definition for 'ThrowIfNull'\n\n</details>\n\n</details>\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/PredictionModelBuilder.cs around lines 885 to 891, replace the call to\nArgumentNullException.ThrowIfNull(options) (which may be unavailable on older\nTFMs) with an explicit null check and throw: use \"if (options == null) throw new\nArgumentNullException(nameof(options));\" then set _knowledgeDistillationOptions\n= options and return this.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:chinchilla -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits f5a28ea to 5667d5b","path":"src/PredictionModelBuilder.cs","position":117,"line":994,"createdAt":"2025-11-11T15:42:48Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hg6Ay","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V46D3","databaseId":2514723063,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Checkpointing calls methods not on IFullModel; breaks compilation.**\n\nSaveState/LoadState arenΓÇÖt in IFullModel; direct calls wonΓÇÖt compile across implementations.\n\n\nUse reflection to optionally invoke if present (non-breaking), or remove checkpointing.\n\n```diff\n-                                    using var stream = File.Create(checkpointPath);\n-                                    vectorStudentModel.SaveState(stream);\n+                                    using var stream = File.Create(checkpointPath);\n+                                    var mi = vectorStudentModel.GetType().GetMethod(\"SaveState\", new[] { typeof(Stream) });\n+                                    if (mi != null) mi.Invoke(vectorStudentModel, new object[] { stream });\n@@\n-                                using var stream = File.Create(checkpointPath);\n-                                vectorStudentModel.SaveState(stream);\n+                                using var stream = File.Create(checkpointPath);\n+                                var mi2 = vectorStudentModel.GetType().GetMethod(\"SaveState\", new[] { typeof(Stream) });\n+                                if (mi2 != null) mi2.Invoke(vectorStudentModel, new object[] { stream });\n@@\n-                        using var stream = File.OpenRead(bestCheckpoint);\n-                        vectorStudentModel.LoadState(stream);\n+                        using var stream = File.OpenRead(bestCheckpoint);\n+                        var mi3 = vectorStudentModel.GetType().GetMethod(\"LoadState\", new[] { typeof(Stream) });\n+                        if (mi3 != null) mi3.Invoke(vectorStudentModel, new object[] { stream });\n```\n\nOptionally: introduce an ICheckpointableModel interface and gate calls via `is ICheckpointableModel`. <!-- Keeps strong typing if you add the interface -->\n\n\nAlso applies to: 1106-1123\n\n<details>\n<summary>≡ƒº░ Tools</summary>\n\n<details>\n<summary>≡ƒ¬¢ GitHub Check: Build All Frameworks</summary>\n\n[failure] 1092-1092: \n'IFullModel<T, Vector<T>, Vector<T>>' does not contain a definition for 'SaveState' and no accessible extension method 'SaveState' accepting a first argument of type 'IFullModel<T, Vector<T>, Vector<T>>' could be found (are you missing a using directive or an assembly reference?)\n\n---\n\n[failure] 1084-1084: \n'IFullModel<T, Vector<T>, Vector<T>>' does not contain a definition for 'SaveState' and no accessible extension method 'SaveState' accepting a first argument of type 'IFullModel<T, Vector<T>, Vector<T>>' could be found (are you missing a using directive or an assembly reference?)\n\n</details>\n<details>\n<summary>≡ƒ¬¢ GitHub Check: Publish Size Analysis</summary>\n\n[failure] 1092-1092: \n'IFullModel<T, Vector<T>, Vector<T>>' does not contain a definition for 'SaveState' and no accessible extension method 'SaveState' accepting a first argument of type 'IFullModel<T, Vector<T>, Vector<T>>' could be found (are you missing a using directive or an assembly reference?)\n\n---\n\n[failure] 1084-1084: \n'IFullModel<T, Vector<T>, Vector<T>>' does not contain a definition for 'SaveState' and no accessible extension method 'SaveState' accepting a first argument of type 'IFullModel<T, Vector<T>, Vector<T>>' could be found (are you missing a using directive or an assembly reference?)\n\n</details>\n\n</details>\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/PredictionModelBuilder.cs around lines 1072-1100 (and similarly at\n1106-1123), the code calls SaveState on vectorStudentModel which is not a member\nof IFullModel and therefore breaks compilation; change the checkpointing to\neither (a) define an ICheckpointableModel with SaveState/LoadState and gate\ncalls via `if (vectorStudentModel is ICheckpointableModel ck) {\nck.SaveState(...) }` so only implementations that support checkpointing are\ninvoked, or (b) use reflection to detect and invoke a SaveState method if\npresent; update the catch/logging paths accordingly and ensure you only\nreference members available on the typed interface to keep builds non-breaking.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:chinchilla -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits cdea7d7 to 01674f5","path":"src/PredictionModelBuilder.cs","position":407,"line":1284,"createdAt":"2025-11-11T15:42:48Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hg6A_","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V46EJ","databaseId":2514723081,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n<details>\n<summary>≡ƒº⌐ Analysis chain</summary>\n\n**Generic mismatch: builder uses Vector<double> input but trainData is Matrix<double>.**\n\nThis is a compile-time error.\n\n\n```diff\n-            var result = await new PredictionModelBuilder<double, Vector<double>, Vector<double>>()\n+            var result = await new PredictionModelBuilder<double, Matrix<double>, Vector<double>>()\n                 .ConfigureModel(studentModel)\n                 .ConfigureKnowledgeDistillation(kdOptions)\n                 .BuildAsync(trainData, trainLabels);\n```\n\nIf you adopt Matrix/Vector generics, ensure studentModel implements IFullModel<double, Matrix<double>, Vector<double>> or provide an adapter. I can supply a minimal adapter if preferred.\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Get the context around lines 113-116 in KnowledgeDistillationExample.cs\necho \"=== Context around lines 113-116 ===\"\nif [ -f \"testconsole/Examples/KnowledgeDistillationExample.cs\" ]; then\n  sed -n '100,125p' testconsole/Examples/KnowledgeDistillationExample.cs | cat -n\nelse\n  echo \"File not found at expected path\"\n  # Try to find it\n  find . -name \"KnowledgeDistillationExample.cs\" -type f\nfi\n```\n\nLength of output: 1530\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Search for trainData declaration to identify its type\necho \"=== trainData declaration ===\"\nrg -n -B5 -A5 'trainData' testconsole/Examples/KnowledgeDistillationExample.cs | head -50\n```\n\nLength of output: 2252\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Search for PredictionModelBuilder class definition\necho \"=== PredictionModelBuilder definition ===\"\nrg -n 'class PredictionModelBuilder' -g '*.cs' | head -20\n```\n\nLength of output: 176\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Search for studentModel declaration\necho \"=== studentModel declaration ===\"\nrg -n -B3 -A3 'studentModel' testconsole/Examples/KnowledgeDistillationExample.cs | head -40\n```\n\nLength of output: 926\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Get full method context to understand the variable types\necho \"=== Full method containing lines 113-116 ===\"\nsed -n '85,130p' testconsole/Examples/KnowledgeDistillationExample.cs | cat -n\n```\n\nLength of output: 2727\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Find CreateMockStudentModel method\necho \"=== CreateMockStudentModel definition ===\"\nrg -n -A10 'CreateMockStudentModel' testconsole/Examples/KnowledgeDistillationExample.cs | grep -A10 'private static'\n```\n\nLength of output: 465\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Find BuildAsync method signature in PredictionModelBuilder\necho \"=== BuildAsync method signature ===\"\nrg -n -B2 -A5 'BuildAsync' src/PredictionModelBuilder.cs | head -40\n```\n\nLength of output: 2042\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check the IFullModel interface to understand what studentModel should implement\necho \"=== IFullModel interface ===\"\nrg -n 'interface IFullModel' -g '*.cs' -A5 | head -30\n```\n\nLength of output: 603\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Look for any overloads of BuildAsync or method that accepts Matrix\necho \"=== All BuildAsync overloads or related methods ===\"\nrg -n 'BuildAsync\\|public.*async' src/PredictionModelBuilder.cs | head -20\n```\n\nLength of output: 51\n\n---\n\n\n\n</details>\n\n**Generic mismatch: builder instantiates with Vector<double> input but trainData is Matrix<double>.**\n\nThis is a compile-time error. The `BuildAsync(TInput x, TOutput y)` method expects `x` to match the generic parameter `TInput = Vector<double>`, but `trainData` is `Matrix<double>`.\n\n```diff\n-            var result = await new PredictionModelBuilder<double, Vector<double>, Vector<double>>()\n+            var result = await new PredictionModelBuilder<double, Matrix<double>, Vector<double>>()\n                 .ConfigureModel(studentModel)\n                 .ConfigureKnowledgeDistillation(kdOptions)\n                 .BuildAsync(trainData, trainLabels);\n```\n\nHowever, `studentModel` is typed as `IFullModel<double, Vector<double>, Vector<double>>` (from `CreateMockStudentModel` at line 229), which won't match the corrected builder generics. Update `studentModel` to implement `IFullModel<double, Matrix<double>, Vector<double>>` or provide an adapter.\n\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn testconsole/Examples/KnowledgeDistillationExample.cs around lines 113 to 116,\nthere is a generic type mismatch: the PredictionModelBuilder is instantiated\nwith TInput = Vector<double> but trainData is Matrix<double>, and the existing\nstudentModel is typed as IFullModel<double, Vector<double>, Vector<double>>;\nchange the builder generics to use Matrix<double> for TInput (e.g.,\nPredictionModelBuilder<double, Matrix<double>, Vector<double>>) and update or\nrecreate studentModel so it implements IFullModel<double, Matrix<double>,\nVector<double>> (or insert a thin adapter converting Matrix<double> to\nVector<double> before passing to the builder) so the TInput types match at\ncompile time.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:chinchilla -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits e713a2b to a5ebe95","path":"testconsole/Examples/KnowledgeDistillationExample.cs","position":1,"line":null,"createdAt":"2025-11-11T15:42:48Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hg6BL","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V46EY","databaseId":2514723096,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n<details>\n<summary>≡ƒº⌐ Analysis chain</summary>\n\n**Generic mismatch: builder expects TInput=Vector<double> but you pass Matrix<double>.**\n\nThis wonΓÇÖt compile: new PredictionModelBuilder<double, Vector<double>, Vector<double>>().BuildAsync(trainX, trainY) where trainX is Matrix<double>.\n\n\nTwo viable fixesΓÇöpick one:\n\n- Vector path (keep model types): change your data pipeline to provide Vector<double> as TInput to BuildAsync, or add a MatrixΓåÆVector adapter API.\n\n- Matrix path (simpler for dataset): switch builder generics to Matrix<double>, Vector<double> and update MockModel to implement IFullModel<double, Matrix<double>, Vector<double>>.\n```diff\n- var result = await new PredictionModelBuilder<double, Vector<double>, Vector<double>>()\n+ var result = await new PredictionModelBuilder<double, Matrix<double>, Vector<double>>()\n```\nI can generate a minimal MockModel(MatrixΓåÆVector) if you choose the Matrix path. <!-- [skip heavy changes until confirmed] -->\n\n---\n\n```\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Locate the file under review\nfd -type f -name \"SimpleKnowledgeDistillationExample.cs\" | head -20\n```\n\nLength of output: 170\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Examine the full file to understand context, variable types, and full code structure\nfind . -type f -name \"SimpleKnowledgeDistillationExample.cs\" -exec cat {} \\;\n```\n\nLength of output: 4932\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Find PredictionModelBuilder class definition to verify generic parameters\nrg -n \"class PredictionModelBuilder\" -A 10 -B 2 --type cs\n```\n\nLength of output: 1118\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Find MockModel implementation to check what it implements\nrg -n \"class MockModel\" -A 10 -B 2 --type cs\n```\n\nLength of output: 6645\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Search for BuildAsync method signature to see expected type parameters\nrg -n \"BuildAsync\" -B 5 -A 5 --type cs | head -50\n```\n\nLength of output: 4243\n\n---\n\n```\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Find the BuildAsync method signature in PredictionModelBuilder\nrg -n \"BuildAsync\" -A 15 --type=cs src/PredictionModelBuilder.cs\n```\n\nLength of output: 4419\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check if there's an overload that accepts Matrix<double>\nrg -n \"BuildAsync.*Matrix\" -A 5 -B 2 --type=cs\n```\n\nLength of output: 0\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Search for any extension methods or implicit conversions between Matrix and Vector\nrg -n \"implicit.*Matrix.*Vector|implicit.*Vector.*Matrix\" --type=cs\n```\n\nLength of output: 0\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Look at the full BuildAsync signature and return type\nrg -n \"Task.*BuildAsync\" -B 3 -A 10 --type=cs src/\n```\n\nLength of output: 4367\n\n---\n\n```\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Find IFullModel interface definition\nrg -n \"interface IFullModel\" -A 10 --type=cs src/\n```\n\nLength of output: 1013\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check MockModel's Predict method and see what it takes\nrg -n \"public.*Predict\" -A 5 -B 2 testconsole/Examples/SimpleKnowledgeDistillationExample.cs\n```\n\nLength of output: 260\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Find all implementations of IFullModel to see the pattern\nrg -n \"IFullModel<\" testconsole/Examples/SimpleKnowledgeDistillationExample.cs\n```\n\nLength of output: 464\n\n---\n\n\n\n</details>\n\n**Fix generic type mismatch: builder expects TInput=Vector<double> but receives Matrix<double>.**\n\nThe code won't compile. `trainX` is `Matrix<double>` but the builder is instantiated as `PredictionModelBuilder<double, Vector<double>, Vector<double>>`, meaning `BuildAsync` expects `(Vector<double> x, Vector<double> y)`. Choose one fix:\n\n- **Vector path**: Reshape data to provide `Vector<double>` as input (inefficient for multi-dimensional data).\n- **Matrix path**: Change builder to `PredictionModelBuilder<double, Matrix<double>, Vector<double>>()` and update `MockModel` to implement `IFullModel<double, Matrix<double>, Vector<double>>` with `Predict(Matrix<double> input)`.\n\n```diff\n- var result = await new PredictionModelBuilder<double, Vector<double>, Vector<double>>()\n+ var result = await new PredictionModelBuilder<double, Matrix<double>, Vector<double>>()\n```\n\nThe Matrix path is recommended; the MockModel's `Predict` method signature must also change to accept `Matrix<double>`.\n\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn testconsole/Examples/SimpleKnowledgeDistillationExample.cs around lines 41 to\n44, the generic types are mismatched: the builder is declared with\nVector<double> input but trainX is a Matrix<double>; change the builder to\nPredictionModelBuilder<double, Matrix<double>, Vector<double>>() so BuildAsync\naccepts Matrix<double> inputs, and update MockModel to implement\nIFullModel<double, Matrix<double>, Vector<double>> with its Predict method\nsignature changed to accept a Matrix<double> input and return Vector<double>\noutputs accordingly.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:chinchilla -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits cdea7d7 to 9ee890f","path":"testconsole/Examples/SimpleKnowledgeDistillationExample.cs","position":1,"line":null,"createdAt":"2025-11-11T15:42:48Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hie7x","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V7ESH","databaseId":2515289223,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Pass the required output dimension for Pretrained teachers.**\n\n`TeacherModelFactory<T>.CreateTeacher` throws here because `TeacherModelType.Pretrained` demands both a model and an explicit `outputDimension`. Without it, the factory rejects the call and this test will fail before the assertions. Add `outputDimension: 5` (and optionally assert it) so the test exercises the happy path.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn\ntests/AiDotNet.Tests/UnitTests/KnowledgeDistillation/TeacherModelFactoryTests.cs\naround lines 81 to 83, the test calls TeacherModelFactory<double>.CreateTeacher\nwith TeacherModelType.Pretrained but omits the required outputDimension\nparameter; add outputDimension: 5 to the CreateTeacher call so the factory\naccepts the pretrained path and the test exercises the happy path (optionally\nadd an assertion that the returned teacher has OutputDimension == 5).\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits f5a28ea to 5699b1c","path":"tests/AiDotNet.Tests/UnitTests/KnowledgeDistillation/TeacherModelFactoryTests.cs","position":1,"line":null,"createdAt":"2025-11-11T18:45:06Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hie73","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V7ESN","databaseId":2515289229,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Provide `outputDimension` when creating a Transformer teacher.**\n\nTransformer teachers also require an explicit output dimension. Omitting it causes `CreateTeacher` to throw, so this test never reaches the assertions. Include the expected dimension (e.g., `outputDimension: 5`) to validate the correct factory behavior.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn\ntests/AiDotNet.Tests/UnitTests/KnowledgeDistillation/TeacherModelFactoryTests.cs\naround lines 97 to 99, the Transformer teacher is being created without the\nrequired outputDimension which causes CreateTeacher to throw; update the call to\ninclude the expected outputDimension (for example outputDimension: 5) so the\nfactory can construct the Transformer teacher and the test can proceed to its\nassertions.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits f5a28ea to 5699b1c","path":"tests/AiDotNet.Tests/UnitTests/KnowledgeDistillation/TeacherModelFactoryTests.cs","position":1,"line":null,"createdAt":"2025-11-11T18:45:06Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hie77","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V7ESU","databaseId":2515289236,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Online teachers need `outputDimension` as well.**\n\n`TeacherModelType.Online` enforces both a model and an output dimension. Without the latter this call throws and the test fails immediately. Supply the expected output size (`outputDimension: 5`) so the test covers the intended scenario.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn\ntests/AiDotNet.Tests/UnitTests/KnowledgeDistillation/TeacherModelFactoryTests.cs\naround lines 151 to 153, the CreateTeacher call for TeacherModelType.Online is\nmissing the required outputDimension parameter; update the call to pass\noutputDimension: 5 so the Online teacher receives both the model and the\nexpected output size and the test no longer throws.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits f5a28ea to 5699b1c","path":"tests/AiDotNet.Tests/UnitTests/KnowledgeDistillation/TeacherModelFactoryTests.cs","position":1,"line":null,"createdAt":"2025-11-11T18:45:06Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hjEh5","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V77XC","databaseId":2515514818,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Fix `StrategyBuilder` generics to match the factory signature**\n\n`Build()` is currently trying to hand `Vector<IDistillationStrategy<Vector<TNum>, TNum>>` to `CreateStrategy`, but the factory expects `Vector<IDistillationStrategy<T, Vector<T>>>`. The static analysis failure you saw is a direct result of this inversion, so the project will not compile.\n\nSimplify the builder so it reuses the outer `T` and keep the strategy vector typed correctly:\n\n```diff\n-    public class StrategyBuilder<TNum>\n+    public class StrategyBuilder\n     {\n         private readonly DistillationStrategyType _strategyType;\n         private double _temperature = 3.0;\n         private double _alpha = 0.3;\n         private double? _featureWeight;\n         private double? _attentionWeight;\n         private ContrastiveMode? _contrastiveMode;\n         private Vector<string>? _featureLayerPairs;\n         private Vector<string>? _attentionLayers;\n-        private Vector<IDistillationStrategy<Vector<TNum>, TNum>>? _strategies;\n+        private Vector<IDistillationStrategy<T, Vector<T>>>? _strategies;\n         private Vector<double>? _strategyWeights;\n\n-        internal StrategyBuilder(DistillationStrategyType strategyType)\n+        internal StrategyBuilder(DistillationStrategyType strategyType)\n         {\n             _strategyType = strategyType;\n         }\n...\n-        public StrategyBuilder<TNum> WithStrategies(\n-            Vector<IDistillationStrategy<Vector<TNum>, TNum>> strategies,\n+        public StrategyBuilder WithStrategies(\n+            Vector<IDistillationStrategy<T, Vector<T>>> strategies,\n             Vector<double>? weights = null)\n         {\n             _strategies = strategies;\n             _strategyWeights = weights;\n             return this;\n         }\n...\n-        public IDistillationStrategy<Vector<TNum>, TNum> Build()\n+        public IDistillationStrategy<T, Vector<T>> Build()\n         {\n             return CreateStrategy(\n                 _strategyType,\n                 _temperature,\n                 _alpha,\n                 _featureWeight,\n                 _attentionWeight,\n                 _contrastiveMode,\n                 _featureLayerPairs,\n                 _attentionLayers,\n                 _strategies,\n                 _strategyWeights);\n         }\n     }\n```\n\nRemember to update `Configure` to return `StrategyBuilder` instead of `StrategyBuilder<T>`. Once you align the generics this way, the factory builds without type errors.  \n\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>≡ƒº░ Tools</summary>\n\n<details>\n<summary>≡ƒ¬¢ GitHub Check: Publish Size Analysis</summary>\n\n[failure] 318-318: \nArgument 9: cannot convert from 'AiDotNet.LinearAlgebra.Vector<AiDotNet.Interfaces.IDistillationStrategy<AiDotNet.LinearAlgebra.Vector<TNum>, TNum>>' to 'AiDotNet.LinearAlgebra.Vector<AiDotNet.Interfaces.IDistillationStrategy<T, AiDotNet.LinearAlgebra.Vector<T>>>?'\n\n</details>\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits a0cfdab to 9f303d8","path":"src/KnowledgeDistillation/DistillationStrategyFactory.cs","position":320,"line":320,"createdAt":"2025-11-11T19:46:44Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hjEiF","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V77Xa","databaseId":2515514842,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Fix the nullable type inference causing compilation failure.**\n\nLine 112 triggers a compilation error: `'TOutput' cannot be made nullable.` The null-conditional operator on `trueLabels?[i]` attempts to infer `TOutput?`, but without proper constraints on the generic type parameter, the compiler rejects this.\n\n\n\nApply this diff to fix the nullable handling:\n\n```diff\n         for (int i = 0; i < inputs.Length; i++)\n         {\n             var input = inputs[i];\n-            var label = trueLabels?[i];\n+            TOutput label = default;\n+            bool hasLabel = false;\n+            if (trueLabels != null)\n+            {\n+                label = trueLabels[i];\n+                hasLabel = true;\n+            }\n \n             // Student forward pass\n             var studentOutput = studentForward(input);\n \n             // Get teacher predictions (may be cached or computed on-demand)\n             var teacherOutput = GetTeacherPredictions(input, i);\n \n             // Compute loss and gradient\n-            var loss = DistillationStrategy.ComputeLoss(studentOutput, teacherOutput, label);\n-            var gradient = DistillationStrategy.ComputeGradient(studentOutput, teacherOutput, label);\n+            var loss = DistillationStrategy.ComputeLoss(studentOutput, teacherOutput, hasLabel ? label : default);\n+            var gradient = DistillationStrategy.ComputeGradient(studentOutput, teacherOutput, hasLabel ? label : default);\n \n             totalLoss = NumOps.Add(totalLoss, loss);\n \n             // Student backward pass\n             studentBackward(gradient);\n         }\n```\n\nAlternatively, if you want to preserve the original intent more directly, declare `TOutput` as a nullable-enabled type parameter by adding the `?` constraint to the class declaration, though this may require broader changes across the codebase.\n\n<details>\n<summary>≡ƒº░ Tools</summary>\n\n<details>\n<summary>≡ƒ¬¢ GitHub Check: Publish Size Analysis</summary>\n\n[failure] 112-112: \n'TOutput' cannot be made nullable.\n\n</details>\n\n</details>\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/KnowledgeDistillationTrainerBase.cs around lines 97\nto 131, the expression trueLabels?[i] causes the compiler error \"'TOutput'\ncannot be made nullable.\" Replace the null-conditional indexing with an explicit\nnull check and a nullable local: declare a local TOutput? label = default; then\nif (trueLabels != null) assign label = trueLabels[i]; use that nullable label in\ndownstream calls; this avoids changing generic constraints and fixes the\nnullable inference error.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits a0cfdab to 0b531ca","path":"src/KnowledgeDistillation/KnowledgeDistillationTrainerBase.cs","position":174,"line":174,"createdAt":"2025-11-11T19:46:44Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hjEiP","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V77Xp","databaseId":2515514857,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Generation 0 teacher logits have length zero**\n\nWhen `_cachedTeacherPredictions` is null (first generation), `GetTeacherPredictions` returns a fresh `Vector<T>(Teacher.OutputDimension)`. The placeholder teacher reports `OutputDimension = 0`, so `DistillationStrategyBase.ValidateOutputDimensions` immediately throws because the student logits are non-empty. As written, self-distillation canΓÇÖt even start. Seed `_cachedTeacherPredictions` with real logits (or bypass distillation for generation 0) so the student and teacher outputs stay aligned.\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits cdea7d7 to a877ebf","path":"src/KnowledgeDistillation/SelfDistillationTrainer.cs","position":164,"line":164,"createdAt":"2025-11-11T19:46:44Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hjEiT","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V77Xx","databaseId":2515514865,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Add feature dimension validation before factor alignment.**\n\n`ComputeFactorLoss` only checks that the batch sizes match. As soon as any student feature vector differs in length from another student vector or from its teacher counterpart, loops like the ones in `ComputeMSE`/`ComputeFactorMatchingLoss` will read past the end of the shorter vector and throw `IndexOutOfRangeException`. Please add a guard that verifies every vector in `studentFeatures` and `teacherFeatures` shares the same dimension before attempting factor extraction, and throw a clear `ArgumentException` if they do not.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/Strategies/FactorTransferDistillationStrategy.cs\naround lines 163 to 212, add a validation step before any factor extraction to\nensure all vectors share the same feature dimension: verify every Vector<T> in\nstudentFeatures has the same Length and every Vector<T> in teacherFeatures has\nthe same Length and that studentFeatures[i].Length == teacherFeatures[i].Length\nfor each batch index; if any mismatch (or zero-length vector) is found throw an\nArgumentException with a clear message (e.g. \"All feature vectors must share the\nsame dimension and match between student and teacher\"); place this guard just\nafter the existing batch-size checks so\nComputeLowRankLoss/ComputeNuclearNormLoss/ComputeFactorMatchingLoss can assume\nuniform dimensions.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","path":"src/KnowledgeDistillation/Strategies/FactorTransferDistillationStrategy.cs","position":212,"line":212,"createdAt":"2025-11-11T19:46:44Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hjEig","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V77YD","databaseId":2515514883,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Check activation dimensions before computing selectivity**\n\n`ComputeSelectivityLoss` only verifies the batch size. We immediately dereference `studentActivations[0].Length` as `numNeurons` and assume that every student/teacher activation vector shares that length. If a teacher activation has a different width (or if any sample contains a vector with a different length), the later loops in `ComputeSelectivityLoss`/`ComputeSelectivityScores` will index past the end of the shorter vector and throw. Please add explicit validation that every activation vector in both arrays matches the expected neuron count before entering the loops, and surface a friendly `ArgumentException` when the dimensions are inconsistent.\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits f5a28ea to 5699b1c","path":"src/KnowledgeDistillation/Strategies/NeuronSelectivityDistillationStrategy.cs","position":208,"line":208,"createdAt":"2025-11-11T19:46:45Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hjEip","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V77YP","databaseId":2515514895,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Guard against mismatched prediction shapes in distributional loss**\n\n`ComputeDistributionalLoss` and the helper methods assume that every prediction vector has the same length as `studentPredictions[0]` and its teacher counterpart. If a single sample has a different number of classes, the loops in `ComputeMeans`, `ComputeVariances`, or `ComputeMMDLoss` index beyond the shorter vector and the method throws. Please validate that each element of `studentPredictions` and `teacherPredictions` shares the same dimensionality before proceeding, and throw an `ArgumentException` with a clear message when they do not.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/Strategies/ProbabilisticDistillationStrategy.cs\naround lines 143 to 205, the distributional-loss code assumes every Vector<T> in\nstudentPredictions and teacherPredictions has the same Length as\nstudentPredictions[0], which can cause out-of-range indexing; add a validation\nstep at the start of ComputeDistributionalLoss that iterates both arrays and\nensures every vector.Length equals studentPredictions[0].Length (and that\nteacherPredictions has same batch size), and if any mismatch is found throw an\nArgumentException with a clear message like \"All prediction vectors must have\nidentical dimensionality\"; this prevents helper methods\n(ComputeMeans/ComputeVariances/ComputeMMDLoss) from indexing beyond shorter\nvectors.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits f5a28ea to 5699b1c","path":"src/KnowledgeDistillation/Strategies/ProbabilisticDistillationStrategy.cs","position":221,"line":221,"createdAt":"2025-11-11T19:46:45Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hjEi1","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V77Yg","databaseId":2515514912,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Relational loss/gradient are applied to the wrong samples**\n\nThe batching logic still breaks RKD:\n\n- Samples 1ΓÇª(NΓêÆ1) in a batch return before `_accumulatedRelationalLoss` is computed, so they never see any relational term.  \n- When the Nth sample arrives you compute the batch loss, but you then divide it across the *next* `N` calls (including the next epochΓÇÖs data) because `_samplesSinceRelationalCompute` starts at 0 after you clear the buffers.  \n- `ComputeGradient` only adds relational gradients while `_batchStudentOutputs.Count > 1`. By the time the batch completes the buffers are cleared, so neither the Nth sample nor the following ones receive their share. Earlier samples never got theirs either because you only update the ΓÇ£currentΓÇ¥ sample.\n\nNet effect: the relational objective barely influences training, so this strategy collapses to plain response distillation.\n\nPlease restructure this to operate on the actual batchΓÇöfor example, accumulate the logits/embeddings, compute relational loss/gradients once the batch is complete, and return/apply those contributions to each sample in that batch before clearing. If the trainer APIs cannot accommodate per-sample updates, youΓÇÖll need to adapt the trainer to feed whole batches so RKD gets the correct signal.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/Strategies/RelationalDistillationStrategy.cs around\nlines 163 to 282, the relational loss/gradient is being computed and amortized\nincorrectly resulting in most samples never receiving their RKD contribution;\nfix by batching semantics: accumulate student/teacher logits for the batch, and\nwhen the batch reaches _relationalBatchSize compute both the batch relational\nloss and per-sample relational gradients for every sample in that batch, store\nthose per-sample relational gradients in a queue or dictionary keyed by sample\nindex (or a parallel buffer aligned with incoming calls), then for each\nsubsequent call to Loss/ComputeGradient return/add the stored per-sample\nrelational contribution for that exact sample and remove it from storage; ensure\nyou do not clear the accumulated logits until all stored per-sample\ncontributions have been handed out and reset _samplesSinceRelationalCompute so\nit tracks consumption, not production, and modify the ComputeGradient\nconditional to fetch and add the precomputed per-sample relational gradient\nrather than recomputing from emptied buffers.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits cdea7d7 to 9ee890f","path":"src/KnowledgeDistillation/Strategies/RelationalDistillationStrategy.cs","position":1,"line":366,"createdAt":"2025-11-11T19:46:45Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hjEjD","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V77Yt","databaseId":2515514925,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Repeated GetLogits calls inside nested loop create a critical performance bottleneck.**\n\nLine 70 calls `GetLogits(input)` for every teacher on every output dimension iteration, resulting in M├ùN calls instead of M calls. For 3 teachers with 1000-dimensional output, this makes 3,000 calls instead of 3. Since each GetLogits likely involves a forward pass, this is extremely expensive.\n\n\n\nApply this diff to cache each teacher's logits before the dimension loop:\n\n```diff\n public override Vector<T> GetLogits(Vector<T> input)\n {\n     int n = _modalityTeachers[0].OutputDimension;\n     var combined = new Vector<T>(n);\n+\n+    // Cache logits from each teacher (call once per teacher, not once per dimension)\n+    var teacherLogits = new Vector<T>[_modalityTeachers.Length];\n+    for (int i = 0; i < _modalityTeachers.Length; i++)\n+    {\n+        teacherLogits[i] = _modalityTeachers[i].GetLogits(input);\n+    }\n \n     for (int j = 0; j < n; j++)\n     {\n         T sum = NumOps.Zero;\n         for (int i = 0; i < _modalityTeachers.Length; i++)\n         {\n-            var logits = _modalityTeachers[i].GetLogits(input);\n-            var weighted = NumOps.Multiply(logits[j], NumOps.FromDouble(_modalityWeights[i]));\n+            var weighted = NumOps.Multiply(teacherLogits[i][j], NumOps.FromDouble(_modalityWeights[i]));\n             sum = NumOps.Add(sum, weighted);\n         }\n         combined[j] = sum;\n     }\n \n     return combined;\n }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    public override Vector<T> GetLogits(Vector<T> input)\n    {\n        int n = _modalityTeachers[0].OutputDimension;\n        var combined = new Vector<T>(n);\n\n        // Cache logits from each teacher (call once per teacher, not once per dimension)\n        var teacherLogits = new Vector<T>[_modalityTeachers.Length];\n        for (int i = 0; i < _modalityTeachers.Length; i++)\n        {\n            teacherLogits[i] = _modalityTeachers[i].GetLogits(input);\n        }\n\n        for (int j = 0; j < n; j++)\n        {\n            T sum = NumOps.Zero;\n            for (int i = 0; i < _modalityTeachers.Length; i++)\n            {\n                var weighted = NumOps.Multiply(teacherLogits[i][j], NumOps.FromDouble(_modalityWeights[i]));\n                sum = NumOps.Add(sum, weighted);\n            }\n            combined[j] = sum;\n        }\n\n        return combined;\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nsrc/KnowledgeDistillation/Teachers/MultiModalTeacherModel.cs around lines 60 to\n78: currently GetLogits(input) is called inside the inner loop causing M├ùN\nforward passes; change to call GetLogits once per teacher and cache the returned\nVector<T> values in an array or list before iterating over output dimensions,\nthen use cachedVectors[i][j] (and apply weights) inside the dimension loop;\nensure you allocate the cache with length _modalityTeachers.Length and handle\nany null/size mismatches or exceptions from GetLogits consistently.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits f5a28ea to 5667d5b","path":"src/KnowledgeDistillation/Teachers/MultiModalTeacherModel.cs","position":94,"line":94,"createdAt":"2025-11-11T19:46:45Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hjEjV","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V77ZC","databaseId":2515514946,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Student gradient path bypasses the optimizer**\n\n`studentBackward` calls `vectorStudentModel.Train(gradient, gradient);`, feeding the gradient as both input and target. That discards the provided optimizer (so you lose learning rate, momentum, etc.) and almost certainly violates the modelΓÇÖs expected training API. Without a proper optimizer update, the student never learns from the distillation gradients. Wire the gradients through the configured `IOptimizer` (or expose the necessary hooks) instead of invoking `Train` with bogus arguments.\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits cdea7d7 to 9ee890f","path":"src/PredictionModelBuilder.cs","position":1,"line":1189,"createdAt":"2025-11-11T19:46:46Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hjEjg","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V77ZN","databaseId":2515514957,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Wrap batches in Vector<Vector<T>> before training/evaluation**\n\n`KnowledgeDistillationTrainer.Train` and `Evaluate` expect `Vector<Vector<T>>`, but youΓÇÖre passing raw `Vector<T>[]` arrays. That causes build failures (ΓÇ£cannot convert from Vector<T>[]ΓÇ¥). Materialize `Vector<Vector<T>>` (or update the API) before calling the trainer, and do the same for validation tensors. \n\nApply this diff:\n\n```diff\n-            var trainInputs = new Vector<T>[trainMatrix.Rows];\n-            var trainLabels = new Vector<T>[trainMatrix.Rows];\n+            var trainInputArray = new Vector<T>[trainMatrix.Rows];\n+            var trainLabelArray = new Vector<T>[trainMatrix.Rows];\n...\n-                trainInputs[i] = trainMatrix.GetRow(i);\n+                trainInputArray[i] = trainMatrix.GetRow(i);\n...\n-                trainLabels[i] = oneHot;\n+                trainLabelArray[i] = oneHot;\n...\n-            Vector<T>[]? valInputs = null;\n-            Vector<T>[]? valLabels = null;\n+            Vector<Vector<T>>? valInputs = null;\n+            Vector<Vector<T>>? valLabels = null;\n...\n-                valInputs = new Vector<T>[valMatrix.Rows];\n-                valLabels = new Vector<T>[valMatrix.Rows];\n+                var valInputArray = new Vector<T>[valMatrix.Rows];\n+                var valLabelArray = new Vector<T>[valMatrix.Rows];\n...\n-                    valInputs[i] = valMatrix.GetRow(i);\n+                    valInputArray[i] = valMatrix.GetRow(i);\n...\n-                    valLabels[i] = oneHot;\n+                    valLabelArray[i] = oneHot;\n+                }\n+\n+                valInputs = new Vector<Vector<T>>(valInputArray);\n+                valLabels = new Vector<Vector<T>>(valLabelArray);\n             }\n \n+            var trainInputs = new Vector<Vector<T>>(trainInputArray);\n+            var trainLabels = new Vector<Vector<T>>(trainLabelArray);\n             trainer.Train(\n                 studentForward,\n                 studentBackward,\n                 trainInputs,\n                 trainLabels,\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>≡ƒº░ Tools</summary>\n\n<details>\n<summary>≡ƒ¬¢ GitHub Check: Build All Frameworks</summary>\n\n[failure] 1067-1067: \nArgument 3: cannot convert from 'AiDotNet.LinearAlgebra.Vector<T>[]' to 'AiDotNet.LinearAlgebra.Vector<AiDotNet.LinearAlgebra.Vector<T>>'\n\n---\n\n[failure] 1067-1067: \nArgument 2: cannot convert from 'AiDotNet.LinearAlgebra.Vector<T>[]' to 'AiDotNet.LinearAlgebra.Vector<AiDotNet.LinearAlgebra.Vector<T>>'\n\n</details>\n<details>\n<summary>≡ƒ¬¢ GitHub Check: Publish Size Analysis</summary>\n\n[failure] 1067-1067: \nArgument 3: cannot convert from 'AiDotNet.LinearAlgebra.Vector<T>[]' to 'AiDotNet.LinearAlgebra.Vector<AiDotNet.LinearAlgebra.Vector<T>>'\n\n---\n\n[failure] 1067-1067: \nArgument 2: cannot convert from 'AiDotNet.LinearAlgebra.Vector<T>[]' to 'AiDotNet.LinearAlgebra.Vector<AiDotNet.LinearAlgebra.Vector<T>>'\n\n</details>\n\n</details>\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/PredictionModelBuilder.cs around lines 1050 to 1071, the trainer.Train\nand trainer.Evaluate calls are passing Vector<T>[] where the API expects\nVector<Vector<T>>; this causes type conversion errors. Fix by materializing the\ninputs and labels into Vector<Vector<T>> before calling Train/Evaluate (and do\nthe same for validation inputs/labels), e.g. convert the arrays into the\nrequired nested Vector container (or wrap each Vector<T> into a new\nVector<Vector<T>> collection) and pass those nested collections into\ntrainer.Train and trainer.Evaluate so the types match the API.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits e713a2b to a770473","path":"src/PredictionModelBuilder.cs","position":345,"line":1222,"createdAt":"2025-11-11T19:46:46Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hkQdm","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V9nOP","databaseId":2515956623,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Document exceptions and clarify stream handling semantics.**\n\nThe method lacks exception documentation, which is important for API consumers to handle errors correctly. Consider documenting:\n- `ArgumentNullException` if `stream` is null\n- `IOException` for I/O failures\n- `InvalidOperationException` if model is in an invalid state\n\nAdditionally, clarify:\n- Whether the stream is flushed after writing\n- Expected stream position (beginning, current position)\n- Stream ownership (does caller dispose, or should method dispose?)\n\n\n\nApply this diff to add exception documentation:\n\n```diff\n     /// <param name=\"stream\">The stream to write the model state to.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when <paramref name=\"stream\"/> is null.</exception>\n+    /// <exception cref=\"IOException\">Thrown when an I/O error occurs during serialization.</exception>\n+    /// <exception cref=\"InvalidOperationException\">Thrown when the model is in an invalid state for checkpointing.</exception>\n     /// <remarks>\n```\n\nAnd clarify stream semantics in the remarks section:\n\n```diff\n     /// <para>\n+    /// <b>Stream Handling:</b> The method writes to the stream starting at its current position\n+    /// and flushes the stream before returning. The caller retains ownership and is responsible\n+    /// for disposing the stream.\n+    /// </para>\n+    /// <para>\n     /// This method serializes all the information needed to recreate the model's current state,\n```\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits f5a28ea to 5699b1c","path":"src/Interfaces/ICheckpointableModel.cs","position":70,"line":70,"createdAt":"2025-11-11T22:01:26Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hkQdr","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V9nOW","databaseId":2515956630,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Add exception documentation and consider versioning support.**\n\nSimilar to `SaveState`, this method needs exception documentation. Additionally, line 92 mentions compatibility requirements but provides no mechanism to verify or enforce them, which could lead to runtime failures with incompatible checkpoints.\n\n\n\nApply this diff to add exception documentation:\n\n```diff\n     /// <param name=\"stream\">The stream to read the model state from.</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when <paramref name=\"stream\"/> is null.</exception>\n+    /// <exception cref=\"IOException\">Thrown when an I/O error occurs during deserialization.</exception>\n+    /// <exception cref=\"InvalidDataException\">Thrown when the stream contains invalid or incompatible checkpoint data.</exception>\n     /// <remarks>\n```\n\nAdditionally, consider adding a versioning mechanism to the interface in a future iteration:\n\n```csharp\n// Future consideration:\nstring GetStateVersion();\nbool IsCompatibleWith(string version);\n```\n\nThis would allow consumers to check compatibility before loading, preventing runtime failures with incompatible checkpoints.\n\n\n\nWould you like me to draft a versioning extension to this interface that could be added in a follow-up?\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/Interfaces/ICheckpointableModel.cs around lines 63 to 95, the LoadState\nXML docs are missing exception tags and do not provide any mechanism to verify\ncompatibility of the checkpoint stream; update the XML doc comments to include\n<exception> entries (e.g., ArgumentNullException for null stream, IOException\nfor read failures, InvalidDataException or InvalidOperationException for\nincompatible or corrupted state, and NotSupportedException if the stream is not\nreadable) and briefly describe when each is thrown, and also add a short note in\nthe remarks recommending callers validate compatibility before calling\nLoadState; for future-proofing, plan a follow-up change that adds versioning\nhelpers to the interface (for example GetStateVersion and IsCompatibleWith) so\nconsumers can check compatibility prior to loading.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits f5a28ea to c2be9b8","path":"src/Interfaces/ICheckpointableModel.cs","position":120,"line":120,"createdAt":"2025-11-11T22:01:26Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hkQdw","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V9nOd","databaseId":2515956637,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**`_strategy` and `_currentDifficulty` fields are unused, breaking curriculum functionality.**\n\nThe constructor stores `_strategy` and initializes `_currentDifficulty`, and `UpdateDifficulty` modifies `_currentDifficulty`, but neither field is ever read. This means:\n- The chosen curriculum strategy (EasyToHard vs. HardToEasy) has no effect\n- Calling `UpdateDifficulty(...)` silently does nothing\n- This class is effectively a no-op pass-through wrapper\n\nEither implement the curriculum logic (e.g., adjust temperature, filter samples by difficulty, or modulate logits based on `_currentDifficulty` and `_strategy`) or remove the unused surface area to avoid misleading callers.\n\n\n\n\nDo you want me to generate a reference implementation that uses these fields to implement curriculum-based behavior, such as gradually introducing harder examples or temperature scheduling?\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/Teachers/CurriculumTeacherModel.cs around lines 11\nto 24, the fields _strategy and _currentDifficulty are set but never read, so\ncurriculum behavior is effectively disabled; either implement curriculum-aware\nbehavior or remove the unused fields. Fix option A (implement): in\nprediction/training paths call into _baseTeacher but modify example selection or\nlogits using _currentDifficulty and _strategy (e.g., apply temperature scaling\nto logits based on difficulty, filter or weight samples so easier examples are\nchosen first for EasyToHard and vice versa for HardToEasy), ensure\nUpdateDifficulty mutates _currentDifficulty and those methods read it, and add\nunit tests verifying difficulty progression affects outputs; Fix option B\n(remove): if curriculum behavior is not required, remove _strategy,\n_currentDifficulty, and related UpdateDifficulty API, simplify constructor and\npublic surface to avoid misleading callers, and update/remove related tests and\ndocs accordingly.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commit ce282b7","path":"src/KnowledgeDistillation/Teachers/CurriculumTeacherModel.cs","position":1,"line":37,"createdAt":"2025-11-11T22:01:26Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hkUWb","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V9sPa","databaseId":2515977178,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Validate `EMADecay` to prevent incorrect blending.**\n\nThe `EMADecay` property can be set to any value, but EMA blending requires it to be in the range [0, 1]. Values outside this range will produce incorrect teacher predictions. Add validation either in the setter or before use.\n\n\nApply this diff to add validation:\n\n```diff\n-    /// <summary>\n-    /// Gets or sets the EMA decay rate (default 0.99). Higher values give more weight to history.\n-    /// </summary>\n-    public double EMADecay { get; set; }\n+    /// <summary>\n+    /// Gets or sets the EMA decay rate (default 0.99). Higher values give more weight to history.\n+    /// Must be in the range [0, 1].\n+    /// </summary>\n+    public double EMADecay\n+    {\n+        get => _emaDecay;\n+        set\n+        {\n+            if (value < 0.0 || value > 1.0)\n+                throw new ArgumentOutOfRangeException(nameof(value), \"EMA decay must be between 0 and 1\");\n+            _emaDecay = value;\n+        }\n+    }\n+    private double _emaDecay = 0.99;\n```\n\nAlternatively, validate in `TrainMultipleGenerations` before the first use:\n\n```diff\n         // Store student forward function for GetTeacherPredictions to use\n         // This is needed for generation 0 where no cached predictions exist yet\n         _studentForward = modelForward;\n+\n+        if (UseEMA && (EMADecay < 0.0 || EMADecay > 1.0))\n+            throw new InvalidOperationException($\"EMA decay must be between 0 and 1, got {EMADecay}\");\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    /// <summary>\n    /// Gets or sets the EMA decay rate (default 0.99). Higher values give more weight to history.\n    /// Must be in the range [0, 1].\n    /// </summary>\n    public double EMADecay\n    {\n        get => _emaDecay;\n        set\n        {\n            if (value < 0.0 || value > 1.0)\n                throw new ArgumentOutOfRangeException(nameof(value), \"EMA decay must be between 0 and 1\");\n            _emaDecay = value;\n        }\n    }\n    private double _emaDecay = 0.99;\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/SelfDistillationTrainer.cs around lines 66 to 69,\nthe EMADecay property currently accepts any double which can break EMA blending;\nadd validation so values must be within [0, 1]. Implement the check in the\nproperty setter (or validate at start of TrainMultipleGenerations before first\nuse) and throw an ArgumentOutOfRangeException with a clear message if the value\nis outside 0..1; also ensure a sensible default (e.g., 0.99) remains documented.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits f5a28ea to d83d239","path":"src/KnowledgeDistillation/SelfDistillationTrainer.cs","position":1,"line":null,"createdAt":"2025-11-11T22:13:06Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hkUWf","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V9sPe","databaseId":2515977182,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Fix off-by-one error in generation documentation.**\n\nThe documentation states that `1 generation` means \"Train normally, then retrain with self as teacher\", but the code only performs self-distillation when `generation > 0`. With `_generations = 1`, only generation 0 runs, which trains normally without self-distillation. Self-distillation requires at least `_generations = 2`.\n\n\nApply this diff to correct the documentation:\n\n```diff\n-        /// <para><b>For Beginners:</b> Generations control how many times the model relearns from itself:\n-        /// - 1 generation: Train normally, then retrain with self as teacher\n-        /// - 2 generations: Do it twice (teacher ΓåÆ student1 ΓåÆ student2)\n-        /// - More generations: Diminishing returns, usually not worth it beyond 2-3</para>\n+        /// <para><b>For Beginners:</b> Generations control how many times the model relearns from itself:\n+        /// - 1 generation: Train normally without self-distillation (baseline)\n+        /// - 2 generations: Train normally, then retrain with self as teacher\n+        /// - 3 generations: Do it twice (teacher ΓåÆ student1 ΓåÆ student2)\n+        /// - More generations: Diminishing returns, usually not worth it beyond 3-4</para>\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    /// <para><b>For Beginners:</b> Generations control how many times the model relearns from itself:\n    /// - 1 generation: Train normally without self-distillation (baseline)\n    /// - 2 generations: Train normally, then retrain with self as teacher\n    /// - 3 generations: Do it twice (teacher ΓåÆ student1 ΓåÆ student2)\n    /// - More generations: Diminishing returns, usually not worth it beyond 3-4</para>\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/SelfDistillationTrainer.cs around lines 79 to 82,\nthe summary misstates the meaning of \"generations\" (off-by-one): with the\ncurrent logic self-distillation only runs when _generations > 1, so update the\ndocumentation to reflect that 1 generation = only initial train (no\nself-distillation), 2 generations = initial train then one self-distillation\npass (teacher ΓåÆ student1), and higher counts repeat additional self-distillation\npasses; adjust the examples and wording accordingly to remove the incorrect\nstatement that 1 generation retrains with self as teacher.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits f5a28ea to d83d239","path":"src/KnowledgeDistillation/SelfDistillationTrainer.cs","position":95,"line":95,"createdAt":"2025-11-11T22:13:07Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hkUWk","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V9sPl","databaseId":2515977189,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Add validation for modality weights array length.**\n\nSimilar to the ensemble case, when `modalityWeights` is provided, its length should be validated against `modalityTeachers.Length` to prevent runtime errors in the MultiModalTeacherModel constructor.\n\n\n\nApply this diff:\n\n```diff\n         if (modalityTeachers == null || modalityTeachers.Length == 0)\n             throw new ArgumentException(\"Modality teachers are required for MultiModal teacher type\");\n+\n+        if (modalityWeights != null && modalityWeights.Length != modalityTeachers.Length)\n+            throw new ArgumentException(\"Modality weights must match number of teachers\", nameof(modalityWeights));\n \n         return new MultiModalTeacherModel<T>(modalityTeachers, modalityWeights);\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    private static ITeacherModel<Vector<T>, Vector<T>> CreateMultiModalTeacher(\n        ITeacherModel<Vector<T>, Vector<T>>[]? modalityTeachers,\n        double[]? modalityWeights)\n    {\n        if (modalityTeachers == null || modalityTeachers.Length == 0)\n            throw new ArgumentException(\"Modality teachers are required for MultiModal teacher type\");\n\n        if (modalityWeights != null && modalityWeights.Length != modalityTeachers.Length)\n            throw new ArgumentException(\"Modality weights must match number of teachers\", nameof(modalityWeights));\n\n        return new MultiModalTeacherModel<T>(modalityTeachers, modalityWeights);\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/TeacherModelFactory.cs around lines 95 to 103, add\nvalidation that when modalityWeights is not null its Length equals\nmodalityTeachers.Length (after the existing null/empty check for\nmodalityTeachers); if the lengths differ throw an ArgumentException with a clear\nmessage like \"modalityWeights length must match modalityTeachers length\" so the\nconstructor isn't called with mismatched arrays.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits f5a28ea to d83d239","path":"src/KnowledgeDistillation/TeacherModelFactory.cs","position":121,"line":121,"createdAt":"2025-11-11T22:13:07Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hkUWq","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V9sPt","databaseId":2515977197,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Remove the unused `strategy` parameter or implement curriculum behavior.**\n\nThe constructor accepts a `strategy` parameter documented as \"kept for backward compatibility\" (line 32), but:\n- This is a new file with no prior version to be backward-compatible with\n- The parameter is completely ignored and never stored or used\n- This misleads callers into thinking the strategy affects behavior\n\nSince the remarks (lines 11-17) explicitly state that curriculum logic belongs in the training loop/strategy layer, either:\n- **Option A (recommended):** Remove the `strategy` parameter entirely to avoid confusion\n- **Option B:** Implement actual curriculum-aware behavior (e.g., temperature scheduling, sample filtering based on difficulty)\n\n\n\nApply this diff to remove the unused parameter:\n\n```diff\n     /// <summary>\n     /// Initializes a new instance of the CurriculumTeacherModel class.\n     /// </summary>\n     /// <param name=\"baseTeacher\">The underlying teacher model.</param>\n-    /// <param name=\"strategy\">Curriculum strategy (kept for backward compatibility, not used).</param>\n     public CurriculumTeacherModel(\n-        ITeacherModel<Vector<T>, Vector<T>> baseTeacher,\n-        CurriculumStrategy strategy = CurriculumStrategy.EasyToHard)\n+        ITeacherModel<Vector<T>, Vector<T>> baseTeacher)\n     {\n         _baseTeacher = baseTeacher ?? throw new ArgumentNullException(nameof(baseTeacher));\n-        // Note: strategy parameter maintained for backward compatibility but curriculum\n-        // logic should be implemented in the training strategy, not the teacher\n     }\n```\n\nNote: If removing the parameter, also update `TeacherModelFactory.CreateCurriculumTeacher` (lines 142-144) to match the new signature.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/Teachers/CurriculumTeacherModel.cs around lines 28\nto 40, the constructor accepts an unused CurriculumStrategy parameter which is\nmisleading; remove the unused strategy parameter from the constructor signature\nand related XML doc/comment and simply store only the ITeacherModel<Vector<T>,\nVector<T>> baseTeacher (throwing ArgumentNullException as currently done). After\nremoving the parameter, update any call sites to match the new\nsignatureΓÇöspecifically update TeacherModelFactory.CreateCurriculumTeacher (lines\n~142-144) to call the constructor without the strategy argument and adjust its\nsignature if needed.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","path":"src/KnowledgeDistillation/Teachers/CurriculumTeacherModel.cs","position":37,"line":37,"createdAt":"2025-11-11T22:13:07Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hkjRT","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V-A_F","databaseId":2516062149,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Fix curriculum progress normalization**\n\nBecause `_currentStep` is clamped to `TotalSteps - 1` while `CurriculumProgress` divides by `TotalSteps`, progress never reaches 1.0. That means the hardest samples (difficulty `1.0`) are permanently excluded in `EasyToHardCurriculumStrategy.ShouldIncludeSample`, and `ComputeCurriculumTemperature` never actually hits `MinTemperature`. Please normalize against `TotalSteps - 1` (with a guard for the single-step case) so the final stage is reachable.\n\n```diff\n-    public double CurriculumProgress => (double)_currentStep / TotalSteps;\n+    public double CurriculumProgress\n+    {\n+        get\n+        {\n+            if (TotalSteps <= 1)\n+            {\n+                return 1.0;\n+            }\n+\n+            return (double)_currentStep / (TotalSteps - 1);\n+        }\n+    }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    public double CurriculumProgress\n    {\n        get\n        {\n            if (TotalSteps <= 1)\n            {\n                return 1.0;\n            }\n\n            return (double)_currentStep / (TotalSteps - 1);\n        }\n    }\n\n    /// <summary>\n    /// Gets the minimum temperature for the curriculum.\n    /// </summary>\n    public double MinTemperature { get; }\n\n    /// <summary>\n    /// Gets the maximum temperature for the curriculum.\n    /// </summary>\n    public double MaxTemperature { get; }\n\n    /// <summary>\n    /// Initializes a new instance of the CurriculumDistillationStrategyBase class.\n    /// </summary>\n    /// <param name=\"baseTemperature\">Base temperature for distillation (default: 3.0).</param>\n    /// <param name=\"alpha\">Balance between hard and soft loss (default: 0.3).</param>\n    /// <param name=\"minTemperature\">Minimum temperature for curriculum (default: 2.0).</param>\n    /// <param name=\"maxTemperature\">Maximum temperature for curriculum (default: 5.0).</param>\n    /// <param name=\"totalSteps\">Total training steps/epochs (default: 100).</param>\n    /// <param name=\"sampleDifficulties\">Optional pre-defined difficulty scores.</param>\n    protected CurriculumDistillationStrategyBase(\n        double baseTemperature = 3.0,\n        double alpha = 0.3,\n        double minTemperature = 2.0,\n        double maxTemperature = 5.0,\n        int totalSteps = 100,\n        Dictionary<int, double>? sampleDifficulties = null)\n        : base(baseTemperature, alpha)\n    {\n        if (minTemperature <= 0 || maxTemperature <= minTemperature)\n            throw new ArgumentException(\"Temperature range invalid: must have 0 < min < max\");\n        if (totalSteps <= 0)\n            throw new ArgumentException(\"Total steps must be positive\", nameof(totalSteps));\n\n        MinTemperature = minTemperature;\n        MaxTemperature = maxTemperature;\n        TotalSteps = totalSteps;\n        _sampleDifficulties = sampleDifficulties ?? new Dictionary<int, double>();\n        _currentStep = 0;\n    }\n\n    /// <summary>\n    /// Updates the current curriculum progress.\n    /// </summary>\n    public virtual void UpdateProgress(int step)\n    {\n        _currentStep = Math.Max(0, Math.Min(step, TotalSteps - 1));\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/Strategies/CurriculumDistillationStrategyBase.cs\naround lines 37-85, CurriculumProgress currently divides _currentStep by\nTotalSteps while UpdateProgress clamps _currentStep to TotalSteps - 1,\npreventing progress from ever reaching 1.0; change the CurriculumProgress getter\nto normalize by (TotalSteps - 1) and add a guard so when TotalSteps == 1 it\nreturns 1.0 (or 0.0 if you prefer initial behavior), and keep UpdateProgress\nclamping to the valid range [0, TotalSteps - 1]; this ensures progress can reach\nthe final stage and MinTemperature/ hardest samples become reachable.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits f5a28ea to d83d239","path":"src/KnowledgeDistillation/Strategies/CurriculumDistillationStrategyBase.cs","position":86,"line":86,"createdAt":"2025-11-11T22:47:41Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hkjRg","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V-A_S","databaseId":2516062162,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Fix curriculum thresholds so easiest samples are reachable.**\n\n`CurriculumProgress` is defined as `_currentStep / TotalSteps`, with `_currentStep` clamped to `TotalSteps - 1` in the base class. As a result, `CurriculumProgress` never reaches 1.0, so `threshold = 1.0 - CurriculumProgress` bottoms out at `1 / TotalSteps` and the temperature never climbs to `MaxTemperature`. The easiest samples (difficulty 0.0) are therefore permanently excluded, and the strategy never delivers the fully ΓÇ£easyΓÇ¥ phase described in the docs. Please normalize progress so the last step is treated as 1.0 before computing both the threshold and the curriculum temperature.\n\n\nApply this diff to normalize progress:\n\n```diff\n-        double threshold = 1.0 - CurriculumProgress;\n-        return difficulty.Value >= threshold;\n+        double progress = CurriculumProgress;\n+        if (progress >= 1.0 - (1.0 / TotalSteps))\n+        {\n+            progress = 1.0;\n+        }\n+\n+        double threshold = 1.0 - progress;\n+        return difficulty.Value >= threshold;\n@@\n-        double temperature = MinTemperature + CurriculumProgress * (MaxTemperature - MinTemperature);\n+        double progress = CurriculumProgress;\n+        if (progress >= 1.0 - (1.0 / TotalSteps))\n+        {\n+            progress = 1.0;\n+        }\n+\n+        double temperature = MinTemperature + progress * (MaxTemperature - MinTemperature);\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/Strategies/HardToEasyCurriculumStrategy.cs around\nlines 144-190, the code uses CurriculumProgress which is computed with\n_currentStep clamped to TotalSteps-1 and so never reaches 1.0; normalize\nprogress so the final step maps to 1.0 before computing both the inclusion\nthreshold and the curriculum temperature. Compute a normalizedProgress =\n(TotalSteps <= 1) ? 1.0 : (double)Math.Min(_currentStep, TotalSteps - 1) /\n(double)(TotalSteps - 1) (ensure double arithmetic and clamp to [0.0,1.0]), then\nreplace uses of CurriculumProgress in the threshold calculation (threshold = 1.0\n- normalizedProgress) and in ComputeCurriculumTemperature (use\nnormalizedProgress when interpolating MinTemperatureΓåÆMaxTemperature) and finally\npass the resulting temperature through ClampTemperature as before.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","path":"src/KnowledgeDistillation/Strategies/HardToEasyCurriculumStrategy.cs","position":192,"line":192,"createdAt":"2025-11-11T22:47:41Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hkjRj","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V-A_V","databaseId":2516062165,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**DonΓÇÖt wire the online teacher with a no-op updater.** The factory currently passes `(pred, target) => { }`, so `OnlineTeacherModel` can never adaptΓÇödefeating the whole ΓÇ£onlineΓÇ¥ contract. Either surface a real update delegate (e.g., expose a training hook from `IFullModel`) or refuse to build the online teacher until the caller supplies one. Leaving it as a silent no-op is a functional bug.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/TeacherModelFactory.cs around lines 137 to 143, do\nnot pass a silent no-op update delegate into OnlineTeacherModel; instead wire a\nreal updater or refuse to build the online teacher. Update the factory to (a)\nobtain and pass a real update delegate from the model (e.g., expose and use an\nIFullModel.Train/Update hook or a delegate like Action<float[], float[]>\nprovided by the model) or (b) change the factory signature to accept an update\ndelegate parameter and pass that through; if neither is available, throw an\nArgumentException/InvalidOperationException instead of using (pred, target) => {\n} so the caller cannot silently create a non-adaptive online teacher.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","path":"src/KnowledgeDistillation/TeacherModelFactory.cs","position":1,"line":null,"createdAt":"2025-11-11T22:47:41Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hlkT2","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V_fl7","databaseId":2516449659,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Align batch checkpoint cadence in Example 5.**\n\n`SaveCheckpointIfNeeded` only runs when `(batch + 1) % 100 == 0`, so `_batchCounter` advances once per 100 training batches. With `SaveEveryBatches = 1000`, the manager actually saves every 100ΓÇ»000 batches, not 1ΓÇ»000 as the text suggests. Please either call `SaveCheckpointIfNeeded` on every batch or adjust the modulus/`SaveEveryBatches` values so the worked example reflects the intended 1ΓÇ»000-batch cadence.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/CHECKPOINTING_GUIDE.md around lines 320 to 353, the\nexample only calls SaveCheckpointIfNeeded when (batch + 1) % 100 == 0 which\nadvances the internal batch counter once per 100 training steps, causing\nSaveEveryBatches = 1000 to actually trigger every 100,000 batches; fix by either\n(A) calling SaveCheckpointIfNeeded on every batch (move the call out of the\nvalidation-modulus block) and keep the validation/expensive checks gated by\n(batch + 1) % 100 == 0, or (B) change the validation modulus to 1000 so the call\noccurs every 1,000 batches to match SaveEveryBatches = 1000 ΓÇö pick one and\nupdate the example accordingly so the described 1,000-batch cadence is accurate.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","path":"src/KnowledgeDistillation/CHECKPOINTING_GUIDE.md","position":432,"line":432,"createdAt":"2025-11-12T02:11:44Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hlkT7","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V_fl_","databaseId":2516449663,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Batch cadence is ignored when epoch cadence is enabled.**\n\n`ShouldSaveCheckpoint` returns immediately from the epoch branch, so the batch branch is never reached when both `SaveEveryEpochs > 0` and `SaveEveryBatches > 0`. That makes batch-based checkpointing silently stop working whenever epoch-based cadence is configured. Please combine the conditions so either cadence can trigger a save. \nApply this diff:\n\n```diff\n-        if (epoch.HasValue && _config.SaveEveryEpochs > 0)\n-        {\n-            return (epoch.Value + 1) % _config.SaveEveryEpochs == 0;\n-        }\n-\n-        if (batch.HasValue && _config.SaveEveryBatches > 0)\n-        {\n-            _batchCounter++;\n-            return _batchCounter % _config.SaveEveryBatches == 0;\n-        }\n-\n-        return false;\n+        bool shouldSave = false;\n+\n+        if (epoch.HasValue && _config.SaveEveryEpochs > 0)\n+        {\n+            shouldSave |= (epoch.Value + 1) % _config.SaveEveryEpochs == 0;\n+        }\n+\n+        if (batch.HasValue && _config.SaveEveryBatches > 0)\n+        {\n+            _batchCounter++;\n+            shouldSave |= _batchCounter % _config.SaveEveryBatches == 0;\n+        }\n+\n+        return shouldSave;\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    public bool ShouldSaveCheckpoint(int? epoch = null, int? batch = null)\n    {\n        bool shouldSave = false;\n\n        if (epoch.HasValue && _config.SaveEveryEpochs > 0)\n        {\n            shouldSave |= (epoch.Value + 1) % _config.SaveEveryEpochs == 0;\n        }\n\n        if (batch.HasValue && _config.SaveEveryBatches > 0)\n        {\n            _batchCounter++;\n            shouldSave |= _batchCounter % _config.SaveEveryBatches == 0;\n        }\n\n        return shouldSave;\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/DistillationCheckpointManager.cs around lines 91 to\n105, the current implementation returns immediately from the epoch branch so\nbatch-based checkpointing is skipped when both cadences are configured; modify\nthe logic to evaluate both cadences and return true if either cadence triggers a\nsave. Concretely: if epoch.HasValue and SaveEveryEpochs>0 compute epochTriggered\n(don't return immediately); if batch.HasValue and SaveEveryBatches>0 increment\n_batchCounter and compute batchTriggered; then return (epochTriggered ||\nbatchTriggered); keep default false when neither triggers.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","path":"src/KnowledgeDistillation/DistillationCheckpointManager.cs","position":1,"line":null,"createdAt":"2025-11-12T02:11:44Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hlur-","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V_uwz","databaseId":2516511795,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Disambiguate `JsonSerializer` to fix the build.**\n\nThe project references both Newtonsoft.Json and System.Text.Json, so the unqualified `JsonSerializer` calls are ambiguous. Qualify them (or add an alias) so the file compiles.\n\n```diff\n-            var json = JsonSerializer.Serialize(metadata, new JsonSerializerOptions { WriteIndented = true });\n+            var json = System.Text.Json.JsonSerializer.Serialize(\n+                metadata,\n+                new System.Text.Json.JsonSerializerOptions { WriteIndented = true });\n```\n\n```diff\n-            var json = JsonSerializer.Serialize(state, new JsonSerializerOptions { WriteIndented = true });\n+            var json = System.Text.Json.JsonSerializer.Serialize(\n+                state,\n+                new System.Text.Json.JsonSerializerOptions { WriteIndented = true });\n```\n\n```diff\n-            var checkpoints = JsonSerializer.Deserialize<List<CheckpointMetadata>>(json);\n+            var checkpoints = System.Text.Json.JsonSerializer.Deserialize<List<CheckpointMetadata>>(json);\n```\n\n```diff\n-        var json = JsonSerializer.Serialize(_savedCheckpoints, new JsonSerializerOptions { WriteIndented = true });\n+        var json = System.Text.Json.JsonSerializer.Serialize(\n+            _savedCheckpoints,\n+            new System.Text.Json.JsonSerializerOptions { WriteIndented = true });\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n            var json = System.Text.Json.JsonSerializer.Serialize(\n                metadata,\n                new System.Text.Json.JsonSerializerOptions { WriteIndented = true });\n            File.WriteAllText(metadataPath, json);\n        }\n    }\n\n    /// <summary>\n    /// Loads the best checkpoint based on the configured metric.\n    /// </summary>\n    /// <param name=\"student\">Student model to load into.</param>\n    /// <param name=\"teacher\">Optional teacher model to load into.</param>\n    /// <returns>Metadata of the loaded checkpoint, or null if no checkpoints exist.</returns>\n    /// <remarks>\n    /// <para><b>For Beginners:</b> Call this after training to load the checkpoint\n    /// with the best validation performance.</para>\n    /// </remarks>\n    public CheckpointMetadata? LoadBestCheckpoint(\n        ICheckpointableModel? student = null,\n        ICheckpointableModel? teacher = null)\n    {\n        var bestCheckpoint = GetBestCheckpoint();\n        if (bestCheckpoint == null)\n        {\n            return null;\n        }\n\n        LoadCheckpoint(bestCheckpoint, student, teacher);\n        return bestCheckpoint;\n    }\n\n    /// <summary>\n    /// Loads a specific checkpoint.\n    /// </summary>\n    /// <param name=\"metadata\">Metadata of the checkpoint to load.</param>\n    /// <param name=\"student\">Student model to load into.</param>\n    /// <param name=\"teacher\">Optional teacher model to load into.</param>\n    public void LoadCheckpoint(\n        CheckpointMetadata metadata,\n        ICheckpointableModel? student = null,\n        ICheckpointableModel? teacher = null)\n    {\n        // Load student\n        if (student != null && metadata.StudentCheckpointPath != null)\n        {\n            using var stream = File.OpenRead(metadata.StudentCheckpointPath);\n            student.LoadState(stream);\n        }\n\n        // Load teacher\n        if (teacher != null && metadata.TeacherCheckpointPath != null)\n        {\n            using var stream = File.OpenRead(metadata.TeacherCheckpointPath);\n            teacher.LoadState(stream);\n        }\n\n        // Strategy state would be loaded separately based on type\n    }\n\n    /// <summary>\n    /// Gets the checkpoint with the best metric value.\n    /// </summary>\n    /// <returns>Metadata of the best checkpoint, or null if no checkpoints exist.</returns>\n    /// <remarks>\n    /// <para><b>For Advanced Users:</b> Returns the checkpoint with the best validation metric\n    /// based on the configuration (e.g., lowest validation loss or highest accuracy).</para>\n    /// </remarks>\n    public CheckpointMetadata? GetBestCheckpoint()\n    {\n        if (_savedCheckpoints.Count == 0)\n        {\n            return null;\n        }\n\n        var checkpointsWithMetric = _savedCheckpoints\n            .Where(c => c.Metrics.ContainsKey(_config.BestMetric))\n            .ToList();\n\n        if (checkpointsWithMetric.Count == 0)\n        {\n            return _savedCheckpoints.Last(); // Return most recent if no metrics\n        }\n\n        return _config.LowerIsBetter\n            ? checkpointsWithMetric.MinBy(c => c.Metrics[_config.BestMetric])\n            : checkpointsWithMetric.MaxBy(c => c.Metrics[_config.BestMetric]);\n    }\n\n    /// <summary>\n    /// Gets the most recently saved checkpoint.\n    /// </summary>\n    /// <returns>Metadata of the most recent checkpoint, or null if no checkpoints exist.</returns>\n    /// <remarks>\n    /// <para><b>For Advanced Users:</b> Useful for resuming interrupted training from the last saved state.</para>\n    /// </remarks>\n    public CheckpointMetadata? GetMostRecentCheckpoint()\n    {\n        return _savedCheckpoints.OrderByDescending(c => c.Epoch).ThenByDescending(c => c.Batch).FirstOrDefault();\n    }\n\n    /// <summary>\n    /// Gets a checkpoint for a specific epoch.\n    /// </summary>\n    /// <param name=\"epoch\">The epoch number to find.</param>\n    /// <returns>Metadata of the checkpoint at the specified epoch, or null if not found.</returns>\n    /// <remarks>\n    /// <para><b>For Advanced Users:</b> Returns the checkpoint saved at a specific epoch number.</para>\n    /// </remarks>\n    public CheckpointMetadata? GetCheckpointByEpoch(int epoch)\n    {\n        return _savedCheckpoints.FirstOrDefault(c => c.Epoch == epoch);\n    }\n\n    /// <summary>\n    /// Gets all saved checkpoint metadata as a readonly collection.\n    /// </summary>\n    /// <returns>Readonly list of all checkpoint metadata.</returns>\n    /// <remarks>\n    /// <para><b>For Advanced Users:</b> Provides read-only access to all saved checkpoints for custom queries.</para>\n    /// </remarks>\n    public IReadOnlyList<CheckpointMetadata> GetAllCheckpoints()\n    {\n        return _savedCheckpoints.AsReadOnly();\n    }\n\n    /// <summary>\n    /// Deletes old checkpoints, keeping only the best N.\n    /// </summary>\n    private void PruneOldCheckpoints()\n    {\n        if (_config.KeepBestN <= 0 || _savedCheckpoints.Count <= _config.KeepBestN)\n        {\n            return;\n        }\n\n        // Sort by metric (best first)\n        var sorted = _savedCheckpoints\n            .Where(c => c.Metrics.ContainsKey(_config.BestMetric))\n            .OrderBy(c => _config.LowerIsBetter ? c.Metrics[_config.BestMetric] : -c.Metrics[_config.BestMetric])\n            .ToList();\n\n        // Keep checkpoints without metrics\n        var withoutMetric = _savedCheckpoints\n            .Where(c => !c.Metrics.ContainsKey(_config.BestMetric))\n            .ToList();\n\n        // Delete checkpoints beyond KeepBestN\n        var toDelete = sorted.Skip(_config.KeepBestN).ToList();\n\n        foreach (var checkpoint in toDelete)\n        {\n            DeleteCheckpointFiles(checkpoint);\n            _savedCheckpoints.Remove(checkpoint);\n        }\n    }\n\n    /// <summary>\n    /// Deletes all files associated with a checkpoint.\n    /// </summary>\n    private void DeleteCheckpointFiles(CheckpointMetadata checkpoint)\n    {\n        if (checkpoint.StudentCheckpointPath != null && File.Exists(checkpoint.StudentCheckpointPath))\n        {\n            File.Delete(checkpoint.StudentCheckpointPath);\n        }\n\n        if (checkpoint.TeacherCheckpointPath != null && File.Exists(checkpoint.TeacherCheckpointPath))\n        {\n            File.Delete(checkpoint.TeacherCheckpointPath);\n        }\n\n        if (checkpoint.StrategyCheckpointPath != null && File.Exists(checkpoint.StrategyCheckpointPath))\n        {\n            File.Delete(checkpoint.StrategyCheckpointPath);\n        }\n\n        string metadataPath = checkpoint.FilePath + \".metadata.json\";\n        if (File.Exists(metadataPath))\n        {\n            File.Delete(metadataPath);\n        }\n    }\n\n    private string GenerateCheckpointPath(int epoch, int? batch)\n    {\n        string timestamp = DateTime.UtcNow.ToString(\"yyyyMMdd_HHmmss\");\n        string filename = batch.HasValue\n            ? $\"{_config.CheckpointPrefix}_epoch{epoch}_batch{batch.Value}_{timestamp}\"\n            : $\"{_config.CheckpointPrefix}_epoch{epoch}_{timestamp}\";\n\n        return Path.Combine(_config.CheckpointDirectory, filename);\n    }\n\n    private void SaveStrategyState(string path, object strategy)\n    {\n        // Serialize curriculum progress if applicable\n        if (strategy is ICurriculumDistillationStrategy<T> curriculum)\n        {\n            var state = new\n            {\n                Type = strategy.GetType().FullName,\n                CurriculumProgress = curriculum.CurriculumProgress,\n                TotalSteps = curriculum.TotalSteps\n            };\n\n            var json = System.Text.Json.JsonSerializer.Serialize(\n                state,\n                new System.Text.Json.JsonSerializerOptions { WriteIndented = true });\n            File.WriteAllText(path, json);\n        }\n    }\n\n    private void LoadExistingCheckpointMetadata()\n    {\n        string metadataIndexPath = Path.Combine(_config.CheckpointDirectory, \"checkpoint_index.json\");\n        if (File.Exists(metadataIndexPath))\n        {\n            var json = File.ReadAllText(metadataIndexPath);\n            var checkpoints = System.Text.Json.JsonSerializer.Deserialize<List<CheckpointMetadata>>(json);\n            if (checkpoints != null)\n            {\n                _savedCheckpoints.AddRange(checkpoints);\n            }\n        }\n    }\n\n    private void SaveCheckpointMetadata()\n    {\n        string metadataIndexPath = Path.Combine(_config.CheckpointDirectory, \"checkpoint_index.json\");\n        var json = System.Text.Json.JsonSerializer.Serialize(\n            _savedCheckpoints,\n            new System.Text.Json.JsonSerializerOptions { WriteIndented = true });\n        File.WriteAllText(metadataIndexPath, json);\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒº░ Tools</summary>\n\n<details>\n<summary>≡ƒ¬¢ GitHub Check: Publish Size Analysis</summary>\n\n[failure] 416-416: \n'JsonSerializer' is an ambiguous reference between 'Newtonsoft.Json.JsonSerializer' and 'System.Text.Json.JsonSerializer'\n\n---\n\n[failure] 405-405: \n'JsonSerializer' is an ambiguous reference between 'Newtonsoft.Json.JsonSerializer' and 'System.Text.Json.JsonSerializer'\n\n---\n\n[failure] 394-394: \n'JsonSerializer' is an ambiguous reference between 'Newtonsoft.Json.JsonSerializer' and 'System.Text.Json.JsonSerializer'\n\n---\n\n[failure] 191-191: \n'JsonSerializer' is an ambiguous reference between 'Newtonsoft.Json.JsonSerializer' and 'System.Text.Json.JsonSerializer'\n\n</details>\n\n</details>\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/DistillationCheckpointManager.cs around lines\n191-417, unqualified uses of JsonSerializer are ambiguous because both\nNewtonsoft.Json and System.Text.Json are referenced; update each JsonSerializer\ncall to be fully qualified (e.g., System.Text.Json.JsonSerializer) or add a\nfile-level alias (e.g., an alias for System.Text.Json) and use that alias\nconsistently for serialization/deserialization calls in this file so the\ncompiler can resolve the correct type; ensure all Serialize/Deserialize usages\nand JsonSerializerOptions references are updated accordingly and keep or remove\nconflicting using directives as needed.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 00af3f8 to 5f82106","path":"src/KnowledgeDistillation/DistillationCheckpointManager.cs","position":1,"line":null,"createdAt":"2025-11-12T02:45:31Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hlusG","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86V_uxA","databaseId":2516511808,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n<details>\n<summary>≡ƒº⌐ Analysis chain</summary>\n\n**Verify that FlowBased should map to ProbabilisticStrategy.**\n\nConfirm that `DistillationStrategyType.FlowBased` is correctly mapped to `ProbabilisticDistillationStrategy`. If flow-based distillation requires distinct flow-matching or normalizing-flow logic, this mapping may be incorrect.\n\n\nRun the following script to check if there's a dedicated `FlowBasedDistillationStrategy` or related documentation:\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Description: Search for flow-based distillation implementation or documentation\n\n# Search for FlowBased or flow distillation references\nrg -nP -C3 'FlowBased|flow.based.distillation|normalizing.flow' --type=cs\n\n# Check if there's a FlowBasedDistillationStrategy or FlowDistillationStrategy class\nfd -e cs -x rg -l 'class.*Flow.*Distillation.*Strategy'\n```\n\nLength of output: 1844\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Find all distillation strategy implementations\necho \"=== Distillation Strategy Implementations ===\"\nfd -e cs -x grep -l 'class.*DistillationStrategy' \n\necho \"\"\necho \"=== ProbabilisticDistillationStrategy Definition ===\"\nrg -nP 'class\\s+ProbabilisticDistillationStrategy' -A 20 --type=cs\n\necho \"\"\necho \"=== All CreateXXXStrategy method names ===\"\nrg -nP 'private.*Create.*Strategy\\s*\\(' --type=cs src/KnowledgeDistillation/DistillationStrategyFactory.cs\n```\n\nLength of output: 5187\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check CreateProbabilisticStrategy implementation\necho \"=== CreateProbabilisticStrategy Implementation ===\"\nrg -nP 'CreateProbabilisticStrategy' -A 10 --type=cs src/KnowledgeDistillation/DistillationStrategyFactory.cs\n\necho \"\"\necho \"=== Test references to FlowBased ===\"\nrg -nP 'FlowBased' -B 2 -A 2 --type=cs tests/\n\necho \"\"\necho \"=== FlowBased enum documentation ===\"\nrg -nP -B 10 'FlowBased\\s*=\\s*6' --type=cs src/Enums/DistillationStrategyType.cs\n```\n\nLength of output: 2255\n\n---\n\n\n\n</details>\n\n**Implement FlowBasedDistillationStrategy or correct the FlowBased mapping.**\n\nThe enum documentation defines `FlowBased` as \"Flow of Solution Procedure / FSP (Yim et al., 2017)\" which requires layer-pair flow matrices to transfer information between layers. However, the current mapping to `CreateProbabilisticStrategy` (line 50) uses `ProbabilisticDistillationStrategy`, which does not support flow matrices or layer-pair configurations needed for FSP. Either:\n- Implement a dedicated `FlowBasedDistillationStrategy` with layer-pair flow matrix support, or  \n- Remove `FlowBased` from the enum and update the factory mapping if FSP is not intended to be supported\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits e713a2b to a5ebe95","path":"src/KnowledgeDistillation/DistillationStrategyFactory.cs","position":1,"line":null,"createdAt":"2025-11-12T02:45:31Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hmHld","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86WARQa","databaseId":2516653082,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Guard against empty batch (divide-by-zero).**\n\nTrainBatch divides by inputs.Length; empty inputs cause undefined behavior. Fail fast.\n\nApply:\n\n```diff\n public virtual T TrainBatch(\n@@\n-    {\n+    {\n         if (studentForward == null) throw new ArgumentNullException(nameof(studentForward));\n         if (studentBackward == null) throw new ArgumentNullException(nameof(studentBackward));\n         if (inputs == null) throw new ArgumentNullException(nameof(inputs));\n+        if (inputs.Length == 0) throw new ArgumentException(\"inputs must not be empty.\", nameof(inputs));\n@@\n-        return NumOps.Divide(totalLoss, NumOps.FromDouble(inputs.Length));\n+        return NumOps.Divide(totalLoss, NumOps.FromDouble(inputs.Length));\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/KnowledgeDistillationTrainerBase.cs around lines\n150 to 178, the training loop divides totalLoss by inputs.Length which will\nthrow/divide-by-zero for an empty or null batch; guard this by validating inputs\nat the start of the method and failing fast: if inputs is null throw\nArgumentNullException, and if inputs.Length == 0 throw an ArgumentException (or\na domain-specific exception) with a clear message; this ensures you never reach\nthe final division with zero and keeps behavior explicit.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:chinchilla -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits f5a28ea to ab00d56","path":"src/KnowledgeDistillation/KnowledgeDistillationTrainerBase.cs","position":174,"line":174,"createdAt":"2025-11-12T04:00:16Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hmHll","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86WARQl","databaseId":2516653093,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Prevent numBatches = 0 (divide-by-zero when dataset is empty).**\n\nEmpty trainInputs yields numBatches=0 and later division by zero. Enforce non-empty dataset.\n\n```diff\n@@ public virtual void Train(\n-        if (epochs <= 0)\n+        if (epochs <= 0)\n             throw new ArgumentException(\"Epochs must be positive\", nameof(epochs));\n         if (batchSize <= 0)\n             throw new ArgumentException(\"BatchSize must be positive\", nameof(batchSize));\n         if (trainLabels != null && trainInputs.Length != trainLabels.Length)\n             throw new ArgumentException(\"Inputs and labels must have the same length\");\n         if (validationInputs != null && validationLabels != null && validationInputs.Length != validationLabels.Length)\n             throw new ArgumentException(\"Validation inputs and labels must have the same length\");\n+        if (trainInputs.Length == 0)\n+            throw new ArgumentException(\"Training set must not be empty.\", nameof(trainInputs));\n@@\n-        var avgEpochLoss = NumOps.Divide(epochLoss, NumOps.FromDouble(numBatches));\n+        var avgEpochLoss = NumOps.Divide(epochLoss, NumOps.FromDouble(numBatches));\n```\n\n\nAlso applies to: 273-274\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/KnowledgeDistillationTrainerBase.cs around lines\n242-274, the code can compute numBatches = 0 when trainInputs is empty which\nlater causes a divide-by-zero; add an explicit validation at the start of the\ntraining method to enforce a non-empty training set (e.g., if trainInputs ==\nnull || trainInputs.Length == 0) and throw an ArgumentException with a clear\nmessage, then compute numBatches only after that check; also ensure any later\ndivision (around lines 273-274) assumes numBatches > 0 so the validation\nprevents the divide-by-zero.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:chinchilla -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits f5a28ea to ab00d56","path":"src/KnowledgeDistillation/KnowledgeDistillationTrainerBase.cs","position":272,"line":272,"createdAt":"2025-11-12T04:00:16Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hmHln","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86WARQo","databaseId":2516653096,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Fix checkpoint metric: accuracy saved under ΓÇ£validation_lossΓÇ¥ (picks worst model).**\n\nDefault BestMetric is \"validation_loss\" (LowerIsBetter=true), but you store accuracy, inverting selection. Save both accuracy and a compatible loss proxy.\n\n```diff\n-            // Include validation metric if available\n-            if (_lastValidationMetric > 0)\n-            {\n-                metrics[_checkpointConfig!.BestMetric] = _lastValidationMetric;\n-            }\n+            // Include validation metrics if available\n+            if (_lastValidationMetric >= 0)\n+            {\n+                // Always record accuracy\n+                metrics[\"validation_accuracy\"] = _lastValidationMetric;\n+                // Also provide a loss-like proxy so default BestMetric=\"validation_loss\" works with LowerIsBetter=true\n+                metrics[\"validation_loss\"] = 100.0 - _lastValidationMetric;\n+            }\n```\nAlternatively: compute true validation loss and set BestMetric=\"validation_loss\"; or set BestMetric=\"validation_accuracy\" with LowerIsBetter=false in DistillationCheckpointConfig.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/KnowledgeDistillationTrainerBase.cs around lines\n587-593, you currently store validation accuracy into the checkpoint under the\nkey backed by _checkpointConfig.BestMetric which defaults to \"validation_loss\"\n(LowerIsBetter=true), causing the wrong model to be selected; fix by saving a\ncompatible loss metric or by saving accuracy under a proper key. Concretely:\nwhen you have _lastValidationMetric (accuracy), add\nmetrics[\"validation_accuracy\"] = _lastValidationMetric and also add a\nloss-compatible entry metrics[\"validation_loss\"] = 1.0 - _lastValidationMetric\n(or, if true validation loss is available, store that instead). Alternatively,\nif you prefer to keep storing only accuracy, change the checkpoint config so\nBestMetric = \"validation_accuracy\" and set LowerIsBetter = false in\nDistillationCheckpointConfig.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:chinchilla -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits e713a2b to 48e902f","path":"src/KnowledgeDistillation/KnowledgeDistillationTrainerBase.cs","position":592,"line":592,"createdAt":"2025-11-12T04:00:16Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hmHlp","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86WARQr","databaseId":2516653099,"body":"_≡ƒ¢á∩╕Å Refactor suggestion_ | _≡ƒƒá Major_\n\n**Provide default implementation using existing Serialize/Deserialize methods.**\n\nThe base class already has working `Serialize()` and `Deserialize(byte[])` methods (lines 1121-1235). Rather than throwing `NotImplementedException`, provide a default implementation that bridges to the existing serialization infrastructureΓÇöthis is exactly what `NeuralNetworkModel` does (lines 1071-1093 in relevant snippets) and `RegressionBase` (lines 932-948 in relevant snippets).\n\n\n\nApply this diff to provide a working default implementation:\n\n```diff\n     /// <summary>\n     /// Saves the model's current state to a stream.\n     /// </summary>\n     /// <param name=\"stream\">The stream to write the model state to.</param>\n     public virtual void SaveState(Stream stream)\n     {\n-        throw new NotImplementedException(\"SaveState is not yet implemented for NeuralNetworkBase. Consider using explicit serialization of layer parameters.\");\n+        if (stream == null)\n+            throw new ArgumentNullException(nameof(stream));\n+\n+        if (!stream.CanWrite)\n+            throw new ArgumentException(\"Stream must be writable.\", nameof(stream));\n+\n+        try\n+        {\n+            byte[] data = Serialize();\n+            stream.Write(data, 0, data.Length);\n+            stream.Flush();\n+        }\n+        catch (IOException ex)\n+        {\n+            throw new IOException($\"Failed to save model state to stream: {ex.Message}\", ex);\n+        }\n+        catch (Exception ex)\n+        {\n+            throw new InvalidOperationException($\"Unexpected error while saving model state: {ex.Message}\", ex);\n+        }\n     }\n\n     /// <summary>\n     /// Loads the model's state from a stream.\n     /// </summary>\n     /// <param name=\"stream\">The stream to read the model state from.</param>\n     public virtual void LoadState(Stream stream)\n     {\n-        throw new NotImplementedException(\"LoadState is not yet implemented for NeuralNetworkBase. Consider using explicit deserialization of layer parameters.\");\n+        if (stream == null)\n+            throw new ArgumentNullException(nameof(stream));\n+\n+        if (!stream.CanRead)\n+            throw new ArgumentException(\"Stream must be readable.\", nameof(stream));\n+\n+        try\n+        {\n+            using var ms = new MemoryStream();\n+            stream.CopyTo(ms);\n+            byte[] data = ms.ToArray();\n+\n+            if (data.Length == 0)\n+                throw new InvalidOperationException(\"Stream contains no data.\");\n+\n+            Deserialize(data);\n+        }\n+        catch (IOException ex)\n+        {\n+            throw new IOException($\"Failed to read model state from stream: {ex.Message}\", ex);\n+        }\n+        catch (InvalidOperationException)\n+        {\n+            throw;\n+        }\n+        catch (Exception ex)\n+        {\n+            throw new InvalidOperationException(\n+                $\"Failed to deserialize model state. The stream may contain corrupted or incompatible data: {ex.Message}\", ex);\n+        }\n     }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    /// <summary>\n    /// Saves the model's current state to a stream.\n    /// </summary>\n    /// <param name=\"stream\">The stream to write the model state to.</param>\n    public virtual void SaveState(Stream stream)\n    {\n        if (stream == null)\n            throw new ArgumentNullException(nameof(stream));\n\n        if (!stream.CanWrite)\n            throw new ArgumentException(\"Stream must be writable.\", nameof(stream));\n\n        try\n        {\n            byte[] data = Serialize();\n            stream.Write(data, 0, data.Length);\n            stream.Flush();\n        }\n        catch (IOException ex)\n        {\n            throw new IOException($\"Failed to save model state to stream: {ex.Message}\", ex);\n        }\n        catch (Exception ex)\n        {\n            throw new InvalidOperationException($\"Unexpected error while saving model state: {ex.Message}\", ex);\n        }\n    }\n\n    /// <summary>\n    /// Loads the model's state from a stream.\n    /// </summary>\n    /// <param name=\"stream\">The stream to read the model state from.</param>\n    public virtual void LoadState(Stream stream)\n    {\n        if (stream == null)\n            throw new ArgumentNullException(nameof(stream));\n\n        if (!stream.CanRead)\n            throw new ArgumentException(\"Stream must be readable.\", nameof(stream));\n\n        try\n        {\n            using var ms = new MemoryStream();\n            stream.CopyTo(ms);\n            byte[] data = ms.ToArray();\n\n            if (data.Length == 0)\n                throw new InvalidOperationException(\"Stream contains no data.\");\n\n            Deserialize(data);\n        }\n        catch (IOException ex)\n        {\n            throw new IOException($\"Failed to read model state from stream: {ex.Message}\", ex);\n        }\n        catch (InvalidOperationException)\n        {\n            throw;\n        }\n        catch (Exception ex)\n        {\n            throw new InvalidOperationException(\n                $\"Failed to deserialize model state. The stream may contain corrupted or incompatible data: {ex.Message}\", ex);\n        }\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/NeuralNetworks/NeuralNetworkBase.cs around lines 2137 to 2153, the\nSaveState and LoadState methods throw NotImplementedException; implement them by\nbridging to the existing Serialize() and Deserialize(byte[]) helpers: SaveState\nshould call Serialize(), write the resulting byte[] to the provided Stream\n(respecting stream write semantics and not closing the stream), and LoadState\nshould read all bytes from the provided Stream into a byte[] and pass it to\nDeserialize(byte[]), again not disposing the incoming stream. Ensure you handle\nstreams of unknown length (e.g., use a MemoryStream or ReadFully pattern) and\npreserve any exceptions so callers can surface serialization errors.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits f5a28ea to d83d239","path":"src/NeuralNetworks/NeuralNetworkBase.cs","position":30,"line":2294,"createdAt":"2025-11-12T04:00:16Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hmHlx","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86WARQ0","databaseId":2516653108,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Implement stream SaveState/LoadState to support checkpointing.**\n\nThrowing NotImplemented will break KD checkpoints. Delegate to Serialize/Deserialize with guards.\n\n```diff\n public virtual void SaveState(Stream stream)\n {\n-    throw new NotImplementedException(\"SaveState is not yet implemented for AsyncDecisionTreeRegressionBase. Consider serializing the tree structure explicitly.\");\n+    if (stream == null) throw new ArgumentNullException(nameof(stream));\n+    if (!stream.CanWrite) throw new ArgumentException(\"Stream must be writable.\", nameof(stream));\n+    var data = Serialize();\n+    stream.Write(data, 0, data.Length);\n+    stream.Flush();\n }\n@@\n public virtual void LoadState(Stream stream)\n {\n-    throw new NotImplementedException(\"LoadState is not yet implemented for AsyncDecisionTreeRegressionBase. Consider deserializing the tree structure explicitly.\");\n+    if (stream == null) throw new ArgumentNullException(nameof(stream));\n+    if (!stream.CanRead) throw new ArgumentException(\"Stream must be readable.\", nameof(stream));\n+    using var ms = new MemoryStream();\n+    stream.CopyTo(ms);\n+    var data = ms.ToArray();\n+    if (data.Length == 0) throw new InvalidOperationException(\"Stream contains no data.\");\n+    Deserialize(data);\n }\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/Regression/DecisionTreeAsyncRegressionBase.cs around lines 1006 to 1020,\nthe SaveState and LoadState methods currently throw NotImplementedException\nwhich breaks KD checkpointing; replace those throws by delegating to the\nexisting serialization helpers (call the class's Serialize/Deserialize methods\nor the equivalent tree-state serializer) with null checks on the stream,\nargument validation, and minimal exception handling (wrap/propagate\nIO/serialization exceptions). Ensure SaveState writes the model state to the\nprovided stream and Flushes; Ensure LoadState reads/deserializes from the\nprovided stream and restores internal tree structures; add appropriate locks if\nthe class is accessed concurrently and validate the stream position before/after\noperations.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:chinchilla -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 00af3f8 to a568592","path":"src/Regression/DecisionTreeAsyncRegressionBase.cs","position":29,"line":1030,"createdAt":"2025-11-12T04:00:16Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hmHlz","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86WARQ2","databaseId":2516653110,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Implement stream SaveState/LoadState to support checkpointing.**\n\nProvide working implementations delegating to Serialize/Deserialize with guards.\n\n```diff\n public virtual void SaveState(Stream stream)\n {\n-    throw new NotImplementedException(\"SaveState is not yet implemented for DecisionTreeRegressionBase. Consider serializing the tree structure explicitly.\");\n+    if (stream == null) throw new ArgumentNullException(nameof(stream));\n+    if (!stream.CanWrite) throw new ArgumentException(\"Stream must be writable.\", nameof(stream));\n+    var data = Serialize();\n+    stream.Write(data, 0, data.Length);\n+    stream.Flush();\n }\n@@\n public virtual void LoadState(Stream stream)\n {\n-    throw new NotImplementedException(\"LoadState is not yet implemented for DecisionTreeRegressionBase. Consider deserializing the tree structure explicitly.\");\n+    if (stream == null) throw new ArgumentNullException(nameof(stream));\n+    if (!stream.CanRead) throw new ArgumentException(\"Stream must be readable.\", nameof(stream));\n+    using var ms = new MemoryStream();\n+    stream.CopyTo(ms);\n+    var data = ms.ToArray();\n+    if (data.Length == 0) throw new InvalidOperationException(\"Stream contains no data.\");\n+    Deserialize(data);\n }\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/Regression/DecisionTreeRegressionBase.cs around lines 1113-1127, the\nSaveState/LoadState methods currently throw NotImplementedException; implement\nthem to delegate to the existing Serialize/Deserialize methods with proper\nguards: validate stream != null, check stream.CanWrite for SaveState and\nstream.CanRead for LoadState, ensure any internal tree/state is non-null before\nserializing/deserializing, and surface or wrap exceptions with a clear message.\nKeep the methods virtual and avoid closing the stream (do not dispose it) so\ncallers control stream lifetime.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:chinchilla -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commit 00af3f8","path":"src/Regression/DecisionTreeRegressionBase.cs","position":1,"line":1137,"createdAt":"2025-11-12T04:00:17Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hmHl3","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86WARQ7","databaseId":2516653115,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Implement stream SaveState/LoadState to support checkpointing.**\n\nTo avoid runtime NotImplemented, delegate to Serialize/Deserialize with guards.\n\n```diff\n public virtual void SaveState(Stream stream)\n {\n-    throw new NotImplementedException(\"SaveState is not yet implemented for NonLinearRegressionBase. Consider using Serialize() method to get serialized data.\");\n+    if (stream == null) throw new ArgumentNullException(nameof(stream));\n+    if (!stream.CanWrite) throw new ArgumentException(\"Stream must be writable.\", nameof(stream));\n+    var data = Serialize();\n+    stream.Write(data, 0, data.Length);\n+    stream.Flush();\n }\n@@\n public virtual void LoadState(Stream stream)\n {\n-    throw new NotImplementedException(\"LoadState is not yet implemented for NonLinearRegressionBase. Consider using Deserialize() method with byte array.\");\n+    if (stream == null) throw new ArgumentNullException(nameof(stream));\n+    if (!stream.CanRead) throw new ArgumentException(\"Stream must be readable.\", nameof(stream));\n+    using var ms = new MemoryStream();\n+    stream.CopyTo(ms);\n+    var data = ms.ToArray();\n+    if (data.Length == 0) throw new InvalidOperationException(\"Stream contains no data.\");\n+    Deserialize(data);\n }\n```\n\n<!-- fingerprinting:phantom:poseidon:chinchilla -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commit 00af3f8","path":"src/Regression/NonLinearRegressionBase.cs","position":1,"line":1136,"createdAt":"2025-11-12T04:00:17Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hrQtL","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86WHWUi","databaseId":2518508834,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n<details>\n<summary>≡ƒº⌐ Analysis chain</summary>\n\n**Verify batch counter behavior across epochs and resumption.**\n\n`_batchCounter` is never reset, so it accumulates across all epochs. This means `SaveEveryBatches` is a global batch interval, not per-epoch. Additionally, when training resumes from a checkpoint, `_batchCounter` restarts at 0, which can cause inconsistent checkpoint intervals.\n\n\n\nConfirm whether:\n1. Global batch counting (vs. per-epoch) is the intended behavior.\n2. Batch counter state should be persisted and restored during checkpoint resumption.\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Search for documentation or tests describing batch checkpoint behavior\nrg -nP --type=cs -C3 'SaveEveryBatches|_batchCounter'\n```\n\nLength of output: 3322\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Find CheckpointMetadata definition to see if batch counter is stored\nrg -nP --type=cs -A15 'class CheckpointMetadata' \n```\n\nLength of output: 1438\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Search for checkpoint loading/resumption logic\nrg -nP --type=cs -C5 'LoadExistingCheckpointMetadata|RestoreCheckpoint|LoadCheckpoint'\n```\n\nLength of output: 4090\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check if batch counter state is serialized anywhere\nrg -nP --type=cs -C3 'batchCounter|batch.*persist|serialize.*batch'\n```\n\nLength of output: 3831\n\n---\n\n\n\n</details>\n\n**`_batchCounter` state is not persisted across checkpoint resumption, breaking batch interval consistency.**\n\nThe global batch counting is working as documented. However, `_batchCounter` is never saved to or restored from checkpoints. When training resumes from a checkpoint, the counter resets to 0, causing `SaveEveryBatches` to save at incorrect intervals after resumption.\n\n**Required fixes:**\n- Add `BatchCounter` field to `CheckpointMetadata` (line 424+) to store state.\n- Save `_batchCounter` when persisting checkpoint data.\n- Restore `_batchCounter` when loading checkpoint in `LoadCheckpoint()` method (line 226).\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/DistillationCheckpointManager.cs around lines 57 to\n96 (constructor/ShouldSaveCheckpoint) and related methods (LoadCheckpoint at\n~line 226 and CheckpointMetadata definition at ~line 424+), the instance field\n_batchCounter is not persisted so it resets after resume; update the checkpoint\nmetadata to include an int BatchCounter field, update the code that\nserializes/persists checkpoint data to write the current _batchCounter into\nCheckpointMetadata when saving, and update LoadCheckpoint() to read\nmetadata.BatchCounter and assign it back to _batchCounter after successful load\nso batch-interval logic continues correctly across restarts. Ensure\nnull-safety/version compatibility when reading older checkpoints that lack the\nnew field.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits f5a28ea to c2be9b8","path":"src/KnowledgeDistillation/DistillationCheckpointManager.cs","position":96,"line":96,"createdAt":"2025-11-12T14:23:08Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85hrQta","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86WHWU1","databaseId":2518508853,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n<details>\n<summary>≡ƒº⌐ Analysis chain</summary>\n\n**Pruning keeps all checkpoints without the configured metric.**\n\nThe method preserves all checkpoints that lack the `BestMetric` key (lines 331-333), so the total checkpoint count can exceed `KeepBestN`. For example, with `KeepBestN = 3`, if you have 5 checkpoints with metrics and 2 without, you'll retain 5 total (3 best + 2 without metrics).\n\n\n\nVerify if this behavior matches the design intent, or if checkpoints without metrics should also be pruned:\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Search for documentation or tests describing pruning behavior for checkpoints without metrics\nrg -nP --type=cs -C3 'KeepBestN|PruneOldCheckpoints' | rg -C3 'without.*metric|no.*metric'\n```\n\nLength of output: 683\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Get the full documentation for KeepBestN in DistillationCheckpointConfig.cs\ncat src/KnowledgeDistillation/DistillationCheckpointConfig.cs | sed -n '35,50p'\n```\n\nLength of output: 575\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Search for tests related to PruneOldCheckpoints\nfd -e '\\.cs$' -path '*[Tt]est*' --exec grep -l 'PruneOldCheckpoints\\|KeepBestN' {} \\;\n```\n\nLength of output: 168\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Look for inline comments or documentation near the checkpoints without metrics logic\nrg -nP --type=cs -B5 -A5 'withoutMetric.*=' \n```\n\nLength of output: 1190\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check if there are any GitHub issues or discussions about this behavior\nrg -nP --type=cs -C5 'TODO|FIXME|NOTE|HACK' | rg -C3 'KeepBestN\\|metric'\n```\n\nLength of output: 0\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Confirm withoutMetric is never used after line 333\nrg -n 'withoutMetric' src/KnowledgeDistillation/DistillationCheckpointManager.cs\n```\n\nLength of output: 49\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# View the complete PruneOldCheckpoints method to ensure nothing else uses withoutMetric\nrg -n -A20 'private void PruneOldCheckpoints' src/KnowledgeDistillation/DistillationCheckpointManager.cs\n```\n\nLength of output: 830\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Search for any integration tests or scenarios involving checkpoint pruning\nfd . -name '*.cs' -type f -exec grep -l 'checkpoint.*prune\\|prune.*checkpoint' {} \\;\n```\n\nLength of output: 230\n\n---\n\n\n\n</details>\n\n**Remove dead code and fix pruning logic to match documentation.**\n\nThe `withoutMetric` variable (line 331) is never usedΓÇöonly `sorted` checkpoints are pruned (line 336). This causes total checkpoint count to exceed `KeepBestN`. Documentation states \"KeepBestN = 3 keeps only the 3 checkpoints,\" but current code keeps 3 with metrics plus all without metrics.\n\nEither include checkpoints without metrics in the deletion logic:\n```csharp\nvar toDelete = sorted.Concat(withoutMetric).Skip(_config.KeepBestN).ToList();\n```\n\nOr remove the unused variable if checkpoints without metrics should always be preserved (then update documentation to clarify).\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/DistillationCheckpointManager.cs around lines 317\nto 343, the local withoutMetric list is never used so checkpoints without the\nmetric are never deleted, causing total saved checkpoints to exceed KeepBestN;\nto fix this, include those without-metric checkpoints in the pruning sequence by\nconcatenating sorted and withoutMetric into the set to prune (preserving the\ndesired ordering), then take Skip(_config.KeepBestN) and delete those items, or\nalternatively remove the unused withoutMetric variable and update docs if you\nintend to always preserve checkpoints without metrics ΓÇö implement the\nconcatenation approach to match the existing documentation that KeepBestN keeps\nexactly that many checkpoints.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits f5a28ea to c2be9b8","path":"src/KnowledgeDistillation/DistillationCheckpointManager.cs","position":354,"line":354,"createdAt":"2025-11-12T14:23:08Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85huXSB","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86WLjSH","databaseId":2519610503,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Remove duplicate XML summary tag**\n\nTwo consecutive <summary> tags render invalid XML docs.\n\nApply:\n\n```diff\n-    /// <summary>\n-    /// <summary>\n+    /// <summary>\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    /// <summary>\n    /// Configures knowledge distillation for training a smaller student model from a larger teacher model.\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/Interfaces/IPredictionModelBuilder.cs around lines 566 to 568, there are\ntwo consecutive XML <summary> tags which makes the XML documentation invalid;\nremove the duplicate so there is only a single <summary> start tag and its\nmatching end tag surrounding the documentation for the knowledge distillation\nconfiguration, ensuring the XML doc is well-formed.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:chinchilla -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commit 285fc01","path":"src/Interfaces/IPredictionModelBuilder.cs","position":1,"line":null,"createdAt":"2025-11-12T19:55:33Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85huXSM","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86WLjSe","databaseId":2519610526,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Gradient scaling inconsistent with loss when no hard labels**\n\nComputeLoss returns softLoss (no (1ΓêÆ╬▒) factor) when trueLabels is null, but ComputeGradient multiplies the soft gradient by (1ΓêÆ╬▒). This breaks the gradient = d(loss)/d(logits) relationship.\n\nFix by removing the extra scaling:\n\n```diff\n-        else\n-        {\n-            // Scale by (1 - alpha) if no hard loss\n-            for (int i = 0; i < n; i++)\n-            {\n-                gradient[i] = NumOps.Multiply(gradient[i], NumOps.FromDouble(1.0 - Alpha));\n-            }\n-        }\n+        // No extra scaling when no hard loss; matches loss = softLoss\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/Strategies/AdaptiveDistillationStrategyBase.cs\naround lines 166 to 186, the ComputeGradient branch for null trueLabels\nmultiplies the soft gradient by (1ΓêÆAlpha) even though ComputeLoss returns the\nsoft loss without that factor; remove the extra scaling so the gradient remains\nthe direct derivative of the returned loss (i.e., do not multiply gradient[i] by\n(1.0 - Alpha) when trueLabels is null, leave gradient[i] unchanged).\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:chinchilla -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commit 285fc01","path":"src/KnowledgeDistillation/Strategies/AdaptiveDistillationStrategyBase.cs","position":186,"line":186,"createdAt":"2025-11-12T19:55:33Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85huXSO","isResolved":false,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86WLjSm","databaseId":2519610534,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Add featureΓÇædimension validation before factor extraction**\n\nOnly batch size is checked here; downstream loops assume uniform vector lengths and may throw IndexOutOfRangeException on mismatch. Validate that:\n- All studentFeatures[i].Length equal,\n- All teacherFeatures[i].Length equal,\n- studentFeatures[i].Length == teacherFeatures[i].Length for each i,\n- Length > 0.\n\nSuggest inserting this guard at the start of ComputeFactorLoss.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/Strategies/FactorTransferDistillationStrategy.cs\naround lines 162 to 179, the method only validates batch size but not vector\nlengths, which can cause IndexOutOfRangeExceptions later; add a guard after the\nbatch-size check that verifies: (1) each studentFeatures[i].Length is the same\nfor all i, (2) each teacherFeatures[i].Length is the same for all i, (3) for\nevery i studentFeatures[i].Length == teacherFeatures[i].Length, and (4) the\nlengths > 0; if any check fails, throw an ArgumentException with a clear message\nindicating mismatched or empty feature dimensions.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:chinchilla -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","path":"src/KnowledgeDistillation/Strategies/FactorTransferDistillationStrategy.cs","position":179,"line":179,"createdAt":"2025-11-12T19:55:33Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85huXSV","isResolved":false,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86WLjSy","databaseId":2519610546,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Guard teacher/student feature alignment.**\n\n`ComputeFlowLoss` assumes `studentFeatures` and `teacherFeatures` have identical lengths; if they differ (common when architectures donΓÇÖt line up), youΓÇÖll index past the end and crash. Please validate the lengths up front and throw a clear exception (or handle mismatched counts).\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/KnowledgeDistillation/Strategies/FlowBasedDistillationStrategy.cs around\nlines 172 to 189, add an upfront validation that studentFeatures and\nteacherFeatures have the same Length and that both have at least 2 layers; if\nlengths differ or either has fewer than 2 elements, throw an ArgumentException\nwith a clear message (e.g. \"studentFeatures and teacherFeatures must have the\nsame number of layers and at least 2 layers to compute flow matrices\"). This\nprevents indexing past the end and makes the failure explicit.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","path":"src/KnowledgeDistillation/Strategies/FlowBasedDistillationStrategy.cs","position":189,"line":189,"createdAt":"2025-11-12T19:55:34Z","author":{"login":"coderabbitai"}}]}},{"id":"PRRT_kwDOKSXUF85huXSc","isResolved":false,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86WLjS9","databaseId":2519610557,"body":"_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Selectivity term not integrated; base loss is reduced regardless**\n\nComputeLoss scales standard distillation by (1 ΓêÆ selectivityWeight) but never adds the selectivity loss, so selectivity never influences training and the base objective is weakened.\n\nOptions:\n- Preferred: integrate selectivity into the optimized loss: final = (1 ΓêÆ w)┬╖KD + w┬╖SelectivityLoss(batch activations); mirror this in gradients.\n- Interim safe fix: remove the (1 ΓêÆ w) scaling until selectivity is properly added.\n\nDiff for the interim fix:\n\n```diff\n-        // Apply selectivity weight reduction exactly once\n-        return NumOps.Multiply(finalLoss, NumOps.FromDouble(1.0 - _selectivityWeight));\n+        return finalLoss;\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    public override T ComputeLoss(Vector<T> studentOutput, Vector<T> teacherOutput, Vector<T>? trueLabels = null)\n    {\n        ValidateOutputDimensions(studentOutput, teacherOutput, v => v.Length);\n\n        // Standard distillation loss\n        var studentSoft = Softmax(studentOutput, Temperature);\n        var teacherSoft = Softmax(teacherOutput, Temperature);\n        var softLoss = KLDivergence(teacherSoft, studentSoft);\n        softLoss = NumOps.Multiply(softLoss, NumOps.FromDouble(Temperature * Temperature));\n\n        T finalLoss;\n        if (trueLabels != null)\n        {\n            ValidateLabelDimensions(studentOutput, trueLabels, v => v.Length);\n            var studentProbs = Softmax(studentOutput, 1.0);\n            var hardLoss = CrossEntropy(studentProbs, trueLabels);\n            finalLoss = NumOps.Add(\n                NumOps.Multiply(NumOps.FromDouble(Alpha), hardLoss),\n                NumOps.Multiply(NumOps.FromDouble(1.0 - Alpha), softLoss));\n        }\n        else\n        {\n            finalLoss = softLoss;\n        }\n\n        return finalLoss;\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<!-- fingerprinting:phantom:poseidon:chinchilla -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","path":"src/KnowledgeDistillation/Strategies/NeuronSelectivityDistillationStrategy.cs","position":71,"line":71,"createdAt":"2025-11-12T19:55:34Z","author":{"login":"coderabbitai"}}]}}],"pageInfo":{"hasNextPage":true,"endCursor":"Y3Vyc29yOnYyOpK0MjAyNS0xMS0xMlQxOTo1NTozNFrOYbl0nA=="}}}}}}
