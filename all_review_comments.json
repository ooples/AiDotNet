[
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102005",
    "pull_request_review_id": 3413273577,
    "id": 2488102005,
    "node_id": "PRRC_kwDOKSXUF86UTWx1",
    "diff_hunk": "@@ -89,35 +92,44 @@ public class Document\n     /// <para>\n     /// The relevance score indicates how well this document matches a query.\n     /// Higher scores indicate stronger relevance. The score scale and interpretation\n-    /// depend on the retrieval or reranking algorithm used. This value is null for\n-    /// documents that haven't been scored.\n+    /// depend on the retrieval or reranking algorithm used. Use HasRelevanceScore to check\n+    /// if a score has been assigned before accessing this value.\n     /// </para>\n     /// <para><b>For Beginners:</b> This is like a match percentage showing how relevant this document is.\n     /// \n     /// Think of it like search results:\n     /// - Score 0.95: Almost perfect match, highly relevant\n     /// - Score 0.50: Somewhat relevant\n     /// - Score 0.10: Barely relevant\n-    /// - null: Not scored yet\n+    /// - Check HasRelevanceScore first to see if scored\n     /// \n     /// Documents with higher scores are more likely to contain the answer to your question.\n     /// </para>\n     /// </remarks>\n-    public double? RelevanceScore { get; set; }\n+    public T RelevanceScore { get; set; }\n+\n+    /// <summary>\n+    /// Gets or sets whether this document has a relevance score assigned.\n+    /// </summary>\n+    public bool HasRelevanceScore { get; set; }\n+\n+    private readonly INumericOperations<T> _numOps;",
    "path": "src/RetrievalAugmentedGeneration/Models/Document.cs",
    "commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "The `_numOps` field is declared but never used in the Document class. Since `RelevanceScore` is a public property that can be set directly, and all numeric operations on scores happen in retriever/reranker classes, this field should be removed to avoid unnecessary memory allocation for every document instance.",
    "created_at": "2025-11-03T23:02:01Z",
    "updated_at": "2025-11-03T23:02:18Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102005",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102005"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102005"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102005/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": null,
    "original_line": 116,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 50,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102011",
    "pull_request_review_id": 3413273577,
    "id": 2488102011,
    "node_id": "PRRC_kwDOKSXUF86UTWx7",
    "diff_hunk": "@@ -142,31 +150,41 @@ protected virtual void ValidateTopK(int topK)\n     /// - Makes scores consistent with other retrievers/rerankers\n     /// </para>\n     /// </remarks>\n-    protected IList<Document> NormalizeScores(IList<Document> documents)\n+    protected IList<Document<T>> NormalizeScores(IList<Document<T>> documents)\n     {\n-        var docsWithScores = documents.Where(d => d.RelevanceScore.HasValue).ToList();\n+        var docsWithScores = documents.Where(d => d.HasRelevanceScore).ToList();\n         if (docsWithScores.Count == 0)\n             return documents;\n \n-        var scores = docsWithScores.Select(d => d.RelevanceScore!.Value).ToList();\n+        var scores = docsWithScores.Select(d => d.RelevanceScore).ToList();\n+        if (scores.Count == 0)\n+            return documents;\n+\n         var minScore = scores.Min();\n         var maxScore = scores.Max();\n-        var range = maxScore - minScore;\n+        \n+        if (minScore == null || maxScore == null)\n+            return documents;",
    "path": "src/RetrievalAugmentedGeneration/Rerankers/RerankerBase.cs",
    "commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "The null checks for `minScore` and `maxScore` are incorrect. Since `scores` is a `List<T>` and `Min()`/`Max()` return `T` (not `T?`), these values cannot be null for value types like `float` or `double`. For reference types, `Min()`/`Max()` could return null if the list contains nulls, but this would throw an exception earlier when adding to `docsWithScores`. These null checks should be removed as they're unreachable and misleading.",
    "created_at": "2025-11-03T23:02:02Z",
    "updated_at": "2025-11-03T23:02:18Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102011",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102011"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102011"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102011/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 152,
    "start_side": "LEFT",
    "line": null,
    "original_line": 167,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 87,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102021",
    "pull_request_review_id": 3413273577,
    "id": 2488102021,
    "node_id": "PRRC_kwDOKSXUF86UTWyF",
    "diff_hunk": "@@ -0,0 +1,89 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using AiDotNet.Helpers;",
    "path": "src/RetrievalAugmentedGeneration/QueryExpansion/HyDEQueryExpansion.cs",
    "commit_id": "7ab7f09e63353ab03d484a171ae474a530b44af1",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "The `Vector<T>` type used in the return type of `GenerateHypotheticalEmbedding` method (line 56) is not imported. Add `using AiDotNet.LinearAlgebra;` to resolve this missing namespace.",
    "created_at": "2025-11-03T23:02:02Z",
    "updated_at": "2025-11-03T23:02:18Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102021",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102021"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102021"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102021/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": null,
    "original_line": 4,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 4,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102026",
    "pull_request_review_id": 3413273577,
    "id": 2488102026,
    "node_id": "PRRC_kwDOKSXUF86UTWyK",
    "diff_hunk": "@@ -0,0 +1,220 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Rerankers;\n+\n+/// <summary>\n+/// Reranks documents to maximize diversity while maintaining relevance.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for scoring.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// This reranker addresses the problem of redundant results by explicitly promoting diversity.\n+/// It uses a greedy algorithm to select documents that are both relevant to the query and\n+/// dissimilar from already-selected documents. This is similar to Maximal Marginal Relevance (MMR)\n+/// but uses a simpler diversity metric based on text overlap.\n+/// </para>\n+/// <para><b>For Beginners:</b> This prevents showing the same information multiple times.\n+/// \n+/// The Problem:\n+/// Imagine searching for \"Python programming\" and getting 10 results:\n+/// - Result 1: \"Python is a programming language...\"\n+/// - Result 2: \"Python is a programming language used for...\"\n+/// - Result 3: \"Python programming language allows...\"\n+/// - Result 4-10: More variations of the same thing\n+/// \n+/// That's redundant! You want variety:\n+/// - Result 1: Python basics\n+/// - Result 2: Python web development\n+/// - Result 3: Python data science\n+/// - Result 4: Python machine learning\n+/// - Result 5: Python performance tips\n+/// \n+/// How it works:\n+/// 1. Pick the most relevant document first\n+/// 2. For remaining docs, balance two factors:\n+///    a) Relevance to the query (should be useful)\n+///    b) Difference from already-picked docs (should be unique)\n+/// 3. Keep picking until you have enough results\n+/// \n+/// Diversity calculation:\n+/// - Compares text overlap (how many words are shared)\n+/// - Higher overlap = less diverse = lower score\n+/// - Lower overlap = more diverse = higher score\n+/// \n+/// Lambda parameter (0 to 1):\n+/// - lambda=1.0: Only care about relevance (might get duplicates)\n+/// - lambda=0.0: Only care about diversity (might get irrelevant docs)\n+/// - lambda=0.5: Balance both (recommended default)\n+/// \n+/// Real example with lambda=0.5:\n+/// Query: \"climate change effects\"\n+/// \n+/// Step 1: Pick most relevant ΓåÆ \"Climate change causes rising temperatures\" (relevance: 0.9)\n+/// Step 2: Next candidates:\n+///   - \"Climate change leads to warmer weather\" (relevance: 0.85, similarity to picked: 0.7)\n+///     ΓåÆ Score: 0.5 * 0.85 - 0.5 * 0.7 = 0.075\n+///   - \"Ocean acidification from CO2\" (relevance: 0.7, similarity: 0.2)\n+///     ΓåÆ Score: 0.5 * 0.7 - 0.5 * 0.2 = 0.25 Γ£ô Pick this!\n+/// \n+/// Result: You get coverage of temperature AND ocean effects, not just temperature twice!\n+/// \n+/// When to use this:\n+/// - Search results where redundancy is common\n+/// - Document recommendation systems\n+/// - Exploratory searches where breadth matters\n+/// - After initial retrieval that returns many similar docs\n+/// </para>\n+/// </remarks>\n+public class DiversityReranker<T> : RerankerBase<T>\n+{\n+    private readonly T _lambda;\n+\n+    /// <summary>\n+    /// Gets a value indicating whether this reranker modifies relevance scores.\n+    /// </summary>\n+    public override bool ModifiesScores => true;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the DiversityReranker class.\n+    /// </summary>\n+    /// <param name=\"numOps\">Numeric operations for type T.</param>\n+    /// <param name=\"lambda\">Trade-off parameter between relevance and diversity (0 to 1). Default: 0.5.\n+    /// Higher values favor relevance, lower values favor diversity.</param>\n+    public DiversityReranker(INumericOperations<T> numOps, T? lambda = default) : base()",
    "path": "src/RetrievalAugmentedGeneration/Rerankers/DiversityReranker.cs",
    "commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "The constructor accepts an `INumericOperations<T>` parameter but doesn't use it - the class uses the static `NumOps` field from the base class instead. Either remove the `numOps` parameter to match other rerankers (IdentityReranker, MaximalMarginalRelevanceReranker), or use the injected instance instead of the static field for better testability.",
    "created_at": "2025-11-03T23:02:02Z",
    "updated_at": "2025-11-03T23:02:18Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102026",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102026"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102026"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102026/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": null,
    "original_line": 85,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 85,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102035",
    "pull_request_review_id": 3413273577,
    "id": 2488102035,
    "node_id": "PRRC_kwDOKSXUF86UTWyT",
    "diff_hunk": "@@ -0,0 +1,99 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies\n+{\n+    /// <summary>\n+    /// Semantic-based text chunking that uses embeddings to group related content.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class SemanticChunkingStrategy<T> : ChunkingStrategyBase\n+    {\n+        private readonly INumericOperations<T> _numOps;\n+        private readonly T _similarityThreshold;\n+\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"SemanticChunkingStrategy{T}\"/> class.\n+        /// </summary>\n+        /// <param name=\"numericOperations\">The numeric operations for type T.</param>\n+        /// <param name=\"similarityThreshold\">The similarity threshold for grouping sentences.</param>\n+        /// <param name=\"maxChunkSize\">The maximum chunk size in characters.</param>\n+        public SemanticChunkingStrategy(\n+            INumericOperations<T> numericOperations,\n+            T similarityThreshold,\n+            int maxChunkSize = 1000,\n+            int chunkOverlap = 200)\n+            : base(maxChunkSize, chunkOverlap)\n+        {\n+            _numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\n+            _similarityThreshold = similarityThreshold;",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/SemanticChunkingStrategy.cs",
    "commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "The `_similarityThreshold` field is assigned in the constructor but never used in the `ChunkCore` method. The semantic chunking implementation doesn't actually use similarity calculations - it just splits by size. Either implement actual semantic similarity-based chunking using this threshold, or remove the unused field and parameter.\n```suggestion\n\n\n        /// <summary>\n        /// Initializes a new instance of the <see cref=\"SemanticChunkingStrategy{T}\"/> class.\n        /// </summary>\n        /// <param name=\"numericOperations\">The numeric operations for type T.</param>\n        /// <param name=\"maxChunkSize\">The maximum chunk size in characters.</param>\n        /// <param name=\"chunkOverlap\">The chunk overlap in characters.</param>\n        public SemanticChunkingStrategy(\n            INumericOperations<T> numericOperations,\n            int maxChunkSize = 1000,\n            int chunkOverlap = 200)\n            : base(maxChunkSize, chunkOverlap)\n        {\n            _numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\n```",
    "created_at": "2025-11-03T23:02:03Z",
    "updated_at": "2025-11-03T23:02:18Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102035",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102035"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102035"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102035/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 15,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 31,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 31,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102043",
    "pull_request_review_id": 3413273577,
    "id": 2488102043,
    "node_id": "PRRC_kwDOKSXUF86UTWyb",
    "diff_hunk": "@@ -0,0 +1,99 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies\n+{\n+    /// <summary>\n+    /// Semantic-based text chunking that uses embeddings to group related content.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class SemanticChunkingStrategy<T> : ChunkingStrategyBase\n+    {\n+        private readonly INumericOperations<T> _numOps;\n+        private readonly T _similarityThreshold;\n+\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"SemanticChunkingStrategy{T}\"/> class.\n+        /// </summary>\n+        /// <param name=\"numericOperations\">The numeric operations for type T.</param>\n+        /// <param name=\"similarityThreshold\">The similarity threshold for grouping sentences.</param>\n+        /// <param name=\"maxChunkSize\">The maximum chunk size in characters.</param>\n+        public SemanticChunkingStrategy(\n+            INumericOperations<T> numericOperations,\n+            T similarityThreshold,\n+            int maxChunkSize = 1000,\n+            int chunkOverlap = 200)\n+            : base(maxChunkSize, chunkOverlap)\n+        {\n+            _numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\n+            _similarityThreshold = similarityThreshold;\n+        }\n+\n+        /// <summary>\n+        /// Chunks text based on semantic similarity between sentences.\n+        /// </summary>\n+        /// <param name=\"text\">The text to chunk.</param>\n+        /// <returns>A collection of semantically coherent chunks with positions.</returns>\n+        protected override IEnumerable<(string Chunk, int StartPosition, int EndPosition)> ChunkCore(string text)\n+        {\n+            var sentences = SplitIntoSentences(text);\n+            var currentChunk = new List<string>();\n+            var currentSize = 0;\n+            var position = 0;\n+\n+            foreach (var sentence in sentences)\n+            {\n+                if (currentSize + sentence.Length > ChunkSize && currentChunk.Count > 0)\n+                {\n+                    var chunkText = string.Join(\" \", currentChunk);\n+                    var endPos = position + chunkText.Length;\n+                    yield return (chunkText, position, endPos);\n+                    \n+                    position = endPos - ChunkOverlap;\n+                    currentChunk.Clear();\n+                    currentSize = 0;\n+                }\n+\n+                currentChunk.Add(sentence);\n+                currentSize += sentence.Length;\n+            }\n+\n+            if (currentChunk.Count > 0)\n+            {\n+                var chunkText = string.Join(\" \", currentChunk);\n+                yield return (chunkText, position, position + chunkText.Length);\n+            }\n+        }\n+\n+        private List<string> SplitIntoSentences(string text)\n+        {\n+            var sentences = new List<string>();\n+            var sentenceEndings = new[] { \". \", \"! \", \"? \", \".\\n\", \"!\\n\", \"?\\n\" };\n+            var currentSentence = string.Empty;\n+\n+            for (int i = 0; i < text.Length; i++)\n+            {\n+                currentSentence += text[i];\n+\n+                foreach (var ending in sentenceEndings)\n+                {\n+                    if (currentSentence.EndsWith(ending))\n+                    {\n+                        sentences.Add(currentSentence.Trim());\n+                        currentSentence = string.Empty;\n+                        break;\n+                    }",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/SemanticChunkingStrategy.cs",
    "commit_id": "84f99e221d10d12f47a4fcccf4d1fffe7a2556f2",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "This foreach loop [implicitly filters its target sequence](1) - consider filtering the sequence explicitly using '.Where(...)'.\n```suggestion\n                if (sentenceEndings.Any(ending => currentSentence.EndsWith(ending)))\n                {\n                    sentences.Add(currentSentence.Trim());\n                    currentSentence = string.Empty;\n```",
    "created_at": "2025-11-03T23:02:03Z",
    "updated_at": "2025-11-03T23:02:18Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102043",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102043"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102043"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102043/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 80,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 87,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 87,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102054",
    "pull_request_review_id": 3413273577,
    "id": 2488102054,
    "node_id": "PRRC_kwDOKSXUF86UTWym",
    "diff_hunk": "@@ -0,0 +1,166 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ContextCompression\n+{\n+    /// <summary>\n+    /// Document summarizer for creating concise summaries of retrieved content.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class DocumentSummarizer<T>\n+    {\n+        private readonly INumericOperations<T> _numOps;\n+        private readonly int _maxSummaryLength;\n+        private readonly string _llmEndpoint;\n+        private readonly string _apiKey;\n+\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"DocumentSummarizer{T}\"/> class.\n+        /// </summary>\n+        /// <param name=\"numericOperations\">The numeric operations for type T.</param>\n+        /// <param name=\"maxSummaryLength\">The maximum length of the summary in characters.</param>\n+        /// <param name=\"llmEndpoint\">The LLM API endpoint.</param>\n+        /// <param name=\"apiKey\">The API key for the LLM service.</param>\n+        public DocumentSummarizer(\n+            INumericOperations<T> numericOperations,\n+            int maxSummaryLength = 500,\n+            string llmEndpoint = \"\",\n+            string apiKey = \"\")\n+        {\n+            _numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\n+            _maxSummaryLength = maxSummaryLength > 0\n+                ? maxSummaryLength\n+                : throw new ArgumentOutOfRangeException(nameof(maxSummaryLength));\n+            _llmEndpoint = llmEndpoint;\n+            _apiKey = apiKey;\n+        }\n+\n+        /// <summary>\n+        /// Summarizes a list of documents.\n+        /// </summary>\n+        /// <param name=\"documents\">The documents to summarize.</param>\n+        /// <returns>A list of summarized documents.</returns>\n+        public List<Document<T>> Summarize(List<Document<T>> documents)\n+        {\n+            if (documents == null) throw new ArgumentNullException(nameof(documents));\n+\n+            var summarized = new List<Document<T>>();\n+\n+            foreach (var doc in documents)\n+            {\n+                var summary = SummarizeText(doc.Content);\n+                var summarizedDoc = new Document<T>\n+                {\n+                    Id = doc.Id,\n+                    Content = summary,\n+                    Metadata = doc.Metadata,\n+                    RelevanceScore = doc.RelevanceScore\n+                };\n+                summarized.Add(summarizedDoc);\n+            }\n+\n+            return summarized;\n+        }\n+\n+        /// <summary>\n+        /// Summarizes text to a maximum length.\n+        /// </summary>\n+        /// <param name=\"text\">The text to summarize.</param>\n+        /// <returns>The summarized text.</returns>\n+        public string SummarizeText(string text)\n+        {\n+            if (string.IsNullOrEmpty(text)) return text;\n+\n+            if (text.Length <= _maxSummaryLength)\n+            {\n+                return text;\n+            }\n+\n+            var sentences = SplitIntoSentences(text);\n+            var importantSentences = ExtractImportantSentences(sentences);\n+\n+            var summary = string.Empty;\n+            foreach (var sentence in importantSentences)\n+            {\n+                if (summary.Length + sentence.Length > _maxSummaryLength)\n+                {\n+                    break;\n+                }\n+                summary += sentence + \" \";\n+            }\n+\n+            return summary.Trim();\n+        }\n+\n+        private List<string> ExtractImportantSentences(List<string> sentences)\n+        {\n+            var scored = new List<(string sentence, double score)>();\n+\n+            foreach (var sentence in sentences)\n+            {\n+                var importance = ComputeImportance(sentence, sentences);\n+                scored.Add((sentence, importance));\n+            }\n+\n+            return scored\n+                .OrderByDescending(x => x.score)\n+                .Select(x => x.sentence)\n+                .ToList();\n+        }\n+\n+        private double ComputeImportance(string sentence, List<string> allSentences)\n+        {\n+            var tokens = Tokenize(sentence);\n+            var uniqueTokens = tokens.Distinct().Count();\n+            var length = sentence.Length;\n+\n+            var positionScore = allSentences.IndexOf(sentence) == 0 ? 1.5 : 1.0;\n+\n+            var importance = (uniqueTokens * 0.5) + (Math.Min(length, 200) / 200.0 * 0.5);\n+            importance *= positionScore;\n+\n+            return importance;\n+        }\n+\n+        private List<string> SplitIntoSentences(string text)\n+        {\n+            var sentences = new List<string>();\n+            var sentenceEndings = new[] { \". \", \"! \", \"? \", \".\\n\", \"!\\n\", \"?\\n\" };\n+            var currentSentence = string.Empty;\n+\n+            for (int i = 0; i < text.Length; i++)\n+            {\n+                currentSentence += text[i];\n+\n+                foreach (var ending in sentenceEndings)\n+                {\n+                    if (currentSentence.EndsWith(ending))\n+                    {\n+                        sentences.Add(currentSentence.Trim());\n+                        currentSentence = string.Empty;\n+                        break;\n+                    }",
    "path": "src/RetrievalAugmentedGeneration/ContextCompression/DocumentSummarizer.cs",
    "commit_id": "84f99e221d10d12f47a4fcccf4d1fffe7a2556f2",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "This foreach loop [implicitly filters its target sequence](1) - consider filtering the sequence explicitly using '.Where(...)'.\n```suggestion\n                if (sentenceEndings.Any(ending => currentSentence.EndsWith(ending)))\n                {\n                    sentences.Add(currentSentence.Trim());\n                    currentSentence = string.Empty;\n```",
    "created_at": "2025-11-03T23:02:03Z",
    "updated_at": "2025-11-03T23:02:18Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102054",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102054"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102054"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102054/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 138,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 145,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 145,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102063",
    "pull_request_review_id": 3413273577,
    "id": 2488102063,
    "node_id": "PRRC_kwDOKSXUF86UTWyv",
    "diff_hunk": "@@ -0,0 +1,148 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ContextCompression\n+{\n+    /// <summary>\n+    /// LLM-based context compression to reduce token usage while preserving key information.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class LLMContextCompressor<T>\n+    {\n+        private readonly INumericOperations<T> _numOps;\n+        private readonly string _llmEndpoint;\n+        private readonly string _apiKey;\n+        private readonly double _compressionRatio;\n+\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"LLMContextCompressor{T}\"/> class.\n+        /// </summary>\n+        /// <param name=\"numericOperations\">The numeric operations for type T.</param>\n+        /// <param name=\"compressionRatio\">The target compression ratio (0.0 to 1.0).</param>\n+        /// <param name=\"llmEndpoint\">The LLM API endpoint.</param>\n+        /// <param name=\"apiKey\">The API key for the LLM service.</param>\n+        public LLMContextCompressor(\n+            INumericOperations<T> numericOperations,\n+            double compressionRatio = 0.5,\n+            string llmEndpoint = \"\",\n+            string apiKey = \"\")\n+        {\n+            _numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\n+            _compressionRatio = compressionRatio >= 0 && compressionRatio <= 1\n+                ? compressionRatio\n+                : throw new ArgumentOutOfRangeException(nameof(compressionRatio), \"Compression ratio must be between 0 and 1\");\n+            _llmEndpoint = llmEndpoint;\n+            _apiKey = apiKey;\n+        }\n+\n+        /// <summary>\n+        /// Compresses a list of documents while preserving relevance to the query.\n+        /// </summary>\n+        /// <param name=\"query\">The query context.</param>\n+        /// <param name=\"documents\">The documents to compress.</param>\n+        /// <returns>A list of compressed documents.</returns>\n+        public List<Document<T>> Compress(string query, List<Document<T>> documents)\n+        {\n+            if (string.IsNullOrEmpty(query)) throw new ArgumentNullException(nameof(query));\n+            if (documents == null) throw new ArgumentNullException(nameof(documents));\n+\n+            var compressed = new List<Document<T>>();\n+\n+            foreach (var doc in documents)\n+            {\n+                var compressedContent = CompressText(query, doc.Content);\n+                var compressedDoc = new Document<T>\n+                {\n+                    Id = doc.Id,\n+                    Content = compressedContent,\n+                    Metadata = doc.Metadata,\n+                    RelevanceScore = doc.RelevanceScore\n+                };\n+                compressed.Add(compressedDoc);\n+            }\n+\n+            return compressed;\n+        }\n+\n+        /// <summary>\n+        /// Compresses text based on relevance to the query.\n+        /// </summary>\n+        /// <param name=\"query\">The query context.</param>\n+        /// <param name=\"text\">The text to compress.</param>\n+        /// <returns>The compressed text.</returns>\n+        public string CompressText(string query, string text)\n+        {\n+            if (string.IsNullOrEmpty(text)) return text;\n+\n+            var sentences = SplitIntoSentences(text);\n+            var scoredSentences = new List<(string sentence, double score)>();\n+\n+            foreach (var sentence in sentences)\n+            {\n+                var relevance = ComputeRelevance(query, sentence);\n+                scoredSentences.Add((sentence, relevance));\n+            }\n+\n+            var targetCount = Math.Max(1, (int)(sentences.Count * _compressionRatio));\n+            var topSentences = scoredSentences\n+                .OrderByDescending(x => x.score)\n+                .Take(targetCount)\n+                .OrderBy(x => sentences.IndexOf(x.sentence))\n+                .Select(x => x.sentence);\n+\n+            return string.Join(\" \", topSentences);\n+        }\n+\n+        private double ComputeRelevance(string query, string sentence)\n+        {\n+            var queryTokens = Tokenize(query);\n+            var sentenceTokens = Tokenize(sentence);\n+\n+            var overlap = queryTokens.Intersect(sentenceTokens).Count();\n+            var total = Math.Max(queryTokens.Count, sentenceTokens.Count);\n+\n+            return total > 0 ? (double)overlap / total : 0.0;\n+        }\n+\n+        private List<string> SplitIntoSentences(string text)\n+        {\n+            var sentences = new List<string>();\n+            var sentenceEndings = new[] { \". \", \"! \", \"? \", \".\\n\", \"!\\n\", \"?\\n\" };\n+            var currentSentence = string.Empty;\n+\n+            for (int i = 0; i < text.Length; i++)\n+            {\n+                currentSentence += text[i];\n+\n+                foreach (var ending in sentenceEndings)\n+                {\n+                    if (currentSentence.EndsWith(ending))\n+                    {\n+                        sentences.Add(currentSentence.Trim());\n+                        currentSentence = string.Empty;\n+                        break;\n+                    }",
    "path": "src/RetrievalAugmentedGeneration/ContextCompression/LLMContextCompressor.cs",
    "commit_id": "84f99e221d10d12f47a4fcccf4d1fffe7a2556f2",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "This foreach loop [implicitly filters its target sequence](1) - consider filtering the sequence explicitly using '.Where(...)'.\n```suggestion\n                var matchedEnding = sentenceEndings.FirstOrDefault(ending => currentSentence.EndsWith(ending));\n                if (matchedEnding != null)\n                {\n                    sentences.Add(currentSentence.Trim());\n                    currentSentence = string.Empty;\n```",
    "created_at": "2025-11-03T23:02:04Z",
    "updated_at": "2025-11-03T23:02:19Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102063",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102063"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102063"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102063/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 120,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 127,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 127,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102068",
    "pull_request_review_id": 3413273577,
    "id": 2488102068,
    "node_id": "PRRC_kwDOKSXUF86UTWy0",
    "diff_hunk": "@@ -0,0 +1,124 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores\n+{\n+    /// <summary>\n+    /// Hybrid document store combining vector and keyword search strategies.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class HybridDocumentStore<T> : DocumentStoreBase<T>\n+    {\n+        private readonly IDocumentStore<T> _vectorStore;\n+        private readonly IDocumentStore<T> _keywordStore;\n+        private readonly T _vectorWeight;\n+        private readonly T _keywordWeight;\n+\n+        public override int DocumentCount => _vectorStore.DocumentCount;\n+        public override int VectorDimension => _vectorStore.VectorDimension;\n+\n+        public HybridDocumentStore(\n+            IDocumentStore<T> vectorStore,\n+            IDocumentStore<T> keywordStore,\n+            T vectorWeight,\n+            T keywordWeight)\n+        {\n+            if (vectorStore == null)\n+                throw new ArgumentNullException(nameof(vectorStore));\n+            if (keywordStore == null)\n+                throw new ArgumentNullException(nameof(keywordStore));\n+\n+            _vectorStore = vectorStore;\n+            _keywordStore = keywordStore;\n+            _vectorWeight = vectorWeight;\n+            _keywordWeight = keywordWeight;\n+        }\n+\n+        protected override void AddCore(VectorDocument<T> vectorDocument)\n+        {\n+            _vectorStore.Add(vectorDocument);\n+            _keywordStore.Add(vectorDocument);\n+        }\n+\n+        protected override void AddBatchCore(IList<VectorDocument<T>> vectorDocuments)\n+        {\n+            _vectorStore.AddBatch(vectorDocuments);\n+            _keywordStore.AddBatch(vectorDocuments);\n+        }\n+\n+        protected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)\n+        {\n+            var vectorResults = _vectorStore.GetSimilarWithFilters(queryVector, topK * 2, metadataFilters);\n+            var keywordResults = _keywordStore.GetSimilarWithFilters(queryVector, topK * 2, metadataFilters);\n+\n+            var combinedScores = new Dictionary<string, T>();\n+\n+            foreach (var doc in vectorResults)\n+            {\n+                if (doc.HasRelevanceScore)\n+                {\n+                    var score = NumOps.Multiply(_vectorWeight, doc.RelevanceScore);\n+                    combinedScores[doc.Id] = score;\n+                }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/HybridDocumentStore.cs",
    "commit_id": "84f99e221d10d12f47a4fcccf4d1fffe7a2556f2",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "This foreach loop [implicitly filters its target sequence](1) - consider filtering the sequence explicitly using '.Where(...)'.\n```suggestion\n            foreach (var doc in vectorResults.Where(d => d.HasRelevanceScore))\n            {\n                var score = NumOps.Multiply(_vectorWeight, doc.RelevanceScore);\n                combinedScores[doc.Id] = score;\n```",
    "created_at": "2025-11-03T23:02:04Z",
    "updated_at": "2025-11-03T23:02:19Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102068",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102068"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102068"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102068/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 61,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 67,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 67,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102073",
    "pull_request_review_id": 3413273577,
    "id": 2488102073,
    "node_id": "PRRC_kwDOKSXUF86UTWy5",
    "diff_hunk": "@@ -0,0 +1,124 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores\n+{\n+    /// <summary>\n+    /// Hybrid document store combining vector and keyword search strategies.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class HybridDocumentStore<T> : DocumentStoreBase<T>\n+    {\n+        private readonly IDocumentStore<T> _vectorStore;\n+        private readonly IDocumentStore<T> _keywordStore;\n+        private readonly T _vectorWeight;\n+        private readonly T _keywordWeight;\n+\n+        public override int DocumentCount => _vectorStore.DocumentCount;\n+        public override int VectorDimension => _vectorStore.VectorDimension;\n+\n+        public HybridDocumentStore(\n+            IDocumentStore<T> vectorStore,\n+            IDocumentStore<T> keywordStore,\n+            T vectorWeight,\n+            T keywordWeight)\n+        {\n+            if (vectorStore == null)\n+                throw new ArgumentNullException(nameof(vectorStore));\n+            if (keywordStore == null)\n+                throw new ArgumentNullException(nameof(keywordStore));\n+\n+            _vectorStore = vectorStore;\n+            _keywordStore = keywordStore;\n+            _vectorWeight = vectorWeight;\n+            _keywordWeight = keywordWeight;\n+        }\n+\n+        protected override void AddCore(VectorDocument<T> vectorDocument)\n+        {\n+            _vectorStore.Add(vectorDocument);\n+            _keywordStore.Add(vectorDocument);\n+        }\n+\n+        protected override void AddBatchCore(IList<VectorDocument<T>> vectorDocuments)\n+        {\n+            _vectorStore.AddBatch(vectorDocuments);\n+            _keywordStore.AddBatch(vectorDocuments);\n+        }\n+\n+        protected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)\n+        {\n+            var vectorResults = _vectorStore.GetSimilarWithFilters(queryVector, topK * 2, metadataFilters);\n+            var keywordResults = _keywordStore.GetSimilarWithFilters(queryVector, topK * 2, metadataFilters);\n+\n+            var combinedScores = new Dictionary<string, T>();\n+\n+            foreach (var doc in vectorResults)\n+            {\n+                if (doc.HasRelevanceScore)\n+                {\n+                    var score = NumOps.Multiply(_vectorWeight, doc.RelevanceScore);\n+                    combinedScores[doc.Id] = score;\n+                }\n+            }\n+\n+            foreach (var doc in keywordResults)\n+            {\n+                if (doc.HasRelevanceScore)\n+                {\n+                    var keywordScore = NumOps.Multiply(_keywordWeight, doc.RelevanceScore);\n+                    if (combinedScores.ContainsKey(doc.Id))\n+                    {\n+                        combinedScores[doc.Id] = NumOps.Add(combinedScores[doc.Id], keywordScore);\n+                    }\n+                    else\n+                    {\n+                        combinedScores[doc.Id] = keywordScore;\n+                    }\n+                }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/HybridDocumentStore.cs",
    "commit_id": "84f99e221d10d12f47a4fcccf4d1fffe7a2556f2",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "This foreach loop [implicitly filters its target sequence](1) - consider filtering the sequence explicitly using '.Where(...)'.\n```suggestion\n            foreach (var doc in vectorResults.Where(doc => doc.HasRelevanceScore))\n            {\n                var score = NumOps.Multiply(_vectorWeight, doc.RelevanceScore);\n                combinedScores[doc.Id] = score;\n            }\n\n            foreach (var doc in keywordResults.Where(doc => doc.HasRelevanceScore))\n            {\n                var keywordScore = NumOps.Multiply(_keywordWeight, doc.RelevanceScore);\n                if (combinedScores.ContainsKey(doc.Id))\n                {\n                    combinedScores[doc.Id] = NumOps.Add(combinedScores[doc.Id], keywordScore);\n                }\n                else\n                {\n                    combinedScores[doc.Id] = keywordScore;\n                }\n```",
    "created_at": "2025-11-03T23:02:04Z",
    "updated_at": "2025-11-03T23:02:19Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102073",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102073"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102073"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102073/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 61,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 83,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 83,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102076",
    "pull_request_review_id": 3413273577,
    "id": 2488102076,
    "node_id": "PRRC_kwDOKSXUF86UTWy8",
    "diff_hunk": "@@ -0,0 +1,206 @@\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Generators;\n+\n+/// <summary>\n+/// Base class for generator implementations providing common functionality and validation.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for relevance scoring.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// This base class provides standard validation, prompt construction, and citation handling\n+/// for generator implementations. It defines the template for implementing custom generation logic.\n+/// </para>\n+/// <para><b>For Beginners:</b> This is the foundation for all text generators in RAG systems.\n+/// \n+/// It handles common tasks so you don't have to repeat them:\n+/// - Checking that inputs aren't null or empty\n+/// - Building prompts that combine the query and retrieved documents\n+/// - Extracting citations from generated text\n+/// - Creating properly formatted answers\n+/// \n+/// When you create a new generator (like OpenAIGenerator or OnnxGenerator):\n+/// 1. Inherit from this class\n+/// 2. Set MaxContextTokens and MaxGenerationTokens in the constructor\n+/// 3. Implement GenerateCore with your specific generation logic\n+/// 4. Everything else (validation, prompt formatting, citations) is handled automatically\n+/// </para>\n+/// </remarks>\n+public abstract class GeneratorBase<T> : IGenerator<T>\n+{\n+    /// <summary>\n+    /// Gets the maximum number of tokens this generator can process in a single request.\n+    /// </summary>\n+    public int MaxContextTokens { get; protected set; }\n+\n+    /// <summary>\n+    /// Gets the maximum number of tokens this generator can generate in a response.\n+    /// </summary>\n+    public int MaxGenerationTokens { get; protected set; }\n+\n+    /// <summary>\n+    /// Initializes a new instance of the GeneratorBase class.\n+    /// </summary>\n+    /// <param name=\"maxContextTokens\">The maximum context window size in tokens.</param>\n+    /// <param name=\"maxGenerationTokens\">The maximum number of tokens to generate.</param>\n+    protected GeneratorBase(int maxContextTokens, int maxGenerationTokens)\n+    {\n+        if (maxContextTokens <= 0)\n+        {\n+            throw new ArgumentException(\"Maximum context tokens must be positive.\", nameof(maxContextTokens));\n+        }\n+\n+        if (maxGenerationTokens <= 0)\n+        {\n+            throw new ArgumentException(\"Maximum generation tokens must be positive.\", nameof(maxGenerationTokens));\n+        }\n+\n+        MaxContextTokens = maxContextTokens;\n+        MaxGenerationTokens = maxGenerationTokens;\n+    }\n+\n+    /// <summary>\n+    /// Generates a text response based on a prompt with validation.\n+    /// </summary>\n+    /// <param name=\"prompt\">The input prompt or question.</param>\n+    /// <returns>The generated text response.</returns>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when prompt is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when prompt is empty or whitespace.</exception>\n+    public string Generate(string prompt)\n+    {\n+        if (prompt == null)\n+        {\n+            throw new ArgumentNullException(nameof(prompt), \"Prompt cannot be null.\");\n+        }\n+\n+        if (string.IsNullOrWhiteSpace(prompt))\n+        {\n+            throw new ArgumentException(\"Prompt cannot be empty or whitespace.\", nameof(prompt));\n+        }\n+\n+        return GenerateCore(prompt);\n+    }\n+\n+    /// <summary>\n+    /// Generates a grounded answer using provided context documents.\n+    /// </summary>\n+    /// <param name=\"query\">The user's original query or question.</param>\n+    /// <param name=\"context\">The retrieved documents providing context for the answer.</param>\n+    /// <returns>A grounded answer with the generated text, source documents, and extracted citations.</returns>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when query or context is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when query is empty or context has no documents.</exception>\n+    public GroundedAnswer<T> GenerateGrounded(string query, IEnumerable<Document<T>> context)\n+    {\n+        if (query == null)\n+        {\n+            throw new ArgumentNullException(nameof(query), \"Query cannot be null.\");\n+        }\n+\n+        if (string.IsNullOrWhiteSpace(query))\n+        {\n+            throw new ArgumentException(\"Query cannot be empty or whitespace.\", nameof(query));\n+        }\n+\n+        if (context == null)\n+        {\n+            throw new ArgumentNullException(nameof(context), \"Context cannot be null.\");\n+        }\n+\n+        var contextList = context.ToList();\n+        if (contextList.Count == 0)\n+        {\n+            throw new ArgumentException(\"Context must contain at least one document.\", nameof(context));\n+        }\n+\n+        // Build the prompt with context\n+        var prompt = BuildPromptWithContext(query, contextList);\n+\n+        // Generate the answer\n+        var generatedText = GenerateCore(prompt);\n+\n+        // Extract citations from the generated text\n+        var citations = ExtractCitations(generatedText, contextList);\n+\n+        return new GroundedAnswer<T>\n+        {\n+            Answer = generatedText,\n+            SourceDocuments = contextList.AsReadOnly(),\n+            Citations = citations.Values.Select(d => d.Id).ToList().AsReadOnly()\n+        };\n+    }\n+\n+    /// <summary>\n+    /// Core generation logic to be implemented by derived classes.\n+    /// </summary>\n+    /// <param name=\"prompt\">The validated prompt string.</param>\n+    /// <returns>The generated text response.</returns>\n+    protected abstract string GenerateCore(string prompt);\n+\n+    /// <summary>\n+    /// Builds a prompt that incorporates the query and retrieved context documents.\n+    /// </summary>\n+    /// <param name=\"query\">The user's query.</param>\n+    /// <param name=\"context\">The retrieved context documents.</param>\n+    /// <returns>A formatted prompt string.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method can be overridden to customize prompt formatting. The default\n+    /// implementation creates a structured prompt with numbered context documents\n+    /// followed by the user's question.\n+    /// </para>\n+    /// </remarks>\n+    protected virtual string BuildPromptWithContext(string query, List<Document<T>> context)\n+    {\n+        var promptBuilder = new System.Text.StringBuilder();\n+        promptBuilder.AppendLine(\"Answer the following question based on the provided context. Include citations using [1], [2], etc. to reference the source documents.\");\n+        promptBuilder.AppendLine();\n+        promptBuilder.AppendLine(\"Context:\");\n+\n+        for (int i = 0; i < context.Count; i++)\n+        {\n+            promptBuilder.AppendLine($\"[{i + 1}] {context[i].Content}\");\n+            promptBuilder.AppendLine();\n+        }\n+\n+        promptBuilder.AppendLine(\"Question:\");\n+        promptBuilder.AppendLine(query);\n+        promptBuilder.AppendLine();\n+        promptBuilder.Append(\"Answer:\");\n+\n+        return promptBuilder.ToString();\n+    }\n+\n+    /// <summary>\n+    /// Extracts citation markers from the generated text and maps them to source documents.\n+    /// </summary>\n+    /// <param name=\"generatedText\">The generated text containing citations.</param>\n+    /// <param name=\"sourceDocuments\">The source documents that were used for generation.</param>\n+    /// <returns>A dictionary mapping citation indices to documents.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method can be overridden to customize citation extraction logic. The default\n+    /// implementation looks for patterns like [1], [2], etc. in the generated text.\n+    /// </para>\n+    /// </remarks>\n+    protected virtual Dictionary<int, Document<T>> ExtractCitations(string generatedText, List<Document<T>> sourceDocuments)\n+    {\n+        var citations = new Dictionary<int, Document<T>>();\n+        var citationPattern = new System.Text.RegularExpressions.Regex(@\"\\[(\\d+)\\]\");\n+        var matches = citationPattern.Matches(generatedText);\n+\n+        foreach (System.Text.RegularExpressions.Match match in matches)\n+        {\n+            if (int.TryParse(match.Groups[1].Value, out int citationIndex))\n+            {\n+                var docIndex = citationIndex - 1; // Convert to 0-based index\n+                if (docIndex >= 0 && docIndex < sourceDocuments.Count)\n+                {\n+                    citations[citationIndex] = sourceDocuments[docIndex];\n+                }\n+            }\n+        }",
    "path": "src/RetrievalAugmentedGeneration/Generators/GeneratorBase.cs",
    "commit_id": "72f2fd674bf9265ffd09a3923f36851f474d2c8c",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "This foreach loop [implicitly filters its target sequence](1) - consider filtering the sequence explicitly using '.Where(...)'.",
    "created_at": "2025-11-03T23:02:05Z",
    "updated_at": "2025-11-03T23:02:19Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102076",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102076"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102076"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102076/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 192,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 202,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 202,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102083",
    "pull_request_review_id": 3413273577,
    "id": 2488102083,
    "node_id": "PRRC_kwDOKSXUF86UTWzD",
    "diff_hunk": "@@ -0,0 +1,122 @@\n+using System;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System.Collections.Generic;\n+using System.Linq;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.RerankingStrategies\n+{\n+    /// <summary>\n+    /// Reciprocal Rank Fusion for combining multiple ranking lists.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class ReciprocalRankFusion<T> : RerankingStrategyBase<T>\n+    {\n+        private readonly INumericOperations<T> _numOps;\n+        private readonly int _k;\n+\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"ReciprocalRankFusion{T}\"/> class.\n+        /// </summary>\n+        /// <param name=\"numericOperations\">The numeric operations for type T.</param>\n+        /// <param name=\"k\">The constant k for reciprocal rank formula (default: 60).</param>\n+        public ReciprocalRankFusion(INumericOperations<T> numericOperations, int k = 60) : base(numericOperations)\n+        {\n+            _numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\n+            _k = k > 0 ? k : throw new ArgumentOutOfRangeException(nameof(k));\n+        }\n+\n+        /// <summary>\n+        /// Reranks documents using reciprocal rank fusion.\n+        /// </summary>\n+        /// <param name=\"query\">The query string.</param>\n+        /// <param name=\"documents\">The documents to rerank.</param>\n+        /// <param name=\"topK\">The number of top documents to return.</param>\n+        /// <returns>A reranked list of documents.</returns>\n+        public override List<Document<T>> Rerank(string query, List<Document<T>> documents, int topK)\n+        {\n+            if (string.IsNullOrEmpty(query)) throw new ArgumentNullException(nameof(query));\n+            if (documents == null) throw new ArgumentNullException(nameof(documents));\n+            if (topK <= 0) throw new ArgumentOutOfRangeException(nameof(topK));\n+\n+            var scores = new Dictionary<string, T>();\n+\n+            for (int rank = 0; rank < documents.Count; rank++)\n+            {\n+                var doc = documents[rank];\n+                var rrfScore = _numOps.FromDouble(1.0 / (_k + rank + 1));\n+\n+                if (scores.ContainsKey(doc.Id))\n+                {\n+                    scores[doc.Id] = _numOps.Add(scores[doc.Id], rrfScore);\n+                }\n+                else\n+                {\n+                    scores[doc.Id] = rrfScore;\n+                }\n+            }\n+\n+            var reranked = documents\n+                .OrderByDescending(d => Convert.ToDouble(scores.ContainsKey(d.Id) ? scores[d.Id] : _numOps.Zero))\n+                .Take(topK)\n+                .ToList();\n+\n+            foreach (var doc in reranked)\n+            {\n+                if (scores.ContainsKey(doc.Id))\n+                {\n+                    doc.RelevanceScore = scores[doc.Id];\n+                }\n+            }",
    "path": "src/RetrievalAugmentedGeneration/RerankingStrategies/ReciprocalRankFusion.cs",
    "commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "This foreach loop [implicitly filters its target sequence](1) - consider filtering the sequence explicitly using '.Where(...)'.",
    "created_at": "2025-11-03T23:02:05Z",
    "updated_at": "2025-11-03T23:02:19Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102083",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102083"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102083"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102083/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 64,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 70,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 70,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102087",
    "pull_request_review_id": 3413273577,
    "id": 2488102087,
    "node_id": "PRRC_kwDOKSXUF86UTWzH",
    "diff_hunk": "@@ -0,0 +1,124 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\n+{\n+    /// <summary>\n+    /// BM25 (Best Matching 25) retrieval algorithm for sparse keyword-based search.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class BM25Retriever<T> : RetrieverBase<T>\n+    {\n+        private readonly IDocumentStore<T> _documentStore;\n+        private readonly T _k1;\n+        private readonly T _b;\n+        private readonly Dictionary<string, Dictionary<string, T>> _termFrequencies;\n+        private readonly Dictionary<string, T> _documentLengths;\n+        private T _avgDocLength;\n+\n+        public BM25Retriever(IDocumentStore<T> documentStore, int defaultTopK = 5, double k1 = 1.5, double b = 0.75) \n+            : base(defaultTopK)\n+        {\n+            if (documentStore == null)\n+                throw new ArgumentNullException(nameof(documentStore));\n+\n+            _documentStore = documentStore;\n+            _k1 = NumOps.FromDouble(k1);\n+            _b = NumOps.FromDouble(b);\n+            _termFrequencies = new Dictionary<string, Dictionary<string, T>>();\n+            _documentLengths = new Dictionary<string, T>();\n+            _avgDocLength = NumOps.Zero;\n+        }\n+\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\n+        {\n+            var queryTerms = Tokenize(query);\n+            var scores = new Dictionary<string, T>();\n+\n+            var candidates = _documentStore.GetSimilar(\n+                new AiDotNet.LinearAlgebra.Vector<T>(new T[Math.Max(1, _documentStore.VectorDimension)]),\n+                _documentStore.DocumentCount\n+            );\n+\n+            foreach (var doc in candidates)\n+            {\n+                if (!MatchesFilters(doc, metadataFilters))\n+                    continue;\n+\n+                var score = NumOps.Zero;\n+                \n+                foreach (var term in queryTerms)\n+                {\n+                    var termScore = CalculateBM25Term(doc.Id, term);\n+                    score = NumOps.Add(score, termScore);\n+                }\n+\n+                scores[doc.Id] = score;\n+            }",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/BM25Retriever.cs",
    "commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "This foreach loop [implicitly filters its target sequence](1) - consider filtering the sequence explicitly using '.Where(...)'.",
    "created_at": "2025-11-03T23:02:05Z",
    "updated_at": "2025-11-03T23:02:19Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102087",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102087"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102087"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102087/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 47,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 61,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 61,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102094",
    "pull_request_review_id": 3413273577,
    "id": 2488102094,
    "node_id": "PRRC_kwDOKSXUF86UTWzO",
    "diff_hunk": "@@ -0,0 +1,92 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\n+{\n+    /// <summary>\n+    /// Hybrid retriever combining dense and sparse retrieval strategies.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class HybridRetriever<T> : RetrieverBase<T>\n+    {\n+        private readonly IRetriever<T> _denseRetriever;\n+        private readonly IRetriever<T> _sparseRetriever;\n+        private readonly T _denseWeight;\n+        private readonly T _sparseWeight;\n+\n+        public HybridRetriever(\n+            IRetriever<T> denseRetriever, \n+            IRetriever<T> sparseRetriever, \n+            double denseWeight = 0.7, \n+            double sparseWeight = 0.3,\n+            int defaultTopK = 5) \n+            : base(defaultTopK)\n+        {\n+            if (denseRetriever == null)\n+                throw new ArgumentNullException(nameof(denseRetriever));\n+            if (sparseRetriever == null)\n+                throw new ArgumentNullException(nameof(sparseRetriever));\n+\n+            _denseRetriever = denseRetriever;\n+            _sparseRetriever = sparseRetriever;\n+            _denseWeight = NumOps.FromDouble(denseWeight);\n+            _sparseWeight = NumOps.FromDouble(sparseWeight);\n+        }\n+\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\n+        {\n+            var denseResults = _denseRetriever.Retrieve(query, topK * 2, metadataFilters);\n+            var sparseResults = _sparseRetriever.Retrieve(query, topK * 2, metadataFilters);\n+\n+            var combinedScores = new Dictionary<string, T>();\n+\n+            foreach (var doc in denseResults)\n+            {\n+                if (doc.HasRelevanceScore)\n+                {\n+                    var score = NumOps.Multiply(_denseWeight, doc.RelevanceScore);\n+                    combinedScores[doc.Id] = score;\n+                }\n+            }",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/HybridRetriever.cs",
    "commit_id": "84f99e221d10d12f47a4fcccf4d1fffe7a2556f2",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "This foreach loop [implicitly filters its target sequence](1) - consider filtering the sequence explicitly using '.Where(...)'.",
    "created_at": "2025-11-03T23:02:05Z",
    "updated_at": "2025-11-03T23:02:19Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102094",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102094"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102094"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102094/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 47,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 54,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 54,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102099",
    "pull_request_review_id": 3413273577,
    "id": 2488102099,
    "node_id": "PRRC_kwDOKSXUF86UTWzT",
    "diff_hunk": "@@ -0,0 +1,92 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\n+{\n+    /// <summary>\n+    /// Hybrid retriever combining dense and sparse retrieval strategies.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class HybridRetriever<T> : RetrieverBase<T>\n+    {\n+        private readonly IRetriever<T> _denseRetriever;\n+        private readonly IRetriever<T> _sparseRetriever;\n+        private readonly T _denseWeight;\n+        private readonly T _sparseWeight;\n+\n+        public HybridRetriever(\n+            IRetriever<T> denseRetriever, \n+            IRetriever<T> sparseRetriever, \n+            double denseWeight = 0.7, \n+            double sparseWeight = 0.3,\n+            int defaultTopK = 5) \n+            : base(defaultTopK)\n+        {\n+            if (denseRetriever == null)\n+                throw new ArgumentNullException(nameof(denseRetriever));\n+            if (sparseRetriever == null)\n+                throw new ArgumentNullException(nameof(sparseRetriever));\n+\n+            _denseRetriever = denseRetriever;\n+            _sparseRetriever = sparseRetriever;\n+            _denseWeight = NumOps.FromDouble(denseWeight);\n+            _sparseWeight = NumOps.FromDouble(sparseWeight);\n+        }\n+\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\n+        {\n+            var denseResults = _denseRetriever.Retrieve(query, topK * 2, metadataFilters);\n+            var sparseResults = _sparseRetriever.Retrieve(query, topK * 2, metadataFilters);\n+\n+            var combinedScores = new Dictionary<string, T>();\n+\n+            foreach (var doc in denseResults)\n+            {\n+                if (doc.HasRelevanceScore)\n+                {\n+                    var score = NumOps.Multiply(_denseWeight, doc.RelevanceScore);\n+                    combinedScores[doc.Id] = score;\n+                }\n+            }\n+\n+            foreach (var doc in sparseResults)\n+            {\n+                if (doc.HasRelevanceScore)\n+                {\n+                    var sparseScore = NumOps.Multiply(_sparseWeight, doc.RelevanceScore);\n+                    if (combinedScores.ContainsKey(doc.Id))\n+                    {\n+                        combinedScores[doc.Id] = NumOps.Add(combinedScores[doc.Id], sparseScore);\n+                    }\n+                    else\n+                    {\n+                        combinedScores[doc.Id] = sparseScore;\n+                    }\n+                }\n+            }",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/HybridRetriever.cs",
    "commit_id": "84f99e221d10d12f47a4fcccf4d1fffe7a2556f2",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "This foreach loop [implicitly filters its target sequence](1) - consider filtering the sequence explicitly using '.Where(...)'.",
    "created_at": "2025-11-03T23:02:06Z",
    "updated_at": "2025-11-03T23:02:19Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102099",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102099"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102099"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102099/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 56,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 70,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 70,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102103",
    "pull_request_review_id": 3413273577,
    "id": 2488102103,
    "node_id": "PRRC_kwDOKSXUF86UTWzX",
    "diff_hunk": "@@ -0,0 +1,83 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\n+{\n+    /// <summary>\n+    /// Multi-query retriever that generates multiple query variations and merges results.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class MultiQueryRetriever<T> : RetrieverBase<T>\n+    {\n+        private readonly IRetriever<T> _baseRetriever;\n+        private readonly int _numQueries;\n+\n+        public MultiQueryRetriever(IRetriever<T> baseRetriever, int numQueries = 3, int defaultTopK = 5) : base(defaultTopK)\n+        {\n+            if (baseRetriever == null)\n+                throw new ArgumentNullException(nameof(baseRetriever));\n+            if (numQueries <= 0)\n+                throw new ArgumentException(\"Number of queries must be positive\", nameof(numQueries));\n+\n+            _baseRetriever = baseRetriever;\n+            _numQueries = numQueries;\n+        }\n+\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\n+        {\n+            var queries = GenerateQueries(query);\n+            var allResults = new Dictionary<string, (Document<T> doc, T score)>();\n+\n+            foreach (var q in queries)\n+            {\n+                var results = _baseRetriever.Retrieve(q, topK, metadataFilters);\n+\n+                foreach (var doc in results)\n+                {\n+                    if (doc.HasRelevanceScore)\n+                    {\n+                        if (allResults.ContainsKey(doc.Id))\n+                        {\n+                            var existing = allResults[doc.Id];\n+                            var newScore = NumOps.Add(existing.score, doc.RelevanceScore);\n+                            allResults[doc.Id] = (doc, newScore);\n+                        }\n+                        else\n+                        {\n+                            allResults[doc.Id] = (doc, doc.RelevanceScore);\n+                        }\n+                    }\n+                }",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/MultiQueryRetriever.cs",
    "commit_id": "84f99e221d10d12f47a4fcccf4d1fffe7a2556f2",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "This foreach loop [implicitly filters its target sequence](1) - consider filtering the sequence explicitly using '.Where(...)'.",
    "created_at": "2025-11-03T23:02:06Z",
    "updated_at": "2025-11-03T23:02:19Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102103",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102103"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102103"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102103/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 39,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 54,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 54,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102108",
    "pull_request_review_id": 3413273577,
    "id": 2488102108,
    "node_id": "PRRC_kwDOKSXUF86UTWzc",
    "diff_hunk": "@@ -0,0 +1,108 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\n+{\n+    /// <summary>\n+    /// TF-IDF (Term Frequency-Inverse Document Frequency) retrieval strategy.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class TFIDFRetriever<T> : RetrieverBase<T>\n+    {\n+        private readonly IDocumentStore<T> _documentStore;\n+        private readonly Dictionary<string, Dictionary<string, T>> _tfidf;\n+        private readonly Dictionary<string, T> _idf;\n+\n+        public TFIDFRetriever(IDocumentStore<T> documentStore, int defaultTopK = 5) : base(defaultTopK)\n+        {\n+            if (documentStore == null)\n+                throw new ArgumentNullException(nameof(documentStore));\n+                \n+            _documentStore = documentStore;\n+            _tfidf = new Dictionary<string, Dictionary<string, T>>();\n+            _idf = new Dictionary<string, T>();\n+        }\n+\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\n+        {\n+            var queryTerms = Tokenize(query);\n+            var scores = new Dictionary<string, T>();\n+\n+            var candidates = _documentStore.GetSimilar(\n+                new AiDotNet.LinearAlgebra.Vector<T>(new T[_documentStore.VectorDimension]), \n+                _documentStore.DocumentCount\n+            );\n+\n+            foreach (var doc in candidates)\n+            {\n+                if (!MatchesFilters(doc, metadataFilters))\n+                    continue;\n+\n+                var score = NumOps.Zero;\n+\n+                if (_tfidf.ContainsKey(doc.Id))\n+                {\n+                    var docTfidf = _tfidf[doc.Id];\n+                    foreach (var term in queryTerms)\n+                    {\n+                        if (docTfidf.ContainsKey(term))\n+                        {\n+                            score = NumOps.Add(score, docTfidf[term]);\n+                        }\n+                    }\n+                }\n+\n+                scores[doc.Id] = score;\n+            }",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/TFIDFRetriever.cs",
    "commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "This foreach loop [implicitly filters its target sequence](1) - consider filtering the sequence explicitly using '.Where(...)'.",
    "created_at": "2025-11-03T23:02:06Z",
    "updated_at": "2025-11-03T23:02:19Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102108",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102108"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102108"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102108/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 40,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 60,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 60,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102115",
    "pull_request_review_id": 3413273577,
    "id": 2488102115,
    "node_id": "PRRC_kwDOKSXUF86UTWzj",
    "diff_hunk": "@@ -0,0 +1,108 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\n+{\n+    /// <summary>\n+    /// TF-IDF (Term Frequency-Inverse Document Frequency) retrieval strategy.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class TFIDFRetriever<T> : RetrieverBase<T>\n+    {\n+        private readonly IDocumentStore<T> _documentStore;\n+        private readonly Dictionary<string, Dictionary<string, T>> _tfidf;\n+        private readonly Dictionary<string, T> _idf;\n+\n+        public TFIDFRetriever(IDocumentStore<T> documentStore, int defaultTopK = 5) : base(defaultTopK)\n+        {\n+            if (documentStore == null)\n+                throw new ArgumentNullException(nameof(documentStore));\n+                \n+            _documentStore = documentStore;\n+            _tfidf = new Dictionary<string, Dictionary<string, T>>();\n+            _idf = new Dictionary<string, T>();\n+        }\n+\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\n+        {\n+            var queryTerms = Tokenize(query);\n+            var scores = new Dictionary<string, T>();\n+\n+            var candidates = _documentStore.GetSimilar(\n+                new AiDotNet.LinearAlgebra.Vector<T>(new T[_documentStore.VectorDimension]), \n+                _documentStore.DocumentCount\n+            );\n+\n+            foreach (var doc in candidates)\n+            {\n+                if (!MatchesFilters(doc, metadataFilters))\n+                    continue;\n+\n+                var score = NumOps.Zero;\n+\n+                if (_tfidf.ContainsKey(doc.Id))\n+                {\n+                    var docTfidf = _tfidf[doc.Id];\n+                    foreach (var term in queryTerms)\n+                    {\n+                        if (docTfidf.ContainsKey(term))\n+                        {\n+                            score = NumOps.Add(score, docTfidf[term]);\n+                        }\n+                    }",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/TFIDFRetriever.cs",
    "commit_id": "84f99e221d10d12f47a4fcccf4d1fffe7a2556f2",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "This foreach loop [implicitly filters its target sequence](1) - consider filtering the sequence explicitly using '.Where(...)'.",
    "created_at": "2025-11-03T23:02:06Z",
    "updated_at": "2025-11-03T23:02:19Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102115",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102115"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102115"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102115/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 50,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 56,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 56,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102118",
    "pull_request_review_id": 3413273577,
    "id": 2488102118,
    "node_id": "PRRC_kwDOKSXUF86UTWzm",
    "diff_hunk": "@@ -0,0 +1,124 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\n+{\n+    /// <summary>\n+    /// BM25 (Best Matching 25) retrieval algorithm for sparse keyword-based search.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class BM25Retriever<T> : RetrieverBase<T>\n+    {\n+        private readonly IDocumentStore<T> _documentStore;\n+        private readonly T _k1;\n+        private readonly T _b;\n+        private readonly Dictionary<string, Dictionary<string, T>> _termFrequencies;\n+        private readonly Dictionary<string, T> _documentLengths;\n+        private T _avgDocLength;\n+\n+        public BM25Retriever(IDocumentStore<T> documentStore, int defaultTopK = 5, double k1 = 1.5, double b = 0.75) \n+            : base(defaultTopK)\n+        {\n+            if (documentStore == null)\n+                throw new ArgumentNullException(nameof(documentStore));\n+\n+            _documentStore = documentStore;\n+            _k1 = NumOps.FromDouble(k1);\n+            _b = NumOps.FromDouble(b);\n+            _termFrequencies = new Dictionary<string, Dictionary<string, T>>();\n+            _documentLengths = new Dictionary<string, T>();\n+            _avgDocLength = NumOps.Zero;\n+        }\n+\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\n+        {\n+            var queryTerms = Tokenize(query);\n+            var scores = new Dictionary<string, T>();\n+\n+            var candidates = _documentStore.GetSimilar(\n+                new AiDotNet.LinearAlgebra.Vector<T>(new T[Math.Max(1, _documentStore.VectorDimension)]),\n+                _documentStore.DocumentCount\n+            );\n+\n+            foreach (var doc in candidates)\n+            {\n+                if (!MatchesFilters(doc, metadataFilters))\n+                    continue;\n+\n+                var score = NumOps.Zero;\n+                \n+                foreach (var term in queryTerms)\n+                {\n+                    var termScore = CalculateBM25Term(doc.Id, term);\n+                    score = NumOps.Add(score, termScore);\n+                }",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/BM25Retriever.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "This foreach loop immediately [maps its iteration variable to another variable](1) - consider mapping the sequence explicitly using '.Select(...)'.",
    "created_at": "2025-11-03T23:02:07Z",
    "updated_at": "2025-11-03T23:02:19Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102118",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102118"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102118"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102118/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 43,
    "original_start_line": 54,
    "start_side": "RIGHT",
    "line": 47,
    "original_line": 58,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 58,
    "position": 47,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102126",
    "pull_request_review_id": 3413273577,
    "id": 2488102126,
    "node_id": "PRRC_kwDOKSXUF86UTWzu",
    "diff_hunk": "@@ -0,0 +1,83 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\n+{\n+    /// <summary>\n+    /// Multi-query retriever that generates multiple query variations and merges results.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class MultiQueryRetriever<T> : RetrieverBase<T>\n+    {\n+        private readonly IRetriever<T> _baseRetriever;\n+        private readonly int _numQueries;\n+\n+        public MultiQueryRetriever(IRetriever<T> baseRetriever, int numQueries = 3, int defaultTopK = 5) : base(defaultTopK)\n+        {\n+            if (baseRetriever == null)\n+                throw new ArgumentNullException(nameof(baseRetriever));\n+            if (numQueries <= 0)\n+                throw new ArgumentException(\"Number of queries must be positive\", nameof(numQueries));\n+\n+            _baseRetriever = baseRetriever;\n+            _numQueries = numQueries;\n+        }\n+\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\n+        {\n+            var queries = GenerateQueries(query);\n+            var allResults = new Dictionary<string, (Document<T> doc, T score)>();\n+\n+            foreach (var q in queries)\n+            {\n+                var results = _baseRetriever.Retrieve(q, topK, metadataFilters);\n+\n+                foreach (var doc in results)\n+                {\n+                    if (doc.HasRelevanceScore)\n+                    {\n+                        if (allResults.ContainsKey(doc.Id))\n+                        {\n+                            var existing = allResults[doc.Id];\n+                            var newScore = NumOps.Add(existing.score, doc.RelevanceScore);\n+                            allResults[doc.Id] = (doc, newScore);\n+                        }\n+                        else\n+                        {\n+                            allResults[doc.Id] = (doc, doc.RelevanceScore);\n+                        }\n+                    }\n+                }\n+            }",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/MultiQueryRetriever.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "This foreach loop immediately [maps its iteration variable to another variable](1) - consider mapping the sequence explicitly using '.Select(...)'.",
    "created_at": "2025-11-03T23:02:07Z",
    "updated_at": "2025-11-03T23:02:19Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102126",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102126"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102126"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102126/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 35,
    "original_start_line": 35,
    "start_side": "RIGHT",
    "line": 51,
    "original_line": 55,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 55,
    "position": 51,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102130",
    "pull_request_review_id": 3413273577,
    "id": 2488102130,
    "node_id": "PRRC_kwDOKSXUF86UTWzy",
    "diff_hunk": "@@ -0,0 +1,108 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\n+{\n+    /// <summary>\n+    /// TF-IDF (Term Frequency-Inverse Document Frequency) retrieval strategy.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class TFIDFRetriever<T> : RetrieverBase<T>\n+    {\n+        private readonly IDocumentStore<T> _documentStore;\n+        private readonly Dictionary<string, Dictionary<string, T>> _tfidf;\n+        private readonly Dictionary<string, T> _idf;",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/TFIDFRetriever.cs",
    "commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "The contents of this container are never accessed.",
    "created_at": "2025-11-03T23:02:07Z",
    "updated_at": "2025-11-03T23:02:19Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102130",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102130"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102130"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102130/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": null,
    "original_line": 18,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 18,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102137",
    "pull_request_review_id": 3413273577,
    "id": 2488102137,
    "node_id": "PRRC_kwDOKSXUF86UTWz5",
    "diff_hunk": "@@ -0,0 +1,124 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\n+{\n+    /// <summary>\n+    /// BM25 (Best Matching 25) retrieval algorithm for sparse keyword-based search.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class BM25Retriever<T> : RetrieverBase<T>\n+    {\n+        private readonly IDocumentStore<T> _documentStore;\n+        private readonly T _k1;\n+        private readonly T _b;\n+        private readonly Dictionary<string, Dictionary<string, T>> _termFrequencies;\n+        private readonly Dictionary<string, T> _documentLengths;",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/BM25Retriever.cs",
    "commit_id": "3f01d96a3a3d3acc6002ef29bdfdec69bfddc09e",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "The contents of this container are never initialized.",
    "created_at": "2025-11-03T23:02:07Z",
    "updated_at": "2025-11-03T23:02:19Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102137",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102137"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102137"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102137/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": null,
    "original_line": 20,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 20,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102145",
    "pull_request_review_id": 3413273577,
    "id": 2488102145,
    "node_id": "PRRC_kwDOKSXUF86UTW0B",
    "diff_hunk": "@@ -0,0 +1,124 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\n+{\n+    /// <summary>\n+    /// BM25 (Best Matching 25) retrieval algorithm for sparse keyword-based search.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class BM25Retriever<T> : RetrieverBase<T>\n+    {\n+        private readonly IDocumentStore<T> _documentStore;\n+        private readonly T _k1;\n+        private readonly T _b;\n+        private readonly Dictionary<string, Dictionary<string, T>> _termFrequencies;",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/BM25Retriever.cs",
    "commit_id": "3f01d96a3a3d3acc6002ef29bdfdec69bfddc09e",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "The contents of this container are never initialized.",
    "created_at": "2025-11-03T23:02:08Z",
    "updated_at": "2025-11-03T23:02:19Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102145",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102145"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102145"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102145/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": null,
    "original_line": 19,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 19,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102152",
    "pull_request_review_id": 3413273577,
    "id": 2488102152,
    "node_id": "PRRC_kwDOKSXUF86UTW0I",
    "diff_hunk": "@@ -0,0 +1,108 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\n+{\n+    /// <summary>\n+    /// TF-IDF (Term Frequency-Inverse Document Frequency) retrieval strategy.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class TFIDFRetriever<T> : RetrieverBase<T>\n+    {\n+        private readonly IDocumentStore<T> _documentStore;\n+        private readonly Dictionary<string, Dictionary<string, T>> _tfidf;",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/TFIDFRetriever.cs",
    "commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "The contents of this container are never initialized.",
    "created_at": "2025-11-03T23:02:08Z",
    "updated_at": "2025-11-03T23:02:19Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102152",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102152"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102152"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102152/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": null,
    "original_line": 17,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 17,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102155",
    "pull_request_review_id": 3413273577,
    "id": 2488102155,
    "node_id": "PRRC_kwDOKSXUF86UTW0L",
    "diff_hunk": "@@ -0,0 +1,73 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\n+using System;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels\n+{\n+    /// <summary>\n+    /// HuggingFace-based embedding model for generating embeddings.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class HuggingFaceEmbeddingModel<T> : EmbeddingModelBase<T>\n+    {\n+        private readonly string _modelName;\n+        private readonly string _apiKey;\n+        private readonly int _dimension;\n+        private readonly int _maxTokens;\n+\n+        public override int EmbeddingDimension => _dimension;\n+        public override int MaxTokens => _maxTokens;\n+\n+        public HuggingFaceEmbeddingModel(string modelName, string apiKey = \"\", int dimension = 768, int maxTokens = 512)\n+        {\n+            if (string.IsNullOrWhiteSpace(modelName))\n+                throw new ArgumentException(\"Model name cannot be empty\", nameof(modelName));\n+            if (dimension <= 0)\n+                throw new ArgumentException(\"Dimension must be positive\", nameof(dimension));\n+            if (maxTokens <= 0)\n+                throw new ArgumentException(\"Max tokens must be positive\", nameof(maxTokens));\n+\n+            _modelName = modelName;\n+            _apiKey = apiKey ?? string.Empty;\n+            _dimension = dimension;\n+            _maxTokens = maxTokens;\n+        }\n+\n+        protected override Vector<T> EmbedCore(string text)\n+        {\n+            var values = new T[_dimension];\n+            var hash = text.GetHashCode();\n+            \n+            for (int i = 0; i < _dimension; i++)\n+            {\n+                var val = NumOps.FromDouble(Math.Cos(hash * (i + 1) * 0.002));",
    "path": "src/RetrievalAugmentedGeneration/EmbeddingModels/HuggingFaceEmbeddingModel.cs",
    "commit_id": "84f99e221d10d12f47a4fcccf4d1fffe7a2556f2",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "Possible overflow: result of integer multiplication cast to double.",
    "created_at": "2025-11-03T23:02:08Z",
    "updated_at": "2025-11-03T23:02:19Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102155",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102155"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102155"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102155/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": null,
    "original_line": 44,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 44,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102162",
    "pull_request_review_id": 3413273577,
    "id": 2488102162,
    "node_id": "PRRC_kwDOKSXUF86UTW0S",
    "diff_hunk": "@@ -0,0 +1,71 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\n+using System;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels\n+{\n+    /// <summary>\n+    /// Local transformer embedding model for generating embeddings without external API calls.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class LocalTransformerEmbedding<T> : EmbeddingModelBase<T>\n+    {\n+        private readonly string _modelPath;\n+        private readonly int _dimension;\n+        private readonly int _maxTokens;\n+\n+        public override int EmbeddingDimension => _dimension;\n+        public override int MaxTokens => _maxTokens;\n+\n+        public LocalTransformerEmbedding(string modelPath, int dimension = 384, int maxTokens = 512)\n+        {\n+            if (string.IsNullOrWhiteSpace(modelPath))\n+                throw new ArgumentException(\"Model path cannot be empty\", nameof(modelPath));\n+            if (dimension <= 0)\n+                throw new ArgumentException(\"Dimension must be positive\", nameof(dimension));\n+            if (maxTokens <= 0)\n+                throw new ArgumentException(\"Max tokens must be positive\", nameof(maxTokens));\n+\n+            _modelPath = modelPath;\n+            _dimension = dimension;\n+            _maxTokens = maxTokens;\n+        }\n+\n+        protected override Vector<T> EmbedCore(string text)\n+        {\n+            var values = new T[_dimension];\n+            var hash = text.GetHashCode();\n+            \n+            for (int i = 0; i < _dimension; i++)\n+            {\n+                var val = NumOps.FromDouble(Math.Sin(hash * (i + 1) * 0.003));",
    "path": "src/RetrievalAugmentedGeneration/EmbeddingModels/LocalTransformerEmbedding.cs",
    "commit_id": "84f99e221d10d12f47a4fcccf4d1fffe7a2556f2",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "Possible overflow: result of integer multiplication cast to double.",
    "created_at": "2025-11-03T23:02:08Z",
    "updated_at": "2025-11-03T23:02:20Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102162",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102162"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102162"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102162/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": null,
    "original_line": 42,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 42,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102167",
    "pull_request_review_id": 3413273577,
    "id": 2488102167,
    "node_id": "PRRC_kwDOKSXUF86UTW0X",
    "diff_hunk": "@@ -0,0 +1,71 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\n+using System;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels\n+{\n+    /// <summary>\n+    /// ONNX-based sentence transformer for generating embeddings.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class ONNXSentenceTransformer<T> : EmbeddingModelBase<T>\n+    {\n+        private readonly string _modelPath;\n+        private readonly int _dimension;\n+        private readonly int _maxTokens;\n+\n+        public override int EmbeddingDimension => _dimension;\n+        public override int MaxTokens => _maxTokens;\n+\n+        public ONNXSentenceTransformer(string modelPath, int dimension = 384, int maxTokens = 512)\n+        {\n+            if (string.IsNullOrWhiteSpace(modelPath))\n+                throw new ArgumentException(\"Model path cannot be empty\", nameof(modelPath));\n+            if (dimension <= 0)\n+                throw new ArgumentException(\"Dimension must be positive\", nameof(dimension));\n+            if (maxTokens <= 0)\n+                throw new ArgumentException(\"Max tokens must be positive\", nameof(maxTokens));\n+\n+            _modelPath = modelPath;\n+            _dimension = dimension;\n+            _maxTokens = maxTokens;\n+        }\n+\n+        protected override Vector<T> EmbedCore(string text)\n+        {\n+            var values = new T[_dimension];\n+            var hash = text.GetHashCode();\n+            \n+            for (int i = 0; i < _dimension; i++)\n+            {\n+                var val = NumOps.FromDouble(Math.Cos(hash * (i + 1) * 0.002));",
    "path": "src/RetrievalAugmentedGeneration/EmbeddingModels/ONNXSentenceTransformer.cs",
    "commit_id": "84f99e221d10d12f47a4fcccf4d1fffe7a2556f2",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "Possible overflow: result of integer multiplication cast to double.",
    "created_at": "2025-11-03T23:02:09Z",
    "updated_at": "2025-11-03T23:02:20Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102167",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102167"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102167"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102167/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": null,
    "original_line": 42,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 42,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102170",
    "pull_request_review_id": 3413273577,
    "id": 2488102170,
    "node_id": "PRRC_kwDOKSXUF86UTW0a",
    "diff_hunk": "@@ -0,0 +1,75 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\n+using System;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels\n+{\n+    /// <summary>\n+    /// OpenAI embedding model for generating embeddings via OpenAI API.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class OpenAIEmbeddingModel<T> : EmbeddingModelBase<T>\n+    {\n+        private readonly string _apiKey;\n+        private readonly string _modelName;\n+        private readonly int _dimension;\n+        private readonly int _maxTokens;\n+\n+        public override int EmbeddingDimension => _dimension;\n+        public override int MaxTokens => _maxTokens;\n+\n+        public OpenAIEmbeddingModel(string apiKey, string modelName = \"text-embedding-ada-002\", int dimension = 1536, int maxTokens = 8191)\n+        {\n+            if (string.IsNullOrWhiteSpace(apiKey))\n+                throw new ArgumentException(\"API key cannot be empty\", nameof(apiKey));\n+            if (string.IsNullOrWhiteSpace(modelName))\n+                throw new ArgumentException(\"Model name cannot be empty\", nameof(modelName));\n+            if (dimension <= 0)\n+                throw new ArgumentException(\"Dimension must be positive\", nameof(dimension));\n+            if (maxTokens <= 0)\n+                throw new ArgumentException(\"Max tokens must be positive\", nameof(maxTokens));\n+\n+            _apiKey = apiKey;\n+            _modelName = modelName;\n+            _dimension = dimension;\n+            _maxTokens = maxTokens;\n+        }\n+\n+        protected override Vector<T> EmbedCore(string text)\n+        {\n+            var values = new T[_dimension];\n+            var hash = text.GetHashCode();\n+            \n+            for (int i = 0; i < _dimension; i++)\n+            {\n+                var val = NumOps.FromDouble(Math.Cos(hash * (i + 1) * 0.001));",
    "path": "src/RetrievalAugmentedGeneration/EmbeddingModels/OpenAIEmbeddingModel.cs",
    "commit_id": "84f99e221d10d12f47a4fcccf4d1fffe7a2556f2",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "Possible overflow: result of integer multiplication cast to double.",
    "created_at": "2025-11-03T23:02:09Z",
    "updated_at": "2025-11-03T23:02:20Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102170",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102170"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102170"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102170/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": null,
    "original_line": 46,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 46,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102177",
    "pull_request_review_id": 3413273577,
    "id": 2488102177,
    "node_id": "PRRC_kwDOKSXUF86UTW0h",
    "diff_hunk": "@@ -0,0 +1,124 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores\n+{\n+    /// <summary>\n+    /// Hybrid document store combining vector and keyword search strategies.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class HybridDocumentStore<T> : DocumentStoreBase<T>\n+    {\n+        private readonly IDocumentStore<T> _vectorStore;\n+        private readonly IDocumentStore<T> _keywordStore;\n+        private readonly T _vectorWeight;\n+        private readonly T _keywordWeight;\n+\n+        public override int DocumentCount => _vectorStore.DocumentCount;\n+        public override int VectorDimension => _vectorStore.VectorDimension;\n+\n+        public HybridDocumentStore(\n+            IDocumentStore<T> vectorStore,\n+            IDocumentStore<T> keywordStore,\n+            T vectorWeight,\n+            T keywordWeight)\n+        {\n+            if (vectorStore == null)\n+                throw new ArgumentNullException(nameof(vectorStore));\n+            if (keywordStore == null)\n+                throw new ArgumentNullException(nameof(keywordStore));\n+\n+            _vectorStore = vectorStore;\n+            _keywordStore = keywordStore;\n+            _vectorWeight = vectorWeight;\n+            _keywordWeight = keywordWeight;\n+        }\n+\n+        protected override void AddCore(VectorDocument<T> vectorDocument)\n+        {\n+            _vectorStore.Add(vectorDocument);\n+            _keywordStore.Add(vectorDocument);\n+        }\n+\n+        protected override void AddBatchCore(IList<VectorDocument<T>> vectorDocuments)\n+        {\n+            _vectorStore.AddBatch(vectorDocuments);\n+            _keywordStore.AddBatch(vectorDocuments);\n+        }\n+\n+        protected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)\n+        {\n+            var vectorResults = _vectorStore.GetSimilarWithFilters(queryVector, topK * 2, metadataFilters);\n+            var keywordResults = _keywordStore.GetSimilarWithFilters(queryVector, topK * 2, metadataFilters);\n+\n+            var combinedScores = new Dictionary<string, T>();\n+\n+            foreach (var doc in vectorResults)\n+            {\n+                if (doc.HasRelevanceScore)\n+                {\n+                    var score = NumOps.Multiply(_vectorWeight, doc.RelevanceScore);\n+                    combinedScores[doc.Id] = score;\n+                }\n+            }\n+\n+            foreach (var doc in keywordResults)\n+            {\n+                if (doc.HasRelevanceScore)\n+                {\n+                    var keywordScore = NumOps.Multiply(_keywordWeight, doc.RelevanceScore);\n+                    if (combinedScores.ContainsKey(doc.Id))\n+                    {\n+                        combinedScores[doc.Id] = NumOps.Add(combinedScores[doc.Id], keywordScore);",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/HybridDocumentStore.cs",
    "commit_id": "84f99e221d10d12f47a4fcccf4d1fffe7a2556f2",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "Inefficient use of 'ContainsKey' and [indexer](1).\n```suggestion\n                    if (combinedScores.TryGetValue(doc.Id, out var existingScore))\n                    {\n                        combinedScores[doc.Id] = NumOps.Add(existingScore, keywordScore);\n```",
    "created_at": "2025-11-03T23:02:09Z",
    "updated_at": "2025-11-03T23:02:20Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102177",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102177"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102177"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102177/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 75,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 77,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 77,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102186",
    "pull_request_review_id": 3413273577,
    "id": 2488102186,
    "node_id": "PRRC_kwDOKSXUF86UTW0q",
    "diff_hunk": "@@ -0,0 +1,122 @@\n+using System;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System.Collections.Generic;\n+using System.Linq;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.RerankingStrategies\n+{\n+    /// <summary>\n+    /// Reciprocal Rank Fusion for combining multiple ranking lists.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class ReciprocalRankFusion<T> : RerankingStrategyBase<T>\n+    {\n+        private readonly INumericOperations<T> _numOps;\n+        private readonly int _k;\n+\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"ReciprocalRankFusion{T}\"/> class.\n+        /// </summary>\n+        /// <param name=\"numericOperations\">The numeric operations for type T.</param>\n+        /// <param name=\"k\">The constant k for reciprocal rank formula (default: 60).</param>\n+        public ReciprocalRankFusion(INumericOperations<T> numericOperations, int k = 60) : base(numericOperations)\n+        {\n+            _numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\n+            _k = k > 0 ? k : throw new ArgumentOutOfRangeException(nameof(k));\n+        }\n+\n+        /// <summary>\n+        /// Reranks documents using reciprocal rank fusion.\n+        /// </summary>\n+        /// <param name=\"query\">The query string.</param>\n+        /// <param name=\"documents\">The documents to rerank.</param>\n+        /// <param name=\"topK\">The number of top documents to return.</param>\n+        /// <returns>A reranked list of documents.</returns>\n+        public override List<Document<T>> Rerank(string query, List<Document<T>> documents, int topK)\n+        {\n+            if (string.IsNullOrEmpty(query)) throw new ArgumentNullException(nameof(query));\n+            if (documents == null) throw new ArgumentNullException(nameof(documents));\n+            if (topK <= 0) throw new ArgumentOutOfRangeException(nameof(topK));\n+\n+            var scores = new Dictionary<string, T>();\n+\n+            for (int rank = 0; rank < documents.Count; rank++)\n+            {\n+                var doc = documents[rank];\n+                var rrfScore = _numOps.FromDouble(1.0 / (_k + rank + 1));\n+\n+                if (scores.ContainsKey(doc.Id))",
    "path": "src/RetrievalAugmentedGeneration/RerankingStrategies/ReciprocalRankFusion.cs",
    "commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "Inefficient use of 'ContainsKey' and [indexer](1).",
    "created_at": "2025-11-03T23:02:10Z",
    "updated_at": "2025-11-03T23:02:20Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102186",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102186"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102186"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102186/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": null,
    "original_line": 49,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 49,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102194",
    "pull_request_review_id": 3413273577,
    "id": 2488102194,
    "node_id": "PRRC_kwDOKSXUF86UTW0y",
    "diff_hunk": "@@ -0,0 +1,122 @@\n+using System;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System.Collections.Generic;\n+using System.Linq;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.RerankingStrategies\n+{\n+    /// <summary>\n+    /// Reciprocal Rank Fusion for combining multiple ranking lists.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class ReciprocalRankFusion<T> : RerankingStrategyBase<T>\n+    {\n+        private readonly INumericOperations<T> _numOps;\n+        private readonly int _k;\n+\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"ReciprocalRankFusion{T}\"/> class.\n+        /// </summary>\n+        /// <param name=\"numericOperations\">The numeric operations for type T.</param>\n+        /// <param name=\"k\">The constant k for reciprocal rank formula (default: 60).</param>\n+        public ReciprocalRankFusion(INumericOperations<T> numericOperations, int k = 60) : base(numericOperations)\n+        {\n+            _numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\n+            _k = k > 0 ? k : throw new ArgumentOutOfRangeException(nameof(k));\n+        }\n+\n+        /// <summary>\n+        /// Reranks documents using reciprocal rank fusion.\n+        /// </summary>\n+        /// <param name=\"query\">The query string.</param>\n+        /// <param name=\"documents\">The documents to rerank.</param>\n+        /// <param name=\"topK\">The number of top documents to return.</param>\n+        /// <returns>A reranked list of documents.</returns>\n+        public override List<Document<T>> Rerank(string query, List<Document<T>> documents, int topK)\n+        {\n+            if (string.IsNullOrEmpty(query)) throw new ArgumentNullException(nameof(query));\n+            if (documents == null) throw new ArgumentNullException(nameof(documents));\n+            if (topK <= 0) throw new ArgumentOutOfRangeException(nameof(topK));\n+\n+            var scores = new Dictionary<string, T>();\n+\n+            for (int rank = 0; rank < documents.Count; rank++)\n+            {\n+                var doc = documents[rank];\n+                var rrfScore = _numOps.FromDouble(1.0 / (_k + rank + 1));\n+\n+                if (scores.ContainsKey(doc.Id))\n+                {\n+                    scores[doc.Id] = _numOps.Add(scores[doc.Id], rrfScore);\n+                }\n+                else\n+                {\n+                    scores[doc.Id] = rrfScore;\n+                }\n+            }\n+\n+            var reranked = documents\n+                .OrderByDescending(d => Convert.ToDouble(scores.ContainsKey(d.Id) ? scores[d.Id] : _numOps.Zero))\n+                .Take(topK)\n+                .ToList();\n+\n+            foreach (var doc in reranked)\n+            {\n+                if (scores.ContainsKey(doc.Id))",
    "path": "src/RetrievalAugmentedGeneration/RerankingStrategies/ReciprocalRankFusion.cs",
    "commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "Inefficient use of 'ContainsKey' and [indexer](1).",
    "created_at": "2025-11-03T23:02:10Z",
    "updated_at": "2025-11-03T23:02:20Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102194",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102194"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102194"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102194/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": null,
    "original_line": 66,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 66,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102199",
    "pull_request_review_id": 3413273577,
    "id": 2488102199,
    "node_id": "PRRC_kwDOKSXUF86UTW03",
    "diff_hunk": "@@ -0,0 +1,122 @@\n+using System;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System.Collections.Generic;\n+using System.Linq;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.RerankingStrategies\n+{\n+    /// <summary>\n+    /// Reciprocal Rank Fusion for combining multiple ranking lists.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class ReciprocalRankFusion<T> : RerankingStrategyBase<T>\n+    {\n+        private readonly INumericOperations<T> _numOps;\n+        private readonly int _k;\n+\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"ReciprocalRankFusion{T}\"/> class.\n+        /// </summary>\n+        /// <param name=\"numericOperations\">The numeric operations for type T.</param>\n+        /// <param name=\"k\">The constant k for reciprocal rank formula (default: 60).</param>\n+        public ReciprocalRankFusion(INumericOperations<T> numericOperations, int k = 60) : base(numericOperations)\n+        {\n+            _numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\n+            _k = k > 0 ? k : throw new ArgumentOutOfRangeException(nameof(k));\n+        }\n+\n+        /// <summary>\n+        /// Reranks documents using reciprocal rank fusion.\n+        /// </summary>\n+        /// <param name=\"query\">The query string.</param>\n+        /// <param name=\"documents\">The documents to rerank.</param>\n+        /// <param name=\"topK\">The number of top documents to return.</param>\n+        /// <returns>A reranked list of documents.</returns>\n+        public override List<Document<T>> Rerank(string query, List<Document<T>> documents, int topK)\n+        {\n+            if (string.IsNullOrEmpty(query)) throw new ArgumentNullException(nameof(query));\n+            if (documents == null) throw new ArgumentNullException(nameof(documents));\n+            if (topK <= 0) throw new ArgumentOutOfRangeException(nameof(topK));\n+\n+            var scores = new Dictionary<string, T>();\n+\n+            for (int rank = 0; rank < documents.Count; rank++)\n+            {\n+                var doc = documents[rank];\n+                var rrfScore = _numOps.FromDouble(1.0 / (_k + rank + 1));\n+\n+                if (scores.ContainsKey(doc.Id))\n+                {\n+                    scores[doc.Id] = _numOps.Add(scores[doc.Id], rrfScore);\n+                }\n+                else\n+                {\n+                    scores[doc.Id] = rrfScore;\n+                }\n+            }\n+\n+            var reranked = documents\n+                .OrderByDescending(d => Convert.ToDouble(scores.ContainsKey(d.Id) ? scores[d.Id] : _numOps.Zero))\n+                .Take(topK)\n+                .ToList();\n+\n+            foreach (var doc in reranked)\n+            {\n+                if (scores.ContainsKey(doc.Id))\n+                {\n+                    doc.RelevanceScore = scores[doc.Id];\n+                }\n+            }\n+\n+            return reranked;\n+        }\n+\n+        /// <summary>\n+        /// Fuses multiple ranking lists using reciprocal rank fusion.\n+        /// </summary>\n+        /// <param name=\"rankingLists\">Multiple lists of ranked documents.</param>\n+        /// <param name=\"topK\">The number of top documents to return.</param>\n+        /// <returns>A fused and reranked list of documents.</returns>\n+        public List<Document<T>> FuseRankings(List<List<Document<T>>> rankingLists, int topK)\n+        {\n+            if (rankingLists == null || rankingLists.Count == 0)\n+                throw new ArgumentException(\"Ranking lists cannot be null or empty\", nameof(rankingLists));\n+            if (topK <= 0) throw new ArgumentOutOfRangeException(nameof(topK));\n+\n+            var scores = new Dictionary<string, T>();\n+            var allDocs = new Dictionary<string, Document<T>>();\n+\n+            foreach (var rankingList in rankingLists)\n+            {\n+                for (int rank = 0; rank < rankingList.Count; rank++)\n+                {\n+                    var doc = rankingList[rank];\n+                    var rrfScore = _numOps.FromDouble(1.0 / (_k + rank + 1));\n+\n+                    if (scores.ContainsKey(doc.Id))",
    "path": "src/RetrievalAugmentedGeneration/RerankingStrategies/ReciprocalRankFusion.cs",
    "commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "Inefficient use of 'ContainsKey' and [indexer](1).",
    "created_at": "2025-11-03T23:02:10Z",
    "updated_at": "2025-11-03T23:02:20Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102199",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102199"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102199"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102199/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": null,
    "original_line": 97,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 97,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102201",
    "pull_request_review_id": 3413273577,
    "id": 2488102201,
    "node_id": "PRRC_kwDOKSXUF86UTW05",
    "diff_hunk": "@@ -0,0 +1,122 @@\n+using System;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System.Collections.Generic;\n+using System.Linq;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.RerankingStrategies\n+{\n+    /// <summary>\n+    /// Reciprocal Rank Fusion for combining multiple ranking lists.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class ReciprocalRankFusion<T> : RerankingStrategyBase<T>\n+    {\n+        private readonly INumericOperations<T> _numOps;\n+        private readonly int _k;\n+\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"ReciprocalRankFusion{T}\"/> class.\n+        /// </summary>\n+        /// <param name=\"numericOperations\">The numeric operations for type T.</param>\n+        /// <param name=\"k\">The constant k for reciprocal rank formula (default: 60).</param>\n+        public ReciprocalRankFusion(INumericOperations<T> numericOperations, int k = 60) : base(numericOperations)\n+        {\n+            _numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\n+            _k = k > 0 ? k : throw new ArgumentOutOfRangeException(nameof(k));\n+        }\n+\n+        /// <summary>\n+        /// Reranks documents using reciprocal rank fusion.\n+        /// </summary>\n+        /// <param name=\"query\">The query string.</param>\n+        /// <param name=\"documents\">The documents to rerank.</param>\n+        /// <param name=\"topK\">The number of top documents to return.</param>\n+        /// <returns>A reranked list of documents.</returns>\n+        public override List<Document<T>> Rerank(string query, List<Document<T>> documents, int topK)\n+        {\n+            if (string.IsNullOrEmpty(query)) throw new ArgumentNullException(nameof(query));\n+            if (documents == null) throw new ArgumentNullException(nameof(documents));\n+            if (topK <= 0) throw new ArgumentOutOfRangeException(nameof(topK));\n+\n+            var scores = new Dictionary<string, T>();\n+\n+            for (int rank = 0; rank < documents.Count; rank++)\n+            {\n+                var doc = documents[rank];\n+                var rrfScore = _numOps.FromDouble(1.0 / (_k + rank + 1));\n+\n+                if (scores.ContainsKey(doc.Id))\n+                {\n+                    scores[doc.Id] = _numOps.Add(scores[doc.Id], rrfScore);\n+                }\n+                else\n+                {\n+                    scores[doc.Id] = rrfScore;\n+                }\n+            }\n+\n+            var reranked = documents\n+                .OrderByDescending(d => Convert.ToDouble(scores.ContainsKey(d.Id) ? scores[d.Id] : _numOps.Zero))",
    "path": "src/RetrievalAugmentedGeneration/RerankingStrategies/ReciprocalRankFusion.cs",
    "commit_id": "0990756bfaebc406430e06b89f59a191b1f29d3a",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "Inefficient use of 'ContainsKey' and [indexer](1).",
    "created_at": "2025-11-03T23:02:10Z",
    "updated_at": "2025-11-03T23:02:20Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102201",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102201"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102201"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102201/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": null,
    "original_line": 60,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 60,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102208",
    "pull_request_review_id": 3413273577,
    "id": 2488102208,
    "node_id": "PRRC_kwDOKSXUF86UTW1A",
    "diff_hunk": "@@ -0,0 +1,124 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\n+{\n+    /// <summary>\n+    /// BM25 (Best Matching 25) retrieval algorithm for sparse keyword-based search.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class BM25Retriever<T> : RetrieverBase<T>\n+    {\n+        private readonly IDocumentStore<T> _documentStore;\n+        private readonly T _k1;\n+        private readonly T _b;\n+        private readonly Dictionary<string, Dictionary<string, T>> _termFrequencies;\n+        private readonly Dictionary<string, T> _documentLengths;\n+        private T _avgDocLength;\n+\n+        public BM25Retriever(IDocumentStore<T> documentStore, int defaultTopK = 5, double k1 = 1.5, double b = 0.75) \n+            : base(defaultTopK)\n+        {\n+            if (documentStore == null)\n+                throw new ArgumentNullException(nameof(documentStore));\n+\n+            _documentStore = documentStore;\n+            _k1 = NumOps.FromDouble(k1);\n+            _b = NumOps.FromDouble(b);\n+            _termFrequencies = new Dictionary<string, Dictionary<string, T>>();\n+            _documentLengths = new Dictionary<string, T>();\n+            _avgDocLength = NumOps.Zero;\n+        }\n+\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\n+        {\n+            var queryTerms = Tokenize(query);\n+            var scores = new Dictionary<string, T>();\n+\n+            var candidates = _documentStore.GetSimilar(\n+                new AiDotNet.LinearAlgebra.Vector<T>(new T[Math.Max(1, _documentStore.VectorDimension)]),\n+                _documentStore.DocumentCount\n+            );\n+\n+            foreach (var doc in candidates)\n+            {\n+                if (!MatchesFilters(doc, metadataFilters))\n+                    continue;\n+\n+                var score = NumOps.Zero;\n+                \n+                foreach (var term in queryTerms)\n+                {\n+                    var termScore = CalculateBM25Term(doc.Id, term);\n+                    score = NumOps.Add(score, termScore);\n+                }\n+\n+                scores[doc.Id] = score;\n+            }\n+\n+            var results = scores\n+                .OrderByDescending(kv => kv.Value)\n+                .Take(topK)\n+                .Select(kv =>\n+                {\n+                    var doc = candidates.FirstOrDefault(d => d.Id == kv.Key);\n+                    if (doc != null)\n+                    {\n+                        doc.RelevanceScore = kv.Value;\n+                        doc.HasRelevanceScore = true;\n+                    }\n+                    return doc;\n+                })\n+                .Where(d => d != null)\n+                .Cast<Document<T>>();\n+\n+            return results;\n+        }\n+\n+        private T CalculateBM25Term(string docId, string term)\n+        {\n+            if (!_termFrequencies.ContainsKey(docId) || !_termFrequencies[docId].ContainsKey(term))\n+                return NumOps.Zero;\n+\n+            var tf = _termFrequencies[docId][term];\n+            var docLength = _documentLengths.ContainsKey(docId) ? _documentLengths[docId] : NumOps.Zero;",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/BM25Retriever.cs",
    "commit_id": "c4d0b13a14278fdf7e993161a6a39747256ba981",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "Inefficient use of 'ContainsKey' and [indexer](1).",
    "created_at": "2025-11-03T23:02:10Z",
    "updated_at": "2025-11-03T23:02:20Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102208",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102208"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102208"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102208/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": null,
    "original_line": 88,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 88,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102213",
    "pull_request_review_id": 3413273577,
    "id": 2488102213,
    "node_id": "PRRC_kwDOKSXUF86UTW1F",
    "diff_hunk": "@@ -0,0 +1,92 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\n+{\n+    /// <summary>\n+    /// Hybrid retriever combining dense and sparse retrieval strategies.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class HybridRetriever<T> : RetrieverBase<T>\n+    {\n+        private readonly IRetriever<T> _denseRetriever;\n+        private readonly IRetriever<T> _sparseRetriever;\n+        private readonly T _denseWeight;\n+        private readonly T _sparseWeight;\n+\n+        public HybridRetriever(\n+            IRetriever<T> denseRetriever, \n+            IRetriever<T> sparseRetriever, \n+            double denseWeight = 0.7, \n+            double sparseWeight = 0.3,\n+            int defaultTopK = 5) \n+            : base(defaultTopK)\n+        {\n+            if (denseRetriever == null)\n+                throw new ArgumentNullException(nameof(denseRetriever));\n+            if (sparseRetriever == null)\n+                throw new ArgumentNullException(nameof(sparseRetriever));\n+\n+            _denseRetriever = denseRetriever;\n+            _sparseRetriever = sparseRetriever;\n+            _denseWeight = NumOps.FromDouble(denseWeight);\n+            _sparseWeight = NumOps.FromDouble(sparseWeight);\n+        }\n+\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\n+        {\n+            var denseResults = _denseRetriever.Retrieve(query, topK * 2, metadataFilters);\n+            var sparseResults = _sparseRetriever.Retrieve(query, topK * 2, metadataFilters);\n+\n+            var combinedScores = new Dictionary<string, T>();\n+\n+            foreach (var doc in denseResults)\n+            {\n+                if (doc.HasRelevanceScore)\n+                {\n+                    var score = NumOps.Multiply(_denseWeight, doc.RelevanceScore);\n+                    combinedScores[doc.Id] = score;\n+                }\n+            }\n+\n+            foreach (var doc in sparseResults)\n+            {\n+                if (doc.HasRelevanceScore)\n+                {\n+                    var sparseScore = NumOps.Multiply(_sparseWeight, doc.RelevanceScore);\n+                    if (combinedScores.ContainsKey(doc.Id))",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/HybridRetriever.cs",
    "commit_id": "84f99e221d10d12f47a4fcccf4d1fffe7a2556f2",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "Inefficient use of 'ContainsKey' and [indexer](1).",
    "created_at": "2025-11-03T23:02:11Z",
    "updated_at": "2025-11-03T23:02:20Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102213",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102213"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102213"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102213/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": null,
    "original_line": 61,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 61,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102219",
    "pull_request_review_id": 3413273577,
    "id": 2488102219,
    "node_id": "PRRC_kwDOKSXUF86UTW1L",
    "diff_hunk": "@@ -0,0 +1,124 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\n+{\n+    /// <summary>\n+    /// BM25 (Best Matching 25) retrieval algorithm for sparse keyword-based search.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class BM25Retriever<T> : RetrieverBase<T>\n+    {\n+        private readonly IDocumentStore<T> _documentStore;\n+        private readonly T _k1;\n+        private readonly T _b;\n+        private readonly Dictionary<string, Dictionary<string, T>> _termFrequencies;\n+        private readonly Dictionary<string, T> _documentLengths;\n+        private T _avgDocLength;\n+\n+        public BM25Retriever(IDocumentStore<T> documentStore, int defaultTopK = 5, double k1 = 1.5, double b = 0.75) \n+            : base(defaultTopK)\n+        {\n+            if (documentStore == null)\n+                throw new ArgumentNullException(nameof(documentStore));\n+\n+            _documentStore = documentStore;\n+            _k1 = NumOps.FromDouble(k1);\n+            _b = NumOps.FromDouble(b);\n+            _termFrequencies = new Dictionary<string, Dictionary<string, T>>();\n+            _documentLengths = new Dictionary<string, T>();\n+            _avgDocLength = NumOps.Zero;\n+        }\n+\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\n+        {\n+            var queryTerms = Tokenize(query);\n+            var scores = new Dictionary<string, T>();\n+\n+            var candidates = _documentStore.GetSimilar(\n+                new AiDotNet.LinearAlgebra.Vector<T>(new T[Math.Max(1, _documentStore.VectorDimension)]),\n+                _documentStore.DocumentCount\n+            );\n+\n+            foreach (var doc in candidates)\n+            {\n+                if (!MatchesFilters(doc, metadataFilters))\n+                    continue;\n+\n+                var score = NumOps.Zero;\n+                \n+                foreach (var term in queryTerms)\n+                {\n+                    var termScore = CalculateBM25Term(doc.Id, term);\n+                    score = NumOps.Add(score, termScore);\n+                }\n+\n+                scores[doc.Id] = score;\n+            }\n+\n+            var results = scores\n+                .OrderByDescending(kv => kv.Value)\n+                .Take(topK)\n+                .Select(kv =>\n+                {\n+                    var doc = candidates.FirstOrDefault(d => d.Id == kv.Key);\n+                    if (doc != null)\n+                    {\n+                        doc.RelevanceScore = kv.Value;\n+                        doc.HasRelevanceScore = true;\n+                    }\n+                    return doc;\n+                })\n+                .Where(d => d != null)\n+                .Cast<Document<T>>();\n+\n+            return results;\n+        }\n+\n+        private T CalculateBM25Term(string docId, string term)\n+        {\n+            if (!_termFrequencies.ContainsKey(docId) || !_termFrequencies[docId].ContainsKey(term))",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/BM25Retriever.cs",
    "commit_id": "c4d0b13a14278fdf7e993161a6a39747256ba981",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "Inefficient use of 'ContainsKey' and [indexer](1).\nInefficient use of 'ContainsKey' and [indexer](2).",
    "created_at": "2025-11-03T23:02:11Z",
    "updated_at": "2025-11-03T23:02:20Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102219",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102219"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102219"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102219/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": null,
    "original_line": 84,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 84,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102226",
    "pull_request_review_id": 3413273577,
    "id": 2488102226,
    "node_id": "PRRC_kwDOKSXUF86UTW1S",
    "diff_hunk": "@@ -0,0 +1,124 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\n+{\n+    /// <summary>\n+    /// BM25 (Best Matching 25) retrieval algorithm for sparse keyword-based search.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class BM25Retriever<T> : RetrieverBase<T>\n+    {\n+        private readonly IDocumentStore<T> _documentStore;\n+        private readonly T _k1;\n+        private readonly T _b;\n+        private readonly Dictionary<string, Dictionary<string, T>> _termFrequencies;\n+        private readonly Dictionary<string, T> _documentLengths;\n+        private T _avgDocLength;\n+\n+        public BM25Retriever(IDocumentStore<T> documentStore, int defaultTopK = 5, double k1 = 1.5, double b = 0.75) \n+            : base(defaultTopK)\n+        {\n+            if (documentStore == null)\n+                throw new ArgumentNullException(nameof(documentStore));\n+\n+            _documentStore = documentStore;\n+            _k1 = NumOps.FromDouble(k1);\n+            _b = NumOps.FromDouble(b);\n+            _termFrequencies = new Dictionary<string, Dictionary<string, T>>();\n+            _documentLengths = new Dictionary<string, T>();\n+            _avgDocLength = NumOps.Zero;\n+        }\n+\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\n+        {\n+            var queryTerms = Tokenize(query);\n+            var scores = new Dictionary<string, T>();\n+\n+            var candidates = _documentStore.GetSimilar(\n+                new AiDotNet.LinearAlgebra.Vector<T>(new T[Math.Max(1, _documentStore.VectorDimension)]),\n+                _documentStore.DocumentCount\n+            );\n+\n+            foreach (var doc in candidates)\n+            {\n+                if (!MatchesFilters(doc, metadataFilters))\n+                    continue;\n+\n+                var score = NumOps.Zero;\n+                \n+                foreach (var term in queryTerms)\n+                {\n+                    var termScore = CalculateBM25Term(doc.Id, term);\n+                    score = NumOps.Add(score, termScore);\n+                }\n+\n+                scores[doc.Id] = score;\n+            }\n+\n+            var results = scores\n+                .OrderByDescending(kv => kv.Value)\n+                .Take(topK)\n+                .Select(kv =>\n+                {\n+                    var doc = candidates.FirstOrDefault(d => d.Id == kv.Key);\n+                    if (doc != null)\n+                    {\n+                        doc.RelevanceScore = kv.Value;\n+                        doc.HasRelevanceScore = true;\n+                    }\n+                    return doc;\n+                })\n+                .Where(d => d != null)\n+                .Cast<Document<T>>();\n+\n+            return results;\n+        }\n+\n+        private T CalculateBM25Term(string docId, string term)\n+        {\n+            if (!_termFrequencies.ContainsKey(docId) || !_termFrequencies[docId].ContainsKey(term))",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/BM25Retriever.cs",
    "commit_id": "c4d0b13a14278fdf7e993161a6a39747256ba981",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "Inefficient use of 'ContainsKey' and [indexer](1).",
    "created_at": "2025-11-03T23:02:11Z",
    "updated_at": "2025-11-03T23:02:20Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102226",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102226"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102226"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102226/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": null,
    "original_line": 84,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 84,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102232",
    "pull_request_review_id": 3413273577,
    "id": 2488102232,
    "node_id": "PRRC_kwDOKSXUF86UTW1Y",
    "diff_hunk": "@@ -0,0 +1,83 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\n+{\n+    /// <summary>\n+    /// Multi-query retriever that generates multiple query variations and merges results.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class MultiQueryRetriever<T> : RetrieverBase<T>\n+    {\n+        private readonly IRetriever<T> _baseRetriever;\n+        private readonly int _numQueries;\n+\n+        public MultiQueryRetriever(IRetriever<T> baseRetriever, int numQueries = 3, int defaultTopK = 5) : base(defaultTopK)\n+        {\n+            if (baseRetriever == null)\n+                throw new ArgumentNullException(nameof(baseRetriever));\n+            if (numQueries <= 0)\n+                throw new ArgumentException(\"Number of queries must be positive\", nameof(numQueries));\n+\n+            _baseRetriever = baseRetriever;\n+            _numQueries = numQueries;\n+        }\n+\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\n+        {\n+            var queries = GenerateQueries(query);\n+            var allResults = new Dictionary<string, (Document<T> doc, T score)>();\n+\n+            foreach (var q in queries)\n+            {\n+                var results = _baseRetriever.Retrieve(q, topK, metadataFilters);\n+\n+                foreach (var doc in results)\n+                {\n+                    if (doc.HasRelevanceScore)\n+                    {\n+                        if (allResults.ContainsKey(doc.Id))",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/MultiQueryRetriever.cs",
    "commit_id": "84f99e221d10d12f47a4fcccf4d1fffe7a2556f2",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "Inefficient use of 'ContainsKey' and [indexer](1).",
    "created_at": "2025-11-03T23:02:11Z",
    "updated_at": "2025-11-03T23:02:20Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102232",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102232"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102232"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102232/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": null,
    "original_line": 43,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 43,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102235",
    "pull_request_review_id": 3413273577,
    "id": 2488102235,
    "node_id": "PRRC_kwDOKSXUF86UTW1b",
    "diff_hunk": "@@ -0,0 +1,108 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\n+{\n+    /// <summary>\n+    /// TF-IDF (Term Frequency-Inverse Document Frequency) retrieval strategy.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class TFIDFRetriever<T> : RetrieverBase<T>\n+    {\n+        private readonly IDocumentStore<T> _documentStore;\n+        private readonly Dictionary<string, Dictionary<string, T>> _tfidf;\n+        private readonly Dictionary<string, T> _idf;\n+\n+        public TFIDFRetriever(IDocumentStore<T> documentStore, int defaultTopK = 5) : base(defaultTopK)\n+        {\n+            if (documentStore == null)\n+                throw new ArgumentNullException(nameof(documentStore));\n+                \n+            _documentStore = documentStore;\n+            _tfidf = new Dictionary<string, Dictionary<string, T>>();\n+            _idf = new Dictionary<string, T>();\n+        }\n+\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\n+        {\n+            var queryTerms = Tokenize(query);\n+            var scores = new Dictionary<string, T>();\n+\n+            var candidates = _documentStore.GetSimilar(\n+                new AiDotNet.LinearAlgebra.Vector<T>(new T[_documentStore.VectorDimension]), \n+                _documentStore.DocumentCount\n+            );\n+\n+            foreach (var doc in candidates)\n+            {\n+                if (!MatchesFilters(doc, metadataFilters))\n+                    continue;\n+\n+                var score = NumOps.Zero;\n+\n+                if (_tfidf.ContainsKey(doc.Id))",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/TFIDFRetriever.cs",
    "commit_id": "84f99e221d10d12f47a4fcccf4d1fffe7a2556f2",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "Inefficient use of 'ContainsKey' and [indexer](1).",
    "created_at": "2025-11-03T23:02:12Z",
    "updated_at": "2025-11-03T23:02:20Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102235",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102235"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102235"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102235/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": null,
    "original_line": 47,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 47,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102240",
    "pull_request_review_id": 3413273577,
    "id": 2488102240,
    "node_id": "PRRC_kwDOKSXUF86UTW1g",
    "diff_hunk": "@@ -0,0 +1,108 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\n+{\n+    /// <summary>\n+    /// TF-IDF (Term Frequency-Inverse Document Frequency) retrieval strategy.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class TFIDFRetriever<T> : RetrieverBase<T>\n+    {\n+        private readonly IDocumentStore<T> _documentStore;\n+        private readonly Dictionary<string, Dictionary<string, T>> _tfidf;\n+        private readonly Dictionary<string, T> _idf;\n+\n+        public TFIDFRetriever(IDocumentStore<T> documentStore, int defaultTopK = 5) : base(defaultTopK)\n+        {\n+            if (documentStore == null)\n+                throw new ArgumentNullException(nameof(documentStore));\n+                \n+            _documentStore = documentStore;\n+            _tfidf = new Dictionary<string, Dictionary<string, T>>();\n+            _idf = new Dictionary<string, T>();\n+        }\n+\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\n+        {\n+            var queryTerms = Tokenize(query);\n+            var scores = new Dictionary<string, T>();\n+\n+            var candidates = _documentStore.GetSimilar(\n+                new AiDotNet.LinearAlgebra.Vector<T>(new T[_documentStore.VectorDimension]), \n+                _documentStore.DocumentCount\n+            );\n+\n+            foreach (var doc in candidates)\n+            {\n+                if (!MatchesFilters(doc, metadataFilters))\n+                    continue;\n+\n+                var score = NumOps.Zero;\n+\n+                if (_tfidf.ContainsKey(doc.Id))\n+                {\n+                    var docTfidf = _tfidf[doc.Id];\n+                    foreach (var term in queryTerms)\n+                    {\n+                        if (docTfidf.ContainsKey(term))",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/TFIDFRetriever.cs",
    "commit_id": "84f99e221d10d12f47a4fcccf4d1fffe7a2556f2",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "Inefficient use of 'ContainsKey' and [indexer](1).",
    "created_at": "2025-11-03T23:02:12Z",
    "updated_at": "2025-11-03T23:02:20Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102240",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102240"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102240"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102240/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": null,
    "original_line": 52,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 52,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102247",
    "pull_request_review_id": 3413273577,
    "id": 2488102247,
    "node_id": "PRRC_kwDOKSXUF86UTW1n",
    "diff_hunk": "@@ -0,0 +1,99 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies\n+{\n+    /// <summary>\n+    /// Semantic-based text chunking that uses embeddings to group related content.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class SemanticChunkingStrategy<T> : ChunkingStrategyBase\n+    {\n+        private readonly INumericOperations<T> _numOps;\n+        private readonly T _similarityThreshold;\n+\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"SemanticChunkingStrategy{T}\"/> class.\n+        /// </summary>\n+        /// <param name=\"numericOperations\">The numeric operations for type T.</param>\n+        /// <param name=\"similarityThreshold\">The similarity threshold for grouping sentences.</param>\n+        /// <param name=\"maxChunkSize\">The maximum chunk size in characters.</param>\n+        public SemanticChunkingStrategy(\n+            INumericOperations<T> numericOperations,\n+            T similarityThreshold,\n+            int maxChunkSize = 1000,\n+            int chunkOverlap = 200)\n+            : base(maxChunkSize, chunkOverlap)\n+        {\n+            _numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\n+            _similarityThreshold = similarityThreshold;\n+        }\n+\n+        /// <summary>\n+        /// Chunks text based on semantic similarity between sentences.\n+        /// </summary>\n+        /// <param name=\"text\">The text to chunk.</param>\n+        /// <returns>A collection of semantically coherent chunks with positions.</returns>\n+        protected override IEnumerable<(string Chunk, int StartPosition, int EndPosition)> ChunkCore(string text)\n+        {\n+            var sentences = SplitIntoSentences(text);\n+            var currentChunk = new List<string>();\n+            var currentSize = 0;\n+            var position = 0;\n+\n+            foreach (var sentence in sentences)\n+            {\n+                if (currentSize + sentence.Length > ChunkSize && currentChunk.Count > 0)\n+                {\n+                    var chunkText = string.Join(\" \", currentChunk);\n+                    var endPos = position + chunkText.Length;\n+                    yield return (chunkText, position, endPos);\n+                    \n+                    position = endPos - ChunkOverlap;\n+                    currentChunk.Clear();\n+                    currentSize = 0;\n+                }\n+\n+                currentChunk.Add(sentence);\n+                currentSize += sentence.Length;\n+            }\n+\n+            if (currentChunk.Count > 0)\n+            {\n+                var chunkText = string.Join(\" \", currentChunk);\n+                yield return (chunkText, position, position + chunkText.Length);\n+            }\n+        }\n+\n+        private List<string> SplitIntoSentences(string text)\n+        {\n+            var sentences = new List<string>();\n+            var sentenceEndings = new[] { \". \", \"! \", \"? \", \".\\n\", \"!\\n\", \"?\\n\" };\n+            var currentSentence = string.Empty;\n+\n+            for (int i = 0; i < text.Length; i++)\n+            {\n+                currentSentence += text[i];",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/SemanticChunkingStrategy.cs",
    "commit_id": "c0be21878ba289b4ea925eacff9c9a9164ad807a",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "String concatenation in loop: use 'StringBuilder'.",
    "created_at": "2025-11-03T23:02:12Z",
    "updated_at": "2025-11-03T23:02:20Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102247",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102247"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102247"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102247/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": null,
    "original_line": 78,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 78,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102254",
    "pull_request_review_id": 3413273577,
    "id": 2488102254,
    "node_id": "PRRC_kwDOKSXUF86UTW1u",
    "diff_hunk": "@@ -0,0 +1,166 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ContextCompression\n+{\n+    /// <summary>\n+    /// Document summarizer for creating concise summaries of retrieved content.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class DocumentSummarizer<T>\n+    {\n+        private readonly INumericOperations<T> _numOps;\n+        private readonly int _maxSummaryLength;\n+        private readonly string _llmEndpoint;\n+        private readonly string _apiKey;\n+\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"DocumentSummarizer{T}\"/> class.\n+        /// </summary>\n+        /// <param name=\"numericOperations\">The numeric operations for type T.</param>\n+        /// <param name=\"maxSummaryLength\">The maximum length of the summary in characters.</param>\n+        /// <param name=\"llmEndpoint\">The LLM API endpoint.</param>\n+        /// <param name=\"apiKey\">The API key for the LLM service.</param>\n+        public DocumentSummarizer(\n+            INumericOperations<T> numericOperations,\n+            int maxSummaryLength = 500,\n+            string llmEndpoint = \"\",\n+            string apiKey = \"\")\n+        {\n+            _numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\n+            _maxSummaryLength = maxSummaryLength > 0\n+                ? maxSummaryLength\n+                : throw new ArgumentOutOfRangeException(nameof(maxSummaryLength));\n+            _llmEndpoint = llmEndpoint;\n+            _apiKey = apiKey;\n+        }\n+\n+        /// <summary>\n+        /// Summarizes a list of documents.\n+        /// </summary>\n+        /// <param name=\"documents\">The documents to summarize.</param>\n+        /// <returns>A list of summarized documents.</returns>\n+        public List<Document<T>> Summarize(List<Document<T>> documents)\n+        {\n+            if (documents == null) throw new ArgumentNullException(nameof(documents));\n+\n+            var summarized = new List<Document<T>>();\n+\n+            foreach (var doc in documents)\n+            {\n+                var summary = SummarizeText(doc.Content);\n+                var summarizedDoc = new Document<T>\n+                {\n+                    Id = doc.Id,\n+                    Content = summary,\n+                    Metadata = doc.Metadata,\n+                    RelevanceScore = doc.RelevanceScore\n+                };\n+                summarized.Add(summarizedDoc);\n+            }\n+\n+            return summarized;\n+        }\n+\n+        /// <summary>\n+        /// Summarizes text to a maximum length.\n+        /// </summary>\n+        /// <param name=\"text\">The text to summarize.</param>\n+        /// <returns>The summarized text.</returns>\n+        public string SummarizeText(string text)\n+        {\n+            if (string.IsNullOrEmpty(text)) return text;\n+\n+            if (text.Length <= _maxSummaryLength)\n+            {\n+                return text;\n+            }\n+\n+            var sentences = SplitIntoSentences(text);\n+            var importantSentences = ExtractImportantSentences(sentences);\n+\n+            var summary = string.Empty;\n+            foreach (var sentence in importantSentences)\n+            {\n+                if (summary.Length + sentence.Length > _maxSummaryLength)\n+                {\n+                    break;\n+                }\n+                summary += sentence + \" \";",
    "path": "src/RetrievalAugmentedGeneration/ContextCompression/DocumentSummarizer.cs",
    "commit_id": "c0be21878ba289b4ea925eacff9c9a9164ad807a",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "String concatenation in loop: use 'StringBuilder'.",
    "created_at": "2025-11-03T23:02:12Z",
    "updated_at": "2025-11-03T23:02:20Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102254",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102254"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102254"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102254/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": null,
    "original_line": 92,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 92,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102261",
    "pull_request_review_id": 3413273577,
    "id": 2488102261,
    "node_id": "PRRC_kwDOKSXUF86UTW11",
    "diff_hunk": "@@ -0,0 +1,166 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ContextCompression\n+{\n+    /// <summary>\n+    /// Document summarizer for creating concise summaries of retrieved content.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class DocumentSummarizer<T>\n+    {\n+        private readonly INumericOperations<T> _numOps;\n+        private readonly int _maxSummaryLength;\n+        private readonly string _llmEndpoint;\n+        private readonly string _apiKey;\n+\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"DocumentSummarizer{T}\"/> class.\n+        /// </summary>\n+        /// <param name=\"numericOperations\">The numeric operations for type T.</param>\n+        /// <param name=\"maxSummaryLength\">The maximum length of the summary in characters.</param>\n+        /// <param name=\"llmEndpoint\">The LLM API endpoint.</param>\n+        /// <param name=\"apiKey\">The API key for the LLM service.</param>\n+        public DocumentSummarizer(\n+            INumericOperations<T> numericOperations,\n+            int maxSummaryLength = 500,\n+            string llmEndpoint = \"\",\n+            string apiKey = \"\")\n+        {\n+            _numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\n+            _maxSummaryLength = maxSummaryLength > 0\n+                ? maxSummaryLength\n+                : throw new ArgumentOutOfRangeException(nameof(maxSummaryLength));\n+            _llmEndpoint = llmEndpoint;\n+            _apiKey = apiKey;\n+        }\n+\n+        /// <summary>\n+        /// Summarizes a list of documents.\n+        /// </summary>\n+        /// <param name=\"documents\">The documents to summarize.</param>\n+        /// <returns>A list of summarized documents.</returns>\n+        public List<Document<T>> Summarize(List<Document<T>> documents)\n+        {\n+            if (documents == null) throw new ArgumentNullException(nameof(documents));\n+\n+            var summarized = new List<Document<T>>();\n+\n+            foreach (var doc in documents)\n+            {\n+                var summary = SummarizeText(doc.Content);\n+                var summarizedDoc = new Document<T>\n+                {\n+                    Id = doc.Id,\n+                    Content = summary,\n+                    Metadata = doc.Metadata,\n+                    RelevanceScore = doc.RelevanceScore\n+                };\n+                summarized.Add(summarizedDoc);\n+            }\n+\n+            return summarized;\n+        }\n+\n+        /// <summary>\n+        /// Summarizes text to a maximum length.\n+        /// </summary>\n+        /// <param name=\"text\">The text to summarize.</param>\n+        /// <returns>The summarized text.</returns>\n+        public string SummarizeText(string text)\n+        {\n+            if (string.IsNullOrEmpty(text)) return text;\n+\n+            if (text.Length <= _maxSummaryLength)\n+            {\n+                return text;\n+            }\n+\n+            var sentences = SplitIntoSentences(text);\n+            var importantSentences = ExtractImportantSentences(sentences);\n+\n+            var summary = string.Empty;\n+            foreach (var sentence in importantSentences)\n+            {\n+                if (summary.Length + sentence.Length > _maxSummaryLength)\n+                {\n+                    break;\n+                }\n+                summary += sentence + \" \";\n+            }\n+\n+            return summary.Trim();\n+        }\n+\n+        private List<string> ExtractImportantSentences(List<string> sentences)\n+        {\n+            var scored = new List<(string sentence, double score)>();\n+\n+            foreach (var sentence in sentences)\n+            {\n+                var importance = ComputeImportance(sentence, sentences);\n+                scored.Add((sentence, importance));\n+            }\n+\n+            return scored\n+                .OrderByDescending(x => x.score)\n+                .Select(x => x.sentence)\n+                .ToList();\n+        }\n+\n+        private double ComputeImportance(string sentence, List<string> allSentences)\n+        {\n+            var tokens = Tokenize(sentence);\n+            var uniqueTokens = tokens.Distinct().Count();\n+            var length = sentence.Length;\n+\n+            var positionScore = allSentences.IndexOf(sentence) == 0 ? 1.5 : 1.0;\n+\n+            var importance = (uniqueTokens * 0.5) + (Math.Min(length, 200) / 200.0 * 0.5);\n+            importance *= positionScore;\n+\n+            return importance;\n+        }\n+\n+        private List<string> SplitIntoSentences(string text)\n+        {\n+            var sentences = new List<string>();\n+            var sentenceEndings = new[] { \". \", \"! \", \"? \", \".\\n\", \"!\\n\", \"?\\n\" };\n+            var currentSentence = string.Empty;\n+\n+            for (int i = 0; i < text.Length; i++)\n+            {\n+                currentSentence += text[i];",
    "path": "src/RetrievalAugmentedGeneration/ContextCompression/DocumentSummarizer.cs",
    "commit_id": "c0be21878ba289b4ea925eacff9c9a9164ad807a",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "String concatenation in loop: use 'StringBuilder'.",
    "created_at": "2025-11-03T23:02:13Z",
    "updated_at": "2025-11-03T23:02:20Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102261",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102261"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102261"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102261/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": null,
    "original_line": 136,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 136,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102265",
    "pull_request_review_id": 3413273577,
    "id": 2488102265,
    "node_id": "PRRC_kwDOKSXUF86UTW15",
    "diff_hunk": "@@ -0,0 +1,148 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ContextCompression\n+{\n+    /// <summary>\n+    /// LLM-based context compression to reduce token usage while preserving key information.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class LLMContextCompressor<T>\n+    {\n+        private readonly INumericOperations<T> _numOps;\n+        private readonly string _llmEndpoint;\n+        private readonly string _apiKey;\n+        private readonly double _compressionRatio;\n+\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"LLMContextCompressor{T}\"/> class.\n+        /// </summary>\n+        /// <param name=\"numericOperations\">The numeric operations for type T.</param>\n+        /// <param name=\"compressionRatio\">The target compression ratio (0.0 to 1.0).</param>\n+        /// <param name=\"llmEndpoint\">The LLM API endpoint.</param>\n+        /// <param name=\"apiKey\">The API key for the LLM service.</param>\n+        public LLMContextCompressor(\n+            INumericOperations<T> numericOperations,\n+            double compressionRatio = 0.5,\n+            string llmEndpoint = \"\",\n+            string apiKey = \"\")\n+        {\n+            _numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\n+            _compressionRatio = compressionRatio >= 0 && compressionRatio <= 1\n+                ? compressionRatio\n+                : throw new ArgumentOutOfRangeException(nameof(compressionRatio), \"Compression ratio must be between 0 and 1\");\n+            _llmEndpoint = llmEndpoint;\n+            _apiKey = apiKey;\n+        }\n+\n+        /// <summary>\n+        /// Compresses a list of documents while preserving relevance to the query.\n+        /// </summary>\n+        /// <param name=\"query\">The query context.</param>\n+        /// <param name=\"documents\">The documents to compress.</param>\n+        /// <returns>A list of compressed documents.</returns>\n+        public List<Document<T>> Compress(string query, List<Document<T>> documents)\n+        {\n+            if (string.IsNullOrEmpty(query)) throw new ArgumentNullException(nameof(query));\n+            if (documents == null) throw new ArgumentNullException(nameof(documents));\n+\n+            var compressed = new List<Document<T>>();\n+\n+            foreach (var doc in documents)\n+            {\n+                var compressedContent = CompressText(query, doc.Content);\n+                var compressedDoc = new Document<T>\n+                {\n+                    Id = doc.Id,\n+                    Content = compressedContent,\n+                    Metadata = doc.Metadata,\n+                    RelevanceScore = doc.RelevanceScore\n+                };\n+                compressed.Add(compressedDoc);\n+            }\n+\n+            return compressed;\n+        }\n+\n+        /// <summary>\n+        /// Compresses text based on relevance to the query.\n+        /// </summary>\n+        /// <param name=\"query\">The query context.</param>\n+        /// <param name=\"text\">The text to compress.</param>\n+        /// <returns>The compressed text.</returns>\n+        public string CompressText(string query, string text)\n+        {\n+            if (string.IsNullOrEmpty(text)) return text;\n+\n+            var sentences = SplitIntoSentences(text);\n+            var scoredSentences = new List<(string sentence, double score)>();\n+\n+            foreach (var sentence in sentences)\n+            {\n+                var relevance = ComputeRelevance(query, sentence);\n+                scoredSentences.Add((sentence, relevance));\n+            }\n+\n+            var targetCount = Math.Max(1, (int)(sentences.Count * _compressionRatio));\n+            var topSentences = scoredSentences\n+                .OrderByDescending(x => x.score)\n+                .Take(targetCount)\n+                .OrderBy(x => sentences.IndexOf(x.sentence))\n+                .Select(x => x.sentence);\n+\n+            return string.Join(\" \", topSentences);\n+        }\n+\n+        private double ComputeRelevance(string query, string sentence)\n+        {\n+            var queryTokens = Tokenize(query);\n+            var sentenceTokens = Tokenize(sentence);\n+\n+            var overlap = queryTokens.Intersect(sentenceTokens).Count();\n+            var total = Math.Max(queryTokens.Count, sentenceTokens.Count);\n+\n+            return total > 0 ? (double)overlap / total : 0.0;\n+        }\n+\n+        private List<string> SplitIntoSentences(string text)\n+        {\n+            var sentences = new List<string>();\n+            var sentenceEndings = new[] { \". \", \"! \", \"? \", \".\\n\", \"!\\n\", \"?\\n\" };\n+            var currentSentence = string.Empty;\n+\n+            for (int i = 0; i < text.Length; i++)\n+            {\n+                currentSentence += text[i];",
    "path": "src/RetrievalAugmentedGeneration/ContextCompression/LLMContextCompressor.cs",
    "commit_id": "c0be21878ba289b4ea925eacff9c9a9164ad807a",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "String concatenation in loop: use 'StringBuilder'.",
    "created_at": "2025-11-03T23:02:13Z",
    "updated_at": "2025-11-03T23:02:20Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102265",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102265"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102265"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102265/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": null,
    "original_line": 118,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 118,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102271",
    "pull_request_review_id": 3413273577,
    "id": 2488102271,
    "node_id": "PRRC_kwDOKSXUF86UTW1_",
    "diff_hunk": "@@ -0,0 +1,110 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Text;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies\n+{\n+    /// <summary>\n+    /// Code-aware text splitter that respects code structure and syntax.\n+    /// </summary>\n+    public class CodeAwareTextSplitter : ChunkingStrategyBase\n+    {\n+        private readonly string _language;\n+\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"CodeAwareTextSplitter\"/> class.\n+        /// </summary>\n+        /// <param name=\"chunkSize\">The maximum size of each chunk.</param>\n+        /// <param name=\"chunkOverlap\">The overlap between consecutive chunks.</param>\n+        /// <param name=\"language\">The programming language (e.g., \"csharp\", \"python\", \"javascript\").</param>\n+        public CodeAwareTextSplitter(int chunkSize = 1000, int chunkOverlap = 200, string language = \"csharp\")\n+            : base(chunkSize, chunkOverlap)\n+        {\n+            _language = language ?? throw new ArgumentNullException(nameof(language));\n+        }\n+\n+        /// <summary>\n+        /// Chunks code while preserving code structure.\n+        /// </summary>\n+        /// <param name=\"text\">The code text to chunk.</param>\n+        /// <returns>A collection of code chunks with positions.</returns>\n+        protected override IEnumerable<(string Chunk, int StartPosition, int EndPosition)> ChunkCore(string text)\n+        {\n+            var separators = GetLanguageSeparators(_language);\n+            var chunks = SplitTextRecursive(text, separators);\n+            \n+            var position = 0;\n+            foreach (var chunk in chunks)\n+            {\n+                var endPos = position + chunk.Length;\n+                yield return (chunk, position, endPos);\n+                position = endPos - ChunkOverlap;\n+            }\n+        }\n+\n+        private string[] GetLanguageSeparators(string language)\n+        {\n+            switch (language.ToLowerInvariant())\n+            {\n+                case \"csharp\":\n+                case \"c#\":\n+                    return new[] { \"\\n    }\\n\", \"\\n}\\n\", \"\\n\\n\", \"\\n\", \" \" };\n+                case \"python\":\n+                    return new[] { \"\\ndef \", \"\\nclass \", \"\\n\\n\", \"\\n\", \" \" };\n+                case \"javascript\":\n+                case \"typescript\":\n+                    return new[] { \"\\n}\\n\", \"\\n\\n\", \"\\n\", \" \" };\n+                default:\n+                    return new[] { \"\\n\\n\", \"\\n\", \" \" };\n+            }\n+        }\n+\n+        private List<string> SplitTextRecursive(string text, string[] separators)\n+        {\n+            var chunks = new List<string>();\n+\n+            if (text.Length <= ChunkSize)\n+            {\n+                chunks.Add(text);\n+                return chunks;\n+            }\n+\n+            foreach (var separator in separators)\n+            {\n+                var splits = text.Split(new[] { separator }, StringSplitOptions.None);\n+                var currentChunk = new StringBuilder();",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/CodeAwareTextSplitter.cs",
    "commit_id": "c0be21878ba289b4ea925eacff9c9a9164ad807a",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "Creating a 'StringBuilder' in a loop.",
    "created_at": "2025-11-03T23:02:13Z",
    "updated_at": "2025-11-03T23:02:20Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102271",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102271"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102271"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102271/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": null,
    "original_line": 75,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 75,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102279",
    "pull_request_review_id": 3413273577,
    "id": 2488102279,
    "node_id": "PRRC_kwDOKSXUF86UTW2H",
    "diff_hunk": "@@ -0,0 +1,89 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Text;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies\n+{\n+    /// <summary>\n+    /// Markdown-aware text splitter that respects markdown structure.\n+    /// </summary>\n+    public class MarkdownTextSplitter : ChunkingStrategyBase\n+    {\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"MarkdownTextSplitter\"/> class.\n+        /// </summary>\n+        /// <param name=\"chunkSize\">The maximum size of each chunk.</param>\n+        /// <param name=\"chunkOverlap\">The overlap between consecutive chunks.</param>\n+        public MarkdownTextSplitter(int chunkSize = 1000, int chunkOverlap = 200)\n+            : base(chunkSize, chunkOverlap)\n+        {\n+        }\n+\n+        /// <summary>\n+        /// Chunks markdown text while preserving structure.\n+        /// </summary>\n+        /// <param name=\"text\">The markdown text to chunk.</param>\n+        /// <returns>A collection of markdown chunks with positions.</returns>\n+        protected override IEnumerable<(string Chunk, int StartPosition, int EndPosition)> ChunkCore(string text)\n+        {\n+            var separators = new[] { \"\\n## \", \"\\n### \", \"\\n#### \", \"\\n\\n\", \"\\n\", \". \", \" \" };\n+            var chunks = SplitTextRecursive(text, separators);\n+            \n+            var position = 0;\n+            foreach (var chunk in chunks)\n+            {\n+                var endPos = position + chunk.Length;\n+                yield return (chunk, position, endPos);\n+                position = endPos - ChunkOverlap;\n+            }\n+        }\n+\n+        private List<string> SplitTextRecursive(string text, string[] separators)\n+        {\n+            var chunks = new List<string>();\n+\n+            if (text.Length <= ChunkSize)\n+            {\n+                chunks.Add(text);\n+                return chunks;\n+            }\n+\n+            foreach (var separator in separators)\n+            {\n+                var splits = text.Split(new[] { separator }, StringSplitOptions.None);\n+                var currentChunk = new StringBuilder();",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/MarkdownTextSplitter.cs",
    "commit_id": "c0be21878ba289b4ea925eacff9c9a9164ad807a",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "Creating a 'StringBuilder' in a loop.",
    "created_at": "2025-11-03T23:02:13Z",
    "updated_at": "2025-11-03T23:02:20Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102279",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102279"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102279"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102279/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": null,
    "original_line": 54,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 54,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102285",
    "pull_request_review_id": 3413273577,
    "id": 2488102285,
    "node_id": "PRRC_kwDOKSXUF86UTW2N",
    "diff_hunk": "@@ -0,0 +1,110 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Text;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies\n+{\n+    /// <summary>\n+    /// Code-aware text splitter that respects code structure and syntax.\n+    /// </summary>\n+    public class CodeAwareTextSplitter : ChunkingStrategyBase\n+    {\n+        private readonly string _language;\n+\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"CodeAwareTextSplitter\"/> class.\n+        /// </summary>\n+        /// <param name=\"chunkSize\">The maximum size of each chunk.</param>\n+        /// <param name=\"chunkOverlap\">The overlap between consecutive chunks.</param>\n+        /// <param name=\"language\">The programming language (e.g., \"csharp\", \"python\", \"javascript\").</param>\n+        public CodeAwareTextSplitter(int chunkSize = 1000, int chunkOverlap = 200, string language = \"csharp\")\n+            : base(chunkSize, chunkOverlap)\n+        {\n+            _language = language ?? throw new ArgumentNullException(nameof(language));\n+        }\n+\n+        /// <summary>\n+        /// Chunks code while preserving code structure.\n+        /// </summary>\n+        /// <param name=\"text\">The code text to chunk.</param>\n+        /// <returns>A collection of code chunks with positions.</returns>\n+        protected override IEnumerable<(string Chunk, int StartPosition, int EndPosition)> ChunkCore(string text)\n+        {\n+            var separators = GetLanguageSeparators(_language);\n+            var chunks = SplitTextRecursive(text, separators);\n+            \n+            var position = 0;\n+            foreach (var chunk in chunks)\n+            {\n+                var endPos = position + chunk.Length;\n+                yield return (chunk, position, endPos);\n+                position = endPos - ChunkOverlap;\n+            }\n+        }\n+\n+        private string[] GetLanguageSeparators(string language)\n+        {\n+            switch (language.ToLowerInvariant())\n+            {\n+                case \"csharp\":\n+                case \"c#\":\n+                    return new[] { \"\\n    }\\n\", \"\\n}\\n\", \"\\n\\n\", \"\\n\", \" \" };\n+                case \"python\":\n+                    return new[] { \"\\ndef \", \"\\nclass \", \"\\n\\n\", \"\\n\", \" \" };\n+                case \"javascript\":\n+                case \"typescript\":\n+                    return new[] { \"\\n}\\n\", \"\\n\\n\", \"\\n\", \" \" };\n+                default:\n+                    return new[] { \"\\n\\n\", \"\\n\", \" \" };\n+            }\n+        }\n+\n+        private List<string> SplitTextRecursive(string text, string[] separators)\n+        {\n+            var chunks = new List<string>();\n+\n+            if (text.Length <= ChunkSize)\n+            {\n+                chunks.Add(text);\n+                return chunks;\n+            }\n+\n+            foreach (var separator in separators)\n+            {\n+                var splits = text.Split(new[] { separator }, StringSplitOptions.None);\n+                var currentChunk = new StringBuilder();\n+\n+                foreach (var split in splits)\n+                {\n+                    if (currentChunk.Length + split.Length + separator.Length > ChunkSize)\n+                    {\n+                        if (currentChunk.Length > 0)\n+                        {\n+                            chunks.Add(currentChunk.ToString());\n+                            var overlap = Math.Min(ChunkOverlap, currentChunk.Length);\n+                            currentChunk = new StringBuilder(currentChunk.ToString(currentChunk.Length - overlap, overlap));\n+                        }\n+                    }\n+\n+                    if (currentChunk.Length > 0)\n+                    {\n+                        currentChunk.Append(separator);\n+                    }",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/CodeAwareTextSplitter.cs",
    "commit_id": "5389cc536ebb509f6c373d578d6191f52e74986b",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "These 'if' statements can be combined.\n```suggestion\n                            currentChunk.Append(separator);\n                        }\n```",
    "created_at": "2025-11-03T23:02:13Z",
    "updated_at": "2025-11-03T23:02:20Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102285",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102285"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102285"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102285/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 86,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 92,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 92,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102293",
    "pull_request_review_id": 3413273577,
    "id": 2488102293,
    "node_id": "PRRC_kwDOKSXUF86UTW2V",
    "diff_hunk": "@@ -0,0 +1,89 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Text;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies\n+{\n+    /// <summary>\n+    /// Markdown-aware text splitter that respects markdown structure.\n+    /// </summary>\n+    public class MarkdownTextSplitter : ChunkingStrategyBase\n+    {\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"MarkdownTextSplitter\"/> class.\n+        /// </summary>\n+        /// <param name=\"chunkSize\">The maximum size of each chunk.</param>\n+        /// <param name=\"chunkOverlap\">The overlap between consecutive chunks.</param>\n+        public MarkdownTextSplitter(int chunkSize = 1000, int chunkOverlap = 200)\n+            : base(chunkSize, chunkOverlap)\n+        {\n+        }\n+\n+        /// <summary>\n+        /// Chunks markdown text while preserving structure.\n+        /// </summary>\n+        /// <param name=\"text\">The markdown text to chunk.</param>\n+        /// <returns>A collection of markdown chunks with positions.</returns>\n+        protected override IEnumerable<(string Chunk, int StartPosition, int EndPosition)> ChunkCore(string text)\n+        {\n+            var separators = new[] { \"\\n## \", \"\\n### \", \"\\n#### \", \"\\n\\n\", \"\\n\", \". \", \" \" };\n+            var chunks = SplitTextRecursive(text, separators);\n+            \n+            var position = 0;\n+            foreach (var chunk in chunks)\n+            {\n+                var endPos = position + chunk.Length;\n+                yield return (chunk, position, endPos);\n+                position = endPos - ChunkOverlap;\n+            }\n+        }\n+\n+        private List<string> SplitTextRecursive(string text, string[] separators)\n+        {\n+            var chunks = new List<string>();\n+\n+            if (text.Length <= ChunkSize)\n+            {\n+                chunks.Add(text);\n+                return chunks;\n+            }\n+\n+            foreach (var separator in separators)\n+            {\n+                var splits = text.Split(new[] { separator }, StringSplitOptions.None);\n+                var currentChunk = new StringBuilder();\n+\n+                foreach (var split in splits)\n+                {\n+                    if (currentChunk.Length + split.Length + separator.Length > ChunkSize)\n+                    {\n+                        if (currentChunk.Length > 0)\n+                        {\n+                            chunks.Add(currentChunk.ToString());\n+                            var overlap = Math.Min(ChunkOverlap, currentChunk.Length);\n+                            currentChunk = new StringBuilder(currentChunk.ToString(currentChunk.Length - overlap, overlap));\n+                        }",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/MarkdownTextSplitter.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "These 'if' statements can be combined.\n```suggestion\n                    if (currentChunk.Length + split.Length + separator.Length > ChunkSize && currentChunk.Length > 0)\n                    {\n                        chunks.Add(currentChunk.ToString());\n                        var overlap = Math.Min(ChunkOverlap, currentChunk.Length);\n                        currentChunk = new StringBuilder(currentChunk.ToString(currentChunk.Length - overlap, overlap));\n```",
    "created_at": "2025-11-03T23:02:14Z",
    "updated_at": "2025-11-03T23:02:20Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102293",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102293"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102293"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102293/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 61,
    "original_start_line": 58,
    "start_side": "RIGHT",
    "line": 68,
    "original_line": 65,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 65,
    "position": 68,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102303",
    "pull_request_review_id": 3413273577,
    "id": 2488102303,
    "node_id": "PRRC_kwDOKSXUF86UTW2f",
    "diff_hunk": "@@ -0,0 +1,106 @@\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using System;\n+using System.Collections.Generic;\n+using System.Text;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies\n+{\n+    /// <summary>\n+    /// Recursive character-based text splitting that preserves semantic meaning.\n+    /// </summary>\n+    public class RecursiveCharacterTextSplitter : ChunkingStrategyBase\n+    {\n+        private readonly string[] _separators;\n+\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"RecursiveCharacterTextSplitter\"/> class.\n+        /// </summary>\n+        /// <param name=\"chunkSize\">The maximum size of each chunk.</param>\n+        /// <param name=\"chunkOverlap\">The overlap between consecutive chunks.</param>\n+        /// <param name=\"separators\">The separators to use for splitting, in order of preference.</param>\n+        public RecursiveCharacterTextSplitter(\n+            int chunkSize = 1000,\n+            int chunkOverlap = 200,\n+            string[]? separators = null)\n+            : base(chunkSize, chunkOverlap)\n+        {\n+            _separators = separators ?? new[] { \"\\n\\n\", \"\\n\", \". \", \" \", \"\" };\n+        }\n+\n+        /// <summary>\n+        /// Splits the input text into chunks recursively.\n+        /// </summary>\n+        /// <param name=\"text\">The text to split.</param>\n+        /// <returns>A collection of text chunks with positions.</returns>\n+        protected override IEnumerable<(string Chunk, int StartPosition, int EndPosition)> ChunkCore(string text)\n+        {\n+            var chunks = SplitTextRecursive(text, _separators);\n+            \n+            var position = 0;\n+            foreach (var chunk in chunks)\n+            {\n+                var endPos = position + chunk.Length;\n+                yield return (chunk, position, endPos);\n+                position = endPos - ChunkOverlap;\n+            }\n+        }\n+\n+        private List<string> SplitTextRecursive(string text, string[] separators)\n+        {\n+            var chunks = new List<string>();\n+\n+            if (text.Length <= ChunkSize)\n+            {\n+                chunks.Add(text);\n+                return chunks;\n+            }\n+\n+            foreach (var separator in separators)\n+            {\n+                if (string.IsNullOrEmpty(separator))\n+                {\n+                    for (int i = 0; i < text.Length; i += ChunkSize - ChunkOverlap)\n+                    {\n+                        var length = Math.Min(ChunkSize, text.Length - i);\n+                        chunks.Add(text.Substring(i, length));\n+                    }\n+                    return chunks;\n+                }\n+\n+                var splits = text.Split(new[] { separator }, StringSplitOptions.None);\n+                var currentChunk = new StringBuilder();\n+\n+                foreach (var split in splits)\n+                {\n+                    if (currentChunk.Length + split.Length + separator.Length > ChunkSize)\n+                    {\n+                        if (currentChunk.Length > 0)\n+                        {\n+                            chunks.Add(currentChunk.ToString());\n+                            var overlap = Math.Min(ChunkOverlap, currentChunk.Length);\n+                            currentChunk = new StringBuilder(currentChunk.ToString(currentChunk.Length - overlap, overlap));\n+                        }",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/RecursiveCharacterTextSplitter.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "These 'if' statements can be combined.\n```suggestion\n                    if (currentChunk.Length + split.Length + separator.Length > ChunkSize && currentChunk.Length > 0)\n                    {\n                        chunks.Add(currentChunk.ToString());\n                        var overlap = Math.Min(ChunkOverlap, currentChunk.Length);\n                        currentChunk = new StringBuilder(currentChunk.ToString(currentChunk.Length - overlap, overlap));\n```",
    "created_at": "2025-11-03T23:02:14Z",
    "updated_at": "2025-11-03T23:02:21Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102303",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102303"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102303"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102303/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 75,
    "original_start_line": 75,
    "start_side": "RIGHT",
    "line": 82,
    "original_line": 82,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 82,
    "position": 82,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102310",
    "pull_request_review_id": 3413273577,
    "id": 2488102310,
    "node_id": "PRRC_kwDOKSXUF86UTW2m",
    "diff_hunk": "@@ -0,0 +1,124 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\n+{\n+    /// <summary>\n+    /// BM25 (Best Matching 25) retrieval algorithm for sparse keyword-based search.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class BM25Retriever<T> : RetrieverBase<T>\n+    {\n+        private readonly IDocumentStore<T> _documentStore;\n+        private readonly T _k1;\n+        private readonly T _b;\n+        private readonly Dictionary<string, Dictionary<string, T>> _termFrequencies;\n+        private readonly Dictionary<string, T> _documentLengths;\n+        private T _avgDocLength;",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/BM25Retriever.cs",
    "commit_id": "3f01d96a3a3d3acc6002ef29bdfdec69bfddc09e",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "Field '_avgDocLength' can be 'readonly'.",
    "created_at": "2025-11-03T23:02:14Z",
    "updated_at": "2025-11-03T23:02:21Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102310",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102310"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102310"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102310/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": null,
    "original_line": 21,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 21,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102316",
    "pull_request_review_id": 3413273577,
    "id": 2488102316,
    "node_id": "PRRC_kwDOKSXUF86UTW2s",
    "diff_hunk": "@@ -0,0 +1,124 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores\n+{\n+    /// <summary>\n+    /// Hybrid document store combining vector and keyword search strategies.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class HybridDocumentStore<T> : DocumentStoreBase<T>\n+    {\n+        private readonly IDocumentStore<T> _vectorStore;\n+        private readonly IDocumentStore<T> _keywordStore;\n+        private readonly T _vectorWeight;\n+        private readonly T _keywordWeight;\n+\n+        public override int DocumentCount => _vectorStore.DocumentCount;\n+        public override int VectorDimension => _vectorStore.VectorDimension;\n+\n+        public HybridDocumentStore(\n+            IDocumentStore<T> vectorStore,\n+            IDocumentStore<T> keywordStore,\n+            T vectorWeight,\n+            T keywordWeight)\n+        {\n+            if (vectorStore == null)\n+                throw new ArgumentNullException(nameof(vectorStore));\n+            if (keywordStore == null)\n+                throw new ArgumentNullException(nameof(keywordStore));\n+\n+            _vectorStore = vectorStore;\n+            _keywordStore = keywordStore;\n+            _vectorWeight = vectorWeight;\n+            _keywordWeight = keywordWeight;\n+        }\n+\n+        protected override void AddCore(VectorDocument<T> vectorDocument)\n+        {\n+            _vectorStore.Add(vectorDocument);\n+            _keywordStore.Add(vectorDocument);\n+        }\n+\n+        protected override void AddBatchCore(IList<VectorDocument<T>> vectorDocuments)\n+        {\n+            _vectorStore.AddBatch(vectorDocuments);\n+            _keywordStore.AddBatch(vectorDocuments);\n+        }\n+\n+        protected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)\n+        {\n+            var vectorResults = _vectorStore.GetSimilarWithFilters(queryVector, topK * 2, metadataFilters);\n+            var keywordResults = _keywordStore.GetSimilarWithFilters(queryVector, topK * 2, metadataFilters);\n+\n+            var combinedScores = new Dictionary<string, T>();\n+\n+            foreach (var doc in vectorResults)\n+            {\n+                if (doc.HasRelevanceScore)\n+                {\n+                    var score = NumOps.Multiply(_vectorWeight, doc.RelevanceScore);\n+                    combinedScores[doc.Id] = score;\n+                }\n+            }\n+\n+            foreach (var doc in keywordResults)\n+            {\n+                if (doc.HasRelevanceScore)\n+                {\n+                    var keywordScore = NumOps.Multiply(_keywordWeight, doc.RelevanceScore);\n+                    if (combinedScores.ContainsKey(doc.Id))\n+                    {\n+                        combinedScores[doc.Id] = NumOps.Add(combinedScores[doc.Id], keywordScore);\n+                    }\n+                    else\n+                    {\n+                        combinedScores[doc.Id] = keywordScore;\n+                    }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/HybridDocumentStore.cs",
    "commit_id": "84f99e221d10d12f47a4fcccf4d1fffe7a2556f2",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "Both branches of this 'if' statement write to the same variable - consider using '?' to express intent better.\n```suggestion\n                    combinedScores[doc.Id] = combinedScores.ContainsKey(doc.Id)\n                        ? NumOps.Add(combinedScores[doc.Id], keywordScore)\n                        : keywordScore;\n```",
    "created_at": "2025-11-03T23:02:15Z",
    "updated_at": "2025-11-03T23:02:21Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102316",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102316"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102316"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102316/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 75,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 82,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 82,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102323",
    "pull_request_review_id": 3413273577,
    "id": 2488102323,
    "node_id": "PRRC_kwDOKSXUF86UTW2z",
    "diff_hunk": "@@ -0,0 +1,122 @@\n+using System;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System.Collections.Generic;\n+using System.Linq;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.RerankingStrategies\n+{\n+    /// <summary>\n+    /// Reciprocal Rank Fusion for combining multiple ranking lists.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class ReciprocalRankFusion<T> : RerankingStrategyBase<T>\n+    {\n+        private readonly INumericOperations<T> _numOps;\n+        private readonly int _k;\n+\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"ReciprocalRankFusion{T}\"/> class.\n+        /// </summary>\n+        /// <param name=\"numericOperations\">The numeric operations for type T.</param>\n+        /// <param name=\"k\">The constant k for reciprocal rank formula (default: 60).</param>\n+        public ReciprocalRankFusion(INumericOperations<T> numericOperations, int k = 60) : base(numericOperations)\n+        {\n+            _numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\n+            _k = k > 0 ? k : throw new ArgumentOutOfRangeException(nameof(k));\n+        }\n+\n+        /// <summary>\n+        /// Reranks documents using reciprocal rank fusion.\n+        /// </summary>\n+        /// <param name=\"query\">The query string.</param>\n+        /// <param name=\"documents\">The documents to rerank.</param>\n+        /// <param name=\"topK\">The number of top documents to return.</param>\n+        /// <returns>A reranked list of documents.</returns>\n+        public override List<Document<T>> Rerank(string query, List<Document<T>> documents, int topK)\n+        {\n+            if (string.IsNullOrEmpty(query)) throw new ArgumentNullException(nameof(query));\n+            if (documents == null) throw new ArgumentNullException(nameof(documents));\n+            if (topK <= 0) throw new ArgumentOutOfRangeException(nameof(topK));\n+\n+            var scores = new Dictionary<string, T>();\n+\n+            for (int rank = 0; rank < documents.Count; rank++)\n+            {\n+                var doc = documents[rank];\n+                var rrfScore = _numOps.FromDouble(1.0 / (_k + rank + 1));\n+\n+                if (scores.ContainsKey(doc.Id))\n+                {\n+                    scores[doc.Id] = _numOps.Add(scores[doc.Id], rrfScore);\n+                }\n+                else\n+                {\n+                    scores[doc.Id] = rrfScore;\n+                }",
    "path": "src/RetrievalAugmentedGeneration/RerankingStrategies/ReciprocalRankFusion.cs",
    "commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "Both branches of this 'if' statement write to the same variable - consider using '?' to express intent better.",
    "created_at": "2025-11-03T23:02:15Z",
    "updated_at": "2025-11-03T23:02:21Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102323",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102323"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102323"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102323/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 49,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 56,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 56,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102332",
    "pull_request_review_id": 3413273577,
    "id": 2488102332,
    "node_id": "PRRC_kwDOKSXUF86UTW28",
    "diff_hunk": "@@ -0,0 +1,92 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\n+{\n+    /// <summary>\n+    /// Hybrid retriever combining dense and sparse retrieval strategies.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class HybridRetriever<T> : RetrieverBase<T>\n+    {\n+        private readonly IRetriever<T> _denseRetriever;\n+        private readonly IRetriever<T> _sparseRetriever;\n+        private readonly T _denseWeight;\n+        private readonly T _sparseWeight;\n+\n+        public HybridRetriever(\n+            IRetriever<T> denseRetriever, \n+            IRetriever<T> sparseRetriever, \n+            double denseWeight = 0.7, \n+            double sparseWeight = 0.3,\n+            int defaultTopK = 5) \n+            : base(defaultTopK)\n+        {\n+            if (denseRetriever == null)\n+                throw new ArgumentNullException(nameof(denseRetriever));\n+            if (sparseRetriever == null)\n+                throw new ArgumentNullException(nameof(sparseRetriever));\n+\n+            _denseRetriever = denseRetriever;\n+            _sparseRetriever = sparseRetriever;\n+            _denseWeight = NumOps.FromDouble(denseWeight);\n+            _sparseWeight = NumOps.FromDouble(sparseWeight);\n+        }\n+\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\n+        {\n+            var denseResults = _denseRetriever.Retrieve(query, topK * 2, metadataFilters);\n+            var sparseResults = _sparseRetriever.Retrieve(query, topK * 2, metadataFilters);\n+\n+            var combinedScores = new Dictionary<string, T>();\n+\n+            foreach (var doc in denseResults)\n+            {\n+                if (doc.HasRelevanceScore)\n+                {\n+                    var score = NumOps.Multiply(_denseWeight, doc.RelevanceScore);\n+                    combinedScores[doc.Id] = score;\n+                }\n+            }\n+\n+            foreach (var doc in sparseResults)\n+            {\n+                if (doc.HasRelevanceScore)\n+                {\n+                    var sparseScore = NumOps.Multiply(_sparseWeight, doc.RelevanceScore);\n+                    if (combinedScores.ContainsKey(doc.Id))\n+                    {\n+                        combinedScores[doc.Id] = NumOps.Add(combinedScores[doc.Id], sparseScore);\n+                    }\n+                    else\n+                    {\n+                        combinedScores[doc.Id] = sparseScore;\n+                    }",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/HybridRetriever.cs",
    "commit_id": "84f99e221d10d12f47a4fcccf4d1fffe7a2556f2",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "Copilot",
      "id": 175728472,
      "node_id": "BOT_kgDOCnlnWA",
      "avatar_url": "https://avatars.githubusercontent.com/in/946600?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Copilot",
      "html_url": "https://github.com/apps/copilot-pull-request-reviewer",
      "followers_url": "https://api.github.com/users/Copilot/followers",
      "following_url": "https://api.github.com/users/Copilot/following{/other_user}",
      "gists_url": "https://api.github.com/users/Copilot/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Copilot/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Copilot/subscriptions",
      "organizations_url": "https://api.github.com/users/Copilot/orgs",
      "repos_url": "https://api.github.com/users/Copilot/repos",
      "events_url": "https://api.github.com/users/Copilot/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Copilot/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "Both branches of this 'if' statement write to the same variable - consider using '?' to express intent better.",
    "created_at": "2025-11-03T23:02:15Z",
    "updated_at": "2025-11-03T23:02:21Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102332",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102332"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488102332"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488102332/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 61,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 68,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 68,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108075",
    "pull_request_review_id": 3413280905,
    "id": 2488108075,
    "node_id": "PRRC_kwDOKSXUF86UTYQr",
    "diff_hunk": "@@ -0,0 +1,72 @@\n+# AI Assistant Project Rules\n+\n+## Code Architecture Rules\n+\n+### Generic Types and INumericOperations\n+1. **NEVER use default(T)** - Instead use:\n+   - NumOps.Zero for zero values\n+   - NumOps.One for one values\n+   - NumOps.FromDouble(value) to convert from double\n+   \n+2. **Always include INumericOperations in base classes**:\n+   `csharp\n+   protected static readonly INumericOperations&lt;T&gt; NumOps = MathHelper.GetNumericOperations&lt;T&gt;();\n+   `",
    "path": ".github/AI_PROJECT_RULES.md",
    "commit_id": "21d8da8115e66a3f5de77dd822141f95e95cb167",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Fix HTML entity encoding in markdown code blocks.**\n\nLines 13 and 18 use `&lt;T&gt;` (HTML entities) instead of `<T>`. In raw markdown, generic syntax should use angle brackets directly.\n\n\nApply this diff:\n\n```diff\n 2. **Always include INumericOperations in base classes**:\n-   `csharp\n-   protected static readonly INumericOperations&lt;T&gt; NumOps = MathHelper.GetNumericOperations&lt;T&gt;();\n-   `\n+   ```csharp\n+   protected static readonly INumericOperations<T> NumOps = MathHelper.GetNumericOperations<T>();\n+   ```\n\n 4. **Use custom data types** - Prefer Vector&lt;T&gt;, Matrix&lt;T&gt;, Tensor&lt;T&gt; over arrays and collections\n+4. **Use custom data types** - Prefer Vector<T>, Matrix<T>, Tensor<T> over arrays and collections\n```\n\n\nAlso applies to: 18-18\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn .github/AI_PROJECT_RULES.md around lines 11ΓÇô14 (and also line 18), fix\nHTML-entity encoded generics by replacing &lt;T&gt; and\nVector&lt;T&gt;/Matrix&lt;T&gt;/Tensor&lt;T&gt; with literal angle-bracket\ngenerics (<T>, Vector<T>, Matrix<T>, Tensor<T>), and ensure the fenced code\nblock is properly opened and closed (move or add the closing triple backticks\nafter the C# code line) so the code renders correctly in Markdown.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-03T23:05:38Z",
    "updated_at": "2025-11-03T23:05:43Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108075",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108075"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108075"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108075/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 11,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 14,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 14,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108088",
    "pull_request_review_id": 3413280905,
    "id": 2488108088,
    "node_id": "PRRC_kwDOKSXUF86UTYQ4",
    "diff_hunk": "@@ -0,0 +1,72 @@\n+# AI Assistant Project Rules\n+\n+## Code Architecture Rules\n+\n+### Generic Types and INumericOperations\n+1. **NEVER use default(T)** - Instead use:\n+   - NumOps.Zero for zero values\n+   - NumOps.One for one values\n+   - NumOps.FromDouble(value) to convert from double\n+   \n+2. **Always include INumericOperations in base classes**:\n+   `csharp\n+   protected static readonly INumericOperations&lt;T&gt; NumOps = MathHelper.GetNumericOperations&lt;T&gt;();\n+   `\n+\n+3. **Use generic types everywhere** - Avoid hardcoding double, float, int, etc.\n+\n+4. **Use custom data types** - Prefer Vector&lt;T&gt;, Matrix&lt;T&gt;, Tensor&lt;T&gt; over arrays and collections\n+\n+5. **NEVER use constraints like where T : struct** - Our architecture doesn't use these\n+\n+### Inheritance Pattern\n+1. **Always create interfaces first**\n+2. **Then create base classes with common methods**\n+3. **Concrete implementations MUST inherit from base class, NOT directly from interface**\n+\n+### Documentation\n+1. **Follow existing documentation format exactly**\n+2. **Include XML documentation for all public members**\n+3. **Add beginner-friendly explanations in remarks**\n+\n+## File Management Rules\n+\n+### DO NOT Create Unless Explicitly Requested\n+1. **NO README files** (AGENTS.md, STATUS.md, REPORT.md, etc.)\n+2. **NO one-off scripts** unless specifically asked\n+3. **NO temporary analysis files**\n+4. **NO status tracking markdown files**\n+\n+### What TO Create\n+1. Source code files (.cs)\n+2. Configuration files when needed for functionality\n+3. Documentation only when explicitly requested by name/path\n+\n+## Git Workflow\n+\n+1. **Always work on feature branches**\n+2. **Commit early and often**\n+3. **Use descriptive commit messages**\n+4. **Create PR when complete**\n+5. **DO NOT close PRs**, fix them properly instead\n+\n+## Code Organization\n+\n+1. **Folders match namespaces** - No \"Base\" folder, use proper namespace hierarchy\n+2. **Examples go in separate test project** - C:\\Users\\cheat\\source\\repos\\AiDotNet\\testconsole\\AiDotNetTestConsole.csproj",
    "path": ".github/AI_PROJECT_RULES.md",
    "commit_id": "21d8da8115e66a3f5de77dd822141f95e95cb167",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Replace hardcoded developer path with relative or generic reference.**\n\nThe absolute Windows path `C:\\Users\\cheat\\source\\repos\\AiDotNet\\testconsole\\AiDotNetTestConsole.csproj` is not portable across machines, operating systems, or team members.\n\n\nApply this diff:\n\n```diff\n-2. **Examples go in separate test project** - C:\\Users\\cheat\\source\\repos\\AiDotNet\\testconsole\\AiDotNetTestConsole.csproj\n+2. **Examples go in separate test project** - `testconsole/AiDotNetTestConsole.csproj`\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n2. **Examples go in separate test project** - `testconsole/AiDotNetTestConsole.csproj`\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn .github/AI_PROJECT_RULES.md around line 56, replace the hardcoded absolute\nWindows path\n\"C:\\Users\\cheat\\source\\repos\\AiDotNet\\testconsole\\AiDotNetTestConsole.csproj\"\nwith a relative or generic reference; update it to a repo-relative path (e.g.,\n./testconsole/AiDotNetTestConsole.csproj) or a platform-agnostic placeholder\nlike <path-to-test-project>/AiDotNetTestConsole.csproj so the rule is portable\nacross machines and OSes.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-03T23:05:38Z",
    "updated_at": "2025-11-03T23:05:43Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108088",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108088"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108088"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108088/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": null,
    "original_line": 56,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 56,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108103",
    "pull_request_review_id": 3413280905,
    "id": 2488108103,
    "node_id": "PRRC_kwDOKSXUF86UTYRH",
    "diff_hunk": "@@ -0,0 +1,110 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Text;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies\n+{\n+    /// <summary>\n+    /// Code-aware text splitter that respects code structure and syntax.\n+    /// </summary>\n+    public class CodeAwareTextSplitter : ChunkingStrategyBase\n+    {\n+        private readonly string _language;\n+\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"CodeAwareTextSplitter\"/> class.\n+        /// </summary>\n+        /// <param name=\"chunkSize\">The maximum size of each chunk.</param>\n+        /// <param name=\"chunkOverlap\">The overlap between consecutive chunks.</param>\n+        /// <param name=\"language\">The programming language (e.g., \"csharp\", \"python\", \"javascript\").</param>\n+        public CodeAwareTextSplitter(int chunkSize = 1000, int chunkOverlap = 200, string language = \"csharp\")\n+            : base(chunkSize, chunkOverlap)\n+        {\n+            _language = language ?? throw new ArgumentNullException(nameof(language));\n+        }\n+\n+        /// <summary>\n+        /// Chunks code while preserving code structure.\n+        /// </summary>\n+        /// <param name=\"text\">The code text to chunk.</param>\n+        /// <returns>A collection of code chunks with positions.</returns>\n+        protected override IEnumerable<(string Chunk, int StartPosition, int EndPosition)> ChunkCore(string text)\n+        {\n+            var separators = GetLanguageSeparators(_language);\n+            var chunks = SplitTextRecursive(text, separators);\n+            \n+            var position = 0;\n+            foreach (var chunk in chunks)\n+            {\n+                var endPos = position + chunk.Length;\n+                yield return (chunk, position, endPos);\n+                position = endPos - ChunkOverlap;\n+            }\n+        }\n+\n+        private string[] GetLanguageSeparators(string language)\n+        {\n+            switch (language.ToLowerInvariant())\n+            {\n+                case \"csharp\":\n+                case \"c#\":\n+                    return new[] { \"\\n    }\\n\", \"\\n}\\n\", \"\\n\\n\", \"\\n\", \" \" };\n+                case \"python\":\n+                    return new[] { \"\\ndef \", \"\\nclass \", \"\\n\\n\", \"\\n\", \" \" };\n+                case \"javascript\":\n+                case \"typescript\":\n+                    return new[] { \"\\n}\\n\", \"\\n\\n\", \"\\n\", \" \" };\n+                default:\n+                    return new[] { \"\\n\\n\", \"\\n\", \" \" };\n+            }\n+        }\n+\n+        private List<string> SplitTextRecursive(string text, string[] separators)\n+        {\n+            var chunks = new List<string>();\n+\n+            if (text.Length <= ChunkSize)\n+            {\n+                chunks.Add(text);\n+                return chunks;\n+            }\n+\n+            foreach (var separator in separators)\n+            {\n+                var splits = text.Split(new[] { separator }, StringSplitOptions.None);\n+                var currentChunk = new StringBuilder();\n+\n+                foreach (var split in splits)\n+                {\n+                    if (currentChunk.Length + split.Length + separator.Length > ChunkSize)\n+                    {\n+                        if (currentChunk.Length > 0)\n+                        {\n+                            chunks.Add(currentChunk.ToString());\n+                            var overlap = Math.Min(ChunkOverlap, currentChunk.Length);\n+                            currentChunk = new StringBuilder(currentChunk.ToString(currentChunk.Length - overlap, overlap));\n+                        }\n+                    }\n+\n+                    if (currentChunk.Length > 0)\n+                    {\n+                        currentChunk.Append(separator);\n+                    }\n+                    currentChunk.Append(split);\n+                }\n+\n+                if (currentChunk.Length > 0)\n+                {\n+                    chunks.Add(currentChunk.ToString());\n+                }",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/CodeAwareTextSplitter.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Enforce the configured chunk size even when no separators match.**\n\nRight now, if a `split` exceeds `ChunkSize` (common with long minified code or binary blobs), we append it wholesale. That produces chunks larger than the configured limit and defeats overlap logic, which can explode memory usage. Please add the same fallback the other splitters useΓÇöslice `split` into `ChunkSize`-bounded segments (respecting overlap) before continuing.\n\n\n\n```diff\n                     if (currentChunk.Length + split.Length + separator.Length > ChunkSize)\n                     {\n                         if (currentChunk.Length > 0)\n                         {\n                             chunks.Add(currentChunk.ToString());\n                             var overlap = Math.Min(ChunkOverlap, currentChunk.Length);\n                             currentChunk = new StringBuilder(currentChunk.ToString(currentChunk.Length - overlap, overlap));\n                         }\n                     }\n \n+                    if (currentChunk.Length == 0 && split.Length > ChunkSize)\n+                    {\n+                        var step = Math.Max(ChunkSize - ChunkOverlap, 1);\n+                        for (int i = 0; i < split.Length; i += step)\n+                        {\n+                            var length = Math.Min(ChunkSize, split.Length - i);\n+                            chunks.Add(split.Substring(i, length));\n+                        }\n+                        currentChunk.Clear();\n+                        continue;\n+                    }\n+\n                     if (currentChunk.Length > 0)\n                     {\n                         currentChunk.Append(separator);\n                     }\n                     currentChunk.Append(split);\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/ChunkingStrategies/CodeAwareTextSplitter.cs\naround lines 77 to 99, the loop currently appends a single `split` even when its\nlength exceeds `ChunkSize`, producing oversized chunks; modify the logic so that\nbefore appending a too-large `split` you slice it into multiple segments each no\nlonger than `ChunkSize`, adding each segment to `chunks` and preserving\n`ChunkOverlap` by starting the next buffer with the last `overlap` chars of the\nprevious segment; ensure separators are only inserted between segments as\ncurrently intended and that after processing all full-sized segments you\ncontinue with the remaining tail (if any) into `currentChunk` so overlap and\nfinal flush behave the same as for other splitters.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-03T23:05:38Z",
    "updated_at": "2025-11-03T23:05:43Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108103",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108103"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108103"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108103/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 79,
    "original_start_line": 77,
    "start_side": "RIGHT",
    "line": 113,
    "original_line": 99,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 99,
    "position": 113,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108112",
    "pull_request_review_id": 3413280905,
    "id": 2488108112,
    "node_id": "PRRC_kwDOKSXUF86UTYRQ",
    "diff_hunk": "@@ -0,0 +1,99 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies\n+{\n+    /// <summary>\n+    /// Semantic-based text chunking that uses embeddings to group related content.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class SemanticChunkingStrategy<T> : ChunkingStrategyBase\n+    {\n+        private readonly INumericOperations<T> _numOps;\n+        private readonly T _similarityThreshold;\n+\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"SemanticChunkingStrategy{T}\"/> class.\n+        /// </summary>\n+        /// <param name=\"numericOperations\">The numeric operations for type T.</param>\n+        /// <param name=\"similarityThreshold\">The similarity threshold for grouping sentences.</param>\n+        /// <param name=\"maxChunkSize\">The maximum chunk size in characters.</param>\n+        public SemanticChunkingStrategy(\n+            INumericOperations<T> numericOperations,\n+            T similarityThreshold,\n+            int maxChunkSize = 1000,\n+            int chunkOverlap = 200)\n+            : base(maxChunkSize, chunkOverlap)\n+        {\n+            _numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\n+            _similarityThreshold = similarityThreshold;\n+        }\n+\n+        /// <summary>\n+        /// Chunks text based on semantic similarity between sentences.\n+        /// </summary>\n+        /// <param name=\"text\">The text to chunk.</param>\n+        /// <returns>A collection of semantically coherent chunks with positions.</returns>\n+        protected override IEnumerable<(string Chunk, int StartPosition, int EndPosition)> ChunkCore(string text)\n+        {\n+            var sentences = SplitIntoSentences(text);\n+            var currentChunk = new List<string>();\n+            var currentSize = 0;\n+            var position = 0;\n+\n+            foreach (var sentence in sentences)\n+            {\n+                if (currentSize + sentence.Length > ChunkSize && currentChunk.Count > 0)\n+                {\n+                    var chunkText = string.Join(\" \", currentChunk);\n+                    var endPos = position + chunkText.Length;\n+                    yield return (chunkText, position, endPos);\n+                    \n+                    position = endPos - ChunkOverlap;\n+                    currentChunk.Clear();\n+                    currentSize = 0;\n+                }\n+\n+                currentChunk.Add(sentence);\n+                currentSize += sentence.Length;\n+            }\n+\n+            if (currentChunk.Count > 0)\n+            {\n+                var chunkText = string.Join(\" \", currentChunk);\n+                yield return (chunkText, position, position + chunkText.Length);\n+            }",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/SemanticChunkingStrategy.cs",
    "commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Semantic chunker never applies semantics or overlap**\n\nThis implementation never uses `_similarityThreshold` (or any embeddings), so we just stream sentences until `ChunkSize` is hitΓÇöidentical to a plain size-based splitter despite the class being wired for semantic grouping. Additionally, when you flush a chunk you `Clear()` the buffer without seeding it with the trailing `ChunkOverlap`, so downstream consumers get no overlapping content even though the returned positions imply there is one. Please wire the chunker into actual similarity scoring (or drop the parameter) and keep the configured overlap in the emitted chunks.\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits ec6a7c9 to 77a1bbb",
    "created_at": "2025-11-03T23:05:38Z",
    "updated_at": "2025-11-04T00:06:52Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108112",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108112"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108112"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108112/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 24,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 67,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 67,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108117",
    "pull_request_review_id": 3413280905,
    "id": 2488108117,
    "node_id": "PRRC_kwDOKSXUF86UTYRV",
    "diff_hunk": "@@ -0,0 +1,44 @@\n+using System;\n+using System.Collections.Generic;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies\n+{\n+    /// <summary>\n+    /// Sliding window chunking strategy with configurable window size and stride.\n+    /// </summary>\n+    public class SlidingWindowChunkingStrategy : ChunkingStrategyBase\n+    {\n+        private readonly int _stride;\n+\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"SlidingWindowChunkingStrategy\"/> class.\n+        /// </summary>\n+        /// <param name=\"windowSize\">The size of the sliding window.</param>\n+        /// <param name=\"stride\">The stride (step size) of the window.</param>\n+        public SlidingWindowChunkingStrategy(int windowSize = 1000, int stride = 500)\n+            : base(windowSize, windowSize - stride)\n+        {\n+            _stride = stride > 0 ? stride : throw new ArgumentOutOfRangeException(nameof(stride));\n+        }",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/SlidingWindowChunkingStrategy.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Prevent negative chunk overlap when stride exceeds window size**\n\nPassing `windowSize - stride` straight to the base class lets `ChunkOverlap` go negative whenever `stride > windowSize`. Downstream consumers rely on that value being non-negative (e.g., for position math and overlap metadata), so this silently corrupts chunk metadata and can break overlap-sensitive logic. Please clamp or reject oversize strides before calling the base constructor.\n\n\n\nApply this diff:\n\n```diff\n-        public SlidingWindowChunkingStrategy(int windowSize = 1000, int stride = 500)\n-            : base(windowSize, windowSize - stride)\n+        public SlidingWindowChunkingStrategy(int windowSize = 1000, int stride = 500)\n+            : base(windowSize, Math.Max(0, windowSize - stride))\n         {\n-            _stride = stride > 0 ? stride : throw new ArgumentOutOfRangeException(nameof(stride));\n+            if (stride <= 0) throw new ArgumentOutOfRangeException(nameof(stride));\n+            if (stride > windowSize)\n+                throw new ArgumentOutOfRangeException(nameof(stride), \"Stride cannot exceed the window size because chunk overlap must stay non-negative.\");\n+            _stride = stride;\n         }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        public SlidingWindowChunkingStrategy(int windowSize = 1000, int stride = 500)\n            : base(windowSize, Math.Max(0, windowSize - stride))\n        {\n            if (stride <= 0) throw new ArgumentOutOfRangeException(nameof(stride));\n            if (stride > windowSize)\n                throw new ArgumentOutOfRangeException(nameof(stride), \"Stride cannot exceed the window size because chunk overlap must stay non-negative.\");\n            _stride = stride;\n        }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn\nsrc/RetrievalAugmentedGeneration/ChunkingStrategies/SlidingWindowChunkingStrategy.cs\naround lines 18 to 22, the constructor currently passes windowSize - stride\ndirectly to the base which can produce a negative ChunkOverlap when stride >\nwindowSize; change the constructor to ensure ChunkOverlap is non-negative by\ncomputing an overlap value (e.g., overlap = Math.Max(0, windowSize - stride))\nand pass that overlap to the base constructor, and also validate stride is\nnon-negative (throw ArgumentOutOfRangeException if stride < 0) before assigning\n_stride.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-03T23:05:38Z",
    "updated_at": "2025-11-03T23:05:43Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108117",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108117"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108117"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108117/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 18,
    "original_start_line": 18,
    "start_side": "RIGHT",
    "line": 25,
    "original_line": 22,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 22,
    "position": 25,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108128",
    "pull_request_review_id": 3413280905,
    "id": 2488108128,
    "node_id": "PRRC_kwDOKSXUF86UTYRg",
    "diff_hunk": "@@ -0,0 +1,35 @@\n+using System.Collections.Generic;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Configuration\n+{\n+    /// <summary>\n+    /// Configuration for embedding models.\n+    /// </summary>\n+    public class EmbeddingConfig\n+    {\n+        /// <summary>\n+        /// Gets or sets the embedding model type.\n+        /// </summary>\n+        public string ModelType { get; set; } = string.Empty;\n+\n+        /// <summary>\n+        /// Gets or sets the path to the embedding model.\n+        /// </summary>\n+        public string ModelPath { get; set; } = string.Empty;\n+\n+        /// <summary>\n+        /// Gets or sets the API key for the embedding service.\n+        /// </summary>\n+        public string ApiKey { get; set; } = string.Empty;",
    "path": "src/RetrievalAugmentedGeneration/Configuration/EmbeddingConfig.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Consider secure handling guidance for API keys.**\n\nStoring API keys as plain strings can lead to inadvertent exposure through logging, serialization, or debugging output. While configuration POCOs typically don't enforce security directly, consider adding XML documentation that warns consumers to protect this value and avoid logging it.\n\n\n\nExample documentation addition:\n\n```diff\n /// <summary>\n /// Gets or sets the API key for the embedding service.\n /// </summary>\n+/// <remarks>\n+/// <b>Security:</b> This value should be protected. Do not log, serialize to disk unencrypted, \n+/// or include in error messages. Consider using secure configuration providers.\n+/// </remarks>\n public string ApiKey { get; set; } = string.Empty;\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        /// <summary>\n        /// Gets or sets the API key for the embedding service.\n        /// </summary>\n        /// <remarks>\n        /// <b>Security:</b> This value should be protected. Do not log, serialize to disk unencrypted, \n        /// or include in error messages. Consider using secure configuration providers.\n        /// </remarks>\n        public string ApiKey { get; set; } = string.Empty;\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Configuration/EmbeddingConfig.cs around\nlines 20 to 23, the ApiKey property lacks guidance about secure handling; update\nits XML documentation to warn consumers not to log, serialize, or expose the API\nkey, and recommend retrieving it from a secure secrets store or protected\nconfiguration (e.g., environment variables, Azure Key Vault, or IOptions bound\nto a secrets provider) rather than embedding it in source or plain-text config;\noptionally mention using secure types or transient in-memory storage and to\ntreat the property as sensitive when serializing or debugging.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-03T23:05:39Z",
    "updated_at": "2025-11-03T23:05:43Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108128",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108128"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108128"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108128/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 20,
    "original_start_line": 20,
    "start_side": "RIGHT",
    "line": 28,
    "original_line": 23,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 23,
    "position": 28,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108140",
    "pull_request_review_id": 3413280905,
    "id": 2488108140,
    "node_id": "PRRC_kwDOKSXUF86UTYRs",
    "diff_hunk": "@@ -0,0 +1,166 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ContextCompression\n+{\n+    /// <summary>\n+    /// Document summarizer for creating concise summaries of retrieved content.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class DocumentSummarizer<T>\n+    {\n+        private readonly INumericOperations<T> _numOps;\n+        private readonly int _maxSummaryLength;\n+        private readonly string _llmEndpoint;\n+        private readonly string _apiKey;\n+\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"DocumentSummarizer{T}\"/> class.\n+        /// </summary>\n+        /// <param name=\"numericOperations\">The numeric operations for type T.</param>\n+        /// <param name=\"maxSummaryLength\">The maximum length of the summary in characters.</param>\n+        /// <param name=\"llmEndpoint\">The LLM API endpoint.</param>\n+        /// <param name=\"apiKey\">The API key for the LLM service.</param>\n+        public DocumentSummarizer(\n+            INumericOperations<T> numericOperations,\n+            int maxSummaryLength = 500,\n+            string llmEndpoint = \"\",\n+            string apiKey = \"\")\n+        {\n+            _numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\n+            _maxSummaryLength = maxSummaryLength > 0\n+                ? maxSummaryLength\n+                : throw new ArgumentOutOfRangeException(nameof(maxSummaryLength));\n+            _llmEndpoint = llmEndpoint;\n+            _apiKey = apiKey;\n+        }\n+\n+        /// <summary>\n+        /// Summarizes a list of documents.\n+        /// </summary>\n+        /// <param name=\"documents\">The documents to summarize.</param>\n+        /// <returns>A list of summarized documents.</returns>\n+        public List<Document<T>> Summarize(List<Document<T>> documents)\n+        {\n+            if (documents == null) throw new ArgumentNullException(nameof(documents));\n+\n+            var summarized = new List<Document<T>>();\n+\n+            foreach (var doc in documents)\n+            {\n+                var summary = SummarizeText(doc.Content);\n+                var summarizedDoc = new Document<T>\n+                {\n+                    Id = doc.Id,\n+                    Content = summary,\n+                    Metadata = doc.Metadata,\n+                    RelevanceScore = doc.RelevanceScore\n+                };\n+                summarized.Add(summarizedDoc);",
    "path": "src/RetrievalAugmentedGeneration/ContextCompression/DocumentSummarizer.cs",
    "commit_id": "2e85607917a4ef53ec4ed06ecd3d2cbc46bd3626",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Preserve relevance metadata when cloning documents**\n\nIncoming documents can arrive with `HasRelevanceScore = true`, but this clone drops that flag, so downstream steps treat the copied `RelevanceScore` as unset. Please propagate the flag.  \n\n\n```diff\n                 var summarizedDoc = new Document<T>\n                 {\n                     Id = doc.Id,\n                     Content = summary,\n                     Metadata = doc.Metadata,\n-                    RelevanceScore = doc.RelevanceScore\n+                    RelevanceScore = doc.RelevanceScore,\n+                    HasRelevanceScore = doc.HasRelevanceScore\n                 };\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n                var summarizedDoc = new Document<T>\n                {\n                    Id = doc.Id,\n                    Content = summary,\n                    Metadata = doc.Metadata,\n                    RelevanceScore = doc.RelevanceScore,\n                    HasRelevanceScore = doc.HasRelevanceScore\n                };\n                summarized.Add(summarizedDoc);\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/ContextCompression/DocumentSummarizer.cs\naround lines 55 to 62, the cloned Document is copying RelevanceScore but not the\nHasRelevanceScore flag, so downstream logic treats the score as unset; update\nthe clone to also set HasRelevanceScore = doc.HasRelevanceScore (or the\nequivalent flag/property name used on Document<T>) when creating summarizedDoc\nso the relevance metadata is preserved.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-03T23:05:39Z",
    "updated_at": "2025-11-03T23:05:44Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108140",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108140"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108140"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108140/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 55,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 62,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 62,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108152",
    "pull_request_review_id": 3413280905,
    "id": 2488108152,
    "node_id": "PRRC_kwDOKSXUF86UTYR4",
    "diff_hunk": "@@ -0,0 +1,166 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ContextCompression\n+{\n+    /// <summary>\n+    /// Document summarizer for creating concise summaries of retrieved content.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class DocumentSummarizer<T>\n+    {\n+        private readonly INumericOperations<T> _numOps;\n+        private readonly int _maxSummaryLength;\n+        private readonly string _llmEndpoint;\n+        private readonly string _apiKey;\n+\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"DocumentSummarizer{T}\"/> class.\n+        /// </summary>\n+        /// <param name=\"numericOperations\">The numeric operations for type T.</param>\n+        /// <param name=\"maxSummaryLength\">The maximum length of the summary in characters.</param>\n+        /// <param name=\"llmEndpoint\">The LLM API endpoint.</param>\n+        /// <param name=\"apiKey\">The API key for the LLM service.</param>\n+        public DocumentSummarizer(\n+            INumericOperations<T> numericOperations,\n+            int maxSummaryLength = 500,\n+            string llmEndpoint = \"\",\n+            string apiKey = \"\")\n+        {\n+            _numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\n+            _maxSummaryLength = maxSummaryLength > 0\n+                ? maxSummaryLength\n+                : throw new ArgumentOutOfRangeException(nameof(maxSummaryLength));\n+            _llmEndpoint = llmEndpoint;\n+            _apiKey = apiKey;\n+        }\n+\n+        /// <summary>\n+        /// Summarizes a list of documents.\n+        /// </summary>\n+        /// <param name=\"documents\">The documents to summarize.</param>\n+        /// <returns>A list of summarized documents.</returns>\n+        public List<Document<T>> Summarize(List<Document<T>> documents)\n+        {\n+            if (documents == null) throw new ArgumentNullException(nameof(documents));\n+\n+            var summarized = new List<Document<T>>();\n+\n+            foreach (var doc in documents)\n+            {\n+                var summary = SummarizeText(doc.Content);\n+                var summarizedDoc = new Document<T>\n+                {\n+                    Id = doc.Id,\n+                    Content = summary,\n+                    Metadata = doc.Metadata,\n+                    RelevanceScore = doc.RelevanceScore\n+                };\n+                summarized.Add(summarizedDoc);\n+            }\n+\n+            return summarized;\n+        }\n+\n+        /// <summary>\n+        /// Summarizes text to a maximum length.\n+        /// </summary>\n+        /// <param name=\"text\">The text to summarize.</param>\n+        /// <returns>The summarized text.</returns>\n+        public string SummarizeText(string text)\n+        {\n+            if (string.IsNullOrEmpty(text)) return text;\n+\n+            if (text.Length <= _maxSummaryLength)\n+            {\n+                return text;\n+            }\n+\n+            var sentences = SplitIntoSentences(text);\n+            var importantSentences = ExtractImportantSentences(sentences);\n+\n+            var summary = string.Empty;\n+            foreach (var sentence in importantSentences)\n+            {\n+                if (summary.Length + sentence.Length > _maxSummaryLength)\n+                {\n+                    break;\n+                }\n+                summary += sentence + \" \";\n+            }\n+\n+            return summary.Trim();",
    "path": "src/RetrievalAugmentedGeneration/ContextCompression/DocumentSummarizer.cs",
    "commit_id": "c0be21878ba289b4ea925eacff9c9a9164ad807a",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Handle single long sentences without returning an empty summary**\n\nWhen the first (or only) sentence exceeds `_maxSummaryLength`, the loop breaks without appending anything and the method returns `string.Empty`. We should still return a truncated slice of the original text.  \n\n\n```diff\n             foreach (var sentence in importantSentences)\n             {\n                 if (summary.Length + sentence.Length > _maxSummaryLength)\n                 {\n-                    break;\n+                    if (summary.Length == 0)\n+                    {\n+                        return text.Substring(0, Math.Min(text.Length, _maxSummaryLength));\n+                    }\n+                    break;\n                 }\n                 summary += sentence + \" \";\n             }\n \n-            return summary.Trim();\n+            return summary.Trim();\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/ContextCompression/DocumentSummarizer.cs\naround lines 86 to 95, the loop breaks when the first (or only) sentence is\nlonger than _maxSummaryLength causing an empty return; modify the logic so that\nif no sentence has been appended and the current sentence length exceeds\n_maxSummaryLength you append a truncated substring of that sentence up to\n_maxSummaryLength (then break), otherwise keep the existing append-and-break\nbehavior, and finally return the trimmed summary.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-03T23:05:39Z",
    "updated_at": "2025-11-03T23:05:44Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108152",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108152"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108152"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108152/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 86,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 95,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 95,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108158",
    "pull_request_review_id": 3413280905,
    "id": 2488108158,
    "node_id": "PRRC_kwDOKSXUF86UTYR-",
    "diff_hunk": "@@ -0,0 +1,148 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ContextCompression\n+{\n+    /// <summary>\n+    /// LLM-based context compression to reduce token usage while preserving key information.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class LLMContextCompressor<T>\n+    {\n+        private readonly INumericOperations<T> _numOps;\n+        private readonly string _llmEndpoint;\n+        private readonly string _apiKey;\n+        private readonly double _compressionRatio;\n+\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"LLMContextCompressor{T}\"/> class.\n+        /// </summary>\n+        /// <param name=\"numericOperations\">The numeric operations for type T.</param>\n+        /// <param name=\"compressionRatio\">The target compression ratio (0.0 to 1.0).</param>\n+        /// <param name=\"llmEndpoint\">The LLM API endpoint.</param>\n+        /// <param name=\"apiKey\">The API key for the LLM service.</param>\n+        public LLMContextCompressor(\n+            INumericOperations<T> numericOperations,\n+            double compressionRatio = 0.5,\n+            string llmEndpoint = \"\",\n+            string apiKey = \"\")\n+        {\n+            _numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\n+            _compressionRatio = compressionRatio >= 0 && compressionRatio <= 1\n+                ? compressionRatio\n+                : throw new ArgumentOutOfRangeException(nameof(compressionRatio), \"Compression ratio must be between 0 and 1\");\n+            _llmEndpoint = llmEndpoint;\n+            _apiKey = apiKey;\n+        }\n+\n+        /// <summary>\n+        /// Compresses a list of documents while preserving relevance to the query.\n+        /// </summary>\n+        /// <param name=\"query\">The query context.</param>\n+        /// <param name=\"documents\">The documents to compress.</param>\n+        /// <returns>A list of compressed documents.</returns>\n+        public List<Document<T>> Compress(string query, List<Document<T>> documents)\n+        {\n+            if (string.IsNullOrEmpty(query)) throw new ArgumentNullException(nameof(query));\n+            if (documents == null) throw new ArgumentNullException(nameof(documents));\n+\n+            var compressed = new List<Document<T>>();\n+\n+            foreach (var doc in documents)\n+            {\n+                var compressedContent = CompressText(query, doc.Content);\n+                var compressedDoc = new Document<T>\n+                {\n+                    Id = doc.Id,\n+                    Content = compressedContent,\n+                    Metadata = doc.Metadata,\n+                    RelevanceScore = doc.RelevanceScore\n+                };\n+                compressed.Add(compressedDoc);",
    "path": "src/RetrievalAugmentedGeneration/ContextCompression/LLMContextCompressor.cs",
    "commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Keep `HasRelevanceScore` in compressed documents**\n\nJust like the summarizer, we copy the numeric score but lose the boolean flag, which downstream logic relies on. Please preserve it.  \n\n\n```diff\n                 var compressedDoc = new Document<T>\n                 {\n                     Id = doc.Id,\n                     Content = compressedContent,\n                     Metadata = doc.Metadata,\n-                    RelevanceScore = doc.RelevanceScore\n+                    RelevanceScore = doc.RelevanceScore,\n+                    HasRelevanceScore = doc.HasRelevanceScore\n                 };\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n                var compressedDoc = new Document<T>\n                {\n                    Id = doc.Id,\n                    Content = compressedContent,\n                    Metadata = doc.Metadata,\n                    RelevanceScore = doc.RelevanceScore,\n                    HasRelevanceScore = doc.HasRelevanceScore\n                };\n                compressed.Add(compressedDoc);\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/ContextCompression/LLMContextCompressor.cs\naround lines 58 to 64, the compressed document initializer copies RelevanceScore\nbut omits the HasRelevanceScore boolean flag that downstream logic depends on;\nupdate the compressedDoc creation to set HasRelevanceScore =\ndoc.HasRelevanceScore (and ensure the compressed document type exposes that\nproperty) so the boolean is preserved when adding to the compressed list.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits ec6a7c9 to 77a1bbb",
    "created_at": "2025-11-03T23:05:39Z",
    "updated_at": "2025-11-04T00:06:44Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108158",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108158"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108158"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108158/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 51,
    "original_start_line": 58,
    "start_side": "RIGHT",
    "line": 56,
    "original_line": 64,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 64,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108165",
    "pull_request_review_id": 3413280905,
    "id": 2488108165,
    "node_id": "PRRC_kwDOKSXUF86UTYSF",
    "diff_hunk": "@@ -0,0 +1,73 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\n+using System;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels\n+{\n+    /// <summary>\n+    /// HuggingFace-based embedding model for generating embeddings.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class HuggingFaceEmbeddingModel<T> : EmbeddingModelBase<T>\n+    {\n+        private readonly string _modelName;\n+        private readonly string _apiKey;\n+        private readonly int _dimension;\n+        private readonly int _maxTokens;\n+\n+        public override int EmbeddingDimension => _dimension;\n+        public override int MaxTokens => _maxTokens;\n+\n+        public HuggingFaceEmbeddingModel(string modelName, string apiKey = \"\", int dimension = 768, int maxTokens = 512)\n+        {\n+            if (string.IsNullOrWhiteSpace(modelName))\n+                throw new ArgumentException(\"Model name cannot be empty\", nameof(modelName));\n+            if (dimension <= 0)\n+                throw new ArgumentException(\"Dimension must be positive\", nameof(dimension));\n+            if (maxTokens <= 0)\n+                throw new ArgumentException(\"Max tokens must be positive\", nameof(maxTokens));\n+\n+            _modelName = modelName;\n+            _apiKey = apiKey ?? string.Empty;\n+            _dimension = dimension;\n+            _maxTokens = maxTokens;\n+        }\n+\n+        protected override Vector<T> EmbedCore(string text)\n+        {\n+            var values = new T[_dimension];\n+            var hash = text.GetHashCode();\n+            \n+            for (int i = 0; i < _dimension; i++)\n+            {\n+                var val = NumOps.FromDouble(Math.Cos(hash * (i + 1) * 0.002));\n+                values[i] = val;\n+            }\n+\n+            return NormalizeVector(new Vector<T>(values));\n+        }",
    "path": "src/RetrievalAugmentedGeneration/Embeddings/HuggingFaceEmbeddingModel.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Replace `GetHashCode` with a deterministic hash**\n\n`string.GetHashCode()` is salted per process in modern .NET, so the same text produces different hashes each time the app restarts. That makes every embedding drift between runs, corrupting persisted vectors and breaking any document/query comparison after a restart. Please switch to a stable hash (e.g., SHA256 of the UTF-8 bytes) and derive the synthetic embedding from those bytes instead.\n\n```diff\n-            var values = new T[_dimension];\n-            var hash = text.GetHashCode();\n-            \n-            for (int i = 0; i < _dimension; i++)\n-            {\n-                var val = NumOps.FromDouble(Math.Cos(hash * (i + 1) * 0.002));\n-                values[i] = val;\n-            }\n+            using var sha256 = System.Security.Cryptography.SHA256.Create();\n+            var hashBytes = sha256.ComputeHash(System.Text.Encoding.UTF8.GetBytes(text));\n+            var values = new T[_dimension];\n+\n+            for (int i = 0; i < _dimension; i++)\n+            {\n+                var byteValue = hashBytes[i % hashBytes.Length] / 255.0;\n+                values[i] = NumOps.FromDouble(byteValue * 2 - 1);\n+            }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        protected override Vector<T> EmbedCore(string text)\n        {\n            using var sha256 = System.Security.Cryptography.SHA256.Create();\n            var hashBytes = sha256.ComputeHash(System.Text.Encoding.UTF8.GetBytes(text));\n            var values = new T[_dimension];\n\n            for (int i = 0; i < _dimension; i++)\n            {\n                var byteValue = hashBytes[i % hashBytes.Length] / 255.0;\n                values[i] = NumOps.FromDouble(byteValue * 2 - 1);\n            }\n\n            return NormalizeVector(new Vector<T>(values));\n        }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/EmbeddingModels/HuggingFaceEmbeddingModel.cs\naround lines 37 to 49, the code currently uses string.GetHashCode() which is\nnon-deterministic across .NET process restarts; replace it by computing a\ndeterministic SHA256 hash of the input text's UTF-8 bytes and derive the\nembedding values from that hash instead. Specifically, compute SHA256(bytes),\nthen expand those bytes deterministically to produce _dimension numeric seeds\n(e.g., by reading successive 8-byte chunks, wrapping as needed, or using a\nKDF/PRF with the hash) and convert each chunk into a double/float input for the\nexisting NumOps.FromDouble(Math.Cos(...)) expression (or directly map to a\nstable float between -1 and 1), then fill values[], normalize and return the\nvector; ensure no use of GetHashCode and that UTF-8 encoding and SHA256 produce\nconsistent results across runs.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-03T23:05:39Z",
    "updated_at": "2025-11-03T23:05:44Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108165",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108165"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108165"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108165/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 37,
    "original_start_line": 37,
    "start_side": "RIGHT",
    "line": 49,
    "original_line": 49,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 49,
    "position": 49,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108175",
    "pull_request_review_id": 3413280905,
    "id": 2488108175,
    "node_id": "PRRC_kwDOKSXUF86UTYSP",
    "diff_hunk": "@@ -0,0 +1,71 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\n+using System;",
    "path": "src/RetrievalAugmentedGeneration/Embeddings/LocalTransformerEmbedding.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Use a stable hash for deterministic embeddings.**\n\n`string.GetHashCode()` is randomized per-process on modern .NET, so the same text produces different embeddings between runsΓÇöbreaking persistence, retrieval, and cache hits. Switch to a cryptographic hash so embeddings stay consistent.\n\n\n```diff\n-using System;\n+using System;\n+using System.Security.Cryptography;\n+using System.Text;\n@@\n-        protected override Vector<T> EmbedCore(string text)\n-        {\n-            var values = new T[_dimension];\n-            var hash = text.GetHashCode();\n-            \n-            for (int i = 0; i < _dimension; i++)\n-            {\n-                var val = NumOps.FromDouble(Math.Sin(hash * (i + 1) * 0.003));\n-                values[i] = val;\n-            }\n-\n-            return NormalizeVector(new Vector<T>(values));\n-        }\n+        protected override Vector<T> EmbedCore(string text)\n+        {\n+            var values = new T[_dimension];\n+            using var sha256 = SHA256.Create();\n+            var hashBytes = sha256.ComputeHash(Encoding.UTF8.GetBytes(text));\n+\n+            for (int i = 0; i < _dimension; i++)\n+            {\n+                var normalized = ((hashBytes[i % hashBytes.Length] / 255.0) * 2.0) - 1.0;\n+                values[i] = NumOps.FromDouble(normalized);\n+            }\n+\n+            return NormalizeVector(new Vector<T>(values));\n+        }\n```\n\n\nAlso applies to: 35-47\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-03T23:05:40Z",
    "updated_at": "2025-11-03T23:05:44Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108175",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108175"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108175"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108175/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 1,
    "original_start_line": 1,
    "start_side": "RIGHT",
    "line": 4,
    "original_line": 4,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 4,
    "position": 4,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108180",
    "pull_request_review_id": 3413280905,
    "id": 2488108180,
    "node_id": "PRRC_kwDOKSXUF86UTYSU",
    "diff_hunk": "@@ -0,0 +1,71 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\n+using System;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels\n+{\n+    /// <summary>\n+    /// ONNX-based sentence transformer for generating embeddings.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class ONNXSentenceTransformer<T> : EmbeddingModelBase<T>\n+    {\n+        private readonly string _modelPath;\n+        private readonly int _dimension;\n+        private readonly int _maxTokens;\n+\n+        public override int EmbeddingDimension => _dimension;\n+        public override int MaxTokens => _maxTokens;\n+\n+        public ONNXSentenceTransformer(string modelPath, int dimension = 384, int maxTokens = 512)\n+        {\n+            if (string.IsNullOrWhiteSpace(modelPath))\n+                throw new ArgumentException(\"Model path cannot be empty\", nameof(modelPath));\n+            if (dimension <= 0)\n+                throw new ArgumentException(\"Dimension must be positive\", nameof(dimension));\n+            if (maxTokens <= 0)\n+                throw new ArgumentException(\"Max tokens must be positive\", nameof(maxTokens));\n+\n+            _modelPath = modelPath;\n+            _dimension = dimension;\n+            _maxTokens = maxTokens;\n+        }\n+\n+        protected override Vector<T> EmbedCore(string text)\n+        {\n+            var values = new T[_dimension];\n+            var hash = text.GetHashCode();\n+            \n+            for (int i = 0; i < _dimension; i++)\n+            {\n+                var val = NumOps.FromDouble(Math.Cos(hash * (i + 1) * 0.002));\n+                values[i] = val;\n+            }",
    "path": "src/RetrievalAugmentedGeneration/Embeddings/ONNXSentenceTransformer.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Fix nondeterministic embeddings**\n\n`string.GetHashCode` is salted per process on modern .NET, so the same text produces different vectors after every restart. That corrupts stored embeddings and breaks similarity search consistency. Please swap to a deterministic digest (e.g., SHA256 over the UTFΓÇæ8 bytes) before projecting into the vector space.\n\n```diff\n using AiDotNet.Helpers;\n using AiDotNet.LinearAlgebra;\n using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\n using System;\n+using System.Security.Cryptography;\n+using System.Text;\n \n ...\n-            var hash = text.GetHashCode();\n-            \n-            for (int i = 0; i < _dimension; i++)\n-            {\n-                var val = NumOps.FromDouble(Math.Cos(hash * (i + 1) * 0.002));\n-                values[i] = val;\n-            }\n+            var hashBytes = SHA256.HashData(Encoding.UTF8.GetBytes(text));\n+\n+            for (int i = 0; i < _dimension; i++)\n+            {\n+                var byteValue = hashBytes[i % hashBytes.Length] / 255.0;\n+                values[i] = NumOps.FromDouble((byteValue * 2) - 1);\n+            }\n```\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits ec6a7c9 to 77a1bbb",
    "created_at": "2025-11-03T23:05:40Z",
    "updated_at": "2025-11-04T00:06:54Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108180",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108180"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108180"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108180/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 1,
    "original_start_line": 1,
    "start_side": "RIGHT",
    "line": 63,
    "original_line": 44,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 44,
    "position": 63,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108189",
    "pull_request_review_id": 3413280905,
    "id": 2488108189,
    "node_id": "PRRC_kwDOKSXUF86UTYSd",
    "diff_hunk": "@@ -0,0 +1,75 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\n+using System;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels\n+{\n+    /// <summary>\n+    /// OpenAI embedding model for generating embeddings via OpenAI API.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class OpenAIEmbeddingModel<T> : EmbeddingModelBase<T>\n+    {\n+        private readonly string _apiKey;\n+        private readonly string _modelName;\n+        private readonly int _dimension;\n+        private readonly int _maxTokens;\n+\n+        public override int EmbeddingDimension => _dimension;\n+        public override int MaxTokens => _maxTokens;\n+\n+        public OpenAIEmbeddingModel(string apiKey, string modelName = \"text-embedding-ada-002\", int dimension = 1536, int maxTokens = 8191)\n+        {\n+            if (string.IsNullOrWhiteSpace(apiKey))\n+                throw new ArgumentException(\"API key cannot be empty\", nameof(apiKey));\n+            if (string.IsNullOrWhiteSpace(modelName))\n+                throw new ArgumentException(\"Model name cannot be empty\", nameof(modelName));\n+            if (dimension <= 0)\n+                throw new ArgumentException(\"Dimension must be positive\", nameof(dimension));\n+            if (maxTokens <= 0)\n+                throw new ArgumentException(\"Max tokens must be positive\", nameof(maxTokens));\n+\n+            _apiKey = apiKey;\n+            _modelName = modelName;\n+            _dimension = dimension;\n+            _maxTokens = maxTokens;\n+        }\n+\n+        protected override Vector<T> EmbedCore(string text)\n+        {\n+            var values = new T[_dimension];\n+            var hash = text.GetHashCode();\n+            \n+            for (int i = 0; i < _dimension; i++)\n+            {\n+                var val = NumOps.FromDouble(Math.Cos(hash * (i + 1) * 0.001));\n+                values[i] = val;\n+            }",
    "path": "src/RetrievalAugmentedGeneration/EmbeddingModels/OpenAIEmbeddingModel.cs",
    "commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**GetHashCode-based embeddings are unstable across runs.**\n\n`string.GetHashCode()` is randomized per process on modern .NET, so embeddings produced during indexing will differ from those generated later during querying, breaking retrieval reproducibility. Replace this placeholder with a stable implementation (e.g., call the OpenAI embeddings API or derive a deterministic vector via a cryptographic hash fed into `NumOps`) before shipping.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/EmbeddingModels/OpenAIEmbeddingModel.cs\naround lines 42 to 48, the code uses string.GetHashCode() which is\nprocess-randomized and produces unstable embeddings; replace this placeholder\nwith a deterministic implementation: either call the OpenAI embeddings API and\nreturn its vector, or compute a cryptographic hash (e.g., SHA256) of the input\ntext, deterministically derive numeric values from the hash bytes\n(slice/expand/interpret as little-endian integers or use HMAC-DRBG), map those\nnumbers into the required dimension using NumOps.FromDouble, and optionally\nnormalize the vector; ensure the implementation is stable across runs and\nhandles errors/async calls appropriately.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits ec6a7c9 to 77a1bbb",
    "created_at": "2025-11-03T23:05:40Z",
    "updated_at": "2025-11-04T00:06:58Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108189",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108189"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108189"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108189/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 42,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 48,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 48,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108194",
    "pull_request_review_id": 3413280905,
    "id": 2488108194,
    "node_id": "PRRC_kwDOKSXUF86UTYSi",
    "diff_hunk": "@@ -128,15 +132,15 @@ public GroundedAnswer GenerateGrounded(string query, IEnumerable<Document> conte\n \n         // Calculate confidence based on retrieval scores (normalized to [0,1])\n         var avgScore = contextList\n-            .Where(d => d.RelevanceScore.HasValue)\n-            .Select(d => d.RelevanceScore!.Value)\n+            .Where(d => d.HasRelevanceScore)\n+            .Select(d => Convert.ToDouble(d.RelevanceScore))\n             .DefaultIfEmpty(0.5)",
    "path": "src/RetrievalAugmentedGeneration/Generators/StubGenerator.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Use the numeric operations helper instead of `Convert.ToDouble`.**\n\n`Convert.ToDouble(d.RelevanceScore)` will throw for any numeric type that doesnΓÇÖt implement `IConvertible` (e.g., the `Complex<T>` support you just enabled via `MathHelper.GetNumericOperations<T>()`). The stub generator therefore crashes as soon as the pipeline runs with one of those valid `T`s. Please route the conversion through the `INumericOperations<T>` abstraction you already cached.\n\n\n\n```diff\n-            .Where(d => d.HasRelevanceScore)\n-            .Select(d => Convert.ToDouble(d.RelevanceScore))\n+            .Where(d => d.HasRelevanceScore)\n+            .Select(d => NumOps.ToDouble(d.RelevanceScore))\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n            .Where(d => d.HasRelevanceScore)\n            .Select(d => NumOps.ToDouble(d.RelevanceScore))\n            .DefaultIfEmpty(0.5)\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-03T23:05:40Z",
    "updated_at": "2025-11-03T23:05:44Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108194",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108194"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108194"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108194/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 135,
    "original_start_line": 135,
    "start_side": "RIGHT",
    "line": 137,
    "original_line": 137,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 59,
    "position": 61,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108202",
    "pull_request_review_id": 3413280905,
    "id": 2488108202,
    "node_id": "PRRC_kwDOKSXUF86UTYSq",
    "diff_hunk": "@@ -0,0 +1,220 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Rerankers;\n+\n+/// <summary>\n+/// Reranks documents to maximize diversity while maintaining relevance.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for scoring.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// This reranker addresses the problem of redundant results by explicitly promoting diversity.\n+/// It uses a greedy algorithm to select documents that are both relevant to the query and\n+/// dissimilar from already-selected documents. This is similar to Maximal Marginal Relevance (MMR)\n+/// but uses a simpler diversity metric based on text overlap.\n+/// </para>\n+/// <para><b>For Beginners:</b> This prevents showing the same information multiple times.\n+/// \n+/// The Problem:\n+/// Imagine searching for \"Python programming\" and getting 10 results:\n+/// - Result 1: \"Python is a programming language...\"\n+/// - Result 2: \"Python is a programming language used for...\"\n+/// - Result 3: \"Python programming language allows...\"\n+/// - Result 4-10: More variations of the same thing\n+/// \n+/// That's redundant! You want variety:\n+/// - Result 1: Python basics\n+/// - Result 2: Python web development\n+/// - Result 3: Python data science\n+/// - Result 4: Python machine learning\n+/// - Result 5: Python performance tips\n+/// \n+/// How it works:\n+/// 1. Pick the most relevant document first\n+/// 2. For remaining docs, balance two factors:\n+///    a) Relevance to the query (should be useful)\n+///    b) Difference from already-picked docs (should be unique)\n+/// 3. Keep picking until you have enough results\n+/// \n+/// Diversity calculation:\n+/// - Compares text overlap (how many words are shared)\n+/// - Higher overlap = less diverse = lower score\n+/// - Lower overlap = more diverse = higher score\n+/// \n+/// Lambda parameter (0 to 1):\n+/// - lambda=1.0: Only care about relevance (might get duplicates)\n+/// - lambda=0.0: Only care about diversity (might get irrelevant docs)\n+/// - lambda=0.5: Balance both (recommended default)\n+/// \n+/// Real example with lambda=0.5:\n+/// Query: \"climate change effects\"\n+/// \n+/// Step 1: Pick most relevant ΓåÆ \"Climate change causes rising temperatures\" (relevance: 0.9)\n+/// Step 2: Next candidates:\n+///   - \"Climate change leads to warmer weather\" (relevance: 0.85, similarity to picked: 0.7)\n+///     ΓåÆ Score: 0.5 * 0.85 - 0.5 * 0.7 = 0.075\n+///   - \"Ocean acidification from CO2\" (relevance: 0.7, similarity: 0.2)\n+///     ΓåÆ Score: 0.5 * 0.7 - 0.5 * 0.2 = 0.25 Γ£ô Pick this!\n+/// \n+/// Result: You get coverage of temperature AND ocean effects, not just temperature twice!\n+/// \n+/// When to use this:\n+/// - Search results where redundancy is common\n+/// - Document recommendation systems\n+/// - Exploratory searches where breadth matters\n+/// - After initial retrieval that returns many similar docs\n+/// </para>\n+/// </remarks>\n+public class DiversityReranker<T> : RerankerBase<T>\n+{\n+    private readonly T _lambda;\n+\n+    /// <summary>\n+    /// Gets a value indicating whether this reranker modifies relevance scores.\n+    /// </summary>\n+    public override bool ModifiesScores => true;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the DiversityReranker class.\n+    /// </summary>\n+    /// <param name=\"numOps\">Numeric operations for type T.</param>\n+    /// <param name=\"lambda\">Trade-off parameter between relevance and diversity (0 to 1). Default: 0.5.\n+    /// Higher values favor relevance, lower values favor diversity.</param>\n+    public DiversityReranker(INumericOperations<T> numOps, T? lambda = default) : base()\n+    {\n+        var lambdaValue = lambda ?? NumOps.FromDouble(0.5);\n+        \n+        // Validate lambda is in [0, 1]\n+        if (NumOps.LessThan(lambdaValue, NumOps.Zero) || NumOps.GreaterThan(lambdaValue, NumOps.One))\n+        {\n+            throw new ArgumentException(\"Lambda must be between 0 and 1.\", nameof(lambda));\n+        }\n+\n+        _lambda = lambdaValue;\n+    }\n+\n+    /// <summary>\n+    /// Core reranking logic that maximizes diversity while maintaining relevance.\n+    /// </summary>\n+    /// <param name=\"query\">The validated query text.</param>\n+    /// <param name=\"documents\">The validated list of documents to rerank.</param>\n+    /// <returns>Documents reranked to balance relevance and diversity.</returns>\n+    protected override IEnumerable<Document<T>> RerankCore(string query, IList<Document<T>> documents)\n+    {\n+        var docList = documents.ToList();\n+        if (docList.Count <= 1)\n+        {\n+            return docList;\n+        }\n+\n+        var rerankedDocs = new List<Document<T>>();\n+        var remainingDocs = new List<Document<T>>(docList);\n+\n+        // Start with the most relevant document\n+        var firstDoc = remainingDocs.OrderByDescending(d => d.RelevanceScore, \n+            Comparer<T>.Create((a, b) => {\n+                if (NumOps.GreaterThan(a, b)) return 1;\n+                if (NumOps.LessThan(a, b)) return -1;\n+                return 0;\n+            })).First();\n+        rerankedDocs.Add(firstDoc);\n+        remainingDocs.Remove(firstDoc);\n+\n+        // Greedily select documents that maximize: lambda * relevance - (1 - lambda) * max_similarity\n+        while (remainingDocs.Count > 0)\n+        {\n+            Document<T>? bestDoc = null;\n+            var bestScore = NumOps.FromDouble(double.MinValue);\n+\n+            foreach (var doc in remainingDocs)\n+            {\n+                // Calculate maximum similarity to already-selected documents\n+                var maxSimilarity = NumOps.Zero;\n+                foreach (var selectedDoc in rerankedDocs)\n+                {\n+                    var similarity = CalculateTextSimilarity(doc.Content, selectedDoc.Content);\n+                    if (NumOps.GreaterThan(similarity, maxSimilarity))\n+                    {\n+                        maxSimilarity = similarity;\n+                    }\n+                }\n+\n+                // Calculate diversity-aware score: lambda * relevance - (1 - lambda) * max_similarity\n+                var relevancePart = NumOps.Multiply(_lambda, doc.RelevanceScore);\n+                var oneMinusLambda = NumOps.Subtract(NumOps.One, _lambda);\n+                var diversityPenalty = NumOps.Multiply(oneMinusLambda, maxSimilarity);\n+                var score = NumOps.Subtract(relevancePart, diversityPenalty);\n+\n+                if (bestDoc == null || NumOps.GreaterThan(score, bestScore))\n+                {\n+                    bestDoc = doc;\n+                    bestScore = score;\n+                }\n+            }\n+",
    "path": "src/RetrievalAugmentedGeneration/Rerankers/DiversityReranker.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Fix `bestScore` initialization overflow**\n\n`NumOps.FromDouble(double.MinValue)` will overflow for decimal and integral `T` (e.g., `Convert.ToInt32(double.MinValue)`), crashing the reranker before it scores anything. Initialize with `NumOps.Zero` and use a guard instead of converting an out-of-range sentinel.\n\n```diff\n-            Document<T>? bestDoc = null;\n-            var bestScore = NumOps.FromDouble(double.MinValue);\n+            Document<T>? bestDoc = null;\n+            var bestScore = NumOps.Zero;\n+            var hasBestScore = false;\n@@\n-                if (bestDoc == null || NumOps.GreaterThan(score, bestScore))\n+                if (!hasBestScore || NumOps.GreaterThan(score, bestScore))\n                 {\n                     bestDoc = doc;\n                     bestScore = score;\n+                    hasBestScore = true;\n                 }\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Rerankers/DiversityReranker.cs around lines\n126-156, replace the unsafe sentinel initialization of bestScore\n(NumOps.FromDouble(double.MinValue)) with NumOps.Zero and rely on the existing\nbestDoc == null guard when selecting the first document; remove any conversion\nfrom double.MinValue so no out-of-range conversions occur, and keep the\nselection condition as \"if (bestDoc == null || NumOps.GreaterThan(score,\nbestScore))\" so the first scored doc sets bestScore safely.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits ec6a7c9 to 77a1bbb",
    "created_at": "2025-11-03T23:05:40Z",
    "updated_at": "2025-11-04T00:07:04Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108202",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108202"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108202"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108202/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 125,
    "original_start_line": 126,
    "start_side": "RIGHT",
    "line": 155,
    "original_line": 156,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 156,
    "position": 155,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108207",
    "pull_request_review_id": 3413280905,
    "id": 2488108207,
    "node_id": "PRRC_kwDOKSXUF86UTYSv",
    "diff_hunk": "@@ -0,0 +1,230 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Rerankers;\n+\n+/// <summary>\n+/// Implements Maximal Marginal Relevance (MMR) reranking to balance relevance and diversity.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// MMR reranking ensures that retrieved documents are not only relevant to the query but also\n+/// diverse from each other. This prevents redundancy where all top results say essentially the\n+/// same thing, providing users with a broader range of information. MMR is particularly valuable\n+/// for exploratory search, news aggregation, and research applications.\n+/// </para>\n+/// <para><b>For Beginners:</b> MMR prevents search results from being too similar to each other.\n+/// \n+/// The problem MMR solves:\n+/// Imagine searching for \"climate change\" and getting:\n+/// 1. \"Climate change threatens polar bears\"\n+/// 2. \"Polar bears endangered by climate change\"\n+/// 3. \"Climate change impact on polar ice affecting bears\"\n+/// 4. \"Global warming threatens polar bear habitats\"\n+/// 5. \"Arctic ice melting endangers polar bears\"\n+/// \n+/// All relevant, but they're all saying the same thing! You're getting one narrow aspect\n+/// repeated 5 times instead of a diverse view of climate change.\n+/// \n+/// What MMR does instead:\n+/// 1. \"Climate change threatens polar bears\" (relevant: Γ£ô, diverse: Γ£ô first result)\n+/// 2. \"Rising sea levels threaten coastal cities\" (relevant: Γ£ô, different topic: Γ£ô)\n+/// 3. \"Carbon emissions reach record highs\" (relevant: Γ£ô, different aspect: Γ£ô)\n+/// 4. \"Renewable energy adoption accelerates globally\" (relevant: Γ£ô, solutions angle: Γ£ô)\n+/// 5. \"Climate refugees increase in developing nations\" (relevant: Γ£ô, human impact: Γ£ô)\n+/// \n+/// Now you get a comprehensive view with diverse perspectives!\n+/// \n+/// How MMR works:\n+/// 1. Pick the most relevant document ΓåÆ Add to results\n+/// 2. For next pick, consider:\n+///    - Relevance to query (you want relevant docs)\n+///    - Dissimilarity to already-picked docs (you want diversity)\n+/// 3. Balance these two goals with a lambda parameter\n+/// 4. Repeat until you have K documents\n+/// \n+/// The lambda parameter (╬╗):\n+/// - ╬╗ = 1.0: Only care about relevance (normal ranking, no diversity)\n+/// - ╬╗ = 0.0: Only care about diversity (might get irrelevant but diverse docs)\n+/// - ╬╗ = 0.7: Balanced (70% relevance, 30% diversity) ΓåÉ Good default\n+/// \n+/// When to use MMR:\n+/// - Research/exploratory queries: Users want comprehensive coverage\n+/// - News aggregation: Don't show 10 articles about the same event\n+/// - Product search: Show variety, not just variations of one product\n+/// - Question answering: Provide multiple perspectives\n+/// \n+/// When NOT to use MMR:\n+/// - User wants very specific info: \"iPhone 15 Pro Max price\" (diversity not helpful)\n+/// - Transactional queries: \"buy Nike Air Max\" (user knows what they want)\n+/// - Fact lookups: \"Paris population\" (one correct answer)\n+/// </para>\n+/// </remarks>\n+public class MaximalMarginalRelevanceReranker<T> : RerankerBase<T>\n+{\n+    private readonly double _lambda;\n+    private readonly Func<Document<T>, Vector<T>> _getEmbedding;\n+\n+    /// <summary>\n+    /// Gets a value indicating whether this reranker modifies relevance scores.\n+    /// </summary>\n+    public override bool ModifiesScores => true;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the MaximalMarginalRelevanceReranker class.\n+    /// </summary>\n+    /// <param name=\"getEmbedding\">Function to get document embeddings for similarity calculation.</param>\n+    /// <param name=\"lambda\">Balance between relevance and diversity (0-1, default: 0.7). Higher values prioritize relevance.</param>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> Configuration explained.\n+    /// \n+    /// **getEmbedding Function:**\n+    /// This function should return the vector embedding of a document. If you don't have\n+    /// embeddings, you can't use MMR (it needs to measure similarity between documents).\n+    /// \n+    /// Example:\n+    /// ```csharp\n+    /// Func&lt;Document&lt;double&gt;, Vector&lt;double&gt;&gt; getEmb = (doc) => {\n+    ///     // Return cached embedding if available\n+    ///     if (doc.Metadata.ContainsKey(\"embedding\"))\n+    ///         return (Vector&lt;double&gt;)doc.Metadata[\"embedding\"];\n+    ///     \n+    ///     // Otherwise compute it\n+    ///     return embeddingModel.Encode(doc.Content);\n+    /// };\n+    /// ```\n+    /// \n+    /// **Lambda Parameter Guidelines:**\n+    /// - 1.0 = Pure relevance (same as no reranking)\n+    /// - 0.9 = Slight diversity boost\n+    /// - 0.7 = Balanced (recommended default)\n+    /// - 0.5 = Equal weight to relevance and diversity\n+    /// - 0.3 = Heavy diversity (may sacrifice relevance)\n+    /// - 0.0 = Pure diversity (probably too extreme)\n+    /// \n+    /// Start with 0.7 and adjust based on user feedback!\n+    /// </para>\n+    /// </remarks>\n+    public MaximalMarginalRelevanceReranker(Func<Document<T>, Vector<T>> getEmbedding, double lambda = 0.7)\n+    {\n+        if (getEmbedding == null)\n+            throw new ArgumentNullException(nameof(getEmbedding));\n+        \n+        if (lambda < 0 || lambda > 1)\n+            throw new ArgumentException(\"Lambda must be between 0 and 1\", nameof(lambda));\n+\n+        _getEmbedding = getEmbedding;\n+        _lambda = lambda;\n+    }\n+\n+    /// <summary>\n+    /// Reranks documents using Maximal Marginal Relevance.\n+    /// </summary>\n+    protected override IEnumerable<Document<T>> RerankCore(string query, IList<Document<T>> documents)\n+    {\n+        var docList = documents.ToList();\n+        \n+        if (docList.Count == 0)\n+            return Enumerable.Empty<Document<T>>();\n+\n+        if (docList.Count == 1)\n+            return docList;\n+\n+        // Get embeddings for all documents\n+        var embeddings = docList.Select(doc => _getEmbedding(doc)).ToList();\n+        \n+        // Track which documents have been selected\n+        var selected = new List<int>();\n+        var unselected = Enumerable.Range(0, docList.Count).ToList();\n+        \n+        // Select first document (most relevant)\n+        var firstIdx = unselected\n+            .OrderByDescending(i => docList[i].HasRelevanceScore ? Convert.ToDouble(docList[i].RelevanceScore) : 0.0)\n+            .First();\n+        \n+        selected.Add(firstIdx);\n+        unselected.Remove(firstIdx);\n+        \n+        // Iteratively select remaining documents using MMR\n+        while (unselected.Count > 0)\n+        {\n+            var bestIdx = -1;\n+            var bestScore = NumOps.FromDouble(double.NegativeInfinity);\n+            \n+            foreach (var i in unselected)\n+            {\n+                var doc = docList[i];\n+                \n+                // Relevance component (original score)\n+                var relevance = doc.HasRelevanceScore ? doc.RelevanceScore : NumOps.Zero;\n+                \n+                // Diversity component (max similarity to selected docs)\n+                var maxSimilarity = NumOps.Zero;\n+                foreach (var j in selected)\n+                {\n+                    var similarity = NumOps.FromDouble(CalculateCosineSimilarity(embeddings[i], embeddings[j]));\n+                    if (NumOps.GreaterThan(similarity, maxSimilarity))\n+                        maxSimilarity = similarity;\n+                }\n+                \n+                // MMR score = ╬╗ * relevance - (1 - ╬╗) * maxSimilarity\n+                var lambdaT = NumOps.FromDouble(_lambda);\n+                var oneMinusLambda = NumOps.FromDouble(1.0 - _lambda);\n+                \n+                var mmrScore = NumOps.Subtract(\n+                    NumOps.Multiply(lambdaT, relevance),\n+                    NumOps.Multiply(oneMinusLambda, maxSimilarity)\n+                );\n+                \n+                if (NumOps.GreaterThan(mmrScore, bestScore))\n+                {\n+                    bestScore = mmrScore;\n+                    bestIdx = i;\n+                }",
    "path": "src/RetrievalAugmentedGeneration/Rerankers/MaximalMarginalRelevanceReranker.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Avoid converting negative infinity for generic numeric types.**\n\nLine 156 calls `NumOps.FromDouble(double.NegativeInfinity)`. For many supported numeric types (e.g., `decimal`, `int`), that conversion throws an `OverflowException`, breaking MMR reranking the moment those generics are used. Initialize the sentinel without relying on infinities by tracking whether a candidate has been seen instead.  \n\n```diff\n-            var bestIdx = -1;\n-            var bestScore = NumOps.FromDouble(double.NegativeInfinity);\n+            var bestIdx = -1;\n+            var bestScore = default(T);\n+            var hasCandidate = false;\n@@\n-                if (NumOps.GreaterThan(mmrScore, bestScore))\n+                if (!hasCandidate || NumOps.GreaterThan(mmrScore, bestScore))\n                 {\n                     bestScore = mmrScore;\n                     bestIdx = i;\n+                    hasCandidate = true;\n                 }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n            var bestIdx = -1;\n            var bestScore = default(T);\n            var hasCandidate = false;\n            \n            foreach (var i in unselected)\n            {\n                var doc = docList[i];\n                \n                // Relevance component (original score)\n                var relevance = doc.HasRelevanceScore ? doc.RelevanceScore : NumOps.Zero;\n                \n                // Diversity component (max similarity to selected docs)\n                var maxSimilarity = NumOps.Zero;\n                foreach (var j in selected)\n                {\n                    var similarity = NumOps.FromDouble(CalculateCosineSimilarity(embeddings[i], embeddings[j]));\n                    if (NumOps.GreaterThan(similarity, maxSimilarity))\n                        maxSimilarity = similarity;\n                }\n                \n                // MMR score = ╬╗ * relevance - (1 - ╬╗) * maxSimilarity\n                var lambdaT = NumOps.FromDouble(_lambda);\n                var oneMinusLambda = NumOps.FromDouble(1.0 - _lambda);\n                \n                var mmrScore = NumOps.Subtract(\n                    NumOps.Multiply(lambdaT, relevance),\n                    NumOps.Multiply(oneMinusLambda, maxSimilarity)\n                );\n                \n                if (!hasCandidate || NumOps.GreaterThan(mmrScore, bestScore))\n                {\n                    bestScore = mmrScore;\n                    bestIdx = i;\n                    hasCandidate = true;\n                }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn\nsrc/RetrievalAugmentedGeneration/Rerankers/MaximalMarginalRelevanceReranker.cs\naround lines 154-186, avoid calling NumOps.FromDouble(double.NegativeInfinity)\nwhich can throw for some numeric T; instead remove the NegativeInfinity sentinel\nand track whether a best candidate has been seen (e.g., add a bool bestFound =\nfalse), then on the first candidate set bestScore = mmrScore and bestIdx = i and\nset bestFound = true; subsequently compare mmrScore to bestScore as before.\nEnsure no conversion from double.NegativeInfinity is used anywhere in this\nblock.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits ec6a7c9 to 77a1bbb",
    "created_at": "2025-11-03T23:05:40Z",
    "updated_at": "2025-11-04T00:07:03Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108207",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108207"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108207"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108207/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 154,
    "original_start_line": 154,
    "start_side": "RIGHT",
    "line": 186,
    "original_line": 186,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 186,
    "position": 186,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108213",
    "pull_request_review_id": 3413280905,
    "id": 2488108213,
    "node_id": "PRRC_kwDOKSXUF86UTYS1",
    "diff_hunk": "@@ -142,31 +150,41 @@ protected virtual void ValidateTopK(int topK)\n     /// - Makes scores consistent with other retrievers/rerankers\n     /// </para>\n     /// </remarks>\n-    protected IList<Document> NormalizeScores(IList<Document> documents)\n+    protected IList<Document<T>> NormalizeScores(IList<Document<T>> documents)\n     {\n-        var docsWithScores = documents.Where(d => d.RelevanceScore.HasValue).ToList();\n+        var docsWithScores = documents.Where(d => d.HasRelevanceScore).ToList();\n         if (docsWithScores.Count == 0)\n             return documents;\n \n-        var scores = docsWithScores.Select(d => d.RelevanceScore!.Value).ToList();\n+        var scores = docsWithScores.Select(d => d.RelevanceScore).ToList();\n+        if (scores.Count == 0)\n+            return documents;\n+\n         var minScore = scores.Min();\n         var maxScore = scores.Max();\n-        var range = maxScore - minScore;\n+        \n+        if (minScore == null || maxScore == null)\n+            return documents;\n+            \n+        var range = NumOps.Subtract(maxScore, minScore);\n \n-        const double epsilon = 1e-8;\n-        if (Math.Abs(range) < epsilon)\n+        var epsilon = NumOps.FromDouble(1e-8);\n+        if (NumOps.LessThan(NumOps.Abs(range), epsilon))\n         {\n             // All scores are the same, set them all to 1.0\n+            var one = NumOps.One;\n             foreach (var doc in docsWithScores)\n             {\n-                doc.RelevanceScore = 1.0;\n+                doc.RelevanceScore = one;\n+                doc.HasRelevanceScore = true;\n             }\n         }\n         else\n         {\n             foreach (var doc in docsWithScores)\n             {\n-                doc.RelevanceScore = (doc.RelevanceScore!.Value - minScore) / range;\n+                doc.RelevanceScore = NumOps.Divide(NumOps.Subtract(doc.RelevanceScore, minScore), range);\n+                doc.HasRelevanceScore = true;\n             }",
    "path": "src/RetrievalAugmentedGeneration/Rerankers/RerankerBase.cs",
    "commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Prevent normalization crashes for generic score types**\n\n`Enumerable.Min/Max` requires `T` to implement `IComparable`. For supported types like `Complex<T>` that lack an ordering, this throws at runtime. Additionally, `NumOps.FromDouble(1e-8)` becomes zero for integral types, so the zero-range case falls through and `Divide(..., range)` dereferences a zero divisor. Both paths break downstream rerankers. Please compute min/max via `NumOps` and treat zero range explicitly before dividing. \n\nApply this diff:\n\n```diff\n-        var minScore = scores.Min();\n-        var maxScore = scores.Max();\n-        \n-        if (minScore == null || maxScore == null)\n-            return documents;\n-            \n-        var range = NumOps.Subtract(maxScore, minScore);\n-\n-        var epsilon = NumOps.FromDouble(1e-8);\n-        if (NumOps.LessThan(NumOps.Abs(range), epsilon))\n+        var minScore = scores[0];\n+        var maxScore = scores[0];\n+        for (var i = 1; i < scores.Count; i++)\n+        {\n+            var score = scores[i];\n+            if (NumOps.LessThan(score, minScore))\n+            {\n+                minScore = score;\n+            }\n+            if (NumOps.LessThan(maxScore, score))\n+            {\n+                maxScore = score;\n+            }\n+        }\n+\n+        var range = NumOps.Subtract(maxScore, minScore);\n+        var epsilon = NumOps.FromDouble(1e-8);\n+        var isZeroRange = EqualityComparer<T>.Default.Equals(range, NumOps.Zero);\n+        if (isZeroRange || NumOps.LessThan(NumOps.Abs(range), epsilon))\n         {\n             // All scores are the same, set them all to 1.0\n             var one = NumOps.One;\n             foreach (var doc in docsWithScores)\n             {\n                 doc.RelevanceScore = one;\n                 doc.HasRelevanceScore = true;\n             }\n         }\n         else\n         {\n             foreach (var doc in docsWithScores)\n             {\n                 doc.RelevanceScore = NumOps.Divide(NumOps.Subtract(doc.RelevanceScore, minScore), range);\n                 doc.HasRelevanceScore = true;\n             }\n         }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    protected IList<Document<T>> NormalizeScores(IList<Document<T>> documents)\n    {\n        var docsWithScores = documents.Where(d => d.HasRelevanceScore).ToList();\n        if (docsWithScores.Count == 0)\n            return documents;\n\n        var scores = docsWithScores.Select(d => d.RelevanceScore).ToList();\n        if (scores.Count == 0)\n            return documents;\n\n        var minScore = scores[0];\n        var maxScore = scores[0];\n        for (var i = 1; i < scores.Count; i++)\n        {\n            var score = scores[i];\n            if (NumOps.LessThan(score, minScore))\n            {\n                minScore = score;\n            }\n            if (NumOps.LessThan(maxScore, score))\n            {\n                maxScore = score;\n            }\n        }\n\n        var range = NumOps.Subtract(maxScore, minScore);\n        var epsilon = NumOps.FromDouble(1e-8);\n        var isZeroRange = EqualityComparer<T>.Default.Equals(range, NumOps.Zero);\n        if (isZeroRange || NumOps.LessThan(NumOps.Abs(range), epsilon))\n        {\n            // All scores are the same, set them all to 1.0\n            var one = NumOps.One;\n            foreach (var doc in docsWithScores)\n            {\n                doc.RelevanceScore = one;\n                doc.HasRelevanceScore = true;\n            }\n        }\n        else\n        {\n            foreach (var doc in docsWithScores)\n            {\n                doc.RelevanceScore = NumOps.Divide(NumOps.Subtract(doc.RelevanceScore, minScore), range);\n                doc.HasRelevanceScore = true;\n            }\n        }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Rerankers/RerankerBase.cs around lines\n153-188, the code uses LINQ Min/Max (which requires IComparable) and a\ndouble-derived epsilon that can be zero for integral score types; replace the\nEnumerable Min/Max with an explicit loop that computes min and max using NumOps\n(e.g., initialize min/max to the first score and update via\nNumOps.Min/NumOps.Max), compute range = NumOps.Subtract(max, min), then handle\nzero/near-zero range before dividing by checking NumOps.Equals(range,\nNumOps.Zero) OR, if NumOps.FromDouble(1e-8) is non-zero,\nNumOps.LessThan(NumOps.Abs(range), epsilon); if the range is zero/near-zero set\nall HasRelevanceScore and RelevanceScore to NumOps.One, otherwise normalize each\nscore with NumOps.Divide(NumOps.Subtract(score, min), range).\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits ec6a7c9 to 77a1bbb",
    "created_at": "2025-11-03T23:05:41Z",
    "updated_at": "2025-11-04T00:07:05Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108213",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108213"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108213"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108213/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 154,
    "original_start_line": 153,
    "start_side": "RIGHT",
    "line": 198,
    "original_line": 188,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 112,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108223",
    "pull_request_review_id": 3413280905,
    "id": 2488108223,
    "node_id": "PRRC_kwDOKSXUF86UTYS_",
    "diff_hunk": "@@ -0,0 +1,88 @@\n+using System;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System.Collections.Generic;\n+using System.Linq;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.RerankingStrategies\n+{\n+    /// <summary>\n+    /// Cross-encoder based reranking for improved relevance scoring.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class CrossEncoderReranker<T> : RerankingStrategyBase<T>\n+    {\n+        private readonly INumericOperations<T> _numOps;\n+        private readonly string _modelPath;\n+\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"CrossEncoderReranker{T}\"/> class.\n+        /// </summary>\n+        /// <param name=\"numericOperations\">The numeric operations for type T.</param>\n+        /// <param name=\"modelPath\">The path to the cross-encoder model.</param>\n+        public CrossEncoderReranker(INumericOperations<T> numericOperations, string? modelPath = null) : base(numericOperations)\n+        {\n+            _numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\n+            _modelPath = modelPath ?? string.Empty;\n+        }\n+\n+        /// <summary>\n+        /// Reranks documents using cross-encoder scoring.\n+        /// </summary>\n+        /// <param name=\"query\">The query string.</param>\n+        /// <param name=\"documents\">The documents to rerank.</param>\n+        /// <param name=\"topK\">The number of top documents to return.</param>\n+        /// <returns>A reranked list of documents.</returns>\n+        public override List<Document<T>> Rerank(string query, List<Document<T>> documents, int topK)\n+        {\n+            if (string.IsNullOrEmpty(query)) throw new ArgumentNullException(nameof(query));\n+            if (documents == null) throw new ArgumentNullException(nameof(documents));\n+            if (topK <= 0) throw new ArgumentOutOfRangeException(nameof(topK));\n+\n+            var scoredDocs = new List<(Document<T> doc, T score)>();\n+\n+            foreach (var doc in documents)\n+            {\n+                var score = ComputeCrossEncoderScore(query, doc.Content);\n+                scoredDocs.Add((doc, score));\n+            }\n+\n+            var reranked = scoredDocs\n+                .OrderByDescending(x => Convert.ToDouble(x.score))\n+                .Take(topK)\n+                .Select(x =>\n+                {\n+                    x.doc.RelevanceScore = x.score;\n+                    return x.doc;\n+                })\n+                .ToList();",
    "path": "src/RetrievalAugmentedGeneration/RerankingStrategies/CrossEncoderReranker.cs",
    "commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Set `HasRelevanceScore` after assigning scores.**\n\nLine 55 assigns a new relevance value but never marks the document as scored. Downstream components (e.g., MMR or diversity rerankers) check `HasRelevanceScore` before using the value; leaving it false causes them to ignore these scores entirely. Please flag the documents as scored when you update the value.  \n\n```diff\n             .Select(x =>\n             {\n                 x.doc.RelevanceScore = x.score;\n+                x.doc.HasRelevanceScore = true;\n                 return x.doc;\n             })\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/RerankingStrategies/CrossEncoderReranker.cs\naround lines 50 to 58, after assigning x.doc.RelevanceScore = x.score you must\nalso set x.doc.HasRelevanceScore = true so downstream components recognize the\ndocument as scored; update the Select block to mark HasRelevanceScore = true\n(alongside setting RelevanceScore) before returning the doc.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits ec6a7c9 to 77a1bbb",
    "created_at": "2025-11-03T23:05:41Z",
    "updated_at": "2025-11-04T00:07:06Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108223",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108223"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108223"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108223/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 50,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 58,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 58,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108232",
    "pull_request_review_id": 3413280905,
    "id": 2488108232,
    "node_id": "PRRC_kwDOKSXUF86UTYTI",
    "diff_hunk": "@@ -0,0 +1,124 @@\n+using System;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System.Collections.Generic;\n+using System.Linq;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.RerankingStrategies\n+{\n+    /// <summary>\n+    /// LLM-based reranking using language model relevance assessment.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class LLMBasedReranker<T> : RerankingStrategyBase<T>\n+    {\n+        private readonly INumericOperations<T> _numOps;\n+        private readonly string _llmEndpoint;\n+        private readonly string _apiKey;\n+\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"LLMBasedReranker{T}\"/> class.\n+        /// </summary>\n+        /// <param name=\"numericOperations\">The numeric operations for type T.</param>\n+        /// <param name=\"llmEndpoint\">The LLM API endpoint.</param>\n+        /// <param name=\"apiKey\">The API key for the LLM service.</param>\n+        public LLMBasedReranker(INumericOperations<T> numericOperations, string? llmEndpoint = null, string? apiKey = null) : base(numericOperations)\n+        {\n+            _numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\n+            _llmEndpoint = llmEndpoint ?? string.Empty;\n+            _apiKey = apiKey ?? string.Empty;\n+        }\n+\n+        /// <summary>\n+        /// Reranks documents using LLM-based relevance scoring.\n+        /// </summary>\n+        /// <param name=\"query\">The query string.</param>\n+        /// <param name=\"documents\">The documents to rerank.</param>\n+        /// <param name=\"topK\">The number of top documents to return.</param>\n+        /// <returns>A reranked list of documents.</returns>\n+        public override List<Document<T>> Rerank(string query, List<Document<T>> documents, int topK)\n+        {\n+            if (string.IsNullOrEmpty(query)) throw new ArgumentNullException(nameof(query));\n+            if (documents == null) throw new ArgumentNullException(nameof(documents));\n+            if (topK <= 0) throw new ArgumentOutOfRangeException(nameof(topK));\n+",
    "path": "src/RetrievalAugmentedGeneration/RerankingStrategies/LLMBasedReranker.cs",
    "commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Prevent zero-token queries from producing bogus scores.**\n\nWhitespace-only or punctuation-only queries slip past the current guard, leaving `queryWords.Count` at 0 and turning `totalScore / queryWords.Count` into `Infinity`, so every document gets the same max score. Tighten the validation and short-circuit empty token lists so we return a zero score instead of poisoning the ranking.\n\n\n```diff\n-            if (string.IsNullOrEmpty(query)) throw new ArgumentNullException(nameof(query));\n+            if (string.IsNullOrWhiteSpace(query)) throw new ArgumentNullException(nameof(query));\n@@\n-            var queryWords = Tokenize(query);\n+            var queryWords = Tokenize(query);\n+            if (queryWords.Count == 0)\n+                return _numOps.Zero;\n```\n\n\nAlso applies to: 68-77\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-03T23:05:41Z",
    "updated_at": "2025-11-03T23:05:44Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108232",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108232"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108232"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108232/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 41,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 44,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 44,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108240",
    "pull_request_review_id": 3413280905,
    "id": 2488108240,
    "node_id": "PRRC_kwDOKSXUF86UTYTQ",
    "diff_hunk": "@@ -0,0 +1,124 @@\n+using System;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System.Collections.Generic;\n+using System.Linq;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.RerankingStrategies\n+{\n+    /// <summary>\n+    /// LLM-based reranking using language model relevance assessment.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class LLMBasedReranker<T> : RerankingStrategyBase<T>\n+    {\n+        private readonly INumericOperations<T> _numOps;\n+        private readonly string _llmEndpoint;\n+        private readonly string _apiKey;\n+\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"LLMBasedReranker{T}\"/> class.\n+        /// </summary>\n+        /// <param name=\"numericOperations\">The numeric operations for type T.</param>\n+        /// <param name=\"llmEndpoint\">The LLM API endpoint.</param>\n+        /// <param name=\"apiKey\">The API key for the LLM service.</param>\n+        public LLMBasedReranker(INumericOperations<T> numericOperations, string? llmEndpoint = null, string? apiKey = null) : base(numericOperations)\n+        {\n+            _numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\n+            _llmEndpoint = llmEndpoint ?? string.Empty;\n+            _apiKey = apiKey ?? string.Empty;\n+        }\n+\n+        /// <summary>\n+        /// Reranks documents using LLM-based relevance scoring.\n+        /// </summary>\n+        /// <param name=\"query\">The query string.</param>\n+        /// <param name=\"documents\">The documents to rerank.</param>\n+        /// <param name=\"topK\">The number of top documents to return.</param>\n+        /// <returns>A reranked list of documents.</returns>\n+        public override List<Document<T>> Rerank(string query, List<Document<T>> documents, int topK)\n+        {\n+            if (string.IsNullOrEmpty(query)) throw new ArgumentNullException(nameof(query));\n+            if (documents == null) throw new ArgumentNullException(nameof(documents));\n+            if (topK <= 0) throw new ArgumentOutOfRangeException(nameof(topK));\n+\n+            var scoredDocs = new List<(Document<T> doc, T score)>();\n+\n+            foreach (var doc in documents)\n+            {\n+                var score = AssessRelevance(query, doc.Content);\n+                scoredDocs.Add((doc, score));\n+            }\n+\n+            var reranked = scoredDocs\n+                .OrderByDescending(x => Convert.ToDouble(x.score))\n+                .Take(topK)\n+                .Select(x =>\n+                {\n+                    x.doc.RelevanceScore = x.score;\n+                    return x.doc;\n+                })",
    "path": "src/RetrievalAugmentedGeneration/RerankingStrategies/LLMBasedReranker.cs",
    "commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Mark reranked documents as scored.**\n\nDownstream components rely on `HasRelevanceScore`. Leaving it `false` after reranking means later stages will think no score was assigned.\n\n\n```diff\n                     x.doc.RelevanceScore = x.score;\n+                    x.doc.HasRelevanceScore = true;\n                     return x.doc;\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/RerankingStrategies/LLMBasedReranker.cs\naround lines 58 to 60, the code sets x.doc.RelevanceScore = x.score but does not\nmark the document as scored; update the reranking mapping to also set\nx.doc.HasRelevanceScore = true (or the equivalent flag/property) immediately\nafter assigning RelevanceScore so downstream components see the document as\nscored.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits ec6a7c9 to 77a1bbb",
    "created_at": "2025-11-03T23:05:41Z",
    "updated_at": "2025-11-04T00:07:10Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108240",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108240"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108240"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108240/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 59,
    "original_start_line": 58,
    "start_side": "RIGHT",
    "line": 62,
    "original_line": 60,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 60,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108250",
    "pull_request_review_id": 3413280905,
    "id": 2488108250,
    "node_id": "PRRC_kwDOKSXUF86UTYTa",
    "diff_hunk": "@@ -0,0 +1,122 @@\n+using System;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System.Collections.Generic;\n+using System.Linq;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.RerankingStrategies\n+{\n+    /// <summary>\n+    /// Reciprocal Rank Fusion for combining multiple ranking lists.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class ReciprocalRankFusion<T> : RerankingStrategyBase<T>\n+    {\n+        private readonly INumericOperations<T> _numOps;\n+        private readonly int _k;\n+\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"ReciprocalRankFusion{T}\"/> class.\n+        /// </summary>\n+        /// <param name=\"numericOperations\">The numeric operations for type T.</param>\n+        /// <param name=\"k\">The constant k for reciprocal rank formula (default: 60).</param>\n+        public ReciprocalRankFusion(INumericOperations<T> numericOperations, int k = 60) : base(numericOperations)\n+        {\n+            _numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\n+            _k = k > 0 ? k : throw new ArgumentOutOfRangeException(nameof(k));\n+        }\n+\n+        /// <summary>\n+        /// Reranks documents using reciprocal rank fusion.\n+        /// </summary>\n+        /// <param name=\"query\">The query string.</param>\n+        /// <param name=\"documents\">The documents to rerank.</param>\n+        /// <param name=\"topK\">The number of top documents to return.</param>\n+        /// <returns>A reranked list of documents.</returns>\n+        public override List<Document<T>> Rerank(string query, List<Document<T>> documents, int topK)\n+        {\n+            if (string.IsNullOrEmpty(query)) throw new ArgumentNullException(nameof(query));\n+            if (documents == null) throw new ArgumentNullException(nameof(documents));\n+            if (topK <= 0) throw new ArgumentOutOfRangeException(nameof(topK));\n+\n+            var scores = new Dictionary<string, T>();\n+\n+            for (int rank = 0; rank < documents.Count; rank++)\n+            {\n+                var doc = documents[rank];\n+                var rrfScore = _numOps.FromDouble(1.0 / (_k + rank + 1));\n+\n+                if (scores.ContainsKey(doc.Id))\n+                {\n+                    scores[doc.Id] = _numOps.Add(scores[doc.Id], rrfScore);\n+                }\n+                else\n+                {\n+                    scores[doc.Id] = rrfScore;\n+                }\n+            }\n+\n+            var reranked = documents\n+                .OrderByDescending(d => Convert.ToDouble(scores.ContainsKey(d.Id) ? scores[d.Id] : _numOps.Zero))\n+                .Take(topK)\n+                .ToList();\n+\n+            foreach (var doc in reranked)\n+            {\n+                if (scores.ContainsKey(doc.Id))\n+                {\n+                    doc.RelevanceScore = scores[doc.Id];\n+                }\n+            }",
    "path": "src/RetrievalAugmentedGeneration/RerankingStrategies/ReciprocalRankFusion.cs",
    "commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Mark documents as having scores**\n\nAfter you assign `RelevanceScore`, you need to set `doc.HasRelevanceScore = true`; otherwise anything that checks the flag (e.g., coverage heuristics) will treat these reranker outputs as unscored.\n\n\n\nAlso applies to: 114-117\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/RerankingStrategies/ReciprocalRankFusion.cs\naround lines 64-70 (and similarly at 114-117), after assigning\ndoc.RelevanceScore = scores[doc.Id] also set doc.HasRelevanceScore = true so\ndownstream checks treat these documents as scored; update both blocks to set the\nflag immediately after assigning the score (and ensure the scores lookup is safe\nas currently implemented).\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits ec6a7c9 to 77a1bbb",
    "created_at": "2025-11-03T23:05:41Z",
    "updated_at": "2025-11-04T00:07:11Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108250",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108250"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108250"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108250/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 64,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 70,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 70,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108255",
    "pull_request_review_id": 3413280905,
    "id": 2488108255,
    "node_id": "PRRC_kwDOKSXUF86UTYTf",
    "diff_hunk": "@@ -0,0 +1,124 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\n+{\n+    /// <summary>\n+    /// BM25 (Best Matching 25) retrieval algorithm for sparse keyword-based search.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class BM25Retriever<T> : RetrieverBase<T>\n+    {\n+        private readonly IDocumentStore<T> _documentStore;\n+        private readonly T _k1;\n+        private readonly T _b;\n+        private readonly Dictionary<string, Dictionary<string, T>> _termFrequencies;\n+        private readonly Dictionary<string, T> _documentLengths;\n+        private T _avgDocLength;\n+\n+        public BM25Retriever(IDocumentStore<T> documentStore, int defaultTopK = 5, double k1 = 1.5, double b = 0.75) \n+            : base(defaultTopK)\n+        {\n+            if (documentStore == null)\n+                throw new ArgumentNullException(nameof(documentStore));\n+\n+            _documentStore = documentStore;\n+            _k1 = NumOps.FromDouble(k1);\n+            _b = NumOps.FromDouble(b);\n+            _termFrequencies = new Dictionary<string, Dictionary<string, T>>();\n+            _documentLengths = new Dictionary<string, T>();\n+            _avgDocLength = NumOps.Zero;\n+        }\n+\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\n+        {\n+            var queryTerms = Tokenize(query);\n+            var scores = new Dictionary<string, T>();\n+\n+            var candidates = _documentStore.GetSimilar(\n+                new AiDotNet.LinearAlgebra.Vector<T>(new T[Math.Max(1, _documentStore.VectorDimension)]),\n+                _documentStore.DocumentCount\n+            );\n+\n+            foreach (var doc in candidates)\n+            {\n+                if (!MatchesFilters(doc, metadataFilters))\n+                    continue;\n+\n+                var score = NumOps.Zero;\n+                \n+                foreach (var term in queryTerms)\n+                {\n+                    var termScore = CalculateBM25Term(doc.Id, term);\n+                    score = NumOps.Add(score, termScore);\n+                }\n+\n+                scores[doc.Id] = score;\n+            }",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/BM25Retriever.cs",
    "commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Initialize BM25 corpus statistics before scoring.**\n\n`_termFrequencies`, `_documentLengths`, and `_avgDocLength` are never populated, so `CalculateBM25Term` returns zero for every term and then divides by `_avgDocLength == 0`, throwing the first time you evaluate a document with content. Please precompute per-document term frequencies, lengths, and a non-zero average (and guard against an empty corpus) before invoking the scoring loop.\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits ec6a7c9 to 77a1bbb",
    "created_at": "2025-11-03T23:05:41Z",
    "updated_at": "2025-11-04T00:07:14Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108255",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108255"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108255"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108255/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 19,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 61,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 61,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108258",
    "pull_request_review_id": 3413280905,
    "id": 2488108258,
    "node_id": "PRRC_kwDOKSXUF86UTYTi",
    "diff_hunk": "@@ -0,0 +1,124 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\n+{\n+    /// <summary>\n+    /// BM25 (Best Matching 25) retrieval algorithm for sparse keyword-based search.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class BM25Retriever<T> : RetrieverBase<T>\n+    {\n+        private readonly IDocumentStore<T> _documentStore;\n+        private readonly T _k1;\n+        private readonly T _b;\n+        private readonly Dictionary<string, Dictionary<string, T>> _termFrequencies;\n+        private readonly Dictionary<string, T> _documentLengths;\n+        private T _avgDocLength;\n+\n+        public BM25Retriever(IDocumentStore<T> documentStore, int defaultTopK = 5, double k1 = 1.5, double b = 0.75) \n+            : base(defaultTopK)\n+        {\n+            if (documentStore == null)\n+                throw new ArgumentNullException(nameof(documentStore));\n+\n+            _documentStore = documentStore;\n+            _k1 = NumOps.FromDouble(k1);\n+            _b = NumOps.FromDouble(b);\n+            _termFrequencies = new Dictionary<string, Dictionary<string, T>>();\n+            _documentLengths = new Dictionary<string, T>();\n+            _avgDocLength = NumOps.Zero;\n+        }\n+\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\n+        {\n+            var queryTerms = Tokenize(query);\n+            var scores = new Dictionary<string, T>();\n+\n+            var candidates = _documentStore.GetSimilar(\n+                new AiDotNet.LinearAlgebra.Vector<T>(new T[Math.Max(1, _documentStore.VectorDimension)]),\n+                _documentStore.DocumentCount\n+            );\n+\n+            foreach (var doc in candidates)\n+            {\n+                if (!MatchesFilters(doc, metadataFilters))\n+                    continue;\n+\n+                var score = NumOps.Zero;\n+                \n+                foreach (var term in queryTerms)\n+                {\n+                    var termScore = CalculateBM25Term(doc.Id, term);\n+                    score = NumOps.Add(score, termScore);\n+                }",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/BM25Retriever.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Include the IDF factor in the BM25 score.**\n\nRight now we only accumulate term-frequency contributions. BM25 weights each term by its inverse document frequency; without IDF, ubiquitous terms dominate the ranking and the retriever no longer behaves like BM25. Please compute doc frequencies (N, n(q)) from the corpus stats you gather and fold the IDF multiplier into each term contribution.\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits ec6a7c9 to 77a1bbb",
    "created_at": "2025-11-03T23:05:42Z",
    "updated_at": "2025-11-04T00:07:13Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108258",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108258"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108258"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108258/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 43,
    "original_start_line": 54,
    "start_side": "RIGHT",
    "line": 47,
    "original_line": 58,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 58,
    "position": 47,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108263",
    "pull_request_review_id": 3413280905,
    "id": 2488108263,
    "node_id": "PRRC_kwDOKSXUF86UTYTn",
    "diff_hunk": "@@ -0,0 +1,124 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\n+{\n+    /// <summary>\n+    /// BM25 (Best Matching 25) retrieval algorithm for sparse keyword-based search.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class BM25Retriever<T> : RetrieverBase<T>\n+    {\n+        private readonly IDocumentStore<T> _documentStore;\n+        private readonly T _k1;\n+        private readonly T _b;\n+        private readonly Dictionary<string, Dictionary<string, T>> _termFrequencies;\n+        private readonly Dictionary<string, T> _documentLengths;\n+        private T _avgDocLength;\n+\n+        public BM25Retriever(IDocumentStore<T> documentStore, int defaultTopK = 5, double k1 = 1.5, double b = 0.75) \n+            : base(defaultTopK)\n+        {\n+            if (documentStore == null)\n+                throw new ArgumentNullException(nameof(documentStore));\n+\n+            _documentStore = documentStore;\n+            _k1 = NumOps.FromDouble(k1);\n+            _b = NumOps.FromDouble(b);\n+            _termFrequencies = new Dictionary<string, Dictionary<string, T>>();\n+            _documentLengths = new Dictionary<string, T>();\n+            _avgDocLength = NumOps.Zero;\n+        }\n+\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\n+        {\n+            var queryTerms = Tokenize(query);\n+            var scores = new Dictionary<string, T>();\n+\n+            var candidates = _documentStore.GetSimilar(\n+                new AiDotNet.LinearAlgebra.Vector<T>(new T[Math.Max(1, _documentStore.VectorDimension)]),\n+                _documentStore.DocumentCount\n+            );\n+\n+            foreach (var doc in candidates)\n+            {\n+                if (!MatchesFilters(doc, metadataFilters))\n+                    continue;\n+\n+                var score = NumOps.Zero;\n+                \n+                foreach (var term in queryTerms)\n+                {\n+                    var termScore = CalculateBM25Term(doc.Id, term);\n+                    score = NumOps.Add(score, termScore);\n+                }\n+\n+                scores[doc.Id] = score;\n+            }\n+\n+            var results = scores\n+                .OrderByDescending(kv => kv.Value)\n+                .Take(topK)\n+                .Select(kv =>\n+                {\n+                    var doc = candidates.FirstOrDefault(d => d.Id == kv.Key);\n+                    if (doc != null)\n+                    {\n+                        doc.RelevanceScore = kv.Value;\n+                        doc.HasRelevanceScore = true;\n+                    }\n+                    return doc;\n+                })\n+                .Where(d => d != null)\n+                .Cast<Document<T>>();\n+\n+            return results;\n+        }\n+\n+        private T CalculateBM25Term(string docId, string term)\n+        {\n+            if (!_termFrequencies.ContainsKey(docId) || !_termFrequencies[docId].ContainsKey(term))\n+                return NumOps.Zero;\n+\n+            var tf = _termFrequencies[docId][term];\n+            var docLength = _documentLengths.ContainsKey(docId) ? _documentLengths[docId] : NumOps.Zero;\n+\n+            var numerator = NumOps.Multiply(tf, NumOps.Add(_k1, NumOps.One));\n+            var denominator = NumOps.Add(tf, NumOps.Multiply(_k1, \n+                NumOps.Subtract(NumOps.One, NumOps.Add(_b, NumOps.Multiply(_b, NumOps.Divide(docLength, _avgDocLength))))));\n+\n+            return NumOps.Divide(numerator, denominator);\n+        }",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/BM25Retriever.cs",
    "commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Fix the BM25 length-normalization formula.**\n\nThe denominator currently computes `1 - (b + b * |d| / avgdl)`, but BM25 requires `1 - b + b * |d| / avgdl`. The inverted sign drives the normalization negative once documents exceed the average length, producing incorrect or even negative scores. Update the algebra and reuse the normalization term when building the denominator:\n\n```diff\n-            var denominator = NumOps.Add(tf, NumOps.Multiply(_k1, \n-                NumOps.Subtract(NumOps.One, NumOps.Add(_b, NumOps.Multiply(_b, NumOps.Divide(docLength, _avgDocLength))))));\n+            var lengthNorm = NumOps.Add(\n+                NumOps.Subtract(NumOps.One, _b),\n+                NumOps.Multiply(_b, NumOps.Divide(docLength, _avgDocLength)));\n+            var denominator = NumOps.Add(tf, NumOps.Multiply(_k1, lengthNorm));\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n            var numerator = NumOps.Multiply(tf, NumOps.Add(_k1, NumOps.One));\n            var lengthNorm = NumOps.Add(\n                NumOps.Subtract(NumOps.One, _b),\n                NumOps.Multiply(_b, NumOps.Divide(docLength, _avgDocLength)));\n            var denominator = NumOps.Add(tf, NumOps.Multiply(_k1, lengthNorm));\n\n            return NumOps.Divide(numerator, denominator);\n        }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Retrievers/BM25Retriever.cs around lines 90\nto 95, the BM25 length-normalization term is built with the wrong sign; replace\nthe current expression so the normalization term equals (1 - b) + b * (|d| /\navgdl) and reuse that term when constructing the denominator. Concretely,\ncompute normalizationTerm = (NumOps.One - _b) + (_b * (docLength /\n_avgDocLength)) using NumOps, then set denominator = NumOps.Add(tf,\nNumOps.Multiply(_k1, normalizationTerm)) so the denominator implements tf + k1 *\n((1 - b) + b * |d|/avgdl).\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits ec6a7c9 to 77a1bbb",
    "created_at": "2025-11-03T23:05:42Z",
    "updated_at": "2025-11-04T00:07:16Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108263",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108263"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108263"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108263/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 80,
    "original_start_line": 90,
    "start_side": "RIGHT",
    "line": 165,
    "original_line": 95,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 95,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108271",
    "pull_request_review_id": 3413280905,
    "id": 2488108271,
    "node_id": "PRRC_kwDOKSXUF86UTYTv",
    "diff_hunk": "@@ -0,0 +1,108 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\n+{\n+    /// <summary>\n+    /// TF-IDF (Term Frequency-Inverse Document Frequency) retrieval strategy.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class TFIDFRetriever<T> : RetrieverBase<T>\n+    {\n+        private readonly IDocumentStore<T> _documentStore;\n+        private readonly Dictionary<string, Dictionary<string, T>> _tfidf;\n+        private readonly Dictionary<string, T> _idf;\n+\n+        public TFIDFRetriever(IDocumentStore<T> documentStore, int defaultTopK = 5) : base(defaultTopK)\n+        {\n+            if (documentStore == null)\n+                throw new ArgumentNullException(nameof(documentStore));\n+                \n+            _documentStore = documentStore;\n+            _tfidf = new Dictionary<string, Dictionary<string, T>>();\n+            _idf = new Dictionary<string, T>();\n+        }\n+\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\n+        {\n+            var queryTerms = Tokenize(query);\n+            var scores = new Dictionary<string, T>();\n+\n+            var candidates = _documentStore.GetSimilar(\n+                new AiDotNet.LinearAlgebra.Vector<T>(new T[_documentStore.VectorDimension]), \n+                _documentStore.DocumentCount\n+            );\n+\n+            foreach (var doc in candidates)\n+            {\n+                if (!MatchesFilters(doc, metadataFilters))\n+                    continue;\n+\n+                var score = NumOps.Zero;\n+\n+                if (_tfidf.ContainsKey(doc.Id))\n+                {\n+                    var docTfidf = _tfidf[doc.Id];\n+                    foreach (var term in queryTerms)\n+                    {\n+                        if (docTfidf.ContainsKey(term))\n+                        {\n+                            score = NumOps.Add(score, docTfidf[term]);\n+                        }\n+                    }\n+                }\n+\n+                scores[doc.Id] = score;\n+            }\n+\n+            var results = scores\n+                .OrderByDescending(kv => kv.Value)\n+                .Take(topK)\n+                .Select(kv =>\n+                {\n+                    var doc = candidates.FirstOrDefault(d => d.Id == kv.Key);\n+                    if (doc != null)\n+                    {\n+                        doc.RelevanceScore = kv.Value;\n+                        doc.HasRelevanceScore = true;\n+                    }\n+                    return doc;\n+                })\n+                .Where(d => d != null)\n+                .Cast<Document<T>>();\n+\n+            return results;\n+        }",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/TFIDFRetriever.cs",
    "commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "original_commit_id": "a7f60f0d884a1052b3400d2facec8358332db4b8",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**TFΓÇæIDF scores never built**\n\n`_tfidf` (and `_idf`) stay empty for the lifetime of this retriever, so every document ends up with the default `NumOps.Zero` score and the ranking collapses. Please add the TF/IDF bookkeeping (e.g., during ingestion or lazily before retrieval) so `RetrieveCore` has real weights to sum.\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits ec6a7c9 to 77a1bbb",
    "created_at": "2025-11-03T23:05:42Z",
    "updated_at": "2025-11-04T00:07:17Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108271",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108271"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488108271"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488108271/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 18,
    "original_start_line": 20,
    "start_side": "RIGHT",
    "line": 68,
    "original_line": 79,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 79,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488221098",
    "pull_request_review_id": 3413430414,
    "id": 2488221098,
    "node_id": "PRRC_kwDOKSXUF86UTz2q",
    "diff_hunk": "@@ -0,0 +1,103 @@\n+# AiDotNet Copilot Instructions - CRITICAL RULES\n+\n+## Architecture Rules (DO NOT VIOLATE)\n+\n+### 1. Generic Types and Numeric Operations\n+- **ALWAYS** use generic types with `INumericOperations<T>` interface\n+- **NEVER** hardcode `double`, `float`, or specific numeric types\n+- Use `NumOps.FromDouble()`, `NumOps.Zero`, `NumOps.One` instead of literals or `default(T)`\n+- **NEVER** request `INumericOperations<T>` in public constructors - this is handled internally",
    "path": ".github/COPILOT_INSTRUCTIONS.md",
    "commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "original_commit_id": "77a1bbb5864b8564f484f102b991a8e5ef80d32f",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n<details>\n<summary>≡ƒº⌐ Analysis chain</summary>\n\n**Verify the INumericOperations<T> constructor pattern is consistently enforced across implementations.**\n\nThe rule states that `INumericOperations<T>` should never be requested in public constructors. This is a strong constraint that must be verified across all concrete implementations in the PR to ensure consistency and prevent future violations.\n\n\n\nRun the following script to verify this pattern is upheld in concrete implementations:\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Description: Check if public constructors in RAG implementations request INumericOperations<T>\n\n# Search for public constructors in RAG concrete implementations that have INumericOperations parameter\n# Expected: Zero matches (indicating the pattern is enforced)\nrg -n 'public\\s+\\w+\\s*\\(' \\\n  --type=cs \\\n  -A 2 'INumericOperations<' \\\n  -g '**/RetrievalAugmentedGeneration/**' \\\n  -g '!**/Interfaces/**' \\\n  | grep -E '(public|INumericOperations)' \\\n  | head -20\n```\n\nLength of output: 59\n\n---\n\n[/run_scripts]\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Search for public constructors with INumericOperations parameter in RAG implementations\n# Corrected approach: search for the pattern directly\n\nrg -n 'public\\s+\\w+\\s*\\([^)]*INumericOperations<' \\\n  --type=cs \\\n  --glob='**/RetrievalAugmentedGeneration/**' \\\n  --glob='!**/Interfaces/**' \\\n  -C 2\n```\n\nLength of output: 3886\n\n---\n\n\n\n</details>\n\n**Fix public constructor violations of INumericOperations<T> constraint.**\n\nThe verification found 5 concrete implementations that violate the guideline - all have public constructors requesting `INumericOperations<T>`. These must be refactored to handle the dependency internally:\n\n- `src/RetrievalAugmentedGeneration/RerankingStrategies/LLMBasedReranker.cs:25`\n- `src/RetrievalAugmentedGeneration/RerankingStrategies/ReciprocalRankFusion.cs:23`\n- `src/RetrievalAugmentedGeneration/RerankingStrategies/CrossEncoderReranker.cs:23`\n- `src/RetrievalAugmentedGeneration/Configuration/RAGConfigurationBuilder.cs:19`\n- `src/RetrievalAugmentedGeneration/QueryExpansion/HyDEQueryExpansion.cs:24`\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn the listed files\n(src/RetrievalAugmentedGeneration/RerankingStrategies/LLMBasedReranker.cs:25,\nsrc/RetrievalAugmentedGeneration/RerankingStrategies/ReciprocalRankFusion.cs:23,\nsrc/RetrievalAugmentedGeneration/RerankingStrategies/CrossEncoderReranker.cs:23,\nsrc/RetrievalAugmentedGeneration/Configuration/RAGConfigurationBuilder.cs:19,\nsrc/RetrievalAugmentedGeneration/QueryExpansion/HyDEQueryExpansion.cs:24) the\npublic constructors currently accept an INumericOperations<T> parameter which\nviolates the guideline; remove INumericOperations<T> from the public constructor\nsignatures, make the dependency internal/private by resolving or creating the\nnumeric operations instance inside the class (e.g., use a private factory\nmethod, internal static provider, or obtain it from an internal service\nlocator/DI configured only inside the assembly), update constructors to accept\nonly necessary public dependencies, and adjust any callers/tests to stop passing\nINumericOperations<T> while preserving behavior by initializing the numeric\noperations within the class.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T00:13:37Z",
    "updated_at": "2025-11-04T00:13:39Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488221098",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488221098"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488221098"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488221098/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": null,
    "original_line": 9,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 9,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488221106",
    "pull_request_review_id": 3413430414,
    "id": 2488221106,
    "node_id": "PRRC_kwDOKSXUF86UTz2y",
    "diff_hunk": "@@ -0,0 +1,91 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\n+using System;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels\n+{\n+    /// <summary>\n+    /// OpenAI embedding model for generating embeddings via OpenAI API.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class OpenAIEmbeddingModel<T> : EmbeddingModelBase<T>\n+    {\n+        private readonly string _apiKey;\n+        private readonly string _modelName;\n+        private readonly int _dimension;\n+        private readonly int _maxTokens;\n+\n+        public override int EmbeddingDimension => _dimension;\n+        public override int MaxTokens => _maxTokens;\n+\n+        public OpenAIEmbeddingModel(string apiKey, string modelName = \"text-embedding-ada-002\", int dimension = 1536, int maxTokens = 8191)\n+        {\n+            if (string.IsNullOrWhiteSpace(apiKey))\n+                throw new ArgumentException(\"API key cannot be empty\", nameof(apiKey));\n+            if (string.IsNullOrWhiteSpace(modelName))\n+                throw new ArgumentException(\"Model name cannot be empty\", nameof(modelName));\n+            if (dimension <= 0)\n+                throw new ArgumentException(\"Dimension must be positive\", nameof(dimension));\n+            if (maxTokens <= 0)\n+                throw new ArgumentException(\"Max tokens must be positive\", nameof(maxTokens));\n+\n+            _apiKey = apiKey;\n+            _modelName = modelName;\n+            _dimension = dimension;\n+            _maxTokens = maxTokens;\n+        }",
    "path": "src/RetrievalAugmentedGeneration/Embeddings/OpenAIEmbeddingModel.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "77a1bbb5864b8564f484f102b991a8e5ef80d32f",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Placeholder implementation creates critical production risk.**\n\nThis class is named `OpenAIEmbeddingModel` and accepts an API key, strongly implying it calls the OpenAI embeddings API. However, `EmbedCore` (lines 39-51) generates synthetic cosine-based vectors instead of calling OpenAI. Developers integrating this code will expect real semantic embeddings but receive synthetic placeholders, breaking retrieval quality and potentially causing silent production failures.\n\n\n\nEither:\n1. Implement actual OpenAI API calls using the stored `_apiKey` and `_modelName`, or\n2. Rename to `SyntheticEmbeddingModel` / `PlaceholderEmbeddingModel` and remove the misleading API parameters.\n\n```diff\n-public class OpenAIEmbeddingModel<T> : EmbeddingModelBase<T>\n+public class SyntheticCosineLikeEmbeddingModel<T> : EmbeddingModelBase<T>\n {\n-    private readonly string _apiKey;\n-    private readonly string _modelName;\n     private readonly int _dimension;\n     private readonly int _maxTokens;\n \n-    public OpenAIEmbeddingModel(string apiKey, string modelName = \"text-embedding-ada-002\", int dimension = 1536, int maxTokens = 8191)\n+    public SyntheticCosineLikeEmbeddingModel(int dimension = 1536, int maxTokens = 8191)\n     {\n-        if (string.IsNullOrWhiteSpace(apiKey))\n-            throw new ArgumentException(\"API key cannot be empty\", nameof(apiKey));\n-        if (string.IsNullOrWhiteSpace(modelName))\n-            throw new ArgumentException(\"Model name cannot be empty\", nameof(modelName));\n         if (dimension <= 0)\n             throw new ArgumentException(\"Dimension must be positive\", nameof(dimension));\n         if (maxTokens <= 0)\n             throw new ArgumentException(\"Max tokens must be positive\", nameof(maxTokens));\n \n-        _apiKey = apiKey;\n-        _modelName = modelName;\n         _dimension = dimension;\n         _maxTokens = maxTokens;\n     }\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/EmbeddingModels/OpenAIEmbeddingModel.cs\naround lines 22-37: the constructor exposes an API key and model name but the\nclass currently returns synthetic cosine-based vectors (placeholder) which is\nmisleading and dangerous; either implement real OpenAI embedding calls using the\nstored _apiKey and _modelName or rename/remove API-specific parameters to make\nit explicitly a synthetic placeholder. To implement real calls: replace the\nplaceholder EmbedCore logic to call the OpenAI embeddings endpoint (or official\nSDK) using _apiKey and _modelName, send the input text, validate response vector\nlength equals _dimension, handle HTTP errors, rate limits, timeouts, and surface\nmeaningful exceptions/logging; add unit/integration tests and update any\ncallers. To instead rename: rename class to SyntheticEmbeddingModel (or\nPlaceholderEmbeddingModel), remove apiKey/modelName parameters from the\nconstructor (or mark them optional and ignored), update usages/tests and\ndocumentation to reflect synthetic behavior.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T00:13:38Z",
    "updated_at": "2025-11-04T00:13:39Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488221106",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488221106"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488221106"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488221106/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 22,
    "original_start_line": 22,
    "start_side": "RIGHT",
    "line": 37,
    "original_line": 37,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 37,
    "position": 37,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488221110",
    "pull_request_review_id": 3413430414,
    "id": 2488221110,
    "node_id": "PRRC_kwDOKSXUF86UTz22",
    "diff_hunk": "@@ -0,0 +1,91 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\n+using System;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels\n+{\n+    /// <summary>\n+    /// OpenAI embedding model for generating embeddings via OpenAI API.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class OpenAIEmbeddingModel<T> : EmbeddingModelBase<T>\n+    {\n+        private readonly string _apiKey;\n+        private readonly string _modelName;\n+        private readonly int _dimension;\n+        private readonly int _maxTokens;\n+\n+        public override int EmbeddingDimension => _dimension;\n+        public override int MaxTokens => _maxTokens;\n+\n+        public OpenAIEmbeddingModel(string apiKey, string modelName = \"text-embedding-ada-002\", int dimension = 1536, int maxTokens = 8191)\n+        {\n+            if (string.IsNullOrWhiteSpace(apiKey))\n+                throw new ArgumentException(\"API key cannot be empty\", nameof(apiKey));\n+            if (string.IsNullOrWhiteSpace(modelName))\n+                throw new ArgumentException(\"Model name cannot be empty\", nameof(modelName));\n+            if (dimension <= 0)\n+                throw new ArgumentException(\"Dimension must be positive\", nameof(dimension));\n+            if (maxTokens <= 0)\n+                throw new ArgumentException(\"Max tokens must be positive\", nameof(maxTokens));\n+\n+            _apiKey = apiKey;\n+            _modelName = modelName;\n+            _dimension = dimension;\n+            _maxTokens = maxTokens;\n+        }\n+\n+        protected override Vector<T> EmbedCore(string text)\n+        {\n+            var values = new T[_dimension];\n+            var hash = GetDeterministicHash(text);\n+            \n+            for (int i = 0; i < _dimension; i++)\n+            {\n+                var val = NumOps.FromDouble(Math.Cos(hash * (i + 1) * 0.001));\n+                values[i] = val;\n+            }\n+\n+            return NormalizeVector(new Vector<T>(values));\n+        }\n+\n+        private int GetDeterministicHash(string text)\n+        {\n+            if (string.IsNullOrEmpty(text))\n+                return 0;\n+\n+            unchecked\n+            {\n+                int hash = 23;\n+                foreach (char c in text)\n+                {\n+                    hash = hash * 31 + c;\n+                }\n+                return hash;\n+            }\n+        }\n+\n+        private Vector<T> NormalizeVector(Vector<T> vector)\n+        {\n+            var magnitude = NumOps.Zero;\n+            for (int i = 0; i < vector.Length; i++)\n+            {\n+                magnitude = NumOps.Add(magnitude, NumOps.Multiply(vector[i], vector[i]));\n+            }\n+            magnitude = NumOps.FromDouble(Math.Sqrt(Convert.ToDouble(magnitude)));\n+\n+            if (NumOps.GreaterThan(magnitude, NumOps.Zero))\n+            {\n+                var normalized = new T[vector.Length];\n+                for (int i = 0; i < vector.Length; i++)\n+                {\n+                    normalized[i] = NumOps.Divide(vector[i], magnitude);\n+                }\n+                return new Vector<T>(normalized);\n+            }\n+            \n+            return vector;\n+        }",
    "path": "src/RetrievalAugmentedGeneration/EmbeddingModels/OpenAIEmbeddingModel.cs",
    "commit_id": "fb037f7036c63184b2417768a5e0933c503d9233",
    "original_commit_id": "77a1bbb5864b8564f484f102b991a8e5ef80d32f",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_≡ƒ¢á∩╕Å Refactor suggestion_ | _≡ƒƒá Major_\n\n**Extract duplicated helper methods to base class.**\n\n`GetDeterministicHash` and `NormalizeVector` are duplicated identically across OpenAIEmbeddingModel.cs, ONNXSentenceTransformer.cs, HuggingFaceEmbeddingModel.cs, and LocalTransformerEmbedding.cs. This violates DRY and complicates maintenance.\n\n\n\nMove both methods to `EmbeddingModelBase<T>` as protected methods so all derived classes can reuse them:\n\nIn `src/RetrievalAugmentedGeneration/Embeddings/EmbeddingModelBase.cs`, add:\n\n```csharp\nprotected int GetDeterministicHash(string text)\n{\n    if (string.IsNullOrEmpty(text))\n        return 0;\n\n    unchecked\n    {\n        int hash = 23;\n        foreach (char c in text)\n        {\n            hash = hash * 31 + c;\n        }\n        return hash;\n    }\n}\n\nprotected Vector<T> NormalizeVector(Vector<T> vector)\n{\n    var magnitude = NumOps.Zero;\n    for (int i = 0; i < vector.Length; i++)\n    {\n        magnitude = NumOps.Add(magnitude, NumOps.Multiply(vector[i], vector[i]));\n    }\n    magnitude = NumOps.FromDouble(Math.Sqrt(Convert.ToDouble(magnitude)));\n\n    if (NumOps.GreaterThan(magnitude, NumOps.Zero))\n    {\n        var normalized = new T[vector.Length];\n        for (int i = 0; i < vector.Length; i++)\n        {\n            normalized[i] = NumOps.Divide(vector[i], magnitude);\n        }\n        return new Vector<T>(normalized);\n    }\n    \n    return vector;\n}\n```\n\nThen remove these methods from all four derived classes.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/EmbeddingModels/OpenAIEmbeddingModel.cs\naround lines 53 to 89 the methods GetDeterministicHash and NormalizeVector are\nduplicated across multiple embedding classes; move both methods into\nsrc/RetrievalAugmentedGeneration/Embeddings/EmbeddingModelBase.cs as protected\ngeneric methods (preserving their logic and using NumOps/Vector<T> generics),\nthen delete the duplicate implementations from OpenAIEmbeddingModel.cs,\nONNXSentenceTransformer.cs, HuggingFaceEmbeddingModel.cs, and\nLocalTransformerEmbedding.cs so all derived classes call the shared protected\nbase implementations.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T00:13:38Z",
    "updated_at": "2025-11-04T00:13:39Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488221110",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488221110"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488221110"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488221110/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 53,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 89,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 89,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488221118",
    "pull_request_review_id": 3413430414,
    "id": 2488221118,
    "node_id": "PRRC_kwDOKSXUF86UTz2-",
    "diff_hunk": "@@ -0,0 +1,123 @@\n+using System;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System.Collections.Generic;\n+using System.Linq;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.RerankingStrategies\n+{\n+    /// <summary>\n+    /// Reciprocal Rank Fusion for combining multiple ranking lists.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class ReciprocalRankFusion<T> : RerankingStrategyBase<T>\n+    {\n+        private readonly INumericOperations<T> _numOps;",
    "path": "src/RetrievalAugmentedGeneration/RerankingStrategies/ReciprocalRankFusion.cs",
    "commit_id": "0990756bfaebc406430e06b89f59a191b1f29d3a",
    "original_commit_id": "77a1bbb5864b8564f484f102b991a8e5ef80d32f",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_≡ƒ¢á∩╕Å Refactor suggestion_ | _≡ƒƒá Major_\n\n**Remove redundant field.**\n\nThe `_numOps` field duplicates the `NumOps` field inherited from `RerankingStrategyBase<T>`. Remove this field and use the inherited `NumOps` directly.\n\n\n```diff\n-        private readonly INumericOperations<T> _numOps;\n         private readonly int _k;\n```\n\nThen update all references from `_numOps` to `NumOps` (lines 25, 47, 51, 55, 96, 100, 104).\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/RerankingStrategies/ReciprocalRankFusion.cs\naround line 15, remove the redundant private field declaration \"private readonly\nINumericOperations<T> _numOps;\" and replace all usages of \"_numOps\" with the\ninherited \"NumOps\" (occurrences at lines ~25, 47, 51, 55, 96, 100, 104). Ensure\nconstructors/initializers no longer assign to _numOps and that only the base\nclass NumOps is referenced throughout the file.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits c0784b0 to a13deec",
    "created_at": "2025-11-04T00:13:38Z",
    "updated_at": "2025-11-04T13:05:12Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488221118",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488221118"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488221118"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488221118/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": null,
    "original_line": 15,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 15,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488221132",
    "pull_request_review_id": 3413430414,
    "id": 2488221132,
    "node_id": "PRRC_kwDOKSXUF86UTz3M",
    "diff_hunk": "@@ -0,0 +1,200 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\n+{\n+    /// <summary>\n+    /// BM25 (Best Matching 25) retrieval algorithm for sparse keyword-based search.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class BM25Retriever<T> : RetrieverBase<T>\n+    {\n+        private readonly IDocumentStore<T> _documentStore;\n+        private readonly T _k1;\n+        private readonly T _b;\n+        private readonly Dictionary<string, Dictionary<string, T>> _termFrequencies;\n+        private readonly Dictionary<string, T> _documentLengths;\n+        private readonly Dictionary<string, int> _documentFrequencies;\n+        private int _totalDocuments;\n+        private T _avgDocLength;\n+\n+        public BM25Retriever(IDocumentStore<T> documentStore, int defaultTopK = 5, double k1 = 1.5, double b = 0.75) \n+            : base(defaultTopK)\n+        {\n+            if (documentStore == null)\n+                throw new ArgumentNullException(nameof(documentStore));\n+\n+            _documentStore = documentStore;\n+            _k1 = NumOps.FromDouble(k1);\n+            _b = NumOps.FromDouble(b);\n+            _termFrequencies = new Dictionary<string, Dictionary<string, T>>();\n+            _documentLengths = new Dictionary<string, T>();\n+            _documentFrequencies = new Dictionary<string, int>();\n+            _avgDocLength = NumOps.Zero;\n+            _totalDocuments = 0;\n+        }\n+\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\n+        {\n+            var queryTerms = Tokenize(query);\n+            var scores = new Dictionary<string, T>();\n+\n+            var candidates = _documentStore.GetSimilar(\n+                new AiDotNet.LinearAlgebra.Vector<T>(new T[Math.Max(1, _documentStore.VectorDimension)]),\n+                _documentStore.DocumentCount\n+            );\n+\n+            var candidatesList = candidates.ToList();\n+            BuildCorpusStatistics(candidatesList);\n+\n+            foreach (var doc in candidatesList)\n+            {\n+                if (!MatchesFilters(doc, metadataFilters))\n+                    continue;\n+\n+                var score = NumOps.Zero;\n+                \n+                foreach (var term in queryTerms)\n+                {\n+                    var termScore = CalculateBM25Term(doc.Id, term);\n+                    score = NumOps.Add(score, termScore);\n+                }\n+\n+                scores[doc.Id] = score;\n+            }\n+\n+            var results = scores\n+                .OrderByDescending(kv => kv.Value)\n+                .Take(topK)\n+                .Select(kv =>\n+                {\n+                    var doc = candidates.FirstOrDefault(d => d.Id == kv.Key);\n+                    if (doc != null)\n+                    {\n+                        doc.RelevanceScore = kv.Value;\n+                        doc.HasRelevanceScore = true;\n+                    }\n+                    return doc;\n+                })\n+                .Where(d => d != null)\n+                .Cast<Document<T>>();\n+\n+            return results;\n+        }\n+\n+        private T CalculateBM25Term(string docId, string term)\n+        {\n+            if (!_termFrequencies.ContainsKey(docId) || !_termFrequencies[docId].ContainsKey(term))\n+                return NumOps.Zero;\n+\n+            var tf = _termFrequencies[docId][term];\n+            var docLength = _documentLengths.ContainsKey(docId) ? _documentLengths[docId] : NumOps.Zero;\n+\n+            var idf = CalculateIDF(term);\n+\n+            var numerator = NumOps.Multiply(tf, NumOps.Add(_k1, NumOps.One));\n+            var lengthNorm = NumOps.Add(\n+                NumOps.Subtract(NumOps.One, _b),\n+                NumOps.Multiply(_b, NumOps.Divide(docLength, _avgDocLength)));\n+            var denominator = NumOps.Add(tf, NumOps.Multiply(_k1, lengthNorm));\n+\n+            return NumOps.Multiply(idf, NumOps.Divide(numerator, denominator));\n+        }\n+\n+        private T CalculateIDF(string term)\n+        {\n+            if (!_documentFrequencies.ContainsKey(term) || _totalDocuments == 0)\n+                return NumOps.Zero;\n+\n+            var df = _documentFrequencies[term];\n+            if (df == 0)\n+                return NumOps.Zero;\n+\n+            var numerator = NumOps.FromDouble(_totalDocuments - df + 0.5);\n+            var denominator = NumOps.FromDouble(df + 0.5);\n+            var ratio = NumOps.Divide(numerator, denominator);\n+            \n+            return StatisticsHelper.Log(ratio, NumOps);\n+        }",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/BM25Retriever.cs",
    "commit_id": "cfcf88f49b6a6c0d9adeac0efe0b2a362d22017b",
    "original_commit_id": "77a1bbb5864b8564f484f102b991a8e5ef80d32f",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Fix generic StatisticsHelper usage to unblock the build**\n\n`StatisticsHelper` is a generic type; calling `StatisticsHelper.Log(...)` triggers CS0305 (ΓÇ£requires 1 type argumentΓÇ¥) and breaks the CI build. Use the generic form so the compiler can resolve the overload.  \n\n\nApply this diff:\n\n```diff\n-            return StatisticsHelper.Log(ratio, NumOps);\n+            return StatisticsHelper<T>.Log(ratio, NumOps);\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n            return StatisticsHelper<T>.Log(ratio, NumOps);\n        }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒº░ Tools</summary>\n\n<details>\n<summary>≡ƒ¬¢ GitHub Actions: Build</summary>\n\n[error] 121-121: dotnet build --no-restore --configuration Debug failed: CS0305: Using the generic type 'StatisticsHelper<T>' requires 1 type argument.\n\n</details>\n<details>\n<summary>≡ƒ¬¢ GitHub Check: Build All Frameworks</summary>\n\n[failure] 121-121: \nUsing the generic type 'StatisticsHelper<T>' requires 1 type arguments\n\n---\n\n[failure] 121-121: \nUsing the generic type 'StatisticsHelper<T>' requires 1 type arguments\n\n</details>\n<details>\n<summary>≡ƒ¬¢ GitHub Check: Publish Size Analysis</summary>\n\n[failure] 121-121: \nUsing the generic type 'StatisticsHelper<T>' requires 1 type arguments\n\n</details>\n\n</details>\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Retrievers/BM25Retriever.cs around lines\n121-122, the call to StatisticsHelper.Log(...) fails because StatisticsHelper is\ngeneric; replace the non-generic call with the generic form using the actual\nnumeric type of ratio (e.g., StatisticsHelper<double>.Log(ratio, NumOps)). If\nratio is a different numeric type, use that type argument instead so the\ncompiler can resolve the overload.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T00:13:38Z",
    "updated_at": "2025-11-04T00:13:39Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488221132",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488221132"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488221132"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488221132/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 121,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 122,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 122,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488221139",
    "pull_request_review_id": 3413430414,
    "id": 2488221139,
    "node_id": "PRRC_kwDOKSXUF86UTz3T",
    "diff_hunk": "@@ -0,0 +1,200 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\n+{\n+    /// <summary>\n+    /// BM25 (Best Matching 25) retrieval algorithm for sparse keyword-based search.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class BM25Retriever<T> : RetrieverBase<T>\n+    {\n+        private readonly IDocumentStore<T> _documentStore;\n+        private readonly T _k1;\n+        private readonly T _b;\n+        private readonly Dictionary<string, Dictionary<string, T>> _termFrequencies;\n+        private readonly Dictionary<string, T> _documentLengths;\n+        private readonly Dictionary<string, int> _documentFrequencies;\n+        private int _totalDocuments;\n+        private T _avgDocLength;\n+\n+        public BM25Retriever(IDocumentStore<T> documentStore, int defaultTopK = 5, double k1 = 1.5, double b = 0.75) \n+            : base(defaultTopK)\n+        {\n+            if (documentStore == null)\n+                throw new ArgumentNullException(nameof(documentStore));\n+\n+            _documentStore = documentStore;\n+            _k1 = NumOps.FromDouble(k1);\n+            _b = NumOps.FromDouble(b);\n+            _termFrequencies = new Dictionary<string, Dictionary<string, T>>();\n+            _documentLengths = new Dictionary<string, T>();\n+            _documentFrequencies = new Dictionary<string, int>();\n+            _avgDocLength = NumOps.Zero;\n+            _totalDocuments = 0;\n+        }\n+\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\n+        {\n+            var queryTerms = Tokenize(query);\n+            var scores = new Dictionary<string, T>();\n+\n+            var candidates = _documentStore.GetSimilar(\n+                new AiDotNet.LinearAlgebra.Vector<T>(new T[Math.Max(1, _documentStore.VectorDimension)]),\n+                _documentStore.DocumentCount\n+            );\n+\n+            var candidatesList = candidates.ToList();\n+            BuildCorpusStatistics(candidatesList);\n+\n+            foreach (var doc in candidatesList)\n+            {\n+                if (!MatchesFilters(doc, metadataFilters))\n+                    continue;\n+\n+                var score = NumOps.Zero;\n+                \n+                foreach (var term in queryTerms)\n+                {\n+                    var termScore = CalculateBM25Term(doc.Id, term);\n+                    score = NumOps.Add(score, termScore);\n+                }\n+\n+                scores[doc.Id] = score;\n+            }\n+\n+            var results = scores\n+                .OrderByDescending(kv => kv.Value)\n+                .Take(topK)\n+                .Select(kv =>\n+                {\n+                    var doc = candidates.FirstOrDefault(d => d.Id == kv.Key);\n+                    if (doc != null)\n+                    {\n+                        doc.RelevanceScore = kv.Value;\n+                        doc.HasRelevanceScore = true;\n+                    }\n+                    return doc;\n+                })\n+                .Where(d => d != null)\n+                .Cast<Document<T>>();\n+\n+            return results;\n+        }\n+\n+        private T CalculateBM25Term(string docId, string term)\n+        {\n+            if (!_termFrequencies.ContainsKey(docId) || !_termFrequencies[docId].ContainsKey(term))\n+                return NumOps.Zero;\n+\n+            var tf = _termFrequencies[docId][term];\n+            var docLength = _documentLengths.ContainsKey(docId) ? _documentLengths[docId] : NumOps.Zero;\n+\n+            var idf = CalculateIDF(term);\n+\n+            var numerator = NumOps.Multiply(tf, NumOps.Add(_k1, NumOps.One));\n+            var lengthNorm = NumOps.Add(\n+                NumOps.Subtract(NumOps.One, _b),\n+                NumOps.Multiply(_b, NumOps.Divide(docLength, _avgDocLength)));\n+            var denominator = NumOps.Add(tf, NumOps.Multiply(_k1, lengthNorm));\n+\n+            return NumOps.Multiply(idf, NumOps.Divide(numerator, denominator));\n+        }\n+\n+        private T CalculateIDF(string term)\n+        {\n+            if (!_documentFrequencies.ContainsKey(term) || _totalDocuments == 0)\n+                return NumOps.Zero;\n+\n+            var df = _documentFrequencies[term];\n+            if (df == 0)\n+                return NumOps.Zero;\n+\n+            var numerator = NumOps.FromDouble(_totalDocuments - df + 0.5);\n+            var denominator = NumOps.FromDouble(df + 0.5);\n+            var ratio = NumOps.Divide(numerator, denominator);\n+            \n+            return StatisticsHelper.Log(ratio, NumOps);\n+        }\n+\n+        private void BuildCorpusStatistics(List<Document<T>> documents)\n+        {\n+            if (documents == null || documents.Count == 0)\n+            {\n+                _avgDocLength = NumOps.One;\n+                _totalDocuments = 0;\n+                return;\n+            }\n+\n+            _termFrequencies.Clear();\n+            _documentLengths.Clear();\n+            _documentFrequencies.Clear();\n+            _totalDocuments = documents.Count;\n+\n+            var totalLength = NumOps.Zero;\n+\n+            foreach (var doc in documents)\n+            {\n+                var terms = Tokenize(doc.Content);\n+                var termCounts = new Dictionary<string, T>();\n+                \n+                foreach (var term in terms)\n+                {\n+                    if (termCounts.ContainsKey(term))\n+                        termCounts[term] = NumOps.Add(termCounts[term], NumOps.One);\n+                    else\n+                        termCounts[term] = NumOps.One;\n+\n+                    if (!_documentFrequencies.ContainsKey(term))\n+                        _documentFrequencies[term] = 0;\n+                }\n+\n+                foreach (var term in termCounts.Keys.Distinct())\n+                {\n+                    _documentFrequencies[term]++;\n+                }\n+\n+                _termFrequencies[doc.Id] = termCounts;\n+                var docLength = NumOps.FromDouble(terms.Count);\n+                _documentLengths[doc.Id] = docLength;\n+                totalLength = NumOps.Add(totalLength, docLength);\n+            }\n+\n+            _avgDocLength = NumOps.Divide(totalLength, NumOps.FromDouble(_totalDocuments));\n+            \n+            if (NumOps.Compare(_avgDocLength, NumOps.Zero) <= 0)\n+                _avgDocLength = NumOps.One;",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/BM25Retriever.cs",
    "commit_id": "cfcf88f49b6a6c0d9adeac0efe0b2a362d22017b",
    "original_commit_id": "77a1bbb5864b8564f484f102b991a8e5ef80d32f",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Replace unavailable `NumOps.Compare` call**\n\n`INumericOperations<T>` has no `Compare` method, so this line fails compilation (CS1061). Use the existing relational helpers to detect a non-positive average length instead of calling the missing member.  \n\n\nApply this diff:\n\n```diff\n-            if (NumOps.Compare(_avgDocLength, NumOps.Zero) <= 0)\n+            var isNonPositiveAvg = NumOps.LessThan(_avgDocLength, NumOps.Zero) ||\n+                                   EqualityComparer<T>.Default.Equals(_avgDocLength, NumOps.Zero);\n+            if (isNonPositiveAvg)\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n            var isNonPositiveAvg = NumOps.LessThan(_avgDocLength, NumOps.Zero) ||\n                                   EqualityComparer<T>.Default.Equals(_avgDocLength, NumOps.Zero);\n            if (isNonPositiveAvg)\n                _avgDocLength = NumOps.One;\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒº░ Tools</summary>\n\n<details>\n<summary>≡ƒ¬¢ GitHub Check: Build All Frameworks</summary>\n\n[failure] 169-169: \n'INumericOperations<T>' does not contain a definition for 'Compare' and no accessible extension method 'Compare' accepting a first argument of type 'INumericOperations<T>' could be found (are you missing a using directive or an assembly reference?)\n\n---\n\n[failure] 169-169: \n'INumericOperations<T>' does not contain a definition for 'Compare' and no accessible extension method 'Compare' accepting a first argument of type 'INumericOperations<T>' could be found (are you missing a using directive or an assembly reference?)\n\n</details>\n<details>\n<summary>≡ƒ¬¢ GitHub Check: Publish Size Analysis</summary>\n\n[failure] 169-169: \n'INumericOperations<T>' does not contain a definition for 'Compare' and no accessible extension method 'Compare' accepting a first argument of type 'INumericOperations<T>' could be found (are you missing a using directive or an assembly reference?)\n\n</details>\n\n</details>\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Retrievers/BM25Retriever.cs around lines\n169-170, the code calls a non-existent NumOps.Compare method causing a compile\nerror; replace that call with the numeric relational helper provided by\nINumericOperations<T> (e.g. use NumOps.LessThanOrEqual or the library's\nequivalent) to test for non-positive _avgDocLength and set it to NumOps.One when\n<= zero.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 1d6eceb to fb037f7",
    "created_at": "2025-11-04T00:13:38Z",
    "updated_at": "2025-11-04T02:31:35Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488221139",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488221139"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488221139"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488221139/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 169,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 170,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 170,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488221142",
    "pull_request_review_id": 3413430414,
    "id": 2488221142,
    "node_id": "PRRC_kwDOKSXUF86UTz3W",
    "diff_hunk": "@@ -0,0 +1,170 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\n+{\n+    /// <summary>\n+    /// TF-IDF (Term Frequency-Inverse Document Frequency) retrieval strategy.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class TFIDFRetriever<T> : RetrieverBase<T>\n+    {\n+        private readonly IDocumentStore<T> _documentStore;\n+        private readonly Dictionary<string, Dictionary<string, T>> _tfidf;\n+        private readonly Dictionary<string, T> _idf;\n+\n+        public TFIDFRetriever(IDocumentStore<T> documentStore, int defaultTopK = 5) : base(defaultTopK)\n+        {\n+            if (documentStore == null)\n+                throw new ArgumentNullException(nameof(documentStore));\n+                \n+            _documentStore = documentStore;\n+            _tfidf = new Dictionary<string, Dictionary<string, T>>();\n+            _idf = new Dictionary<string, T>();\n+        }\n+\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\n+        {\n+            var queryTerms = Tokenize(query);\n+            var scores = new Dictionary<string, T>();\n+\n+            var candidates = _documentStore.GetSimilar(\n+                new AiDotNet.LinearAlgebra.Vector<T>(new T[_documentStore.VectorDimension]), \n+                _documentStore.DocumentCount\n+            );\n+\n+            var candidatesList = candidates.ToList();\n+            BuildTFIDFStatistics(candidatesList);\n+\n+            foreach (var doc in candidatesList)\n+            {\n+                if (!MatchesFilters(doc, metadataFilters))\n+                    continue;\n+\n+                var score = NumOps.Zero;\n+\n+                if (_tfidf.ContainsKey(doc.Id))\n+                {\n+                    var docTfidf = _tfidf[doc.Id];\n+                    foreach (var term in queryTerms)\n+                    {\n+                        if (docTfidf.ContainsKey(term))\n+                        {\n+                            score = NumOps.Add(score, docTfidf[term]);\n+                        }\n+                    }\n+                }\n+\n+                scores[doc.Id] = score;\n+            }\n+\n+            var results = scores\n+                .OrderByDescending(kv => kv.Value)\n+                .Take(topK)\n+                .Select(kv =>\n+                {\n+                    var doc = candidates.FirstOrDefault(d => d.Id == kv.Key);\n+                    if (doc != null)\n+                    {\n+                        doc.RelevanceScore = kv.Value;\n+                        doc.HasRelevanceScore = true;\n+                    }\n+                    return doc;\n+                })\n+                .Where(d => d != null)\n+                .Cast<Document<T>>();\n+\n+            return results;\n+        }\n+\n+        private List<string> Tokenize(string text)\n+        {\n+            if (string.IsNullOrWhiteSpace(text))\n+                return new List<string>();\n+\n+            return text.ToLowerInvariant()\n+                .Split(new[] { ' ', '\\t', '\\n', '\\r', ',', '.', '!', '?' }, StringSplitOptions.RemoveEmptyEntries)\n+                .ToList();\n+        }\n+\n+        private void BuildTFIDFStatistics(List<Document<T>> documents)\n+        {\n+            if (documents == null || documents.Count == 0)\n+                return;\n+\n+            _tfidf.Clear();\n+            _idf.Clear();\n+\n+            var termDocFreq = new Dictionary<string, int>();\n+            var docTermFreq = new Dictionary<string, Dictionary<string, int>>();\n+\n+            foreach (var doc in documents)\n+            {\n+                var terms = Tokenize(doc.Content);\n+                var termCounts = new Dictionary<string, int>();\n+                \n+                foreach (var term in terms)\n+                {\n+                    if (termCounts.ContainsKey(term))\n+                        termCounts[term]++;\n+                    else\n+                        termCounts[term] = 1;\n+                }\n+\n+                docTermFreq[doc.Id] = termCounts;\n+\n+                foreach (var term in termCounts.Keys)\n+                {\n+                    if (termDocFreq.ContainsKey(term))\n+                        termDocFreq[term]++;\n+                    else\n+                        termDocFreq[term] = 1;\n+                }\n+            }\n+\n+            foreach (var term in termDocFreq.Keys)\n+            {\n+                var df = termDocFreq[term];\n+                var idf = NumOps.FromDouble(Math.Log((double)documents.Count / (double)df));\n+                _idf[term] = idf;\n+            }\n+\n+            foreach (var doc in documents)\n+            {\n+                var termTfidf = new Dictionary<string, T>();\n+                var termCounts = docTermFreq[doc.Id];\n+                var maxFreq = termCounts.Values.Max();\n+\n+                foreach (var termCount in termCounts)\n+                {\n+                    var tf = NumOps.FromDouble((double)termCount.Value / (double)maxFreq);\n+                    var tfidf = NumOps.Multiply(tf, _idf[termCount.Key]);\n+                    termTfidf[termCount.Key] = tfidf;\n+                }",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/TFIDFRetriever.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "77a1bbb5864b8564f484f102b991a8e5ef80d32f",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Guard against empty term frequency lists**\n\nWhen a documentΓÇÖs content tokenizes to zero terms, `termCounts.Values.Max()` throws `InvalidOperationException`, breaking retrieval at runtime. Handle the empty case before calling `Max()` by skipping the doc or short-circuiting the TF-IDF computation.  \n\n\nApply this diff:\n\n```diff\n-                var termTfidf = new Dictionary<string, T>();\n-                var termCounts = docTermFreq[doc.Id];\n-                var maxFreq = termCounts.Values.Max();\n+                var termTfidf = new Dictionary<string, T>();\n+                var termCounts = docTermFreq[doc.Id];\n+                if (termCounts.Count == 0)\n+                {\n+                    _tfidf[doc.Id] = termTfidf;\n+                    continue;\n+                }\n+                var maxFreq = termCounts.Values.Max();\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Retrievers/TFIDFRetriever.cs around lines\n140 to 147, guard against calling termCounts.Values.Max() when termCounts is\nempty: check if termCounts is null or termCounts.Count == 0 (or\n!termCounts.Values.Any()) and short-circuit (e.g., continue to next document or\nset termTfidf empty) before calling Max(); otherwise compute maxFreq and TF-IDF\nas before. Ensure the early-return/continue preserves downstream behavior (no\nentries added for empty-token docs) and avoids the InvalidOperationException.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T00:13:38Z",
    "updated_at": "2025-11-04T00:13:39Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488221142",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488221142"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488221142"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488221142/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 131,
    "original_start_line": 140,
    "start_side": "RIGHT",
    "line": 137,
    "original_line": 147,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 147,
    "position": 137,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261263",
    "pull_request_review_id": 3413480474,
    "id": 2488261263,
    "node_id": "PRRC_kwDOKSXUF86UT9qP",
    "diff_hunk": "@@ -0,0 +1,60 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using AiDotNet.RetrievalAugmentedGeneration.Retrievers;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.AdvancedPatterns;\n+\n+/// <summary>\n+/// Chain-of-Thought retriever that generates reasoning steps before retrieving documents.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// Generates a chain of thought reasoning process based on the query, then uses\n+/// that reasoning to inform retrieval, leading to more targeted and relevant results.\n+/// </remarks>\n+public class ChainOfThoughtRetriever<T>\n+{\n+    private readonly INumericOperations<T> _numericOperations;\n+    private readonly string _llmEndpoint;\n+    private readonly string _llmApiKey;\n+    private readonly RetrieverBase<T> _baseRetriever;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"ChainOfThoughtRetriever{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"llmEndpoint\">The LLM API endpoint.</param>\n+    /// <param name=\"llmApiKey\">The API key for the LLM service.</param>\n+    /// <param name=\"baseRetriever\">The underlying retriever to use.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public ChainOfThoughtRetriever(\n+        string llmEndpoint,\n+        string llmApiKey,\n+        RetrieverBase<T> baseRetriever,\n+        INumericOperations<T> numericOperations)\n+    {\n+        _llmEndpoint = llmEndpoint ?? throw new ArgumentNullException(nameof(llmEndpoint));\n+        _llmApiKey = llmApiKey ?? throw new ArgumentNullException(nameof(llmApiKey));\n+        _baseRetriever = baseRetriever ?? throw new ArgumentNullException(nameof(baseRetriever));\n+        _numericOperations = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\n+    }\n+\n+    /// <summary>\n+    /// Retrieves documents using chain-of-thought reasoning.\n+    /// </summary>\n+    public IEnumerable<Document<T>> Retrieve(string query, int topK)\n+    {\n+        if (string.IsNullOrWhiteSpace(query))\n+            throw new ArgumentException(\"Query cannot be null or whitespace\", nameof(query));\n+\n+        if (topK <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(topK), \"topK must be positive\");\n+\n+        // TODO: Implement chain-of-thought retrieval\n+        // 1. Send query to LLM to generate reasoning steps\n+        // 2. Extract key concepts and sub-questions from reasoning\n+        // 3. Use base retriever to fetch documents for each concept\n+        // 4. Combine and deduplicate results\n+        // 5. Return top-K documents\n+        throw new NotImplementedException(\"Chain-of-thought retrieval requires LLM integration\");\n+    }",
    "path": "src/RetrievalAugmentedGeneration/AdvancedPatterns/ChainOfThoughtRetriever.cs",
    "commit_id": "fb037f7036c63184b2417768a5e0933c503d9233",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Finish chain-of-thought retrieval logic.**\n\n`Retrieve` currently throws `NotImplementedException`, so this ΓÇ£concreteΓÇ¥ retriever cannot be used without crashing. Please deliver the reasoning+retrieval workflow outlined in the TODO (or defer by making the type non-public) before shipping.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/AdvancedPatterns/ChainOfThoughtRetriever.cs\naround lines 44 to 59, Retrieve currently throws NotImplementedException and\nmust implement the chain-of-thought workflow described in the TODO. Replace the\nthrow with an implementation that: 1) validates inputs (already present), 2)\ncalls the configured LLM client to prompt for step-by-step reasoning for the\nquery and parse the response into a list of concepts/sub-questions, 3) for each\nconcept call the existing base retriever to fetch documents, 4) merge results\ninto a single list, deduplicate by document ID or content fingerprint,\nscore/rank them (e.g., by aggregate relevance), and 5) return the topK\ndocuments; ensure LLM errors are caught and surfaced as meaningful exceptions\nand make the method unit-testable by extracting LLM and retriever calls into\ninjectable interfaces if not already.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T00:33:59Z",
    "updated_at": "2025-11-04T00:34:05Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261263",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261263"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261263"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261263/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 139,
    "original_start_line": 44,
    "start_side": "RIGHT",
    "line": 220,
    "original_line": 59,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 59,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261271",
    "pull_request_review_id": 3413480474,
    "id": 2488261271,
    "node_id": "PRRC_kwDOKSXUF86UT9qX",
    "diff_hunk": "@@ -0,0 +1,64 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using AiDotNet.RetrievalAugmentedGeneration.Retrievers;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.AdvancedPatterns;\n+\n+/// <summary>\n+/// FLARE (Forward-Looking Active REtrieval) pattern that actively decides when and what to retrieve.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// During generation, FLARE monitors the model's confidence and actively retrieves additional\n+/// information when uncertainty is detected, enabling dynamic and adaptive retrieval.\n+/// </remarks>\n+public class FLARERetriever<T>\n+{\n+    private readonly INumericOperations<T> _numericOperations;\n+    private readonly string _llmEndpoint;\n+    private readonly string _llmApiKey;\n+    private readonly RetrieverBase<T> _baseRetriever;\n+    private readonly T _uncertaintyThreshold;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"FLARERetriever{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"llmEndpoint\">The LLM API endpoint.</param>\n+    /// <param name=\"llmApiKey\">The API key for the LLM service.</param>\n+    /// <param name=\"baseRetriever\">The underlying retriever to use.</param>\n+    /// <param name=\"uncertaintyThreshold\">Threshold for triggering retrieval.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public FLARERetriever(\n+        string llmEndpoint,\n+        string llmApiKey,\n+        RetrieverBase<T> baseRetriever,\n+        T uncertaintyThreshold,\n+        INumericOperations<T> numericOperations)\n+    {\n+        _llmEndpoint = llmEndpoint ?? throw new ArgumentNullException(nameof(llmEndpoint));\n+        _llmApiKey = llmApiKey ?? throw new ArgumentNullException(nameof(llmApiKey));\n+        _baseRetriever = baseRetriever ?? throw new ArgumentNullException(nameof(baseRetriever));\n+        _uncertaintyThreshold = uncertaintyThreshold;\n+        _numericOperations = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\n+    }\n+\n+    /// <summary>\n+    /// Generates answer with active retrieval based on uncertainty.\n+    /// </summary>\n+    public string GenerateWithActiveRetrieval(string query)\n+    {\n+        if (string.IsNullOrWhiteSpace(query))\n+            throw new ArgumentException(\"Query cannot be null or whitespace\", nameof(query));\n+\n+        // TODO: Implement FLARE\n+        // 1. Start generating answer\n+        // 2. Monitor token-level confidence\n+        // 3. When confidence drops below threshold:\n+        //    a. Identify what information is needed\n+        //    b. Retrieve relevant documents\n+        //    c. Continue generation with new context\n+        // 4. Repeat until answer is complete\n+        // 5. Return final answer\n+        throw new NotImplementedException(\"FLARE requires LLM integration with confidence monitoring\");\n+    }",
    "path": "src/RetrievalAugmentedGeneration/AdvancedPatterns/FLARERetriever.cs",
    "commit_id": "fb037f7036c63184b2417768a5e0933c503d9233",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Implement FLARE retrieval instead of throwing.**\n\n`GenerateWithActiveRetrieval` always throws `NotImplementedException`, so any caller of this supposedly concrete retriever will fail at runtime. Please either ship the actual FLARE control loop (confidence monitoring, dynamic retrieval, final answer) or keep this type internal/abstract until the behavior is complete.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/AdvancedPatterns/FLARERetriever.cs around\nlines 48ΓÇô63, the GenerateWithActiveRetrieval method currently throws\nNotImplementedException; replace this with either a real FLARE control loop or\nmake the type/method abstract/internal. To implement FLARE: validate query,\ninitialize or accept an LLM streaming client, start token-by-token generation\nwhile collecting per-token/confidence scores, detect confidence drop below a\nconfigurable threshold, then pause generation, call the existing retrieval\ncomponent to fetch relevant documents, augment the prompt/context with retrieved\ncontent, resume generation (repeat until EOS or max length), and finally return\nthe composed answer; remove the NotImplementedException and add configuration\nfor confidence threshold, max retrievals, and unit/integration tests.\nAlternatively, if FLARE isnΓÇÖt ready, change the method to abstract or mark the\nclass internal and update callers to avoid runtime failures.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 71fdd3b to 0990756",
    "created_at": "2025-11-04T00:33:59Z",
    "updated_at": "2025-11-04T02:53:38Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261271",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261271"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261271"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261271/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 163,
    "original_start_line": 48,
    "start_side": "RIGHT",
    "line": 288,
    "original_line": 63,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 63,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261281",
    "pull_request_review_id": 3413480474,
    "id": 2488261281,
    "node_id": "PRRC_kwDOKSXUF86UT9qh",
    "diff_hunk": "@@ -0,0 +1,72 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using AiDotNet.RetrievalAugmentedGeneration.Retrievers;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.AdvancedPatterns;\n+\n+/// <summary>\n+/// Self-correcting retriever that iteratively refines answers through critique and re-retrieval.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// Retrieves documents, generates an answer, critiques the answer, and retrieves additional\n+/// documents to address gaps or errors, repeating until a satisfactory answer is achieved.\n+/// </remarks>\n+public class SelfCorrectingRetriever<T>\n+{\n+    private readonly INumericOperations<T> _numericOperations;\n+    private readonly string _llmEndpoint;\n+    private readonly string _llmApiKey;\n+    private readonly RetrieverBase<T> _baseRetriever;\n+    private readonly int _maxIterations;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"SelfCorrectingRetriever{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"llmEndpoint\">The LLM API endpoint.</param>\n+    /// <param name=\"llmApiKey\">The API key for the LLM service.</param>\n+    /// <param name=\"baseRetriever\">The underlying retriever to use.</param>\n+    /// <param name=\"maxIterations\">Maximum number of correction iterations.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public SelfCorrectingRetriever(\n+        string llmEndpoint,\n+        string llmApiKey,\n+        RetrieverBase<T> baseRetriever,\n+        int maxIterations,\n+        INumericOperations<T> numericOperations)\n+    {\n+        _llmEndpoint = llmEndpoint ?? throw new ArgumentNullException(nameof(llmEndpoint));\n+        _llmApiKey = llmApiKey ?? throw new ArgumentNullException(nameof(llmApiKey));\n+        _baseRetriever = baseRetriever ?? throw new ArgumentNullException(nameof(baseRetriever));\n+        \n+        if (maxIterations <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(maxIterations), \"Max iterations must be positive\");\n+            \n+        _maxIterations = maxIterations;\n+        _numericOperations = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\n+    }",
    "path": "src/RetrievalAugmentedGeneration/AdvancedPatterns/SelfCorrectingRetriever.cs",
    "commit_id": "fb037f7036c63184b2417768a5e0933c503d9233",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Use consistent string validation in constructor.**\n\nThe constructor validates `llmEndpoint` and `llmApiKey` with null checks only (lines 38-39), but `RetrieveAndAnswer` uses `IsNullOrWhiteSpace` for the `query` parameter (line 54). Empty or whitespace-only strings would pass constructor validation but likely cause issues when used with the LLM API.\n\nApply this diff to use consistent validation:\n\n```diff\n-        _llmEndpoint = llmEndpoint ?? throw new ArgumentNullException(nameof(llmEndpoint));\n-        _llmApiKey = llmApiKey ?? throw new ArgumentNullException(nameof(llmApiKey));\n+        if (string.IsNullOrWhiteSpace(llmEndpoint))\n+            throw new ArgumentException(\"LLM endpoint cannot be null or whitespace\", nameof(llmEndpoint));\n+        if (string.IsNullOrWhiteSpace(llmApiKey))\n+            throw new ArgumentException(\"API key cannot be null or whitespace\", nameof(llmApiKey));\n+            \n+        _llmEndpoint = llmEndpoint;\n+        _llmApiKey = llmApiKey;\n         _baseRetriever = baseRetriever ?? throw new ArgumentNullException(nameof(baseRetriever));\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    public SelfCorrectingRetriever(\n        string llmEndpoint,\n        string llmApiKey,\n        RetrieverBase<T> baseRetriever,\n        int maxIterations,\n        INumericOperations<T> numericOperations)\n    {\n        if (string.IsNullOrWhiteSpace(llmEndpoint))\n            throw new ArgumentException(\"LLM endpoint cannot be null or whitespace\", nameof(llmEndpoint));\n        if (string.IsNullOrWhiteSpace(llmApiKey))\n            throw new ArgumentException(\"API key cannot be null or whitespace\", nameof(llmApiKey));\n            \n        _llmEndpoint = llmEndpoint;\n        _llmApiKey = llmApiKey;\n        _baseRetriever = baseRetriever ?? throw new ArgumentNullException(nameof(baseRetriever));\n        \n        if (maxIterations <= 0)\n            throw new ArgumentOutOfRangeException(nameof(maxIterations), \"Max iterations must be positive\");\n            \n        _maxIterations = maxIterations;\n        _numericOperations = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/AdvancedPatterns/SelfCorrectingRetriever.cs\naround lines 31 to 47, the constructor currently only performs null checks for\nllmEndpoint and llmApiKey but other code uses IsNullOrWhiteSpace for inputs;\nupdate the constructor to validate llmEndpoint and llmApiKey with\nstring.IsNullOrWhiteSpace and throw an ArgumentException (or\nArgumentNullException with a clear message) when they are null/empty/whitespace,\nleaving the null check for baseRetriever and numericOperations as-is and keeping\nthe maxIterations range check unchanged.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T00:33:59Z",
    "updated_at": "2025-11-04T00:34:05Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261281",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261281"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261281"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261281/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 98,
    "original_start_line": 31,
    "start_side": "RIGHT",
    "line": 110,
    "original_line": 47,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 47,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261290",
    "pull_request_review_id": 3413480474,
    "id": 2488261290,
    "node_id": "PRRC_kwDOKSXUF86UT9qq",
    "diff_hunk": "@@ -0,0 +1,57 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies;\n+\n+/// <summary>\n+/// LLM-powered agentic chunker that decides where to split text based on content meaning.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// Uses an LLM to analyze text and determine optimal split points based on semantic boundaries,\n+/// topic changes, and natural breaks in the content flow.\n+/// </remarks>\n+public class AgenticChunker<T> : ChunkingStrategyBase<T>\n+{\n+    private readonly string _llmEndpoint;\n+    private readonly string _llmApiKey;",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/AgenticChunker.cs",
    "commit_id": "80630e82c71aabc214615014532fd5063336af16",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Security posture gap: API key stored as plain string without secure cleanup.**\n\nStoring the API key as a plain string in memory poses a security risk, as it remains readable until garbage collected and cannot be reliably zeroed. Consider implementing `IDisposable` to securely clear sensitive fields, or use a credential management approach that avoids storing secrets directly in application memory.\n\n\n\nExample secure disposal pattern:\n\n```diff\n-public class AgenticChunker<T> : ChunkingStrategyBase<T>\n+public class AgenticChunker<T> : ChunkingStrategyBase<T>, IDisposable\n {\n     private readonly string _llmEndpoint;\n-    private readonly string _llmApiKey;\n+    private string? _llmApiKey;\n     private readonly int _maxChunkSize;\n+    private bool _disposed;\n+\n+    public void Dispose()\n+    {\n+        if (!_disposed)\n+        {\n+            _llmApiKey = null; // Allow GC; consider SecureString for true zeroing\n+            _disposed = true;\n+        }\n+    }\n```\n\n\nAlso applies to: 27-41\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T00:33:59Z",
    "updated_at": "2025-11-04T00:34:06Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261290",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261290"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261290"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261290/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 16,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 17,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 17,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261295",
    "pull_request_review_id": 3413480474,
    "id": 2488261295,
    "node_id": "PRRC_kwDOKSXUF86UT9qv",
    "diff_hunk": "@@ -0,0 +1,146 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies;\n+\n+/// <summary>\n+/// Splits structured documents based on header tags (H1, H2, H3, etc.).\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// Ideal for Markdown and HTML documents where headers provide natural semantic boundaries.\n+/// Preserves document structure and hierarchy.\n+/// </remarks>\n+public class HeaderBasedTextSplitter<T> : ChunkingStrategyBase<T>\n+{\n+    private readonly int _maxChunkSize;\n+    private readonly bool _combineSmallChunks;\n+    private readonly int _minChunkSize;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"HeaderBasedTextSplitter{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"maxChunkSize\">Maximum size of each chunk in characters.</param>\n+    /// <param name=\"minChunkSize\">Minimum size for chunk combination.</param>\n+    /// <param name=\"combineSmallChunks\">Whether to combine small chunks.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public HeaderBasedTextSplitter(\n+        int maxChunkSize,\n+        int minChunkSize,\n+        bool combineSmallChunks,\n+        INumericOperations<T> numericOperations)\n+        : base(numericOperations)\n+    {\n+        if (maxChunkSize <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(maxChunkSize), \"Max chunk size must be positive\");\n+            \n+        if (minChunkSize < 0)\n+            throw new ArgumentOutOfRangeException(nameof(minChunkSize), \"Min chunk size cannot be negative\");\n+            \n+        _maxChunkSize = maxChunkSize;\n+        _minChunkSize = minChunkSize;\n+        _combineSmallChunks = combineSmallChunks;\n+    }",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/HeaderBasedTextSplitter.cs",
    "commit_id": "7fbd5910239204f8852637c980b53b76349cc626",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Add validation to ensure `minChunkSize` does not exceed `maxChunkSize`.**\n\nThe constructor validates each parameter individually but doesn't verify their relationship. If `minChunkSize > maxChunkSize`, it creates an impossible constraint where no chunk can satisfy both conditions, leading to unpredictable behavior.\n\n\n\nApply this diff to add the validation:\n\n```diff\n     if (minChunkSize < 0)\n         throw new ArgumentOutOfRangeException(nameof(minChunkSize), \"Min chunk size cannot be negative\");\n+        \n+    if (minChunkSize > maxChunkSize)\n+        throw new ArgumentException($\"Min chunk size ({minChunkSize}) cannot exceed max chunk size ({maxChunkSize})\");\n         \n     _maxChunkSize = maxChunkSize;\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    public HeaderBasedTextSplitter(\n        int maxChunkSize,\n        int minChunkSize,\n        bool combineSmallChunks,\n        INumericOperations<T> numericOperations)\n        : base(numericOperations)\n    {\n        if (maxChunkSize <= 0)\n            throw new ArgumentOutOfRangeException(nameof(maxChunkSize), \"Max chunk size must be positive\");\n            \n        if (minChunkSize < 0)\n            throw new ArgumentOutOfRangeException(nameof(minChunkSize), \"Min chunk size cannot be negative\");\n            \n        if (minChunkSize > maxChunkSize)\n            throw new ArgumentException($\"Min chunk size ({minChunkSize}) cannot exceed max chunk size ({maxChunkSize})\");\n            \n        _maxChunkSize = maxChunkSize;\n        _minChunkSize = minChunkSize;\n        _combineSmallChunks = combineSmallChunks;\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn\nsrc/RetrievalAugmentedGeneration/ChunkingStrategies/HeaderBasedTextSplitter.cs\naround lines 27 to 43, the constructor validates maxChunkSize and minChunkSize\nindependently but doesn't check their relationship; add a check after the\nindividual validations that throws an ArgumentException (or\nArgumentOutOfRangeException) when minChunkSize > maxChunkSize with a clear\nmessage (e.g., \"minChunkSize cannot be greater than maxChunkSize\") so callers\nget an explicit error; ensure the new validation runs before assigning fields.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T00:33:59Z",
    "updated_at": "2025-11-04T00:34:06Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261295",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261295"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261295"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261295/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 24,
    "original_start_line": 27,
    "start_side": "RIGHT",
    "line": 39,
    "original_line": 43,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 43,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261299",
    "pull_request_review_id": 3413480474,
    "id": 2488261299,
    "node_id": "PRRC_kwDOKSXUF86UT9qz",
    "diff_hunk": "@@ -0,0 +1,146 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies;\n+\n+/// <summary>\n+/// Splits structured documents based on header tags (H1, H2, H3, etc.).\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// Ideal for Markdown and HTML documents where headers provide natural semantic boundaries.\n+/// Preserves document structure and hierarchy.\n+/// </remarks>\n+public class HeaderBasedTextSplitter<T> : ChunkingStrategyBase<T>\n+{\n+    private readonly int _maxChunkSize;\n+    private readonly bool _combineSmallChunks;\n+    private readonly int _minChunkSize;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"HeaderBasedTextSplitter{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"maxChunkSize\">Maximum size of each chunk in characters.</param>\n+    /// <param name=\"minChunkSize\">Minimum size for chunk combination.</param>\n+    /// <param name=\"combineSmallChunks\">Whether to combine small chunks.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public HeaderBasedTextSplitter(\n+        int maxChunkSize,\n+        int minChunkSize,\n+        bool combineSmallChunks,\n+        INumericOperations<T> numericOperations)\n+        : base(numericOperations)\n+    {\n+        if (maxChunkSize <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(maxChunkSize), \"Max chunk size must be positive\");\n+            \n+        if (minChunkSize < 0)\n+            throw new ArgumentOutOfRangeException(nameof(minChunkSize), \"Min chunk size cannot be negative\");\n+            \n+        _maxChunkSize = maxChunkSize;\n+        _minChunkSize = minChunkSize;\n+        _combineSmallChunks = combineSmallChunks;\n+    }\n+\n+    /// <summary>\n+    /// Splits text based on header hierarchy.\n+    /// </summary>\n+    public override IEnumerable<Document<T>> Chunk(string text)\n+    {\n+        if (string.IsNullOrWhiteSpace(text))\n+            return Enumerable.Empty<Document<T>>();\n+\n+        var chunks = new List<Document<T>>();\n+        var lines = text.Split(new[] { \"\\r\\n\", \"\\r\", \"\\n\" }, StringSplitOptions.None);\n+        var currentChunk = new List<string>();\n+        var currentHeader = string.Empty;\n+\n+        foreach (var line in lines)\n+        {\n+            // Check if line is a header (Markdown ## or HTML <h>)\n+            if (IsHeader(line))\n+            {\n+                // Save current chunk if not empty\n+                if (currentChunk.Count > 0)\n+                {\n+                    var content = string.Join(Environment.NewLine, currentChunk);\n+                    if (content.Length >= _minChunkSize || !_combineSmallChunks)\n+                    {\n+                        chunks.Add(new Document<T>\n+                        {\n+                            Id = Guid.NewGuid().ToString(),\n+                            Content = content,\n+                            Metadata = new Dictionary<string, object>\n+                            {\n+                                [\"header\"] = currentHeader,\n+                                [\"chunkIndex\"] = chunks.Count\n+                            }\n+                        });\n+                        currentChunk.Clear();\n+                    }\n+                }\n+\n+                currentHeader = line.Trim();\n+                currentChunk.Add(line);",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/HeaderBasedTextSplitter.cs",
    "commit_id": "7fbd5910239204f8852637c980b53b76349cc626",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Critical: Small chunks lose their header context when combined.**\n\nWhen `combineSmallChunks` is true and a chunk is smaller than `minChunkSize`, the chunk is neither saved nor cleared (line 67-80 condition is false, so the inner block doesn't execute). The code then updates `currentHeader` to the new header (line 83) and adds the new header line to the existing `currentChunk` (line 84). This means content from the previous section gets merged with the next section under the wrong header, violating semantic boundaries.\n\nFor example:\n```\n## Section A\nSmall content here\n## Section B\nMore content\n```\n\nIf \"Small content here\" is below `minChunkSize`, it will be merged with Section B content but labeled with \"## Section B\" header in metadata, losing its association with Section A.\n\n\n\nApply this diff to fix the semantic boundary issue:\n\n```diff\n             // Check if line is a header (Markdown ## or HTML <h>)\n             if (IsHeader(line))\n             {\n                 // Save current chunk if not empty\n                 if (currentChunk.Count > 0)\n                 {\n                     var content = string.Join(Environment.NewLine, currentChunk);\n-                    if (content.Length >= _minChunkSize || !_combineSmallChunks)\n+                    \n+                    // Always save chunks at header boundaries to preserve semantic structure\n+                    // Small chunks can be combined in a post-processing step if needed\n+                    chunks.Add(new Document<T>\n                     {\n-                        chunks.Add(new Document<T>\n-                        {\n-                            Id = Guid.NewGuid().ToString(),\n-                            Content = content,\n-                            Metadata = new Dictionary<string, object>\n-                            {\n-                                [\"header\"] = currentHeader,\n-                                [\"chunkIndex\"] = chunks.Count\n-                            }\n-                        });\n-                        currentChunk.Clear();\n-                    }\n+                        Id = Guid.NewGuid().ToString(),\n+                        Content = content,\n+                        Metadata = new Dictionary<string, object>\n+                        {\n+                            [\"header\"] = currentHeader,\n+                            [\"chunkIndex\"] = chunks.Count,\n+                            [\"isBelowMinSize\"] = content.Length < _minChunkSize\n+                        }\n+                    });\n+                    currentChunk.Clear();\n                 }\n\n                 currentHeader = line.Trim();\n```\n\nNote: If chunk combination is still desired, implement it as a separate post-processing step that can merge adjacent chunks while preserving metadata about their original structure.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn\nsrc/RetrievalAugmentedGeneration/ChunkingStrategies/HeaderBasedTextSplitter.cs\naround lines 64 to 84, when encountering a new header the code currently skips\ncreating a chunk if it's below _minChunkSize and _combineSmallChunks is true,\nthen replaces currentHeader and appends the new header to the same currentChunk\nwhich loses the original header metadata; always flush the existing currentChunk\nas a chunk tied to currentHeader before updating currentHeader (i.e., create and\nadd a Document with the currentHeader and content even if itΓÇÖs smaller than\n_minChunkSize), clear currentChunk, then set currentHeader = line.Trim() and add\nthe new header line; if you still want to combine small adjacent chunks,\nimplement that as an independent post-processing pass that merges neighboring\nchunks while preserving an array/list of original headers/indices in metadata\nrather than overwriting header information.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 15063ed to 8468360",
    "created_at": "2025-11-04T00:33:59Z",
    "updated_at": "2025-11-04T13:34:39Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261299",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261299"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261299"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261299/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 60,
    "original_start_line": 64,
    "start_side": "RIGHT",
    "line": 68,
    "original_line": 84,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 84,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261305",
    "pull_request_review_id": 3413480474,
    "id": 2488261305,
    "node_id": "PRRC_kwDOKSXUF86UT9q5",
    "diff_hunk": "@@ -0,0 +1,146 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies;\n+\n+/// <summary>\n+/// Splits structured documents based on header tags (H1, H2, H3, etc.).\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// Ideal for Markdown and HTML documents where headers provide natural semantic boundaries.\n+/// Preserves document structure and hierarchy.\n+/// </remarks>\n+public class HeaderBasedTextSplitter<T> : ChunkingStrategyBase<T>\n+{\n+    private readonly int _maxChunkSize;\n+    private readonly bool _combineSmallChunks;\n+    private readonly int _minChunkSize;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"HeaderBasedTextSplitter{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"maxChunkSize\">Maximum size of each chunk in characters.</param>\n+    /// <param name=\"minChunkSize\">Minimum size for chunk combination.</param>\n+    /// <param name=\"combineSmallChunks\">Whether to combine small chunks.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public HeaderBasedTextSplitter(\n+        int maxChunkSize,\n+        int minChunkSize,\n+        bool combineSmallChunks,\n+        INumericOperations<T> numericOperations)\n+        : base(numericOperations)\n+    {\n+        if (maxChunkSize <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(maxChunkSize), \"Max chunk size must be positive\");\n+            \n+        if (minChunkSize < 0)\n+            throw new ArgumentOutOfRangeException(nameof(minChunkSize), \"Min chunk size cannot be negative\");\n+            \n+        _maxChunkSize = maxChunkSize;\n+        _minChunkSize = minChunkSize;\n+        _combineSmallChunks = combineSmallChunks;\n+    }\n+\n+    /// <summary>\n+    /// Splits text based on header hierarchy.\n+    /// </summary>\n+    public override IEnumerable<Document<T>> Chunk(string text)\n+    {\n+        if (string.IsNullOrWhiteSpace(text))\n+            return Enumerable.Empty<Document<T>>();\n+\n+        var chunks = new List<Document<T>>();\n+        var lines = text.Split(new[] { \"\\r\\n\", \"\\r\", \"\\n\" }, StringSplitOptions.None);\n+        var currentChunk = new List<string>();\n+        var currentHeader = string.Empty;\n+\n+        foreach (var line in lines)\n+        {\n+            // Check if line is a header (Markdown ## or HTML <h>)\n+            if (IsHeader(line))\n+            {\n+                // Save current chunk if not empty\n+                if (currentChunk.Count > 0)\n+                {\n+                    var content = string.Join(Environment.NewLine, currentChunk);\n+                    if (content.Length >= _minChunkSize || !_combineSmallChunks)\n+                    {\n+                        chunks.Add(new Document<T>\n+                        {\n+                            Id = Guid.NewGuid().ToString(),\n+                            Content = content,\n+                            Metadata = new Dictionary<string, object>\n+                            {\n+                                [\"header\"] = currentHeader,\n+                                [\"chunkIndex\"] = chunks.Count\n+                            }\n+                        });\n+                        currentChunk.Clear();\n+                    }\n+                }\n+\n+                currentHeader = line.Trim();\n+                currentChunk.Add(line);\n+            }\n+            else\n+            {\n+                currentChunk.Add(line);\n+\n+                // Split if chunk gets too large\n+                var currentSize = string.Join(Environment.NewLine, currentChunk).Length;\n+                if (currentSize >= _maxChunkSize)\n+                {\n+                    chunks.Add(new Document<T>\n+                    {\n+                        Id = Guid.NewGuid().ToString(),\n+                        Content = string.Join(Environment.NewLine, currentChunk),\n+                        Metadata = new Dictionary<string, object>\n+                        {\n+                            [\"header\"] = currentHeader,\n+                            [\"chunkIndex\"] = chunks.Count\n+                        }\n+                    });\n+                    currentChunk.Clear();\n+                }\n+            }\n+        }\n+\n+        // Add remaining content\n+        if (currentChunk.Count > 0)\n+        {\n+            chunks.Add(new Document<T>\n+            {\n+                Id = Guid.NewGuid().ToString(),\n+                Content = string.Join(Environment.NewLine, currentChunk),\n+                Metadata = new Dictionary<string, object>\n+                {\n+                    [\"header\"] = currentHeader,\n+                    [\"chunkIndex\"] = chunks.Count\n+                }\n+            });\n+        }",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/HeaderBasedTextSplitter.cs",
    "commit_id": "7fbd5910239204f8852637c980b53b76349cc626",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Final chunk handling is inconsistent with header boundary logic.**\n\nThe final chunk is always saved regardless of size (line 110-122), while chunks at header boundaries are conditionally saved based on `minChunkSize` (line 67). This creates inconsistent behavior where a small final chunk is kept but a small intermediate chunk might be combined or lost.\n\nIf the header boundary logic is fixed per the previous comment to always save chunks at boundaries, this inconsistency will be resolved. Otherwise, consider applying the same size logic here.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn\nsrc/RetrievalAugmentedGeneration/ChunkingStrategies/HeaderBasedTextSplitter.cs\naround lines 110 to 122, the final chunk is always emitted regardless of size,\ncausing inconsistent behavior compared to header-boundary chunking which\nconditionally saves based on minChunkSize; update the final-chunk handling to\napply the same size rule (e.g., only add the final chunk if currentChunk.Count\n>= minChunkSize) or, if header-boundary behavior is changed to always save, make\nthe final-chunk logic consistent with that decision; ensure metadata (header and\nchunkIndex) is set the same way as other chunks when added.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T00:34:00Z",
    "updated_at": "2025-11-04T00:34:06Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261305",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261305"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261305"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261305/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 89,
    "original_start_line": 110,
    "start_side": "RIGHT",
    "line": 93,
    "original_line": 122,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 122,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261308",
    "pull_request_review_id": 3413480474,
    "id": 2488261308,
    "node_id": "PRRC_kwDOKSXUF86UT9q8",
    "diff_hunk": "@@ -0,0 +1,194 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies;\n+\n+/// <summary>\n+/// Multi-modal splitter for documents containing both text and images.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// Creates chunks that keep text and related images together, preserving the relationship\n+/// between visual and textual content for better context preservation.\n+/// </remarks>\n+public class MultiModalTextSplitter<T> : ChunkingStrategyBase<T>\n+{\n+    private readonly int _maxChunkSize;\n+    private readonly bool _preserveImageContext;\n+    private readonly int _contextWindowSize;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"MultiModalTextSplitter{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"maxChunkSize\">Maximum size of text portion in each chunk.</param>\n+    /// <param name=\"contextWindowSize\">Number of characters before/after image to include.</param>\n+    /// <param name=\"preserveImageContext\">Whether to keep surrounding text with images.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public MultiModalTextSplitter(\n+        int maxChunkSize,\n+        int contextWindowSize,\n+        bool preserveImageContext,\n+        INumericOperations<T> numericOperations)\n+        : base(numericOperations)\n+    {\n+        if (maxChunkSize <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(maxChunkSize), \"Max chunk size must be positive\");\n+            \n+        if (contextWindowSize < 0)\n+            throw new ArgumentOutOfRangeException(nameof(contextWindowSize), \"Context window size cannot be negative\");\n+            \n+        _maxChunkSize = maxChunkSize;\n+        _contextWindowSize = contextWindowSize;\n+        _preserveImageContext = preserveImageContext;\n+    }\n+\n+    /// <summary>\n+    /// Splits text while preserving text-image relationships.\n+    /// </summary>\n+    public override IEnumerable<Document<T>> Chunk(string text)\n+    {\n+        if (string.IsNullOrWhiteSpace(text))\n+            return Enumerable.Empty<Document<T>>();\n+\n+        var chunks = new List<Document<T>>();\n+        var lines = text.Split(new[] { \"\\r\\n\", \"\\r\", \"\\n\" }, StringSplitOptions.None);\n+        \n+        var currentChunk = new List<string>();\n+        var imageReferences = new List<string>();\n+\n+        for (var i = 0; i < lines.Length; i++)\n+        {\n+            var line = lines[i];\n+\n+            // Detect image references (Markdown ![alt](url) or HTML <img>)\n+            if (IsImageReference(line))\n+            {\n+                if (_preserveImageContext)\n+                {\n+                    // Include context before image\n+                    var contextStart = Math.Max(0, i - (_contextWindowSize / 50)); // Approximate lines\n+                    for (var j = contextStart; j < i; j++)\n+                    {\n+                        if (!currentChunk.Contains(lines[j]))\n+                        {\n+                            currentChunk.Add(lines[j]);\n+                        }\n+                    }\n+                }\n+\n+                imageReferences.Add(ExtractImageReference(line));\n+                currentChunk.Add(line);\n+\n+                if (_preserveImageContext)\n+                {\n+                    // Include context after image\n+                    var contextEnd = Math.Min(lines.Length, i + (_contextWindowSize / 50));\n+                    for (var j = i + 1; j < contextEnd; j++)\n+                    {\n+                        currentChunk.Add(lines[j]);\n+                    }\n+                    i = contextEnd - 1; // Skip ahead\n+                }\n+\n+                // Create chunk with image",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/MultiModalTextSplitter.cs",
    "commit_id": "7fbd5910239204f8852637c980b53b76349cc626",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Prevent infinite loop when context window <ΓÇ»50 chars**  \nThe logic sets `contextEnd = Math.Min(lines.Length, i + (_contextWindowSize / 50))` and then forces `i = contextEnd - 1`. If `_contextWindowSize` is less than 50 (the constructor even permits 0), the integer division yields `0`, making `contextEnd == i` and `i` becomes `i - 1`. After the loopΓÇÖs `i++`, the index returns to the original value, so the same image line is processed again forever. Any multimodal document with a small context window will hang. Please ensure you only rewind `i` when `contextEnd > i`; otherwise leave it unchanged (or explicitly advance) before continuing.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/ChunkingStrategies/MultiModalTextSplitter.cs\naround lines 85 to 93, the calculation contextEnd uses integer division\n(_contextWindowSize / 50) which can be zero and causes i to be set to contextEnd\n- 1 producing an infinite loop; change the logic so you only rewind/assign i\nwhen contextEnd > i (i.e., if contextEnd <= i leave i unchanged or set i = i to\nexplicitly advance via the loop), or ensure a minimum advance (e.g., compute a\nnonzero advance = Math.Max(1, _contextWindowSize / 50) before using it) so the\nloop always progresses when adding context lines.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T00:34:00Z",
    "updated_at": "2025-11-04T00:34:06Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261308",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261308"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261308"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261308/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 85,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 93,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 93,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261314",
    "pull_request_review_id": 3413480474,
    "id": 2488261314,
    "node_id": "PRRC_kwDOKSXUF86UT9rC",
    "diff_hunk": "@@ -0,0 +1,81 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ContextCompression;\n+\n+/// <summary>\n+/// Auto-compressor using a sequence-to-sequence model fine-tuned for document compression.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// Uses a trained seq2seq model to compress documents into shorter, more informative summaries\n+/// while preserving query-relevant information.\n+/// </remarks>\n+public class AutoCompressor<T>\n+{\n+    private readonly INumericOperations<T> _numericOperations;\n+    private readonly string _modelPath;\n+    private readonly int _maxOutputLength;\n+    private readonly T _compressionRatio;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"AutoCompressor{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"modelPath\">Path to the compression model.</param>\n+    /// <param name=\"maxOutputLength\">Maximum length of compressed output.</param>\n+    /// <param name=\"compressionRatio\">Target compression ratio (0-1).</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public AutoCompressor(\n+        string modelPath,\n+        int maxOutputLength,\n+        T compressionRatio,\n+        INumericOperations<T> numericOperations)\n+    {\n+        _modelPath = modelPath ?? throw new ArgumentNullException(nameof(modelPath));\n+        \n+        if (maxOutputLength <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(maxOutputLength), \"Max output length must be positive\");\n+            \n+        _maxOutputLength = maxOutputLength;\n+        _compressionRatio = compressionRatio;\n+        _numericOperations = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\n+    }",
    "path": "src/RetrievalAugmentedGeneration/ContextCompression/AutoCompressor.cs",
    "commit_id": "fb037f7036c63184b2417768a5e0933c503d9233",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Minor issues in constructor: unused field and missing validation.**\n\nTwo minor concerns:\n1. The `_compressionRatio` field is stored but never used in the class methods.\n2. The XML documentation states `compressionRatio` should be a \"Target compression ratio (0-1)\", but there's no validation to ensure it's within this range.\n\nAdd validation for the compression ratio range:\n\n```diff\n public AutoCompressor(\n     string modelPath,\n     int maxOutputLength,\n     T compressionRatio,\n     INumericOperations<T> numericOperations)\n {\n     _modelPath = modelPath ?? throw new ArgumentNullException(nameof(modelPath));\n     \n     if (maxOutputLength <= 0)\n         throw new ArgumentOutOfRangeException(nameof(maxOutputLength), \"Max output length must be positive\");\n         \n+    _numericOperations = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\n+    \n+    if (_numericOperations.LessThan(compressionRatio, _numericOperations.Zero) || \n+        _numericOperations.GreaterThan(compressionRatio, _numericOperations.One))\n+        throw new ArgumentOutOfRangeException(nameof(compressionRatio), \"Compression ratio must be between 0 and 1\");\n+    \n     _maxOutputLength = maxOutputLength;\n     _compressionRatio = compressionRatio;\n-    _numericOperations = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\n }\n```\n\nNote: The `_compressionRatio` field will likely be used once the compression logic is implemented.\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    public AutoCompressor(\n        string modelPath,\n        int maxOutputLength,\n        T compressionRatio,\n        INumericOperations<T> numericOperations)\n    {\n        _modelPath = modelPath ?? throw new ArgumentNullException(nameof(modelPath));\n        \n        if (maxOutputLength <= 0)\n            throw new ArgumentOutOfRangeException(nameof(maxOutputLength), \"Max output length must be positive\");\n            \n        _numericOperations = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\n        \n        if (_numericOperations.LessThan(compressionRatio, _numericOperations.Zero) || \n            _numericOperations.GreaterThan(compressionRatio, _numericOperations.One))\n            throw new ArgumentOutOfRangeException(nameof(compressionRatio), \"Compression ratio must be between 0 and 1\");\n        \n        _maxOutputLength = maxOutputLength;\n        _compressionRatio = compressionRatio;\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/ContextCompression/AutoCompressor.cs around\nlines 28-42, the constructor currently stores _compressionRatio but doesn't\nvalidate it; add an argument check that ensures compressionRatio is in a valid\nrange (e.g., > 0 and <= 1) and throw an ArgumentOutOfRangeException with a clear\nmessage if it isn't, and keep the existing null checks; you can also suppress or\nleave as-is the unused-field warning for _compressionRatio until compression\nlogic is implemented.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T00:34:00Z",
    "updated_at": "2025-11-04T00:34:06Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261314",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261314"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261314"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261314/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 28,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 42,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 42,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261320",
    "pull_request_review_id": 3413480474,
    "id": 2488261320,
    "node_id": "PRRC_kwDOKSXUF86UT9rI",
    "diff_hunk": "@@ -0,0 +1,108 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ContextCompression;\n+\n+/// <summary>\n+/// Selective context compressor that picks the most relevant sentences based on the query.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// Analyzes retrieved documents and selectively extracts only the sentences most relevant\n+/// to the query, reducing context length while preserving important information.\n+/// </remarks>\n+public class SelectiveContextCompressor<T>\n+{\n+    private readonly INumericOperations<T> _numericOperations;\n+    private readonly int _maxSentences;\n+    private readonly T _relevanceThreshold;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"SelectiveContextCompressor{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"maxSentences\">Maximum number of sentences to keep.</param>\n+    /// <param name=\"relevanceThreshold\">Minimum relevance score to keep a sentence.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public SelectiveContextCompressor(\n+        int maxSentences,\n+        T relevanceThreshold,\n+        INumericOperations<T> numericOperations)\n+    {\n+        if (maxSentences <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(maxSentences), \"Max sentences must be positive\");\n+            \n+        _maxSentences = maxSentences;\n+        _relevanceThreshold = relevanceThreshold;\n+        _numericOperations = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\n+    }\n+\n+    /// <summary>\n+    /// Compresses documents by selecting relevant sentences.\n+    /// </summary>\n+    public IEnumerable<Document<T>> Compress(string query, IEnumerable<Document<T>> documents)\n+    {\n+        if (string.IsNullOrWhiteSpace(query))\n+            throw new ArgumentException(\"Query cannot be null or whitespace\", nameof(query));\n+\n+        if (documents == null)\n+            throw new ArgumentNullException(nameof(documents));\n+\n+        var compressed = new List<Document<T>>();\n+\n+        foreach (var doc in documents)\n+        {\n+            var sentences = SplitIntoSentences(doc.Content);\n+            var scoredSentences = new List<(string sentence, T score)>();\n+\n+            foreach (var sentence in sentences)\n+            {\n+                var score = CalculateRelevance(query, sentence);\n+                if (_numericOperations.GreaterThanOrEqual(score, _relevanceThreshold))\n+                {\n+                    scoredSentences.Add((sentence, score));\n+                }\n+            }\n+\n+            var selectedSentences = scoredSentences\n+                .OrderByDescending(s => s.score)\n+                .Take(_maxSentences)\n+                .Select(s => s.sentence);",
    "path": "src/RetrievalAugmentedGeneration/ContextCompression/SelectiveContextCompressor.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Critical: OrderByDescending requires IComparable constraint on T.**\n\nThe `OrderByDescending(s => s.score)` call will fail at runtime if `T` does not implement `IComparable<T>`. Generic type `T` is currently unconstrained, so this will throw an `InvalidOperationException` for non-comparable types.\n\nConsider one of these solutions:\n\n**Solution 1 (Recommended)**: Add an `IComparable<T>` constraint to the class:\n\n```diff\n-public class SelectiveContextCompressor<T>\n+public class SelectiveContextCompressor<T> where T : IComparable<T>\n```\n\n**Solution 2**: Use `INumericOperations<T>` to perform comparisons manually and sort the list yourself:\n\n```diff\n-            var selectedSentences = scoredSentences\n-                .OrderByDescending(s => s.score)\n-                .Take(_maxSentences)\n-                .Select(s => s.sentence);\n+            scoredSentences.Sort((a, b) => _numericOperations.GreaterThan(a.score, b.score) ? -1 : \n+                                            _numericOperations.LessThan(a.score, b.score) ? 1 : 0);\n+            var selectedSentences = scoredSentences\n+                .Take(_maxSentences)\n+                .Select(s => s.sentence);\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn\nsrc/RetrievalAugmentedGeneration/ContextCompression/SelectiveContextCompressor.cs\naround lines 66ΓÇô69, OrderByDescending(s => s.score) can throw at runtime because\nT is unconstrained and may not implement IComparable<T>; fix by adding an\nIComparable<T> constraint to the generic (recommended) so comparisons are valid\nat compile time and adjust the class/method signature accordingly, or if you\ncannot change the generic constraint, replace the LINQ ordering with a\ncomparison that uses Comparer<T>.Default (or an injected IComparer<T>/IComparer)\nto perform safe comparisons before Take(_maxSentences).\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T00:34:00Z",
    "updated_at": "2025-11-04T00:34:06Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261320",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261320"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261320"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261320/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 63,
    "original_start_line": 66,
    "start_side": "RIGHT",
    "line": 66,
    "original_line": 69,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 69,
    "position": 66,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261328",
    "pull_request_review_id": 3413480474,
    "id": 2488261328,
    "node_id": "PRRC_kwDOKSXUF86UT9rQ",
    "diff_hunk": "@@ -0,0 +1,82 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// Azure Cognitive Search document store providing fully managed search capabilities.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Azure Cognitive Search combines full-text search, semantic search, and vector search\n+/// in a fully managed cloud service with enterprise-grade security and compliance.\n+/// </remarks>\n+public class AzureSearchDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly string _serviceName;\n+    private readonly string _indexName;\n+    private readonly string _apiKey;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"AzureSearchDocumentStore{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"serviceName\">The Azure Search service name.</param>\n+    /// <param name=\"indexName\">The name of the index to use.</param>\n+    /// <param name=\"apiKey\">The admin API key for authentication.</param>\n+    /// <param name=\"vectorDimension\">The dimensionality of document vectors.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public AzureSearchDocumentStore(\n+        string serviceName,\n+        string indexName,\n+        string apiKey,\n+        int vectorDimension,\n+        INumericOperations<T> numericOperations)\n+        : base(vectorDimension, numericOperations)\n+    {\n+        _serviceName = serviceName ?? throw new ArgumentNullException(nameof(serviceName));\n+        _indexName = indexName ?? throw new ArgumentNullException(nameof(indexName));\n+        _apiKey = apiKey ?? throw new ArgumentNullException(nameof(apiKey));\n+    }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/AzureSearchDocumentStore.cs",
    "commit_id": "7fbd5910239204f8852637c980b53b76349cc626",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n<details>\n<summary>≡ƒº⌐ Analysis chain</summary>\n\n**Verify vectorDimension validation in base constructor.**\n\nThe constructor validates string parameters but not `vectorDimension`. Ensure the base constructor validates that `vectorDimension > 0`, as negative or zero dimensions would be invalid.\n\n\n\nRun the following script to verify the base constructor's validation:\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Description: Check if DocumentStoreBase constructor validates vectorDimension\n\nast-grep --pattern $'class DocumentStoreBase<$_> {\n  $$$\n  DocumentStoreBase($_, $_) {\n    $$$\n  }\n  $$$\n}'\n```\n\nLength of output: 0\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Find DocumentStoreBase class definition and examine constructor\n\n# First, locate the DocumentStoreBase file\nfd -type f -name \"*DocumentStore*\" | head -20\n\n# Then search for DocumentStoreBase class and constructor with context\nrg -n \"class DocumentStoreBase\" -A 30 --type cs\n```\n\nLength of output: 3500\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Find DocumentStoreBase constructor implementation\n\ncat src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs | head -100 | tail -70\n```\n\nLength of output: 2945\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Search for constructor in DocumentStoreBase\n\nrg -n \"protected.*DocumentStoreBase|public.*DocumentStoreBase\" src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs -A 5\n```\n\nLength of output: 283\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# View entire DocumentStoreBase file to find constructors\n\nwc -l src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs\n\n# Search for any method that looks like constructor (pattern matching)\nrg -n \"^\\s*(protected|public|private)?\\s*DocumentStoreBase\\s*\\(\" src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs\n```\n\nLength of output: 72\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# View the entire DocumentStoreBase file\n\ncat src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs\n```\n\nLength of output: 14172\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# View AzureSearchDocumentStore constructor to verify base() call\n\ncat src/RetrievalAugmentedGeneration/DocumentStores/AzureSearchDocumentStore.cs | head -50\n```\n\nLength of output: 2097\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Search for DocumentStore* files and look for constructors\n\nfd -type f -name \"*DocumentStore*\" -path \"*/DocumentStores/*\"\n\n# Check if there's any protected constructor in DocumentStoreBase\nrg -n \"protected DocumentStoreBase\" src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs\n```\n\nLength of output: 170\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Verify actual AzureSearchDocumentStore constructor\n\nrg -n \"public AzureSearchDocumentStore\" src/RetrievalAugmentedGeneration/DocumentStores/AzureSearchDocumentStore.cs -A 10\n```\n\nLength of output: 536\n\n---\n\n\n\n</details>\n\n**Critical compilation error: base constructor call has no matching constructor.**\n\nThe `base(vectorDimension, numericOperations)` call on line 35 fails because `DocumentStoreBase<T>` has no explicit constructor. The base class only provides an implicit parameterless constructor and defines `VectorDimension` as an abstract property that derived classes must implement.\n\nAdditionally, `AzureSearchDocumentStore<T>` must implement the following abstract members from `DocumentStoreBase<T>`:\n- Properties: `DocumentCount`, `VectorDimension`\n- Methods: `AddCore()`, `GetSimilarCore()`, `GetByIdCore()`, `RemoveCore()`, `Clear()`\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/AzureSearchDocumentStore.cs\naround lines 29ΓÇô40, the constructor currently calls base(vectorDimension,\nnumericOperations) but the base class has no matching constructor and exposes\nabstract members that this class must implement. Remove the base(...) call (use\nthe implicit base constructor), store the incoming vectorDimension and\nnumericOperations into private readonly fields or implement VectorDimension to\nreturn the provided dimension, and implement all abstract members from\nDocumentStoreBase<T>: the properties DocumentCount and VectorDimension, and the\nmethods AddCore(...), GetSimilarCore(...), GetByIdCore(...), RemoveCore(...),\nand Clear() with concrete logic (or throw NotImplementedException if stubbing)\nso the class compiles and fulfills the base contract.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T00:34:00Z",
    "updated_at": "2025-11-04T00:34:06Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261328",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261328"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261328"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261328/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 29,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 40,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 40,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261332",
    "pull_request_review_id": 3413480474,
    "id": 2488261332,
    "node_id": "PRRC_kwDOKSXUF86UT9rU",
    "diff_hunk": "@@ -0,0 +1,82 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// Azure Cognitive Search document store providing fully managed search capabilities.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Azure Cognitive Search combines full-text search, semantic search, and vector search\n+/// in a fully managed cloud service with enterprise-grade security and compliance.\n+/// </remarks>\n+public class AzureSearchDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly string _serviceName;\n+    private readonly string _indexName;\n+    private readonly string _apiKey;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"AzureSearchDocumentStore{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"serviceName\">The Azure Search service name.</param>\n+    /// <param name=\"indexName\">The name of the index to use.</param>\n+    /// <param name=\"apiKey\">The admin API key for authentication.</param>\n+    /// <param name=\"vectorDimension\">The dimensionality of document vectors.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public AzureSearchDocumentStore(\n+        string serviceName,\n+        string indexName,\n+        string apiKey,\n+        int vectorDimension,\n+        INumericOperations<T> numericOperations)\n+        : base(vectorDimension, numericOperations)\n+    {\n+        _serviceName = serviceName ?? throw new ArgumentNullException(nameof(serviceName));\n+        _indexName = indexName ?? throw new ArgumentNullException(nameof(indexName));\n+        _apiKey = apiKey ?? throw new ArgumentNullException(nameof(apiKey));\n+    }\n+\n+    /// <summary>\n+    /// Adds a document to the Azure Search index.\n+    /// </summary>\n+    public override void AddDocument(Document<T> document)\n+    {\n+        if (document == null)\n+            throw new ArgumentNullException(nameof(document));\n+\n+        // TODO: Implement Azure Search indexing via REST API\n+        throw new NotImplementedException(\"Azure Search integration requires HTTP client implementation\");\n+    }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/AzureSearchDocumentStore.cs",
    "commit_id": "7fbd5910239204f8852637c980b53b76349cc626",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Implement AddCore instead of overriding AddDocument.**\n\nThe base class `DocumentStoreBase<T>` establishes a template method pattern where public methods perform validation and call protected abstract `*Core` methods. According to the base class documentation:\n\n- Public method: `Add(VectorDocument<T>)` handles validation\n- Abstract method to implement: `AddCore(VectorDocument<T>)` \n\nThis class appears to override `AddDocument(Document<T>)` directly, which bypasses the base class validation logic and violates the established pattern. Additionally, the parameter type differsΓÇöbase expects `VectorDocument<T>` but this takes `Document<T>`.\n\n\n\nVerify the correct method to override:\n\n```shell\n#!/bin/bash\n# Description: Verify which Add methods exist in DocumentStoreBase\n\nast-grep --pattern $'class DocumentStoreBase<$_> {\n  $$$\n  AddDocument($$$) {\n    $$$\n  }\n  $$$\n}'\n\nast-grep --pattern $'class DocumentStoreBase<$_> {\n  $$$\n  AddCore($$$) {\n    $$$\n  }\n  $$$\n}'\n```\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T00:34:00Z",
    "updated_at": "2025-11-04T00:34:06Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261332",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261332"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261332"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261332/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 45,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 52,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 52,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261334",
    "pull_request_review_id": 3413480474,
    "id": 2488261334,
    "node_id": "PRRC_kwDOKSXUF86UT9rW",
    "diff_hunk": "@@ -0,0 +1,82 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// Azure Cognitive Search document store providing fully managed search capabilities.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Azure Cognitive Search combines full-text search, semantic search, and vector search\n+/// in a fully managed cloud service with enterprise-grade security and compliance.\n+/// </remarks>\n+public class AzureSearchDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly string _serviceName;\n+    private readonly string _indexName;\n+    private readonly string _apiKey;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"AzureSearchDocumentStore{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"serviceName\">The Azure Search service name.</param>\n+    /// <param name=\"indexName\">The name of the index to use.</param>\n+    /// <param name=\"apiKey\">The admin API key for authentication.</param>\n+    /// <param name=\"vectorDimension\">The dimensionality of document vectors.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public AzureSearchDocumentStore(\n+        string serviceName,\n+        string indexName,\n+        string apiKey,\n+        int vectorDimension,\n+        INumericOperations<T> numericOperations)\n+        : base(vectorDimension, numericOperations)\n+    {\n+        _serviceName = serviceName ?? throw new ArgumentNullException(nameof(serviceName));\n+        _indexName = indexName ?? throw new ArgumentNullException(nameof(indexName));\n+        _apiKey = apiKey ?? throw new ArgumentNullException(nameof(apiKey));\n+    }\n+\n+    /// <summary>\n+    /// Adds a document to the Azure Search index.\n+    /// </summary>\n+    public override void AddDocument(Document<T> document)\n+    {\n+        if (document == null)\n+            throw new ArgumentNullException(nameof(document));\n+\n+        // TODO: Implement Azure Search indexing via REST API\n+        throw new NotImplementedException(\"Azure Search integration requires HTTP client implementation\");\n+    }\n+\n+    /// <summary>\n+    /// Retrieves documents similar to the query vector.\n+    /// </summary>\n+    public override IEnumerable<Document<T>> GetSimilar(Vector<T> queryVector, int topK)\n+    {\n+        if (queryVector == null)\n+            throw new ArgumentNullException(nameof(queryVector));\n+\n+        if (topK <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(topK), \"topK must be positive\");\n+\n+        // TODO: Implement Azure Search vector search via REST API\n+        throw new NotImplementedException(\"Azure Search integration requires HTTP client implementation\");\n+    }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/AzureSearchDocumentStore.cs",
    "commit_id": "7fbd5910239204f8852637c980b53b76349cc626",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Implement GetSimilarCore instead of overriding GetSimilar.**\n\nThe base class provides a public `GetSimilar(Vector<T>, int)` method that handles validation and calls `GetSimilarWithFilters(Vector<T>, int, Dictionary<string, object>)`, which then invokes the protected abstract `GetSimilarCore`. By overriding `GetSimilar` directly:\n\n1. You bypass the base class's validation logic\n2. You lose metadata filtering support that the base class provides via `GetSimilarWithFilters`\n3. You duplicate validation that the base already performs\n\nAccording to the base class pattern, implement `GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)` instead.\n\n\n\nVerify the correct method signature:\n\n```shell\n#!/bin/bash\n# Description: Find GetSimilarCore method signature in base class\n\nrg -A 5 'GetSimilarCore' --type cs -g '*DocumentStoreBase.cs'\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/AzureSearchDocumentStore.cs\naround lines 57ΓÇô67, don't override the public GetSimilar method; instead\nimplement the protected override GetSimilarCore(Vector<T> queryVector, int topK,\nDictionary<string, object> metadataFilters) that the base class expects. Remove\nthe current public override, implement GetSimilarCore with that exact signature,\naccept the provided metadataFilters (do not re-do validation ΓÇö the base handles\nit), call Azure Search via the REST/HTTP client already available on this class\n(build the vector search request, include metadataFilters as query/body\nfilters), translate Azure Search results into IEnumerable<Document<T>> and\nreturn the mapped documents; if HTTP injection is missing, use the existing\nHttpClient or add one via constructor DI and use it here.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T00:34:00Z",
    "updated_at": "2025-11-04T00:34:06Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261334",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261334"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261334"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261334/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 57,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 67,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 67,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261339",
    "pull_request_review_id": 3413480474,
    "id": 2488261339,
    "node_id": "PRRC_kwDOKSXUF86UT9rb",
    "diff_hunk": "@@ -0,0 +1,82 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// Azure Cognitive Search document store providing fully managed search capabilities.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Azure Cognitive Search combines full-text search, semantic search, and vector search\n+/// in a fully managed cloud service with enterprise-grade security and compliance.\n+/// </remarks>\n+public class AzureSearchDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly string _serviceName;\n+    private readonly string _indexName;\n+    private readonly string _apiKey;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"AzureSearchDocumentStore{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"serviceName\">The Azure Search service name.</param>\n+    /// <param name=\"indexName\">The name of the index to use.</param>\n+    /// <param name=\"apiKey\">The admin API key for authentication.</param>\n+    /// <param name=\"vectorDimension\">The dimensionality of document vectors.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public AzureSearchDocumentStore(\n+        string serviceName,\n+        string indexName,\n+        string apiKey,\n+        int vectorDimension,\n+        INumericOperations<T> numericOperations)\n+        : base(vectorDimension, numericOperations)\n+    {\n+        _serviceName = serviceName ?? throw new ArgumentNullException(nameof(serviceName));\n+        _indexName = indexName ?? throw new ArgumentNullException(nameof(indexName));\n+        _apiKey = apiKey ?? throw new ArgumentNullException(nameof(apiKey));\n+    }\n+\n+    /// <summary>\n+    /// Adds a document to the Azure Search index.\n+    /// </summary>\n+    public override void AddDocument(Document<T> document)\n+    {\n+        if (document == null)\n+            throw new ArgumentNullException(nameof(document));\n+\n+        // TODO: Implement Azure Search indexing via REST API\n+        throw new NotImplementedException(\"Azure Search integration requires HTTP client implementation\");\n+    }\n+\n+    /// <summary>\n+    /// Retrieves documents similar to the query vector.\n+    /// </summary>\n+    public override IEnumerable<Document<T>> GetSimilar(Vector<T> queryVector, int topK)\n+    {\n+        if (queryVector == null)\n+            throw new ArgumentNullException(nameof(queryVector));\n+\n+        if (topK <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(topK), \"topK must be positive\");\n+\n+        // TODO: Implement Azure Search vector search via REST API\n+        throw new NotImplementedException(\"Azure Search integration requires HTTP client implementation\");\n+    }\n+\n+    /// <summary>\n+    /// Gets all documents from the index.\n+    /// </summary>\n+    public override IEnumerable<Document<T>> GetAllDocuments()\n+    {\n+        // TODO: Implement Azure Search document retrieval\n+        throw new NotImplementedException(\"Azure Search integration requires HTTP client implementation\");\n+    }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/AzureSearchDocumentStore.cs",
    "commit_id": "7fbd5910239204f8852637c980b53b76349cc626",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n<details>\n<summary>≡ƒº⌐ Analysis chain</summary>\n\n**Verify GetAllDocuments exists in the base class.**\n\nThe `GetAllDocuments()` method is not present in the provided base class snippet. Verify that `DocumentStoreBase<T>` defines this as an abstract or virtual method that requires implementation.\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Description: Check if GetAllDocuments is defined in DocumentStoreBase\n\nrg -n 'GetAllDocuments' --type cs -g '*DocumentStoreBase.cs'\n```\n\nLength of output: 0\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Description: Find DocumentStoreBase class definition and check for GetAllDocuments method\n\n# First, locate all occurrences of DocumentStoreBase\necho \"=== Finding DocumentStoreBase class definition ===\"\nrg -n 'class DocumentStoreBase' --type cs -C 5\n\n# Also check if there's a method named GetAllDocuments anywhere in the codebase\necho -e \"\\n=== Finding GetAllDocuments anywhere in codebase ===\"\nrg -n 'GetAllDocuments' --type cs -B 2 -A 2\n\n# Search for the AzureSearchDocumentStore class to understand its inheritance\necho -e \"\\n=== Finding AzureSearchDocumentStore class ===\"\nrg -n 'class AzureSearchDocumentStore' --type cs -C 3\n```\n\nLength of output: 6396\n\n---\n\n</thinking>\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Description: Find where GetAllDocuments is declared as abstract/virtual\n\n# Search for IDocumentStore interface definition\necho \"=== Finding IDocumentStore interface ===\"\nrg -n 'interface IDocumentStore' --type cs -A 20\n\n# Search for all method signatures (abstract/virtual) in DocumentStoreBase\necho -e \"\\n=== Finding method signatures in DocumentStoreBase ===\"\nrg -n 'abstract|virtual' src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs --type cs -B 1 -A 1\n```\n\nLength of output: 3733\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Description: Find GetAllDocuments declaration\n\n# Get full IDocumentStore interface content\necho \"=== Full IDocumentStore interface ===\"\ncat src/Interfaces/IDocumentStore.cs\n\n# Also search for GetAllDocuments declaration (method signature without implementation details)\necho -e \"\\n=== Searching for GetAllDocuments declaration ===\"\nrg -n 'GetAllDocuments\\s*\\(' src/Interfaces/IDocumentStore.cs src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs --type cs -B 3 -A 1\n```\n\nLength of output: 8634\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Description: Search explicitly for GetAllDocuments in DocumentStoreBase\n\n# Search for GetAllDocuments in DocumentStoreBase file\necho \"=== Searching for GetAllDocuments in DocumentStoreBase.cs ===\"\nrg -n 'GetAllDocuments' src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs --type cs -B 5 -A 5\n\n# Also check the full length of DocumentStoreBase to understand its structure\necho -e \"\\n=== Line count and end of DocumentStoreBase.cs ===\"\nwc -l src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs\ntail -20 src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs\n```\n\nLength of output: 723\n\n---\n\n\n\n</details>\n\n**Remove `override` keyword or add `GetAllDocuments()` abstract method to base class.**\n\nThe `GetAllDocuments()` method is not declared in `DocumentStoreBase<T>` or `IDocumentStore<T>`, making the `override` keyword at line 72 invalid. Either:\n1. Add `public abstract IEnumerable<Document<T>> GetAllDocuments();` to `DocumentStoreBase<T>`, or\n2. Change line 72 from `public override` to `public`\n\nOther implementations (SQLiteVSSDocumentStore, RedisVLDocumentStore, QdrantDocumentStore, ElasticsearchDocumentStore, ChromaDBDocumentStore) have the same issue.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/AzureSearchDocumentStore.cs\naround lines 72-76, the method is declared with `public override` but no\n`GetAllDocuments()` exists in DocumentStoreBase<T> or IDocumentStore<T>, causing\na compile error; add `public abstract IEnumerable<Document<T>>\nGetAllDocuments();` to DocumentStoreBase<T> (and if you want the interface to\nrequire it, also add the signature to IDocumentStore<T>), then keep the\n`override` implementations in AzureSearchDocumentStore and the other stores\n(SQLiteVSSDocumentStore, RedisVLDocumentStore, QdrantDocumentStore,\nElasticsearchDocumentStore, ChromaDBDocumentStore); alternatively, if you prefer\nnot to change the base types, change each storeΓÇÖs declaration from `public\noverride` to `public` insteadΓÇöpick one approach and apply it consistently across\nall listed files.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T00:34:00Z",
    "updated_at": "2025-11-04T00:34:06Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261339",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261339"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261339"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261339/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 72,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 76,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 76,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261346",
    "pull_request_review_id": 3413480474,
    "id": 2488261346,
    "node_id": "PRRC_kwDOKSXUF86UT9ri",
    "diff_hunk": "@@ -0,0 +1,82 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// Azure Cognitive Search document store providing fully managed search capabilities.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Azure Cognitive Search combines full-text search, semantic search, and vector search\n+/// in a fully managed cloud service with enterprise-grade security and compliance.\n+/// </remarks>\n+public class AzureSearchDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly string _serviceName;\n+    private readonly string _indexName;\n+    private readonly string _apiKey;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"AzureSearchDocumentStore{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"serviceName\">The Azure Search service name.</param>\n+    /// <param name=\"indexName\">The name of the index to use.</param>\n+    /// <param name=\"apiKey\">The admin API key for authentication.</param>\n+    /// <param name=\"vectorDimension\">The dimensionality of document vectors.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public AzureSearchDocumentStore(\n+        string serviceName,\n+        string indexName,\n+        string apiKey,\n+        int vectorDimension,\n+        INumericOperations<T> numericOperations)\n+        : base(vectorDimension, numericOperations)\n+    {\n+        _serviceName = serviceName ?? throw new ArgumentNullException(nameof(serviceName));\n+        _indexName = indexName ?? throw new ArgumentNullException(nameof(indexName));\n+        _apiKey = apiKey ?? throw new ArgumentNullException(nameof(apiKey));\n+    }\n+\n+    /// <summary>\n+    /// Adds a document to the Azure Search index.\n+    /// </summary>\n+    public override void AddDocument(Document<T> document)\n+    {\n+        if (document == null)\n+            throw new ArgumentNullException(nameof(document));\n+\n+        // TODO: Implement Azure Search indexing via REST API\n+        throw new NotImplementedException(\"Azure Search integration requires HTTP client implementation\");\n+    }\n+\n+    /// <summary>\n+    /// Retrieves documents similar to the query vector.\n+    /// </summary>\n+    public override IEnumerable<Document<T>> GetSimilar(Vector<T> queryVector, int topK)\n+    {\n+        if (queryVector == null)\n+            throw new ArgumentNullException(nameof(queryVector));\n+\n+        if (topK <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(topK), \"topK must be positive\");\n+\n+        // TODO: Implement Azure Search vector search via REST API\n+        throw new NotImplementedException(\"Azure Search integration requires HTTP client implementation\");\n+    }\n+\n+    /// <summary>\n+    /// Gets all documents from the index.\n+    /// </summary>\n+    public override IEnumerable<Document<T>> GetAllDocuments()\n+    {\n+        // TODO: Implement Azure Search document retrieval\n+        throw new NotImplementedException(\"Azure Search integration requires HTTP client implementation\");\n+    }\n+\n+    /// <summary>\n+    /// Gets the total number of documents in the index.\n+    /// </summary>\n+    public override int DocumentCount => 0; // TODO: Implement via Azure Search API",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/AzureSearchDocumentStore.cs",
    "commit_id": "7fbd5910239204f8852637c980b53b76349cc626",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n<details>\n<summary>≡ƒº⌐ Analysis chain</summary>\n\n**Verify all abstract base class methods are implemented.**\n\nThe `DocumentCount` placeholder is acceptable for a stub. However, the base class defines several abstract methods that must be implemented:\n\n- `protected abstract void AddCore(VectorDocument<T>)`\n- `protected abstract IEnumerable<Document<T>> GetSimilarCore(Vector<T>, int, Dictionary<string, object>)`\n- `protected abstract Document<T>? GetByIdCore(string)`\n- `protected abstract bool RemoveCore(string)`\n- `public abstract void Clear()`\n- `public abstract int VectorDimension { get; }`\n\nThis file shows none of these implementations. Verify that:\n1. All required abstract members are implemented (or the code wouldn't compile)\n2. The correct methods are being overridden per the base class pattern\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Description: List all abstract members in DocumentStoreBase that need implementation\n\nrg -n 'abstract' --type cs -g '*DocumentStoreBase.cs' | grep -E '(method|property|void|IEnumerable|bool|int)'\n```\n\nLength of output: 1006\n\n---\n\nI'll search the codebase directly to verify all abstract methods are implemented in AzureSearchDocumentStore.\n\n\n\n```\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# List all public and protected methods/properties in AzureSearchDocumentStore\nrg -n 'public\\s+(override|abstract)' src/RetrievalAugmentedGeneration/DocumentStores/AzureSearchDocumentStore.cs\nrg -n 'protected\\s+(override|abstract)' src/RetrievalAugmentedGeneration/DocumentStores/AzureSearchDocumentStore.cs\n```\n\nLength of output: 306\n\n---\n\n```\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Get full file to see all implementations\ncat src/RetrievalAugmentedGeneration/DocumentStores/AzureSearchDocumentStore.cs\n```\n\nLength of output: 3297\n\n---\n\n```\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check if DocumentStoreBase actually defines these members as abstract or virtual\ncat src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs\n```\n\nLength of output: 14172\n\n---\n\n\n\n</details>\n\n**Implement all required abstract base class members to fix compilation errors.**\n\nThe code is missing critical implementations. DocumentStoreBase defines 7 abstract members: `DocumentCount`, `VectorDimension`, `Clear()`, `AddCore()`, `GetSimilarCore()`, `GetByIdCore()`, and `RemoveCore()`. AzureSearchDocumentStore currently implements only `DocumentCount`; all other abstract members are missing.\n\nRequired implementations in `src/RetrievalAugmentedGeneration/DocumentStores/AzureSearchDocumentStore.cs`:\n\n- `public override int VectorDimension { get; }`\n- `public override void Clear()`\n- `protected override void AddCore(VectorDocument<T> vectorDocument)`\n- `protected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)`\n- `protected override Document<T>? GetByIdCore(string documentId)`\n- `protected override bool RemoveCore(string documentId)`\n\nThese can throw `NotImplementedException` with TODO comments (matching the existing stub pattern), but they must be declared to allow compilation.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/AzureSearchDocumentStore.cs\naround line 81, the class currently only implements DocumentCount and is missing\nthe other required abstract members from DocumentStoreBase; add the following\noverrides with TODO NotImplementedException stubs so the class compiles: public\noverride int VectorDimension { get; }, public override void Clear(), protected\noverride void AddCore(VectorDocument<T> vectorDocument), protected override\nIEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK,\nDictionary<string, object> metadataFilters), protected override Document<T>?\nGetByIdCore(string documentId), and protected override bool RemoveCore(string\ndocumentId) ΓÇö each should throw new NotImplementedException() and include a TODO\ncomment matching the existing stub pattern.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T00:34:01Z",
    "updated_at": "2025-11-04T00:34:06Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261346",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261346"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261346"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261346/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": null,
    "original_line": 81,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 81,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261352",
    "pull_request_review_id": 3413480474,
    "id": 2488261352,
    "node_id": "PRRC_kwDOKSXUF86UT9ro",
    "diff_hunk": "@@ -0,0 +1,78 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// ChromaDB-based document store designed for simplicity and developer experience.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// ChromaDB is an open-source vector database that emphasizes ease of use while maintaining\n+/// high performance for similarity search operations.\n+/// </remarks>\n+public class ChromaDBDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly string _endpoint;\n+    private readonly string _collectionName;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"ChromaDBDocumentStore{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"endpoint\">The ChromaDB endpoint URL.</param>\n+    /// <param name=\"collectionName\">The name of the collection to use.</param>\n+    /// <param name=\"vectorDimension\">The dimensionality of document vectors.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public ChromaDBDocumentStore(\n+        string endpoint,\n+        string collectionName,\n+        int vectorDimension,\n+        INumericOperations<T> numericOperations)\n+        : base(vectorDimension, numericOperations)\n+    {\n+        _endpoint = endpoint ?? throw new ArgumentNullException(nameof(endpoint));\n+        _collectionName = collectionName ?? throw new ArgumentNullException(nameof(collectionName));\n+    }\n+\n+    /// <summary>\n+    /// Adds a document to the ChromaDB collection.\n+    /// </summary>\n+    public override void AddDocument(Document<T> document)\n+    {\n+        if (document == null)\n+            throw new ArgumentNullException(nameof(document));\n+\n+        // TODO: Implement ChromaDB add via REST API\n+        throw new NotImplementedException(\"ChromaDB integration requires HTTP client implementation\");\n+    }\n+\n+    /// <summary>\n+    /// Retrieves documents similar to the query vector.\n+    /// </summary>\n+    public override IEnumerable<Document<T>> GetSimilar(Vector<T> queryVector, int topK)\n+    {\n+        if (queryVector == null)\n+            throw new ArgumentNullException(nameof(queryVector));\n+\n+        if (topK <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(topK), \"topK must be positive\");\n+\n+        // TODO: Implement ChromaDB query via REST API\n+        throw new NotImplementedException(\"ChromaDB integration requires HTTP client implementation\");\n+    }\n+\n+    /// <summary>\n+    /// Gets all documents from the collection.\n+    /// </summary>\n+    public override IEnumerable<Document<T>> GetAllDocuments()\n+    {\n+        // TODO: Implement ChromaDB get all documents\n+        throw new NotImplementedException(\"ChromaDB integration requires HTTP client implementation\");\n+    }\n+\n+    /// <summary>\n+    /// Gets the total number of documents in the collection.\n+    /// </summary>\n+    public override int DocumentCount => 0; // TODO: Implement via ChromaDB API\n+}",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/ChromaDBDocumentStore.cs",
    "commit_id": "7fbd5910239204f8852637c980b53b76349cc626",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Implement missing abstract members from base class.**\n\nThe class is missing implementations for several abstract members required by `DocumentStoreBase<T>`:\n\n- `VectorDimension` property\n- `GetByIdCore(string documentId)` method\n- `RemoveCore(string documentId)` method  \n- `Clear()` method\n\nWithout these implementations, the class will not compile.\n\n\n\nAdd these implementations to the class:\n\n```csharp\nprivate readonly int _vectorDimension;\n\n// Update constructor to store vectorDimension:\npublic ChromaDBDocumentStore(\n    string endpoint,\n    string collectionName,\n    int vectorDimension,\n    INumericOperations<T> numericOperations)\n    : base(vectorDimension, numericOperations)\n{\n    if (string.IsNullOrWhiteSpace(endpoint))\n        throw new ArgumentException(\"Endpoint cannot be null or empty\", nameof(endpoint));\n    if (string.IsNullOrWhiteSpace(collectionName))\n        throw new ArgumentException(\"Collection name cannot be null or empty\", nameof(collectionName));\n    if (vectorDimension <= 0)\n        throw new ArgumentOutOfRangeException(nameof(vectorDimension), \"Vector dimension must be positive\");\n\n    _endpoint = endpoint;\n    _collectionName = collectionName;\n    _vectorDimension = vectorDimension;\n}\n\n/// <summary>\n/// Gets the dimensionality of vectors stored in this document store.\n/// </summary>\npublic override int VectorDimension => _vectorDimension;\n\n/// <summary>\n/// Core logic for retrieving a document by ID.\n/// </summary>\nprotected override Document<T>? GetByIdCore(string documentId)\n{\n    // TODO: Implement ChromaDB get by ID via REST API\n    throw new NotImplementedException(\"ChromaDB integration requires HTTP client implementation\");\n}\n\n/// <summary>\n/// Core logic for removing a document by ID.\n/// </summary>\nprotected override bool RemoveCore(string documentId)\n{\n    // TODO: Implement ChromaDB remove via REST API\n    throw new NotImplementedException(\"ChromaDB integration requires HTTP client implementation\");\n}\n\n/// <summary>\n/// Removes all documents from the collection.\n/// </summary>\npublic override void Clear()\n{\n    // TODO: Implement ChromaDB clear collection via REST API\n    throw new NotImplementedException(\"ChromaDB integration requires HTTP client implementation\");\n}\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/ChromaDBDocumentStore.cs\naround lines 15 to 78, the class is missing required abstract members from\nDocumentStoreBase<T> ΓÇö specifically a stored vector dimension field, the\nVectorDimension property, and the core methods GetByIdCore(string),\nRemoveCore(string) and Clear(); update the constructor to validate and store\nvectorDimension (and validate endpoint and collectionName), add a private\nreadonly int _vectorDimension, implement public override int VectorDimension =>\n_vectorDimension, and add the protected override Document<T>? GetByIdCore(string\ndocumentId), protected override bool RemoveCore(string documentId) and public\noverride void Clear() methods, each throwing NotImplementedException until the\nChromaDB HTTP client code is implemented.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T00:34:01Z",
    "updated_at": "2025-11-04T00:34:06Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261352",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261352"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261352"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261352/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 22,
    "original_start_line": 15,
    "start_side": "RIGHT",
    "line": 287,
    "original_line": 78,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 78,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261354",
    "pull_request_review_id": 3413480474,
    "id": 2488261354,
    "node_id": "PRRC_kwDOKSXUF86UT9rq",
    "diff_hunk": "@@ -0,0 +1,78 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// ChromaDB-based document store designed for simplicity and developer experience.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// ChromaDB is an open-source vector database that emphasizes ease of use while maintaining\n+/// high performance for similarity search operations.\n+/// </remarks>\n+public class ChromaDBDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly string _endpoint;\n+    private readonly string _collectionName;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"ChromaDBDocumentStore{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"endpoint\">The ChromaDB endpoint URL.</param>\n+    /// <param name=\"collectionName\">The name of the collection to use.</param>\n+    /// <param name=\"vectorDimension\">The dimensionality of document vectors.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public ChromaDBDocumentStore(\n+        string endpoint,\n+        string collectionName,\n+        int vectorDimension,\n+        INumericOperations<T> numericOperations)\n+        : base(vectorDimension, numericOperations)\n+    {\n+        _endpoint = endpoint ?? throw new ArgumentNullException(nameof(endpoint));\n+        _collectionName = collectionName ?? throw new ArgumentNullException(nameof(collectionName));\n+    }\n+\n+    /// <summary>\n+    /// Adds a document to the ChromaDB collection.\n+    /// </summary>\n+    public override void AddDocument(Document<T> document)\n+    {\n+        if (document == null)\n+            throw new ArgumentNullException(nameof(document));\n+\n+        // TODO: Implement ChromaDB add via REST API\n+        throw new NotImplementedException(\"ChromaDB integration requires HTTP client implementation\");\n+    }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/ChromaDBDocumentStore.cs",
    "commit_id": "7fbd5910239204f8852637c980b53b76349cc626",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Incorrect method signature causes compilation failure.**\n\nThis class attempts to override `AddDocument(Document<T>)`, but the base class `DocumentStoreBase<T>` requires implementing `protected abstract void AddCore(VectorDocument<T> vectorDocument)` instead. The base class provides a public `Add` method that performs validation and calls `AddCore`.\n\n\n\nApply this diff to implement the correct abstract method:\n\n```diff\n-    /// <summary>\n-    /// Adds a document to the ChromaDB collection.\n-    /// </summary>\n-    public override void AddDocument(Document<T> document)\n-    {\n-        if (document == null)\n-            throw new ArgumentNullException(nameof(document));\n-\n-        // TODO: Implement ChromaDB add via REST API\n-        throw new NotImplementedException(\"ChromaDB integration requires HTTP client implementation\");\n-    }\n+    /// <summary>\n+    /// Core logic for adding a vector document to the ChromaDB collection.\n+    /// </summary>\n+    protected override void AddCore(VectorDocument<T> vectorDocument)\n+    {\n+        // TODO: Implement ChromaDB add via REST API\n+        throw new NotImplementedException(\"ChromaDB integration requires HTTP client implementation\");\n+    }\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/ChromaDBDocumentStore.cs\naround lines 41-48, the class incorrectly declares public override void\nAddDocument(Document<T> document) which doesn't match the base class abstract\nmethod; replace that method with the required protected override void\nAddCore(VectorDocument<T> vectorDocument) implementation. Remove/rename the\nexisting AddDocument method, implement the protected override\nAddCore(VectorDocument<T> vectorDocument) with appropriate null-check (if\nneeded) and currently throw a NotImplementedException (or keep the existing TODO\ncomment) indicating ChromaDB integration requires an HTTP client, ensuring the\nsignature, accessibility (protected override), and parameter type match the base\nclass exactly.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T00:34:01Z",
    "updated_at": "2025-11-04T00:34:06Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261354",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261354"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261354"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261354/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 41,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 48,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 48,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261362",
    "pull_request_review_id": 3413480474,
    "id": 2488261362,
    "node_id": "PRRC_kwDOKSXUF86UT9ry",
    "diff_hunk": "@@ -0,0 +1,78 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// ChromaDB-based document store designed for simplicity and developer experience.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// ChromaDB is an open-source vector database that emphasizes ease of use while maintaining\n+/// high performance for similarity search operations.\n+/// </remarks>\n+public class ChromaDBDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly string _endpoint;\n+    private readonly string _collectionName;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"ChromaDBDocumentStore{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"endpoint\">The ChromaDB endpoint URL.</param>\n+    /// <param name=\"collectionName\">The name of the collection to use.</param>\n+    /// <param name=\"vectorDimension\">The dimensionality of document vectors.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public ChromaDBDocumentStore(\n+        string endpoint,\n+        string collectionName,\n+        int vectorDimension,\n+        INumericOperations<T> numericOperations)\n+        : base(vectorDimension, numericOperations)\n+    {\n+        _endpoint = endpoint ?? throw new ArgumentNullException(nameof(endpoint));\n+        _collectionName = collectionName ?? throw new ArgumentNullException(nameof(collectionName));\n+    }\n+\n+    /// <summary>\n+    /// Adds a document to the ChromaDB collection.\n+    /// </summary>\n+    public override void AddDocument(Document<T> document)\n+    {\n+        if (document == null)\n+            throw new ArgumentNullException(nameof(document));\n+\n+        // TODO: Implement ChromaDB add via REST API\n+        throw new NotImplementedException(\"ChromaDB integration requires HTTP client implementation\");\n+    }\n+\n+    /// <summary>\n+    /// Retrieves documents similar to the query vector.\n+    /// </summary>\n+    public override IEnumerable<Document<T>> GetSimilar(Vector<T> queryVector, int topK)\n+    {\n+        if (queryVector == null)\n+            throw new ArgumentNullException(nameof(queryVector));\n+\n+        if (topK <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(topK), \"topK must be positive\");\n+\n+        // TODO: Implement ChromaDB query via REST API\n+        throw new NotImplementedException(\"ChromaDB integration requires HTTP client implementation\");\n+    }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/ChromaDBDocumentStore.cs",
    "commit_id": "7fbd5910239204f8852637c980b53b76349cc626",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Incorrect method signature causes compilation failure.**\n\nThis class attempts to override `GetSimilar(Vector<T>, int)`, but this is a public non-virtual method in the base class. You must implement `protected abstract IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)` instead. The base class handles validation and calls `GetSimilarCore`.\n\n\n\nApply this diff to implement the correct abstract method:\n\n```diff\n-    /// <summary>\n-    /// Retrieves documents similar to the query vector.\n-    /// </summary>\n-    public override IEnumerable<Document<T>> GetSimilar(Vector<T> queryVector, int topK)\n-    {\n-        if (queryVector == null)\n-            throw new ArgumentNullException(nameof(queryVector));\n-\n-        if (topK <= 0)\n-            throw new ArgumentOutOfRangeException(nameof(topK), \"topK must be positive\");\n-\n-        // TODO: Implement ChromaDB query via REST API\n-        throw new NotImplementedException(\"ChromaDB integration requires HTTP client implementation\");\n-    }\n+    /// <summary>\n+    /// Core logic for similarity search with optional filtering.\n+    /// </summary>\n+    protected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)\n+    {\n+        // TODO: Implement ChromaDB query via REST API\n+        throw new NotImplementedException(\"ChromaDB integration requires HTTP client implementation\");\n+    }\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T00:34:01Z",
    "updated_at": "2025-11-04T00:34:06Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261362",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261362"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261362"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261362/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 53,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 63,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 63,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261366",
    "pull_request_review_id": 3413480474,
    "id": 2488261366,
    "node_id": "PRRC_kwDOKSXUF86UT9r2",
    "diff_hunk": "@@ -0,0 +1,78 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// ChromaDB-based document store designed for simplicity and developer experience.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// ChromaDB is an open-source vector database that emphasizes ease of use while maintaining\n+/// high performance for similarity search operations.\n+/// </remarks>\n+public class ChromaDBDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly string _endpoint;\n+    private readonly string _collectionName;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"ChromaDBDocumentStore{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"endpoint\">The ChromaDB endpoint URL.</param>\n+    /// <param name=\"collectionName\">The name of the collection to use.</param>\n+    /// <param name=\"vectorDimension\">The dimensionality of document vectors.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public ChromaDBDocumentStore(\n+        string endpoint,\n+        string collectionName,\n+        int vectorDimension,\n+        INumericOperations<T> numericOperations)\n+        : base(vectorDimension, numericOperations)\n+    {\n+        _endpoint = endpoint ?? throw new ArgumentNullException(nameof(endpoint));\n+        _collectionName = collectionName ?? throw new ArgumentNullException(nameof(collectionName));\n+    }\n+\n+    /// <summary>\n+    /// Adds a document to the ChromaDB collection.\n+    /// </summary>\n+    public override void AddDocument(Document<T> document)\n+    {\n+        if (document == null)\n+            throw new ArgumentNullException(nameof(document));\n+\n+        // TODO: Implement ChromaDB add via REST API\n+        throw new NotImplementedException(\"ChromaDB integration requires HTTP client implementation\");\n+    }\n+\n+    /// <summary>\n+    /// Retrieves documents similar to the query vector.\n+    /// </summary>\n+    public override IEnumerable<Document<T>> GetSimilar(Vector<T> queryVector, int topK)\n+    {\n+        if (queryVector == null)\n+            throw new ArgumentNullException(nameof(queryVector));\n+\n+        if (topK <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(topK), \"topK must be positive\");\n+\n+        // TODO: Implement ChromaDB query via REST API\n+        throw new NotImplementedException(\"ChromaDB integration requires HTTP client implementation\");\n+    }\n+\n+    /// <summary>\n+    /// Gets all documents from the collection.\n+    /// </summary>\n+    public override IEnumerable<Document<T>> GetAllDocuments()\n+    {\n+        // TODO: Implement ChromaDB get all documents\n+        throw new NotImplementedException(\"ChromaDB integration requires HTTP client implementation\");\n+    }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/ChromaDBDocumentStore.cs",
    "commit_id": "7fbd5910239204f8852637c980b53b76349cc626",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Remove method that doesn't exist in the base class.**\n\nThe base class `DocumentStoreBase<T>` does not define a `GetAllDocuments` method (abstract or otherwise), so this override will cause a compilation error. If this functionality is needed, the base class interface must be extended first.\n\n\n\nApply this diff to remove the invalid override:\n\n```diff\n-    /// <summary>\n-    /// Gets all documents from the collection.\n-    /// </summary>\n-    public override IEnumerable<Document<T>> GetAllDocuments()\n-    {\n-        // TODO: Implement ChromaDB get all documents\n-        throw new NotImplementedException(\"ChromaDB integration requires HTTP client implementation\");\n-    }\n-\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/ChromaDBDocumentStore.cs\naround lines 68 to 72, the method public override IEnumerable<Document<T>>\nGetAllDocuments() is overriding a non-existent member and will not compile;\nremove this method entirely (or change it to a new non-override method name if\nyou need local functionality) and if GetAllDocuments is required across stores,\ninstead add it to DocumentStoreBase<T> first and implement the abstract/virtual\ncontract there before overriding in this class.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 1d6eceb to 7ab7f09",
    "created_at": "2025-11-04T00:34:01Z",
    "updated_at": "2025-11-04T02:27:09Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261366",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261366"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261366"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261366/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 68,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 72,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 72,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261369",
    "pull_request_review_id": 3413480474,
    "id": 2488261369,
    "node_id": "PRRC_kwDOKSXUF86UT9r5",
    "diff_hunk": "@@ -0,0 +1,83 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// Elasticsearch-based document store providing hybrid search capabilities (BM25 + dense vectors).\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Elasticsearch combines traditional full-text search (BM25) with vector similarity search,\n+/// making it ideal for hybrid retrieval scenarios where both keyword matching and semantic\n+/// similarity are important.\n+/// </remarks>\n+public class ElasticsearchDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly string _endpoint;\n+    private readonly string _indexName;\n+    private readonly string _apiKey;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"ElasticsearchDocumentStore{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"endpoint\">The Elasticsearch endpoint URL.</param>\n+    /// <param name=\"indexName\">The name of the index to use.</param>\n+    /// <param name=\"apiKey\">The API key for authentication.</param>\n+    /// <param name=\"vectorDimension\">The dimensionality of document vectors.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public ElasticsearchDocumentStore(\n+        string endpoint,\n+        string indexName,\n+        string apiKey,\n+        int vectorDimension,\n+        INumericOperations<T> numericOperations)\n+        : base(vectorDimension, numericOperations)\n+    {\n+        _endpoint = endpoint ?? throw new ArgumentNullException(nameof(endpoint));\n+        _indexName = indexName ?? throw new ArgumentNullException(nameof(indexName));\n+        _apiKey = apiKey ?? throw new ArgumentNullException(nameof(apiKey));\n+    }\n+\n+    /// <summary>\n+    /// Adds a document to the Elasticsearch index.\n+    /// </summary>\n+    public override void AddDocument(Document<T> document)\n+    {\n+        if (document == null)\n+            throw new ArgumentNullException(nameof(document));\n+\n+        // TODO: Implement Elasticsearch indexing via REST API\n+        throw new NotImplementedException(\"Elasticsearch integration requires HTTP client implementation\");\n+    }\n+\n+    /// <summary>\n+    /// Retrieves documents similar to the query vector using k-NN search.\n+    /// </summary>\n+    public override IEnumerable<Document<T>> GetSimilar(Vector<T> queryVector, int topK)\n+    {\n+        if (queryVector == null)\n+            throw new ArgumentNullException(nameof(queryVector));\n+\n+        if (topK <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(topK), \"topK must be positive\");\n+\n+        // TODO: Implement Elasticsearch k-NN search via REST API\n+        throw new NotImplementedException(\"Elasticsearch integration requires HTTP client implementation\");\n+    }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/ElasticsearchDocumentStore.cs",
    "commit_id": "7fbd5910239204f8852637c980b53b76349cc626",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Implement Elasticsearch calls instead of throwing.**\n\nBoth `AddDocument` and `GetSimilar` immediately throw `NotImplementedException`, so any caller that exercises this concrete store will crash. The base `DocumentStoreBase<T>` expects derived stores to provide real persistence/search logic, and the PR description claims these concrete implementations are complete. Please wire these methods to the Elasticsearch REST APIs (including auth, serialization, and error handling) so the store can actually index and query documents.\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 15063ed to 8468360",
    "created_at": "2025-11-04T00:34:01Z",
    "updated_at": "2025-11-04T13:34:52Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261369",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261369"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261369"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261369/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 46,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 68,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 68,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261373",
    "pull_request_review_id": 3413480474,
    "id": 2488261373,
    "node_id": "PRRC_kwDOKSXUF86UT9r9",
    "diff_hunk": "@@ -0,0 +1,83 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// Elasticsearch-based document store providing hybrid search capabilities (BM25 + dense vectors).\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Elasticsearch combines traditional full-text search (BM25) with vector similarity search,\n+/// making it ideal for hybrid retrieval scenarios where both keyword matching and semantic\n+/// similarity are important.\n+/// </remarks>\n+public class ElasticsearchDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly string _endpoint;\n+    private readonly string _indexName;\n+    private readonly string _apiKey;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"ElasticsearchDocumentStore{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"endpoint\">The Elasticsearch endpoint URL.</param>\n+    /// <param name=\"indexName\">The name of the index to use.</param>\n+    /// <param name=\"apiKey\">The API key for authentication.</param>\n+    /// <param name=\"vectorDimension\">The dimensionality of document vectors.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public ElasticsearchDocumentStore(\n+        string endpoint,\n+        string indexName,\n+        string apiKey,\n+        int vectorDimension,\n+        INumericOperations<T> numericOperations)\n+        : base(vectorDimension, numericOperations)\n+    {\n+        _endpoint = endpoint ?? throw new ArgumentNullException(nameof(endpoint));\n+        _indexName = indexName ?? throw new ArgumentNullException(nameof(indexName));\n+        _apiKey = apiKey ?? throw new ArgumentNullException(nameof(apiKey));\n+    }\n+\n+    /// <summary>\n+    /// Adds a document to the Elasticsearch index.\n+    /// </summary>\n+    public override void AddDocument(Document<T> document)\n+    {\n+        if (document == null)\n+            throw new ArgumentNullException(nameof(document));\n+\n+        // TODO: Implement Elasticsearch indexing via REST API\n+        throw new NotImplementedException(\"Elasticsearch integration requires HTTP client implementation\");\n+    }\n+\n+    /// <summary>\n+    /// Retrieves documents similar to the query vector using k-NN search.\n+    /// </summary>\n+    public override IEnumerable<Document<T>> GetSimilar(Vector<T> queryVector, int topK)\n+    {\n+        if (queryVector == null)\n+            throw new ArgumentNullException(nameof(queryVector));\n+\n+        if (topK <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(topK), \"topK must be positive\");\n+\n+        // TODO: Implement Elasticsearch k-NN search via REST API\n+        throw new NotImplementedException(\"Elasticsearch integration requires HTTP client implementation\");\n+    }\n+\n+    /// <summary>\n+    /// Gets all documents from the index.\n+    /// </summary>\n+    public override IEnumerable<Document<T>> GetAllDocuments()\n+    {\n+        // TODO: Implement Elasticsearch scroll API for retrieving all documents\n+        throw new NotImplementedException(\"Elasticsearch integration requires HTTP client implementation\");\n+    }\n+\n+    /// <summary>\n+    /// Gets the total number of documents in the index.\n+    /// </summary>\n+    public override int DocumentCount => 0; // TODO: Implement via Elasticsearch count API",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/ElasticsearchDocumentStore.cs",
    "commit_id": "7fbd5910239204f8852637c980b53b76349cc626",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Return the real document count.**\n\nHard-coding `DocumentCount` to zero breaks dimension validation in `DocumentStoreBase<T>.ValidateVectorDocument` and misleads consumers that rely on this property for capacity checks or monitoring. Please query the Elasticsearch `_count` endpoint (or maintain an internal counter) so this property reflects the true index size.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/ElasticsearchDocumentStore.cs\naround lines 79-82, the DocumentCount property is hard-coded to 0; replace it\nwith a real count by querying Elasticsearch's _count API (or by maintaining an\ninternal counter updated on add/delete operations). Use the Elastic client\nCount/CountAsync endpoint to get the index document count, handle errors (log\nand return 0 on failure), and ensure thread-safety and minimal blocking (prefer\nupdating a cached value async or call the synchronous Count API if a sync\nproperty must be used). Make sure the returned int maps from the long result and\nthat exceptions do not crash callers.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T00:34:01Z",
    "updated_at": "2025-11-04T00:34:06Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261373",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261373"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261373"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261373/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 79,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 82,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 82,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261379",
    "pull_request_review_id": 3413480474,
    "id": 2488261379,
    "node_id": "PRRC_kwDOKSXUF86UT9sD",
    "diff_hunk": "@@ -0,0 +1,78 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// Redis-based vector document store for low-latency applications.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Redis with RedisSearch module provides fast vector similarity search with sub-millisecond latency.\n+/// Ideal for real-time applications requiring instant retrieval.\n+/// </remarks>\n+public class RedisVLDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly string _connectionString;\n+    private readonly string _indexName;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"RedisVLDocumentStore{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"connectionString\">The Redis connection string.</param>\n+    /// <param name=\"indexName\">The name of the index to use.</param>\n+    /// <param name=\"vectorDimension\">The dimensionality of document vectors.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public RedisVLDocumentStore(\n+        string connectionString,\n+        string indexName,\n+        int vectorDimension,\n+        INumericOperations<T> numericOperations)\n+        : base(vectorDimension, numericOperations)\n+    {\n+        _connectionString = connectionString ?? throw new ArgumentNullException(nameof(connectionString));\n+        _indexName = indexName ?? throw new ArgumentNullException(nameof(indexName));\n+    }\n+\n+    /// <summary>\n+    /// Adds a document to the Redis index.\n+    /// </summary>\n+    public override void AddDocument(Document<T> document)\n+    {\n+        if (document == null)\n+            throw new ArgumentNullException(nameof(document));\n+\n+        // TODO: Implement Redis vector indexing\n+        throw new NotImplementedException(\"Redis integration requires StackExchange.Redis implementation\");\n+    }\n+\n+    /// <summary>\n+    /// Retrieves documents similar to the query vector.\n+    /// </summary>\n+    public override IEnumerable<Document<T>> GetSimilar(Vector<T> queryVector, int topK)\n+    {\n+        if (queryVector == null)\n+            throw new ArgumentNullException(nameof(queryVector));\n+\n+        if (topK <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(topK), \"topK must be positive\");\n+\n+        // TODO: Implement Redis vector search\n+        throw new NotImplementedException(\"Redis integration requires StackExchange.Redis implementation\");\n+    }\n+\n+    /// <summary>\n+    /// Gets all documents from the index.\n+    /// </summary>\n+    public override IEnumerable<Document<T>> GetAllDocuments()\n+    {\n+        // TODO: Implement Redis scan operation\n+        throw new NotImplementedException(\"Redis integration requires StackExchange.Redis implementation\");\n+    }\n+\n+    /// <summary>\n+    /// Gets the total number of documents in the index.\n+    /// </summary>\n+    public override int DocumentCount => 0; // TODO: Implement via Redis",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/RedisVLDocumentStore.cs",
    "commit_id": "7fbd5910239204f8852637c980b53b76349cc626",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Critical: Missing required abstract method implementations and incorrect method overrides.**\n\nThis class will not compile due to several API mismatches with `DocumentStoreBase<T>`:\n\n**Missing abstract implementations (required):**\n- `AddCore(VectorDocument<T>)` - core logic for adding documents\n- `GetSimilarCore(Vector<T>, int, Dictionary<string, object>)` - core similarity search with filters\n- `GetByIdCore(string)` - retrieve document by ID\n- `RemoveCore(string)` - remove document by ID  \n- `Clear()` - remove all documents\n- `VectorDimension` (property getter) - return vector dimensionality\n\n**Incorrect overrides:**\n- Line 41: `AddDocument(Document<T>)` does not exist in the base class; the base class has `Add(VectorDocument<T>)` and expects concrete classes to implement `AddCore(VectorDocument<T>)`\n- Line 53: `GetSimilar(Vector<T>, int)` is already implemented in the base class (non-abstract); implement `GetSimilarCore` instead\n- Line 68: `GetAllDocuments()` does not appear in `DocumentStoreBase<T>`; verify if this method exists\n\n\n\nThe correct pattern (as shown in DocumentStoreBase.cs lines 28-348) is:\n1. Implement the abstract `*Core` methods where your storage logic goes\n2. The base class public methods (Add, GetSimilar, etc.) handle validation and call your Core implementations\n3. Do not override public methods unless you have a specific reason to bypass base class validation\n\nApply this structure:\n\n```diff\n-    public override void AddDocument(Document<T> document)\n+    protected override void AddCore(VectorDocument<T> vectorDocument)\n     {\n-        if (document == null)\n-            throw new ArgumentNullException(nameof(document));\n-\n         // TODO: Implement Redis vector indexing\n         throw new NotImplementedException(\"Redis integration requires StackExchange.Redis implementation\");\n     }\n\n-    public override IEnumerable<Document<T>> GetSimilar(Vector<T> queryVector, int topK)\n+    protected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)\n     {\n-        if (queryVector == null)\n-            throw new ArgumentNullException(nameof(queryVector));\n-\n-        if (topK <= 0)\n-            throw new ArgumentOutOfRangeException(nameof(topK), \"topK must be positive\");\n-\n         // TODO: Implement Redis vector search\n         throw new NotImplementedException(\"Redis integration requires StackExchange.Redis implementation\");\n     }\n\n-    public override IEnumerable<Document<T>> GetAllDocuments()\n+    protected override Document<T>? GetByIdCore(string documentId)\n     {\n-        // TODO: Implement Redis scan operation\n+        // TODO: Implement Redis document lookup\n         throw new NotImplementedException(\"Redis integration requires StackExchange.Redis implementation\");\n     }\n\n+    protected override bool RemoveCore(string documentId)\n+    {\n+        // TODO: Implement Redis document removal\n+        throw new NotImplementedException(\"Redis integration requires StackExchange.Redis implementation\");\n+    }\n+\n+    public override void Clear()\n+    {\n+        // TODO: Implement Redis index clearing\n+        throw new NotImplementedException(\"Redis integration requires StackExchange.Redis implementation\");\n+    }\n+\n+    public override int VectorDimension { get; }\n+\n     public override int DocumentCount => 0; // TODO: Implement via Redis\n```\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T00:34:02Z",
    "updated_at": "2025-11-04T00:34:06Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261379",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261379"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261379"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261379/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 15,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 77,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 77,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261385",
    "pull_request_review_id": 3413480474,
    "id": 2488261385,
    "node_id": "PRRC_kwDOKSXUF86UT9sJ",
    "diff_hunk": "@@ -0,0 +1,78 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// Redis-based vector document store for low-latency applications.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Redis with RedisSearch module provides fast vector similarity search with sub-millisecond latency.\n+/// Ideal for real-time applications requiring instant retrieval.\n+/// </remarks>\n+public class RedisVLDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly string _connectionString;\n+    private readonly string _indexName;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"RedisVLDocumentStore{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"connectionString\">The Redis connection string.</param>\n+    /// <param name=\"indexName\">The name of the index to use.</param>\n+    /// <param name=\"vectorDimension\">The dimensionality of document vectors.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public RedisVLDocumentStore(\n+        string connectionString,\n+        string indexName,\n+        int vectorDimension,\n+        INumericOperations<T> numericOperations)\n+        : base(vectorDimension, numericOperations)\n+    {\n+        _connectionString = connectionString ?? throw new ArgumentNullException(nameof(connectionString));\n+        _indexName = indexName ?? throw new ArgumentNullException(nameof(indexName));\n+    }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/RedisVLDocumentStore.cs",
    "commit_id": "7fbd5910239204f8852637c980b53b76349cc626",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n<details>\n<summary>≡ƒº⌐ Analysis chain</summary>\n\n**Verify base constructor signature and numeric operations usage.**\n\nThe constructor passes `numericOperations` to the base class, but `DocumentStoreBase<T>` (line 33) uses a static field `NumOps = MathHelper.GetNumericOperations<T>()` to obtain numeric operations. This suggests the base class may not accept `numericOperations` as a constructor parameter.\n\nAdditionally, ensure the base constructor accepts `vectorDimension` and `numericOperations` parameters. If `VectorDimension` is set via the base constructor, you'll need to expose it as a property:\n\n```csharp\npublic override int VectorDimension { get; }\n```\n\nIf the base constructor doesn't match this signature, remove the `numericOperations` parameter and rely on the static `NumOps` field:\n\n```diff\n public RedisVLDocumentStore(\n     string connectionString,\n     string indexName,\n-    int vectorDimension,\n-    INumericOperations<T> numericOperations)\n-    : base(vectorDimension, numericOperations)\n+    int vectorDimension)\n+    : base(vectorDimension)\n {\n     _connectionString = connectionString ?? throw new ArgumentNullException(nameof(connectionString));\n     _indexName = indexName ?? throw new ArgumentNullException(nameof(indexName));\n }\n```\n\n\n\n\nRun this script to verify the base class constructor signature:\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Description: Check DocumentStoreBase constructor signatures\n\nast-grep --pattern $'class DocumentStoreBase<$_> {\n  $$$\n  public DocumentStoreBase($$$) {\n    $$$\n  }\n  $$$\n}'\n```\n\nLength of output: 0\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Find DocumentStoreBase class definition and constructors\nrg -n \"class DocumentStoreBase\" --type=cs -A 30\n```\n\nLength of output: 3329\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Search for constructor definitions in DocumentStoreBase files\nrg -n \"public DocumentStoreBase\\s*\\(\" --type=cs -A 5 -B 2\n```\n\nLength of output: 0\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check VectorDimension property in DocumentStoreBase\nrg -n \"VectorDimension\" --type=cs -B 2 -A 2\n```\n\nLength of output: 8171\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Find and display RedisVLDocumentStore constructor\nrg -n \"class RedisVLDocumentStore\" --type=cs -A 50\n```\n\nLength of output: 5825\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check if DocumentStoreBase has any constructors (including protected/private)\ncat src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs | head -100\n```\n\nLength of output: 4339\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Search for all constructors in DocumentStoreBase (entire file)\nrg -n \"DocumentStoreBase\\s*\\(\" src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs\n```\n\nLength of output: 0\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check full DocumentStoreBase for constructor patterns\ngrep -n \"public\\|protected\\|private\" src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs | grep -i \"documentstorebase\\|{.*}\"\n```\n\nLength of output: 167\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check if RedisVLDocumentStore implements VectorDimension property\nrg -n \"VectorDimension\" src/RetrievalAugmentedGeneration/DocumentStores/RedisVLDocumentStore.cs -A 2 -B 2\n```\n\nLength of output: 0\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Get entire RedisVLDocumentStore.cs file to check all properties\ncat src/RetrievalAugmentedGeneration/DocumentStores/RedisVLDocumentStore.cs\n```\n\nLength of output: 3019\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check what constructors can accept in abstract class\nast-grep --pattern $'public abstract class DocumentStoreBase<$_> {\n  $$$\n}'\n```\n\nLength of output: 0\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Get full DocumentStoreBase.cs to verify constructor definitions\ncat src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs\n```\n\nLength of output: 14172\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Compile check: try to find if RedisVLDocumentStore compiles by checking for VectorDimension override\nrg -n \"public override\" src/RetrievalAugmentedGeneration/DocumentStores/RedisVLDocumentStore.cs\n```\n\nLength of output: 295\n\n---\n\n\n\n</details>\n\n**RedisVLDocumentStore constructor is incompatible with base class and missing required property implementation.**\n\nThe constructor calls `: base(vectorDimension, numericOperations)`, but `DocumentStoreBase<T>` defines no constructor accepting parametersΓÇöit only has an implicit parameterless constructor. Additionally, `RedisVLDocumentStore` does not override the abstract `VectorDimension` property, which is required. \n\nThe `vectorDimension` and `numericOperations` parameters are passed to a non-existent base constructor and the `vectorDimension` value is never stored. Store it in a private field and override the `VectorDimension` property, following the pattern used by other document store implementations:\n\n```csharp\nprivate int _vectorDimension;\n\npublic RedisVLDocumentStore(\n    string connectionString,\n    string indexName,\n    int vectorDimension)\n{\n    _connectionString = connectionString ?? throw new ArgumentNullException(nameof(connectionString));\n    _indexName = indexName ?? throw new ArgumentNullException(nameof(indexName));\n    _vectorDimension = vectorDimension;\n}\n\npublic override int VectorDimension => _vectorDimension;\n```\n\nThe `INumericOperations<T> numericOperations` parameter is unnecessaryΓÇöuse the static `NumOps` field from the base class instead.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/RedisVLDocumentStore.cs\naround lines 27 to 36, the constructor incorrectly calls a non-existent base\nconstructor and accepts an unnecessary INumericOperations<T> parameter while not\nstoring vectorDimension or overriding the required VectorDimension property;\nchange the constructor to remove the base(...) call and the numericOperations\nparameter, add a private int _vectorDimension field set from the vectorDimension\nparameter, and implement public override int VectorDimension =>\n_vectorDimension; keep the null checks for connectionString and indexName and\nuse the base class static NumOps where numeric operations are needed.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T00:34:02Z",
    "updated_at": "2025-11-04T00:34:06Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261385",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261385"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261385"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261385/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 27,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 36,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 36,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261389",
    "pull_request_review_id": 3413480474,
    "id": 2488261389,
    "node_id": "PRRC_kwDOKSXUF86UT9sN",
    "diff_hunk": "@@ -0,0 +1,78 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// Redis-based vector document store for low-latency applications.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Redis with RedisSearch module provides fast vector similarity search with sub-millisecond latency.\n+/// Ideal for real-time applications requiring instant retrieval.\n+/// </remarks>\n+public class RedisVLDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly string _connectionString;\n+    private readonly string _indexName;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"RedisVLDocumentStore{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"connectionString\">The Redis connection string.</param>\n+    /// <param name=\"indexName\">The name of the index to use.</param>\n+    /// <param name=\"vectorDimension\">The dimensionality of document vectors.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public RedisVLDocumentStore(\n+        string connectionString,\n+        string indexName,\n+        int vectorDimension,\n+        INumericOperations<T> numericOperations)\n+        : base(vectorDimension, numericOperations)\n+    {\n+        _connectionString = connectionString ?? throw new ArgumentNullException(nameof(connectionString));\n+        _indexName = indexName ?? throw new ArgumentNullException(nameof(indexName));\n+    }\n+\n+    /// <summary>\n+    /// Adds a document to the Redis index.\n+    /// </summary>\n+    public override void AddDocument(Document<T> document)\n+    {\n+        if (document == null)\n+            throw new ArgumentNullException(nameof(document));\n+\n+        // TODO: Implement Redis vector indexing\n+        throw new NotImplementedException(\"Redis integration requires StackExchange.Redis implementation\");\n+    }\n+\n+    /// <summary>\n+    /// Retrieves documents similar to the query vector.\n+    /// </summary>\n+    public override IEnumerable<Document<T>> GetSimilar(Vector<T> queryVector, int topK)\n+    {\n+        if (queryVector == null)\n+            throw new ArgumentNullException(nameof(queryVector));\n+\n+        if (topK <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(topK), \"topK must be positive\");\n+\n+        // TODO: Implement Redis vector search\n+        throw new NotImplementedException(\"Redis integration requires StackExchange.Redis implementation\");\n+    }\n+\n+    /// <summary>\n+    /// Gets all documents from the index.\n+    /// </summary>\n+    public override IEnumerable<Document<T>> GetAllDocuments()\n+    {\n+        // TODO: Implement Redis scan operation\n+        throw new NotImplementedException(\"Redis integration requires StackExchange.Redis implementation\");\n+    }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/RedisVLDocumentStore.cs",
    "commit_id": "7fbd5910239204f8852637c980b53b76349cc626",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n<details>\n<summary>≡ƒº⌐ Analysis chain</summary>\n\n**Verify GetAllDocuments method exists in base class.**\n\n`GetAllDocuments()` is not present in the `DocumentStoreBase<T>` snippet (lines 28-348). Verify whether this method is part of the `IDocumentStore<T>` interface or if it should be removed.\n\nIf it's not part of the base API, remove this method. If needed for Redis-specific functionality, consider making it a separate utility method rather than an override.\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Description: Search for GetAllDocuments in IDocumentStore interface and base class\n\nast-grep --pattern $'GetAllDocuments'\n```\n\nLength of output: 0\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Search for IDocumentStore interface definition and GetAllDocuments method\nrg -n -A5 -B2 'interface IDocumentStore|class DocumentStoreBase' --type cs\n```\n\nLength of output: 1563\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Search for GetAllDocuments method definition or references\nrg -n 'GetAllDocuments' --type cs -C3\n```\n\nLength of output: 5909\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Search within the RedisVLDocumentStore file for complete context\nrg -n . 'src/RetrievalAugmentedGeneration/DocumentStores/RedisVLDocumentStore.cs' --type cs | head -100\n```\n\nLength of output: 3206\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Get full IDocumentStore interface definition\nrg -n 'interface IDocumentStore' -A100 src/Interfaces/IDocumentStore.cs --type cs\n```\n\nLength of output: 4944\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check DocumentStoreBase for abstract or virtual GetAllDocuments\nrg -n -B3 -A5 'abstract.*GetAllDocuments|virtual.*GetAllDocuments|GetAllDocuments' src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs --type cs\n```\n\nLength of output: 0\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Get complete IDocumentStore interface file\ncat src/Interfaces/IDocumentStore.cs\n```\n\nLength of output: 8545\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Get complete DocumentStoreBase file to check all methods\ncat src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs\n```\n\nLength of output: 14172\n\n---\n\n\n\n</details>\n\n**Remove the `override` keyword; GetAllDocuments is not part of IDocumentStore<T> or DocumentStoreBase<T>.**\n\nUsing `override` on a method that doesn't exist in the base class or interface is invalid. GetAllDocuments does not appear in either IDocumentStore<T> or DocumentStoreBase<T>. \n\nTo fix this, either:\n1. Remove the `override` keyword and implement it as a new public method (as InMemoryDocumentStore does)\n2. Add GetAllDocuments as an abstract member to IDocumentStore<T> and DocumentStoreBase<T> if it's part of the public API\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/RedisVLDocumentStore.cs\naround lines 68 to 72, the method declaration uses the override keyword but\nGetAllDocuments is not declared on IDocumentStore<T> or DocumentStoreBase<T>;\nremove the override and implement it as a normal public method (matching\nInMemoryDocumentStore) or alternatively add GetAllDocuments as an abstract\nmember to the interface/base if you intend it to be part of the public API ΓÇö\npick one: (A) remove override and keep the method public with the Redis scan\nTODO implementation, or (B) add the GetAllDocuments signature to\nIDocumentStore<T> and DocumentStoreBase<T> then implement with override here.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 15063ed to 8468360",
    "created_at": "2025-11-04T00:34:02Z",
    "updated_at": "2025-11-04T13:34:58Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261389",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261389"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261389"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261389/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 68,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 72,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 72,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261392",
    "pull_request_review_id": 3413480474,
    "id": 2488261392,
    "node_id": "PRRC_kwDOKSXUF86UT9sQ",
    "diff_hunk": "@@ -0,0 +1,78 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// Redis-based vector document store for low-latency applications.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Redis with RedisSearch module provides fast vector similarity search with sub-millisecond latency.\n+/// Ideal for real-time applications requiring instant retrieval.\n+/// </remarks>\n+public class RedisVLDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly string _connectionString;\n+    private readonly string _indexName;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"RedisVLDocumentStore{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"connectionString\">The Redis connection string.</param>\n+    /// <param name=\"indexName\">The name of the index to use.</param>\n+    /// <param name=\"vectorDimension\">The dimensionality of document vectors.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public RedisVLDocumentStore(\n+        string connectionString,\n+        string indexName,\n+        int vectorDimension,\n+        INumericOperations<T> numericOperations)\n+        : base(vectorDimension, numericOperations)\n+    {\n+        _connectionString = connectionString ?? throw new ArgumentNullException(nameof(connectionString));\n+        _indexName = indexName ?? throw new ArgumentNullException(nameof(indexName));\n+    }\n+\n+    /// <summary>\n+    /// Adds a document to the Redis index.\n+    /// </summary>\n+    public override void AddDocument(Document<T> document)\n+    {\n+        if (document == null)\n+            throw new ArgumentNullException(nameof(document));\n+\n+        // TODO: Implement Redis vector indexing\n+        throw new NotImplementedException(\"Redis integration requires StackExchange.Redis implementation\");\n+    }\n+\n+    /// <summary>\n+    /// Retrieves documents similar to the query vector.\n+    /// </summary>\n+    public override IEnumerable<Document<T>> GetSimilar(Vector<T> queryVector, int topK)\n+    {\n+        if (queryVector == null)\n+            throw new ArgumentNullException(nameof(queryVector));\n+\n+        if (topK <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(topK), \"topK must be positive\");\n+\n+        // TODO: Implement Redis vector search\n+        throw new NotImplementedException(\"Redis integration requires StackExchange.Redis implementation\");\n+    }\n+\n+    /// <summary>\n+    /// Gets all documents from the index.\n+    /// </summary>\n+    public override IEnumerable<Document<T>> GetAllDocuments()\n+    {\n+        // TODO: Implement Redis scan operation\n+        throw new NotImplementedException(\"Redis integration requires StackExchange.Redis implementation\");\n+    }\n+\n+    /// <summary>\n+    /// Gets the total number of documents in the index.\n+    /// </summary>\n+    public override int DocumentCount => 0; // TODO: Implement via Redis",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/RedisVLDocumentStore.cs",
    "commit_id": "7fbd5910239204f8852637c980b53b76349cc626",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_≡ƒ¢á∩╕Å Refactor suggestion_ | _≡ƒƒá Major_\n\n**DocumentCount placeholder implementation will break functionality.**\n\nThe property currently returns `0` as a placeholder. When implementing Redis support, ensure this property queries the actual document count from Redis. The current implementation will cause issues in base class validation (e.g., `ValidateVectorDocument` checks `DocumentCount > 0` to validate dimensions).\n\n\n\nUpdate the TODO comment to be more specific:\n\n```diff\n-public override int DocumentCount => 0; // TODO: Implement via Redis\n+public override int DocumentCount \n+{\n+    get\n+    {\n+        // TODO: Implement via Redis FT.INFO command to get index document count\n+        // For now, return 0 which may cause dimension validation to be skipped\n+        return 0;\n+    }\n+}\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    public override int DocumentCount \n    {\n        get\n        {\n            // TODO: Implement via Redis FT.INFO command to get index document count\n            // For now, return 0 which may cause dimension validation to be skipped\n            return 0;\n        }\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/RedisVLDocumentStore.cs\naround line 77, the DocumentCount property currently returns 0 which breaks\nvalidation; replace the placeholder with an implementation that queries Redis\nfor the actual stored document count (use the same Redis key/structure this\nclass uses to store documents and call the appropriate Redis command ΓÇö e.g.,\nSCARD for a set, HLEN for a hash, or LLEN for a list ΓÇö and return that integer),\nand update the TODO comment to explicitly state which Redis key and command are\nbeing used instead of the generic TODO.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T00:34:02Z",
    "updated_at": "2025-11-04T00:34:06Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261392",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261392"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261392"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261392/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": null,
    "original_line": 77,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 77,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261396",
    "pull_request_review_id": 3413480474,
    "id": 2488261396,
    "node_id": "PRRC_kwDOKSXUF86UT9sU",
    "diff_hunk": "@@ -0,0 +1,78 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// Lightweight SQLite-based vector store using the SQLite-VSS extension.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// SQLite-VSS provides vector similarity search in a serverless, file-based database,\n+/// ideal for development, testing, and edge deployments.\n+/// </remarks>\n+public class SQLiteVSSDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly string _databasePath;\n+    private readonly string _tableName;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"SQLiteVSSDocumentStore{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"databasePath\">The path to the SQLite database file.</param>\n+    /// <param name=\"tableName\">The name of the table to use.</param>\n+    /// <param name=\"vectorDimension\">The dimensionality of document vectors.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public SQLiteVSSDocumentStore(\n+        string databasePath,\n+        string tableName,\n+        int vectorDimension,\n+        INumericOperations<T> numericOperations)\n+        : base(vectorDimension, numericOperations)\n+    {\n+        _databasePath = databasePath ?? throw new ArgumentNullException(nameof(databasePath));\n+        _tableName = tableName ?? throw new ArgumentNullException(nameof(tableName));\n+    }\n+\n+    /// <summary>\n+    /// Adds a document to the SQLite database.\n+    /// </summary>\n+    public override void AddDocument(Document<T> document)\n+    {\n+        if (document == null)\n+            throw new ArgumentNullException(nameof(document));\n+\n+        // TODO: Implement SQLite-VSS insert\n+        throw new NotImplementedException(\"SQLite-VSS integration requires System.Data.SQLite implementation\");\n+    }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/SQLiteVSSDocumentStore.cs",
    "commit_id": "7fbd5910239204f8852637c980b53b76349cc626",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**AddDocument must be implemented before release**\n\nThis concrete store still throws `NotImplementedException`, so any attempt to persist a document will fail. Please wire up the SQLite-VSS insert (or keep the type abstract until itΓÇÖs functional).\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/SQLiteVSSDocumentStore.cs\naround lines 41 to 48, the AddDocument method currently throws\nNotImplementedException and must persist documents to SQLite-VSS; replace the\nTODO/exception with an actual insert: validate input, open a SQLite connection\n(using System.Data.SQLite or configured provider), begin a transaction, prepare\na parameterized INSERT or UPSERT into the VSS table(s) for the document (id,\nmetadata, vector blob/columns, created/updated timestamps), serialize any\ncomplex metadata as JSON if required, add the vector bytes/embedding properly\n(matching schema), execute the command, commit the transaction, and\nhandle/propagate exceptions while disposing connection/command via using\nstatements; alternatively, if schema or provider is not ready, change the class\nto be abstract or throw a descriptive NotSupportedException indicating the store\nis not enabled in this build.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 15063ed to 8468360",
    "created_at": "2025-11-04T00:34:02Z",
    "updated_at": "2025-11-04T13:35:03Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261396",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261396"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261396"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261396/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 41,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 48,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 48,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261402",
    "pull_request_review_id": 3413480474,
    "id": 2488261402,
    "node_id": "PRRC_kwDOKSXUF86UT9sa",
    "diff_hunk": "@@ -0,0 +1,78 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// Lightweight SQLite-based vector store using the SQLite-VSS extension.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// SQLite-VSS provides vector similarity search in a serverless, file-based database,\n+/// ideal for development, testing, and edge deployments.\n+/// </remarks>\n+public class SQLiteVSSDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly string _databasePath;\n+    private readonly string _tableName;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"SQLiteVSSDocumentStore{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"databasePath\">The path to the SQLite database file.</param>\n+    /// <param name=\"tableName\">The name of the table to use.</param>\n+    /// <param name=\"vectorDimension\">The dimensionality of document vectors.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public SQLiteVSSDocumentStore(\n+        string databasePath,\n+        string tableName,\n+        int vectorDimension,\n+        INumericOperations<T> numericOperations)\n+        : base(vectorDimension, numericOperations)\n+    {\n+        _databasePath = databasePath ?? throw new ArgumentNullException(nameof(databasePath));\n+        _tableName = tableName ?? throw new ArgumentNullException(nameof(tableName));\n+    }\n+\n+    /// <summary>\n+    /// Adds a document to the SQLite database.\n+    /// </summary>\n+    public override void AddDocument(Document<T> document)\n+    {\n+        if (document == null)\n+            throw new ArgumentNullException(nameof(document));\n+\n+        // TODO: Implement SQLite-VSS insert\n+        throw new NotImplementedException(\"SQLite-VSS integration requires System.Data.SQLite implementation\");\n+    }\n+\n+    /// <summary>\n+    /// Retrieves documents similar to the query vector.\n+    /// </summary>\n+    public override IEnumerable<Document<T>> GetSimilar(Vector<T> queryVector, int topK)\n+    {\n+        if (queryVector == null)\n+            throw new ArgumentNullException(nameof(queryVector));\n+\n+        if (topK <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(topK), \"topK must be positive\");\n+\n+        // TODO: Implement SQLite-VSS vector search\n+        throw new NotImplementedException(\"SQLite-VSS integration requires System.Data.SQLite implementation\");\n+    }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/SQLiteVSSDocumentStore.cs",
    "commit_id": "7fbd5910239204f8852637c980b53b76349cc626",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**GetSimilar cannot ship as a stub**\n\n`GetSimilar` also throws `NotImplementedException`, meaning similarity search is unusable. Implement the SQLite-VSS query (and consider moving the validation into `GetSimilarCore` so base-class checks still run).\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 15063ed to 602cbe2",
    "created_at": "2025-11-04T00:34:02Z",
    "updated_at": "2025-11-04T13:39:09Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261402",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261402"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261402"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261402/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 53,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 63,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 63,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261405",
    "pull_request_review_id": 3413480474,
    "id": 2488261405,
    "node_id": "PRRC_kwDOKSXUF86UT9sd",
    "diff_hunk": "@@ -0,0 +1,78 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// Lightweight SQLite-based vector store using the SQLite-VSS extension.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// SQLite-VSS provides vector similarity search in a serverless, file-based database,\n+/// ideal for development, testing, and edge deployments.\n+/// </remarks>\n+public class SQLiteVSSDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly string _databasePath;\n+    private readonly string _tableName;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"SQLiteVSSDocumentStore{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"databasePath\">The path to the SQLite database file.</param>\n+    /// <param name=\"tableName\">The name of the table to use.</param>\n+    /// <param name=\"vectorDimension\">The dimensionality of document vectors.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public SQLiteVSSDocumentStore(\n+        string databasePath,\n+        string tableName,\n+        int vectorDimension,\n+        INumericOperations<T> numericOperations)\n+        : base(vectorDimension, numericOperations)\n+    {\n+        _databasePath = databasePath ?? throw new ArgumentNullException(nameof(databasePath));\n+        _tableName = tableName ?? throw new ArgumentNullException(nameof(tableName));\n+    }\n+\n+    /// <summary>\n+    /// Adds a document to the SQLite database.\n+    /// </summary>\n+    public override void AddDocument(Document<T> document)\n+    {\n+        if (document == null)\n+            throw new ArgumentNullException(nameof(document));\n+\n+        // TODO: Implement SQLite-VSS insert\n+        throw new NotImplementedException(\"SQLite-VSS integration requires System.Data.SQLite implementation\");\n+    }\n+\n+    /// <summary>\n+    /// Retrieves documents similar to the query vector.\n+    /// </summary>\n+    public override IEnumerable<Document<T>> GetSimilar(Vector<T> queryVector, int topK)\n+    {\n+        if (queryVector == null)\n+            throw new ArgumentNullException(nameof(queryVector));\n+\n+        if (topK <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(topK), \"topK must be positive\");\n+\n+        // TODO: Implement SQLite-VSS vector search\n+        throw new NotImplementedException(\"SQLite-VSS integration requires System.Data.SQLite implementation\");\n+    }\n+\n+    /// <summary>\n+    /// Gets all documents from the table.\n+    /// </summary>\n+    public override IEnumerable<Document<T>> GetAllDocuments()\n+    {\n+        // TODO: Implement SQLite query\n+        throw new NotImplementedException(\"SQLite-VSS integration requires System.Data.SQLite implementation\");\n+    }\n+\n+    /// <summary>\n+    /// Gets the total number of documents in the table.\n+    /// </summary>\n+    public override int DocumentCount => 0; // TODO: Implement via SQLite query\n+}",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/SQLiteVSSDocumentStore.cs",
    "commit_id": "7fbd5910239204f8852637c980b53b76349cc626",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**GetAllDocuments/DocumentCount need real implementations**\n\n`GetAllDocuments` still throws, and `DocumentCount` always returns 0. Downstream callers (and base-class validation that depends on `DocumentCount`) will behave incorrectly. Please connect both members to real SQLite queries before merging.\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T00:34:02Z",
    "updated_at": "2025-11-04T00:34:06Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261405",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261405"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261405"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261405/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 68,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 78,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 78,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261412",
    "pull_request_review_id": 3413480474,
    "id": 2488261412,
    "node_id": "PRRC_kwDOKSXUF86UT9sk",
    "diff_hunk": "@@ -0,0 +1,92 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels;\n+\n+/// <summary>\n+/// Multi-modal embedding model supporting both text and images (e.g., CLIP).\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Enables creation of unified embeddings for text and images in the same vector space,\n+/// allowing cross-modal similarity search and retrieval.\n+/// </remarks>\n+public class MultiModalEmbeddingModel<T> : EmbeddingModelBase<T>\n+{\n+    private readonly string _modelPath;\n+    private readonly bool _normalizeEmbeddings;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"MultiModalEmbeddingModel{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"modelPath\">Path to the multi-modal model (e.g., CLIP).</param>\n+    /// <param name=\"normalizeEmbeddings\">Whether to normalize embeddings to unit length.</param>\n+    /// <param name=\"dimension\">The embedding dimension.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public MultiModalEmbeddingModel(\n+        string modelPath,\n+        bool normalizeEmbeddings,\n+        int dimension,\n+        INumericOperations<T> numericOperations)\n+        : base(dimension, numericOperations)\n+    {\n+        _modelPath = modelPath ?? throw new ArgumentNullException(nameof(modelPath));\n+        _normalizeEmbeddings = normalizeEmbeddings;\n+    }\n+\n+    /// <summary>\n+    /// Generates text embeddings.\n+    /// </summary>\n+    public override Vector<T> Embed(string text)\n+    {\n+        if (string.IsNullOrWhiteSpace(text))\n+            throw new ArgumentException(\"Text cannot be null or whitespace\", nameof(text));\n+\n+        // TODO: Implement text embedding with CLIP or similar\n+        throw new NotImplementedException(\"Multi-modal embedding requires CLIP/ONNX model integration\");\n+    }",
    "path": "src/RetrievalAugmentedGeneration/EmbeddingModels/MultiModalEmbeddingModel.cs",
    "commit_id": "7fbd5910239204f8852637c980b53b76349cc626",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Fix overrides to satisfy `EmbeddingModelBase<T>` contract.**\n\n`EmbeddingModelBase<T>` already implements `Embed(string)` and requires you to override `protected abstract Vector<T> EmbedCore(string text)`. Overriding `Embed` here wonΓÇÖt compile and leaves the abstract `EmbedCore` unimplemented, so the class fails to build. Please override the core method instead (keeping the TODO stub if needed):\n\n```diff\n-    public override Vector<T> Embed(string text)\n-    {\n-        if (string.IsNullOrWhiteSpace(text))\n-            throw new ArgumentException(\"Text cannot be null or whitespace\", nameof(text));\n-\n-        // TODO: Implement text embedding with CLIP or similar\n-        throw new NotImplementedException(\"Multi-modal embedding requires CLIP/ONNX model integration\");\n-    }\n+    protected override Vector<T> EmbedCore(string text)\n+    {\n+        // TODO: Implement text embedding with CLIP or similar\n+        throw new NotImplementedException(\"Multi-modal embedding requires CLIP/ONNX model integration\");\n+    }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    protected override Vector<T> EmbedCore(string text)\n    {\n        // TODO: Implement text embedding with CLIP or similar\n        throw new NotImplementedException(\"Multi-modal embedding requires CLIP/ONNX model integration\");\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/EmbeddingModels/MultiModalEmbeddingModel.cs\naround lines 41 to 48, the class incorrectly overrides public Vector<T>\nEmbed(string) which conflicts with EmbeddingModelBase<T> that already implements\nEmbed and requires you to override protected abstract Vector<T> EmbedCore(string\ntext); remove the public override Embed method, and instead add a protected\noverride Vector<T> EmbedCore(string text) method (keep the existing\nnull/whitespace check if desired) and inside leave the TODO stub or throw\nNotImplementedException(\"Multi-modal embedding requires CLIP/ONNX model\nintegration\") so the abstract contract is satisfied and the class will compile.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T00:34:02Z",
    "updated_at": "2025-11-04T00:34:06Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261412",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261412"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261412"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261412/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 41,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 48,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 48,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261415",
    "pull_request_review_id": 3413480474,
    "id": 2488261415,
    "node_id": "PRRC_kwDOKSXUF86UT9sn",
    "diff_hunk": "@@ -0,0 +1,92 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels;\n+\n+/// <summary>\n+/// Multi-modal embedding model supporting both text and images (e.g., CLIP).\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Enables creation of unified embeddings for text and images in the same vector space,\n+/// allowing cross-modal similarity search and retrieval.\n+/// </remarks>\n+public class MultiModalEmbeddingModel<T> : EmbeddingModelBase<T>\n+{\n+    private readonly string _modelPath;\n+    private readonly bool _normalizeEmbeddings;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"MultiModalEmbeddingModel{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"modelPath\">Path to the multi-modal model (e.g., CLIP).</param>\n+    /// <param name=\"normalizeEmbeddings\">Whether to normalize embeddings to unit length.</param>\n+    /// <param name=\"dimension\">The embedding dimension.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public MultiModalEmbeddingModel(\n+        string modelPath,\n+        bool normalizeEmbeddings,\n+        int dimension,\n+        INumericOperations<T> numericOperations)\n+        : base(dimension, numericOperations)\n+    {\n+        _modelPath = modelPath ?? throw new ArgumentNullException(nameof(modelPath));\n+        _normalizeEmbeddings = normalizeEmbeddings;\n+    }\n+\n+    /// <summary>\n+    /// Generates text embeddings.\n+    /// </summary>\n+    public override Vector<T> Embed(string text)\n+    {\n+        if (string.IsNullOrWhiteSpace(text))\n+            throw new ArgumentException(\"Text cannot be null or whitespace\", nameof(text));\n+\n+        // TODO: Implement text embedding with CLIP or similar\n+        throw new NotImplementedException(\"Multi-modal embedding requires CLIP/ONNX model integration\");\n+    }\n+\n+    /// <summary>\n+    /// Generates image embeddings from file path.\n+    /// </summary>\n+    /// <param name=\"imagePath\">Path to the image file.</param>\n+    /// <returns>The embedding vector for the image.</returns>\n+    public Vector<T> EmbedImage(string imagePath)\n+    {\n+        if (string.IsNullOrWhiteSpace(imagePath))\n+            throw new ArgumentException(\"Image path cannot be null or whitespace\", nameof(imagePath));\n+\n+        if (!File.Exists(imagePath))\n+            throw new FileNotFoundException($\"Image file not found: {imagePath}\");\n+\n+        // TODO: Implement image embedding with CLIP or similar\n+        throw new NotImplementedException(\"Multi-modal embedding requires CLIP/ONNX model integration\");\n+    }\n+\n+    /// <summary>\n+    /// Batch embedding generation for text.\n+    /// </summary>\n+    public override IEnumerable<Vector<T>> EmbedBatch(IEnumerable<string> texts)\n+    {\n+        if (texts == null)\n+            throw new ArgumentNullException(nameof(texts));\n+\n+        // TODO: Implement batch text embedding\n+        throw new NotImplementedException(\"Multi-modal embedding requires CLIP/ONNX model integration\");\n+    }",
    "path": "src/RetrievalAugmentedGeneration/EmbeddingModels/MultiModalEmbeddingModel.cs",
    "commit_id": "7fbd5910239204f8852637c980b53b76349cc626",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Override `EmbedBatchCore` with the correct signature.**\n\n`EmbeddingModelBase<T>.EmbedBatch` returns a `Matrix<T>` and delegates to `protected virtual Matrix<T> EmbedBatchCore(IList<string> texts)`. Overriding `EmbedBatch` here (and changing the return type to `IEnumerable<Vector<T>>`) breaks the inheritance contract and prevents compilation. Move the stub to `EmbedBatchCore` instead:\n\n```diff\n-    public override IEnumerable<Vector<T>> EmbedBatch(IEnumerable<string> texts)\n-    {\n-        if (texts == null)\n-            throw new ArgumentNullException(nameof(texts));\n-\n-        // TODO: Implement batch text embedding\n-        throw new NotImplementedException(\"Multi-modal embedding requires CLIP/ONNX model integration\");\n-    }\n+    protected override Matrix<T> EmbedBatchCore(IList<string> texts)\n+    {\n+        // TODO: Implement batch text embedding\n+        throw new NotImplementedException(\"Multi-modal embedding requires CLIP/ONNX model integration\");\n+    }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    protected override Matrix<T> EmbedBatchCore(IList<string> texts)\n    {\n        // TODO: Implement batch text embedding\n        throw new NotImplementedException(\"Multi-modal embedding requires CLIP/ONNX model integration\");\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/EmbeddingModels/MultiModalEmbeddingModel.cs\naround lines 70 to 77, the class incorrectly overrides EmbedBatch with the wrong\nreturn type (IEnumerable<Vector<T>>), breaking the base contract; replace this\noverride by implementing protected override Matrix<T>\nEmbedBatchCore(IList<string> texts) with the same stub logic (argument null\ncheck and throw new NotImplementedException(\"Multi-modal embedding requires\nCLIP/ONNX model integration\")), and remove the incorrect EmbedBatch override so\nthe base class can delegate to your EmbedBatchCore implementation.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T00:34:03Z",
    "updated_at": "2025-11-04T00:34:06Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261415",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261415"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261415"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261415/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 70,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 77,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 77,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261426",
    "pull_request_review_id": 3413480474,
    "id": 2488261426,
    "node_id": "PRRC_kwDOKSXUF86UT9sy",
    "diff_hunk": "@@ -0,0 +1,58 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Evaluation;\n+\n+/// <summary>\n+/// Evaluates the factual correctness of generated answers.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// Assesses whether the generated answer contains factually correct information\n+/// by comparing against ground truth or using fact-checking mechanisms.\n+/// </remarks>\n+public class AnswerCorrectnessMetric<T> : RAGMetricBase<T>\n+{\n+    private readonly string _llmEndpoint;\n+    private readonly string _llmApiKey;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"AnswerCorrectnessMetric{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"llmEndpoint\">The LLM API endpoint for fact checking.</param>\n+    /// <param name=\"llmApiKey\">The API key for the LLM service.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public AnswerCorrectnessMetric(\n+        string llmEndpoint,\n+        string llmApiKey,\n+        INumericOperations<T> numericOperations)\n+        : base(numericOperations)\n+    {\n+        _llmEndpoint = llmEndpoint ?? throw new ArgumentNullException(nameof(llmEndpoint));\n+        _llmApiKey = llmApiKey ?? throw new ArgumentNullException(nameof(llmApiKey));\n+    }\n+\n+    /// <summary>\n+    /// Evaluates answer correctness.\n+    /// </summary>\n+    protected override T EvaluateCore(\n+        string query,\n+        string answer,\n+        IEnumerable<Document<T>> retrievedDocuments,\n+        string groundTruth)\n+    {\n+        if (string.IsNullOrWhiteSpace(answer))\n+            return NumOps.Zero;\n+\n+        if (string.IsNullOrWhiteSpace(groundTruth))\n+        {\n+            // TODO: Use LLM to fact-check against retrieved documents\n+            throw new NotImplementedException(\"LLM-based fact checking requires HTTP client implementation\");\n+        }\n+\n+        // Simple correctness based on similarity to ground truth\n+        var similarity = StatisticsHelper.JaccardSimilarity(answer, groundTruth);\n+        return NumOps.FromDouble(similarity);\n+    }",
    "path": "src/RetrievalAugmentedGeneration/Evaluation/AnswerCorrectnessMetric.cs",
    "commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Restore required overrides to fix build.**\n\n`AnswerCorrectnessMetric<T>` never implements `Name`, `Description`, or `RequiresGroundTruth`, and it overrides the wrong `EvaluateCore` signature. The build fails with CS0115 and the static analysis errors in the pipeline (see ΓÇ£no suitable method found to overrideΓÇ¥). Align this class with `RAGMetricBase<T>`ΓÇÖs contract so it actually compiles.\n\n\n\n```diff\n@@\n-public class AnswerCorrectnessMetric<T> : RAGMetricBase<T>\n+public class AnswerCorrectnessMetric<T> : RAGMetricBase<T>\n {\n+    public override string Name => \"Answer Correctness\";\n+\n+    public override string Description =>\n+        \"Scores how closely the generated answer matches the provided ground truth.\";\n+\n+    protected override bool RequiresGroundTruth => true;\n@@\n-    protected override T EvaluateCore(\n-        string query,\n-        string answer,\n-        IEnumerable<Document<T>> retrievedDocuments,\n-        string groundTruth)\n+    protected override double EvaluateCore(\n+        GroundedAnswer<T> groundedAnswer,\n+        string? groundTruth)\n     {\n-        if (string.IsNullOrWhiteSpace(answer))\n-            return NumOps.Zero;\n+        if (groundedAnswer == null || string.IsNullOrWhiteSpace(groundedAnswer.Answer))\n+            return 0.0;\n \n         if (string.IsNullOrWhiteSpace(groundTruth))\n         {\n             // TODO: Use LLM to fact-check against retrieved documents\n             throw new NotImplementedException(\"LLM-based fact checking requires HTTP client implementation\");\n         }\n \n         // Simple correctness based on similarity to ground truth\n-        var similarity = StatisticsHelper.JaccardSimilarity(answer, groundTruth);\n-        return NumOps.FromDouble(similarity);\n+        var similarity = StatisticsHelper.JaccardSimilarity(groundedAnswer.Answer, groundTruth);\n+        return similarity;\n     }\n }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\npublic class AnswerCorrectnessMetric<T> : RAGMetricBase<T>\n{\n    public override string Name => \"Answer Correctness\";\n\n    public override string Description =>\n        \"Scores how closely the generated answer matches the provided ground truth.\";\n\n    protected override bool RequiresGroundTruth => true;\n\n    private readonly string _llmEndpoint;\n    private readonly string _llmApiKey;\n\n    /// <summary>\n    /// Initializes a new instance of the <see cref=\"AnswerCorrectnessMetric{T}\"/> class.\n    /// </summary>\n    /// <param name=\"llmEndpoint\">The LLM API endpoint for fact checking.</param>\n    /// <param name=\"llmApiKey\">The API key for the LLM service.</param>\n    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n    public AnswerCorrectnessMetric(\n        string llmEndpoint,\n        string llmApiKey,\n        INumericOperations<T> numericOperations)\n        : base(numericOperations)\n    {\n        _llmEndpoint = llmEndpoint ?? throw new ArgumentNullException(nameof(llmEndpoint));\n        _llmApiKey = llmApiKey ?? throw new ArgumentNullException(nameof(llmApiKey));\n    }\n\n    /// <summary>\n    /// Evaluates answer correctness.\n    /// </summary>\n    protected override double EvaluateCore(\n        GroundedAnswer<T> groundedAnswer,\n        string? groundTruth)\n    {\n        if (groundedAnswer == null || string.IsNullOrWhiteSpace(groundedAnswer.Answer))\n            return 0.0;\n\n        if (string.IsNullOrWhiteSpace(groundTruth))\n        {\n            // TODO: Use LLM to fact-check against retrieved documents\n            throw new NotImplementedException(\"LLM-based fact checking requires HTTP client implementation\");\n        }\n\n        // Simple correctness based on similarity to ground truth\n        var similarity = StatisticsHelper.JaccardSimilarity(groundedAnswer.Answer, groundTruth);\n        return similarity;\n    }\n}\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒº░ Tools</summary>\n\n<details>\n<summary>≡ƒ¬¢ GitHub Actions: Build</summary>\n\n[error] 39-39: CS0115: 'AnswerCorrectnessMetric<T>.EvaluateCore(string, string, IEnumerable<Document<T>>, string)': no suitable method found to override\n\n</details>\n<details>\n<summary>≡ƒ¬¢ GitHub Actions: Quality Gates (.NET)</summary>\n\n[error] 39-39: CS0115: 'AnswerCorrectnessMetric<T>.EvaluateCore(string, string, IEnumerable<Document<T>>, string)' overrides no suitable method to override. (dotnet publish step: dotnet publish src/AiDotNet.csproj -c Release -f net8.0 -o publish)\n\n</details>\n<details>\n<summary>≡ƒ¬¢ GitHub Check: Build All Frameworks</summary>\n\n[failure] 15-15: \n'AnswerCorrectnessMetric<T>' does not implement inherited abstract member 'RAGMetricBase<T>.EvaluateCore(GroundedAnswer<T>, string?)'\n\n---\n\n[failure] 15-15: \n'AnswerCorrectnessMetric<T>' does not implement inherited abstract member 'RAGMetricBase<T>.Description.get'\n\n---\n\n[failure] 15-15: \n'AnswerCorrectnessMetric<T>' does not implement inherited abstract member 'RAGMetricBase<T>.Name.get'\n\n---\n\n[failure] 15-15: \n'AnswerCorrectnessMetric<T>' does not implement inherited abstract member 'RAGMetricBase<T>.RequiresGroundTruth.get'\n\n---\n\n[failure] 39-39: \n'AnswerCorrectnessMetric<T>.EvaluateCore(string, string, IEnumerable<Document<T>>, string)': no suitable method found to override\n\n</details>\n<details>\n<summary>≡ƒ¬¢ GitHub Check: Publish Size Analysis</summary>\n\n[failure] 15-15: \n'AnswerCorrectnessMetric<T>' does not implement inherited abstract member 'RAGMetricBase<T>.Name.get'\n\n---\n\n[failure] 15-15: \n'AnswerCorrectnessMetric<T>' does not implement inherited abstract member 'RAGMetricBase<T>.EvaluateCore(GroundedAnswer<T>, string?)'\n\n---\n\n[failure] 15-15: \n'AnswerCorrectnessMetric<T>' does not implement inherited abstract member 'RAGMetricBase<T>.Description.get'\n\n---\n\n[failure] 15-15: \n'AnswerCorrectnessMetric<T>' does not implement inherited abstract member 'RAGMetricBase<T>.RequiresGroundTruth.get'\n\n---\n\n[failure] 39-39: \n'AnswerCorrectnessMetric<T>.EvaluateCore(string, string, IEnumerable<Document<T>>, string)': no suitable method found to override\n\n</details>\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commit 7fbd591",
    "created_at": "2025-11-04T00:34:03Z",
    "updated_at": "2025-11-04T01:56:44Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261426",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261426"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261426"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261426/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 14,
    "original_start_line": 15,
    "start_side": "RIGHT",
    "line": 49,
    "original_line": 57,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 57,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261441",
    "pull_request_review_id": 3413480474,
    "id": 2488261441,
    "node_id": "PRRC_kwDOKSXUF86UT9tB",
    "diff_hunk": "@@ -0,0 +1,56 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Evaluation;\n+\n+/// <summary>\n+/// Evaluates the relevance of retrieved context to the query.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// Measures how relevant the retrieved documents are to answering the query,\n+/// helping identify retrieval quality issues.\n+/// </remarks>\n+public class ContextRelevanceMetric<T> : RAGMetricBase<T>\n+{\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"ContextRelevanceMetric{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public ContextRelevanceMetric(INumericOperations<T> numericOperations)\n+        : base(numericOperations)\n+    {\n+    }",
    "path": "src/RetrievalAugmentedGeneration/Evaluation/ContextRelevanceMetric.cs",
    "commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Critical: Missing required abstract property implementations.**\n\nThe class does not implement the required abstract properties from `RAGMetricBase<T>`:\n- `Name` property\n- `Description` property  \n- `RequiresGroundTruth` property\n\nThis will prevent compilation.\n\n\n\nApply this diff to add the missing properties:\n\n```diff\n     /// <param name=\"numericOperations\">The numeric operations provider.</param>\n     public ContextRelevanceMetric(INumericOperations<T> numericOperations)\n         : base(numericOperations)\n     {\n     }\n+\n+    /// <summary>\n+    /// Gets the name of this metric.\n+    /// </summary>\n+    public override string Name => \"Context Relevance\";\n+\n+    /// <summary>\n+    /// Gets the description of what this metric measures.\n+    /// </summary>\n+    public override string Description => \"Measures the relevance of retrieved documents to the query using Jaccard similarity.\";\n+\n+    /// <summary>\n+    /// Gets a value indicating whether this metric requires ground truth data.\n+    /// </summary>\n+    public override bool RequiresGroundTruth => false;\n \n     /// <summary>\n     /// Evaluates context relevance.\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>≡ƒº░ Tools</summary>\n\n<details>\n<summary>≡ƒ¬¢ GitHub Check: Build All Frameworks</summary>\n\n[failure] 15-15: \n'ContextRelevanceMetric<T>' does not implement inherited abstract member 'RAGMetricBase<T>.EvaluateCore(GroundedAnswer<T>, string?)'\n\n---\n\n[failure] 15-15: \n'ContextRelevanceMetric<T>' does not implement inherited abstract member 'RAGMetricBase<T>.Description.get'\n\n---\n\n[failure] 15-15: \n'ContextRelevanceMetric<T>' does not implement inherited abstract member 'RAGMetricBase<T>.Name.get'\n\n---\n\n[failure] 15-15: \n'ContextRelevanceMetric<T>' does not implement inherited abstract member 'RAGMetricBase<T>.RequiresGroundTruth.get'\n\n</details>\n<details>\n<summary>≡ƒ¬¢ GitHub Check: Publish Size Analysis</summary>\n\n[failure] 15-15: \n'ContextRelevanceMetric<T>' does not implement inherited abstract member 'RAGMetricBase<T>.Name.get'\n\n---\n\n[failure] 15-15: \n'ContextRelevanceMetric<T>' does not implement inherited abstract member 'RAGMetricBase<T>.EvaluateCore(GroundedAnswer<T>, string?)'\n\n---\n\n[failure] 15-15: \n'ContextRelevanceMetric<T>' does not implement inherited abstract member 'RAGMetricBase<T>.Description.get'\n\n---\n\n[failure] 15-15: \n'ContextRelevanceMetric<T>' does not implement inherited abstract member 'RAGMetricBase<T>.RequiresGroundTruth.get'\n\n</details>\n\n</details>\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Evaluation/ContextRelevanceMetric.cs around\nlines 15 to 24, the class ContextRelevanceMetric<T> is missing required\noverrides from RAGMetricBase<T>; add overridden properties: override string Name\nreturning a concise identifier (e.g. \"ContextRelevance\"), override string\nDescription returning a short human-readable explanation of the metric, and\noverride bool RequiresGroundTruth returning the correct value (true if this\nmetric needs ground-truth labels, otherwise false); implement them as simple\nget-only properties on the class.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commit 7fbd591",
    "created_at": "2025-11-04T00:34:03Z",
    "updated_at": "2025-11-04T01:56:42Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261441",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261441"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261441"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261441/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 15,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 24,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 24,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261446",
    "pull_request_review_id": 3413480474,
    "id": 2488261446,
    "node_id": "PRRC_kwDOKSXUF86UT9tG",
    "diff_hunk": "@@ -0,0 +1,56 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Evaluation;\n+\n+/// <summary>\n+/// Evaluates the relevance of retrieved context to the query.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// Measures how relevant the retrieved documents are to answering the query,\n+/// helping identify retrieval quality issues.\n+/// </remarks>\n+public class ContextRelevanceMetric<T> : RAGMetricBase<T>\n+{\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"ContextRelevanceMetric{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public ContextRelevanceMetric(INumericOperations<T> numericOperations)\n+        : base(numericOperations)\n+    {\n+    }\n+\n+    /// <summary>\n+    /// Evaluates context relevance.\n+    /// </summary>\n+    protected override T EvaluateCore(\n+        string query,\n+        string answer,\n+        IEnumerable<Document<T>> retrievedDocuments,\n+        string groundTruth)\n+    {\n+        if (string.IsNullOrWhiteSpace(query))\n+            return NumOps.Zero;\n+\n+        var docs = retrievedDocuments?.ToList();\n+        if (docs == null || docs.Count == 0)\n+            return NumOps.Zero;\n+\n+        var totalRelevance = NumOps.Zero;\n+        var count = 0;\n+\n+        foreach (var doc in docs)\n+        {\n+            var relevance = StatisticsHelper.JaccardSimilarity(query, doc.Content);\n+            totalRelevance = NumOps.Add(totalRelevance, NumOps.FromDouble(relevance));\n+            count++;\n+        }\n+\n+        return count > 0 \n+            ? NumOps.Divide(totalRelevance, NumOps.FromInt(count))\n+            : NumOps.Zero;\n+    }",
    "path": "src/RetrievalAugmentedGeneration/Evaluation/ContextRelevanceMetric.cs",
    "commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Critical: Incorrect method signature prevents compilation.**\n\nThe `EvaluateCore` method signature does not match the abstract method in `RAGMetricBase<T>`. The base class expects `EvaluateCore(GroundedAnswer<T>, string?)` but this implementation uses outdated parameters `(string, string, IEnumerable<Document<T>>, string)`.\n\n\n\nApply this diff to fix the signature:\n\n```diff\n-    protected override T EvaluateCore(\n-        string query,\n-        string answer,\n-        IEnumerable<Document<T>> retrievedDocuments,\n-        string groundTruth)\n+    protected override T EvaluateCore(\n+        GroundedAnswer<T> groundedAnswer,\n+        string? groundTruth)\n     {\n-        if (string.IsNullOrWhiteSpace(query))\n+        if (string.IsNullOrWhiteSpace(groundedAnswer?.Query))\n             return NumOps.Zero;\n \n-        var docs = retrievedDocuments?.ToList();\n+        var docs = groundedAnswer?.RetrievedDocuments?.ToList();\n         if (docs == null || docs.Count == 0)\n             return NumOps.Zero;\n \n         var totalRelevance = NumOps.Zero;\n         var count = 0;\n \n         foreach (var doc in docs)\n         {\n-            var relevance = StatisticsHelper.JaccardSimilarity(query, doc.Content);\n+            var relevance = StatisticsHelper.JaccardSimilarity(groundedAnswer.Query, doc.Content);\n             totalRelevance = NumOps.Add(totalRelevance, NumOps.FromDouble(relevance));\n             count++;\n         }\n \n         return count > 0 \n             ? NumOps.Divide(totalRelevance, NumOps.FromInt(count))\n             : NumOps.Zero;\n     }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    protected override T EvaluateCore(\n        GroundedAnswer<T> groundedAnswer,\n        string? groundTruth)\n    {\n        if (string.IsNullOrWhiteSpace(groundedAnswer?.Query))\n            return NumOps.Zero;\n\n        var docs = groundedAnswer?.RetrievedDocuments?.ToList();\n        if (docs == null || docs.Count == 0)\n            return NumOps.Zero;\n\n        var totalRelevance = NumOps.Zero;\n        var count = 0;\n\n        foreach (var doc in docs)\n        {\n            var relevance = StatisticsHelper.JaccardSimilarity(groundedAnswer.Query, doc.Content);\n            totalRelevance = NumOps.Add(totalRelevance, NumOps.FromDouble(relevance));\n            count++;\n        }\n\n        return count > 0 \n            ? NumOps.Divide(totalRelevance, NumOps.FromInt(count))\n            : NumOps.Zero;\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒº░ Tools</summary>\n\n<details>\n<summary>≡ƒ¬¢ GitHub Check: Build All Frameworks</summary>\n\n[failure] 29-29: \n'ContextRelevanceMetric<T>.EvaluateCore(string, string, IEnumerable<Document<T>>, string)': no suitable method found to override\n\n</details>\n<details>\n<summary>≡ƒ¬¢ GitHub Check: Publish Size Analysis</summary>\n\n[failure] 29-29: \n'ContextRelevanceMetric<T>.EvaluateCore(string, string, IEnumerable<Document<T>>, string)': no suitable method found to override\n\n</details>\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commit 7fbd591",
    "created_at": "2025-11-04T00:34:03Z",
    "updated_at": "2025-11-04T01:56:35Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261446",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261446"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261446"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261446/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 29,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 55,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 55,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261457",
    "pull_request_review_id": 3413480474,
    "id": 2488261457,
    "node_id": "PRRC_kwDOKSXUF86UT9tR",
    "diff_hunk": "@@ -0,0 +1,45 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Evaluation;\n+\n+/// <summary>\n+/// Evaluates the model's robustness to noisy or irrelevant documents in the context.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// Tests how well the RAG system performs when irrelevant documents are included\n+/// in the retrieved context, measuring ability to filter signal from noise.\n+/// </remarks>\n+public class NoiseRobustnessMetric<T> : RAGMetricBase<T>",
    "path": "src/RetrievalAugmentedGeneration/Evaluation/NoiseRobustnessMetric.cs",
    "commit_id": "fb037f7036c63184b2417768a5e0933c503d9233",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Critical: Missing required abstract property implementations.**\n\nThe class inherits from `RAGMetricBase<T>`, which defines abstract properties that must be implemented:\n- `public abstract string Name { get; }`\n- `public abstract string Description { get; }`  \n- `protected abstract bool RequiresGroundTruth { get; }`\n\nWithout these implementations, the code will not compile.\n\n\n\nApply this diff to add the missing properties:\n\n```diff\n public class NoiseRobustnessMetric<T> : RAGMetricBase<T>\n {\n     private readonly T _noiseRatio;\n+\n+    /// <inheritdoc />\n+    public override string Name => \"Noise Robustness\";\n+\n+    /// <inheritdoc />\n+    public override string Description => \"Measures the model's ability to filter signal from noise when irrelevant documents are present in the context\";\n+\n+    /// <inheritdoc />\n+    protected override bool RequiresGroundTruth => true;\n \n     /// <summary>\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Evaluation/NoiseRobustnessMetric.cs around\nlines 6 to 14, the class inherits RAGMetricBase<T> but does not implement the\nrequired abstract properties; add implementations for public override string\nName { get; } returning a concise metric name (e.g., \"NoiseRobustness\"), public\noverride string Description { get; } returning a one-line description of the\nmetric, and protected override bool RequiresGroundTruth { get; } returning true\nor false depending on whether this metric needs ground truth (choose false if it\nevaluates filtering of irrelevant docs without labels); ensure signatures match\naccessibility and override keywords so the class compiles.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commit 7fbd591",
    "created_at": "2025-11-04T00:34:03Z",
    "updated_at": "2025-11-04T01:56:41Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261457",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261457"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261457"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261457/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 6,
    "original_start_line": 6,
    "start_side": "RIGHT",
    "line": 54,
    "original_line": 14,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 14,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261459",
    "pull_request_review_id": 3413480474,
    "id": 2488261459,
    "node_id": "PRRC_kwDOKSXUF86UT9tT",
    "diff_hunk": "@@ -0,0 +1,45 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Evaluation;\n+\n+/// <summary>\n+/// Evaluates the model's robustness to noisy or irrelevant documents in the context.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// Tests how well the RAG system performs when irrelevant documents are included\n+/// in the retrieved context, measuring ability to filter signal from noise.\n+/// </remarks>\n+public class NoiseRobustnessMetric<T> : RAGMetricBase<T>\n+{\n+    private readonly T _noiseRatio;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"NoiseRobustnessMetric{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"noiseRatio\">Ratio of noise documents to inject (0-1).</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public NoiseRobustnessMetric(T noiseRatio, INumericOperations<T> numericOperations)\n+        : base(numericOperations)\n+    {\n+        _noiseRatio = noiseRatio;\n+    }",
    "path": "src/RetrievalAugmentedGeneration/Evaluation/NoiseRobustnessMetric.cs",
    "commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Add validation for noiseRatio parameter.**\n\nThe XML documentation states that `noiseRatio` should be in the range 0-1, but the constructor does not validate this constraint. Invalid values could lead to incorrect evaluation results.\n\n\n\nApply this diff to add validation:\n\n```diff\n     public NoiseRobustnessMetric(T noiseRatio, INumericOperations<T> numericOperations)\n         : base(numericOperations)\n     {\n+        var numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\n+        \n+        // Validate noise ratio is between 0 and 1\n+        if (numOps.Compare(noiseRatio, numOps.Zero) < 0 || numOps.Compare(noiseRatio, numOps.One) > 0)\n+        {\n+            throw new ArgumentOutOfRangeException(nameof(noiseRatio), \"Noise ratio must be between 0 and 1\");\n+        }\n+        \n         _noiseRatio = noiseRatio;\n     }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    /// <summary>\n    /// Initializes a new instance of the <see cref=\"NoiseRobustnessMetric{T}\"/> class.\n    /// </summary>\n    /// <param name=\"noiseRatio\">Ratio of noise documents to inject (0-1).</param>\n    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n    public NoiseRobustnessMetric(T noiseRatio, INumericOperations<T> numericOperations)\n        : base(numericOperations)\n    {\n        var numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\n        \n        // Validate noise ratio is between 0 and 1\n        if (numOps.Compare(noiseRatio, numOps.Zero) < 0 || numOps.Compare(noiseRatio, numOps.One) > 0)\n        {\n            throw new ArgumentOutOfRangeException(nameof(noiseRatio), \"Noise ratio must be between 0 and 1\");\n        }\n        \n        _noiseRatio = noiseRatio;\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Evaluation/NoiseRobustnessMetric.cs around\nlines 18ΓÇô27, the constructor accepts noiseRatio but doesn't validate it against\nthe documented 0ΓÇô1 range; add a guard that uses the provided\nINumericOperations<T> to compare noiseRatio with its Zero and One values and\nthrow an ArgumentOutOfRangeException(nameof(noiseRatio)) (with a clear message)\nif noiseRatio is less than 0 or greater than 1; keep the numericOperations\nforwarding to base and ensure the validation happens before assigning\n_noiseRatio.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T00:34:03Z",
    "updated_at": "2025-11-04T00:34:07Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261459",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261459"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261459"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261459/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 73,
    "original_start_line": 18,
    "start_side": "RIGHT",
    "line": 86,
    "original_line": 27,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 27,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261469",
    "pull_request_review_id": 3413480474,
    "id": 2488261469,
    "node_id": "PRRC_kwDOKSXUF86UT9td",
    "diff_hunk": "@@ -0,0 +1,59 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.QueryExpansion;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.QueryExpansion;\n+\n+/// <summary>\n+/// Learned sparse encoder expansion using models like SPLADE.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// Uses a learned sparse model (e.g., SPLADE) to expand queries with relevant terms\n+/// weighted by their importance, combining benefits of sparse and dense retrieval.\n+/// </remarks>\n+public class LearnedSparseEncoderExpansion<T> : QueryExpansionBase<T>",
    "path": "src/RetrievalAugmentedGeneration/QueryExpansion/LearnedSparseEncoderExpansion.cs",
    "commit_id": "7fbd5910239204f8852637c980b53b76349cc626",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n<details>\n<summary>≡ƒº⌐ Analysis chain</summary>\n\n**Verify base class is generic.**\n\nThe provided snippet shows `QueryExpansionBase` as non-generic, but this class inherits from `QueryExpansionBase<T>`. While the PR summary indicates the framework was generified, please confirm the base class signature matches to avoid compilation errors.\n\n\n\nRun the following script to verify the base class signature:\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Verify QueryExpansionBase is generic and check its signature\n\necho \"=== Checking QueryExpansionBase class signature ===\"\nast-grep --pattern $'class QueryExpansionBase$$$'\n\necho -e \"\\n=== Checking for generic QueryExpansionBase<T> ===\"\nrg -nP 'class\\s+QueryExpansionBase<' --type=cs\n\necho -e \"\\n=== Checking ExpandQuery method signature ===\"\nrg -nP 'ExpandQuery\\s*\\(' src/RetrievalAugmentedGeneration/QueryExpansion/QueryExpansionBase.cs -A2\n```\n\nLength of output: 89241\n\n---\n\n\n\n</details>\n\n**Base class is non-generic; derived class inheritance will fail to compile.**\n\nThe base class `QueryExpansionBase` is not generic, but `LearnedSparseEncoderExpansion<T>` attempts to inherit from `QueryExpansionBase<T>`. This causes a compilation error. Either:\n1. Generify `QueryExpansionBase` to `QueryExpansionBase<T>`, or\n2. Remove the `<T>` parameter from the inheritance.\n\nGiven the PR generifies the framework and other query expansion classes also use `<T>` inheritance, the base class should be made generic.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn\nsrc/RetrievalAugmentedGeneration/QueryExpansion/LearnedSparseEncoderExpansion.cs\naround line 14, the derived class declares LearnedSparseEncoderExpansion<T> :\nQueryExpansionBase<T> but the base QueryExpansionBase is not generic; change the\nbase declaration to QueryExpansionBase<T>. To fix: open the QueryExpansionBase\nfile and make the class generic (public abstract class QueryExpansionBase<T>),\nupdate any type parameters, constructors and members to use T as needed, and\nadjust all other derived query-expansion classes and usages to reference\nQueryExpansionBase<T> (including using statements and unit tests) so the\ninheritance compiles consistently across the framework.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 1d6eceb to 7ab7f09",
    "created_at": "2025-11-04T00:34:04Z",
    "updated_at": "2025-11-04T02:27:29Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261469",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261469"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261469"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261469/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": null,
    "original_line": 14,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 14,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261472",
    "pull_request_review_id": 3413480474,
    "id": 2488261472,
    "node_id": "PRRC_kwDOKSXUF86UT9tg",
    "diff_hunk": "@@ -0,0 +1,57 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.QueryExpansion;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.QueryExpansion;",
    "path": "src/RetrievalAugmentedGeneration/QueryExpansion/MultiQueryExpansion.cs",
    "commit_id": "7fbd5910239204f8852637c980b53b76349cc626",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Remove redundant using statement.**\n\nThe using directive on line 2 imports the same namespace that is declared on line 4, making it redundant.\n\nApply this diff:\n\n```diff\n using AiDotNet.Interfaces;\n-using AiDotNet.RetrievalAugmentedGeneration.QueryExpansion;\n \n namespace AiDotNet.RetrievalAugmentedGeneration.QueryExpansion;\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\nusing AiDotNet.Interfaces;\n\nnamespace AiDotNet.RetrievalAugmentedGeneration.QueryExpansion;\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/QueryExpansion/MultiQueryExpansion.cs lines\n1-4, remove the redundant using directive \"using\nAiDotNet.RetrievalAugmentedGeneration.QueryExpansion;\" because that namespace is\ndeclared by the file's namespace; delete that using line and keep any other\nrequired usings (e.g., AiDotNet.Interfaces), then save the file and run a quick\nbuild/compile to ensure no missing references.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 15063ed to 8468360",
    "created_at": "2025-11-04T00:34:04Z",
    "updated_at": "2025-11-04T13:35:12Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261472",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261472"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261472"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261472/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 1,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 4,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 4,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261478",
    "pull_request_review_id": 3413480474,
    "id": 2488261478,
    "node_id": "PRRC_kwDOKSXUF86UT9tm",
    "diff_hunk": "@@ -0,0 +1,58 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.QueryExpansion;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.QueryExpansion;",
    "path": "src/RetrievalAugmentedGeneration/QueryExpansion/SubQueryExpansion.cs",
    "commit_id": "7fbd5910239204f8852637c980b53b76349cc626",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Remove redundant using statement.**\n\nLine 2 imports the same namespace that this file declares on line 4. A file doesn't need to import its own namespace.\n\n\n\nApply this diff:\n\n```diff\n using AiDotNet.Interfaces;\n-using AiDotNet.RetrievalAugmentedGeneration.QueryExpansion;\n \n namespace AiDotNet.RetrievalAugmentedGeneration.QueryExpansion;\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\nusing AiDotNet.Interfaces;\n\nnamespace AiDotNet.RetrievalAugmentedGeneration.QueryExpansion;\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/QueryExpansion/SubQueryExpansion.cs lines 1\nto 4, the using AiDotNet.RetrievalAugmentedGeneration.QueryExpansion; on line 2\nis redundant because the file is already declared in that namespace on line 4;\nremove the redundant using line to avoid unnecessary import.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 15063ed to 8468360",
    "created_at": "2025-11-04T00:34:04Z",
    "updated_at": "2025-11-04T13:35:14Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261478",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261478"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261478"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261478/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 1,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 4,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 4,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261483",
    "pull_request_review_id": 3413480474,
    "id": 2488261483,
    "node_id": "PRRC_kwDOKSXUF86UT9tr",
    "diff_hunk": "@@ -0,0 +1,75 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using AiDotNet.RetrievalAugmentedGeneration.Rerankers;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.RerankingStrategies;\n+\n+/// <summary>\n+/// Addresses the \"lost in the middle\" problem by strategically reordering documents.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// Research shows LLMs often ignore information in the middle of long contexts.\n+/// This reranker places most relevant documents at the beginning and end of the context.\n+/// </remarks>\n+public class LostInTheMiddleReranker<T> : RerankerBase<T>\n+{",
    "path": "src/RetrievalAugmentedGeneration/RerankingStrategies/LostInTheMiddleReranker.cs",
    "commit_id": "7fbd5910239204f8852637c980b53b76349cc626",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Missing required `ModifiesScores` property implementation.**\n\nThe base class `RerankerBase<T>` declares an abstract property `public abstract bool ModifiesScores { get; }` that must be implemented by all derived classes. This class is missing that implementation, which would prevent compilation.\n\n\n\nAdd the property implementation. Since this reranker only reorders documents without changing their relevance scores, it should return `false`:\n\n```diff\n public class LostInTheMiddleReranker<T> : RerankerBase<T>\n {\n+    /// <summary>\n+    /// Gets a value indicating whether this reranker modifies relevance scores.\n+    /// </summary>\n+    public override bool ModifiesScores => false;\n+\n     /// <summary>\n     /// Initializes a new instance of the <see cref=\"LostInTheMiddleReranker{T}\"/> class.\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\nusing AiDotNet.Helpers;\nusing AiDotNet.Interfaces;\nusing AiDotNet.RetrievalAugmentedGeneration.Models;\nusing AiDotNet.RetrievalAugmentedGeneration.Rerankers;\n\nnamespace AiDotNet.RetrievalAugmentedGeneration.RerankingStrategies;\n\n/// <summary>\n/// Addresses the \"lost in the middle\" problem by strategically reordering documents.\n/// </summary>\n/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n/// <remarks>\n/// Research shows LLMs often ignore information in the middle of long contexts.\n/// This reranker places most relevant documents at the beginning and end of the context.\n/// </remarks>\npublic class LostInTheMiddleReranker<T> : RerankerBase<T>\n{\n    /// <summary>\n    /// Gets a value indicating whether this reranker modifies relevance scores.\n    /// </summary>\n    public override bool ModifiesScores => false;\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn\nsrc/RetrievalAugmentedGeneration/RerankingStrategies/LostInTheMiddleReranker.cs\naround lines 1 to 17, implement the missing abstract property ModifiesScores\nfrom RerankerBase<T>; add a public override bool ModifiesScores { get; } that\nreturns false because this strategy only reorders documents and does not change\nrelevance scores.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T00:34:04Z",
    "updated_at": "2025-11-04T00:34:07Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261483",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261483"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261483"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261483/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 1,
    "original_start_line": 1,
    "start_side": "RIGHT",
    "line": 17,
    "original_line": 17,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 17,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261493",
    "pull_request_review_id": 3413480474,
    "id": 2488261493,
    "node_id": "PRRC_kwDOKSXUF86UT9t1",
    "diff_hunk": "@@ -0,0 +1,75 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using AiDotNet.RetrievalAugmentedGeneration.Rerankers;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.RerankingStrategies;\n+\n+/// <summary>\n+/// Addresses the \"lost in the middle\" problem by strategically reordering documents.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// Research shows LLMs often ignore information in the middle of long contexts.\n+/// This reranker places most relevant documents at the beginning and end of the context.\n+/// </remarks>\n+public class LostInTheMiddleReranker<T> : RerankerBase<T>\n+{\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"LostInTheMiddleReranker{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public LostInTheMiddleReranker(INumericOperations<T> numericOperations)\n+        : base(numericOperations)\n+    {\n+    }\n+\n+    /// <summary>\n+    /// Reranks documents to avoid the \"lost in the middle\" problem.\n+    /// </summary>\n+    /// <remarks>\n+    /// Strategy: Place most relevant at start, 2nd most relevant at end, 3rd in middle,\n+    /// alternating to distribute important documents to positions LLMs pay attention to.\n+    /// </remarks>\n+    public override IEnumerable<Document<T>> Rerank(string query, IEnumerable<Document<T>> documents, int topK)\n+    {\n+        if (string.IsNullOrWhiteSpace(query))\n+            throw new ArgumentException(\"Query cannot be null or whitespace\", nameof(query));\n+\n+        if (documents == null)\n+            throw new ArgumentNullException(nameof(documents));\n+\n+        if (topK <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(topK), \"topK must be positive\");\n+\n+        var docList = documents\n+            .OrderByDescending(d => d.HasRelevanceScore ? d.RelevanceScore : NumOps.Zero)\n+            .Take(topK)\n+            .ToList();\n+\n+        if (docList.Count <= 2)\n+            return docList;\n+\n+        var reordered = new List<Document<T>>(new Document<T>[docList.Count]);\n+        var startIdx = 0;\n+        var endIdx = docList.Count - 1;\n+        var useStart = true;\n+\n+        foreach (var doc in docList)\n+        {\n+            if (useStart)\n+            {\n+                reordered[startIdx] = doc;\n+                startIdx++;\n+            }\n+            else\n+            {\n+                reordered[endIdx] = doc;\n+                endIdx--;\n+            }\n+            useStart = !useStart;\n+        }\n+\n+        return reordered;\n+    }",
    "path": "src/RetrievalAugmentedGeneration/RerankingStrategies/LostInTheMiddleReranker.cs",
    "commit_id": "7fbd5910239204f8852637c980b53b76349cc626",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_≡ƒ¢á∩╕Å Refactor suggestion_ | _≡ƒƒá Major_\n\n**Consider implementing `RerankCore` instead of overriding `Rerank`.**\n\nThe base class `RerankerBase<T>` provides a template pattern where:\n- Public `Rerank(query, documents, topK)` handles validation and calls the base `Rerank(query, documents)`, then takes the top K results.\n- Derived classes should implement the protected abstract `RerankCore(string query, IList<Document<T>> documents)` method.\n\nBy overriding `Rerank` directly, this implementation:\n1. Duplicates validation logic already present in the base class (lines 36-43).\n2. Bypasses the base class's `Rerank(query, documents)` overload, making that entry point inconsistent.\n3. Deviates from the documented design pattern in the base class.\n\n\n\nRefactor to implement `RerankCore` instead:\n\n```diff\n-    public override IEnumerable<Document<T>> Rerank(string query, IEnumerable<Document<T>> documents, int topK)\n+    protected override IEnumerable<Document<T>> RerankCore(string query, IList<Document<T>> documents)\n     {\n-        if (string.IsNullOrWhiteSpace(query))\n-            throw new ArgumentException(\"Query cannot be null or whitespace\", nameof(query));\n-\n-        if (documents == null)\n-            throw new ArgumentNullException(nameof(documents));\n-\n-        if (topK <= 0)\n-            throw new ArgumentOutOfRangeException(nameof(topK), \"topK must be positive\");\n-\n-        var docList = documents\n+        // Sort by relevance score (descending)\n+        var docList = documents\n             .OrderByDescending(d => d.HasRelevanceScore ? d.RelevanceScore : NumOps.Zero)\n-            .Take(topK)\n             .ToList();\n \n         if (docList.Count <= 2)\n             return docList;\n```\n\nThen let callers use the base class's `Rerank(query, documents, topK)` method to handle the top-K selection.\n\n**Note:** If the pattern in this codebase is intentionally to override `Rerank` directly (as seen in `CohereReranker` and `DiversityReranker`), please verify this is consistent across all rerankers and consider updating the base class documentation to reflect the actual pattern.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T00:34:04Z",
    "updated_at": "2025-11-04T00:34:07Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261493",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261493"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488261493"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488261493/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 34,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 74,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 74,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263467",
    "pull_request_review_id": 3413484042,
    "id": 2488263467,
    "node_id": "PRRC_kwDOKSXUF86UT-Mr",
    "diff_hunk": "@@ -0,0 +1,179 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies;\n+\n+/// <summary>\n+/// Specialized splitter that correctly parses and chunks tabular data from documents.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// Handles various table formats (Markdown, CSV, HTML tables) and ensures table integrity\n+/// by keeping related rows together and preserving column headers.\n+/// </remarks>\n+public class TableAwareTextSplitter<T> : ChunkingStrategyBase<T>\n+{\n+    private readonly int _maxRowsPerChunk;\n+    private readonly bool _includeHeadersInEachChunk;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"TableAwareTextSplitter{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"maxRowsPerChunk\">Maximum number of table rows per chunk.</param>\n+    /// <param name=\"includeHeadersInEachChunk\">Whether to include table headers in each chunk.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public TableAwareTextSplitter(\n+        int maxRowsPerChunk,\n+        bool includeHeadersInEachChunk,\n+        INumericOperations<T> numericOperations)\n+        : base(numericOperations)\n+    {\n+        if (maxRowsPerChunk <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(maxRowsPerChunk), \"Max rows per chunk must be positive\");\n+            \n+        _maxRowsPerChunk = maxRowsPerChunk;\n+        _includeHeadersInEachChunk = includeHeadersInEachChunk;\n+    }\n+\n+    /// <summary>\n+    /// Splits text while preserving table structure.\n+    /// </summary>\n+    public override IEnumerable<Document<T>> Chunk(string text)\n+    {\n+        if (string.IsNullOrWhiteSpace(text))\n+            return Enumerable.Empty<Document<T>>();\n+\n+        var chunks = new List<Document<T>>();\n+        var lines = text.Split(new[] { \"\\r\\n\", \"\\r\", \"\\n\" }, StringSplitOptions.None);\n+        \n+        var i = 0;\n+        while (i < lines.Length)\n+        {\n+            // Check if current position is start of a table\n+            if (IsTableStart(lines, i))\n+            {\n+                var tableChunks = ProcessTable(lines, ref i);\n+                chunks.AddRange(tableChunks);\n+            }\n+            else\n+            {\n+                // Regular text line\n+                chunks.Add(new Document<T>\n+                {\n+                    Id = Guid.NewGuid().ToString(),\n+                    Content = lines[i],\n+                    Metadata = new Dictionary<string, object>\n+                    {\n+                        [\"type\"] = \"text\",\n+                        [\"chunkIndex\"] = chunks.Count\n+                    }\n+                });\n+                i++;\n+            }\n+        }\n+\n+        return chunks;\n+    }\n+\n+    private bool IsTableStart(string[] lines, int index)\n+    {\n+        if (index >= lines.Length)\n+            return false;\n+\n+        var line = lines[index].Trim();\n+        \n+        // Markdown table (starts with |)\n+        if (line.StartsWith(\"|\"))\n+            return true;\n+\n+        // CSV (contains commas)\n+        if (line.Contains(\",\") && index + 1 < lines.Length && lines[index + 1].Contains(\",\"))\n+            return true;\n+\n+        // HTML table\n+        if (line.StartsWith(\"<table\", StringComparison.OrdinalIgnoreCase))\n+            return true;\n+\n+        return false;\n+    }\n+\n+    private List<Document<T>> ProcessTable(string[] lines, ref int index)\n+    {\n+        var chunks = new List<Document<T>>();\n+        var tableStart = index;\n+        var headerRows = new List<string>();\n+        var dataRows = new List<string>();\n+\n+        // Extract table header\n+        while (index < lines.Length && IsTableRow(lines[index]))\n+        {\n+            if (headerRows.Count < 2) // Typically header + separator\n+            {\n+                headerRows.Add(lines[index]);\n+            }\n+            else\n+            {\n+                dataRows.Add(lines[index]);\n+            }\n+            index++;\n+\n+            // Create chunk when we hit max rows\n+            if (dataRows.Count >= _maxRowsPerChunk)\n+            {\n+                var chunkContent = new List<string>();\n+                if (_includeHeadersInEachChunk)\n+                {\n+                    chunkContent.AddRange(headerRows);\n+                }\n+                chunkContent.AddRange(dataRows);\n+\n+                chunks.Add(new Document<T>\n+                {\n+                    Id = Guid.NewGuid().ToString(),\n+                    Content = string.Join(Environment.NewLine, chunkContent),\n+                    Metadata = new Dictionary<string, object>\n+                    {\n+                        [\"type\"] = \"table\",\n+                        [\"chunkIndex\"] = chunks.Count,\n+                        [\"tableStart\"] = tableStart\n+                    }\n+                });\n+\n+                dataRows.Clear();\n+            }\n+        }",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/TableAwareTextSplitter.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Fix infinite loop when chunking HTML tables.**\n\n`ProcessTable` never advances `index` for the initial `<table>` (or `<thead>/<tbody>`) line, so `Chunk()` keeps calling it with the same `i`, leading to an infinite loop and pegging the caller thread. Please consume the opening/closing scaffold before entering the row loop.\n\n```diff\n-        var headerRows = new List<string>();\n-        var dataRows = new List<string>();\n+        var headerRows = new List<string>();\n+        var dataRows = new List<string>();\n+\n+        var trimmed = lines[index].Trim();\n+        if (trimmed.StartsWith(\"<table\", StringComparison.OrdinalIgnoreCase))\n+        {\n+            headerRows.Add(lines[index]);\n+            index++;\n+\n+            while (index < lines.Length &&\n+                   !IsTableRow(lines[index]) &&\n+                   !lines[index].Trim().StartsWith(\"</table\", StringComparison.OrdinalIgnoreCase))\n+            {\n+                headerRows.Add(lines[index]);\n+                index++;\n+            }\n+        }\n```\n\nAlso ensure you advance past the closing `</table>` once processing completes (same method).\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 1d6eceb to cfcf88f",
    "created_at": "2025-11-04T00:35:13Z",
    "updated_at": "2025-11-04T02:29:55Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488263467",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263467"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488263467"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263467/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 53,
    "original_start_line": 53,
    "start_side": "RIGHT",
    "line": 134,
    "original_line": 144,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 144,
    "position": 134,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263482",
    "pull_request_review_id": 3413484042,
    "id": 2488263482,
    "node_id": "PRRC_kwDOKSXUF86UT-M6",
    "diff_hunk": "@@ -0,0 +1,179 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies;\n+\n+/// <summary>\n+/// Specialized splitter that correctly parses and chunks tabular data from documents.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// Handles various table formats (Markdown, CSV, HTML tables) and ensures table integrity\n+/// by keeping related rows together and preserving column headers.\n+/// </remarks>\n+public class TableAwareTextSplitter<T> : ChunkingStrategyBase<T>\n+{\n+    private readonly int _maxRowsPerChunk;\n+    private readonly bool _includeHeadersInEachChunk;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"TableAwareTextSplitter{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"maxRowsPerChunk\">Maximum number of table rows per chunk.</param>\n+    /// <param name=\"includeHeadersInEachChunk\">Whether to include table headers in each chunk.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public TableAwareTextSplitter(\n+        int maxRowsPerChunk,\n+        bool includeHeadersInEachChunk,\n+        INumericOperations<T> numericOperations)\n+        : base(numericOperations)\n+    {\n+        if (maxRowsPerChunk <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(maxRowsPerChunk), \"Max rows per chunk must be positive\");\n+            \n+        _maxRowsPerChunk = maxRowsPerChunk;\n+        _includeHeadersInEachChunk = includeHeadersInEachChunk;\n+    }\n+\n+    /// <summary>\n+    /// Splits text while preserving table structure.\n+    /// </summary>\n+    public override IEnumerable<Document<T>> Chunk(string text)\n+    {\n+        if (string.IsNullOrWhiteSpace(text))\n+            return Enumerable.Empty<Document<T>>();\n+\n+        var chunks = new List<Document<T>>();\n+        var lines = text.Split(new[] { \"\\r\\n\", \"\\r\", \"\\n\" }, StringSplitOptions.None);\n+        \n+        var i = 0;\n+        while (i < lines.Length)\n+        {\n+            // Check if current position is start of a table\n+            if (IsTableStart(lines, i))\n+            {\n+                var tableChunks = ProcessTable(lines, ref i);\n+                chunks.AddRange(tableChunks);\n+            }\n+            else\n+            {\n+                // Regular text line\n+                chunks.Add(new Document<T>\n+                {\n+                    Id = Guid.NewGuid().ToString(),\n+                    Content = lines[i],\n+                    Metadata = new Dictionary<string, object>\n+                    {\n+                        [\"type\"] = \"text\",\n+                        [\"chunkIndex\"] = chunks.Count\n+                    }\n+                });\n+                i++;\n+            }\n+        }\n+\n+        return chunks;\n+    }\n+\n+    private bool IsTableStart(string[] lines, int index)\n+    {\n+        if (index >= lines.Length)\n+            return false;\n+\n+        var line = lines[index].Trim();\n+        \n+        // Markdown table (starts with |)\n+        if (line.StartsWith(\"|\"))\n+            return true;\n+\n+        // CSV (contains commas)\n+        if (line.Contains(\",\") && index + 1 < lines.Length && lines[index + 1].Contains(\",\"))\n+            return true;\n+\n+        // HTML table\n+        if (line.StartsWith(\"<table\", StringComparison.OrdinalIgnoreCase))\n+            return true;\n+\n+        return false;\n+    }\n+\n+    private List<Document<T>> ProcessTable(string[] lines, ref int index)\n+    {\n+        var chunks = new List<Document<T>>();\n+        var tableStart = index;\n+        var headerRows = new List<string>();\n+        var dataRows = new List<string>();\n+\n+        // Extract table header\n+        while (index < lines.Length && IsTableRow(lines[index]))\n+        {\n+            if (headerRows.Count < 2) // Typically header + separator\n+            {\n+                headerRows.Add(lines[index]);\n+            }\n+            else\n+            {\n+                dataRows.Add(lines[index]);\n+            }\n+            index++;\n+\n+            // Create chunk when we hit max rows\n+            if (dataRows.Count >= _maxRowsPerChunk)\n+            {\n+                var chunkContent = new List<string>();\n+                if (_includeHeadersInEachChunk)\n+                {\n+                    chunkContent.AddRange(headerRows);\n+                }\n+                chunkContent.AddRange(dataRows);\n+\n+                chunks.Add(new Document<T>\n+                {\n+                    Id = Guid.NewGuid().ToString(),\n+                    Content = string.Join(Environment.NewLine, chunkContent),\n+                    Metadata = new Dictionary<string, object>\n+                    {\n+                        [\"type\"] = \"table\",\n+                        [\"chunkIndex\"] = chunks.Count,\n+                        [\"tableStart\"] = tableStart\n+                    }\n+                });\n+\n+                dataRows.Clear();\n+            }\n+        }\n+\n+        // Add remaining rows\n+        if (dataRows.Count > 0)\n+        {\n+            var chunkContent = new List<string>();\n+            if (_includeHeadersInEachChunk)\n+            {\n+                chunkContent.AddRange(headerRows);\n+            }\n+            chunkContent.AddRange(dataRows);\n+\n+            chunks.Add(new Document<T>\n+            {\n+                Id = Guid.NewGuid().ToString(),\n+                Content = string.Join(Environment.NewLine, chunkContent),\n+                Metadata = new Dictionary<string, object>\n+                {\n+                    [\"type\"] = \"table\",\n+                    [\"chunkIndex\"] = chunks.Count,\n+                    [\"tableStart\"] = tableStart\n+                }",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/TableAwareTextSplitter.cs",
    "commit_id": "7fbd5910239204f8852637c980b53b76349cc626",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Keep chunkIndex globally monotonic.**\n\n`[\"chunkIndex\"] = chunks.Count` uses the *local* list inside `ProcessTable`, so each table restarts at 0 and duplicates earlier chunk indices. Pass the base count from `Chunk()` and add it to the local counter when emitting metadata.\n\n```diff\n-                var tableChunks = ProcessTable(lines, ref i);\n+                var tableChunks = ProcessTable(lines, ref i, chunks.Count);\n                 chunks.AddRange(tableChunks);\n```\n\n```diff\n-    private List<Document<T>> ProcessTable(string[] lines, ref int index)\n+    private List<Document<T>> ProcessTable(string[] lines, ref int index, int chunkBaseIndex)\n```\n\n```diff\n-                        [\"chunkIndex\"] = chunks.Count,\n+                        [\"chunkIndex\"] = chunkBaseIndex + chunks.Count,\n```\n\nApply the same adjustment to the final chunk metadata. \n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T00:35:13Z",
    "updated_at": "2025-11-04T00:35:17Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488263482",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263482"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488263482"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263482/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 55,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 165,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 165,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263489",
    "pull_request_review_id": 3413484042,
    "id": 2488263489,
    "node_id": "PRRC_kwDOKSXUF86UT-NB",
    "diff_hunk": "@@ -0,0 +1,179 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies;\n+\n+/// <summary>\n+/// Specialized splitter that correctly parses and chunks tabular data from documents.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// Handles various table formats (Markdown, CSV, HTML tables) and ensures table integrity\n+/// by keeping related rows together and preserving column headers.\n+/// </remarks>\n+public class TableAwareTextSplitter<T> : ChunkingStrategyBase<T>\n+{\n+    private readonly int _maxRowsPerChunk;\n+    private readonly bool _includeHeadersInEachChunk;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"TableAwareTextSplitter{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"maxRowsPerChunk\">Maximum number of table rows per chunk.</param>\n+    /// <param name=\"includeHeadersInEachChunk\">Whether to include table headers in each chunk.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public TableAwareTextSplitter(\n+        int maxRowsPerChunk,\n+        bool includeHeadersInEachChunk,\n+        INumericOperations<T> numericOperations)\n+        : base(numericOperations)\n+    {\n+        if (maxRowsPerChunk <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(maxRowsPerChunk), \"Max rows per chunk must be positive\");\n+            \n+        _maxRowsPerChunk = maxRowsPerChunk;\n+        _includeHeadersInEachChunk = includeHeadersInEachChunk;\n+    }\n+\n+    /// <summary>\n+    /// Splits text while preserving table structure.\n+    /// </summary>\n+    public override IEnumerable<Document<T>> Chunk(string text)\n+    {\n+        if (string.IsNullOrWhiteSpace(text))\n+            return Enumerable.Empty<Document<T>>();\n+\n+        var chunks = new List<Document<T>>();\n+        var lines = text.Split(new[] { \"\\r\\n\", \"\\r\", \"\\n\" }, StringSplitOptions.None);\n+        \n+        var i = 0;\n+        while (i < lines.Length)\n+        {\n+            // Check if current position is start of a table\n+            if (IsTableStart(lines, i))\n+            {\n+                var tableChunks = ProcessTable(lines, ref i);\n+                chunks.AddRange(tableChunks);\n+            }\n+            else\n+            {\n+                // Regular text line\n+                chunks.Add(new Document<T>\n+                {\n+                    Id = Guid.NewGuid().ToString(),\n+                    Content = lines[i],\n+                    Metadata = new Dictionary<string, object>\n+                    {\n+                        [\"type\"] = \"text\",\n+                        [\"chunkIndex\"] = chunks.Count\n+                    }\n+                });\n+                i++;\n+            }\n+        }\n+\n+        return chunks;\n+    }\n+\n+    private bool IsTableStart(string[] lines, int index)\n+    {\n+        if (index >= lines.Length)\n+            return false;\n+\n+        var line = lines[index].Trim();\n+        \n+        // Markdown table (starts with |)\n+        if (line.StartsWith(\"|\"))\n+            return true;\n+\n+        // CSV (contains commas)\n+        if (line.Contains(\",\") && index + 1 < lines.Length && lines[index + 1].Contains(\",\"))\n+            return true;\n+\n+        // HTML table\n+        if (line.StartsWith(\"<table\", StringComparison.OrdinalIgnoreCase))\n+            return true;\n+\n+        return false;\n+    }\n+\n+    private List<Document<T>> ProcessTable(string[] lines, ref int index)\n+    {\n+        var chunks = new List<Document<T>>();\n+        var tableStart = index;\n+        var headerRows = new List<string>();\n+        var dataRows = new List<string>();\n+\n+        // Extract table header\n+        while (index < lines.Length && IsTableRow(lines[index]))\n+        {\n+            if (headerRows.Count < 2) // Typically header + separator\n+            {\n+                headerRows.Add(lines[index]);\n+            }\n+            else\n+            {\n+                dataRows.Add(lines[index]);\n+            }\n+            index++;\n+\n+            // Create chunk when we hit max rows\n+            if (dataRows.Count >= _maxRowsPerChunk)\n+            {\n+                var chunkContent = new List<string>();\n+                if (_includeHeadersInEachChunk)\n+                {\n+                    chunkContent.AddRange(headerRows);\n+                }\n+                chunkContent.AddRange(dataRows);\n+\n+                chunks.Add(new Document<T>\n+                {\n+                    Id = Guid.NewGuid().ToString(),\n+                    Content = string.Join(Environment.NewLine, chunkContent),\n+                    Metadata = new Dictionary<string, object>\n+                    {\n+                        [\"type\"] = \"table\",\n+                        [\"chunkIndex\"] = chunks.Count,\n+                        [\"tableStart\"] = tableStart\n+                    }\n+                });\n+\n+                dataRows.Clear();\n+            }\n+        }\n+\n+        // Add remaining rows\n+        if (dataRows.Count > 0)\n+        {\n+            var chunkContent = new List<string>();\n+            if (_includeHeadersInEachChunk)\n+            {\n+                chunkContent.AddRange(headerRows);\n+            }\n+            chunkContent.AddRange(dataRows);\n+\n+            chunks.Add(new Document<T>\n+            {\n+                Id = Guid.NewGuid().ToString(),\n+                Content = string.Join(Environment.NewLine, chunkContent),\n+                Metadata = new Dictionary<string, object>\n+                {\n+                    [\"type\"] = \"table\",\n+                    [\"chunkIndex\"] = chunks.Count,\n+                    [\"tableStart\"] = tableStart\n+                }\n+            });\n+        }",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/TableAwareTextSplitter.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Preserve headers on the first chunk even when not duplicating them.**\n\nWhen `_includeHeadersInEachChunk` is `false`, header rows never get emitted, so consumers lose column names entirelyΓÇöcontrary to the class docstring. Always include headers on the first chunk (and optionally on later ones when requested).\n\n```diff\n-                if (_includeHeadersInEachChunk)\n+                if (_includeHeadersInEachChunk || chunks.Count == 0)\n                 {\n                     chunkContent.AddRange(headerRows);\n                 }\n```\n\nApply the same condition to the trailing chunk below.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/ChunkingStrategies/TableAwareTextSplitter.cs\naround lines 120 to 167, the current logic only adds headerRows when\n_includeHeadersInEachChunk is true, which omits headers on the first emitted\nchunk when duplication is disabled; change the header inclusion condition in\nboth the \"create chunk when we hit max rows\" block and the trailing \"Add\nremaining rows\" block to add headers when _includeHeadersInEachChunk is true OR\nthis is the first chunk (chunks.Count == 0), i.e., replace checks of if\n(_includeHeadersInEachChunk) with if (_includeHeadersInEachChunk || chunks.Count\n== 0) so the initial chunk always contains headers while later chunks only\ninclude them if requested.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T00:35:13Z",
    "updated_at": "2025-11-04T00:35:17Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488263489",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263489"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488263489"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263489/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 115,
    "original_start_line": 120,
    "start_side": "RIGHT",
    "line": 151,
    "original_line": 167,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 167,
    "position": 151,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263496",
    "pull_request_review_id": 3413484042,
    "id": 2488263496,
    "node_id": "PRRC_kwDOKSXUF86UT-NI",
    "diff_hunk": "@@ -0,0 +1,82 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// Qdrant-based document store built for performance and scalability with advanced filtering.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Qdrant provides high-performance vector similarity search with powerful filtering capabilities\n+/// and horizontal scalability for production workloads.\n+/// </remarks>\n+public class QdrantDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly string _endpoint;\n+    private readonly string _collectionName;\n+    private readonly string _apiKey;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"QdrantDocumentStore{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"endpoint\">The Qdrant endpoint URL.</param>\n+    /// <param name=\"collectionName\">The name of the collection to use.</param>\n+    /// <param name=\"apiKey\">The API key for authentication.</param>\n+    /// <param name=\"vectorDimension\">The dimensionality of document vectors.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public QdrantDocumentStore(\n+        string endpoint,\n+        string collectionName,\n+        string apiKey,\n+        int vectorDimension,\n+        INumericOperations<T> numericOperations)\n+        : base(vectorDimension, numericOperations)\n+    {\n+        _endpoint = endpoint ?? throw new ArgumentNullException(nameof(endpoint));\n+        _collectionName = collectionName ?? throw new ArgumentNullException(nameof(collectionName));\n+        _apiKey = apiKey ?? throw new ArgumentNullException(nameof(apiKey));\n+    }\n+\n+    /// <summary>\n+    /// Adds a document to the Qdrant collection.\n+    /// </summary>\n+    public override void AddDocument(Document<T> document)\n+    {\n+        if (document == null)\n+            throw new ArgumentNullException(nameof(document));\n+\n+        // TODO: Implement Qdrant upsert via REST API\n+        throw new NotImplementedException(\"Qdrant integration requires HTTP client implementation\");\n+    }\n+\n+    /// <summary>\n+    /// Retrieves documents similar to the query vector with optional filtering.\n+    /// </summary>\n+    public override IEnumerable<Document<T>> GetSimilar(Vector<T> queryVector, int topK)\n+    {\n+        if (queryVector == null)\n+            throw new ArgumentNullException(nameof(queryVector));\n+\n+        if (topK <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(topK), \"topK must be positive\");\n+\n+        // TODO: Implement Qdrant search via REST API\n+        throw new NotImplementedException(\"Qdrant integration requires HTTP client implementation\");\n+    }\n+\n+    /// <summary>\n+    /// Gets all documents from the collection.\n+    /// </summary>\n+    public override IEnumerable<Document<T>> GetAllDocuments()\n+    {\n+        // TODO: Implement Qdrant scroll API\n+        throw new NotImplementedException(\"Qdrant integration requires HTTP client implementation\");\n+    }\n+\n+    /// <summary>\n+    /// Gets the total number of documents in the collection.\n+    /// </summary>\n+    public override int DocumentCount => 0; // TODO: Implement via Qdrant API\n+}",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/QdrantDocumentStore.cs",
    "commit_id": "7fbd5910239204f8852637c980b53b76349cc626",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n<details>\n<summary>≡ƒº⌐ Analysis chain</summary>\n\n**Stub implementation: verify all abstract base class methods are implemented.**\n\nThis is a placeholder implementation with all methods throwing `NotImplementedException`. Ensure all abstract methods from `DocumentStoreBase<T>` are implemented (even as stubs):\n- `Clear()`\n- `GetByIdCore(string documentId)`\n- `RemoveCore(string documentId)`\n- Any others required by the base class\n\nThe constructor stores configuration (`_endpoint`, `_collectionName`, `_apiKey`) but these fields are unused. When implementing the HTTP client logic, ensure these are properly utilized for REST API calls.\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Description: Verify all abstract methods from DocumentStoreBase are implemented in QdrantDocumentStore\n\n# Find all abstract methods in DocumentStoreBase\necho \"=== Abstract methods in DocumentStoreBase ===\"\nrg -n 'protected abstract|public abstract' --type cs src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs\n\necho \"\"\necho \"=== Methods implemented in QdrantDocumentStore ===\"\nrg -n 'public override|protected override' --type cs src/RetrievalAugmentedGeneration/DocumentStores/QdrantDocumentStore.cs\n```\n\nLength of output: 959\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check if QdrantDocumentStore has implementations beyond line 82\n\necho \"=== Full QdrantDocumentStore file ===\"\nwc -l src/RetrievalAugmentedGeneration/DocumentStores/QdrantDocumentStore.cs\n\necho \"\"\necho \"=== All method signatures in QdrantDocumentStore ===\"\nrg -n 'public |protected |private ' --type cs src/RetrievalAugmentedGeneration/DocumentStores/QdrantDocumentStore.cs | head -30\n\necho \"\"\necho \"=== Checking VectorDimension implementation ===\"\nrg -n 'VectorDimension' --type cs src/RetrievalAugmentedGeneration/DocumentStores/QdrantDocumentStore.cs\n```\n\nLength of output: 464\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Locate QdrantDocumentStore.cs file\n\necho \"=== Searching for QdrantDocumentStore.cs ===\"\nfd -i qdrant --type f\n\necho \"\"\necho \"=== Checking if DocumentStores directory exists ===\"\nfd -type d DocumentStores\n\necho \"\"\necho \"=== List all .cs files in RetrievalAugmentedGeneration if it exists ===\"\nfd -type f -name \"*.cs\" RetrievalAugmentedGeneration/ 2>/dev/null | head -20\n```\n\nLength of output: 412\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Get full file content and verify implementations\n\necho \"=== Full QdrantDocumentStore.cs file ===\"\ncat src/RetrievalAugmentedGeneration/DocumentStores/QdrantDocumentStore.cs\n\necho \"\"\necho \"=== Line count ===\"\nwc -l src/RetrievalAugmentedGeneration/DocumentStores/QdrantDocumentStore.cs\n```\n\nLength of output: 3387\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check if abstract methods have default implementations in base class\n\necho \"=== DocumentStoreBase.cs clear method definition ===\"\nrg -A 5 'public abstract void Clear' src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs\n\necho \"\"\necho \"=== DocumentStoreBase.cs protected abstract methods ===\"\nrg -n -A 3 'protected abstract' src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs\n\necho \"\"\necho \"=== Check VectorDimension in base ===\"\nrg -n -B 1 -A 2 'public abstract int VectorDimension' src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs\n```\n\nLength of output: 1183\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check if VectorDimension is auto-implemented in base class\n\necho \"=== DocumentStoreBase constructor and VectorDimension ===\"\nrg -n -B 5 -A 10 'public DocumentStoreBase' src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs\n\necho \"\"\necho \"=== Check if VectorDimension has backing field ===\"\nrg -n 'VectorDimension' src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs\n```\n\nLength of output: 553\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Get full DocumentStoreBase to understand the pattern\n\necho \"=== Full DocumentStoreBase.cs ===\"\ncat src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs | head -100\n\necho \"\"\necho \"=== Search for VectorDimension implementation/initialization ===\"\ngrep -n \"VectorDimension\\|_vectorDimension\" src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs | head -15\n```\n\nLength of output: 4883\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Look for complete class definition and any other base classes\n\necho \"=== QdrantDocumentStore full class definition ===\"\nrg -n 'class QdrantDocumentStore' -A 1 src/RetrievalAugmentedGeneration/DocumentStores/QdrantDocumentStore.cs\n\necho \"\"\necho \"=== Check if there are any default/virtual implementations in DocumentStoreBase ===\"\ncat src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs | grep -n \"protected virtual\\|public virtual\\|=> \" | head -20\n\necho \"\"\necho \"=== Search for Clear() implementation in DocumentStoreBase ===\"\nrg -n 'Clear' src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs\n```\n\nLength of output: 773\n\n---\n\n\n\n</details>\n\n**Critical: QdrantDocumentStore has missing abstract member implementations.**\n\nThe class is missing implementations for six abstract members required by `DocumentStoreBase<T>`:\n- `VectorDimension` property\n- `Clear()` method\n- `AddCore(VectorDocument<T> vectorDocument)` method\n- `GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)` method\n- `GetByIdCore(string documentId)` method\n- `RemoveCore(string documentId)` method\n\nThese must be implemented (even as stubs throwing `NotImplementedException`) to make the code compile. The constructor stores `_endpoint`, `_collectionName`, and `_apiKey` but these fields remain unusedΓÇöensure they're utilized when implementing the HTTP client logic.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/QdrantDocumentStore.cs around\nlines 1 to 82, the class fails to implement six abstract members from\nDocumentStoreBase<T> (VectorDimension property, Clear(),\nAddCore(VectorDocument<T>), GetSimilarCore(Vector<T>, int, Dictionary<string,\nobject>), GetByIdCore(string), RemoveCore(string)), causing compilation errors;\nimplement each missing member (you may stub them by throwing\nNotImplementedException) and have VectorDimension return the\nbase/vectorDimension value or a stored backing field; ensure the existing\n_endpoint, _collectionName and _apiKey fields are referenced (even if only in\nNotImplementedException messages) so they are marked as used and ready for\nupcoming HTTP client logic.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T00:35:13Z",
    "updated_at": "2025-11-04T00:35:17Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488263496",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263496"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488263496"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263496/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 1,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 82,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 82,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263506",
    "pull_request_review_id": 3413484042,
    "id": 2488263506,
    "node_id": "PRRC_kwDOKSXUF86UT-NS",
    "diff_hunk": "@@ -0,0 +1,82 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// Qdrant-based document store built for performance and scalability with advanced filtering.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Qdrant provides high-performance vector similarity search with powerful filtering capabilities\n+/// and horizontal scalability for production workloads.\n+/// </remarks>\n+public class QdrantDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly string _endpoint;\n+    private readonly string _collectionName;\n+    private readonly string _apiKey;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"QdrantDocumentStore{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"endpoint\">The Qdrant endpoint URL.</param>\n+    /// <param name=\"collectionName\">The name of the collection to use.</param>\n+    /// <param name=\"apiKey\">The API key for authentication.</param>\n+    /// <param name=\"vectorDimension\">The dimensionality of document vectors.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public QdrantDocumentStore(\n+        string endpoint,\n+        string collectionName,\n+        string apiKey,\n+        int vectorDimension,\n+        INumericOperations<T> numericOperations)\n+        : base(vectorDimension, numericOperations)\n+    {\n+        _endpoint = endpoint ?? throw new ArgumentNullException(nameof(endpoint));\n+        _collectionName = collectionName ?? throw new ArgumentNullException(nameof(collectionName));\n+        _apiKey = apiKey ?? throw new ArgumentNullException(nameof(apiKey));\n+    }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/QdrantDocumentStore.cs",
    "commit_id": "7fbd5910239204f8852637c980b53b76349cc626",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n<details>\n<summary>≡ƒº⌐ Analysis chain</summary>\n\n**Verify null handling for `numericOperations` parameter.**\n\nThe constructor validates `endpoint`, `collectionName`, and `apiKey` for null, but `numericOperations` is passed to the base constructor without validation. Ensure the base class constructor handles null appropriately, or add a null check here for consistency.\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Description: Check how the base class constructor handles the numericOperations parameter\n\n# Find the DocumentStoreBase constructor signature\nast-grep --pattern 'class DocumentStoreBase<$_> {\n  $$$\n  DocumentStoreBase($$$) {\n    $$$\n  }\n  $$$\n}'\n```\n\nLength of output: 0\n\n---\n\n```\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Search for DocumentStoreBase class definition\nrg -nP 'class DocumentStoreBase' --type=cs -A 20\n```\n\nLength of output: 2271\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Find the file containing DocumentStoreBase\nfd -e cs | xargs grep -l 'class DocumentStoreBase'\n```\n\nLength of output: 68\n\n---\n\n```\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Search for DocumentStoreBase constructor\nrg -nP 'protected\\s+(DocumentStoreBase|constructor)|public\\s+(DocumentStoreBase|constructor)' src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs -A 10\n```\n\nLength of output: 0\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Get the full DocumentStoreBase.cs file to see constructor\ncat src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs\n```\n\nLength of output: 14172\n\n---\n\n```\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Get QdrantDocumentStore.cs to verify the constructor call\ncat src/RetrievalAugmentedGeneration/DocumentStores/QdrantDocumentStore.cs\n```\n\nLength of output: 3252\n\n---\n\n```\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Find IDocumentStore interface\nrg -nP 'interface IDocumentStore' --type=cs -A 30\n```\n\nLength of output: 2334\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check for compilation by looking for .csproj or build issues\nfd -e csproj | head -5\n```\n\nLength of output: 138\n\n---\n\n```\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Get full IDocumentStore interface\ncat src/Interfaces/IDocumentStore.cs\n```\n\nLength of output: 8545\n\n---\n\n\n\n</details>\n\n<!-- [scratchpad_start]\n[task review_file_1/1 review_segment_1/5]\nComplete analysis of QdrantDocumentStore critical issues\n\n[observations]\n- IDocumentStore<T> requires: Add(VectorDocument<T>), AddBatch(), GetSimilar(), GetSimilarWithFilters(), GetById(), Remove(), Clear(), VectorDimension property\n- DocumentStoreBase has parameterless constructor only (no parameters accepted)\n- QdrantDocumentStore calls: base(vectorDimension, numericOperations) - WILL NOT COMPILE\n- QdrantDocumentStore defines: AddDocument(), GetAllDocuments() - these don't exist in interface\n- QdrantDocumentStore missing overrides: Add(), AddBatch(), GetSimilarWithFilters(), GetById(), Remove(), Clear()\n- QdrantDocumentStore missing property override: VectorDimension\n\n[critical_issues]\n1. COMPILATION ERROR: base(vectorDimension, numericOperations) - DocumentStoreBase has no constructor accepting parameters\n2. API MISMATCH: Method names wrong (AddDocument vs Add, GetAllDocuments not in interface)\n3. INCOMPLETE STUB: Missing 6 required method overrides and VectorDimension property\n4. PARAMETER UNUSED: numericOperations not stored, base class uses static NumOps field\n5. DIMENSION LOST: vectorDimension parameter passed but not stored, VectorDimension property not overridden\n\n[analysis]\nOriginal review comment about null checking numericOperations is actually a red herring.\nThe REAL problems are:\n- Code does not compile (base constructor call invalid)\n- API structure fundamentally wrong (wrong method signatures)\n- Stub implementation incomplete (missing all required overrides)\n\n[conclusion]\nThis requires  tag. Multiple compilation errors and API mismatches.\n</scratchpad_end] -->\n\n**Fix base constructor call and complete API implementation.**\n\nThe code has several critical issues preventing compilation and correct implementation:\n\n1. **Base constructor call invalid**: `base(vectorDimension, numericOperations)` will failΓÇö`DocumentStoreBase` has only an implicit parameterless constructor. Store `vectorDimension` as a field and override the `VectorDimension` property instead.\n\n2. **Wrong method signatures**: \n   - Implement `Add(VectorDocument<T>)` not `AddDocument(Document<T>)`\n   - Remove `GetAllDocuments()` (not in `IDocumentStore`)\n   \n3. **Missing required overrides**: Add implementations (or `throw NotImplementedException`) for:\n   - `AddBatch(IEnumerable<VectorDocument<T>>)`\n   - `GetSimilarWithFilters(Vector<T>, int, Dictionary<string, object>)`\n   - `GetById(string)`\n   - `Remove(string)`\n   - `Clear()`\n   - `VectorDimension` property\n\n4. **Unused parameter**: The `numericOperations` parameter is unnecessaryΓÇö`DocumentStoreBase` uses static `NumOps` field. Remove this parameter or store it if needed for HTTP client initialization.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/QdrantDocumentStore.cs around\nlines 29-40, the constructor calls a non-existent base(vectorDimension,\nnumericOperations) and the class omits/renames required IDocumentStore<T\nmembers; fix by removing the invalid base(...) call, add a private readonly int\n_vectorDimension field assigned from the constructor parameter and override the\nVectorDimension property to return it, remove or stop accepting the unused\nnumericOperations parameter (or store it in a private field if truly needed),\nand implement the correct interface method signatures (Add(VectorDocument<T>),\nAddBatch(IEnumerable<VectorDocument<T>>), GetSimilar(...),\nGetSimilarWithFilters(...), GetById(string), Remove(string), Clear()) ΓÇö\nprovisionally implement bodies to throw NotImplementedException if full logic\nisnΓÇÖt ready and remove any non-interface methods like\nGetAllDocuments()/AddDocument so the class compiles and matches the\nIDocumentStore<T> API.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T00:35:14Z",
    "updated_at": "2025-11-04T00:35:17Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488263506",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263506"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488263506"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263506/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 29,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 40,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 40,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263512",
    "pull_request_review_id": 3413484042,
    "id": 2488263512,
    "node_id": "PRRC_kwDOKSXUF86UT-NY",
    "diff_hunk": "@@ -0,0 +1,82 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// Qdrant-based document store built for performance and scalability with advanced filtering.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Qdrant provides high-performance vector similarity search with powerful filtering capabilities\n+/// and horizontal scalability for production workloads.\n+/// </remarks>\n+public class QdrantDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly string _endpoint;\n+    private readonly string _collectionName;\n+    private readonly string _apiKey;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"QdrantDocumentStore{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"endpoint\">The Qdrant endpoint URL.</param>\n+    /// <param name=\"collectionName\">The name of the collection to use.</param>\n+    /// <param name=\"apiKey\">The API key for authentication.</param>\n+    /// <param name=\"vectorDimension\">The dimensionality of document vectors.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public QdrantDocumentStore(\n+        string endpoint,\n+        string collectionName,\n+        string apiKey,\n+        int vectorDimension,\n+        INumericOperations<T> numericOperations)\n+        : base(vectorDimension, numericOperations)\n+    {\n+        _endpoint = endpoint ?? throw new ArgumentNullException(nameof(endpoint));\n+        _collectionName = collectionName ?? throw new ArgumentNullException(nameof(collectionName));\n+        _apiKey = apiKey ?? throw new ArgumentNullException(nameof(apiKey));\n+    }\n+\n+    /// <summary>\n+    /// Adds a document to the Qdrant collection.\n+    /// </summary>\n+    public override void AddDocument(Document<T> document)\n+    {\n+        if (document == null)\n+            throw new ArgumentNullException(nameof(document));\n+\n+        // TODO: Implement Qdrant upsert via REST API\n+        throw new NotImplementedException(\"Qdrant integration requires HTTP client implementation\");\n+    }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/QdrantDocumentStore.cs",
    "commit_id": "7fbd5910239204f8852637c980b53b76349cc626",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n<details>\n<summary>≡ƒº⌐ Analysis chain</summary>\n\n**Verify alignment with base class API pattern.**\n\nThe base class snippet shows an `Add(VectorDocument<T>)` ΓåÆ `AddCore(VectorDocument<T>)` pattern, but this implementation overrides `AddDocument(Document<T>)`. Ensure this aligns with the updated base class API. If the pattern is to implement `AddCore` instead of overriding public methods, adjust accordingly to leverage base class validation logic.\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Description: Verify the DocumentStoreBase API for adding documents\n\n# Search for Add and AddDocument method signatures in DocumentStoreBase\nrg -n -A 3 'void Add(Document|void AddDocument\\(|void AddCore\\(' --type cs src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs\n```\n\nLength of output: 110\n\n---\n\n```\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Description: Verify DocumentStoreBase API pattern - fixed regex\n\n# Search for Add method signatures\necho \"=== Searching for Add methods ===\"\nrg -n -A 3 'void Add\\(' --type cs src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs\n\necho \"\"\necho \"=== Searching for AddDocument methods ===\"\nrg -n -A 3 'void AddDocument\\(' --type cs src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs\n\necho \"\"\necho \"=== Searching for AddCore methods ===\"\nrg -n -A 3 'void AddCore\\(' --type cs src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs\n\necho \"\"\necho \"=== Check QdrantDocumentStore class declaration and methods ===\"\nrg -n -A 10 'class QdrantDocumentStore' --type cs src/RetrievalAugmentedGeneration/DocumentStores/QdrantDocumentStore.cs\n```\n\nLength of output: 618\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Description: Discover DocumentStore files and verify API pattern\n\n# Find all DocumentStore related files\necho \"=== Locating DocumentStore files ===\"\nfd -e cs -i documentstore\n\necho \"\"\necho \"=== List directory structure of DocumentStores ===\"\nfd . -t d -i documentstores\n\necho \"\"\necho \"=== Find DocumentStoreBase ===\"\nfd DocumentStoreBase -e cs\n\necho \"\"\necho \"=== Find QdrantDocumentStore ===\"\nfd QdrantDocumentStore -e cs\n```\n\nLength of output: 1522\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Description: Inspect DocumentStoreBase and QdrantDocumentStore to verify API alignment\n\necho \"=== DocumentStoreBase.cs content ===\"\ncat src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs\n\necho \"\"\necho \"=== QdrantDocumentStore.cs content ===\"\ncat src/RetrievalAugmentedGeneration/DocumentStores/QdrantDocumentStore.cs\n```\n\nLength of output: 17502\n\n---\n\n\n\n</details>\n\n**Implement `AddCore` instead of `AddDocument` to follow the base class pattern.**\n\nThe base class `DocumentStoreBase<T>` expects derived classes to implement `protected abstract AddCore(VectorDocument<T>)`, not override `AddDocument()`. The base class provides public `Add(VectorDocument<T>)` which validates the input before delegating to `AddCore`. Override `AddCore` instead to align with the Template Method pattern used throughout the base class.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/QdrantDocumentStore.cs around\nlines 42 to 52, you implemented a public override AddDocument and threw\nNotImplementedException, but the base class DocumentStoreBase<T> expects derived\nstores to implement the protected abstract AddCore(VectorDocument<T>) method;\nremove or revert the override of AddDocument and instead implement the protected\noverride void AddCore(VectorDocument<T> document) (matching the base signature),\nmove the null-checks/validation to the base's public Add which will call\nAddCore, and implement the Qdrant upsert logic inside AddCore (using the HTTP\nclient integration) so the class follows the Template Method pattern used by the\nbase class.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 1d6eceb to 7ab7f09",
    "created_at": "2025-11-04T00:35:14Z",
    "updated_at": "2025-11-04T02:27:49Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488263512",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263512"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488263512"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263512/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 42,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 52,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 52,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263516",
    "pull_request_review_id": 3413484042,
    "id": 2488263516,
    "node_id": "PRRC_kwDOKSXUF86UT-Nc",
    "diff_hunk": "@@ -0,0 +1,82 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// Qdrant-based document store built for performance and scalability with advanced filtering.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Qdrant provides high-performance vector similarity search with powerful filtering capabilities\n+/// and horizontal scalability for production workloads.\n+/// </remarks>\n+public class QdrantDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly string _endpoint;\n+    private readonly string _collectionName;\n+    private readonly string _apiKey;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"QdrantDocumentStore{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"endpoint\">The Qdrant endpoint URL.</param>\n+    /// <param name=\"collectionName\">The name of the collection to use.</param>\n+    /// <param name=\"apiKey\">The API key for authentication.</param>\n+    /// <param name=\"vectorDimension\">The dimensionality of document vectors.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public QdrantDocumentStore(\n+        string endpoint,\n+        string collectionName,\n+        string apiKey,\n+        int vectorDimension,\n+        INumericOperations<T> numericOperations)\n+        : base(vectorDimension, numericOperations)\n+    {\n+        _endpoint = endpoint ?? throw new ArgumentNullException(nameof(endpoint));\n+        _collectionName = collectionName ?? throw new ArgumentNullException(nameof(collectionName));\n+        _apiKey = apiKey ?? throw new ArgumentNullException(nameof(apiKey));\n+    }\n+\n+    /// <summary>\n+    /// Adds a document to the Qdrant collection.\n+    /// </summary>\n+    public override void AddDocument(Document<T> document)\n+    {\n+        if (document == null)\n+            throw new ArgumentNullException(nameof(document));\n+\n+        // TODO: Implement Qdrant upsert via REST API\n+        throw new NotImplementedException(\"Qdrant integration requires HTTP client implementation\");\n+    }\n+\n+    /// <summary>\n+    /// Retrieves documents similar to the query vector with optional filtering.\n+    /// </summary>\n+    public override IEnumerable<Document<T>> GetSimilar(Vector<T> queryVector, int topK)\n+    {\n+        if (queryVector == null)\n+            throw new ArgumentNullException(nameof(queryVector));\n+\n+        if (topK <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(topK), \"topK must be positive\");\n+\n+        // TODO: Implement Qdrant search via REST API\n+        throw new NotImplementedException(\"Qdrant integration requires HTTP client implementation\");\n+    }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/QdrantDocumentStore.cs",
    "commit_id": "7fbd5910239204f8852637c980b53b76349cc626",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_≡ƒ¢á∩╕Å Refactor suggestion_ | _≡ƒƒá Major_\n\n**Implement `GetSimilarCore` instead of overriding `GetSimilar`.**\n\nThe base class provides a public `GetSimilar` method that delegates to `GetSimilarWithFilters`, which validates inputs and calls the protected abstract `GetSimilarCore`. By overriding `GetSimilar` directly, this implementation:\n- Bypasses the base class validation logic (`ValidateQueryVector`, `ValidateTopK`)\n- Duplicates validation code that already exists in the base class\n- Breaks the template method pattern and loses metadata filtering support\n\nImplement `GetSimilarCore(Vector<T>, int, Dictionary<string, object>)` instead to properly integrate with the base class infrastructure.\n\n\n\nBased on the DocumentStoreBase pattern, apply this refactor:\n\n```diff\n-    /// <summary>\n-    /// Retrieves documents similar to the query vector with optional filtering.\n-    /// </summary>\n-    public override IEnumerable<Document<T>> GetSimilar(Vector<T> queryVector, int topK)\n-    {\n-        if (queryVector == null)\n-            throw new ArgumentNullException(nameof(queryVector));\n-\n-        if (topK <= 0)\n-            throw new ArgumentOutOfRangeException(nameof(topK), \"topK must be positive\");\n-\n-        // TODO: Implement Qdrant search via REST API\n-        throw new NotImplementedException(\"Qdrant integration requires HTTP client implementation\");\n-    }\n+    /// <summary>\n+    /// Core implementation for retrieving similar documents with metadata filtering.\n+    /// </summary>\n+    protected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)\n+    {\n+        // TODO: Implement Qdrant search via REST API with filtering support\n+        throw new NotImplementedException(\"Qdrant integration requires HTTP client implementation\");\n+    }\n```\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T00:35:14Z",
    "updated_at": "2025-11-04T00:35:17Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488263516",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263516"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488263516"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263516/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 54,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 67,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 67,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263520",
    "pull_request_review_id": 3413484042,
    "id": 2488263520,
    "node_id": "PRRC_kwDOKSXUF86UT-Ng",
    "diff_hunk": "@@ -0,0 +1,82 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// Qdrant-based document store built for performance and scalability with advanced filtering.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Qdrant provides high-performance vector similarity search with powerful filtering capabilities\n+/// and horizontal scalability for production workloads.\n+/// </remarks>\n+public class QdrantDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly string _endpoint;\n+    private readonly string _collectionName;\n+    private readonly string _apiKey;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"QdrantDocumentStore{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"endpoint\">The Qdrant endpoint URL.</param>\n+    /// <param name=\"collectionName\">The name of the collection to use.</param>\n+    /// <param name=\"apiKey\">The API key for authentication.</param>\n+    /// <param name=\"vectorDimension\">The dimensionality of document vectors.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public QdrantDocumentStore(\n+        string endpoint,\n+        string collectionName,\n+        string apiKey,\n+        int vectorDimension,\n+        INumericOperations<T> numericOperations)\n+        : base(vectorDimension, numericOperations)\n+    {\n+        _endpoint = endpoint ?? throw new ArgumentNullException(nameof(endpoint));\n+        _collectionName = collectionName ?? throw new ArgumentNullException(nameof(collectionName));\n+        _apiKey = apiKey ?? throw new ArgumentNullException(nameof(apiKey));\n+    }\n+\n+    /// <summary>\n+    /// Adds a document to the Qdrant collection.\n+    /// </summary>\n+    public override void AddDocument(Document<T> document)\n+    {\n+        if (document == null)\n+            throw new ArgumentNullException(nameof(document));\n+\n+        // TODO: Implement Qdrant upsert via REST API\n+        throw new NotImplementedException(\"Qdrant integration requires HTTP client implementation\");\n+    }\n+\n+    /// <summary>\n+    /// Retrieves documents similar to the query vector with optional filtering.\n+    /// </summary>\n+    public override IEnumerable<Document<T>> GetSimilar(Vector<T> queryVector, int topK)\n+    {\n+        if (queryVector == null)\n+            throw new ArgumentNullException(nameof(queryVector));\n+\n+        if (topK <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(topK), \"topK must be positive\");\n+\n+        // TODO: Implement Qdrant search via REST API\n+        throw new NotImplementedException(\"Qdrant integration requires HTTP client implementation\");\n+    }\n+\n+    /// <summary>\n+    /// Gets all documents from the collection.\n+    /// </summary>\n+    public override IEnumerable<Document<T>> GetAllDocuments()\n+    {\n+        // TODO: Implement Qdrant scroll API\n+        throw new NotImplementedException(\"Qdrant integration requires HTTP client implementation\");\n+    }\n+\n+    /// <summary>\n+    /// Gets the total number of documents in the collection.\n+    /// </summary>\n+    public override int DocumentCount => 0; // TODO: Implement via Qdrant API\n+}",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/QdrantDocumentStore.cs",
    "commit_id": "7fbd5910239204f8852637c980b53b76349cc626",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Hardcoded `DocumentCount` return value causes misleading behavior.**\n\nReturning a constant `0` makes the store appear perpetually empty, which:\n- Bypasses base class vector dimension validation (checks `DocumentCount > 0`)\n- Misleads client code that checks whether the store contains documents\n- Silently fails rather than explicitly indicating the method is not implemented\n\nConsider throwing `NotImplementedException` instead to make the incomplete state explicit and consistent with other methods.\n\n\n\n```diff\n-    /// <summary>\n-    /// Gets the total number of documents in the collection.\n-    /// </summary>\n-    public override int DocumentCount => 0; // TODO: Implement via Qdrant API\n+    /// <summary>\n+    /// Gets the total number of documents in the collection.\n+    /// </summary>\n+    public override int DocumentCount => throw new NotImplementedException(\"Qdrant integration requires HTTP client implementation\");\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    /// <summary>\n    /// Gets the total number of documents in the collection.\n    /// </summary>\n    public override int DocumentCount => throw new NotImplementedException(\"Qdrant integration requires HTTP client implementation\");\n}\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/QdrantDocumentStore.cs around\nlines 78 to 82, the DocumentCount property currently returns a hardcoded 0 which\nfalsely signals an empty store; replace this with a clear failure or real\nimplementation ΓÇö either implement the count retrieval via the Qdrant API (query\ncollection/point count) and return the actual int, or if you cannot implement\nnow, throw a NotImplementedException to make the incomplete state explicit and\navoid misleading consumers and bypassing base-class checks.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T00:35:14Z",
    "updated_at": "2025-11-04T00:35:17Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488263520",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263520"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488263520"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263520/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 78,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 82,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 82,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263526",
    "pull_request_review_id": 3413484042,
    "id": 2488263526,
    "node_id": "PRRC_kwDOKSXUF86UT-Nm",
    "diff_hunk": "@@ -0,0 +1,65 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels;\n+\n+/// <summary>\n+/// Cohere embedding model integration for high-performance embeddings.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Cohere provides state-of-the-art embeddings with multiple model sizes optimized\n+/// for different use cases (English, multilingual, search, classification).\n+/// </remarks>\n+public class CohereEmbeddingModel<T> : EmbeddingModelBase<T>\n+{\n+    private readonly string _apiKey;\n+    private readonly string _model;\n+    private readonly string _inputType;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"CohereEmbeddingModel{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"apiKey\">The Cohere API key.</param>\n+    /// <param name=\"model\">The model name (e.g., \"embed-english-v3.0\").</param>\n+    /// <param name=\"inputType\">The input type (\"search_document\" or \"search_query\").</param>\n+    /// <param name=\"dimension\">The embedding dimension.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public CohereEmbeddingModel(\n+        string apiKey,\n+        string model,\n+        string inputType,\n+        int dimension,\n+        INumericOperations<T> numericOperations)\n+        : base(dimension, numericOperations)\n+    {\n+        _apiKey = apiKey ?? throw new ArgumentNullException(nameof(apiKey));\n+        _model = model ?? throw new ArgumentNullException(nameof(model));\n+        _inputType = inputType ?? throw new ArgumentNullException(nameof(inputType));\n+    }\n+\n+    /// <summary>\n+    /// Generates embeddings using Cohere API.\n+    /// </summary>\n+    public override Vector<T> Embed(string text)\n+    {\n+        if (string.IsNullOrWhiteSpace(text))\n+            throw new ArgumentException(\"Text cannot be null or whitespace\", nameof(text));\n+\n+        // TODO: Implement Cohere API call\n+        throw new NotImplementedException(\"Cohere integration requires HTTP client implementation\");\n+    }\n+\n+    /// <summary>\n+    /// Batch embedding generation.\n+    /// </summary>\n+    public override IEnumerable<Vector<T>> EmbedBatch(IEnumerable<string> texts)\n+    {\n+        if (texts == null)\n+            throw new ArgumentNullException(nameof(texts));\n+\n+        // TODO: Implement Cohere batch API call\n+        throw new NotImplementedException(\"Cohere integration requires HTTP client implementation\");\n+    }",
    "path": "src/RetrievalAugmentedGeneration/EmbeddingModels/CohereEmbeddingModel.cs",
    "commit_id": "7fbd5910239204f8852637c980b53b76349cc626",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Finish the Cohere integration before release.**  \n`Embed` and `EmbedBatch` are just TODO stubs that throw `NotImplementedException`, so any consumer calling into this model will crash immediately. We need the real Cohere Embed API call (with batching support) in place before landing this file.\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 15063ed to 8468360",
    "created_at": "2025-11-04T00:35:14Z",
    "updated_at": "2025-11-04T13:35:27Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488263526",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263526"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488263526"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263526/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 45,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 64,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 64,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263532",
    "pull_request_review_id": 3413484042,
    "id": 2488263532,
    "node_id": "PRRC_kwDOKSXUF86UT-Ns",
    "diff_hunk": "@@ -0,0 +1,69 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels;\n+\n+/// <summary>\n+/// Google PaLM embedding model integration.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Provides access to Google's PaLM (Pathways Language Model) embedding capabilities\n+/// through the Google Cloud Vertex AI platform.\n+/// </remarks>\n+public class GooglePalmEmbeddingModel<T> : EmbeddingModelBase<T>\n+{\n+    private readonly string _projectId;\n+    private readonly string _location;\n+    private readonly string _model;\n+    private readonly string _apiKey;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"GooglePalmEmbeddingModel{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"projectId\">The Google Cloud project ID.</param>\n+    /// <param name=\"location\">The Google Cloud location.</param>\n+    /// <param name=\"model\">The PaLM model name.</param>\n+    /// <param name=\"apiKey\">The API key for authentication.</param>\n+    /// <param name=\"dimension\">The embedding dimension.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public GooglePalmEmbeddingModel(\n+        string projectId,\n+        string location,\n+        string model,\n+        string apiKey,\n+        int dimension,\n+        INumericOperations<T> numericOperations)\n+        : base(dimension, numericOperations)\n+    {\n+        _projectId = projectId ?? throw new ArgumentNullException(nameof(projectId));\n+        _location = location ?? throw new ArgumentNullException(nameof(location));\n+        _model = model ?? throw new ArgumentNullException(nameof(model));\n+        _apiKey = apiKey ?? throw new ArgumentNullException(nameof(apiKey));\n+    }",
    "path": "src/RetrievalAugmentedGeneration/EmbeddingModels/GooglePalmEmbeddingModel.cs",
    "commit_id": "7fbd5910239204f8852637c980b53b76349cc626",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Avoid shipping a fresh PaLM client against retired endpoints.**  \nGoogle shut down the public PaLM API on August 15, 2024, and Vertex AI retired the textembedding-gecko family on May 24, 2025 in favor of Gemini Embedding models. Standing up a new `GooglePalmEmbeddingModel` today will therefore fail once it tries to talk to those endpoints. Please pivot this implementation to the currently supported Gemini embedding stack (for example `gemini-embedding-001`) before exposing it. ([ai.google.dev](https://ai.google.dev/palm_docs/deprecation?utm_source=openai))\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 1d6eceb to 7ab7f09",
    "created_at": "2025-11-04T00:35:14Z",
    "updated_at": "2025-11-04T02:27:52Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488263532",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263532"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488263532"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263532/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 22,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 44,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 44,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263536",
    "pull_request_review_id": 3413484042,
    "id": 2488263536,
    "node_id": "PRRC_kwDOKSXUF86UT-Nw",
    "diff_hunk": "@@ -0,0 +1,69 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels;\n+\n+/// <summary>\n+/// Google PaLM embedding model integration.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Provides access to Google's PaLM (Pathways Language Model) embedding capabilities\n+/// through the Google Cloud Vertex AI platform.\n+/// </remarks>\n+public class GooglePalmEmbeddingModel<T> : EmbeddingModelBase<T>\n+{\n+    private readonly string _projectId;\n+    private readonly string _location;\n+    private readonly string _model;\n+    private readonly string _apiKey;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"GooglePalmEmbeddingModel{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"projectId\">The Google Cloud project ID.</param>\n+    /// <param name=\"location\">The Google Cloud location.</param>\n+    /// <param name=\"model\">The PaLM model name.</param>\n+    /// <param name=\"apiKey\">The API key for authentication.</param>\n+    /// <param name=\"dimension\">The embedding dimension.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public GooglePalmEmbeddingModel(\n+        string projectId,\n+        string location,\n+        string model,\n+        string apiKey,\n+        int dimension,\n+        INumericOperations<T> numericOperations)\n+        : base(dimension, numericOperations)\n+    {\n+        _projectId = projectId ?? throw new ArgumentNullException(nameof(projectId));\n+        _location = location ?? throw new ArgumentNullException(nameof(location));\n+        _model = model ?? throw new ArgumentNullException(nameof(model));\n+        _apiKey = apiKey ?? throw new ArgumentNullException(nameof(apiKey));\n+    }\n+\n+    /// <summary>\n+    /// Generates embeddings using Google PaLM API.\n+    /// </summary>\n+    public override Vector<T> Embed(string text)\n+    {\n+        if (string.IsNullOrWhiteSpace(text))\n+            throw new ArgumentException(\"Text cannot be null or whitespace\", nameof(text));\n+\n+        // TODO: Implement Google PaLM API call\n+        throw new NotImplementedException(\"Google PaLM integration requires HTTP client implementation\");\n+    }\n+\n+    /// <summary>\n+    /// Batch embedding generation.\n+    /// </summary>\n+    public override IEnumerable<Vector<T>> EmbedBatch(IEnumerable<string> texts)\n+    {\n+        if (texts == null)\n+            throw new ArgumentNullException(nameof(texts));\n+\n+        // TODO: Implement Google PaLM batch API call\n+        throw new NotImplementedException(\"Google PaLM integration requires HTTP client implementation\");\n+    }",
    "path": "src/RetrievalAugmentedGeneration/EmbeddingModels/GooglePalmEmbeddingModel.cs",
    "commit_id": "7fbd5910239204f8852637c980b53b76349cc626",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Implement the embedding calls before merging.**  \nBoth `Embed` and `EmbedBatch` immediately throw `NotImplementedException`. Any code path that instantiates this model will crash the first time it embeds text, so the class cannot be used in production. Please wire these methods up to the actual service (or keep the type internal until they work) before we ship.\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 1d6eceb to 7ab7f09",
    "created_at": "2025-11-04T00:35:14Z",
    "updated_at": "2025-11-04T02:27:54Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488263536",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263536"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488263536"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263536/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 49,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 68,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 68,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263543",
    "pull_request_review_id": 3413484042,
    "id": 2488263543,
    "node_id": "PRRC_kwDOKSXUF86UT-N3",
    "diff_hunk": "@@ -0,0 +1,90 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels;\n+\n+/// <summary>\n+/// Fine-tuner for sentence transformer models on domain-specific data.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Enables fine-tuning of pre-trained sentence transformer models on custom datasets\n+/// to improve embedding quality for specific domains or tasks.\n+/// </remarks>\n+public class SentenceTransformersFineTuner<T> : EmbeddingModelBase<T>\n+{\n+    private readonly string _baseModelPath;\n+    private readonly string _outputModelPath;\n+    private readonly int _epochs;\n+    private readonly T _learningRate;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"SentenceTransformersFineTuner{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"baseModelPath\">Path to the base model to fine-tune.</param>\n+    /// <param name=\"outputModelPath\">Path where fine-tuned model will be saved.</param>\n+    /// <param name=\"epochs\">Number of training epochs.</param>\n+    /// <param name=\"learningRate\">Learning rate for fine-tuning.</param>\n+    /// <param name=\"dimension\">The embedding dimension.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public SentenceTransformersFineTuner(\n+        string baseModelPath,\n+        string outputModelPath,\n+        int epochs,\n+        T learningRate,\n+        int dimension,\n+        INumericOperations<T> numericOperations)\n+        : base(dimension, numericOperations)\n+    {\n+        _baseModelPath = baseModelPath ?? throw new ArgumentNullException(nameof(baseModelPath));\n+        _outputModelPath = outputModelPath ?? throw new ArgumentNullException(nameof(outputModelPath));\n+        \n+        if (epochs <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(epochs), \"Epochs must be positive\");\n+            \n+        _epochs = epochs;\n+        _learningRate = learningRate;\n+    }",
    "path": "src/RetrievalAugmentedGeneration/EmbeddingModels/SentenceTransformersFineTuner.cs",
    "commit_id": "7fbd5910239204f8852637c980b53b76349cc626",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n<details>\n<summary>≡ƒº⌐ Analysis chain</summary>\n\n**Critical: Missing abstract property implementations and constructor mismatch.**\n\nThe class extends `EmbeddingModelBase<T>` but does not implement the required abstract properties `EmbeddingDimension` and `MaxTokens`. According to the provided base class snippet, these properties must be overridden or the code will not compile.\n\nAdditionally, the constructor calls `base(dimension, numericOperations)` at line 38, but the base class snippet shows `NumOps` is initialized inline via `MathHelper.GetNumericOperations<T>()` rather than through a constructor parameter. This suggests either:\n1. The base class has been modified but the snippets are outdated, or  \n2. This constructor call is incorrect.\n\n\n\n\nAdd the missing property implementations:\n\n```diff\n+    /// <inheritdoc/>\n+    public override int EmbeddingDimension { get; }\n+    \n+    /// <inheritdoc/>\n+    public override int MaxTokens { get; }\n+\n     private readonly string _baseModelPath;\n     private readonly string _outputModelPath;\n     private readonly int _epochs;\n     private readonly T _learningRate;\n```\n\nThen update the constructor to initialize these properties:\n\n```diff\n     public SentenceTransformersFineTuner(\n         string baseModelPath,\n         string outputModelPath,\n         int epochs,\n         T learningRate,\n         int dimension,\n+        int maxTokens,\n         INumericOperations<T> numericOperations)\n         : base(dimension, numericOperations)\n     {\n         _baseModelPath = baseModelPath ?? throw new ArgumentNullException(nameof(baseModelPath));\n         _outputModelPath = outputModelPath ?? throw new ArgumentNullException(nameof(outputModelPath));\n         \n         if (epochs <= 0)\n             throw new ArgumentOutOfRangeException(nameof(epochs), \"Epochs must be positive\");\n             \n         _epochs = epochs;\n         _learningRate = learningRate;\n+        EmbeddingDimension = dimension;\n+        MaxTokens = maxTokens;\n     }\n```\n\nRun this script to verify the actual base class implementation:\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Verify EmbeddingModelBase constructor and abstract members\n\necho \"=== Checking EmbeddingModelBase constructors ===\"\nast-grep --pattern 'class EmbeddingModelBase<$_> {\n  $$$\n  public EmbeddingModelBase($$$) {\n    $$$\n  }\n  $$$\n}'\n\necho -e \"\\n=== Checking abstract properties ===\"\nrg -n -A2 'public abstract.*EmbeddingDimension|public abstract.*MaxTokens' --type cs\n\necho -e \"\\n=== Checking if properties are already implemented ===\"\nrg -n 'override.*EmbeddingDimension|override.*MaxTokens' src/RetrievalAugmentedGeneration/EmbeddingModels/SentenceTransformersFineTuner.cs\n```\n\nLength of output: 692\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Get full EmbeddingModelBase class definition including constructor\necho \"=== EmbeddingModelBase full definition ===\"\nrg -n -A50 'public class EmbeddingModelBase<' src/RetrievalAugmentedGeneration/Embeddings/EmbeddingModelBase.cs | head -100\n\necho -e \"\\n=== SentenceTransformersFineTuner full definition ===\"\ncat src/RetrievalAugmentedGeneration/EmbeddingModels/SentenceTransformersFineTuner.cs\n\necho -e \"\\n=== Check for _dimension or EmbeddingDimension assignment in SentenceTransformersFineTuner ===\"\nrg -n 'EmbeddingDimension|_dimension' src/RetrievalAugmentedGeneration/EmbeddingModels/SentenceTransformersFineTuner.cs\n```\n\nLength of output: 3833\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Get full EmbeddingModelBase.cs file\necho \"=== Full EmbeddingModelBase.cs ===\"\ncat src/RetrievalAugmentedGeneration/Embeddings/EmbeddingModelBase.cs\n\necho -e \"\\n=== Check if other implementations override the properties ===\"\nfind src/RetrievalAugmentedGeneration/EmbeddingModels -name \"*.cs\" -type f | xargs rg -l 'class.*EmbeddingModelBase'\n\necho -e \"\\n=== Show other implementations to see pattern ===\"\nfind src/RetrievalAugmentedGeneration/EmbeddingModels -name \"*.cs\" -type f | head -5 | xargs -I {} sh -c 'echo \"=== {} ===\" && head -80 {}'\n```\n\nLength of output: 22132\n\n---\n\n\n\n</details>\n\n**Critical: Invalid base constructor call and missing abstract property implementations.**\n\nThe base class `EmbeddingModelBase<T>` does not have a constructor accepting `dimension` and `numericOperations` parameters. The call to `base(dimension, numericOperations)` at line 38 will not compile.\n\nAdditionally, the class is missing implementations of the required abstract properties `EmbeddingDimension` and `MaxTokens`, which prevents compilation.\n\nThe class also overrides public `Embed()` and `EmbedBatch()` methods, but these are not virtual in the base class. The correct pattern is to implement protected abstract `EmbedCore()` instead (as shown by `HuggingFaceEmbeddingModel` and `LocalTransformerEmbedding`).\n\n**Required fixes:**\n\n1. Remove the incorrect base constructor call\n2. Add private fields for dimension and maxTokens\n3. Implement the abstract properties\n4. Rename methods to match the base class pattern (implement `EmbedCore()` instead of overriding `Embed()`)\n\n```diff\n public class SentenceTransformersFineTuner<T> : EmbeddingModelBase<T>\n {\n     private readonly string _baseModelPath;\n     private readonly string _outputModelPath;\n     private readonly int _epochs;\n     private readonly T _learningRate;\n+    private readonly int _dimension;\n+    private readonly int _maxTokens;\n+\n+    public override int EmbeddingDimension => _dimension;\n+    public override int MaxTokens => _maxTokens;\n\n     public SentenceTransformersFineTuner(\n         string baseModelPath,\n         string outputModelPath,\n         int epochs,\n         T learningRate,\n         int dimension,\n+        int maxTokens,\n         INumericOperations<T> numericOperations)\n-        : base(dimension, numericOperations)\n+        : base()\n     {\n         _baseModelPath = baseModelPath ?? throw new ArgumentNullException(nameof(baseModelPath));\n         _outputModelPath = outputModelPath ?? throw new ArgumentNullException(nameof(outputModelPath));\n         \n         if (epochs <= 0)\n             throw new ArgumentOutOfRangeException(nameof(epochs), \"Epochs must be positive\");\n         \n+        if (dimension <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(dimension), \"Dimension must be positive\");\n+        if (maxTokens <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(maxTokens), \"Max tokens must be positive\");\n+            \n         _epochs = epochs;\n         _learningRate = learningRate;\n+        _dimension = dimension;\n+        _maxTokens = maxTokens;\n     }\n```\n\nThen update `Embed()` and `EmbedBatch()` to implement the protected core methods:\n\n```diff\n-    public override Vector<T> Embed(string text)\n+    protected override Vector<T> EmbedCore(string text)\n     {\n         if (string.IsNullOrWhiteSpace(text))\n             throw new ArgumentException(\"Text cannot be null or whitespace\", nameof(text));\n         \n         throw new NotImplementedException(\"Fine-tuned model embedding requires model loading implementation\");\n     }\n\n-    public override IEnumerable<Vector<T>> EmbedBatch(IEnumerable<string> texts)\n+    protected override Matrix<T> EmbedBatchCore(IList<string> texts)\n     {\n-        if (texts == null)\n-            throw new ArgumentNullException(nameof(texts));\n+        // Optional: override for optimized batch processing, otherwise base implementation will call EmbedCore for each text\n         \n         throw new NotImplementedException(\"Fine-tuned model embedding requires model loading implementation\");\n     }\n```\n\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn\nsrc/RetrievalAugmentedGeneration/EmbeddingModels/SentenceTransformersFineTuner.cs\naround lines 15 to 48, remove the incorrect call to base(dimension,\nnumericOperations) (the base class has no such ctor), add private fields for\ndimension and maxTokens and set them in the ctor, implement the required\nabstract properties EmbeddingDimension and MaxTokens to return those fields, and\nreplace any public overrides of Embed()/EmbedBatch() with the protected core\nimplementations the base expects (e.g., protected override EmbedCore(...) and\nprotected override EmbedBatchCore(...)) so the class compiles and follows the\nbase-class extension pattern.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T00:35:15Z",
    "updated_at": "2025-11-04T00:35:18Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488263543",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263543"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488263543"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263543/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 15,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 48,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 48,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263547",
    "pull_request_review_id": 3413484042,
    "id": 2488263547,
    "node_id": "PRRC_kwDOKSXUF86UT-N7",
    "diff_hunk": "@@ -0,0 +1,90 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels;\n+\n+/// <summary>\n+/// Fine-tuner for sentence transformer models on domain-specific data.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Enables fine-tuning of pre-trained sentence transformer models on custom datasets\n+/// to improve embedding quality for specific domains or tasks.\n+/// </remarks>\n+public class SentenceTransformersFineTuner<T> : EmbeddingModelBase<T>\n+{\n+    private readonly string _baseModelPath;\n+    private readonly string _outputModelPath;\n+    private readonly int _epochs;\n+    private readonly T _learningRate;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"SentenceTransformersFineTuner{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"baseModelPath\">Path to the base model to fine-tune.</param>\n+    /// <param name=\"outputModelPath\">Path where fine-tuned model will be saved.</param>\n+    /// <param name=\"epochs\">Number of training epochs.</param>\n+    /// <param name=\"learningRate\">Learning rate for fine-tuning.</param>\n+    /// <param name=\"dimension\">The embedding dimension.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public SentenceTransformersFineTuner(\n+        string baseModelPath,\n+        string outputModelPath,\n+        int epochs,\n+        T learningRate,\n+        int dimension,\n+        INumericOperations<T> numericOperations)\n+        : base(dimension, numericOperations)\n+    {\n+        _baseModelPath = baseModelPath ?? throw new ArgumentNullException(nameof(baseModelPath));\n+        _outputModelPath = outputModelPath ?? throw new ArgumentNullException(nameof(outputModelPath));\n+        \n+        if (epochs <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(epochs), \"Epochs must be positive\");\n+            \n+        _epochs = epochs;\n+        _learningRate = learningRate;\n+    }\n+\n+    /// <summary>\n+    /// Fine-tunes the model on provided training data.\n+    /// </summary>\n+    /// <param name=\"trainingPairs\">Training pairs of (anchor, positive, negative) texts.</param>\n+    public void FineTune(IEnumerable<(string anchor, string positive, string negative)> trainingPairs)\n+    {\n+        if (trainingPairs == null)\n+            throw new ArgumentNullException(nameof(trainingPairs));\n+\n+        // TODO: Implement model fine-tuning\n+        // 1. Load base model\n+        // 2. Create training dataset from pairs\n+        // 3. Train using triplet loss or similar\n+        // 4. Save fine-tuned model\n+        throw new NotImplementedException(\"Fine-tuning requires ML framework integration\");\n+    }\n+\n+    /// <summary>\n+    /// Generates embeddings using the fine-tuned model.\n+    /// </summary>\n+    public override Vector<T> Embed(string text)\n+    {\n+        if (string.IsNullOrWhiteSpace(text))\n+            throw new ArgumentException(\"Text cannot be null or whitespace\", nameof(text));\n+\n+        // TODO: Implement embedding with fine-tuned model\n+        throw new NotImplementedException(\"Fine-tuned model embedding requires model loading implementation\");\n+    }\n+\n+    /// <summary>\n+    /// Batch embedding generation.\n+    /// </summary>\n+    public override IEnumerable<Vector<T>> EmbedBatch(IEnumerable<string> texts)\n+    {\n+        if (texts == null)\n+            throw new ArgumentNullException(nameof(texts));\n+\n+        // TODO: Implement batch embedding\n+        throw new NotImplementedException(\"Fine-tuned model embedding requires model loading implementation\");\n+    }",
    "path": "src/RetrievalAugmentedGeneration/EmbeddingModels/SentenceTransformersFineTuner.cs",
    "commit_id": "7fbd5910239204f8852637c980b53b76349cc626",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n<details>\n<summary>≡ƒº⌐ Analysis chain</summary>\n\n**Major: Implementation incomplete despite PR claiming completion.**\n\nAll core methods (`FineTune`, `Embed`, and `EmbedBatch`) throw `NotImplementedException`. While the PR summary states \"Implementations completed\" and \"closes #303\", this class provides no actual functionality.\n\nIf this is intentionally a placeholder for future ML framework integration, consider:\n1. Making the class `internal` rather than `public` to avoid exposing non-functional APIs\n2. Adding XML documentation warning that implementation requires external ML framework\n3. Tracking this as a separate work item rather than marking #303 as complete\n\n\n\n\nIf this is meant to be a stub, apply this diff to reduce public API surface:\n\n```diff\n-public class SentenceTransformersFineTuner<T> : EmbeddingModelBase<T>\n+internal class SentenceTransformersFineTuner<T> : EmbeddingModelBase<T>\n```\n\nAnd enhance the class documentation:\n\n```diff\n /// <summary>\n /// Fine-tuner for sentence transformer models on domain-specific data.\n+/// <para><b>Note:</b> This is a placeholder implementation that requires integration\n+/// with an external ML framework (e.g., ONNX Runtime, TorchSharp) to function.</para>\n /// </summary>\n```\n\nCheck if other embedding models in this PR have similar placeholder implementations:\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Search for other NotImplementedException in embedding models\n\necho \"=== Checking for NotImplementedException in embedding models ===\"\nrg -n 'NotImplementedException' src/RetrievalAugmentedGeneration/EmbeddingModels/ -C2\n\necho -e \"\\n=== Checking public vs internal class declarations ===\"\nrg -n '^public class.*EmbeddingModel' src/RetrievalAugmentedGeneration/EmbeddingModels/\n```\n\nLength of output: 8262\n\n---\n\n\n\n</details>\n\n**Critical: Systemic issueΓÇömultiple public embedding models expose non-functional APIs.**\n\nScript verification reveals this is not isolated to `SentenceTransformersFineTuner`. Five embedding models throw `NotImplementedException`:\n- `SentenceTransformersFineTuner` (3 methods)\n- `VoyageAIEmbeddingModel` (2 methods)\n- `GooglePalmEmbeddingModel` (2 methods)\n- `CohereEmbeddingModel` (2 methods)\n- `MultiModalEmbeddingModel` (5 methods)\n\nAll are declared `public`, contradicting the PR's claim that \"Implementations completed.\"\n\nEither:\n1. **Mark as internal**: These are stubs requiring external ML/HTTP client integration and should not be part of the public API surface.\n2. **Reclassify the PR**: This is a scaffolding/interface PR, not a feature completion. Update PR title/description and #303 tracking accordingly.\n\nIf proceeding as public stubs, enhance each class with XML documentation clearly stating implementation requires external framework integration (ONNX, HTTP client, etc.). Do not close #303 until functional implementations exist.\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 1d6eceb to 7ab7f09",
    "created_at": "2025-11-04T00:35:15Z",
    "updated_at": "2025-11-04T02:27:58Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488263547",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263547"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488263547"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263547/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 50,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 89,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 89,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263550",
    "pull_request_review_id": 3413484042,
    "id": 2488263550,
    "node_id": "PRRC_kwDOKSXUF86UT-N-",
    "diff_hunk": "@@ -0,0 +1,90 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels;\n+\n+/// <summary>\n+/// Fine-tuner for sentence transformer models on domain-specific data.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Enables fine-tuning of pre-trained sentence transformer models on custom datasets\n+/// to improve embedding quality for specific domains or tasks.\n+/// </remarks>\n+public class SentenceTransformersFineTuner<T> : EmbeddingModelBase<T>\n+{\n+    private readonly string _baseModelPath;\n+    private readonly string _outputModelPath;\n+    private readonly int _epochs;\n+    private readonly T _learningRate;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"SentenceTransformersFineTuner{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"baseModelPath\">Path to the base model to fine-tune.</param>\n+    /// <param name=\"outputModelPath\">Path where fine-tuned model will be saved.</param>\n+    /// <param name=\"epochs\">Number of training epochs.</param>\n+    /// <param name=\"learningRate\">Learning rate for fine-tuning.</param>\n+    /// <param name=\"dimension\">The embedding dimension.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public SentenceTransformersFineTuner(\n+        string baseModelPath,\n+        string outputModelPath,\n+        int epochs,\n+        T learningRate,\n+        int dimension,\n+        INumericOperations<T> numericOperations)\n+        : base(dimension, numericOperations)\n+    {\n+        _baseModelPath = baseModelPath ?? throw new ArgumentNullException(nameof(baseModelPath));\n+        _outputModelPath = outputModelPath ?? throw new ArgumentNullException(nameof(outputModelPath));\n+        \n+        if (epochs <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(epochs), \"Epochs must be positive\");\n+            \n+        _epochs = epochs;\n+        _learningRate = learningRate;\n+    }\n+\n+    /// <summary>\n+    /// Fine-tunes the model on provided training data.\n+    /// </summary>\n+    /// <param name=\"trainingPairs\">Training pairs of (anchor, positive, negative) texts.</param>\n+    public void FineTune(IEnumerable<(string anchor, string positive, string negative)> trainingPairs)\n+    {\n+        if (trainingPairs == null)\n+            throw new ArgumentNullException(nameof(trainingPairs));\n+\n+        // TODO: Implement model fine-tuning\n+        // 1. Load base model\n+        // 2. Create training dataset from pairs\n+        // 3. Train using triplet loss or similar\n+        // 4. Save fine-tuned model\n+        throw new NotImplementedException(\"Fine-tuning requires ML framework integration\");\n+    }\n+\n+    /// <summary>\n+    /// Generates embeddings using the fine-tuned model.\n+    /// </summary>\n+    public override Vector<T> Embed(string text)\n+    {\n+        if (string.IsNullOrWhiteSpace(text))\n+            throw new ArgumentException(\"Text cannot be null or whitespace\", nameof(text));\n+\n+        // TODO: Implement embedding with fine-tuned model\n+        throw new NotImplementedException(\"Fine-tuned model embedding requires model loading implementation\");\n+    }\n+\n+    /// <summary>\n+    /// Batch embedding generation.\n+    /// </summary>\n+    public override IEnumerable<Vector<T>> EmbedBatch(IEnumerable<string> texts)\n+    {\n+        if (texts == null)\n+            throw new ArgumentNullException(nameof(texts));\n+\n+        // TODO: Implement batch embedding\n+        throw new NotImplementedException(\"Fine-tuned model embedding requires model loading implementation\");\n+    }",
    "path": "src/RetrievalAugmentedGeneration/EmbeddingModels/SentenceTransformersFineTuner.cs",
    "commit_id": "7fbd5910239204f8852637c980b53b76349cc626",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n<details>\n<summary>≡ƒº⌐ Analysis chain</summary>\n\n**Critical: Incorrect method override pattern.**\n\nThe `Embed` and `EmbedBatch` methods are declared as `override`, but according to the base class snippet, these are non-virtual public methods that perform validation and delegate to protected abstract/virtual methods. The correct pattern is:\n\n- Override `protected abstract Vector<T> EmbedCore(string text)` instead of `Embed`\n- Override `protected virtual Matrix<T> EmbedBatchCore(IList<string> texts)` instead of `EmbedBatch`\n\nAdditionally, `EmbedBatch` returns `IEnumerable<Vector<T>>` but the base class signature returns `Matrix<T>`.\n\nThe base class already handles input validation in the public methods, so derived classes should focus only on the core embedding logic in the protected methods.\n\n\n\n\nApply this diff to follow the correct pattern:\n\n```diff\n     /// <summary>\n-    /// Generates embeddings using the fine-tuned model.\n+    /// Core embedding logic using the fine-tuned model.\n     /// </summary>\n-    public override Vector<T> Embed(string text)\n+    protected override Vector<T> EmbedCore(string text)\n     {\n-        if (string.IsNullOrWhiteSpace(text))\n-            throw new ArgumentException(\"Text cannot be null or whitespace\", nameof(text));\n-\n         // TODO: Implement embedding with fine-tuned model\n         throw new NotImplementedException(\"Fine-tuned model embedding requires model loading implementation\");\n     }\n\n     /// <summary>\n-    /// Batch embedding generation.\n+    /// Core batch embedding logic using the fine-tuned model.\n     /// </summary>\n-    public override IEnumerable<Vector<T>> EmbedBatch(IEnumerable<string> texts)\n+    protected override Matrix<T> EmbedBatchCore(IList<string> texts)\n     {\n-        if (texts == null)\n-            throw new ArgumentNullException(nameof(texts));\n-\n         // TODO: Implement batch embedding\n         throw new NotImplementedException(\"Fine-tuned model embedding requires model loading implementation\");\n     }\n```\n\nVerify the base class method signatures:\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check EmbeddingModelBase public and protected method signatures\n\necho \"=== Public Embed methods ===\"\nrg -n -B2 -A5 'public.*Embed\\(' src/RetrievalAugmentedGeneration/Embeddings/EmbeddingModelBase.cs\n\necho -e \"\\n=== Protected EmbedCore methods ===\"\nrg -n -B2 -A5 'protected.*EmbedCore' src/RetrievalAugmentedGeneration/Embeddings/EmbeddingModelBase.cs\n\necho -e \"\\n=== Checking if methods are virtual ===\"\nrg -n 'virtual.*Embed|override.*Embed' src/RetrievalAugmentedGeneration/Embeddings/EmbeddingModelBase.cs\n```\n\nLength of output: 767\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check for public EmbedBatch method in base class\n\necho \"=== All EmbedBatch methods ===\"\nrg -n -B2 -A5 'EmbedBatch' src/RetrievalAugmentedGeneration/Embeddings/EmbeddingModelBase.cs\n```\n\nLength of output: 916\n\n---\n\n\n\n</details>\n\n**Critical: Incorrect method override patternΓÇöreview comment is verified and accurate.**\n\nThe base class confirms your analysis:\n- `public Vector<T> Embed(string text)` (line 47) and `public Matrix<T> EmbedBatch(...)` (line 58) are **non-virtual** public methods\n- These delegate to `protected abstract Vector<T> EmbedCore(string text)` and `protected virtual Matrix<T> EmbedBatchCore(IList<string> texts)`\n\nThe code being reviewed incorrectly attempts to override the non-virtual public methods instead of implementing the protected abstract/virtual methods. Additionally, `EmbedBatch` has the wrong return type (`IEnumerable<Vector<T>>` vs `Matrix<T>`), and validation is unnecessarily duplicated.\n\nThe suggested diff is correct and must be applied.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn\nsrc/RetrievalAugmentedGeneration/EmbeddingModels/SentenceTransformersFineTuner.cs\naround lines 67 to 89, the class incorrectly overrides the non-virtual public\nEmbed and EmbedBatch methods and uses the wrong return type; instead implement\nthe base-class extension points: remove/replace the public overrides and\nimplement protected override Vector<T> EmbedCore(string text) and protected\noverride Matrix<T> EmbedBatchCore(IList<string> texts) (matching the base\nsignatures), ensure EmbedBatchCore returns Matrix<T> not IEnumerable<Vector<T>>,\nand keep parameter validation in the public methods (so the core methods assume\ninputs are already validated) and implement embedding logic (or throw\nNotImplementedException from the core methods) accordingly.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 15063ed to 8468360",
    "created_at": "2025-11-04T00:35:15Z",
    "updated_at": "2025-11-04T13:35:33Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488263550",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263550"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488263550"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263550/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 67,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 89,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 89,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263557",
    "pull_request_review_id": 3413484042,
    "id": 2488263557,
    "node_id": "PRRC_kwDOKSXUF86UT-OF",
    "diff_hunk": "@@ -0,0 +1,65 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels;\n+\n+/// <summary>\n+/// Voyage AI embedding model integration for high-performance embeddings.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Voyage AI provides specialized embedding models optimized for retrieval tasks\n+/// with industry-leading performance on benchmark datasets.\n+/// </remarks>\n+public class VoyageAIEmbeddingModel<T> : EmbeddingModelBase<T>\n+{\n+    private readonly string _apiKey;\n+    private readonly string _model;\n+    private readonly string _inputType;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"VoyageAIEmbeddingModel{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"apiKey\">The Voyage AI API key.</param>\n+    /// <param name=\"model\">The model name (e.g., \"voyage-02\").</param>\n+    /// <param name=\"inputType\">The input type (\"document\" or \"query\").</param>\n+    /// <param name=\"dimension\">The embedding dimension.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public VoyageAIEmbeddingModel(\n+        string apiKey,\n+        string model,\n+        string inputType,\n+        int dimension,\n+        INumericOperations<T> numericOperations)\n+        : base(dimension, numericOperations)\n+    {\n+        _apiKey = apiKey ?? throw new ArgumentNullException(nameof(apiKey));\n+        _model = model ?? throw new ArgumentNullException(nameof(model));\n+        _inputType = inputType ?? throw new ArgumentNullException(nameof(inputType));\n+    }\n+\n+    /// <summary>\n+    /// Generates embeddings using Voyage AI API.\n+    /// </summary>\n+    public override Vector<T> Embed(string text)\n+    {\n+        if (string.IsNullOrWhiteSpace(text))\n+            throw new ArgumentException(\"Text cannot be null or whitespace\", nameof(text));\n+\n+        // TODO: Implement Voyage AI API call\n+        throw new NotImplementedException(\"Voyage AI integration requires HTTP client implementation\");\n+    }\n+\n+    /// <summary>\n+    /// Batch embedding generation.\n+    /// </summary>\n+    public override IEnumerable<Vector<T>> EmbedBatch(IEnumerable<string> texts)\n+    {\n+        if (texts == null)\n+            throw new ArgumentNullException(nameof(texts));\n+\n+        // TODO: Implement Voyage AI batch API call\n+        throw new NotImplementedException(\"Voyage AI integration requires HTTP client implementation\");\n+    }",
    "path": "src/RetrievalAugmentedGeneration/EmbeddingModels/VoyageAIEmbeddingModel.cs",
    "commit_id": "7fbd5910239204f8852637c980b53b76349cc626",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Complete the Voyage API hook-up.**  \n`Embed` and `EmbedBatch` still throw `NotImplementedException`, which turns this model into a runtime landmine. Please implement the Voyage AI embedding requests (single and batch) so callers can actually use the class.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/EmbeddingModels/VoyageAIEmbeddingModel.cs\naround lines 45ΓÇô64, both Embed and EmbedBatch currently throw\nNotImplementedException; implement real HTTP calls to Voyage AI so the class can\nbe used. Wire an HttpClient (prefer an injected HttpClient or IHttpClientFactory\nfrom the class constructor), build the POST request to Voyage's embedding\nendpoint with the API key in the Authorization header, send JSON payloads\n(single input for Embed, array for EmbedBatch), validate inputs (non-empty\nstring / non-null enumerable), handle HTTP error statuses by throwing\ninformative exceptions, parse the JSON response to extract the embedding\nvector(s) and convert numeric values into Vector<T> elements, and return the\nresulting Vector<T> or IEnumerable<Vector<T>>; ensure proper disposal/async\nusage consistent with project conventions and include minimal retry/timeout\nhandling and clear error messages instead of NotImplementedException.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T00:35:15Z",
    "updated_at": "2025-11-04T00:35:18Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488263557",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263557"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488263557"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263557/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 45,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 64,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 64,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263561",
    "pull_request_review_id": 3413484042,
    "id": 2488263561,
    "node_id": "PRRC_kwDOKSXUF86UT-OJ",
    "diff_hunk": "@@ -0,0 +1,58 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using AiDotNet.RetrievalAugmentedGeneration.Rerankers;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.RerankingStrategies;\n+\n+/// <summary>\n+/// Cohere Rerank model integration for high-performance reranking.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// Leverages Cohere's specialized reranking model to improve document ordering\n+/// with state-of-the-art relevance scoring.\n+/// </remarks>\n+public class CohereReranker<T> : RerankerBase<T>\n+{\n+    private readonly string _apiKey;\n+    private readonly string _model;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"CohereReranker{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"apiKey\">The Cohere API key.</param>\n+    /// <param name=\"model\">The reranking model name (e.g., \"rerank-english-v2.0\").</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public CohereReranker(\n+        string apiKey,\n+        string model,\n+        INumericOperations<T> numericOperations)\n+        : base(numericOperations)\n+    {\n+        _apiKey = apiKey ?? throw new ArgumentNullException(nameof(apiKey));\n+        _model = model ?? throw new ArgumentNullException(nameof(model));\n+    }\n+\n+    /// <summary>\n+    /// Reranks documents using Cohere Rerank API.\n+    /// </summary>\n+    public override IEnumerable<Document<T>> Rerank(string query, IEnumerable<Document<T>> documents, int topK)\n+    {\n+        if (string.IsNullOrWhiteSpace(query))\n+            throw new ArgumentException(\"Query cannot be null or whitespace\", nameof(query));\n+\n+        if (documents == null)\n+            throw new ArgumentNullException(nameof(documents));\n+\n+        if (topK <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(topK), \"topK must be positive\");\n+\n+        // TODO: Implement Cohere Rerank API call\n+        // 1. Send query and documents to Cohere Rerank API\n+        // 2. Receive relevance scores\n+        // 3. Update document scores\n+        // 4. Return top-K documents\n+        throw new NotImplementedException(\"Cohere Rerank integration requires HTTP client implementation\");\n+    }",
    "path": "src/RetrievalAugmentedGeneration/RerankingStrategies/CohereReranker.cs",
    "commit_id": "7fbd5910239204f8852637c980b53b76349cc626",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Restore required overrides for RerankerBase**  \n\n`RerankerBase<T>` still declares the abstract members `ModifiesScores` and `RerankCore`. Because this class neither overrides the property nor supplies a `RerankCore` implementation, the project no longer compiles. Please add both overrides (you can keep the `NotImplementedException` placeholder inside `RerankCore` until the API integration is done) and let the base class handle the overloads.\n\n```diff\n public class CohereReranker<T> : RerankerBase<T>\n {\n     private readonly string _apiKey;\n     private readonly string _model;\n\n+    public override bool ModifiesScores => true;\n+\n     public CohereReranker(\n         string apiKey,\n         string model,\n         INumericOperations<T> numericOperations)\n         : base(numericOperations)\n     {\n         _apiKey = apiKey ?? throw new ArgumentNullException(nameof(apiKey));\n         _model = model ?? throw new ArgumentNullException(nameof(model));\n     }\n \n-    public override IEnumerable<Document<T>> Rerank(string query, IEnumerable<Document<T>> documents, int topK)\n-    {\n-        if (string.IsNullOrWhiteSpace(query))\n-            throw new ArgumentException(\"Query cannot be null or whitespace\", nameof(query));\n-\n-        if (documents == null)\n-            throw new ArgumentNullException(nameof(documents));\n-\n-        if (topK <= 0)\n-            throw new ArgumentOutOfRangeException(nameof(topK), \"topK must be positive\");\n-\n-        // TODO: Implement Cohere Rerank API call\n-        // 1. Send query and documents to Cohere Rerank API\n-        // 2. Receive relevance scores\n-        // 3. Update document scores\n-        // 4. Return top-K documents\n-        throw new NotImplementedException(\"Cohere Rerank integration requires HTTP client implementation\");\n-    }\n+    public override IEnumerable<Document<T>> Rerank(string query, IEnumerable<Document<T>> documents, int topK)\n+    {\n+        if (string.IsNullOrWhiteSpace(query))\n+            throw new ArgumentException(\"Query cannot be null or whitespace\", nameof(query));\n+\n+        if (documents == null)\n+            throw new ArgumentNullException(nameof(documents));\n+\n+        if (topK <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(topK), \"topK must be positive\");\n+\n+        throw new NotImplementedException(\"Cohere Rerank integration requires HTTP client implementation\");\n+    }\n+\n+    protected override IEnumerable<Document<T>> RerankCore(string query, IList<Document<T>> documents)\n+    {\n+        throw new NotImplementedException(\"Cohere Rerank integration requires HTTP client implementation\");\n+    }\n }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\npublic class CohereReranker<T> : RerankerBase<T>\n{\n    private readonly string _apiKey;\n    private readonly string _model;\n\n    public override bool ModifiesScores => true;\n\n    /// <summary>\n    /// Initializes a new instance of the <see cref=\"CohereReranker{T}\"/> class.\n    /// </summary>\n    /// <param name=\"apiKey\">The Cohere API key.</param>\n    /// <param name=\"model\">The reranking model name (e.g., \"rerank-english-v2.0\").</param>\n    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n    public CohereReranker(\n        string apiKey,\n        string model,\n        INumericOperations<T> numericOperations)\n        : base(numericOperations)\n    {\n        _apiKey = apiKey ?? throw new ArgumentNullException(nameof(apiKey));\n        _model = model ?? throw new ArgumentNullException(nameof(model));\n    }\n\n    /// <summary>\n    /// Reranks documents using Cohere Rerank API.\n    /// </summary>\n    public override IEnumerable<Document<T>> Rerank(string query, IEnumerable<Document<T>> documents, int topK)\n    {\n        if (string.IsNullOrWhiteSpace(query))\n            throw new ArgumentException(\"Query cannot be null or whitespace\", nameof(query));\n\n        if (documents == null)\n            throw new ArgumentNullException(nameof(documents));\n\n        if (topK <= 0)\n            throw new ArgumentOutOfRangeException(nameof(topK), \"topK must be positive\");\n\n        throw new NotImplementedException(\"Cohere Rerank integration requires HTTP client implementation\");\n    }\n\n    protected override IEnumerable<Document<T>> RerankCore(string query, IList<Document<T>> documents)\n    {\n        throw new NotImplementedException(\"Cohere Rerank integration requires HTTP client implementation\");\n    }\n}\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T00:35:15Z",
    "updated_at": "2025-11-04T00:35:18Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488263561",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263561"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488263561"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263561/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 16,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 57,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 57,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263564",
    "pull_request_review_id": 3413484042,
    "id": 2488263564,
    "node_id": "PRRC_kwDOKSXUF86UT-OM",
    "diff_hunk": "@@ -0,0 +1,105 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using AiDotNet.RetrievalAugmentedGeneration.Rerankers;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.RerankingStrategies;\n+\n+/// <summary>\n+/// Diversity-based reranker that prioritizes variety among retrieved documents.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// Reduces redundancy by ensuring retrieved documents cover diverse aspects of the topic\n+/// rather than returning multiple similar documents.\n+/// </remarks>\n+public class DiversityReranker<T> : RerankerBase<T>\n+{\n+    private readonly T _diversityWeight;\n+    private readonly T _relevanceWeight;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"DiversityReranker{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"diversityWeight\">Weight for diversity component (0-1).</param>\n+    /// <param name=\"relevanceWeight\">Weight for relevance component (0-1).</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public DiversityReranker(\n+        T diversityWeight,\n+        T relevanceWeight,\n+        INumericOperations<T> numericOperations)\n+        : base(numericOperations)\n+    {\n+        _diversityWeight = diversityWeight;\n+        _relevanceWeight = relevanceWeight;\n+    }\n+\n+    /// <summary>\n+    /// Reranks documents balancing relevance and diversity.\n+    /// </summary>\n+    public override IEnumerable<Document<T>> Rerank(string query, IEnumerable<Document<T>> documents, int topK)\n+    {",
    "path": "src/RetrievalAugmentedGeneration/RerankingStrategies/DiversityReranker.cs",
    "commit_id": "7fbd5910239204f8852637c980b53b76349cc626",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Implement RerankerBase contract**  \n\n`RerankerBase<T>` still mandates overrides for `ModifiesScores` and `RerankCore`. Without them this class fails to compile. Please add the property and delegate `RerankCore` to your existing logic so the type satisfies the base class contract.\n\n```diff\n public class DiversityReranker<T> : RerankerBase<T>\n {\n     private readonly T _diversityWeight;\n     private readonly T _relevanceWeight;\n \n+    public override bool ModifiesScores => true;\n+\n     public DiversityReranker(\n         T diversityWeight,\n         T relevanceWeight,\n         INumericOperations<T> numericOperations)\n         : base(numericOperations)\n     {\n         _diversityWeight = diversityWeight;\n         _relevanceWeight = relevanceWeight;\n     }\n+\n+    protected override IEnumerable<Document<T>> RerankCore(string query, IList<Document<T>> documents)\n+        => Rerank(query, documents, documents.Count).ToList();\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/RerankingStrategies/DiversityReranker.cs\naround lines 16 to 41, the class is missing required overrides from\nRerankerBase<T> ΓÇö specifically the ModifiesScores property and the RerankCore\nmethod; add a public override bool ModifiesScores { get; } returning the correct\nvalue (true/false per class semantics) and implement protected override\nIEnumerable<Document<T>> RerankCore(string query, IEnumerable<Document<T>>\ndocuments, int topK) that simply delegates to your existing Rerank method\n(return Rerank(query, documents, topK)), so the type satisfies the base class\ncontract and compiles.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T00:35:15Z",
    "updated_at": "2025-11-04T00:35:18Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488263564",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263564"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488263564"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263564/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 16,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 41,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 41,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263572",
    "pull_request_review_id": 3413484042,
    "id": 2488263572,
    "node_id": "PRRC_kwDOKSXUF86UT-OU",
    "diff_hunk": "@@ -0,0 +1,105 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using AiDotNet.RetrievalAugmentedGeneration.Rerankers;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.RerankingStrategies;\n+\n+/// <summary>\n+/// Diversity-based reranker that prioritizes variety among retrieved documents.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// Reduces redundancy by ensuring retrieved documents cover diverse aspects of the topic\n+/// rather than returning multiple similar documents.\n+/// </remarks>\n+public class DiversityReranker<T> : RerankerBase<T>\n+{\n+    private readonly T _diversityWeight;\n+    private readonly T _relevanceWeight;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"DiversityReranker{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"diversityWeight\">Weight for diversity component (0-1).</param>\n+    /// <param name=\"relevanceWeight\">Weight for relevance component (0-1).</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public DiversityReranker(\n+        T diversityWeight,\n+        T relevanceWeight,\n+        INumericOperations<T> numericOperations)\n+        : base(numericOperations)\n+    {\n+        _diversityWeight = diversityWeight;\n+        _relevanceWeight = relevanceWeight;\n+    }\n+\n+    /// <summary>\n+    /// Reranks documents balancing relevance and diversity.\n+    /// </summary>\n+    public override IEnumerable<Document<T>> Rerank(string query, IEnumerable<Document<T>> documents, int topK)\n+    {\n+        if (string.IsNullOrWhiteSpace(query))\n+            throw new ArgumentException(\"Query cannot be null or whitespace\", nameof(query));\n+\n+        if (documents == null)\n+            throw new ArgumentNullException(nameof(documents));\n+\n+        if (topK <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(topK), \"topK must be positive\");\n+\n+        var docList = documents.ToList();\n+        if (docList.Count == 0)\n+            return Enumerable.Empty<Document<T>>();\n+\n+        var selected = new List<Document<T>>();\n+        var remaining = new List<Document<T>>(docList);\n+\n+        // Select first document by relevance\n+        var first = remaining.OrderByDescending(d => d.HasRelevanceScore ? d.RelevanceScore : NumOps.Zero).First();\n+        selected.Add(first);\n+        remaining.Remove(first);\n+\n+        // Iteratively select documents balancing relevance and diversity\n+        while (selected.Count < topK && remaining.Count > 0)\n+        {\n+            var bestDoc = remaining[0];\n+            var bestScore = NumOps.FromDouble(double.NegativeInfinity);\n+\n+            foreach (var doc in remaining)\n+            {\n+                // Relevance score\n+                var relevance = doc.HasRelevanceScore ? doc.RelevanceScore : NumOps.Zero;\n+\n+                // Diversity score (minimum similarity to selected documents)\n+                var minSimilarity = NumOps.One;\n+                foreach (var selectedDoc in selected)\n+                {\n+                    var similarity = StatisticsHelper.JaccardSimilarity(doc.Content, selectedDoc.Content);\n+                    var simT = NumOps.FromDouble(similarity);\n+                    if (NumOps.LessThan(simT, minSimilarity))\n+                    {\n+                        minSimilarity = simT;\n+                    }\n+                }\n+\n+                // Combined score\n+                var score = NumOps.Add(\n+                    NumOps.Multiply(_relevanceWeight, relevance),\n+                    NumOps.Multiply(_diversityWeight, NumOps.Subtract(NumOps.One, minSimilarity))\n+                );\n+\n+                if (NumOps.GreaterThan(score, bestScore))\n+                {\n+                    bestScore = score;\n+                    bestDoc = doc;\n+                }\n+            }\n+",
    "path": "src/RetrievalAugmentedGeneration/RerankingStrategies/DiversityReranker.cs",
    "commit_id": "7fbd5910239204f8852637c980b53b76349cc626",
    "original_commit_id": "4d459a5baaeed2f7174d07d29893c445365ef894",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Avoid `double.NegativeInfinity` conversions**  \n\n`NumOps.FromDouble(double.NegativeInfinity)` overflows for supported non-floating numeric types such as `decimal`; the framework explicitly allows `decimal`, so this blows up at runtime before any scoring happens. MicrosoftΓÇÖs conversion rules state that converting `double` infinity to `decimal` throws `OverflowException`.([learn.microsoft.com](https://learn.microsoft.com/en-us/dotnet/csharp/language-reference/builtin-types/numeric-conversions?utm_source=openai)) Please seed `bestScore` using a sentinel flag instead of `NegativeInfinity`, e.g. track whether a best score has been seen yet and initialize on the first iteration, so every numeric implementation remains safe.\n\n```diff\n-        while (selected.Count < topK && remaining.Count > 0)\n+        while (selected.Count < topK && remaining.Count > 0)\n         {\n-            var bestDoc = remaining[0];\n-            var bestScore = NumOps.FromDouble(double.NegativeInfinity);\n+            Document<T>? bestDoc = null;\n+            var hasBestScore = false;\n+            T bestScore = default!;\n \n             foreach (var doc in remaining)\n             {\n                 // Relevance score\n                 var relevance = doc.HasRelevanceScore ? doc.RelevanceScore : NumOps.Zero;\n@@\n-                if (NumOps.GreaterThan(score, bestScore))\n+                if (!hasBestScore || NumOps.GreaterThan(score, bestScore))\n                 {\n                     bestScore = score;\n                     bestDoc = doc;\n+                    hasBestScore = true;\n                 }\n             }\n \n-            selected.Add(bestDoc);\n-            remaining.Remove(bestDoc);\n+            if (!hasBestScore || bestDoc is null)\n+                break;\n+\n+            selected.Add(bestDoc);\n+            remaining.Remove(bestDoc);\n         }\n```\n\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/RerankingStrategies/DiversityReranker.cs\naround lines 66 to 98, the code seeds bestScore with\nNumOps.FromDouble(double.NegativeInfinity) which throws for non-floating numeric\ntypes (e.g. decimal); replace this by introducing a boolean sentinel (e.g.,\nhasBest=false) and on the first loop iteration set bestScore = score and hasBest\n= true (or if comparing, guard comparisons to initialize bestScore when\n!hasBest), and remove the NegativeInfinity conversion so every numeric\nimplementation is safe across types.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T00:35:16Z",
    "updated_at": "2025-11-04T00:35:18Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488263572",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263572"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488263572"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488263572/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 66,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 98,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 98,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488444942",
    "pull_request_review_id": 3413727520,
    "id": 2488444942,
    "node_id": "PRRC_kwDOKSXUF86UUqgO",
    "diff_hunk": "@@ -0,0 +1,54 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies;\n+\n+/// <summary>\n+/// LLM-powered agentic chunker that decides where to split text based on content meaning.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// Uses an LLM to analyze text and determine optimal split points based on semantic boundaries,\n+/// topic changes, and natural breaks in the content flow.\n+/// </remarks>\n+public class AgenticChunker : ChunkingStrategyBase",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/AgenticChunker.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "fb037f7036c63184b2417768a5e0933c503d9233",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Fix generic type parameter mismatch.**\n\nThe XML documentation declares a generic type parameter `<typeparam name=\"T\">`, but the class declaration on line 14 is not generic. This is inconsistent with the PR's objective to convert the RAG framework to generic types and contradicts the documentation.\n\n\n\nApply this diff to make the class generic:\n\n```diff\n /// <summary>\n /// LLM-powered agentic chunker that decides where to split text based on content meaning.\n /// </summary>\n /// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n /// <remarks>\n /// Uses an LLM to analyze text and determine optimal split points based on semantic boundaries,\n /// topic changes, and natural breaks in the content flow.\n /// </remarks>\n-public class AgenticChunker : ChunkingStrategyBase\n+public class AgenticChunker<T> : ChunkingStrategyBase<T>\n {\n```\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T02:36:43Z",
    "updated_at": "2025-11-04T02:36:45Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488444942",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488444942"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488444942"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488444942/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 14,
    "original_start_line": 9,
    "start_side": "RIGHT",
    "line": 56,
    "original_line": 14,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 14,
    "position": 56,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488444961",
    "pull_request_review_id": 3413727520,
    "id": 2488444961,
    "node_id": "PRRC_kwDOKSXUF86UUqgh",
    "diff_hunk": "@@ -0,0 +1,54 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies;\n+\n+/// <summary>\n+/// LLM-powered agentic chunker that decides where to split text based on content meaning.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// Uses an LLM to analyze text and determine optimal split points based on semantic boundaries,\n+/// topic changes, and natural breaks in the content flow.\n+/// </remarks>\n+public class AgenticChunker : ChunkingStrategyBase\n+{\n+    private readonly string _llmEndpoint;\n+    private readonly string _llmApiKey;\n+    private readonly int _maxChunkSize;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"AgenticChunker\"/> class.\n+    /// </summary>\n+    /// <param name=\"llmEndpoint\">The LLM API endpoint.</param>\n+    /// <param name=\"llmApiKey\">The API key for the LLM service.</param>\n+    /// <param name=\"maxChunkSize\">Maximum size of each chunk in characters.</param>\n+    public AgenticChunker(\n+        string llmEndpoint,\n+        string llmApiKey,\n+        int maxChunkSize)\n+        : base(maxChunkSize, maxChunkSize / 10)\n+    {\n+        _llmEndpoint = llmEndpoint ?? throw new ArgumentNullException(nameof(llmEndpoint));\n+        _llmApiKey = llmApiKey ?? throw new ArgumentNullException(nameof(llmApiKey));\n+        \n+        if (maxChunkSize <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(maxChunkSize), \"Max chunk size must be positive\");\n+            \n+        _maxChunkSize = maxChunkSize;\n+    }\n+\n+    /// <summary>\n+    /// Splits text into chunks using LLM-guided boundaries.\n+    /// </summary>\n+    protected override IEnumerable<(string Chunk, int StartPosition, int EndPosition)> ChunkCore(string text)\n+    {\n+        // TODO: Implement LLM-based chunking\n+        // 1. Send text to LLM with instructions to identify semantic boundaries\n+        // 2. Parse LLM response to extract split points\n+        // 3. Create chunks based on identified boundaries\n+        \n+        // For now, fall back to default chunking\n+        return CreateOverlappingChunks(text);",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/AgenticChunker.cs",
    "commit_id": "80630e82c71aabc214615014532fd5063336af16",
    "original_commit_id": "fb037f7036c63184b2417768a5e0933c503d9233",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Core LLM-based chunking logic not implemented.**\n\nThe TODO comment indicates that the primary featureΓÇöLLM-guided semantic boundary detectionΓÇöis not yet implemented. The method currently falls back to default overlapping chunks, which defeats the purpose of an \"agentic\" chunker. The `_llmEndpoint` and `_llmApiKey` fields remain unused.\n\n\n\n\nWould you like me to generate a skeleton implementation that calls the LLM endpoint, or would you prefer to open a tracking issue for this work?\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/ChunkingStrategies/AgenticChunker.cs around\nlines 46 to 52, the method currently returns default overlapping chunks and does\nnot use the _llmEndpoint or _llmApiKey; implement the core LLM-based chunking by\ncalling the configured LLM endpoint with the input text and clear instructions\nto identify semantic split points, parse the LLM response to extract those split\nindices/ranges, convert them into chunks, and return them; ensure you handle\nnetwork/errors/timeouts and fall back to CreateOverlappingChunks(text) if the\nLLM call fails or returns invalid output, keep the call asynchronous, include\nbasic input validation and logging, and add unit-testable separation (e.g., a\nprivate method to build the request and parse the response) so parsing logic can\nbe tested independently.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T02:36:43Z",
    "updated_at": "2025-11-04T02:36:45Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488444961",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488444961"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488444961"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488444961/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 46,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 52,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 52,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488444969",
    "pull_request_review_id": 3413727520,
    "id": 2488444969,
    "node_id": "PRRC_kwDOKSXUF86UUqgp",
    "diff_hunk": "@@ -0,0 +1,164 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies;\n+\n+/// <summary>\n+/// Specialized splitter that correctly parses and chunks tabular data from documents.\n+/// </summary>\n+/// <remarks>\n+/// Handles various table formats (Markdown, CSV, HTML tables) and ensures table integrity\n+/// by keeping related rows together and preserving column headers.\n+/// </remarks>\n+public class TableAwareTextSplitter : ChunkingStrategyBase\n+{\n+    private readonly int _maxRowsPerChunk;\n+    private readonly bool _includeHeadersInEachChunk;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"TableAwareTextSplitter\"/> class.\n+    /// </summary>\n+    /// <param name=\"chunkSize\">The maximum size of each chunk.</param>\n+    /// <param name=\"chunkOverlap\">The overlap between consecutive chunks.</param>\n+    /// <param name=\"maxRowsPerChunk\">Maximum number of table rows per chunk.</param>\n+    /// <param name=\"includeHeadersInEachChunk\">Whether to include table headers in each chunk.</param>\n+    public TableAwareTextSplitter(\n+        int chunkSize = 2000,\n+        int chunkOverlap = 200,\n+        int maxRowsPerChunk = 50,\n+        bool includeHeadersInEachChunk = true)\n+        : base(chunkSize, chunkOverlap)\n+    {\n+        if (maxRowsPerChunk <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(maxRowsPerChunk), \"Max rows per chunk must be positive\");\n+            \n+        _maxRowsPerChunk = maxRowsPerChunk;\n+        _includeHeadersInEachChunk = includeHeadersInEachChunk;\n+    }\n+\n+    /// <summary>\n+    /// Splits text while preserving table structure.\n+    /// </summary>\n+    protected override IEnumerable<(string Chunk, int StartPosition, int EndPosition)> ChunkCore(string text)\n+    {\n+        var chunks = new List<(string Chunk, int StartPosition, int EndPosition)>();\n+        var lines = text.Split(new[] { \"\\r\\n\", \"\\r\", \"\\n\" }, StringSplitOptions.None);\n+        \n+        var position = 0;\n+        var i = 0;\n+        while (i < lines.Length)\n+        {\n+            // Check if current position is start of a table\n+            if (IsTableStart(lines, i))\n+            {\n+                var tableChunks = ProcessTable(lines, ref i, ref position);\n+                chunks.AddRange(tableChunks);\n+            }\n+            else\n+            {\n+                // Regular text line - add as single chunk\n+                var lineLength = lines[i].Length + Environment.NewLine.Length;\n+                chunks.Add((lines[i], position, position + lines[i].Length));\n+                position += lineLength;\n+                i++;\n+            }",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/TableAwareTextSplitter.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "fb037f7036c63184b2417768a5e0933c503d9233",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Batch non-table lines by ChunkSize instead of creating one chunk per line.**\n\nEach non-table line currently becomes a single chunk (line 62), ignoring the `ChunkSize` parameter. For documents with many non-table lines, this produces an excessive number of tiny chunks and defeats the purpose of the chunking strategy.\n\n\n\nAccumulate non-table lines until `ChunkSize` is reached, similar to how `SemanticChunkingStrategy` batches content (see `src/RetrievalAugmentedGeneration/ChunkingStrategies/SemanticChunkingStrategy.cs` lines 29-58):\n\n```diff\n+        var currentChunk = new List<string>();\n+        var currentChunkSize = 0;\n+        var chunkStartPosition = 0;\n+\n         var position = 0;\n         var i = 0;\n         while (i < lines.Length)\n         {\n             // Check if current position is start of a table\n             if (IsTableStart(lines, i))\n             {\n+                // Flush accumulated non-table lines first\n+                if (currentChunk.Count > 0)\n+                {\n+                    var chunkText = string.Join(Environment.NewLine, currentChunk);\n+                    chunks.Add((chunkText, chunkStartPosition, position));\n+                    currentChunk.Clear();\n+                    currentChunkSize = 0;\n+                }\n+\n                 var tableChunks = ProcessTable(lines, ref i, ref position);\n                 chunks.AddRange(tableChunks);\n+                chunkStartPosition = position;\n             }\n             else\n             {\n-                // Regular text line - add as single chunk\n+                // Accumulate regular text lines\n+                if (currentChunkSize + lines[i].Length > ChunkSize && currentChunk.Count > 0)\n+                {\n+                    var chunkText = string.Join(Environment.NewLine, currentChunk);\n+                    chunks.Add((chunkText, chunkStartPosition, position));\n+                    currentChunk.Clear();\n+                    currentChunkSize = 0;\n+                    chunkStartPosition = position;\n+                }\n+\n+                currentChunk.Add(lines[i]);\n+                currentChunkSize += lines[i].Length;\n                 var lineLength = lines[i].Length + Environment.NewLine.Length;\n-                chunks.Add((lines[i], position, position + lines[i].Length));\n                 position += lineLength;\n                 i++;\n             }\n         }\n+\n+        // Flush remaining non-table lines\n+        if (currentChunk.Count > 0)\n+        {\n+            var chunkText = string.Join(Environment.NewLine, currentChunk);\n+            chunks.Add((chunkText, chunkStartPosition, position));\n+        }\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/ChunkingStrategies/TableAwareTextSplitter.cs\naround lines 58 to 65, non-table lines are currently emitted one chunk per line\nwhich ignores ChunkSize; change the logic to accumulate consecutive non-table\nlines into a buffer until the accumulated length (including Environment.NewLine\nbetween lines) reaches or would exceed ChunkSize, then add a single chunk with\nthe concatenated text and correct start/end positions; update position by the\ntotal buffered length, advance i by the number of lines consumed, and after the\nloop ensure any remaining buffered text is added as a final chunk.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T02:36:43Z",
    "updated_at": "2025-11-04T02:36:45Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488444969",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488444969"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488444969"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488444969/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 58,
    "original_start_line": 58,
    "start_side": "RIGHT",
    "line": 65,
    "original_line": 65,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 65,
    "position": 65,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488444978",
    "pull_request_review_id": 3413727520,
    "id": 2488444978,
    "node_id": "PRRC_kwDOKSXUF86UUqgy",
    "diff_hunk": "@@ -0,0 +1,109 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ContextCompression;\n+\n+/// <summary>\n+/// Selective context compressor that picks the most relevant sentences based on the query.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// Analyzes retrieved documents and selectively extracts only the sentences most relevant\n+/// to the query, reducing context length while preserving important information.\n+/// </remarks>\n+public class SelectiveContextCompressor<T>\n+{\n+    private readonly INumericOperations<T> _numericOperations;\n+    private readonly int _maxSentences;\n+    private readonly T _relevanceThreshold;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"SelectiveContextCompressor{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"maxSentences\">Maximum number of sentences to keep.</param>\n+    /// <param name=\"relevanceThreshold\">Minimum relevance score to keep a sentence.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public SelectiveContextCompressor(\n+        int maxSentences,\n+        T relevanceThreshold,\n+        INumericOperations<T> numericOperations)\n+    {\n+        if (maxSentences <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(maxSentences), \"Max sentences must be positive\");\n+            \n+        _maxSentences = maxSentences;\n+        _relevanceThreshold = relevanceThreshold;\n+        _numericOperations = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\n+    }\n+\n+    /// <summary>\n+    /// Compresses documents by selecting relevant sentences.\n+    /// </summary>\n+    public IEnumerable<Document<T>> Compress(string query, IEnumerable<Document<T>> documents)\n+    {\n+        if (string.IsNullOrWhiteSpace(query))\n+            throw new ArgumentException(\"Query cannot be null or whitespace\", nameof(query));\n+\n+        if (documents == null)\n+            throw new ArgumentNullException(nameof(documents));\n+\n+        var compressed = new List<Document<T>>();\n+\n+        foreach (var doc in documents)\n+        {\n+            var sentences = SplitIntoSentences(doc.Content);\n+            var scoredSentences = new List<(string sentence, T score)>();\n+\n+            foreach (var sentence in sentences)\n+            {\n+                var score = CalculateRelevance(query, sentence);\n+                if (_numericOperations.GreaterThan(score, _relevanceThreshold) || \n+                    _numericOperations.Equals(score, _relevanceThreshold))\n+                {\n+                    scoredSentences.Add((sentence, score));\n+                }\n+            }\n+\n+            var selectedSentences = scoredSentences\n+                .OrderByDescending(s => s.score)\n+                .Take(_maxSentences)\n+                .Select(s => s.sentence);\n+\n+            if (selectedSentences.Any())\n+            {\n+                compressed.Add(new Document<T>\n+                {\n+                    Id = doc.Id,\n+                    Content = string.Join(\" \", selectedSentences),\n+                    Metadata = doc.Metadata,\n+                    RelevanceScore = doc.RelevanceScore,\n+                    HasRelevanceScore = doc.HasRelevanceScore\n+                });\n+            }\n+        }\n+\n+        return compressed;\n+    }\n+\n+    private List<string> SplitIntoSentences(string text)\n+    {\n+        // Simple sentence splitting - in production would use NLP library\n+        return text\n+            .Split(new[] { \". \", \"! \", \"? \" }, StringSplitOptions.RemoveEmptyEntries)\n+            .Select(s => s.Trim())\n+            .Where(s => !string.IsNullOrWhiteSpace(s))\n+            .ToList();\n+    }",
    "path": "src/RetrievalAugmentedGeneration/ContextCompression/SelectiveContextCompressor.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "fb037f7036c63184b2417768a5e0933c503d9233",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Fix sentence splitting to handle text ending without trailing space.**\n\nThe split delimiters `\". \"`, `\"! \"`, `\"? \"` include trailing spaces, which means sentences at the end of text (without trailing space) won't split correctly. For example:\n- `\"First. Second.\"` ΓåÆ `[\"First\", \"Second.\"]` (period retained on last sentence)\n- `\"First. Second\"` ΓåÆ `[\"First\", \"Second\"]`\n\n\n\nApply this diff to split on punctuation marks with or without trailing space:\n\n```diff\n-        return text\n-            .Split(new[] { \". \", \"! \", \"? \" }, StringSplitOptions.RemoveEmptyEntries)\n-            .Select(s => s.Trim())\n-            .Where(s => !string.IsNullOrWhiteSpace(s))\n-            .ToList();\n+        return text\n+            .Split(new[] { '.', '!', '?' }, StringSplitOptions.RemoveEmptyEntries)\n+            .Select(s => s.Trim())\n+            .Where(s => !string.IsNullOrWhiteSpace(s))\n+            .ToList();\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    private List<string> SplitIntoSentences(string text)\n    {\n        // Simple sentence splitting - in production would use NLP library\n        return text\n            .Split(new[] { '.', '!', '?' }, StringSplitOptions.RemoveEmptyEntries)\n            .Select(s => s.Trim())\n            .Where(s => !string.IsNullOrWhiteSpace(s))\n            .ToList();\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn\nsrc/RetrievalAugmentedGeneration/ContextCompression/SelectiveContextCompressor.cs\naround lines 88 to 96, the current SplitIntoSentences uses delimiters with\ntrailing spaces which fails on end-of-text punctuation; change it to split on\npunctuation with optional whitespace using a regex (e.g. use Regex.Split with\npattern like [.!?]\\s*), then Trim each resulting piece and filter out empties\n(also consider trimming trailing punctuation if you want sentences without\nterminal punctuation); ensure you add the necessary using for\nSystem.Text.RegularExpressions.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T02:36:43Z",
    "updated_at": "2025-11-04T02:36:45Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488444978",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488444978"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488444978"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488444978/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 82,
    "original_start_line": 88,
    "start_side": "RIGHT",
    "line": 90,
    "original_line": 96,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 96,
    "position": 90,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488444997",
    "pull_request_review_id": 3413727520,
    "id": 2488444997,
    "node_id": "PRRC_kwDOKSXUF86UUqhF",
    "diff_hunk": "@@ -0,0 +1,50 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Evaluation;\n+\n+/// <summary>\n+/// Evaluates the factual correctness of generated answers.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// Assesses whether the generated answer contains factually correct information\n+/// by comparing against ground truth or using fact-checking mechanisms.\n+/// </remarks>\n+public class AnswerCorrectnessMetric<T> : RAGMetricBase<T>\n+{\n+    private readonly string _llmEndpoint;\n+    private readonly string _llmApiKey;",
    "path": "src/RetrievalAugmentedGeneration/Evaluation/AnswerCorrectnessMetric.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "fb037f7036c63184b2417768a5e0933c503d9233",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Remove unused LLM configuration or implement LLM-based fact checking.**\n\nThe constructor accepts and stores `llmEndpoint` and `llmApiKey`, but these fields are never used in the implementation. The `EvaluateCore` method only performs word-based Jaccard similarity without any LLM interaction. This creates confusion about the metric's actual capabilities.\n\nConsider one of these approaches:\n1. **Remove the unused parameters and fields** if LLM-based fact checking is not planned.\n2. **Implement actual LLM-based fact checking** using these credentials to truly evaluate factual correctness.\n\nAdditionally, the constructor should validate that the strings are not empty or whitespace:\n\n```diff\n public AnswerCorrectnessMetric(string llmEndpoint, string llmApiKey)\n {\n-    _llmEndpoint = llmEndpoint ?? throw new ArgumentNullException(nameof(llmEndpoint));\n-    _llmApiKey = llmApiKey ?? throw new ArgumentNullException(nameof(llmApiKey));\n+    if (string.IsNullOrWhiteSpace(llmEndpoint))\n+        throw new ArgumentException(\"LLM endpoint cannot be null or empty\", nameof(llmEndpoint));\n+    if (string.IsNullOrWhiteSpace(llmApiKey))\n+        throw new ArgumentException(\"API key cannot be null or empty\", nameof(llmApiKey));\n+\n+    _llmEndpoint = llmEndpoint;\n+    _llmApiKey = llmApiKey;\n }\n```\n\n\n\n\n\nAlso applies to: 28-32\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Evaluation/AnswerCorrectnessMetric.cs around\nlines 16-17 and 28-32, the constructor stores llmEndpoint and llmApiKey which\nare never used by EvaluateCore (currently only Jaccard similarity); either\nremove the unused parameters and private fields (and any related constructor\nargs) if you do not intend to use LLM checking, or implement LLM-based fact\nchecking using those credentials (e.g., validate non-empty/whitespace args in\nthe constructor, store them, and call the LLM in EvaluateCore to perform factual\nverification of the answer vs. ground truth); if keeping the LLM path, add\nconstructor validation that llmEndpoint and llmApiKey are not\nnull/empty/whitespace and throw ArgumentException when invalid.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T02:36:43Z",
    "updated_at": "2025-11-04T02:36:45Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488444997",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488444997"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488444997"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488444997/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 16,
    "original_start_line": 16,
    "start_side": "RIGHT",
    "line": 17,
    "original_line": 17,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 17,
    "position": 17,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488445024",
    "pull_request_review_id": 3413727520,
    "id": 2488445024,
    "node_id": "PRRC_kwDOKSXUF86UUqhg",
    "diff_hunk": "@@ -0,0 +1,50 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Evaluation;\n+\n+/// <summary>\n+/// Evaluates the factual correctness of generated answers.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// Assesses whether the generated answer contains factually correct information\n+/// by comparing against ground truth or using fact-checking mechanisms.\n+/// </remarks>\n+public class AnswerCorrectnessMetric<T> : RAGMetricBase<T>\n+{\n+    private readonly string _llmEndpoint;\n+    private readonly string _llmApiKey;\n+\n+    public override string Name => \"Answer Correctness\";\n+    public override string Description => \"Evaluates the factual correctness of generated answers by comparing against ground truth.\";\n+    protected override bool RequiresGroundTruth => true;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"AnswerCorrectnessMetric{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"llmEndpoint\">The LLM API endpoint for fact checking.</param>\n+    /// <param name=\"llmApiKey\">The API key for the LLM service.</param>\n+    public AnswerCorrectnessMetric(string llmEndpoint, string llmApiKey)\n+    {\n+        _llmEndpoint = llmEndpoint ?? throw new ArgumentNullException(nameof(llmEndpoint));\n+        _llmApiKey = llmApiKey ?? throw new ArgumentNullException(nameof(llmApiKey));\n+    }\n+\n+    protected override T EvaluateCore(GroundedAnswer<T> answer, string? groundTruth)\n+    {\n+        if (string.IsNullOrWhiteSpace(answer.Answer) || string.IsNullOrWhiteSpace(groundTruth))\n+            return NumOps.Zero;\n+\n+        var words1 = GetWords(answer.Answer);\n+        var words2 = GetWords(groundTruth!);\n+\n+        var intersection = words1.Intersect(words2).Count();\n+        var union = words1.Union(words2).Count();\n+\n+        if (union == 0)\n+            return NumOps.Zero;\n+\n+        return NumOps.Divide(NumOps.FromDouble(intersection), NumOps.FromDouble(union));\n+    }",
    "path": "src/RetrievalAugmentedGeneration/Evaluation/AnswerCorrectnessMetric.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "fb037f7036c63184b2417768a5e0933c503d9233",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Implementation doesn't match the metric's stated purpose.**\n\nThis implementation is identical to `AnswerSimilarityMetric<T>` (using word-based Jaccard similarity), which doesn't actually measure \"factual correctness.\" Two answers can have similar word overlap but different factual accuracy, or vice versa. For example:\n- Ground truth: \"The capital of France is Paris.\"\n- Answer A: \"Paris is the capital of France.\" (100% correct, high overlap)\n- Answer B: \"The capital of France is London.\" (0% correct, high overlap)\n\nBoth answers would score similarly with Jaccard similarity despite having completely different factual correctness.\n\nAdditionally, this creates code duplication with `AnswerSimilarityMetric<T>` (see relevant code snippet), violating DRY principles.\n\n**Recommended approach:**\n\nEither differentiate this metric from `AnswerSimilarityMetric<T>` by implementing actual fact-checking logic (using the stored LLM credentials), or consolidate these two metrics into one if they serve the same purpose.\n\nNote: The null check on line 36 for `answer.Answer` is redundant since `RAGMetricBase<T>.ValidateAnswer` already performs this validation before calling `EvaluateCore`.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T02:36:44Z",
    "updated_at": "2025-11-04T02:36:45Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488445024",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488445024"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488445024"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488445024/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 34,
    "original_start_line": 34,
    "start_side": "RIGHT",
    "line": 49,
    "original_line": 49,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 49,
    "position": 49,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488445040",
    "pull_request_review_id": 3413727520,
    "id": 2488445040,
    "node_id": "PRRC_kwDOKSXUF86UUqhw",
    "diff_hunk": "@@ -0,0 +1,67 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.QueryExpansion\n+{\n+    /// <summary>\n+    /// Hypothetical Document Embeddings (HyDE) query expansion strategy.\n+    /// </summary>\n+    public class HyDEQueryExpansion : QueryExpansionBase\n+    {\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"HyDEQueryExpansion\"/> class.\n+        /// </summary>\n+        public HyDEQueryExpansion()\n+        {\n+        }\n+\n+        /// <summary>\n+        /// Expands a query by generating hypothetical documents.\n+        /// </summary>\n+        /// <param name=\"query\">The original query.</param>\n+        /// <returns>A list of hypothetical document variations.</returns>\n+        public override List<string> ExpandQuery(string query)\n+        {\n+            if (string.IsNullOrEmpty(query)) throw new ArgumentNullException(nameof(query));\n+\n+            var expansions = new List<string> { query };\n+\n+            var hypoDoc1 = GenerateHypotheticalDocument(query, \"detailed answer\");\n+            var hypoDoc2 = GenerateHypotheticalDocument(query, \"concise explanation\");\n+            var hypoDoc3 = GenerateHypotheticalDocument(query, \"technical description\");\n+\n+            expansions.Add(hypoDoc1);\n+            expansions.Add(hypoDoc2);\n+            expansions.Add(hypoDoc3);\n+\n+            return expansions;\n+        }",
    "path": "src/RetrievalAugmentedGeneration/QueryExpansion/HyDEQueryExpansion.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "fb037f7036c63184b2417768a5e0933c503d9233",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n<details>\n<summary>≡ƒº⌐ Analysis chain</summary>\n\n**HyDE implementation missing LLM-based document generation.**\n\nThe current implementation generates hypothetical documents using simple templates rather than an LLM, which fundamentally deviates from the HyDE algorithm. HyDE should:\n\n1. Use an LLM to generate realistic hypothetical documents that would answer the query\n2. Generate embeddings from those documents\n3. Use the embeddings to retrieve similar real documents from the corpus\n\nA past review comment mentioned a `GenerateHypotheticalEmbedding` method returning `Vector<T>`, which is absent in the current code. The template-based approach here will not provide the semantic richness that makes HyDE effective for retrieval.\n\n\n\n\nDo you want me to provide a revised implementation that integrates with `IGenerator<T>` and produces proper embeddings?\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Locate the HyDEQueryExpansion file and examine its full content\nfind . -name \"HyDEQueryExpansion.cs\" -type f | head -5\n```\n\nLength of output: 71\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Search for GenerateHypotheticalDocument and GenerateHypotheticalEmbedding methods\nrg -n \"GenerateHypothetical\" --type=cs -B2 -A10\n```\n\nLength of output: 2793\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Look for LLMQueryExpansion class to compare implementations\nfind . -name \"*QueryExpansion*.cs\" -type f | head -10\n```\n\nLength of output: 287\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Search for IGenerator interface definition\nrg -n \"interface IGenerator\" --type=cs -A5\n```\n\nLength of output: 381\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check for any embedding-related code\nrg -n \"embedding|Embedding|Vector<\" --type=cs | head -20\n```\n\nLength of output: 2404\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# View complete HyDEQueryExpansion.cs file\ncat ./src/RetrievalAugmentedGeneration/QueryExpansion/HyDEQueryExpansion.cs\n```\n\nLength of output: 2687\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# View LLMQueryExpansion.cs for comparison\ncat ./src/RetrievalAugmentedGeneration/QueryExpansion/LLMQueryExpansion.cs\n```\n\nLength of output: 2464\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# View QueryExpansionBase to understand base class\ncat ./src/RetrievalAugmentedGeneration/QueryExpansion/QueryExpansionBase.cs\n```\n\nLength of output: 528\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check for any embedding generation in the entire QueryExpansion directory\nrg -n \"Embed|embed|Vector\" ./src/RetrievalAugmentedGeneration/QueryExpansion/ --type=cs\n```\n\nLength of output: 147\n\n---\n\n\n\n</details>\n\n**HyDE implementation uses hardcoded templates instead of LLM-based document generation.**\n\nThe current implementation fundamentally deviates from the HyDE algorithm. The `GenerateHypotheticalDocument` method generates documents using hardcoded string templates via a switch statement (e.g., \"A comprehensive analysis of {query} reveals...\") rather than using an LLM to generate realistic hypothetical documents.\n\nHyDE requires three essential steps:\n1. Use an LLM to generate plausible hypothetical documents\n2. Generate embeddings from those documents\n3. Retrieve similar real documents using the embeddings\n\nThe current code skips all three: it has no LLM integration, no embedding generation, and returns text strings instead of semantic vectors. The `GenerateHypotheticalEmbedding` method returning `Vector<T>` (referenced in past feedback) is entirely absent, and there is no dependency on `IGenerator<T>` for language model access.\n\nAdd LLM-based document generation and embedding support to align with the HyDE algorithm.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T02:36:44Z",
    "updated_at": "2025-11-04T02:36:45Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488445040",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488445040"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488445040"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488445040/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 25,
    "original_start_line": 24,
    "start_side": "RIGHT",
    "line": 40,
    "original_line": 39,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 39,
    "position": 40,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488445056",
    "pull_request_review_id": 3413727520,
    "id": 2488445056,
    "node_id": "PRRC_kwDOKSXUF86UUqiA",
    "diff_hunk": "@@ -0,0 +1,67 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.QueryExpansion\n+{\n+    /// <summary>\n+    /// Hypothetical Document Embeddings (HyDE) query expansion strategy.\n+    /// </summary>\n+    public class HyDEQueryExpansion : QueryExpansionBase\n+    {\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"HyDEQueryExpansion\"/> class.\n+        /// </summary>\n+        public HyDEQueryExpansion()\n+        {\n+        }\n+\n+        /// <summary>\n+        /// Expands a query by generating hypothetical documents.\n+        /// </summary>\n+        /// <param name=\"query\">The original query.</param>\n+        /// <returns>A list of hypothetical document variations.</returns>\n+        public override List<string> ExpandQuery(string query)\n+        {\n+            if (string.IsNullOrEmpty(query)) throw new ArgumentNullException(nameof(query));\n+\n+            var expansions = new List<string> { query };\n+\n+            var hypoDoc1 = GenerateHypotheticalDocument(query, \"detailed answer\");\n+            var hypoDoc2 = GenerateHypotheticalDocument(query, \"concise explanation\");\n+            var hypoDoc3 = GenerateHypotheticalDocument(query, \"technical description\");\n+\n+            expansions.Add(hypoDoc1);\n+            expansions.Add(hypoDoc2);\n+            expansions.Add(hypoDoc3);\n+\n+            return expansions;\n+        }\n+\n+        private string GenerateHypotheticalDocument(string query, string style)\n+        {\n+            var words = query.Split(' ').Where(w => !string.IsNullOrWhiteSpace(w)).ToList();\n+\n+            switch (style)\n+            {\n+                case \"detailed answer\":\n+                    return $\"A comprehensive analysis of {query} reveals that it encompasses multiple aspects. \" +\n+                           $\"The key components include {string.Join(\", \", words)}. \" +\n+                           $\"This topic is important because it addresses fundamental concepts.\";\n+\n+                case \"concise explanation\":\n+                    return $\"{query} refers to {string.Join(\" and \", words.Take(3))}. \" +\n+                           $\"It is characterized by its practical applications.\";\n+\n+                case \"technical description\":\n+                    return $\"From a technical perspective, {query} involves {string.Join(\", \", words)}. \" +\n+                           $\"The implementation requires careful consideration of these elements.\";\n+\n+                default:\n+                    return $\"{query} is an important concept that involves {string.Join(\", \", words)}. \" +\n+                           $\"Understanding this topic requires examination of its core principles.\";\n+            }\n+        }",
    "path": "src/RetrievalAugmentedGeneration/QueryExpansion/HyDEQueryExpansion.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "fb037f7036c63184b2417768a5e0933c503d9233",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Template-based generation insufficient for HyDE.**\n\nThe template-based approach with simple string interpolation doesn't produce the semantically rich hypothetical documents that HyDE requires. Real HyDE implementations use an LLM to generate documents that:\n- Sound like actual answers to the query\n- Contain domain-specific terminology and phrasing\n- Capture the semantic intent of what a good answer would look like\n\nThe simple word-splitting and concatenation here won't achieve these goals.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/QueryExpansion/HyDEQueryExpansion.cs around\nlines 41 to 64, the current template-based GenerateHypotheticalDocument simply\nsplits and concatenates words and must be replaced with an actual LLM-based\ngeneration to produce semantically rich hypothetical documents; update the\nmethod to call the injected LLM client (or create one if missing), build a\nprompt that includes the query and desired style (system + user prompt), invoke\nthe model asynchronously with appropriate parameters\n(temperature/top_p/max_tokens), return the LLM response text, and add error\nhandling and a small deterministic fallback for reliability; ensure the method\nsignature and callers are adjusted for async if necessary and that the style\nstring is passed into the prompt rather than driving naive string templates.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T02:36:44Z",
    "updated_at": "2025-11-04T02:36:45Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488445056",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488445056"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488445056"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488445056/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 42,
    "original_start_line": 41,
    "start_side": "RIGHT",
    "line": 65,
    "original_line": 64,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 64,
    "position": 65,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488484569",
    "pull_request_review_id": 3413776699,
    "id": 2488484569,
    "node_id": "PRRC_kwDOKSXUF86UU0LZ",
    "diff_hunk": "@@ -0,0 +1,152 @@\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System.Text;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ContextCompression;\n+\n+/// <summary>\n+/// Auto-compressor using rule-based text compression for document content reduction.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// Compresses documents by extracting the most relevant sentences based on keyword importance\n+/// and position in the document. This is a production implementation that doesn't require\n+/// external ML models.\n+/// </remarks>\n+public class AutoCompressor<T>\n+{\n+    private readonly int _maxOutputLength;\n+    private readonly double _compressionRatio;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"AutoCompressor{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"maxOutputLength\">Maximum length of compressed output in characters.</param>\n+    /// <param name=\"compressionRatio\">Target compression ratio (0-1).</param>\n+    public AutoCompressor(int maxOutputLength = 500, double compressionRatio = 0.5)\n+    {\n+        if (maxOutputLength <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(maxOutputLength), \"Max output length must be positive\");\n+\n+        if (compressionRatio <= 0 || compressionRatio > 1)\n+            throw new ArgumentOutOfRangeException(nameof(compressionRatio), \"Compression ratio must be between 0 and 1\");\n+\n+        _maxOutputLength = maxOutputLength;\n+        _compressionRatio = compressionRatio;\n+    }",
    "path": "src/RetrievalAugmentedGeneration/ContextCompression/AutoCompressor.cs",
    "commit_id": "2e85607917a4ef53ec4ed06ecd3d2cbc46bd3626",
    "original_commit_id": "0990756bfaebc406430e06b89f59a191b1f29d3a",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Unused generic type parameter T.**\n\nThe class declares a generic type parameter `T` but never uses it. Unlike `SelectiveContextCompressor<T>` (which uses `INumericOperations<T>` for numeric operations), this class stores `_compressionRatio` as `double` and performs all calculations using primitive numeric types. The `Document<T>` generic is independent of the class's type parameter.\n\nEither remove the generic parameter entirely or add `INumericOperations<T>` to maintain consistency with the RAG framework's design pattern.\n\n\n\nApply this diff to remove the unused generic:\n\n```diff\n-public class AutoCompressor<T>\n+public class AutoCompressor\n```\n\nAnd update the method signature:\n\n```diff\n-public IEnumerable<Document<T>> Compress(string query, IEnumerable<Document<T>> documents)\n+public IEnumerable<Document> Compress(string query, IEnumerable<Document> documents)\n```\n\nNote: If you intend to use `INumericOperations<T>` in the future, keep the generic but add the numeric operations support.\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T02:58:04Z",
    "updated_at": "2025-11-04T02:58:04Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488484569",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488484569"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488484569"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488484569/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 15,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 35,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 35,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488484571",
    "pull_request_review_id": 3413776699,
    "id": 2488484571,
    "node_id": "PRRC_kwDOKSXUF86UU0Lb",
    "diff_hunk": "@@ -0,0 +1,152 @@\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System.Text;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ContextCompression;\n+\n+/// <summary>\n+/// Auto-compressor using rule-based text compression for document content reduction.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// Compresses documents by extracting the most relevant sentences based on keyword importance\n+/// and position in the document. This is a production implementation that doesn't require\n+/// external ML models.\n+/// </remarks>\n+public class AutoCompressor<T>\n+{\n+    private readonly int _maxOutputLength;\n+    private readonly double _compressionRatio;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"AutoCompressor{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"maxOutputLength\">Maximum length of compressed output in characters.</param>\n+    /// <param name=\"compressionRatio\">Target compression ratio (0-1).</param>\n+    public AutoCompressor(int maxOutputLength = 500, double compressionRatio = 0.5)\n+    {\n+        if (maxOutputLength <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(maxOutputLength), \"Max output length must be positive\");\n+\n+        if (compressionRatio <= 0 || compressionRatio > 1)\n+            throw new ArgumentOutOfRangeException(nameof(compressionRatio), \"Compression ratio must be between 0 and 1\");\n+\n+        _maxOutputLength = maxOutputLength;\n+        _compressionRatio = compressionRatio;\n+    }\n+\n+    /// <summary>\n+    /// Compresses documents using rule-based sentence extraction.\n+    /// </summary>\n+    public IEnumerable<Document<T>> Compress(string query, IEnumerable<Document<T>> documents)\n+    {\n+        if (string.IsNullOrWhiteSpace(query))\n+            throw new ArgumentException(\"Query cannot be null or whitespace\", nameof(query));\n+\n+        if (documents == null)\n+            throw new ArgumentNullException(nameof(documents));\n+\n+        var queryTokens = new HashSet<string>(query.ToLowerInvariant().Split(new[] { ' ', '\\t', '\\n', '\\r' }, StringSplitOptions.RemoveEmptyEntries));\n+        var compressedDocs = new List<Document<T>>();\n+\n+        foreach (var doc in documents)\n+        {\n+            var compressed = CompressDocument(doc.Content, queryTokens);\n+            var compressedDoc = new Document<T>(doc.Id, compressed)\n+            {\n+                RelevanceScore = doc.RelevanceScore,\n+                HasRelevanceScore = doc.HasRelevanceScore\n+            };\n+            foreach (var kvp in doc.Metadata)\n+            {\n+                compressedDoc.Metadata[kvp.Key] = kvp.Value;\n+            }\n+            compressedDocs.Add(compressedDoc);\n+        }\n+\n+        return compressedDocs;\n+    }\n+\n+    private string CompressDocument(string content, HashSet<string> queryTokens)\n+    {\n+        if (string.IsNullOrWhiteSpace(content))\n+            return string.Empty;\n+\n+        var sentences = SplitIntoSentences(content);\n+        if (sentences.Count == 0)\n+            return string.Empty;\n+\n+        var targetSentenceCount = Math.Max(1, (int)(sentences.Count * _compressionRatio));\n+        \n+        var scoredSentences = sentences\n+            .Select((sentence, index) => new\n+            {\n+                Sentence = sentence,\n+                Score = ScoreSentence(sentence, queryTokens, index, sentences.Count)\n+            })\n+            .OrderByDescending(x => x.Score)\n+            .Take(targetSentenceCount)\n+            .OrderBy(x => sentences.IndexOf(x.Sentence))\n+            .Select(x => x.Sentence)\n+            .ToList();",
    "path": "src/RetrievalAugmentedGeneration/ContextCompression/AutoCompressor.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "0990756bfaebc406430e06b89f59a191b1f29d3a",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Performance: O(n┬▓) complexity in sentence ordering.**\n\nLine 88 uses `OrderBy(x => sentences.IndexOf(x.Sentence))` which calls `IndexOf` for each selected sentence, resulting in O(n┬▓) complexity. For documents with many sentences, this could cause noticeable slowdowns.\n\n\n\nApply this diff to track indices during scoring:\n\n```diff\n var scoredSentences = sentences\n-    .Select((sentence, index) => new\n+    .Select((sentence, idx) => new\n     {\n         Sentence = sentence,\n-        Score = ScoreSentence(sentence, queryTokens, index, sentences.Count)\n+        Score = ScoreSentence(sentence, queryTokens, idx, sentences.Count),\n+        OriginalIndex = idx\n     })\n     .OrderByDescending(x => x.Score)\n     .Take(targetSentenceCount)\n-    .OrderBy(x => sentences.IndexOf(x.Sentence))\n+    .OrderBy(x => x.OriginalIndex)\n     .Select(x => x.Sentence)\n     .ToList();\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        var scoredSentences = sentences\n            .Select((sentence, idx) => new\n            {\n                Sentence = sentence,\n                Score = ScoreSentence(sentence, queryTokens, idx, sentences.Count),\n                OriginalIndex = idx\n            })\n            .OrderByDescending(x => x.Score)\n            .Take(targetSentenceCount)\n            .OrderBy(x => x.OriginalIndex)\n            .Select(x => x.Sentence)\n            .ToList();\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/ContextCompression/AutoCompressor.cs around\nlines 80 to 90, the current code reorders selected sentences by calling\nsentences.IndexOf for each item which creates O(n┬▓) behavior; capture the\noriginal index during the Select that computes scores (e.g., include Index field\nalongside Sentence and Score), then when ordering the top scored sentences\nrestore original document order using that captured Index (OrderBy(x =>\nx.Index)) before selecting Sentence, eliminating repeated IndexOf calls and\nreducing complexity to O(n log n).\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T02:58:04Z",
    "updated_at": "2025-11-04T02:58:05Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488484571",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488484571"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2488484571"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2488484571/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 77,
    "original_start_line": 80,
    "start_side": "RIGHT",
    "line": 87,
    "original_line": 90,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 90,
    "position": 87,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595736",
    "pull_request_review_id": 3416572453,
    "id": 2490595736,
    "node_id": "PRRC_kwDOKSXUF86Uc3mY",
    "diff_hunk": "@@ -0,0 +1,26 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.RAG.ContextCompression\n+{\n+    public abstract class ContextCompressorBase<T>\n+        where T : struct, IComparable, IConvertible, IFormattable\n+    {\n+        public List<Document<T>> Compress(\n+            List<Document<T>> documents,\n+            string query,\n+            Dictionary<string, object>? options = null)\n+        {\n+            if (documents == null || documents.Count == 0)\n+            {\n+                return new List<Document<T>>();\n+            }\n+\n+            return CompressCore(documents, query, options);\n+        }\n+\n+        protected abstract List<Document<T>> CompressCore(\n+            List<Document<T>> documents,\n+            string query,\n+            Dictionary<string, object>? options = null);",
    "path": "src/RetrievalAugmentedGeneration/ContextCompression/ContextCompressorBase.cs",
    "commit_id": "2e85607917a4ef53ec4ed06ecd3d2cbc46bd3626",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Restore missing imports for `Document<T>`.**\n\n`Document<T>` lives in `AiDotNet.RetrievalAugmentedGeneration.Models`, but this file imports only `AiDotNet.Interfaces`. The build log (CS0246) shows the compiler cannot resolve `Document<T>`, so the project currently fails to compile. Add the correct namespace (and the collections namespace, unless brought in globally) before merging.  \n\n\n```diff\n-using AiDotNet.Interfaces;\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System.Collections.Generic;\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\nusing AiDotNet.Interfaces;\nusing AiDotNet.RetrievalAugmentedGeneration.Models;\nusing System.Collections.Generic;\n\nnamespace AiDotNet.RAG.ContextCompression\n{\n    public abstract class ContextCompressorBase<T>\n        where T : struct, IComparable, IConvertible, IFormattable\n    {\n        public List<Document<T>> Compress(\n            List<Document<T>> documents,\n            string query,\n            Dictionary<string, object>? options = null)\n        {\n            if (documents == null || documents.Count == 0)\n            {\n                return new List<Document<T>>();\n            }\n\n            return CompressCore(documents, query, options);\n        }\n\n        protected abstract List<Document<T>> CompressCore(\n            List<Document<T>> documents,\n            string query,\n            Dictionary<string, object>? options = null);\n}\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒº░ Tools</summary>\n\n<details>\n<summary>≡ƒ¬¢ GitHub Actions: Build</summary>\n\n[error] 9-9: dotnet build failed: CS0246: The type or namespace name 'Document<>' could not be found (are you missing a using directive or an assembly reference?)\n\n</details>\n<details>\n<summary>≡ƒ¬¢ GitHub Check: Build All Frameworks</summary>\n\n[failure] 21-21: \nThe type or namespace name 'Document<>' could not be found (are you missing a using directive or an assembly reference?)\n\n---\n\n[failure] 22-22: \nThe type or namespace name 'Document<>' could not be found (are you missing a using directive or an assembly reference?)\n\n---\n\n[failure] 8-8: \nThe type or namespace name 'Document<>' could not be found (are you missing a using directive or an assembly reference?)\n\n---\n\n[failure] 9-9: \nThe type or namespace name 'Document<>' could not be found (are you missing a using directive or an assembly reference?)\n\n</details>\n<details>\n<summary>≡ƒ¬¢ GitHub Check: Publish Size Analysis</summary>\n\n[failure] 21-21: \nThe type or namespace name 'Document<>' could not be found (are you missing a using directive or an assembly reference?)\n\n---\n\n[failure] 22-22: \nThe type or namespace name 'Document<>' could not be found (are you missing a using directive or an assembly reference?)\n\n---\n\n[failure] 8-8: \nThe type or namespace name 'Document<>' could not be found (are you missing a using directive or an assembly reference?)\n\n---\n\n[failure] 9-9: \nThe type or namespace name 'Document<>' could not be found (are you missing a using directive or an assembly reference?)\n\n</details>\n\n</details>\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RAG/ContextCompression/ContextCompressorBase.cs around lines 1 to 24, the\ncompiler cannot resolve Document<T> because the file is missing the namespace\nimport for AiDotNet.RetrievalAugmentedGeneration.Models and the collections\nnamespace; add using AiDotNet.RetrievalAugmentedGeneration.Models; and using\nSystem.Collections.Generic; at the top of the file (retain the existing\nAiDotNet.Interfaces import) so Document<T> and List/Dictionary types are\nresolved.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits e0f1f5e to b07884d",
    "created_at": "2025-11-04T13:49:47Z",
    "updated_at": "2025-11-04T14:42:14Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490595736",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595736"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490595736"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595736/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 1,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 24,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 24,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595755",
    "pull_request_review_id": 3416572453,
    "id": 2490595755,
    "node_id": "PRRC_kwDOKSXUF86Uc3mr",
    "diff_hunk": "@@ -0,0 +1,89 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.RAG.ContextCompression\n+{\n+    public class DocumentSummarizer<T> : ContextCompressorBase<T>\n+        where T : struct, IComparable, IConvertible, IFormattable\n+    {\n+        private readonly int _maxSentences;\n+        private readonly bool _extractive;\n+\n+        public DocumentSummarizer(int maxSentences = 5, bool extractive = true)\n+        {\n+            _maxSentences = maxSentences;\n+            _extractive = extractive;\n+        }\n+\n+        protected override List<Document<T>> CompressCore(\n+            List<Document<T>> documents,\n+            string query,\n+            Dictionary<string, object>? options = null)\n+        {\n+            var summarized = new List<Document<T>>();\n+            \n+            foreach (var doc in documents)\n+            {\n+                var summary = _extractive \n+                    ? ExtractiveSummarize(doc.Content, query)\n+                    : SimpleAbstractiveSummarize(doc.Content);\n+                \n+                summarized.Add(new Document<T>\n+                {\n+                    Id = doc.Id,\n+                    Content = summary,\n+                    Metadata = doc.Metadata,\n+                    Embedding = doc.Embedding\n+                });\n+            }",
    "path": "src/RAG/ContextCompression/DocumentSummarizer.cs",
    "commit_id": "c035d8f033d8076130c2f72ee5ff1916ff1cfea1",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Add the missing Document namespace import to restore compilation**\n\n`Document<T>` lives outside this fileΓÇÖs current imports, so the compiler canΓÇÖt resolve the type here (the build log already flags it). Please add the correct `using` directive (for example `AiDotNet.RetrievalAugmentedGeneration.Models`) or fully qualify the type before merging.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RAG/ContextCompression/DocumentSummarizer.cs around lines 30 to 37 the\ntype Document<T> is unresolved because its namespace is not imported; add the\nappropriate using directive (for example using\nAiDotNet.RetrievalAugmentedGeneration.Models;) at the top of the file or fully\nqualify the type (AiDotNet.RetrievalAugmentedGeneration.Models.Document<T>)\nwhere itΓÇÖs instantiated so the compiler can resolve the type and the project\nwill build.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits e0f1f5e to b07884d",
    "created_at": "2025-11-04T13:49:47Z",
    "updated_at": "2025-11-04T14:42:18Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490595755",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595755"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490595755"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595755/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 30,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 37,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 37,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595762",
    "pull_request_review_id": 3416572453,
    "id": 2490595762,
    "node_id": "PRRC_kwDOKSXUF86Uc3my",
    "diff_hunk": "@@ -0,0 +1,74 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.RAG.ContextCompression\n+{\n+    public class LLMContextCompressor<T> : ContextCompressorBase<T>\n+        where T : struct, IComparable, IConvertible, IFormattable\n+    {\n+        private readonly string _model;\n+        private readonly int _maxTokens;\n+\n+        public LLMContextCompressor(string model = \"gpt-3.5-turbo\", int maxTokens = 500)\n+        {\n+            _model = model;\n+            _maxTokens = maxTokens;\n+        }\n+\n+        protected override List<Document<T>> CompressCore(\n+            List<Document<T>> documents,\n+            string query,\n+            Dictionary<string, object>? options = null)\n+        {\n+            var compressed = new List<Document<T>>();\n+            \n+            foreach (var doc in documents)\n+            {\n+                var tokens = EstimateTokens(doc.Content);\n+                \n+                if (tokens <= _maxTokens)\n+                {\n+                    compressed.Add(doc);\n+                }\n+                else\n+                {\n+                    var summarized = SummarizeDocument(doc.Content, query);\n+                    compressed.Add(new Document<T>\n+                    {\n+                        Id = doc.Id,\n+                        Content = summarized,\n+                        Metadata = doc.Metadata,\n+                        Embedding = doc.Embedding\n+                    });\n+                }\n+            }",
    "path": "src/RAG/ContextCompression/LLMContextCompressor.cs",
    "commit_id": "c035d8f033d8076130c2f72ee5ff1916ff1cfea1",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Import the Document<T> namespace so this file compiles**\n\n`Document<T>` isnΓÇÖt defined in the namespaces currently imported here, which causes the compiler error reported in CI. Add the proper `using` (for example `AiDotNet.RetrievalAugmentedGeneration.Models`) or fully qualify the type before merging.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RAG/ContextCompression/LLMContextCompressor.cs around lines 35 to 43, the\ncode references Document<T> but the file lacks the namespace import that defines\nit, causing a compile error; fix by adding the appropriate using statement (for\nexample: using AiDotNet.RetrievalAugmentedGeneration.Models;) at the top of the\nfile or by fully qualifying the type where itΓÇÖs used (e.g.,\nAiDotNet.RetrievalAugmentedGeneration.Models.Document<T>) so the compiler can\nresolve Document<T>.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits e0f1f5e to b07884d",
    "created_at": "2025-11-04T13:49:47Z",
    "updated_at": "2025-11-04T14:42:17Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490595762",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595762"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490595762"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595762/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 35,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 43,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 43,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595772",
    "pull_request_review_id": 3416572453,
    "id": 2490595772,
    "node_id": "PRRC_kwDOKSXUF86Uc3m8",
    "diff_hunk": "@@ -0,0 +1,70 @@\n+using AiDotNet.Interfaces;\n+using System.Text;\n+\n+namespace AiDotNet.RAG.QueryExpansion\n+{\n+    public class HyDEQueryExpansion<T> : QueryExpansionBase<T>\n+        where T : struct, IComparable, IConvertible, IFormattable\n+    {\n+        private readonly string _model;\n+        private readonly int _numHypotheticalDocs;\n+\n+        public HyDEQueryExpansion(string model = \"gpt-3.5-turbo\", int numHypotheticalDocs = 3)\n+        {\n+            _model = model;\n+            _numHypotheticalDocs = numHypotheticalDocs;\n+        }\n+\n+        protected override List<string> ExpandCore(string query, Dictionary<string, object>? options = null)\n+        {\n+            var expandedQueries = new List<string> { query };\n+            \n+            // Generate hypothetical documents\n+            for (int i = 0; i < _numHypotheticalDocs; i++)\n+            {\n+                var hypotheticalDoc = GenerateHypotheticalDocument(query, i);\n+                expandedQueries.Add(hypotheticalDoc);\n+            }\n+            \n+            return expandedQueries;\n+        }\n+\n+        private string GenerateHypotheticalDocument(string query, int variation)\n+        {\n+            var sb = new StringBuilder();\n+            \n+            // Generate a hypothetical answer based on the query\n+            if (query.ToLower().StartsWith(\"what is\"))\n+            {\n+                var topic = query.Substring(7).Trim('?', ' ');\n+                sb.AppendLine($\"{topic} is a concept that involves several key aspects.\");\n+                sb.AppendLine($\"The fundamental principles of {topic} include various components and features.\");\n+                sb.AppendLine($\"Understanding {topic} requires knowledge of its applications and implications.\");\n+            }\n+            else if (query.ToLower().StartsWith(\"how to\") || query.ToLower().StartsWith(\"how do\"))\n+            {\n+                var task = query.Substring(query.IndexOf(\"to\") + 2).Trim('?', ' ');\n+                sb.AppendLine($\"To {task}, follow these general steps:\");\n+                sb.AppendLine($\"1. Prepare the necessary components and understand the requirements.\");\n+                sb.AppendLine($\"2. Execute the process systematically following best practices.\");\n+                sb.AppendLine($\"3. Verify the results and make adjustments as needed.\");\n+            }",
    "path": "src/RAG/QueryExpansion/HyDEQueryExpansion.cs",
    "commit_id": "c035d8f033d8076130c2f72ee5ff1916ff1cfea1",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Fix ΓÇ£how do ΓÇªΓÇ¥ parsing to avoid mangled expansions**\n\nFor queries like ΓÇ£How do I configureΓÇªΓÇ¥, `query.IndexOf(\"to\")` returns -1, so `Substring(-1 + 2)` becomes `Substring(1)` and we lose the leading ΓÇ£HΓÇ¥. The generated hypothetical document starts with ΓÇ£ow doΓÇªΓÇ¥, degrading retrieval quality for a very common query pattern. Split the handling so that ΓÇ£how to ΓÇªΓÇ¥ and ΓÇ£how do ΓÇªΓÇ¥ are parsed independently instead of always searching for `\"to\"`.\n\n```diff\n-            else if (query.ToLower().StartsWith(\"how to\") || query.ToLower().StartsWith(\"how do\"))\n-            {\n-                var task = query.Substring(query.IndexOf(\"to\") + 2).Trim('?', ' ');\n+            else if (queryLower.StartsWith(\"how to\"))\n+            {\n+                var task = query.Substring(6).Trim('?', ' ');\n+                sb.AppendLine($\"To {task}, follow these general steps:\");\n+                sb.AppendLine($\"1. Prepare the necessary components and understand the requirements.\");\n+                sb.AppendLine($\"2. Execute the process systematically following best practices.\");\n+                sb.AppendLine($\"3. Verify the results and make adjustments as needed.\");\n+            }\n+            else if (queryLower.StartsWith(\"how do\"))\n+            {\n+                var task = query.Substring(6).Trim('?', ' ');\n                 sb.AppendLine($\"To {task}, follow these general steps:\");\n                 sb.AppendLine($\"1. Prepare the necessary components and understand the requirements.\");\n                 sb.AppendLine($\"2. Execute the process systematically following best practices.\");\n                 sb.AppendLine($\"3. Verify the results and make adjustments as needed.\");\n             }\n```\n\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T13:49:48Z",
    "updated_at": "2025-11-04T13:49:56Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490595772",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595772"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490595772"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595772/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 44,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 51,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 51,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595779",
    "pull_request_review_id": 3416572453,
    "id": 2490595779,
    "node_id": "PRRC_kwDOKSXUF86Uc3nD",
    "diff_hunk": "@@ -0,0 +1,163 @@\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Text.RegularExpressions;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies\n+{\n+    /// <summary>\n+    /// Chunking strategy that splits text based on header hierarchies\n+    /// </summary>\n+    public class HeaderBasedTextSplitter : ChunkingStrategyBase\n+    {\n+        private readonly Dictionary<int, string> _headerPatterns;\n+        private readonly int _chunkSize;\n+        private readonly int _chunkOverlap;\n+\n+        public HeaderBasedTextSplitter(int chunkSize = 1000, int chunkOverlap = 200)\n+        {\n+            _chunkSize = chunkSize;\n+            _chunkOverlap = chunkOverlap;\n+            _headerPatterns = new Dictionary<int, string>\n+            {\n+                { 1, @\"^#\\s+(.+)$\" },\n+                { 2, @\"^##\\s+(.+)$\" },\n+                { 3, @\"^###\\s+(.+)$\" },\n+                { 4, @\"^####\\s+(.+)$\" },\n+                { 5, @\"^#####\\s+(.+)$\" },\n+                { 6, @\"^######\\s+(.+)$\" }\n+            };\n+        }",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/HeaderBasedTextSplitter.cs",
    "commit_id": "33a79e89f3137dc80c940c48d7ab74f127dd4d91",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_≡ƒ¢á∩╕Å Refactor suggestion_ | _≡ƒƒá Major_\n\n**Add parameter validation to the constructor.**\n\nThe constructor accepts `chunkSize` and `chunkOverlap` without validating their values. Invalid parameters could cause runtime issues or unexpected behavior in the chunking logic.\n\n\n\nApply this diff to add validation:\n\n```diff\n public HeaderBasedTextSplitter(int chunkSize = 1000, int chunkOverlap = 200)\n {\n+    if (chunkSize <= 0)\n+        throw new ArgumentOutOfRangeException(nameof(chunkSize), \"Chunk size must be greater than zero\");\n+    if (chunkOverlap < 0)\n+        throw new ArgumentOutOfRangeException(nameof(chunkOverlap), \"Chunk overlap cannot be negative\");\n+    if (chunkOverlap >= chunkSize)\n+        throw new ArgumentException($\"Chunk overlap ({chunkOverlap}) must be less than chunk size ({chunkSize})\");\n+        \n     _chunkSize = chunkSize;\n     _chunkOverlap = chunkOverlap;\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        public HeaderBasedTextSplitter(int chunkSize = 1000, int chunkOverlap = 200)\n        {\n            if (chunkSize <= 0)\n                throw new ArgumentOutOfRangeException(nameof(chunkSize), \"Chunk size must be greater than zero\");\n            if (chunkOverlap < 0)\n                throw new ArgumentOutOfRangeException(nameof(chunkOverlap), \"Chunk overlap cannot be negative\");\n            if (chunkOverlap >= chunkSize)\n                throw new ArgumentException($\"Chunk overlap ({chunkOverlap}) must be less than chunk size ({chunkSize})\");\n            \n            _chunkSize = chunkSize;\n            _chunkOverlap = chunkOverlap;\n            _headerPatterns = new Dictionary<int, string>\n            {\n                { 1, @\"^#\\s+(.+)$\" },\n                { 2, @\"^##\\s+(.+)$\" },\n                { 3, @\"^###\\s+(.+)$\" },\n                { 4, @\"^####\\s+(.+)$\" },\n                { 5, @\"^#####\\s+(.+)$\" },\n                { 6, @\"^######\\s+(.+)$\" }\n            };\n        }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn\nsrc/RetrievalAugmentedGeneration/ChunkingStrategies/HeaderBasedTextSplitter.cs\naround lines 18 to 31, the constructor accepts chunkSize and chunkOverlap\nwithout validation; add parameter checks to guard against invalid values by\nthrowing ArgumentOutOfRangeException (or ArgumentException) when chunkSize is\nless than or equal to zero, when chunkOverlap is negative, and when chunkOverlap\nis greater than or equal to chunkSize (or otherwise invalid for your chunking\nlogic); perform these checks at the top of the constructor and only assign to\n_chunkSize and _chunkOverlap after validations pass, including clear exception\nmessages indicating which parameter is invalid and why.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T13:49:48Z",
    "updated_at": "2025-11-04T13:49:56Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490595779",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595779"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490595779"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595779/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 18,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 31,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 31,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595786",
    "pull_request_review_id": 3416572453,
    "id": 2490595786,
    "node_id": "PRRC_kwDOKSXUF86Uc3nK",
    "diff_hunk": "@@ -0,0 +1,163 @@\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Text.RegularExpressions;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies\n+{\n+    /// <summary>\n+    /// Chunking strategy that splits text based on header hierarchies\n+    /// </summary>\n+    public class HeaderBasedTextSplitter : ChunkingStrategyBase\n+    {\n+        private readonly Dictionary<int, string> _headerPatterns;\n+        private readonly int _chunkSize;\n+        private readonly int _chunkOverlap;\n+\n+        public HeaderBasedTextSplitter(int chunkSize = 1000, int chunkOverlap = 200)\n+        {\n+            _chunkSize = chunkSize;\n+            _chunkOverlap = chunkOverlap;\n+            _headerPatterns = new Dictionary<int, string>\n+            {\n+                { 1, @\"^#\\s+(.+)$\" },\n+                { 2, @\"^##\\s+(.+)$\" },\n+                { 3, @\"^###\\s+(.+)$\" },\n+                { 4, @\"^####\\s+(.+)$\" },\n+                { 5, @\"^#####\\s+(.+)$\" },\n+                { 6, @\"^######\\s+(.+)$\" }\n+            };\n+        }\n+\n+        protected override List<TextChunk> SplitCore(string text, Dictionary<string, string>? metadata = null)\n+        {\n+            if (string.IsNullOrWhiteSpace(text))\n+                return new List<TextChunk>();\n+\n+            var lines = text.Split(new[] { '\\r', '\\n' }, StringSplitOptions.None);\n+            var chunks = new List<TextChunk>();\n+            var currentSection = new List<string>();\n+            var currentHeaders = new Stack<(int level, string text)>();\n+            var chunkIndex = 0;\n+\n+            foreach (var line in lines)\n+            {\n+                var headerMatch = false;\n+                foreach (var (level, pattern) in _headerPatterns.OrderBy(x => x.Key))\n+                {\n+                    var match = Regex.Match(line, pattern, RegexOptions.Multiline);\n+                    if (match.Success)\n+                    {\n+                        if (currentSection.Count > 0)\n+                        {\n+                            chunks.AddRange(CreateChunksFromSection(\n+                                string.Join(Environment.NewLine, currentSection),\n+                                currentHeaders,\n+                                metadata,\n+                                ref chunkIndex));\n+                            currentSection.Clear();\n+                        }\n+\n+                        while (currentHeaders.Count > 0 && currentHeaders.Peek().level >= level)\n+                        {\n+                            currentHeaders.Pop();\n+                        }\n+                        currentHeaders.Push((level, match.Groups[1].Value));\n+                        headerMatch = true;\n+                        break;\n+                    }\n+                }\n+\n+                currentSection.Add(line);\n+            }\n+\n+            if (currentSection.Count > 0)\n+            {\n+                chunks.AddRange(CreateChunksFromSection(\n+                    string.Join(Environment.NewLine, currentSection),\n+                    currentHeaders,\n+                    metadata,\n+                    ref chunkIndex));\n+            }\n+\n+            return chunks;\n+        }\n+\n+        private List<TextChunk> CreateChunksFromSection(\n+            string sectionText,\n+            Stack<(int level, string text)> headers,\n+            Dictionary<string, string>? metadata,\n+            ref int chunkIndex)\n+        {\n+            var chunks = new List<TextChunk>();\n+            var headerPath = string.Join(\" > \", headers.Reverse().Select(h => h.text));\n+\n+            if (sectionText.Length <= _chunkSize)\n+            {\n+                var chunkMetadata = new Dictionary<string, string>(metadata ?? new Dictionary<string, string>())\n+                {\n+                    [\"header_path\"] = headerPath,\n+                    [\"chunk_index\"] = chunkIndex.ToString()\n+                };\n+\n+                chunks.Add(new TextChunk\n+                {\n+                    Text = sectionText,\n+                    Metadata = chunkMetadata\n+                });\n+                chunkIndex++;\n+            }\n+            else\n+            {\n+                var words = sectionText.Split(new[] { ' ', '\\t' }, StringSplitOptions.RemoveEmptyEntries);\n+                var currentChunk = new List<string>();\n+                var currentLength = 0;\n+\n+                foreach (var word in words)\n+                {\n+                    if (currentLength + word.Length + 1 > _chunkSize && currentChunk.Count > 0)\n+                    {\n+                        var chunkMetadata = new Dictionary<string, string>(metadata ?? new Dictionary<string, string>())\n+                        {\n+                            [\"header_path\"] = headerPath,\n+                            [\"chunk_index\"] = chunkIndex.ToString()\n+                        };\n+\n+                        chunks.Add(new TextChunk\n+                        {\n+                            Text = string.Join(\" \", currentChunk),\n+                            Metadata = chunkMetadata\n+                        });\n+                        chunkIndex++;\n+\n+                        var overlapWords = currentChunk.TakeLast((int)(_chunkOverlap / 5.0)).ToList();\n+                        currentChunk = overlapWords;\n+                        currentLength = overlapWords.Sum(w => w.Length + 1);\n+                    }\n+\n+                    currentChunk.Add(word);\n+                    currentLength += word.Length + 1;\n+                }",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/HeaderBasedTextSplitter.cs",
    "commit_id": "33a79e89f3137dc80c940c48d7ab74f127dd4d91",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Fix overlap calculation to respect character-based configuration.**\n\nThe overlap logic on line 134 uses `(int)(_chunkOverlap / 5.0)` which assumes an average word length of 5 characters. This arbitrary heuristic doesn't respect the configured `_chunkOverlap` value and will produce inconsistent results depending on actual word lengths.\n\nFor example, if `_chunkOverlap = 200`:\n- With 3-character words: actual overlap Γëê 120 characters (40% less than configured)\n- With 10-character words: actual overlap Γëê 400 characters (100% more than configured)\n\n\n\nApply this diff to use proper character-based overlap tracking:\n\n```diff\n             var words = sectionText.Split(new[] { ' ', '\\t' }, StringSplitOptions.RemoveEmptyEntries);\n             var currentChunk = new List<string>();\n             var currentLength = 0;\n\n             foreach (var word in words)\n             {\n                 if (currentLength + word.Length + 1 > _chunkSize && currentChunk.Count > 0)\n                 {\n                     var chunkMetadata = new Dictionary<string, string>(metadata ?? new Dictionary<string, string>())\n                     {\n                         [\"header_path\"] = headerPath,\n                         [\"chunk_index\"] = chunkIndex.ToString()\n                     };\n\n                     chunks.Add(new TextChunk\n                     {\n                         Text = string.Join(\" \", currentChunk),\n                         Metadata = chunkMetadata\n                     });\n                     chunkIndex++;\n\n-                    var overlapWords = currentChunk.TakeLast((int)(_chunkOverlap / 5.0)).ToList();\n-                    currentChunk = overlapWords;\n-                    currentLength = overlapWords.Sum(w => w.Length + 1);\n+                    // Keep words from the end until we reach the overlap size\n+                    var overlapWords = new List<string>();\n+                    var overlapLength = 0;\n+                    for (int i = currentChunk.Count - 1; i >= 0 && overlapLength < _chunkOverlap; i--)\n+                    {\n+                        overlapWords.Insert(0, currentChunk[i]);\n+                        overlapLength += currentChunk[i].Length + 1; // +1 for space\n+                    }\n+                    currentChunk = overlapWords;\n+                    currentLength = overlapLength;\n                 }\n\n                 currentChunk.Add(word);\n                 currentLength += word.Length + 1;\n             }\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn\nsrc/RetrievalAugmentedGeneration/ChunkingStrategies/HeaderBasedTextSplitter.cs\naround lines 113 to 141, the overlap is computed by converting a character-based\n_chunkOverlap into a fixed number of words via (int)(_chunkOverlap / 5.0), which\nis inaccurate; replace that heuristic with a character-count based selection:\nwhen creating the overlap after emitting a chunk, walk backward through\ncurrentChunk collecting words until the cumulative length (including spaces) is\nat least _chunkOverlap (or you run out of words), set currentChunk to that\ncollected tail (preserving order), and recompute currentLength as the exact char\nlength of those words plus spaces so the next chunk starts with the configured\ncharacter overlap. Ensure you handle edge cases where _chunkOverlap <= 0 or\nlarger than the chunk by falling back to empty or full chunk appropriately.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commit 5e2879c",
    "created_at": "2025-11-04T13:49:48Z",
    "updated_at": "2025-11-04T14:11:22Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490595786",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595786"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490595786"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595786/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 113,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 141,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 141,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595797",
    "pull_request_review_id": 3416572453,
    "id": 2490595797,
    "node_id": "PRRC_kwDOKSXUF86Uc3nV",
    "diff_hunk": "@@ -0,0 +1,86 @@\n+using AiDotNet.Interfaces;\n+using System.Text.RegularExpressions;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies\n+{\n+    public class MultiModalTextSplitter : ChunkingStrategyBase\n+    {\n+        private readonly int _chunkSize;\n+        private readonly int _chunkOverlap;\n+\n+        public MultiModalTextSplitter(int chunkSize = 1000, int chunkOverlap = 200)\n+        {\n+            _chunkSize = chunkSize;\n+            _chunkOverlap = chunkOverlap;",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/MultiModalTextSplitter.cs",
    "commit_id": "33a79e89f3137dc80c940c48d7ab74f127dd4d91",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Remove unused `_chunkOverlap` field or implement overlap logic.**\n\nThe `_chunkOverlap` parameter is accepted in the constructor and stored but never used in the chunking logic. This is inconsistent with the parameter's name and creates misleading API expectations.\n\n\n\nEither remove the parameter entirely or implement proper overlap logic (e.g., sliding window approach where consecutive chunks share `_chunkOverlap` characters):\n\n```diff\n-        private readonly int _chunkOverlap;\n-\n-        public MultiModalTextSplitter(int chunkSize = 1000, int chunkOverlap = 200)\n+        public MultiModalTextSplitter(int chunkSize = 1000)\n         {\n             _chunkSize = chunkSize;\n-            _chunkOverlap = chunkOverlap;\n         }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        private readonly int _chunkSize;\n\n        public MultiModalTextSplitter(int chunkSize = 1000)\n        {\n            _chunkSize = chunkSize;\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T13:49:48Z",
    "updated_at": "2025-11-04T13:49:56Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490595797",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595797"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490595797"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595797/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 8,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 14,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 14,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595822",
    "pull_request_review_id": 3416572453,
    "id": 2490595822,
    "node_id": "PRRC_kwDOKSXUF86Uc3nu",
    "diff_hunk": "@@ -0,0 +1,86 @@\n+using AiDotNet.Interfaces;\n+using System.Text.RegularExpressions;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies\n+{\n+    public class MultiModalTextSplitter : ChunkingStrategyBase\n+    {\n+        private readonly int _chunkSize;\n+        private readonly int _chunkOverlap;\n+\n+        public MultiModalTextSplitter(int chunkSize = 1000, int chunkOverlap = 200)\n+        {\n+            _chunkSize = chunkSize;\n+            _chunkOverlap = chunkOverlap;\n+        }\n+\n+        protected override List<string> ChunkCore(string text)\n+        {\n+            var chunks = new List<string>();\n+\n+            if (string.IsNullOrWhiteSpace(text))\n+            {\n+                return chunks;\n+            }\n+\n+            // Extract code blocks\n+            var codeBlockPattern = @\"```[\\s\\S]*?```\";\n+            var codeBlocks = Regex.Matches(text, codeBlockPattern);\n+            var codeBlockIndices = new List<Tuple<int, int>>();\n+\n+            foreach (Match match in codeBlocks)\n+            {\n+                codeBlockIndices.Add(Tuple.Create(match.Index, match.Index + match.Length));\n+                chunks.Add(match.Value);\n+            }\n+\n+            // Extract tables\n+            var tablePattern = @\"\\|[^\\n]+\\|\\n\\|[-:\\s|]+\\|(?:\\n\\|[^\\n]+\\|)*\";\n+            var tables = Regex.Matches(text, tablePattern);\n+\n+            foreach (Match match in tables)\n+            {\n+                var index = match.Index;\n+                var endIndex = match.Index + match.Length;\n+                \n+                // Check if this table is not inside a code block\n+                var inCodeBlock = codeBlockIndices.Any(cb => index >= cb.Item1 && endIndex <= cb.Item2);\n+                \n+                if (!inCodeBlock)\n+                {\n+                    chunks.Add(match.Value);\n+                }\n+            }\n+\n+            // Process remaining text\n+            var processedText = Regex.Replace(text, codeBlockPattern, \"\");\n+            processedText = Regex.Replace(processedText, tablePattern, \"\");\n+\n+            var paragraphs = processedText.Split(new[] { \"\\n\\n\" }, StringSplitOptions.RemoveEmptyEntries);\n+            \n+            var currentChunk = \"\";\n+            foreach (var paragraph in paragraphs)\n+            {\n+                if (currentChunk.Length + paragraph.Length <= _chunkSize)\n+                {\n+                    currentChunk += paragraph + \"\\n\\n\";\n+                }\n+                else\n+                {\n+                    if (!string.IsNullOrWhiteSpace(currentChunk))\n+                    {\n+                        chunks.Add(currentChunk.Trim());\n+                    }\n+                    currentChunk = paragraph + \"\\n\\n\";\n+                }\n+            }\n+\n+            if (!string.IsNullOrWhiteSpace(currentChunk))\n+            {\n+                chunks.Add(currentChunk.Trim());\n+            }\n+\n+            return chunks;\n+        }",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/MultiModalTextSplitter.cs",
    "commit_id": "33a79e89f3137dc80c940c48d7ab74f127dd4d91",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Preserve document order and add XML documentation.**\n\nThe current implementation adds code blocks first, then tables, then regular text, which doesn't preserve the original document structure. Additionally, the class and method lack XML documentation.\n\n\n\n\n1. **Document order**: Track positions of extracted elements and emit chunks in original order, or process the document sequentially rather than extracting all code blocks first.\n\n2. **XML documentation**: Add comprehensive documentation per PR objectives:\n\n```diff\n+    /// <summary>\n+    /// Splits text containing code blocks, tables, and regular paragraphs into chunks.\n+    /// Code blocks and tables are extracted as separate chunks, while remaining text is split by paragraphs.\n+    /// </summary>\n     public class MultiModalTextSplitter : ChunkingStrategyBase\n     {\n         ...\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"MultiModalTextSplitter\"/> class.\n+        /// </summary>\n+        /// <param name=\"chunkSize\">Maximum size of each chunk in characters.</param>\n         public MultiModalTextSplitter(int chunkSize = 1000)\n         \n+        /// <summary>\n+        /// Splits the text into chunks by extracting code blocks, tables, and paragraphs.\n+        /// </summary>\n+        /// <param name=\"text\">The text to split.</param>\n+        /// <returns>A list of text chunks.</returns>\n         protected override List<string> ChunkCore(string text)\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/ChunkingStrategies/MultiModalTextSplitter.cs\naround lines 17 to 84, the method extracts code blocks and tables first which\nbreaks original document order and the class/method lack XML documentation;\nchange the implementation to scan the document once and build a list of spans\n(type, start, end, text) for code blocks, tables and the remaining text\nsegments, skip table spans that fall inside code-block spans, then sort the\nspans by start index and emit chunks in that sorted order (for text spans apply\nthe existing paragraph-splitting and _chunkSize logic, splitting long text spans\ninto multiple chunks as needed), and add appropriate XML documentation comments\nto the class and ChunkCore method describing purpose, parameters, return value\nand behavior (including how code blocks/tables are preserved and how text is\nchunked).\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commit 5e2879c",
    "created_at": "2025-11-04T13:49:49Z",
    "updated_at": "2025-11-04T14:11:26Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490595822",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595822"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490595822"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595822/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 17,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 84,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 84,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595829",
    "pull_request_review_id": 3416572453,
    "id": 2490595829,
    "node_id": "PRRC_kwDOKSXUF86Uc3n1",
    "diff_hunk": "@@ -0,0 +1,86 @@\n+using AiDotNet.Interfaces;\n+using System.Text.RegularExpressions;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies\n+{\n+    public class MultiModalTextSplitter : ChunkingStrategyBase\n+    {\n+        private readonly int _chunkSize;\n+        private readonly int _chunkOverlap;\n+\n+        public MultiModalTextSplitter(int chunkSize = 1000, int chunkOverlap = 200)\n+        {\n+            _chunkSize = chunkSize;\n+            _chunkOverlap = chunkOverlap;\n+        }\n+\n+        protected override List<string> ChunkCore(string text)\n+        {\n+            var chunks = new List<string>();\n+\n+            if (string.IsNullOrWhiteSpace(text))\n+            {\n+                return chunks;\n+            }\n+\n+            // Extract code blocks\n+            var codeBlockPattern = @\"```[\\s\\S]*?```\";\n+            var codeBlocks = Regex.Matches(text, codeBlockPattern);\n+            var codeBlockIndices = new List<Tuple<int, int>>();\n+\n+            foreach (Match match in codeBlocks)\n+            {\n+                codeBlockIndices.Add(Tuple.Create(match.Index, match.Index + match.Length));\n+                chunks.Add(match.Value);\n+            }",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/MultiModalTextSplitter.cs",
    "commit_id": "33a79e89f3137dc80c940c48d7ab74f127dd4d91",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Enforce size limits on code block chunks.**\n\nCode blocks are added directly to chunks without checking if they exceed `_chunkSize`. A large code block will create a chunk that violates the configured size constraint, potentially causing issues in downstream processing (e.g., embedding models with token limits).\n\n\n\nConsider splitting large code blocks:\n\n```diff\n         foreach (Match match in codeBlocks)\n         {\n             codeBlockIndices.Add(Tuple.Create(match.Index, match.Index + match.Length));\n-            chunks.Add(match.Value);\n+            var codeBlock = match.Value;\n+            if (codeBlock.Length <= _chunkSize)\n+            {\n+                chunks.Add(codeBlock);\n+            }\n+            else\n+            {\n+                // Split large code blocks while preserving delimiters\n+                var lines = codeBlock.Split('\\n');\n+                var currentChunk = \"```\\n\";\n+                for (int i = 1; i < lines.Length - 1; i++)\n+                {\n+                    if (currentChunk.Length + lines[i].Length > _chunkSize - 4)\n+                    {\n+                        chunks.Add(currentChunk + \"```\");\n+                        currentChunk = \"```\\n\";\n+                    }\n+                    currentChunk += lines[i] + \"\\n\";\n+                }\n+                if (currentChunk.Length > 4)\n+                {\n+                    chunks.Add(currentChunk + \"```\");\n+                }\n+            }\n         }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n// Extract code blocks\n            var codeBlockPattern = @\"\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T13:49:49Z",
    "updated_at": "2025-11-04T13:49:56Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490595829",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595829"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490595829"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595829/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 26,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 35,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 35,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595834",
    "pull_request_review_id": 3416572453,
    "id": 2490595834,
    "node_id": "PRRC_kwDOKSXUF86Uc3n6",
    "diff_hunk": "@@ -0,0 +1,86 @@\n+using AiDotNet.Interfaces;\n+using System.Text.RegularExpressions;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies\n+{\n+    public class MultiModalTextSplitter : ChunkingStrategyBase\n+    {\n+        private readonly int _chunkSize;\n+        private readonly int _chunkOverlap;\n+\n+        public MultiModalTextSplitter(int chunkSize = 1000, int chunkOverlap = 200)\n+        {\n+            _chunkSize = chunkSize;\n+            _chunkOverlap = chunkOverlap;\n+        }\n+\n+        protected override List<string> ChunkCore(string text)\n+        {\n+            var chunks = new List<string>();\n+\n+            if (string.IsNullOrWhiteSpace(text))\n+            {\n+                return chunks;\n+            }\n+\n+            // Extract code blocks\n+            var codeBlockPattern = @\"```[\\s\\S]*?```\";\n+            var codeBlocks = Regex.Matches(text, codeBlockPattern);\n+            var codeBlockIndices = new List<Tuple<int, int>>();\n+\n+            foreach (Match match in codeBlocks)\n+            {\n+                codeBlockIndices.Add(Tuple.Create(match.Index, match.Index + match.Length));\n+                chunks.Add(match.Value);\n+            }\n+\n+            // Extract tables\n+            var tablePattern = @\"\\|[^\\n]+\\|\\n\\|[-:\\s|]+\\|(?:\\n\\|[^\\n]+\\|)*\";\n+            var tables = Regex.Matches(text, tablePattern);\n+\n+            foreach (Match match in tables)\n+            {\n+                var index = match.Index;\n+                var endIndex = match.Index + match.Length;\n+                \n+                // Check if this table is not inside a code block\n+                var inCodeBlock = codeBlockIndices.Any(cb => index >= cb.Item1 && endIndex <= cb.Item2);\n+                \n+                if (!inCodeBlock)\n+                {\n+                    chunks.Add(match.Value);\n+                }\n+            }",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/MultiModalTextSplitter.cs",
    "commit_id": "33a79e89f3137dc80c940c48d7ab74f127dd4d91",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Enforce size limits on table chunks.**\n\nSimilar to code blocks, tables are added without size validation. Large tables will violate `_chunkSize`.\n\n\n\nApply size checking for tables. Consider referencing the `TableAwareTextSplitter.ProcessTable` method (shown in relevant snippets) which implements `_maxRowsPerChunk` to split large tables while preserving headers.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T13:49:49Z",
    "updated_at": "2025-11-04T13:49:56Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490595834",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595834"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490595834"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595834/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 37,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 53,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 53,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595837",
    "pull_request_review_id": 3416572453,
    "id": 2490595837,
    "node_id": "PRRC_kwDOKSXUF86Uc3n9",
    "diff_hunk": "@@ -0,0 +1,86 @@\n+using AiDotNet.Interfaces;\n+using System.Text.RegularExpressions;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies\n+{\n+    public class MultiModalTextSplitter : ChunkingStrategyBase\n+    {\n+        private readonly int _chunkSize;\n+        private readonly int _chunkOverlap;\n+\n+        public MultiModalTextSplitter(int chunkSize = 1000, int chunkOverlap = 200)\n+        {\n+            _chunkSize = chunkSize;\n+            _chunkOverlap = chunkOverlap;\n+        }\n+\n+        protected override List<string> ChunkCore(string text)\n+        {\n+            var chunks = new List<string>();\n+\n+            if (string.IsNullOrWhiteSpace(text))\n+            {\n+                return chunks;\n+            }\n+\n+            // Extract code blocks\n+            var codeBlockPattern = @\"```[\\s\\S]*?```\";\n+            var codeBlocks = Regex.Matches(text, codeBlockPattern);\n+            var codeBlockIndices = new List<Tuple<int, int>>();\n+\n+            foreach (Match match in codeBlocks)\n+            {\n+                codeBlockIndices.Add(Tuple.Create(match.Index, match.Index + match.Length));\n+                chunks.Add(match.Value);\n+            }\n+\n+            // Extract tables\n+            var tablePattern = @\"\\|[^\\n]+\\|\\n\\|[-:\\s|]+\\|(?:\\n\\|[^\\n]+\\|)*\";\n+            var tables = Regex.Matches(text, tablePattern);\n+\n+            foreach (Match match in tables)\n+            {\n+                var index = match.Index;\n+                var endIndex = match.Index + match.Length;\n+                \n+                // Check if this table is not inside a code block\n+                var inCodeBlock = codeBlockIndices.Any(cb => index >= cb.Item1 && endIndex <= cb.Item2);\n+                \n+                if (!inCodeBlock)\n+                {\n+                    chunks.Add(match.Value);\n+                }\n+            }\n+\n+            // Process remaining text\n+            var processedText = Regex.Replace(text, codeBlockPattern, \"\");\n+            processedText = Regex.Replace(processedText, tablePattern, \"\");\n+\n+            var paragraphs = processedText.Split(new[] { \"\\n\\n\" }, StringSplitOptions.RemoveEmptyEntries);\n+            \n+            var currentChunk = \"\";\n+            foreach (var paragraph in paragraphs)\n+            {\n+                if (currentChunk.Length + paragraph.Length <= _chunkSize)\n+                {\n+                    currentChunk += paragraph + \"\\n\\n\";\n+                }\n+                else\n+                {\n+                    if (!string.IsNullOrWhiteSpace(currentChunk))\n+                    {\n+                        chunks.Add(currentChunk.Trim());\n+                    }\n+                    currentChunk = paragraph + \"\\n\\n\";\n+                }",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/MultiModalTextSplitter.cs",
    "commit_id": "33a79e89f3137dc80c940c48d7ab74f127dd4d91",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Handle paragraphs exceeding chunk size.**\n\nWhen a single paragraph is larger than `_chunkSize`, it's added as-is to `currentChunk` (line 74), creating an oversized chunk. This is inconsistent with the size constraint.\n\n\n\nConsider recursively splitting large paragraphs by sentences or fixed-size segments:\n\n```diff\n             if (currentChunk.Length + paragraph.Length <= _chunkSize)\n             {\n                 currentChunk += paragraph + \"\\n\\n\";\n             }\n             else\n             {\n                 if (!string.IsNullOrWhiteSpace(currentChunk))\n                 {\n                     chunks.Add(currentChunk.Trim());\n                 }\n-                currentChunk = paragraph + \"\\n\\n\";\n+                if (paragraph.Length <= _chunkSize)\n+                {\n+                    currentChunk = paragraph + \"\\n\\n\";\n+                }\n+                else\n+                {\n+                    // Split large paragraph by sentences or fixed size\n+                    var sentences = paragraph.Split(new[] { \". \", \"! \", \"? \" }, StringSplitOptions.RemoveEmptyEntries);\n+                    currentChunk = \"\";\n+                    foreach (var sentence in sentences)\n+                    {\n+                        if (currentChunk.Length + sentence.Length <= _chunkSize)\n+                        {\n+                            currentChunk += sentence + \". \";\n+                        }\n+                        else\n+                        {\n+                            if (!string.IsNullOrWhiteSpace(currentChunk))\n+                            {\n+                                chunks.Add(currentChunk.Trim());\n+                            }\n+                            currentChunk = sentence + \". \";\n+                        }\n+                    }\n+                }\n             }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n                if (currentChunk.Length + paragraph.Length <= _chunkSize)\n                {\n                    currentChunk += paragraph + \"\\n\\n\";\n                }\n                else\n                {\n                    if (!string.IsNullOrWhiteSpace(currentChunk))\n                    {\n                        chunks.Add(currentChunk.Trim());\n                    }\n                    if (paragraph.Length <= _chunkSize)\n                    {\n                        currentChunk = paragraph + \"\\n\\n\";\n                    }\n                    else\n                    {\n                        // Split large paragraph by sentences or fixed size\n                        var sentences = paragraph.Split(new[] { \". \", \"! \", \"? \" }, StringSplitOptions.RemoveEmptyEntries);\n                        currentChunk = \"\";\n                        foreach (var sentence in sentences)\n                        {\n                            if (currentChunk.Length + sentence.Length <= _chunkSize)\n                            {\n                                currentChunk += sentence + \". \";\n                            }\n                            else\n                            {\n                                if (!string.IsNullOrWhiteSpace(currentChunk))\n                                {\n                                    chunks.Add(currentChunk.Trim());\n                                }\n                                currentChunk = sentence + \". \";\n                            }\n                        }\n                    }\n                }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/ChunkingStrategies/MultiModalTextSplitter.cs\naround lines 64 to 75, the code appends an entire paragraph to currentChunk even\nwhen the paragraph itself exceeds _chunkSize, producing oversized chunks; change\nthe logic so that when paragraph.Length > _chunkSize you split that paragraph\ninto smaller segments (preferably by sentences using a sentence-split helper, or\nfallback to fixed-size substrings) and then feed those segments into the same\nchunk-accumulation flow: iterate through sentence/segment pieces, for each piece\nif currentChunk + piece fits append it, otherwise flush currentChunk to chunks\n(after Trim) and start a new currentChunk with the piece (or if piece itself >\n_chunkSize split further into substrings), ensuring no produced chunk exceeds\n_chunkSize and trimming/adding separators consistently.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T13:49:49Z",
    "updated_at": "2025-11-04T13:49:56Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490595837",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595837"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490595837"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595837/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 64,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 75,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 75,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595856",
    "pull_request_review_id": 3416572453,
    "id": 2490595856,
    "node_id": "PRRC_kwDOKSXUF86Uc3oQ",
    "diff_hunk": "@@ -0,0 +1,84 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies\n+{\n+    public class RecursiveCharacterChunkingStrategy : ChunkingStrategyBase\n+    {",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/RecursiveCharacterChunkingStrategy.cs",
    "commit_id": "33a79e89f3137dc80c940c48d7ab74f127dd4d91",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_≡ƒ¢á∩╕Å Refactor suggestion_ | _≡ƒƒá Major_\n\n**Add XML documentation.**\n\nThe class and its public members lack XML documentation comments, which is required per the PR objectives (\"comprehensive XML documentation\").\n\n\n\nAdd documentation:\n\n```diff\n+    /// <summary>\n+    /// Implements a recursive character-based chunking strategy that splits text using a prioritized list of separators.\n+    /// Text is split recursively using the configured separators, and smaller segments are merged into chunks not exceeding the specified size.\n+    /// </summary>\n     public class RecursiveCharacterChunkingStrategy : ChunkingStrategyBase\n```\n\nAlso document the constructor and public behavior.\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    /// <summary>\n    /// Implements a recursive character-based chunking strategy that splits text using a prioritized list of separators.\n    /// Text is split recursively using the configured separators, and smaller segments are merged into chunks not exceeding the specified size.\n    /// </summary>\n    public class RecursiveCharacterChunkingStrategy : ChunkingStrategyBase\n    {\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn\nsrc/RetrievalAugmentedGeneration/ChunkingStrategies/RecursiveCharacterChunkingStrategy.cs\naround lines 5-6, the class and its public members lack XML documentation; add\nXML documentation comments for the class, its public constructor and any public\nmethods/properties describing purpose, parameters, return values, exceptions and\nexamples/remarks where applicable. Specifically, add a summary for the class\nexplaining what recursive character chunking does, param documentation for the\nconstructor parameters and any configurable behavior, XML comments for each\npublic method describing inputs/outputs and side effects, and include remarks\nabout algorithmic behavior (e.g., overlap handling, max chunk size) and\nthread-safety if relevant.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commit 5e2879c",
    "created_at": "2025-11-04T13:49:49Z",
    "updated_at": "2025-11-04T14:11:33Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490595856",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595856"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490595856"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595856/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 5,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 6,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 6,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595865",
    "pull_request_review_id": 3416572453,
    "id": 2490595865,
    "node_id": "PRRC_kwDOKSXUF86Uc3oZ",
    "diff_hunk": "@@ -0,0 +1,84 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies\n+{\n+    public class RecursiveCharacterChunkingStrategy : ChunkingStrategyBase\n+    {\n+        private readonly int _chunkSize;\n+        private readonly int _chunkOverlap;\n+        private readonly string[] _separators;\n+\n+        public RecursiveCharacterChunkingStrategy(\n+            int chunkSize = 1000,\n+            int chunkOverlap = 200,\n+            string[]? separators = null)\n+        {\n+            _chunkSize = chunkSize;\n+            _chunkOverlap = chunkOverlap;\n+            _separators = separators ?? new[] { \"\\n\\n\", \"\\n\", \". \", \" \", \"\" };\n+        }",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/RecursiveCharacterChunkingStrategy.cs",
    "commit_id": "33a79e89f3137dc80c940c48d7ab74f127dd4d91",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Critical: Chunk overlap parameter is unused.**\n\nThe `_chunkOverlap` field is stored but never used in the implementation. The `ChunkCore` method merges splits without applying any overlap between consecutive chunks, making this a non-functional parameter that misleads users.\n\n\n\nTo implement overlap correctly, modify the chunk merging logic in `ChunkCore`:\n\n```diff\n-            var currentChunk = \"\";\n+            var currentChunk = \"\";\n+            var previousChunk = \"\";\n             foreach (var split in splits)\n             {\n                 if (currentChunk.Length + split.Length <= _chunkSize)\n                 {\n                     currentChunk += split;\n                 }\n                 else\n                 {\n                     if (!string.IsNullOrWhiteSpace(currentChunk))\n                     {\n                         chunks.Add(currentChunk.Trim());\n+                        previousChunk = currentChunk;\n                     }\n-                    currentChunk = split;\n+                    // Apply overlap by taking the last _chunkOverlap characters from previous chunk\n+                    var overlapText = previousChunk.Length > _chunkOverlap \n+                        ? previousChunk.Substring(previousChunk.Length - _chunkOverlap) \n+                        : previousChunk;\n+                    currentChunk = overlapText + split;\n                 }\n             }\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commit 5e2879c",
    "created_at": "2025-11-04T13:49:49Z",
    "updated_at": "2025-11-04T14:11:35Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490595865",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595865"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490595865"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595865/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 7,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 19,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 19,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595870",
    "pull_request_review_id": 3416572453,
    "id": 2490595870,
    "node_id": "PRRC_kwDOKSXUF86Uc3oe",
    "diff_hunk": "@@ -0,0 +1,84 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies\n+{\n+    public class RecursiveCharacterChunkingStrategy : ChunkingStrategyBase\n+    {\n+        private readonly int _chunkSize;\n+        private readonly int _chunkOverlap;\n+        private readonly string[] _separators;\n+\n+        public RecursiveCharacterChunkingStrategy(\n+            int chunkSize = 1000,\n+            int chunkOverlap = 200,\n+            string[]? separators = null)\n+        {\n+            _chunkSize = chunkSize;\n+            _chunkOverlap = chunkOverlap;\n+            _separators = separators ?? new[] { \"\\n\\n\", \"\\n\", \". \", \" \", \"\" };\n+        }\n+\n+        protected override List<string> ChunkCore(string text)\n+        {\n+            var chunks = new List<string>();\n+\n+            if (string.IsNullOrWhiteSpace(text))\n+            {\n+                return chunks;\n+            }\n+\n+            var splits = RecursiveSplit(text, _separators, 0);\n+            \n+            // Merge small splits into chunks\n+            var currentChunk = \"\";\n+            foreach (var split in splits)\n+            {\n+                if (currentChunk.Length + split.Length <= _chunkSize)\n+                {\n+                    currentChunk += split;\n+                }\n+                else\n+                {\n+                    if (!string.IsNullOrWhiteSpace(currentChunk))\n+                    {\n+                        chunks.Add(currentChunk.Trim());\n+                    }\n+                    currentChunk = split;\n+                }\n+            }\n+\n+            if (!string.IsNullOrWhiteSpace(currentChunk))\n+            {\n+                chunks.Add(currentChunk.Trim());\n+            }\n+\n+            return chunks;\n+        }\n+\n+        private List<string> RecursiveSplit(string text, string[] separators, int sepIndex)\n+        {\n+            if (sepIndex >= separators.Length || text.Length <= _chunkSize)\n+            {\n+                return new List<string> { text };\n+            }\n+\n+            var separator = separators[sepIndex];\n+            var splits = text.Split(new[] { separator }, StringSplitOptions.None);\n+            var result = new List<string>();\n+\n+            foreach (var split in splits)\n+            {\n+                if (split.Length > _chunkSize)\n+                {\n+                    result.AddRange(RecursiveSplit(split, separators, sepIndex + 1));\n+                }\n+                else if (!string.IsNullOrWhiteSpace(split))\n+                {\n+                    result.Add(split + separator);\n+                }\n+            }\n+\n+            return result;\n+        }",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/RecursiveCharacterChunkingStrategy.cs",
    "commit_id": "33a79e89f3137dc80c940c48d7ab74f127dd4d91",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Major: Incorrect separator handling in RecursiveSplit.**\n\nLine 77 unconditionally appends the separator to each split, but `text.Split()` discards the separator. This means the last segment (which didn't have a trailing separator in the original text) incorrectly gets one appended.\n\nExample: `\"A\\n\\nB\"` split by `\"\\n\\n\"` produces `[\"A\", \"B\"]`, but the current logic yields `[\"A\\n\\n\", \"B\\n\\n\"]`, adding a spurious trailing separator.\n\n\n\nTrack whether to add the separator based on position:\n\n```diff\n-            foreach (var split in splits)\n+            for (int i = 0; i < splits.Length; i++)\n             {\n+                var split = splits[i];\n+                var isLast = i == splits.Length - 1;\n+                \n                 if (split.Length > _chunkSize)\n                 {\n                     result.AddRange(RecursiveSplit(split, separators, sepIndex + 1));\n                 }\n                 else if (!string.IsNullOrWhiteSpace(split))\n                 {\n-                    result.Add(split + separator);\n+                    // Only append separator if not the last split\n+                    result.Add(isLast ? split : split + separator);\n                 }\n             }\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn\nsrc/RetrievalAugmentedGeneration/ChunkingStrategies/RecursiveCharacterChunkingStrategy.cs\naround lines 58 to 82, the code currently appends the separator unconditionally\nto each non-empty split which incorrectly adds a trailing separator to the last\nsegment; change the loop to track the split index (or use a for loop) and only\nappend the separator when the current split is not the last element from\ntext.Split(...) ΓÇö preserve the existing recursive behavior for oversized splits\nand still skip whitespace-only segments, and ensure that when text originally\nended with a separator the split logic still yields the correct trailing\nempty/handled segment without adding an extra separator.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T13:49:49Z",
    "updated_at": "2025-11-04T13:49:56Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490595870",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595870"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490595870"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595870/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 58,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 82,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 82,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595885",
    "pull_request_review_id": 3416572453,
    "id": 2490595885,
    "node_id": "PRRC_kwDOKSXUF86Uc3ot",
    "diff_hunk": "@@ -0,0 +1,113 @@\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Text.RegularExpressions;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies\n+{\n+    /// <summary>\n+    /// Sentence-based chunking strategy that preserves sentence boundaries\n+    /// </summary>\n+    public class SentenceChunkingStrategy : ChunkingStrategyBase\n+    {\n+        private readonly int _chunkSize;\n+        private readonly int _chunkOverlap;\n+        private static readonly Regex SentencePattern = new Regex(\n+            @\"(?<=[.!?])\\s+(?=[A-Z])|(?<=[.!?][\"\"])\\s+(?=[A-Z])\",\n+            RegexOptions.Compiled);",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/SentenceChunkingStrategy.cs",
    "commit_id": "33a79e89f3137dc80c940c48d7ab74f127dd4d91",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n<details>\n<summary>≡ƒº⌐ Analysis chain</summary>\n\n**Regex pattern has limitations with abbreviations and non-standard sentence boundaries.**\n\nThe current pattern `(?<=[.!?])\\s+(?=[A-Z])|(?<=[.!?][\"\"])\\s+(?=[A-Z])` will not correctly handle:\n- Abbreviations like \"Dr.\", \"Mr.\", \"U.S.A.\", \"etc.\" (will split incorrectly)\n- Sentences not starting with capital letters\n- Sentences ending without punctuation\n\nFor production use, consider either documenting these limitations in the XML comments or using a more robust sentence tokenizer (e.g., from an NLP library).\n\n---\n\n≡ƒîÉ Web query:\n\n```web\nBest practices for sentence boundary detection in C# NLP\n```\n\n≡ƒÆí Result:\n\nShort best-practice checklist for sentence-boundary detection in C# NLP:\n\n- Use a proven sentence-detector library (C# ports of OpenNLP / SharpNLP or a native C# splitter) rather than adΓÇæhoc regexes. ([opennlp.apache.org](https://opennlp.apache.org/docs/2.5.5/manual/opennlp.html?utm_source=openai))  \n- Prefer statistical / model-based detectors (Punkt / MaxEnt) and keep language-specific pretrained models; retrain or adapt models for your domain (legal/clinical/finance). ([deepwiki.com](https://deepwiki.com/nltk/nltk/2.3.1-sentence-tokenization?utm_source=openai))  \n- Normalize input first: fix encoding, normalize newlines, remove or mark HTML, preserve vs. collapse whitespace per downstream needs. Add an HTML sanitizer or treat tags as boundaries when appropriate. ([npmjs.com](https://www.npmjs.com/package/%40scriptollc/sbd?utm_source=openai))  \n- Handle abbreviations, initials, numbers, URLs, emails, ellipses, parentheses and citations explicitly (use model vocab/abbreviation lists or rules to avoid false splits). Combine model decisions with lightweight rules for corner cases. ([deepwiki.com](https://deepwiki.com/nltk/nltk/2.3.1-sentence-tokenization?utm_source=openai))  \n- Tokenization interplay: test both ΓÇ£sentence-first then tokenizeΓÇ¥ and ΓÇ£tokenize-first then sentence-detectΓÇ¥ if using libraries that support both; use the same convention the model was trained on. ([opennlp.apache.org](https://opennlp.apache.org/docs/2.5.5/manual/opennlp.html?utm_source=openai))  \n- Domain tune: collect a small annotated sample (hundredsΓÇôthousands of sentences) and fine-tune or add domain-specific abbreviation lists and post-processing rules ΓÇö small labeled sets give large gains. ([arxiv.org](https://arxiv.org/abs/2305.18893?utm_source=openai))  \n- Evaluate with precision/recall/F1 on held-out, domain-representative data; measure downstream impact (e.g., NER, parsing, retrieval). ([arxiv.org](https://arxiv.org/abs/2102.08513?utm_source=openai))  \n- Performance & scale: for large corpora, prefer optimized implementations or batch processing; consider streaming detectors and benchmark throughput/memory. ([arxiv.org](https://arxiv.org/abs/2504.04131?utm_source=openai))  \n- Edge strategies: for noisy or punctuationΓÇæsparse text (transcripts, chat), consider punctuationΓÇæagnostic or ML segmentation models (selfΓÇæsupervised / transformer approaches). ([arxiv.org](https://arxiv.org/abs/2305.18893?utm_source=openai))  \n- Logging & manual review: log lowΓÇæconfidence splits for periodic human review to iteratively improve rules/models.\n\nIf you want, I can:\n- recommend specific C# packages with install and sample code, or  \n- produce a small evaluation plan + sample annotated set to fineΓÇætune a detector for your domain.\n\nCitations:\n\n- 1: https://opennlp.apache.org/docs/2.5.5/manual/opennlp.html?utm_source=openai\n- 2: https://deepwiki.com/nltk/nltk/2.3.1-sentence-tokenization?utm_source=openai\n- 3: https://www.npmjs.com/package/%40scriptollc/sbd?utm_source=openai\n- 4: https://deepwiki.com/nltk/nltk/2.3.1-sentence-tokenization?utm_source=openai\n- 5: https://opennlp.apache.org/docs/2.5.5/manual/opennlp.html?utm_source=openai\n- 6: https://arxiv.org/abs/2305.18893?utm_source=openai\n- 7: https://arxiv.org/abs/2102.08513?utm_source=openai\n- 8: https://arxiv.org/abs/2504.04131?utm_source=openai\n- 9: https://arxiv.org/abs/2305.18893?utm_source=openai\n\n---\n\n\n\n</details>\n\n**Regex-based sentence splitting is not recommended; use a proven library instead.**\n\nBest practices recommend using a proven sentence-detector library (C# ports of OpenNLP/SharpNLP or a native C# splitter) rather than ad-hoc regexes. The current pattern will fail on:\n- Abbreviations (Dr., Mr., U.S.A., etc.)\n- Sentences not starting with capital letters\n- Non-standard punctuation, initials, numbers, URLs, and ellipses\n\nReplace with a library-based approach, or at minimum document these limitations prominently in XML comments and add handling for common abbreviations via explicit rules.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T13:49:50Z",
    "updated_at": "2025-11-04T13:49:56Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490595885",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595885"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490595885"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595885/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 16,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 18,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 18,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595906",
    "pull_request_review_id": 3416572453,
    "id": 2490595906,
    "node_id": "PRRC_kwDOKSXUF86Uc3pC",
    "diff_hunk": "@@ -0,0 +1,113 @@\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Text.RegularExpressions;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies\n+{\n+    /// <summary>\n+    /// Sentence-based chunking strategy that preserves sentence boundaries\n+    /// </summary>\n+    public class SentenceChunkingStrategy : ChunkingStrategyBase\n+    {\n+        private readonly int _chunkSize;\n+        private readonly int _chunkOverlap;\n+        private static readonly Regex SentencePattern = new Regex(\n+            @\"(?<=[.!?])\\s+(?=[A-Z])|(?<=[.!?][\"\"])\\s+(?=[A-Z])\",\n+            RegexOptions.Compiled);\n+\n+        public SentenceChunkingStrategy(int chunkSize = 1000, int chunkOverlap = 200)\n+        {\n+            _chunkSize = chunkSize;\n+            _chunkOverlap = chunkOverlap;\n+        }\n+\n+        protected override List<TextChunk> SplitCore(string text, Dictionary<string, string>? metadata = null)\n+        {\n+            if (string.IsNullOrWhiteSpace(text))\n+                return new List<TextChunk>();\n+\n+            var sentences = SplitIntoSentences(text);\n+            var chunks = new List<TextChunk>();\n+            var currentChunk = new List<string>();\n+            var currentLength = 0;\n+            var chunkIndex = 0;\n+\n+            foreach (var sentence in sentences)\n+            {\n+                var sentenceLength = sentence.Length + 1;\n+\n+                if (currentLength + sentenceLength > _chunkSize && currentChunk.Count > 0)\n+                {\n+                    var chunkMetadata = new Dictionary<string, string>(metadata ?? new Dictionary<string, string>())\n+                    {\n+                        [\"chunk_index\"] = chunkIndex.ToString(),\n+                        [\"sentence_count\"] = currentChunk.Count.ToString()\n+                    };\n+\n+                    chunks.Add(new TextChunk\n+                    {\n+                        Text = string.Join(\" \", currentChunk),\n+                        Metadata = chunkMetadata\n+                    });\n+                    chunkIndex++;\n+\n+                    var overlapSentences = new List<string>();\n+                    var overlapLength = 0;\n+\n+                    for (int i = currentChunk.Count - 1; i >= 0 && overlapLength < _chunkOverlap; i--)\n+                    {\n+                        overlapSentences.Insert(0, currentChunk[i]);\n+                        overlapLength += currentChunk[i].Length + 1;\n+                    }\n+\n+                    currentChunk = overlapSentences;\n+                    currentLength = overlapLength;\n+                }\n+\n+                currentChunk.Add(sentence);\n+                currentLength += sentenceLength;",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/SentenceChunkingStrategy.cs",
    "commit_id": "33a79e89f3137dc80c940c48d7ab74f127dd4d91",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Sentences exceeding chunk size are not handled.**\n\nThe condition on line 41 includes `currentChunk.Count > 0`, which means if a single sentence exceeds `_chunkSize`, it will be added to a chunk without any size enforcement (lines 69-70). This can create chunks significantly larger than the configured limit, potentially causing issues with downstream components (e.g., embedding model token limits).\n\nConsider either:\n1. Documenting this behavior clearly in the XML comments (sentence boundary preservation takes precedence over size limits)\n2. Adding a warning log when a sentence exceeds the chunk size\n3. Implementing a fallback to split long sentences by words or characters\n\n\n\nExample to illustrate:\n\n```csharp\n// If a sentence is 5000 chars but _chunkSize is 1000:\n// - currentChunk.Count == 0 initially\n// - Condition on line 41 evaluates to false\n// - Sentence is added at lines 69-70\n// - Resulting chunk is 5000 chars (5x the limit)\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn\nsrc/RetrievalAugmentedGeneration/ChunkingStrategies/SentenceChunkingStrategy.cs\naround lines 41 to 70, a single sentence longer than _chunkSize bypasses the\nchunk-size check and is appended whole, producing chunks larger than the\nconfigured limit; update the logic to detect when sentenceLength > _chunkSize\nand handle it by either (a) splitting that sentence into smaller segments\n(preferably on word boundaries, falling back to character splits) and feeding\nthose segments through the existing chunking flow so metadata/overlap semantics\nremain correct, or (b) at minimum emit a warning log when a sentence exceeds\n_chunkSize (preferably with sentence length and chunkSize) and then split as a\nfallback; implement a small private helper SplitLongSentence(sentence, maxSize)\nto return segments and integrate it before currentChunk.Add(sentence) so you\nnever append a segment longer than _chunkSize while preserving chunk overlap and\nupdating currentLength and chunkIndex appropriately.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T13:49:50Z",
    "updated_at": "2025-11-04T13:49:56Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490595906",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595906"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490595906"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595906/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 41,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 70,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 70,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595924",
    "pull_request_review_id": 3416572453,
    "id": 2490595924,
    "node_id": "PRRC_kwDOKSXUF86Uc3pU",
    "diff_hunk": "@@ -0,0 +1,146 @@\n+#if NETCOREAPP || NETSTANDARD2_1_OR_GREATER\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Text.Json;\n+using System.Threading.Tasks;\n+using StackExchange.Redis;",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/RedisVLDocumentStore.cs",
    "commit_id": "62aea1a1b376f23b0693eb1f0ecd24a268f42be2",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Critical: Missing StackExchange.Redis NuGet package reference.**\n\nThe build is failing because the `StackExchange.Redis` package is not referenced in the project file.\n\n\n\nAdd the package reference to your `.csproj` file:\n\n```xml\n<PackageReference Include=\"StackExchange.Redis\" Version=\"2.8.0\" />\n```\n\nVerify the latest stable version at https://www.nuget.org/packages/StackExchange.Redis/ before adding.\n\n<details>\n<summary>≡ƒº░ Tools</summary>\n\n<details>\n<summary>≡ƒ¬¢ GitHub Actions: Quality Gates (.NET)</summary>\n\n[error] 9-9: CS0246: The type or namespace name 'StackExchange' could not be found (are you missing a using directive or an assembly reference?)\n\n</details>\n<details>\n<summary>≡ƒ¬¢ GitHub Check: Publish Size Analysis</summary>\n\n[failure] 9-9: \nThe type or namespace name 'StackExchange' could not be found (are you missing a using directive or an assembly reference?)\n\n</details>\n\n</details>\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/RedisVLDocumentStore.cs\naround line 9, the file imports StackExchange.Redis but the project is missing\nthe NuGet package reference; add a PackageReference for StackExchange.Redis to\nthe corresponding .csproj (use the current latest stable version, e.g. 2.8.0) so\nthe assembly resolves, then restore packages/build to verify successful compile.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T13:49:50Z",
    "updated_at": "2025-11-04T13:49:56Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490595924",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595924"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490595924"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595924/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": null,
    "original_line": 9,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 9,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595940",
    "pull_request_review_id": 3416572453,
    "id": 2490595940,
    "node_id": "PRRC_kwDOKSXUF86Uc3pk",
    "diff_hunk": "@@ -0,0 +1,146 @@\n+#if NETCOREAPP || NETSTANDARD2_1_OR_GREATER\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Text.Json;\n+using System.Threading.Tasks;\n+using StackExchange.Redis;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores\n+{\n+    /// <summary>\n+    /// Document store using Redis Vector Library for vector search\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations</typeparam>\n+    public class RedisVLDocumentStore<T> : DocumentStoreBase<T> where T : struct, IComparable, IComparable<T>, IConvertible, IEquatable<T>, IFormattable\n+    {\n+        private readonly ConnectionMultiplexer _redis;\n+        private readonly IDatabase _db;\n+        private readonly string _indexName;\n+\n+        public RedisVLDocumentStore(string connectionString, string indexName = \"documents\")\n+        {\n+            if (string.IsNullOrEmpty(connectionString))\n+                throw new ArgumentException(\"Connection string cannot be null or empty\", nameof(connectionString));\n+\n+            _redis = ConnectionMultiplexer.Connect(connectionString);\n+            _db = _redis.GetDatabase();\n+            _indexName = indexName;\n+        }\n+\n+        public override async Task AddDocumentAsync(Document<T> document)\n+        {\n+            if (document == null)\n+                throw new ArgumentNullException(nameof(document));\n+\n+            var key = $\"{_indexName}:{document.Id}\";\n+            var hash = new HashEntry[]\n+            {\n+                new HashEntry(\"id\", document.Id),\n+                new HashEntry(\"content\", document.Content),\n+                new HashEntry(\"embedding\", JsonSerializer.Serialize(ConvertVectorToDoubleArray(document.Embedding))),\n+                new HashEntry(\"metadata\", JsonSerializer.Serialize(document.Metadata))\n+            };\n+\n+            await _db.HashSetAsync(key, hash);\n+        }\n+\n+        public override async Task<List<Document<T>>> SearchAsync(Vector<T> queryEmbedding, int topK = 5)\n+        {\n+            if (queryEmbedding == null)\n+                throw new ArgumentNullException(nameof(queryEmbedding));\n+\n+            var pattern = $\"{_indexName}:*\";\n+            var server = _redis.GetServer(_redis.GetEndPoints()[0]);\n+            var keys = server.Keys(pattern: pattern).ToList();\n+\n+            var results = new List<(Document<T> doc, T similarity)>();\n+\n+            foreach (var key in keys)\n+            {\n+                var hash = await _db.HashGetAllAsync(key);\n+                var hashDict = hash.ToDictionary(x => x.Name.ToString(), x => x.Value.ToString());\n+\n+                if (!hashDict.ContainsKey(\"embedding\"))\n+                    continue;\n+\n+                var embedding = JsonSerializer.Deserialize<double[]>(hashDict[\"embedding\"]) ?? Array.Empty<double>();\n+                var embeddingVector = new Vector<T>(\n+                    embedding.Select(x => (T)Convert.ChangeType(x, typeof(T))).ToArray(),\n+                    NumOps);\n+\n+                var similarity = StatisticsHelper.CosineSimilarity(queryEmbedding, embeddingVector, NumOps);\n+\n+                var doc = new Document<T>\n+                {\n+                    Id = hashDict.GetValueOrDefault(\"id\", string.Empty),\n+                    Content = hashDict.GetValueOrDefault(\"content\", string.Empty),\n+                    Metadata = JsonSerializer.Deserialize<Dictionary<string, string>>(\n+                        hashDict.GetValueOrDefault(\"metadata\", \"{}\")) ?? new Dictionary<string, string>()\n+                };\n+\n+                results.Add((doc, similarity));\n+            }\n+\n+            return results\n+                .OrderByDescending(x => x.similarity)\n+                .Take(topK)\n+                .Select(x => x.doc)\n+                .ToList();\n+        }\n+\n+        public override async Task DeleteDocumentAsync(string documentId)\n+        {\n+            if (string.IsNullOrEmpty(documentId))\n+                throw new ArgumentException(\"Document ID cannot be null or empty\", nameof(documentId));\n+\n+            var key = $\"{_indexName}:{documentId}\";\n+            await _db.KeyDeleteAsync(key);\n+        }\n+\n+        public override async Task<Document<T>?> GetDocumentAsync(string documentId)\n+        {\n+            if (string.IsNullOrEmpty(documentId))\n+                throw new ArgumentException(\"Document ID cannot be null or empty\", nameof(documentId));\n+\n+            var key = $\"{_indexName}:{documentId}\";\n+            var hash = await _db.HashGetAllAsync(key);\n+\n+            if (hash.Length == 0)\n+                return null;\n+\n+            var hashDict = hash.ToDictionary(x => x.Name.ToString(), x => x.Value.ToString());\n+\n+            return new Document<T>\n+            {\n+                Id = hashDict.GetValueOrDefault(\"id\", string.Empty),\n+                Content = hashDict.GetValueOrDefault(\"content\", string.Empty),\n+                Metadata = JsonSerializer.Deserialize<Dictionary<string, string>>(\n+                    hashDict.GetValueOrDefault(\"metadata\", \"{}\")) ?? new Dictionary<string, string>()\n+            };\n+        }\n+\n+        private double[] ConvertVectorToDoubleArray(Vector<T> vector)\n+        {\n+            var result = new double[vector.Length];\n+            for (int i = 0; i < vector.Length; i++)\n+            {\n+                result[i] = Convert.ToDouble(vector[i]);\n+            }\n+            return result;\n+        }\n+\n+        protected override void Dispose(bool disposing)\n+        {\n+            if (disposing)\n+            {\n+                _redis?.Dispose();\n+            }\n+            base.Dispose(disposing);\n+        }\n+    }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/RedisVLDocumentStore.cs",
    "commit_id": "62aea1a1b376f23b0693eb1f0ecd24a268f42be2",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Critical: Class does not implement required abstract members and uses incompatible method signatures.**\n\nThis implementation will not compile because it fails to implement the abstract contract from `DocumentStoreBase<T>`. The base class requires synchronous `*Core` methods, but this implementation provides async public methods with different signatures.\n\n**Missing required abstract implementations:**\n- `AddCore(VectorDocument<T>)` - you have `AddDocumentAsync(Document<T>)` instead\n- `GetSimilarCore(Vector<T>, int, Dictionary<string, object>)` - you have `SearchAsync(Vector<T>, int)` instead\n- `GetByIdCore(string)` - you have `GetDocumentAsync(string)` instead\n- `RemoveCore(string)` returning `bool` - you have `DeleteDocumentAsync(string)` returning `Task` instead\n- `Clear()` - completely missing\n\n**Missing required abstract properties:**\n- `VectorDimension { get; }` - must return the dimensionality of stored vectors\n- `DocumentCount { get; }` - must return the count of stored documents\n\nThe correct pattern (from DocumentStoreBase.cs lines 28-348):\n1. Implement synchronous `*Core` methods where your storage logic goes\n2. The base class public methods (Add, GetSimilar, etc.) handle validation and call your Core implementations\n3. If you need async operations, perform them inside the Core methods (they can be sync wrappers around async operations)\n\n\n\nBased on the base class pattern, you need to implement:\n\n```csharp\npublic override int VectorDimension \n{ \n    get \n    {\n        // TODO: Track vector dimension when first document is added\n        // For now, return a configured dimension or query from Redis\n        throw new NotImplementedException();\n    } \n}\n\npublic override int DocumentCount \n{ \n    get \n    {\n        var pattern = $\"{_indexName}:*\";\n        var server = _redis.GetServer(_redis.GetEndPoints()[0]);\n        return server.Keys(pattern: pattern).Count();\n    } \n}\n\nprotected override void AddCore(VectorDocument<T> vectorDocument)\n{\n    // Sync wrapper around async - this is a common pattern when base expects sync\n    AddDocumentAsync(vectorDocument.Document).GetAwaiter().GetResult();\n}\n\nprotected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)\n{\n    var results = SearchAsync(queryVector, topK).GetAwaiter().GetResult();\n    // Apply metadataFilters here using MatchesFilters helper from base class\n    return results.Where(doc => MatchesFilters(doc, metadataFilters));\n}\n\nprotected override Document<T>? GetByIdCore(string documentId)\n{\n    return GetDocumentAsync(documentId).GetAwaiter().GetResult();\n}\n\nprotected override bool RemoveCore(string documentId)\n{\n    DeleteDocumentAsync(documentId).GetAwaiter().GetResult();\n    return true; // Redis KeyDeleteAsync doesn't return bool, so assume success\n}\n\npublic override void Clear()\n{\n    var pattern = $\"{_indexName}:*\";\n    var server = _redis.GetServer(_redis.GetEndPoints()[0]);\n    var keys = server.Keys(pattern: pattern);\n    foreach (var key in keys)\n    {\n        _db.KeyDelete(key);\n    }\n}\n```\n\nNote: The async methods you've implemented can remain as helper methods, but you must also implement the synchronous abstract members.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/RedisVLDocumentStore.cs\naround lines 17 to 143, the class fails to implement the synchronous abstract\nmembers and properties required by DocumentStoreBase<T>; add implementations for\nVectorDimension and DocumentCount, implement protected overrides\nAddCore(VectorDocument<T>), GetSimilarCore(Vector<T>, int, Dictionary<string,\nobject>), GetByIdCore(string), RemoveCore(string) (returning bool), and Clear();\nimplement these cores as synchronous wrappers around your existing async helpers\n(call AddDocumentAsync/SearchAsync/GetDocumentAsync/DeleteDocumentAsync using\n.GetAwaiter().GetResult()), make RemoveCore return the actual deletion result\nfrom _db.KeyDelete(key) or true/false accordingly, implement DocumentCount by\nenumerating server.Keys with the index pattern, implement VectorDimension by\nderiving from the first stored document's embedding length or store/set the\ndimension when adding the first document, and ensure GetSimilarCore applies\nmetadataFilters using the base MatchesFilters helper before returning results.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T13:49:50Z",
    "updated_at": "2025-11-04T13:49:56Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490595940",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595940"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490595940"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595940/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 17,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 143,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 143,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595950",
    "pull_request_review_id": 3416572453,
    "id": 2490595950,
    "node_id": "PRRC_kwDOKSXUF86Uc3pu",
    "diff_hunk": "@@ -0,0 +1,146 @@\n+#if NETCOREAPP || NETSTANDARD2_1_OR_GREATER\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Text.Json;\n+using System.Threading.Tasks;\n+using StackExchange.Redis;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores\n+{\n+    /// <summary>\n+    /// Document store using Redis Vector Library for vector search\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations</typeparam>\n+    public class RedisVLDocumentStore<T> : DocumentStoreBase<T> where T : struct, IComparable, IComparable<T>, IConvertible, IEquatable<T>, IFormattable\n+    {\n+        private readonly ConnectionMultiplexer _redis;\n+        private readonly IDatabase _db;\n+        private readonly string _indexName;\n+\n+        public RedisVLDocumentStore(string connectionString, string indexName = \"documents\")\n+        {\n+            if (string.IsNullOrEmpty(connectionString))\n+                throw new ArgumentException(\"Connection string cannot be null or empty\", nameof(connectionString));\n+\n+            _redis = ConnectionMultiplexer.Connect(connectionString);\n+            _db = _redis.GetDatabase();\n+            _indexName = indexName;\n+        }\n+\n+        public override async Task AddDocumentAsync(Document<T> document)\n+        {\n+            if (document == null)\n+                throw new ArgumentNullException(nameof(document));\n+\n+            var key = $\"{_indexName}:{document.Id}\";\n+            var hash = new HashEntry[]\n+            {\n+                new HashEntry(\"id\", document.Id),\n+                new HashEntry(\"content\", document.Content),\n+                new HashEntry(\"embedding\", JsonSerializer.Serialize(ConvertVectorToDoubleArray(document.Embedding))),\n+                new HashEntry(\"metadata\", JsonSerializer.Serialize(document.Metadata))\n+            };\n+\n+            await _db.HashSetAsync(key, hash);\n+        }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/RedisVLDocumentStore.cs",
    "commit_id": "62aea1a1b376f23b0693eb1f0ecd24a268f42be2",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Critical: Null reference exception risk and incorrect parameter type.**\n\nLine 43 accesses `document.Embedding` without null checking, which will throw `NullReferenceException` if the embedding is null. According to the Document<T> model (Document.cs line 110), the Embedding property is nullable (`Vector<T>?`).\n\nAdditionally, the method should accept `VectorDocument<T>` (which pairs a document with its embedding) rather than `Document<T>` to align with the base class pattern.\n\n\n\nApply this fix:\n\n```diff\n-public override async Task AddDocumentAsync(Document<T> document)\n+private async Task AddDocumentAsync(Document<T> document, Vector<T> embedding)\n {\n     if (document == null)\n         throw new ArgumentNullException(nameof(document));\n+    \n+    if (embedding == null)\n+        throw new ArgumentNullException(nameof(embedding));\n\n     var key = $\"{_indexName}:{document.Id}\";\n     var hash = new HashEntry[]\n     {\n         new HashEntry(\"id\", document.Id),\n         new HashEntry(\"content\", document.Content),\n-        new HashEntry(\"embedding\", JsonSerializer.Serialize(ConvertVectorToDoubleArray(document.Embedding))),\n+        new HashEntry(\"embedding\", JsonSerializer.Serialize(ConvertVectorToDoubleArray(embedding))),\n         new HashEntry(\"metadata\", JsonSerializer.Serialize(document.Metadata))\n     };\n\n     await _db.HashSetAsync(key, hash);\n }\n```\n\nThen in `AddCore`:\n```csharp\nprotected override void AddCore(VectorDocument<T> vectorDocument)\n{\n    AddDocumentAsync(vectorDocument.Document, vectorDocument.Embedding).GetAwaiter().GetResult();\n}\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/RedisVLDocumentStore.cs\naround lines 33 to 48, change the AddDocumentAsync signature to accept\nVectorDocument<T> (or accept both Document<T> and a separate Vector<T>?\nembedding) instead of Document<T>, validate that the VectorDocument and its\nDocument are not null and explicitly check that the embedding is not null (throw\nArgumentNullException with a clear param name if it is), then build the hash\nusing vectorDocument.Document for id/content/metadata and serialize the\nembedding from vectorDocument.Embedding after converting it to a double array;\nfinally update AddCore to call\nAddDocumentAsync(vectorDocument).GetAwaiter().GetResult() (or the provided\nAddDocumentAsync(vectorDocument.Document, vectorDocument.Embedding) pattern) so\nthe base-class flow aligns with the new signature.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T13:49:50Z",
    "updated_at": "2025-11-04T13:49:56Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490595950",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595950"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490595950"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595950/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 33,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 48,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 48,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595958",
    "pull_request_review_id": 3416572453,
    "id": 2490595958,
    "node_id": "PRRC_kwDOKSXUF86Uc3p2",
    "diff_hunk": "@@ -0,0 +1,146 @@\n+#if NETCOREAPP || NETSTANDARD2_1_OR_GREATER\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Text.Json;\n+using System.Threading.Tasks;\n+using StackExchange.Redis;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores\n+{\n+    /// <summary>\n+    /// Document store using Redis Vector Library for vector search\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations</typeparam>\n+    public class RedisVLDocumentStore<T> : DocumentStoreBase<T> where T : struct, IComparable, IComparable<T>, IConvertible, IEquatable<T>, IFormattable\n+    {\n+        private readonly ConnectionMultiplexer _redis;\n+        private readonly IDatabase _db;\n+        private readonly string _indexName;\n+\n+        public RedisVLDocumentStore(string connectionString, string indexName = \"documents\")\n+        {\n+            if (string.IsNullOrEmpty(connectionString))\n+                throw new ArgumentException(\"Connection string cannot be null or empty\", nameof(connectionString));\n+\n+            _redis = ConnectionMultiplexer.Connect(connectionString);\n+            _db = _redis.GetDatabase();\n+            _indexName = indexName;\n+        }\n+\n+        public override async Task AddDocumentAsync(Document<T> document)\n+        {\n+            if (document == null)\n+                throw new ArgumentNullException(nameof(document));\n+\n+            var key = $\"{_indexName}:{document.Id}\";\n+            var hash = new HashEntry[]\n+            {\n+                new HashEntry(\"id\", document.Id),\n+                new HashEntry(\"content\", document.Content),\n+                new HashEntry(\"embedding\", JsonSerializer.Serialize(ConvertVectorToDoubleArray(document.Embedding))),\n+                new HashEntry(\"metadata\", JsonSerializer.Serialize(document.Metadata))\n+            };\n+\n+            await _db.HashSetAsync(key, hash);\n+        }\n+\n+        public override async Task<List<Document<T>>> SearchAsync(Vector<T> queryEmbedding, int topK = 5)\n+        {\n+            if (queryEmbedding == null)\n+                throw new ArgumentNullException(nameof(queryEmbedding));\n+\n+            var pattern = $\"{_indexName}:*\";\n+            var server = _redis.GetServer(_redis.GetEndPoints()[0]);\n+            var keys = server.Keys(pattern: pattern).ToList();\n+\n+            var results = new List<(Document<T> doc, T similarity)>();\n+\n+            foreach (var key in keys)\n+            {\n+                var hash = await _db.HashGetAllAsync(key);\n+                var hashDict = hash.ToDictionary(x => x.Name.ToString(), x => x.Value.ToString());\n+\n+                if (!hashDict.ContainsKey(\"embedding\"))\n+                    continue;\n+\n+                var embedding = JsonSerializer.Deserialize<double[]>(hashDict[\"embedding\"]) ?? Array.Empty<double>();\n+                var embeddingVector = new Vector<T>(\n+                    embedding.Select(x => (T)Convert.ChangeType(x, typeof(T))).ToArray(),\n+                    NumOps);\n+\n+                var similarity = StatisticsHelper.CosineSimilarity(queryEmbedding, embeddingVector, NumOps);\n+\n+                var doc = new Document<T>\n+                {\n+                    Id = hashDict.GetValueOrDefault(\"id\", string.Empty),\n+                    Content = hashDict.GetValueOrDefault(\"content\", string.Empty),\n+                    Metadata = JsonSerializer.Deserialize<Dictionary<string, string>>(\n+                        hashDict.GetValueOrDefault(\"metadata\", \"{}\")) ?? new Dictionary<string, string>()\n+                };\n+\n+                results.Add((doc, similarity));\n+            }\n+\n+            return results\n+                .OrderByDescending(x => x.similarity)\n+                .Take(topK)\n+                .Select(x => x.doc)\n+                .ToList();\n+        }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/RedisVLDocumentStore.cs",
    "commit_id": "62aea1a1b376f23b0693eb1f0ecd24a268f42be2",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Major issue: Computed similarity scores not assigned to documents and missing metadata filtering.**\n\nThe method computes cosine similarity (line 74) but discards it without setting the `RelevanceScore` and `HasRelevanceScore` properties on the returned documents. Additionally, the method lacks metadata filtering support required by the base class contract.\n\n\n\nApply these fixes:\n\n```diff\n-public override async Task<List<Document<T>>> SearchAsync(Vector<T> queryEmbedding, int topK = 5)\n+private async Task<List<Document<T>>> SearchAsync(Vector<T> queryEmbedding, int topK, Dictionary<string, object> metadataFilters)\n {\n     if (queryEmbedding == null)\n         throw new ArgumentNullException(nameof(queryEmbedding));\n\n     var pattern = $\"{_indexName}:*\";\n     var server = _redis.GetServer(_redis.GetEndPoints()[0]);\n     var keys = server.Keys(pattern: pattern).ToList();\n\n     var results = new List<(Document<T> doc, T similarity)>();\n\n     foreach (var key in keys)\n     {\n         var hash = await _db.HashGetAllAsync(key);\n         var hashDict = hash.ToDictionary(x => x.Name.ToString(), x => x.Value.ToString());\n\n         if (!hashDict.ContainsKey(\"embedding\"))\n             continue;\n\n         var embedding = JsonSerializer.Deserialize<double[]>(hashDict[\"embedding\"]) ?? Array.Empty<double>();\n         var embeddingVector = new Vector<T>(\n             embedding.Select(x => (T)Convert.ChangeType(x, typeof(T))).ToArray(),\n             NumOps);\n\n         var similarity = StatisticsHelper.CosineSimilarity(queryEmbedding, embeddingVector, NumOps);\n\n         var doc = new Document<T>\n         {\n             Id = hashDict.GetValueOrDefault(\"id\", string.Empty),\n             Content = hashDict.GetValueOrDefault(\"content\", string.Empty),\n-            Metadata = JsonSerializer.Deserialize<Dictionary<string, string>>(\n-                hashDict.GetValueOrDefault(\"metadata\", \"{}\")) ?? new Dictionary<string, string>()\n+            Metadata = JsonSerializer.Deserialize<Dictionary<string, object>>(\n+                hashDict.GetValueOrDefault(\"metadata\", \"{}\")) ?? new Dictionary<string, object>(),\n+            RelevanceScore = similarity,\n+            HasRelevanceScore = true\n         };\n\n+        // Apply metadata filtering using base class helper\n+        if (!MatchesFilters(doc, metadataFilters))\n+            continue;\n+\n         results.Add((doc, similarity));\n     }\n\n     return results\n         .OrderByDescending(x => x.similarity)\n         .Take(topK)\n         .Select(x => x.doc)\n         .ToList();\n }\n```\n\nThen implement the required abstract method:\n```csharp\nprotected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)\n{\n    return SearchAsync(queryVector, topK, metadataFilters).GetAwaiter().GetResult();\n}\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/RedisVLDocumentStore.cs\naround lines 50 to 92, the SearchAsync implementation computes cosine similarity\nbut never assigns it to the Document's RelevanceScore/HasRelevanceScore and it\ndoes not support metadata filtering required by the base contract; update\nSearchAsync to accept an optional Dictionary<string, object> metadataFilters\nparameter, apply those filters when iterating Redis hashes (skip documents that\ndon't match all metadataFilters), set doc.RelevanceScore =\n(double)Convert.ChangeType(similarity, typeof(double)) and doc.HasRelevanceScore\n= true before adding to results, and then return the topK documents; finally\nimplement the abstract GetSimilarCore override to synchronously call\nSearchAsync(queryVector, topK, metadataFilters) and return its result as\nrequested.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T13:49:50Z",
    "updated_at": "2025-11-04T13:49:56Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490595958",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595958"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490595958"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595958/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 50,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 92,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 92,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595980",
    "pull_request_review_id": 3416572453,
    "id": 2490595980,
    "node_id": "PRRC_kwDOKSXUF86Uc3qM",
    "diff_hunk": "@@ -0,0 +1,146 @@\n+#if NETCOREAPP || NETSTANDARD2_1_OR_GREATER\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Text.Json;\n+using System.Threading.Tasks;\n+using StackExchange.Redis;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores\n+{\n+    /// <summary>\n+    /// Document store using Redis Vector Library for vector search\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations</typeparam>\n+    public class RedisVLDocumentStore<T> : DocumentStoreBase<T> where T : struct, IComparable, IComparable<T>, IConvertible, IEquatable<T>, IFormattable\n+    {\n+        private readonly ConnectionMultiplexer _redis;\n+        private readonly IDatabase _db;\n+        private readonly string _indexName;\n+\n+        public RedisVLDocumentStore(string connectionString, string indexName = \"documents\")\n+        {\n+            if (string.IsNullOrEmpty(connectionString))\n+                throw new ArgumentException(\"Connection string cannot be null or empty\", nameof(connectionString));\n+\n+            _redis = ConnectionMultiplexer.Connect(connectionString);\n+            _db = _redis.GetDatabase();\n+            _indexName = indexName;\n+        }\n+\n+        public override async Task AddDocumentAsync(Document<T> document)\n+        {\n+            if (document == null)\n+                throw new ArgumentNullException(nameof(document));\n+\n+            var key = $\"{_indexName}:{document.Id}\";\n+            var hash = new HashEntry[]\n+            {\n+                new HashEntry(\"id\", document.Id),\n+                new HashEntry(\"content\", document.Content),\n+                new HashEntry(\"embedding\", JsonSerializer.Serialize(ConvertVectorToDoubleArray(document.Embedding))),\n+                new HashEntry(\"metadata\", JsonSerializer.Serialize(document.Metadata))\n+            };\n+\n+            await _db.HashSetAsync(key, hash);\n+        }\n+\n+        public override async Task<List<Document<T>>> SearchAsync(Vector<T> queryEmbedding, int topK = 5)\n+        {\n+            if (queryEmbedding == null)\n+                throw new ArgumentNullException(nameof(queryEmbedding));\n+\n+            var pattern = $\"{_indexName}:*\";\n+            var server = _redis.GetServer(_redis.GetEndPoints()[0]);\n+            var keys = server.Keys(pattern: pattern).ToList();\n+\n+            var results = new List<(Document<T> doc, T similarity)>();\n+\n+            foreach (var key in keys)\n+            {\n+                var hash = await _db.HashGetAllAsync(key);\n+                var hashDict = hash.ToDictionary(x => x.Name.ToString(), x => x.Value.ToString());\n+\n+                if (!hashDict.ContainsKey(\"embedding\"))\n+                    continue;\n+\n+                var embedding = JsonSerializer.Deserialize<double[]>(hashDict[\"embedding\"]) ?? Array.Empty<double>();\n+                var embeddingVector = new Vector<T>(\n+                    embedding.Select(x => (T)Convert.ChangeType(x, typeof(T))).ToArray(),\n+                    NumOps);\n+\n+                var similarity = StatisticsHelper.CosineSimilarity(queryEmbedding, embeddingVector, NumOps);\n+\n+                var doc = new Document<T>\n+                {\n+                    Id = hashDict.GetValueOrDefault(\"id\", string.Empty),\n+                    Content = hashDict.GetValueOrDefault(\"content\", string.Empty),\n+                    Metadata = JsonSerializer.Deserialize<Dictionary<string, string>>(\n+                        hashDict.GetValueOrDefault(\"metadata\", \"{}\")) ?? new Dictionary<string, string>()\n+                };\n+\n+                results.Add((doc, similarity));\n+            }\n+\n+            return results\n+                .OrderByDescending(x => x.similarity)\n+                .Take(topK)\n+                .Select(x => x.doc)\n+                .ToList();\n+        }\n+\n+        public override async Task DeleteDocumentAsync(string documentId)\n+        {\n+            if (string.IsNullOrEmpty(documentId))\n+                throw new ArgumentException(\"Document ID cannot be null or empty\", nameof(documentId));\n+\n+            var key = $\"{_indexName}:{documentId}\";\n+            await _db.KeyDeleteAsync(key);\n+        }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/RedisVLDocumentStore.cs",
    "commit_id": "62aea1a1b376f23b0693eb1f0ecd24a268f42be2",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_≡ƒ¢á∩╕Å Refactor suggestion_ | _≡ƒƒá Major_\n\n**Required: DeleteDocumentAsync should indicate success/failure.**\n\nThe base class `RemoveCore` method returns `bool` to indicate whether a document was found and removed. The current implementation doesn't provide this information.\n\n\n\nModify to return success status:\n\n```diff\n-public override async Task DeleteDocumentAsync(string documentId)\n+private async Task<bool> DeleteDocumentAsync(string documentId)\n {\n     if (string.IsNullOrEmpty(documentId))\n         throw new ArgumentException(\"Document ID cannot be null or empty\", nameof(documentId));\n\n     var key = $\"{_indexName}:{documentId}\";\n-    await _db.KeyDeleteAsync(key);\n+    return await _db.KeyDeleteAsync(key);\n }\n```\n\nThen implement the required abstract method:\n```csharp\nprotected override bool RemoveCore(string documentId)\n{\n    return DeleteDocumentAsync(documentId).GetAwaiter().GetResult();\n}\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/RedisVLDocumentStore.cs\naround lines 94-101, DeleteDocumentAsync currently returns Task with no success\nindication; change its signature to return Task<bool>, use the result of\n_db.KeyDeleteAsync(key) (await and return that bool) so callers know if a key\nwas found/deleted, and then implement the required abstract override protected\noverride bool RemoveCore(string documentId) to synchronously call\nDeleteDocumentAsync(documentId).GetAwaiter().GetResult() and return that value;\nupdate any call sites if needed to handle the new return type.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T13:49:51Z",
    "updated_at": "2025-11-04T13:49:56Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490595980",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595980"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490595980"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595980/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 94,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 101,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 101,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595985",
    "pull_request_review_id": 3416572453,
    "id": 2490595985,
    "node_id": "PRRC_kwDOKSXUF86Uc3qR",
    "diff_hunk": "@@ -0,0 +1,146 @@\n+#if NETCOREAPP || NETSTANDARD2_1_OR_GREATER\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Text.Json;\n+using System.Threading.Tasks;\n+using StackExchange.Redis;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores\n+{\n+    /// <summary>\n+    /// Document store using Redis Vector Library for vector search\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations</typeparam>\n+    public class RedisVLDocumentStore<T> : DocumentStoreBase<T> where T : struct, IComparable, IComparable<T>, IConvertible, IEquatable<T>, IFormattable\n+    {\n+        private readonly ConnectionMultiplexer _redis;\n+        private readonly IDatabase _db;\n+        private readonly string _indexName;\n+\n+        public RedisVLDocumentStore(string connectionString, string indexName = \"documents\")\n+        {\n+            if (string.IsNullOrEmpty(connectionString))\n+                throw new ArgumentException(\"Connection string cannot be null or empty\", nameof(connectionString));\n+\n+            _redis = ConnectionMultiplexer.Connect(connectionString);\n+            _db = _redis.GetDatabase();\n+            _indexName = indexName;\n+        }\n+\n+        public override async Task AddDocumentAsync(Document<T> document)\n+        {\n+            if (document == null)\n+                throw new ArgumentNullException(nameof(document));\n+\n+            var key = $\"{_indexName}:{document.Id}\";\n+            var hash = new HashEntry[]\n+            {\n+                new HashEntry(\"id\", document.Id),\n+                new HashEntry(\"content\", document.Content),\n+                new HashEntry(\"embedding\", JsonSerializer.Serialize(ConvertVectorToDoubleArray(document.Embedding))),\n+                new HashEntry(\"metadata\", JsonSerializer.Serialize(document.Metadata))\n+            };\n+\n+            await _db.HashSetAsync(key, hash);\n+        }\n+\n+        public override async Task<List<Document<T>>> SearchAsync(Vector<T> queryEmbedding, int topK = 5)\n+        {\n+            if (queryEmbedding == null)\n+                throw new ArgumentNullException(nameof(queryEmbedding));\n+\n+            var pattern = $\"{_indexName}:*\";\n+            var server = _redis.GetServer(_redis.GetEndPoints()[0]);\n+            var keys = server.Keys(pattern: pattern).ToList();\n+\n+            var results = new List<(Document<T> doc, T similarity)>();\n+\n+            foreach (var key in keys)\n+            {\n+                var hash = await _db.HashGetAllAsync(key);\n+                var hashDict = hash.ToDictionary(x => x.Name.ToString(), x => x.Value.ToString());\n+\n+                if (!hashDict.ContainsKey(\"embedding\"))\n+                    continue;\n+\n+                var embedding = JsonSerializer.Deserialize<double[]>(hashDict[\"embedding\"]) ?? Array.Empty<double>();\n+                var embeddingVector = new Vector<T>(\n+                    embedding.Select(x => (T)Convert.ChangeType(x, typeof(T))).ToArray(),\n+                    NumOps);\n+\n+                var similarity = StatisticsHelper.CosineSimilarity(queryEmbedding, embeddingVector, NumOps);\n+\n+                var doc = new Document<T>\n+                {\n+                    Id = hashDict.GetValueOrDefault(\"id\", string.Empty),\n+                    Content = hashDict.GetValueOrDefault(\"content\", string.Empty),\n+                    Metadata = JsonSerializer.Deserialize<Dictionary<string, string>>(\n+                        hashDict.GetValueOrDefault(\"metadata\", \"{}\")) ?? new Dictionary<string, string>()\n+                };\n+\n+                results.Add((doc, similarity));\n+            }\n+\n+            return results\n+                .OrderByDescending(x => x.similarity)\n+                .Take(topK)\n+                .Select(x => x.doc)\n+                .ToList();\n+        }\n+\n+        public override async Task DeleteDocumentAsync(string documentId)\n+        {\n+            if (string.IsNullOrEmpty(documentId))\n+                throw new ArgumentException(\"Document ID cannot be null or empty\", nameof(documentId));\n+\n+            var key = $\"{_indexName}:{documentId}\";\n+            await _db.KeyDeleteAsync(key);\n+        }\n+\n+        public override async Task<Document<T>?> GetDocumentAsync(string documentId)\n+        {\n+            if (string.IsNullOrEmpty(documentId))\n+                throw new ArgumentException(\"Document ID cannot be null or empty\", nameof(documentId));\n+\n+            var key = $\"{_indexName}:{documentId}\";\n+            var hash = await _db.HashGetAllAsync(key);\n+\n+            if (hash.Length == 0)\n+                return null;\n+\n+            var hashDict = hash.ToDictionary(x => x.Name.ToString(), x => x.Value.ToString());\n+\n+            return new Document<T>\n+            {\n+                Id = hashDict.GetValueOrDefault(\"id\", string.Empty),\n+                Content = hashDict.GetValueOrDefault(\"content\", string.Empty),\n+                Metadata = JsonSerializer.Deserialize<Dictionary<string, string>>(\n+                    hashDict.GetValueOrDefault(\"metadata\", \"{}\")) ?? new Dictionary<string, string>()\n+            };\n+        }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/RedisVLDocumentStore.cs",
    "commit_id": "62aea1a1b376f23b0693eb1f0ecd24a268f42be2",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Major issue: Metadata type mismatch in GetDocumentAsync.**\n\nLines 120-121 deserialize metadata to `Dictionary<string, string>`, but the `Document<T>.Metadata` property is `Dictionary<string, object>` (per Document.cs line 63). This type mismatch can cause runtime errors or data loss for non-string metadata values.\n\n\n\nApply this fix:\n\n```diff\n return new Document<T>\n {\n     Id = hashDict.GetValueOrDefault(\"id\", string.Empty),\n     Content = hashDict.GetValueOrDefault(\"content\", string.Empty),\n-    Metadata = JsonSerializer.Deserialize<Dictionary<string, string>>(\n-        hashDict.GetValueOrDefault(\"metadata\", \"{}\")) ?? new Dictionary<string, string>()\n+    Metadata = JsonSerializer.Deserialize<Dictionary<string, object>>(\n+        hashDict.GetValueOrDefault(\"metadata\", \"{}\")) ?? new Dictionary<string, object>()\n };\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        public override async Task<Document<T>?> GetDocumentAsync(string documentId)\n        {\n            if (string.IsNullOrEmpty(documentId))\n                throw new ArgumentException(\"Document ID cannot be null or empty\", nameof(documentId));\n\n            var key = $\"{_indexName}:{documentId}\";\n            var hash = await _db.HashGetAllAsync(key);\n\n            if (hash.Length == 0)\n                return null;\n\n            var hashDict = hash.ToDictionary(x => x.Name.ToString(), x => x.Value.ToString());\n\n            return new Document<T>\n            {\n                Id = hashDict.GetValueOrDefault(\"id\", string.Empty),\n                Content = hashDict.GetValueOrDefault(\"content\", string.Empty),\n                Metadata = JsonSerializer.Deserialize<Dictionary<string, object>>(\n                    hashDict.GetValueOrDefault(\"metadata\", \"{}\")) ?? new Dictionary<string, object>()\n            };\n        }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/RedisVLDocumentStore.cs\naround lines 103 to 123, the code deserializes metadata into Dictionary<string,\nstring> while Document<T>.Metadata is Dictionary<string, object>; change the\ndeserialization to JsonSerializer.Deserialize<Dictionary<string, object>>(json)\n(or equivalent) and ensure the fallback is new Dictionary<string, object>() so\nnon-string metadata types are preserved and no runtime cast errors occur.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T13:49:51Z",
    "updated_at": "2025-11-04T13:49:56Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490595985",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595985"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490595985"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490595985/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 103,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 123,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 123,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596002",
    "pull_request_review_id": 3416572453,
    "id": 2490596002,
    "node_id": "PRRC_kwDOKSXUF86Uc3qi",
    "diff_hunk": "@@ -0,0 +1,165 @@\n+#if NETCOREAPP || NETSTANDARD2_1_OR_GREATER\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Data.SQLite;",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/SQLiteVSSDocumentStore.cs",
    "commit_id": "62aea1a1b376f23b0693eb1f0ecd24a268f42be2",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**System.Data.SQLite isnΓÇÖt available here**\n\n`System.Data.SQLite` isnΓÇÖt part of the BCL for our target frameworks, and the build already fails because the namespace canΓÇÖt be found. Please either add the appropriate NuGet package (platform-specific) or switch to a supported provider such as `Microsoft.Data.Sqlite` before landing this file.\n\n<details>\n<summary>≡ƒº░ Tools</summary>\n\n<details>\n<summary>≡ƒ¬¢ GitHub Check: Publish Size Analysis</summary>\n\n[failure] 6-6: \nThe type or namespace name 'SQLite' does not exist in the namespace 'System.Data' (are you missing an assembly reference?)\n\n</details>\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits e352386 to 33a79e8",
    "created_at": "2025-11-04T13:49:51Z",
    "updated_at": "2025-11-04T13:56:21Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490596002",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596002"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490596002"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596002/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": null,
    "original_line": 6,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 6,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596014",
    "pull_request_review_id": 3416572453,
    "id": 2490596014,
    "node_id": "PRRC_kwDOKSXUF86Uc3qu",
    "diff_hunk": "@@ -0,0 +1,99 @@\n+using AiDotNet.LinearAlgebra;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Threading.Tasks;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels\n+{\n+    /// <summary>\n+    /// Fine-tuning wrapper for sentence transformer models\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for embeddings</typeparam>\n+    public class SentenceTransformersFineTuner<T> : EmbeddingModelBase<T> where T : struct, IComparable, IComparable<T>, IConvertible, IEquatable<T>, IFormattable\n+    {\n+        private readonly IEmbeddingModel<T> _baseModel;\n+        private readonly Dictionary<string, Vector<T>> _finetuneCache;\n+        private bool _isFineTuned;\n+\n+        public SentenceTransformersFineTuner(IEmbeddingModel<T> baseModel, INormalizer<T>? normalizer = null)\n+            : base(normalizer)\n+        {\n+            _baseModel = baseModel ?? throw new ArgumentNullException(nameof(baseModel));\n+            _finetuneCache = new Dictionary<string, Vector<T>>();\n+            _isFineTuned = false;\n+        }\n+\n+        protected override async Task<Vector<T>> GenerateEmbeddingCoreAsync(string text)\n+        {\n+            if (_finetuneCache.TryGetValue(text, out var cachedEmbedding))\n+            {\n+                return Normalizer?.Normalize(cachedEmbedding) ?? cachedEmbedding;\n+            }\n+\n+            var embedding = await _baseModel.GenerateEmbeddingAsync(text);\n+            \n+            if (_isFineTuned)\n+            {\n+                _finetuneCache[text] = embedding;\n+            }",
    "path": "src/RetrievalAugmentedGeneration/EmbeddingModels/SentenceTransformersFineTuner.cs",
    "commit_id": "33a79e89f3137dc80c940c48d7ab74f127dd4d91",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Major: Unbounded cache growth for arbitrary texts.**\n\nThe logic caches embeddings for ANY text encountered after fine-tuning (not just training pairs). This causes unbounded memory growth and defeats the purpose of caching only fine-tuned embeddings.\n\nIf the intent is to cache only training pairs, remove this block since `FineTuneAsync` already populates the cache (lines 67-68). If the intent is to cache all embeddings after fine-tuning for performance, add explicit cache size limits and eviction policies.\n\n\n\nConsider removing the automatic caching:\n\n```diff\n     var embedding = await _baseModel.GenerateEmbeddingAsync(text);\n-    \n-    if (_isFineTuned)\n-    {\n-        _finetuneCache[text] = embedding;\n-    }\n\n     return Normalizer?.Normalize(embedding) ?? embedding;\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn\nsrc/RetrievalAugmentedGeneration/EmbeddingModels/SentenceTransformersFineTuner.cs\naround lines 36 to 39, the code unconditionally caches embeddings for any text\nonce _isFineTuned is true, causing unbounded memory growth; either remove this\nblock so only training pairs (already cached in FineTuneAsync at lines ~67-68)\npopulate _finetuneCache, or implement a bounded cache with size limit and\neviction (e.g., LRU) and thread-safety: pick one of the two approaches, remove\nthe unconditional _finetuneCache[text] = embedding; if you only want\ntraining-pair caching, or replace it with a safe AddToCache method that enforces\nmax capacity, eviction policy, and concurrency control if you intend to cache\ngeneral texts after fine-tuning.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commit 5e2879c",
    "created_at": "2025-11-04T13:49:51Z",
    "updated_at": "2025-11-04T14:11:55Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490596014",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596014"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490596014"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596014/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 36,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 39,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 39,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596028",
    "pull_request_review_id": 3416572453,
    "id": 2490596028,
    "node_id": "PRRC_kwDOKSXUF86Uc3q8",
    "diff_hunk": "@@ -0,0 +1,99 @@\n+using AiDotNet.LinearAlgebra;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Threading.Tasks;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels\n+{\n+    /// <summary>\n+    /// Fine-tuning wrapper for sentence transformer models\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for embeddings</typeparam>\n+    public class SentenceTransformersFineTuner<T> : EmbeddingModelBase<T> where T : struct, IComparable, IComparable<T>, IConvertible, IEquatable<T>, IFormattable\n+    {\n+        private readonly IEmbeddingModel<T> _baseModel;\n+        private readonly Dictionary<string, Vector<T>> _finetuneCache;\n+        private bool _isFineTuned;\n+\n+        public SentenceTransformersFineTuner(IEmbeddingModel<T> baseModel, INormalizer<T>? normalizer = null)\n+            : base(normalizer)\n+        {\n+            _baseModel = baseModel ?? throw new ArgumentNullException(nameof(baseModel));\n+            _finetuneCache = new Dictionary<string, Vector<T>>();\n+            _isFineTuned = false;\n+        }\n+\n+        protected override async Task<Vector<T>> GenerateEmbeddingCoreAsync(string text)\n+        {\n+            if (_finetuneCache.TryGetValue(text, out var cachedEmbedding))\n+            {\n+                return Normalizer?.Normalize(cachedEmbedding) ?? cachedEmbedding;\n+            }\n+\n+            var embedding = await _baseModel.GenerateEmbeddingAsync(text);\n+            \n+            if (_isFineTuned)\n+            {\n+                _finetuneCache[text] = embedding;\n+            }\n+\n+            return Normalizer?.Normalize(embedding) ?? embedding;\n+        }\n+\n+        public async Task FineTuneAsync(\n+            List<(string positive, string negative)> contrastivePairs,\n+            int epochs = 3,\n+            T learningRate = default)\n+        {\n+            if (contrastivePairs == null || contrastivePairs.Count == 0)\n+                throw new ArgumentException(\"Contrastive pairs cannot be null or empty\", nameof(contrastivePairs));\n+\n+            if (NumOps.Equals(learningRate, NumOps.Zero))\n+            {\n+                learningRate = NumOps.FromDouble(0.001);\n+            }\n+\n+            for (int epoch = 0; epoch < epochs; epoch++)\n+            {\n+                foreach (var (positive, negative) in contrastivePairs)\n+                {\n+                    var positiveEmbedding = await _baseModel.GenerateEmbeddingAsync(positive);\n+                    var negativeEmbedding = await _baseModel.GenerateEmbeddingAsync(negative);\n+\n+                    var adjustedPositive = AdjustEmbedding(positiveEmbedding, learningRate, isPositive: true);\n+                    var adjustedNegative = AdjustEmbedding(negativeEmbedding, learningRate, isPositive: false);\n+\n+                    _finetuneCache[positive] = adjustedPositive;\n+                    _finetuneCache[negative] = adjustedNegative;\n+                }\n+            }",
    "path": "src/RetrievalAugmentedGeneration/EmbeddingModels/SentenceTransformersFineTuner.cs",
    "commit_id": "33a79e89f3137dc80c940c48d7ab74f127dd4d91",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Major: Inefficient and incorrect fine-tuning algorithm.**\n\nThree significant issues:\n\n1. **Performance**: Base model embeddings are regenerated for each pair in every epoch. For 100 pairs over 3 epochs, this makes 600 embedding calls instead of 200.\n\n2. **Correctness**: Each epoch starts from fresh base model embeddings (lines 61-62), so adjustments don't accumulate. Multiple epochs have no effect beyond the last one. The algorithm should either:\n   - Apply adjustments cumulatively to the previous epoch's results, OR\n   - Apply `adjustment * epoch` to implement a learning schedule\n\n3. **Concurrency**: `_finetuneCache` (Dictionary) is not thread-safe. Concurrent calls to `FineTuneAsync` or `GenerateEmbeddingCoreAsync` risk corruption.\n\n\n\nCache base embeddings before the epoch loop:\n\n```diff\n     if (NumOps.Equals(learningRate, NumOps.Zero))\n     {\n         learningRate = NumOps.FromDouble(0.001);\n     }\n+\n+    // Cache base embeddings once\n+    var baseEmbeddings = new Dictionary<string, Vector<T>>();\n+    foreach (var (positive, negative) in contrastivePairs)\n+    {\n+        if (!baseEmbeddings.ContainsKey(positive))\n+            baseEmbeddings[positive] = await _baseModel.GenerateEmbeddingAsync(positive);\n+        if (!baseEmbeddings.ContainsKey(negative))\n+            baseEmbeddings[negative] = await _baseModel.GenerateEmbeddingAsync(negative);\n+    }\n\n     for (int epoch = 0; epoch < epochs; epoch++)\n     {\n         foreach (var (positive, negative) in contrastivePairs)\n         {\n-            var positiveEmbedding = await _baseModel.GenerateEmbeddingAsync(positive);\n-            var negativeEmbedding = await _baseModel.GenerateEmbeddingAsync(negative);\n+            var positiveEmbedding = _finetuneCache.TryGetValue(positive, out var cachedPos) \n+                ? cachedPos \n+                : baseEmbeddings[positive];\n+            var negativeEmbedding = _finetuneCache.TryGetValue(negative, out var cachedNeg)\n+                ? cachedNeg\n+                : baseEmbeddings[negative];\n\n             var adjustedPositive = AdjustEmbedding(positiveEmbedding, learningRate, isPositive: true);\n```\n\nFor thread safety, consider using `ConcurrentDictionary<string, Vector<T>>` or add locking.\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T13:49:51Z",
    "updated_at": "2025-11-04T13:49:57Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490596028",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596028"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490596028"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596028/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 57,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 70,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 70,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596045",
    "pull_request_review_id": 3416572453,
    "id": 2490596045,
    "node_id": "PRRC_kwDOKSXUF86Uc3rN",
    "diff_hunk": "@@ -0,0 +1,99 @@\n+using AiDotNet.LinearAlgebra;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Threading.Tasks;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels\n+{\n+    /// <summary>\n+    /// Fine-tuning wrapper for sentence transformer models\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for embeddings</typeparam>\n+    public class SentenceTransformersFineTuner<T> : EmbeddingModelBase<T> where T : struct, IComparable, IComparable<T>, IConvertible, IEquatable<T>, IFormattable\n+    {\n+        private readonly IEmbeddingModel<T> _baseModel;\n+        private readonly Dictionary<string, Vector<T>> _finetuneCache;\n+        private bool _isFineTuned;\n+\n+        public SentenceTransformersFineTuner(IEmbeddingModel<T> baseModel, INormalizer<T>? normalizer = null)\n+            : base(normalizer)\n+        {\n+            _baseModel = baseModel ?? throw new ArgumentNullException(nameof(baseModel));\n+            _finetuneCache = new Dictionary<string, Vector<T>>();\n+            _isFineTuned = false;\n+        }\n+\n+        protected override async Task<Vector<T>> GenerateEmbeddingCoreAsync(string text)\n+        {\n+            if (_finetuneCache.TryGetValue(text, out var cachedEmbedding))\n+            {\n+                return Normalizer?.Normalize(cachedEmbedding) ?? cachedEmbedding;\n+            }\n+\n+            var embedding = await _baseModel.GenerateEmbeddingAsync(text);\n+            \n+            if (_isFineTuned)\n+            {\n+                _finetuneCache[text] = embedding;\n+            }\n+\n+            return Normalizer?.Normalize(embedding) ?? embedding;\n+        }\n+\n+        public async Task FineTuneAsync(\n+            List<(string positive, string negative)> contrastivePairs,\n+            int epochs = 3,\n+            T learningRate = default)\n+        {\n+            if (contrastivePairs == null || contrastivePairs.Count == 0)\n+                throw new ArgumentException(\"Contrastive pairs cannot be null or empty\", nameof(contrastivePairs));\n+\n+            if (NumOps.Equals(learningRate, NumOps.Zero))\n+            {\n+                learningRate = NumOps.FromDouble(0.001);\n+            }\n+\n+            for (int epoch = 0; epoch < epochs; epoch++)\n+            {\n+                foreach (var (positive, negative) in contrastivePairs)\n+                {\n+                    var positiveEmbedding = await _baseModel.GenerateEmbeddingAsync(positive);\n+                    var negativeEmbedding = await _baseModel.GenerateEmbeddingAsync(negative);\n+\n+                    var adjustedPositive = AdjustEmbedding(positiveEmbedding, learningRate, isPositive: true);\n+                    var adjustedNegative = AdjustEmbedding(negativeEmbedding, learningRate, isPositive: false);\n+\n+                    _finetuneCache[positive] = adjustedPositive;\n+                    _finetuneCache[negative] = adjustedNegative;\n+                }\n+            }\n+\n+            _isFineTuned = true;\n+        }\n+\n+        private Vector<T> AdjustEmbedding(Vector<T> embedding, T learningRate, bool isPositive)\n+        {\n+            var adjustment = isPositive ? learningRate : NumOps.Negate(learningRate);\n+            var adjustedValues = new T[embedding.Length];\n+\n+            for (int i = 0; i < embedding.Length; i++)\n+            {\n+                var delta = NumOps.Multiply(embedding[i], adjustment);\n+                adjustedValues[i] = NumOps.Add(embedding[i], delta);\n+            }\n+\n+            return new Vector<T>(adjustedValues, NumOps);\n+        }",
    "path": "src/RetrievalAugmentedGeneration/EmbeddingModels/SentenceTransformersFineTuner.cs",
    "commit_id": "33a79e89f3137dc80c940c48d7ab74f127dd4d91",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Major: Algorithm doesn't implement contrastive learning.**\n\nThe adjustment simply scales each embedding independently: `adjusted[i] = embedding[i] * (1 + learningRate)`. This:\n- Does NOT pull positive pairs closer together\n- Does NOT push negative pairs farther apart  \n- Ignores the relational \"contrastive\" aspect entirely\n\nTrue contrastive learning adjusts embeddings based on their pairwise distances. The class name \"SentenceTransformersFineTuner\" is misleadingΓÇöthis doesn't fine-tune the underlying model.\n\nConsider either:\n1. Renaming to `CachedEmbeddingAdjuster<T>` or similar to reflect actual behavior\n2. Implementing proper contrastive adjustment (e.g., move positive pairs toward their average, push negatives away from positives)\n\n\n\nExample proper contrastive adjustment:\n\n```diff\n private Vector<T> AdjustEmbedding(Vector<T> embedding, T learningRate, bool isPositive)\n {\n-    var adjustment = isPositive ? learningRate : NumOps.Negate(learningRate);\n     var adjustedValues = new T[embedding.Length];\n-\n+    \n+    // For true contrastive learning, compute target direction and move embeddings toward/away from it\n+    // This is a simplified example - real contrastive learning uses pair-wise loss gradients\n     for (int i = 0; i < embedding.Length; i++)\n     {\n-        var delta = NumOps.Multiply(embedding[i], adjustment);\n-        adjustedValues[i] = NumOps.Add(embedding[i], delta);\n+        // Move positive embeddings toward a target (e.g., centroid of positive cluster)\n+        // Move negative embeddings away from positives\n+        // This requires tracking pair relationships, not independent scaling\n     }\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn\nsrc/RetrievalAugmentedGeneration/EmbeddingModels/SentenceTransformersFineTuner.cs\naround lines 75 to 87, the AdjustEmbedding implementation merely scales each\ncomponent and does not perform contrastive updates; either rename the\nclass/method to reflect simple cached/scaling behavior or implement true\ncontrastive updates. To fix: if keeping current behavior, rename class/method to\nCachedEmbeddingAdjuster or ScaleAdjuster and update comments/tests/usage to\nreflect non-model-finetuning semantics; otherwise implement contrastive logic by\naccepting positive and negative embedding(s), computing a target direction\n(e.g., move positives toward their centroid and push negatives away by computing\npairwise differences or gradients), and update embeddings using a\nlearning-rate-weighted vector update (embedding += lr * (target - embedding) for\npositives and embedding -= lr * (neg_direction) for negatives), ensuring vector\noperations use NumOps and preserve numeric types and lengths.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T13:49:51Z",
    "updated_at": "2025-11-04T13:49:57Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490596045",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596045"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490596045"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596045/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 75,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 87,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 87,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596066",
    "pull_request_review_id": 3416572453,
    "id": 2490596066,
    "node_id": "PRRC_kwDOKSXUF86Uc3ri",
    "diff_hunk": "@@ -0,0 +1,74 @@\n+#if NETCOREAPP || NETSTANDARD2_1_OR_GREATER\n+using AiDotNet.LinearAlgebra;\n+using System;\n+using System.Net.Http;\n+using System.Text;\n+using System.Text.Json;\n+using System.Threading.Tasks;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels\n+{\n+    /// <summary>\n+    /// Embedding model using Voyage AI's API for text embeddings\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for embeddings</typeparam>\n+    public class VoyageAIEmbeddingModel<T> : EmbeddingModelBase<T> where T : struct, IComparable, IComparable<T>, IConvertible, IEquatable<T>, IFormattable\n+    {\n+        private readonly string _apiKey;\n+        private readonly string _model;\n+        private readonly HttpClient _httpClient;\n+        private const string ApiBaseUrl = \"https://api.voyageai.com/v1\";\n+\n+        public VoyageAIEmbeddingModel(string apiKey, string model = \"voyage-2\", INormalizer<T>? normalizer = null)\n+            : base(normalizer)\n+        {\n+            if (string.IsNullOrEmpty(apiKey))\n+                throw new ArgumentException(\"API key cannot be null or empty\", nameof(apiKey));\n+\n+            _apiKey = apiKey;\n+            _model = model;\n+            _httpClient = new HttpClient();\n+            _httpClient.DefaultRequestHeaders.Add(\"Authorization\", $\"Bearer {_apiKey}\");\n+        }",
    "path": "src/RetrievalAugmentedGeneration/EmbeddingModels/VoyageAIEmbeddingModel.cs",
    "commit_id": "33a79e89f3137dc80c940c48d7ab74f127dd4d91",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n<details>\n<summary>≡ƒº⌐ Analysis chain</summary>\n\n**HttpClient instantiation anti-pattern detected.**\n\nCreating a new `HttpClient` instance in the constructor can lead to socket exhaustion under load. The recommended approach is to either:\n1. Accept an `HttpClient` via constructor dependency injection, or  \n2. Use `IHttpClientFactory` for proper lifecycle management.\n\nNote: This pattern is consistent with `CohereEmbeddingModel` and `GooglePalmEmbeddingModel`, so consider refactoring all embedding models together.\n\n\n\n\nApply this diff to accept HttpClient via dependency injection:\n\n```diff\n-        private readonly HttpClient _httpClient;\n-\n-        public VoyageAIEmbeddingModel(string apiKey, string model = \"voyage-2\", INormalizer<T>? normalizer = null)\n+        public VoyageAIEmbeddingModel(string apiKey, HttpClient httpClient, string model = \"voyage-2\", INormalizer<T>? normalizer = null)\n             : base(normalizer)\n         {\n             if (string.IsNullOrEmpty(apiKey))\n                 throw new ArgumentException(\"API key cannot be null or empty\", nameof(apiKey));\n \n             _apiKey = apiKey;\n             _model = model;\n-            _httpClient = new HttpClient();\n+            _httpClient = httpClient ?? throw new ArgumentNullException(nameof(httpClient));\n             _httpClient.DefaultRequestHeaders.Add(\"Authorization\", $\"Bearer {_apiKey}\");\n         }\n```\n\nRun this script to verify the same pattern exists in other embedding models:\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Find all embedding model classes that instantiate HttpClient in constructor\nrg -n \"new HttpClient\\(\\)\" --type cs -g '*EmbeddingModel.cs'\n```\n\nLength of output: 365\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check Dispose methods in embedding models\nrg -n \"public void Dispose\" --type cs -A 5 -g '*EmbeddingModel.cs' | head -50\n```\n\nLength of output: 0\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Search for instantiation of VoyageAIEmbeddingModel, GooglePalmEmbeddingModel, CohereEmbeddingModel\nrg -n \"new (Voyage|GooglePalm|Cohere)EmbeddingModel\" --type cs -C 3 | head -60\n```\n\nLength of output: 0\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check if IHttpClientFactory is already used in the codebase\nrg -n \"IHttpClientFactory\" --type cs\n```\n\nLength of output: 0\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check for DI container setup or service registration patterns\nrg -n \"(services\\.Add|AddHttpClient|HttpClientBuilder)\" --type cs | head -30\n```\n\nLength of output: 0\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# View full VoyageAIEmbeddingModel.cs to understand class structure and Dispose implementation\ncat src/RetrievalAugmentedGeneration/EmbeddingModels/VoyageAIEmbeddingModel.cs\n```\n\nLength of output: 2657\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Find EmbeddingModelBase to see parent class implementation\nfind . -name \"EmbeddingModelBase.cs\" -type f -exec cat {} \\;\n```\n\nLength of output: 6815\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Search for test files or usages of embedding models\nrg -n \"EmbeddingModel\" --type cs -g '*Test*.cs' -g '*test*.cs' | head -20\n```\n\nLength of output: 0\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Broader search for where VoyageAIEmbeddingModel might be instantiated or referenced\nrg -n \"VoyageAI\" --type cs | head -20\n```\n\nLength of output: 426\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check CohereEmbeddingModel and GooglePalmEmbeddingModel for JsonDocument usage\nrg -n \"JsonDocument\" --type cs -g '*EmbeddingModel.cs' -A 2 -B 2\n```\n\nLength of output: 1900\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check if EmbeddingModelBase implements IDisposable\nrg -n \"IDisposable|protected.*Dispose\" src/RetrievalAugmentedGeneration/Embeddings/EmbeddingModelBase.cs\n```\n\nLength of output: 0\n\n---\n\n\n\n</details>\n\n**Fix JsonDocument resource leak in GenerateEmbeddingCoreAsync.**\n\nThe `JsonDocument` created at line 49 is not disposed. Wrap it in a `using` statement to prevent resource leaks:\n\n```diff\n             var responseJson = await response.Content.ReadAsStringAsync();\n-            var document = JsonDocument.Parse(responseJson);\n+            using var document = JsonDocument.Parse(responseJson);\n             \n             var embedding = document.RootElement.GetProperty(\"data\")[0].GetProperty(\"embedding\");\n```\n\nThis same issue exists in `CohereEmbeddingModel.cs:50` and `GooglePalmEmbeddingModel.cs:48`ΓÇöupdate all three together.\n\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/EmbeddingModels/VoyageAIEmbeddingModel.cs\n(GenerateEmbeddingCoreAsync around line 49) and also in\nsrc/RetrievalAugmentedGeneration/EmbeddingModels/CohereEmbeddingModel.cs (around\nline 50) and\nsrc/RetrievalAugmentedGeneration/EmbeddingModels/GooglePalmEmbeddingModel.cs\n(around line 48), the JsonDocument created when parsing the response is not\ndisposed; wrap the JsonDocument creation in a using block (or use \"using var\njson = ...\") so the document is disposed after use, and adjust code inside the\nblock to read values before exiting to prevent the resource leak.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T13:49:52Z",
    "updated_at": "2025-11-04T13:49:57Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490596066",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596066"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490596066"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596066/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 22,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 32,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 32,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596095",
    "pull_request_review_id": 3416572453,
    "id": 2490596095,
    "node_id": "PRRC_kwDOKSXUF86Uc3r_",
    "diff_hunk": "@@ -0,0 +1,74 @@\n+#if NETCOREAPP || NETSTANDARD2_1_OR_GREATER\n+using AiDotNet.LinearAlgebra;\n+using System;\n+using System.Net.Http;\n+using System.Text;\n+using System.Text.Json;\n+using System.Threading.Tasks;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels\n+{\n+    /// <summary>\n+    /// Embedding model using Voyage AI's API for text embeddings\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for embeddings</typeparam>\n+    public class VoyageAIEmbeddingModel<T> : EmbeddingModelBase<T> where T : struct, IComparable, IComparable<T>, IConvertible, IEquatable<T>, IFormattable\n+    {\n+        private readonly string _apiKey;\n+        private readonly string _model;\n+        private readonly HttpClient _httpClient;\n+        private const string ApiBaseUrl = \"https://api.voyageai.com/v1\";\n+\n+        public VoyageAIEmbeddingModel(string apiKey, string model = \"voyage-2\", INormalizer<T>? normalizer = null)\n+            : base(normalizer)\n+        {\n+            if (string.IsNullOrEmpty(apiKey))\n+                throw new ArgumentException(\"API key cannot be null or empty\", nameof(apiKey));\n+\n+            _apiKey = apiKey;\n+            _model = model;\n+            _httpClient = new HttpClient();\n+            _httpClient.DefaultRequestHeaders.Add(\"Authorization\", $\"Bearer {_apiKey}\");\n+        }\n+\n+        protected override async Task<Vector<T>> GenerateEmbeddingCoreAsync(string text)\n+        {\n+            var requestBody = new\n+            {\n+                input = new[] { text },\n+                model = _model\n+            };\n+\n+            var json = JsonSerializer.Serialize(requestBody);\n+            var content = new StringContent(json, Encoding.UTF8, \"application/json\");\n+\n+            var response = await _httpClient.PostAsync($\"{ApiBaseUrl}/embeddings\", content);\n+            response.EnsureSuccessStatusCode();\n+\n+            var responseJson = await response.Content.ReadAsStringAsync();\n+            var document = JsonDocument.Parse(responseJson);\n+            \n+            var embedding = document.RootElement.GetProperty(\"data\")[0].GetProperty(\"embedding\");\n+            var values = new T[embedding.GetArrayLength()];\n+            \n+            for (int i = 0; i < values.Length; i++)\n+            {\n+                values[i] = (T)Convert.ChangeType(embedding[i].GetDouble(), typeof(T));\n+            }\n+\n+            var vector = new Vector<T>(values, NumOps);\n+            return Normalizer?.Normalize(vector) ?? vector;\n+        }",
    "path": "src/RetrievalAugmentedGeneration/EmbeddingModels/VoyageAIEmbeddingModel.cs",
    "commit_id": "33a79e89f3137dc80c940c48d7ab74f127dd4d91",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Dispose JsonDocument to prevent resource leak.**\n\n`JsonDocument.Parse` returns an `IDisposable` that must be disposed. Wrap it in a `using` statement to ensure proper cleanup.\n\n\n\nApply this diff to fix the resource leak:\n\n```diff\n             var responseJson = await response.Content.ReadAsStringAsync();\n-            var document = JsonDocument.Parse(responseJson);\n-            \n-            var embedding = document.RootElement.GetProperty(\"data\")[0].GetProperty(\"embedding\");\n-            var values = new T[embedding.GetArrayLength()];\n-            \n-            for (int i = 0; i < values.Length; i++)\n+            using var document = JsonDocument.Parse(responseJson);\n+            \n+            var embedding = document.RootElement.GetProperty(\"data\")[0].GetProperty(\"embedding\");\n+            var values = new T[embedding.GetArrayLength()];\n+            \n+            for (int i = 0; i < values.Length; i++)\n             {\n                 values[i] = (T)Convert.ChangeType(embedding[i].GetDouble(), typeof(T));\n             }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n            var responseJson = await response.Content.ReadAsStringAsync();\n            using var document = JsonDocument.Parse(responseJson);\n            \n            var embedding = document.RootElement.GetProperty(\"data\")[0].GetProperty(\"embedding\");\n            var values = new T[embedding.GetArrayLength()];\n            \n            for (int i = 0; i < values.Length; i++)\n            {\n                values[i] = (T)Convert.ChangeType(embedding[i].GetDouble(), typeof(T));\n            }\n\n            var vector = new Vector<T>(values, NumOps);\n            return Normalizer?.Normalize(vector) ?? vector;\n        }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/EmbeddingModels/VoyageAIEmbeddingModel.cs\naround lines 48 to 61 the JsonDocument created by JsonDocument.Parse is not\ndisposed, causing a resource leak; wrap the JsonDocument in a using statement\n(or a C# using declaration) so it gets disposed after use, move all accesses to\ndocument.RootElement inside the using scope, and then proceed to construct and\nreturn the normalized vector.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commit 5e2879c",
    "created_at": "2025-11-04T13:49:52Z",
    "updated_at": "2025-11-04T14:12:09Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490596095",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596095"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490596095"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596095/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 48,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 61,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 61,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596119",
    "pull_request_review_id": 3416572453,
    "id": 2490596119,
    "node_id": "PRRC_kwDOKSXUF86Uc3sX",
    "diff_hunk": "@@ -0,0 +1,80 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Threading.Tasks;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.QueryExpansion\n+{\n+    /// <summary>\n+    /// Query expansion using learned sparse encoders\n+    /// </summary>\n+    public class LearnedSparseEncoderExpansion : QueryExpansionBase\n+    {\n+        private readonly Dictionary<string, double> _termWeights;\n+\n+        public LearnedSparseEncoderExpansion()\n+        {\n+            _termWeights = new Dictionary<string, double>();\n+        }\n+\n+        public void TrainWeights(Dictionary<string, double> termWeights)\n+        {\n+            if (termWeights == null)\n+                throw new ArgumentNullException(nameof(termWeights));\n+\n+            _termWeights.Clear();\n+            foreach (var (term, weight) in termWeights)\n+            {\n+                _termWeights[term] = weight;\n+            }\n+        }\n+\n+        protected override Task<List<string>> ExpandCoreAsync(string query)\n+        {\n+            if (string.IsNullOrWhiteSpace(query))\n+                return Task.FromResult(new List<string>());\n+\n+            var queryTerms = query.Split(new[] { ' ', '\\t', '\\r', '\\n' }, StringSplitOptions.RemoveEmptyEntries);\n+            var expandedTerms = new Dictionary<string, double>();\n+\n+            foreach (var term in queryTerms)\n+            {\n+                var normalizedTerm = term.ToLowerInvariant();\n+                expandedTerms[normalizedTerm] = 1.0;\n+\n+                if (_termWeights.TryGetValue(normalizedTerm, out var weight))\n+                {\n+                    foreach (var (relatedTerm, relatedWeight) in _termWeights)\n+                    {\n+                        if (relatedTerm != normalizedTerm && relatedWeight > 0.1)\n+                        {\n+                            var combinedWeight = weight * relatedWeight;\n+                            if (!expandedTerms.ContainsKey(relatedTerm))\n+                            {\n+                                expandedTerms[relatedTerm] = combinedWeight;\n+                            }\n+                            else\n+                            {\n+                                expandedTerms[relatedTerm] = Math.Max(expandedTerms[relatedTerm], combinedWeight);\n+                            }\n+                        }\n+                    }\n+                }",
    "path": "src/RetrievalAugmentedGeneration/QueryExpansion/LearnedSparseEncoderExpansion.cs",
    "commit_id": "33a79e89f3137dc80c940c48d7ab74f127dd4d91",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Critical: Expansion logic doesn't align with learned sparse encoder semantics.**\n\nThe nested loop treats every term in `_termWeights` as related to every query term, which is semantically incorrect for learned sparse encoders. For a query term \"cat\" with weight 0.9, this logic would expand it to include \"car\" (if weight > 0.1), even though they're semantically unrelated.\n\nLearned sparse encoders should map each term to its specific related terms, not treat the entire weight dictionary as universally connected. The data structure should represent term-to-related-terms mappings, e.g., `Dictionary<string, Dictionary<string, double>>`.\n\n\n\nConsider refactoring to:\n\n```diff\n-        private readonly Dictionary<string, double> _termWeights;\n+        private readonly Dictionary<string, Dictionary<string, double>> _termExpansions;\n\n-        public void TrainWeights(Dictionary<string, double> termWeights)\n+        public void TrainWeights(Dictionary<string, Dictionary<string, double>> termExpansions)\n         {\n-            if (termWeights == null)\n+            if (termExpansions == null)\n-                throw new ArgumentNullException(nameof(termWeights));\n+                throw new ArgumentNullException(nameof(termExpansions));\n\n-            _termWeights.Clear();\n+            _termExpansions.Clear();\n-            foreach (var (term, weight) in termWeights)\n+            foreach (var (term, relatedTerms) in termExpansions)\n             {\n-                _termWeights[term] = weight;\n+                _termExpansions[term] = relatedTerms;\n             }\n         }\n```\n\nThen update the expansion logic to only iterate through related terms for each query term:\n\n```diff\n-                if (_termWeights.TryGetValue(normalizedTerm, out var weight))\n+                if (_termExpansions.TryGetValue(normalizedTerm, out var relatedTerms))\n                 {\n-                    foreach (var (relatedTerm, relatedWeight) in _termWeights)\n+                    foreach (var (relatedTerm, relatedWeight) in relatedTerms)\n                     {\n-                        if (relatedTerm != normalizedTerm && relatedWeight > 0.1)\n+                        if (relatedWeight > 0.1)\n                         {\n-                            var combinedWeight = weight * relatedWeight;\n+                            var combinedWeight = relatedWeight;\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn\nsrc/RetrievalAugmentedGeneration/QueryExpansion/LearnedSparseEncoderExpansion.cs\naround lines 45 to 62, the expansion loop incorrectly treats _termWeights as a\nglobal flat map and iterates every term against every query term; instead use a\nterm-to-related-terms mapping (e.g., Dictionary<string,\nDictionary<string,double>>) and only iterate the related-term entries for the\nspecific query term. Change the field to a Dictionary<string,\nDictionary<string,double>> (or add a new one), update the TryGetValue to\nretrieve the related-terms dictionary for normalizedTerm, iterate that\ndictionary and apply the >0.1 filter, compute combinedWeight and merge into\nexpandedTerms exactly as before, and update any places that populate\n_termWeights to populate the new term->related map accordingly.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T13:49:52Z",
    "updated_at": "2025-11-04T13:49:57Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490596119",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596119"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490596119"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596119/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 45,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 62,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 62,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596141",
    "pull_request_review_id": 3416572453,
    "id": 2490596141,
    "node_id": "PRRC_kwDOKSXUF86Uc3st",
    "diff_hunk": "@@ -0,0 +1,85 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Threading.Tasks;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.QueryExpansion\n+{\n+    /// <summary>\n+    /// Multi-query expansion generating multiple variations of the query\n+    /// </summary>\n+    public class MultiQueryExpansion : QueryExpansionBase\n+    {\n+        private readonly int _numVariations;\n+        private readonly Dictionary<string, List<string>> _synonymMap;\n+\n+        public MultiQueryExpansion(int numVariations = 3)\n+        {\n+            _numVariations = numVariations;\n+            _synonymMap = new Dictionary<string, List<string>>();\n+        }",
    "path": "src/RetrievalAugmentedGeneration/QueryExpansion/MultiQueryExpansion.cs",
    "commit_id": "33a79e89f3137dc80c940c48d7ab74f127dd4d91",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Add generic type parameter and validate constructor parameter.**\n\nTwo issues:\n\n1. The class inherits from `QueryExpansionBase` without a generic type parameter, but the base class is `QueryExpansionBase<T>` (see relevant snippet from QueryExpansionBase.cs). This PR aims to add generics throughout the RAG framework.\n\n2. The constructor doesn't validate that `numVariations` is positive. A zero or negative value would lead to unexpected behavior in the expansion logic.\n\n\n\nApply this diff to add the generic type parameter and validation:\n\n```diff\n-    public class MultiQueryExpansion : QueryExpansionBase\n+    public class MultiQueryExpansion<T> : QueryExpansionBase<T>\n+        where T : struct, IComparable, IConvertible, IFormattable\n     {\n         private readonly int _numVariations;\n         private readonly Dictionary<string, List<string>> _synonymMap;\n \n         public MultiQueryExpansion(int numVariations = 3)\n         {\n+            if (numVariations <= 0)\n+                throw new ArgumentOutOfRangeException(nameof(numVariations), \"Number of variations must be positive\");\n+\n             _numVariations = numVariations;\n             _synonymMap = new Dictionary<string, List<string>>();\n         }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    public class MultiQueryExpansion<T> : QueryExpansionBase<T>\n        where T : struct, IComparable, IConvertible, IFormattable\n    {\n        private readonly int _numVariations;\n        private readonly Dictionary<string, List<string>> _synonymMap;\n\n        public MultiQueryExpansion(int numVariations = 3)\n        {\n            if (numVariations <= 0)\n                throw new ArgumentOutOfRangeException(nameof(numVariations), \"Number of variations must be positive\");\n\n            _numVariations = numVariations;\n            _synonymMap = new Dictionary<string, List<string>>();\n        }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T13:49:52Z",
    "updated_at": "2025-11-04T13:49:57Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490596141",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596141"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490596141"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596141/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 11,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 20,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 20,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596160",
    "pull_request_review_id": 3416572453,
    "id": 2490596160,
    "node_id": "PRRC_kwDOKSXUF86Uc3tA",
    "diff_hunk": "@@ -0,0 +1,85 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Threading.Tasks;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.QueryExpansion\n+{\n+    /// <summary>\n+    /// Multi-query expansion generating multiple variations of the query\n+    /// </summary>\n+    public class MultiQueryExpansion : QueryExpansionBase\n+    {\n+        private readonly int _numVariations;\n+        private readonly Dictionary<string, List<string>> _synonymMap;\n+\n+        public MultiQueryExpansion(int numVariations = 3)\n+        {\n+            _numVariations = numVariations;\n+            _synonymMap = new Dictionary<string, List<string>>();\n+        }\n+\n+        public void AddSynonyms(string term, List<string> synonyms)\n+        {\n+            if (string.IsNullOrEmpty(term))\n+                throw new ArgumentException(\"Term cannot be null or empty\", nameof(term));\n+            if (synonyms == null)\n+                throw new ArgumentNullException(nameof(synonyms));\n+\n+            _synonymMap[term.ToLowerInvariant()] = synonyms;\n+        }",
    "path": "src/RetrievalAugmentedGeneration/QueryExpansion/MultiQueryExpansion.cs",
    "commit_id": "33a79e89f3137dc80c940c48d7ab74f127dd4d91",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Use consistent string validation.**\n\nLine 24 uses `string.IsNullOrEmpty`, but line 34 in `ExpandCoreAsync` uses `string.IsNullOrWhiteSpace`. For consistency and robustness, prefer `IsNullOrWhiteSpace` throughout to handle whitespace-only strings.\n\n\n\nApply this diff:\n\n```diff\n         public void AddSynonyms(string term, List<string> synonyms)\n         {\n-            if (string.IsNullOrEmpty(term))\n+            if (string.IsNullOrWhiteSpace(term))\n                 throw new ArgumentException(\"Term cannot be null or empty\", nameof(term));\n             if (synonyms == null)\n                 throw new ArgumentNullException(nameof(synonyms));\n \n             _synonymMap[term.ToLowerInvariant()] = synonyms;\n         }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\npublic void AddSynonyms(string term, List<string> synonyms)\n{\n    if (string.IsNullOrWhiteSpace(term))\n        throw new ArgumentException(\"Term cannot be null or empty\", nameof(term));\n    if (synonyms == null)\n        throw new ArgumentNullException(nameof(synonyms));\n\n    _synonymMap[term.ToLowerInvariant()] = synonyms;\n}\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/QueryExpansion/MultiQueryExpansion.cs around\nlines 22 to 30, the AddSynonyms method uses string.IsNullOrEmpty to validate the\nterm while other code uses string.IsNullOrWhiteSpace; update the validation to\nuse string.IsNullOrWhiteSpace(term) instead of IsNullOrEmpty to reject\nwhitespace-only terms and keep validation consistent across the class.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T13:49:52Z",
    "updated_at": "2025-11-04T13:49:57Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490596160",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596160"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490596160"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596160/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 22,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 30,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 30,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596179",
    "pull_request_review_id": 3416572453,
    "id": 2490596179,
    "node_id": "PRRC_kwDOKSXUF86Uc3tT",
    "diff_hunk": "@@ -0,0 +1,85 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Threading.Tasks;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.QueryExpansion\n+{\n+    /// <summary>\n+    /// Multi-query expansion generating multiple variations of the query\n+    /// </summary>\n+    public class MultiQueryExpansion : QueryExpansionBase\n+    {\n+        private readonly int _numVariations;\n+        private readonly Dictionary<string, List<string>> _synonymMap;\n+\n+        public MultiQueryExpansion(int numVariations = 3)\n+        {\n+            _numVariations = numVariations;\n+            _synonymMap = new Dictionary<string, List<string>>();\n+        }\n+\n+        public void AddSynonyms(string term, List<string> synonyms)\n+        {\n+            if (string.IsNullOrEmpty(term))\n+                throw new ArgumentException(\"Term cannot be null or empty\", nameof(term));\n+            if (synonyms == null)\n+                throw new ArgumentNullException(nameof(synonyms));\n+\n+            _synonymMap[term.ToLowerInvariant()] = synonyms;\n+        }\n+\n+        protected override Task<List<string>> ExpandCoreAsync(string query)\n+        {\n+            if (string.IsNullOrWhiteSpace(query))\n+                return Task.FromResult(new List<string>());\n+\n+            var variations = new List<string> { query };\n+            var queryTerms = query.Split(new[] { ' ', '\\t', '\\r', '\\n' }, StringSplitOptions.RemoveEmptyEntries);\n+\n+            for (int i = 0; i < _numVariations - 1; i++)\n+            {\n+                var variation = new List<string>();\n+                foreach (var term in queryTerms)\n+                {\n+                    var normalizedTerm = term.ToLowerInvariant();\n+                    if (_synonymMap.TryGetValue(normalizedTerm, out var synonyms) && synonyms.Count > 0)\n+                    {\n+                        var synonymIndex = i % synonyms.Count;\n+                        variation.Add(synonyms[synonymIndex]);\n+                    }\n+                    else\n+                    {\n+                        variation.Add(term);\n+                    }\n+                }\n+                variations.Add(string.Join(\" \", variation));\n+            }\n+\n+            variations.Add(GenerateReorderedQuery(queryTerms));\n+            variations.Add(GenerateExpandedQuery(queryTerms));\n+\n+            return Task.FromResult(variations.Distinct().ToList());\n+        }",
    "path": "src/RetrievalAugmentedGeneration/QueryExpansion/MultiQueryExpansion.cs",
    "commit_id": "33a79e89f3137dc80c940c48d7ab74f127dd4d91",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**The actual number of variations exceeds the `_numVariations` parameter.**\n\nThe method generates more variations than `_numVariations` specifies:\n- 1 original query (line 37)\n- `_numVariations - 1` from the loop (lines 40-57)\n- 2 additional variations unconditionally (lines 59-60: reordered + expanded)\n\nIf `_numVariations = 3`, this produces 5 variations before deduplication, which may confuse users expecting exactly 3 variations.\n\n\n\nConsider limiting the result to respect the parameter:\n\n```diff\n         protected override Task<List<string>> ExpandCoreAsync(string query)\n         {\n             if (string.IsNullOrWhiteSpace(query))\n                 return Task.FromResult(new List<string>());\n \n             var variations = new List<string> { query };\n             var queryTerms = query.Split(new[] { ' ', '\\t', '\\r', '\\n' }, StringSplitOptions.RemoveEmptyEntries);\n \n             for (int i = 0; i < _numVariations - 1; i++)\n             {\n                 var variation = new List<string>();\n                 foreach (var term in queryTerms)\n                 {\n                     var normalizedTerm = term.ToLowerInvariant();\n                     if (_synonymMap.TryGetValue(normalizedTerm, out var synonyms) && synonyms.Count > 0)\n                     {\n                         var synonymIndex = i % synonyms.Count;\n                         variation.Add(synonyms[synonymIndex]);\n                     }\n                     else\n                     {\n                         variation.Add(term);\n                     }\n                 }\n                 variations.Add(string.Join(\" \", variation));\n             }\n \n-            variations.Add(GenerateReorderedQuery(queryTerms));\n-            variations.Add(GenerateExpandedQuery(queryTerms));\n+            // Add reordered/expanded variations only if we haven't reached the limit\n+            if (variations.Count < _numVariations)\n+                variations.Add(GenerateReorderedQuery(queryTerms));\n+            if (variations.Count < _numVariations)\n+                variations.Add(GenerateExpandedQuery(queryTerms));\n \n-            return Task.FromResult(variations.Distinct().ToList());\n+            return Task.FromResult(variations.Distinct().Take(_numVariations).ToList());\n         }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        protected override Task<List<string>> ExpandCoreAsync(string query)\n        {\n            if (string.IsNullOrWhiteSpace(query))\n                return Task.FromResult(new List<string>());\n\n            var variations = new List<string> { query };\n            var queryTerms = query.Split(new[] { ' ', '\\t', '\\r', '\\n' }, StringSplitOptions.RemoveEmptyEntries);\n\n            for (int i = 0; i < _numVariations - 1; i++)\n            {\n                var variation = new List<string>();\n                foreach (var term in queryTerms)\n                {\n                    var normalizedTerm = term.ToLowerInvariant();\n                    if (_synonymMap.TryGetValue(normalizedTerm, out var synonyms) && synonyms.Count > 0)\n                    {\n                        var synonymIndex = i % synonyms.Count;\n                        variation.Add(synonyms[synonymIndex]);\n                    }\n                    else\n                    {\n                        variation.Add(term);\n                    }\n                }\n                variations.Add(string.Join(\" \", variation));\n            }\n\n            // Add reordered/expanded variations only if we haven't reached the limit\n            if (variations.Count < _numVariations)\n                variations.Add(GenerateReorderedQuery(queryTerms));\n            if (variations.Count < _numVariations)\n                variations.Add(GenerateExpandedQuery(queryTerms));\n\n            return Task.FromResult(variations.Distinct().Take(_numVariations).ToList());\n        }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T13:49:53Z",
    "updated_at": "2025-11-04T13:49:57Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490596179",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596179"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490596179"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596179/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 32,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 63,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 63,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596199",
    "pull_request_review_id": 3416572453,
    "id": 2490596199,
    "node_id": "PRRC_kwDOKSXUF86Uc3tn",
    "diff_hunk": "@@ -0,0 +1,80 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Threading.Tasks;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.QueryExpansion\n+{\n+    /// <summary>\n+    /// Sub-query expansion that breaks complex queries into simpler sub-queries\n+    /// </summary>\n+    public class SubQueryExpansion : QueryExpansionBase\n+    {\n+        private readonly int _maxSubQueries;\n+\n+        public SubQueryExpansion(int maxSubQueries = 3)\n+        {\n+            _maxSubQueries = maxSubQueries;\n+        }",
    "path": "src/RetrievalAugmentedGeneration/QueryExpansion/SubQueryExpansion.cs",
    "commit_id": "33a79e89f3137dc80c940c48d7ab74f127dd4d91",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Add validation for the maxSubQueries parameter.**\n\nThe constructor does not validate that `maxSubQueries` is positive. Negative or zero values would cause unexpected behavior in the expansion logic, particularly in `Take(_maxSubQueries)` calls throughout the method.\n\n\n\nApply this diff to add validation:\n\n```diff\n public SubQueryExpansion(int maxSubQueries = 3)\n {\n+    if (maxSubQueries <= 0)\n+        throw new ArgumentOutOfRangeException(nameof(maxSubQueries), \"Must be greater than zero\");\n     _maxSubQueries = maxSubQueries;\n }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        public SubQueryExpansion(int maxSubQueries = 3)\n        {\n            if (maxSubQueries <= 0)\n                throw new ArgumentOutOfRangeException(nameof(maxSubQueries), \"Must be greater than zero\");\n            _maxSubQueries = maxSubQueries;\n        }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/QueryExpansion/SubQueryExpansion.cs around\nlines 15 to 18, the constructor does not validate maxSubQueries which can be\nzero or negative and break Take(_maxSubQueries) calls; add validation to ensure\nmaxSubQueries is greater than zero and throw an ArgumentOutOfRangeException (use\nnameof(maxSubQueries)) if the value is less than or equal to zero, otherwise\nassign the field as before.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commit 5e2879c",
    "created_at": "2025-11-04T13:49:53Z",
    "updated_at": "2025-11-04T14:12:15Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490596199",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596199"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490596199"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596199/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 15,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 18,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 18,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596219",
    "pull_request_review_id": 3416572453,
    "id": 2490596219,
    "node_id": "PRRC_kwDOKSXUF86Uc3t7",
    "diff_hunk": "@@ -0,0 +1,74 @@\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Threading.Tasks;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.RerankingStrategies\n+{\n+    /// <summary>\n+    /// Diversity-based reranking to maximize coverage\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for scoring</typeparam>\n+    public class DiversityReranker<T> : RerankingStrategyBase<T> where T : struct, IComparable, IComparable<T>, IConvertible, IEquatable<T>, IFormattable\n+    {\n+        private readonly T _diversityWeight;\n+\n+        public DiversityReranker(T diversityWeight = default)\n+        {\n+            _diversityWeight = NumOps.Equals(diversityWeight, NumOps.Zero)\n+                ? NumOps.FromDouble(0.5)\n+                : diversityWeight;\n+        }",
    "path": "src/RetrievalAugmentedGeneration/RerankingStrategies/DiversityReranker.cs",
    "commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Validate diversityWeight range.**\n\nThe constructor accepts any value for `diversityWeight` but diversity-based reranking algorithms typically expect weights in [0,1] to properly balance diversity and relevance. Values outside this range can produce unexpected or incorrect scoring behavior.\n\n\n\nApply this diff to add validation:\n\n```diff\n public DiversityReranker(T diversityWeight = default)\n {\n+    if (!NumOps.Equals(diversityWeight, NumOps.Zero))\n+    {\n+        var zero = NumOps.Zero;\n+        var one = NumOps.One;\n+        if (NumOps.LessThan(diversityWeight, zero) || NumOps.GreaterThan(diversityWeight, one))\n+            throw new ArgumentOutOfRangeException(nameof(diversityWeight), \"Diversity weight must be between 0 and 1.\");\n+    }\n+\n     _diversityWeight = NumOps.Equals(diversityWeight, NumOps.Zero)\n         ? NumOps.FromDouble(0.5)\n         : diversityWeight;\n }\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/RerankingStrategies/DiversityReranker.cs\naround lines 18 to 23, the constructor currently accepts any T for\ndiversityWeight; add validation to ensure the value falls within [0,1] (use\nNumOps.Zero and NumOps.One for comparisons) and throw an\nArgumentOutOfRangeException with a clear message if itΓÇÖs outside that range;\nkeep the existing default of 0.5 when a zero/unspecified value is provided.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T13:49:53Z",
    "updated_at": "2025-11-04T13:49:57Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490596219",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596219"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490596219"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596219/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 18,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 23,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 23,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596239",
    "pull_request_review_id": 3416572453,
    "id": 2490596239,
    "node_id": "PRRC_kwDOKSXUF86Uc3uP",
    "diff_hunk": "@@ -0,0 +1,74 @@\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Threading.Tasks;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.RerankingStrategies\n+{\n+    /// <summary>\n+    /// Diversity-based reranking to maximize coverage\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for scoring</typeparam>\n+    public class DiversityReranker<T> : RerankingStrategyBase<T> where T : struct, IComparable, IComparable<T>, IConvertible, IEquatable<T>, IFormattable\n+    {\n+        private readonly T _diversityWeight;\n+\n+        public DiversityReranker(T diversityWeight = default)\n+        {\n+            _diversityWeight = NumOps.Equals(diversityWeight, NumOps.Zero)\n+                ? NumOps.FromDouble(0.5)\n+                : diversityWeight;\n+        }\n+\n+        protected override Task<List<Document<T>>> RerankCoreAsync(string query, List<Document<T>> documents, int topK)\n+        {\n+            if (documents == null || documents.Count == 0)\n+                return Task.FromResult(new List<Document<T>>());\n+\n+            var selected = new List<Document<T>>();\n+            var remaining = new List<Document<T>>(documents);\n+\n+            selected.Add(remaining[0]);\n+            remaining.RemoveAt(0);\n+\n+            while (selected.Count < topK && remaining.Count > 0)\n+            {\n+                var bestDoc = remaining[0];\n+                var bestScore = NumOps.FromDouble(-1.0);\n+\n+                foreach (var doc in remaining)\n+                {\n+                    var minSimilarity = NumOps.One;\n+                    foreach (var selectedDoc in selected)\n+                    {\n+                        var similarity = StatisticsHelper.CosineSimilarity(\n+                            doc.Embedding,\n+                            selectedDoc.Embedding,\n+                            NumOps);",
    "path": "src/RetrievalAugmentedGeneration/RerankingStrategies/DiversityReranker.cs",
    "commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Add null checks for document embeddings.**\n\nThe code assumes all documents have embeddings, but `doc.Embedding` and `selectedDoc.Embedding` can be null, leading to a `NullReferenceException` in `StatisticsHelper.CosineSimilarity`.\n\n\n\nApply this diff to add validation:\n\n```diff\n+            // Validate all documents have embeddings\n+            if (remaining.Any(d => d.Embedding == null))\n+                throw new InvalidOperationException(\"All documents must have embeddings for diversity-based reranking.\");\n+\n             while (selected.Count < topK && remaining.Count > 0)\n             {\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n            // Validate all documents have embeddings\n            if (remaining.Any(d => d.Embedding == null))\n                throw new InvalidOperationException(\"All documents must have embeddings for diversity-based reranking.\");\n\n            while (selected.Count < topK && remaining.Count > 0)\n            {\n                Document<T>? bestDoc = null;\n                var hasBestScore = false;\n                T bestScore = default!;\n\n                foreach (var doc in remaining)\n                {\n                    // Relevance score\n                    var relevance = doc.HasRelevanceScore ? doc.RelevanceScore : NumOps.Zero;\n\n                    // Diversity penalty\n                    var diversityPenalty = NumOps.Zero;\n\n                    foreach (var selectedDoc in selected)\n                    {\n                        var similarity = StatisticsHelper.CosineSimilarity(\n                            doc.Embedding,\n                            selectedDoc.Embedding,\n                            NumOps);\n                        // ... rest of diversity calculation ...\n                    }\n\n                    // ... rest of scoring logic ...\n                }\n\n                if (!hasBestScore || bestDoc is null)\n                    break;\n\n                selected.Add(bestDoc);\n                remaining.Remove(bestDoc);\n            }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/RerankingStrategies/DiversityReranker.cs\naround lines 46-49, the call to StatisticsHelper.CosineSimilarity assumes\ndoc.Embedding and selectedDoc.Embedding are non-null; add null checks before\ncalling CosineSimilarity and avoid passing nulls. If either embedding is null,\nskip similarity computation for that pair (or treat similarity as a safe default\nsuch as 0 or -1), and optionally log or count the skipped document; ensure the\nrest of the loop continues without calling CosineSimilarity with null arguments.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits e0f1f5e to 5c09cd6",
    "created_at": "2025-11-04T13:49:53Z",
    "updated_at": "2025-11-04T15:00:34Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490596239",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596239"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490596239"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596239/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 46,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 49,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 49,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596249",
    "pull_request_review_id": 3416572453,
    "id": 2490596249,
    "node_id": "PRRC_kwDOKSXUF86Uc3uZ",
    "diff_hunk": "@@ -0,0 +1,74 @@\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Threading.Tasks;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.RerankingStrategies\n+{\n+    /// <summary>\n+    /// Diversity-based reranking to maximize coverage\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for scoring</typeparam>\n+    public class DiversityReranker<T> : RerankingStrategyBase<T> where T : struct, IComparable, IComparable<T>, IConvertible, IEquatable<T>, IFormattable\n+    {\n+        private readonly T _diversityWeight;\n+\n+        public DiversityReranker(T diversityWeight = default)\n+        {\n+            _diversityWeight = NumOps.Equals(diversityWeight, NumOps.Zero)\n+                ? NumOps.FromDouble(0.5)\n+                : diversityWeight;\n+        }\n+\n+        protected override Task<List<Document<T>>> RerankCoreAsync(string query, List<Document<T>> documents, int topK)\n+        {\n+            if (documents == null || documents.Count == 0)\n+                return Task.FromResult(new List<Document<T>>());\n+\n+            var selected = new List<Document<T>>();\n+            var remaining = new List<Document<T>>(documents);\n+\n+            selected.Add(remaining[0]);\n+            remaining.RemoveAt(0);\n+\n+            while (selected.Count < topK && remaining.Count > 0)\n+            {\n+                var bestDoc = remaining[0];\n+                var bestScore = NumOps.FromDouble(-1.0);\n+\n+                foreach (var doc in remaining)\n+                {\n+                    var minSimilarity = NumOps.One;\n+                    foreach (var selectedDoc in selected)\n+                    {\n+                        var similarity = StatisticsHelper.CosineSimilarity(\n+                            doc.Embedding,\n+                            selectedDoc.Embedding,\n+                            NumOps);\n+\n+                        if (NumOps.LessThan(similarity, minSimilarity))\n+                        {\n+                            minSimilarity = similarity;\n+                        }\n+                    }\n+\n+                    var diversityScore = NumOps.Subtract(NumOps.One, minSimilarity);\n+                    var combinedScore = NumOps.Multiply(diversityScore, _diversityWeight);",
    "path": "src/RetrievalAugmentedGeneration/RerankingStrategies/DiversityReranker.cs",
    "commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n<details>\n<summary>≡ƒº⌐ Analysis chain</summary>\n\n**Verify the absence of relevance scoring.**\n\nStandard diversity reranking algorithms (e.g., Maximal Marginal Relevance) typically balance relevance and diversity using a formula like `╬╗ ├ù relevance ΓêÆ (1ΓêÆ╬╗) ├ù similarity`. The current implementation only considers diversity (`diversityScore ├ù diversityWeight`) without incorporating document relevance scores.\n\nIs this intentional? If relevance should be considered, you'll need to incorporate `doc.RelevanceScore` (checking `doc.HasRelevanceScore` first) into the combined score calculation.\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Search for other diversity-based reranking patterns in the codebase\nrg -n -C3 --type=cs \"RelevanceScore.*diversity|diversity.*RelevanceScore|Maximal.*Marginal\"\n```\n\nLength of output: 5294\n\n---\n\n```\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Get the full implementation of DiversityReranker\ncat src/RetrievalAugmentedGeneration/Rerankers/DiversityReranker.cs\n```\n\nLength of output: 8612\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Verify the file path mentioned in review\ncat src/RetrievalAugmentedGeneration/RerankingStrategies/DiversityReranker.cs 2>/dev/null || echo \"File not found at RerankingStrategies path\"\n```\n\nLength of output: 2646\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Search for all DiversityReranker references\nrg -n --type=cs \"DiversityReranker\" -A2 -B2\n```\n\nLength of output: 2602\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check if RelevanceScore is used anywhere in diversity reranker context\nrg -n --type=cs \"RelevanceScore\" -C5 | head -50\n```\n\nLength of output: 5276\n\n---\n\n```\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check if the old DiversityReranker (RerankingStrategies namespace) is actually used\nrg -n --type=cs \"RerankingStrategies.*DiversityReranker|using.*RerankingStrategies\" -B2 -A2\n```\n\nLength of output: 0\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check which version is in actual use - search for instantiation patterns\nrg -n --type=cs \"new DiversityReranker|DiversityReranker<\" -B3 -A3 | head -40\n```\n\nLength of output: 1534\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check the namespace declarations in the new/old files\ngrep -n \"namespace\" src/RetrievalAugmentedGeneration/Rerankers/DiversityReranker.cs src/RetrievalAugmentedGeneration/RerankingStrategies/DiversityReranker.cs\n```\n\nLength of output: 268\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Look for any deprecation attributes or comments on the old version\nhead -20 src/RetrievalAugmentedGeneration/RerankingStrategies/DiversityReranker.cs\n```\n\nLength of output: 763\n\n---\n\n```\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check if there are any tests or examples for the old RerankingStrategies\nrg -n --type=cs \"RerankingStrategy|RerankingStrategyBase\" | head -30\n```\n\nLength of output: 1266\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Look for any documentation or README mentioning diversity reranking\nfind . -name \"*.md\" -o -name \"*.txt\" | xargs grep -l \"DiversityReranker\\|diversity.*rerank\" 2>/dev/null | head -10\n```\n\nLength of output: 0\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check git history/status to see if the old file is actively maintained\nls -la src/RetrievalAugmentedGeneration/RerankingStrategies/ 2>/dev/null | head -20\n```\n\nLength of output: 622\n\n---\n\n\n\n</details>\n\n**Incorporate relevance scoring into the diversity calculation.**\n\nThe algorithm calculates `diversityScore ├ù _diversityWeight` but ignores `doc.RelevanceScore` entirely. Standard diversity-aware reranking balances both factorsΓÇöconsider using the formula: `_lambda ├ù doc.RelevanceScore ΓêÆ (1 ΓêÆ _lambda) ├ù maxSimilarity` (similar to Maximal Marginal Relevance). This ensures results are both diverse and relevant to the query.\n\nAlternatively, verify this is intentionally a diversity-only reranker, and if so, document that design choice explicitly in the class remarks.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/RerankingStrategies/DiversityReranker.cs\naround lines 57 to 58, the current calculation builds combinedScore as\ndiversityScore ├ù _diversityWeight but ignores doc.RelevanceScore; change the\nscoring to combine relevance and diversity (e.g., combinedScore = _lambda *\ndoc.RelevanceScore - (1 - _lambda) * maxSimilarity as per MMR) or, if the intent\nis to be diversity-only, add a clear XML doc comment on the class/method\nexplaining that relevance is intentionally omitted and why; ensure\n_lambda/_diversityWeight naming and normalization are consistent and unit-tested\nafter the change.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits e0f1f5e to 2e85607",
    "created_at": "2025-11-04T13:49:53Z",
    "updated_at": "2025-11-04T15:38:48Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490596249",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596249"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490596249"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596249/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 57,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 58,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 58,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596260",
    "pull_request_review_id": 3416572453,
    "id": 2490596260,
    "node_id": "PRRC_kwDOKSXUF86Uc3uk",
    "diff_hunk": "@@ -0,0 +1,108 @@\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Threading.Tasks;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\n+{\n+    /// <summary>\n+    /// Graph-based retriever for traversing document relationships\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations</typeparam>\n+    public class GraphRetriever<T> : RetrieverBase<T> where T : struct, IComparable, IComparable<T>, IConvertible, IEquatable<T>, IFormattable\n+    {\n+        private readonly IEmbeddingModel<T> _embeddingModel;\n+        private readonly IDocumentStore<T> _documentStore;\n+        private readonly Dictionary<string, List<string>> _graph;\n+        private readonly int _maxDepth;\n+\n+        public GraphRetriever(\n+            IEmbeddingModel<T> embeddingModel,\n+            IDocumentStore<T> documentStore,\n+            int maxDepth = 2)\n+        {\n+            _embeddingModel = embeddingModel ?? throw new ArgumentNullException(nameof(embeddingModel));\n+            _documentStore = documentStore ?? throw new ArgumentNullException(nameof(documentStore));\n+            _graph = new Dictionary<string, List<string>>();\n+            _maxDepth = maxDepth;\n+        }\n+\n+        public void AddEdge(string fromDocId, string toDocId)\n+        {\n+            if (!_graph.ContainsKey(fromDocId))\n+            {\n+                _graph[fromDocId] = new List<string>();\n+            }\n+            if (!_graph[fromDocId].Contains(toDocId))\n+            {\n+                _graph[fromDocId].Add(toDocId);\n+            }\n+        }\n+\n+        protected override async Task<List<Document<T>>> RetrieveCoreAsync(string query, int topK = 5)\n+        {\n+            var queryEmbedding = await _embeddingModel.GenerateEmbeddingAsync(query);\n+            var initialDocs = await _documentStore.SearchAsync(queryEmbedding, Math.Max(topK / 2, 1));\n+\n+            var visited = new HashSet<string>();\n+            var results = new Dictionary<string, (Document<T> doc, T score)>();\n+\n+            foreach (var doc in initialDocs)\n+            {\n+                var score = StatisticsHelper.CosineSimilarity(\n+                    queryEmbedding,\n+                    doc.Embedding,\n+                    NumOps);\n+                results[doc.Id] = (doc, score);\n+                visited.Add(doc.Id);\n+            }\n+\n+            foreach (var initialDoc in initialDocs)\n+            {\n+                await TraverseGraph(initialDoc.Id, queryEmbedding, 0, visited, results);\n+            }\n+\n+            return results.Values\n+                .OrderByDescending(x => x.score)\n+                .Take(topK)\n+                .Select(x => x.doc)\n+                .ToList();\n+        }\n+\n+        private async Task TraverseGraph(\n+            string docId,\n+            Vector<T> queryEmbedding,\n+            int depth,\n+            HashSet<string> visited,\n+            Dictionary<string, (Document<T> doc, T score)> results)\n+        {\n+            if (depth >= _maxDepth || !_graph.ContainsKey(docId))\n+                return;\n+\n+            foreach (var neighborId in _graph[docId])\n+            {\n+                if (visited.Contains(neighborId))\n+                    continue;\n+\n+                visited.Add(neighborId);\n+                var neighbor = await _documentStore.GetDocumentAsync(neighborId);\n+                \n+                if (neighbor != null)\n+                {\n+                    var score = StatisticsHelper.CosineSimilarity(\n+                        queryEmbedding,\n+                        neighbor.Embedding,\n+                        NumOps);\n+\n+                    var decayFactor = NumOps.FromDouble(Math.Pow(0.8, depth + 1));\n+                    var decayedScore = NumOps.Multiply(score, decayFactor);\n+\n+                    results[neighborId] = (neighbor, decayedScore);\n+                    await TraverseGraph(neighborId, queryEmbedding, depth + 1, visited, results);\n+                }",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/GraphRetriever.cs",
    "commit_id": "33a79e89f3137dc80c940c48d7ab74f127dd4d91",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Guard against null embeddings before cosine similarity.**\n\nSeveral document stores in this codebase (e.g., `ChromaDBDocumentStore`, `AzureSearchDocumentStore`) return `Document<T>` instances without populating `Embedding`. Here we pass those `null` embeddings straight into `StatisticsHelper.CosineSimilarity`, which will throw at runtime. Make the similarity calculation null-safeΓÇöe.g., treat missing embeddings as zero-score while still traversing neighborsΓÇöso graph retrieval doesnΓÇÖt crash on stores that omit vector payloads.  \n\n\n```diff\n-            foreach (var doc in initialDocs)\n-            {\n-                var score = StatisticsHelper.CosineSimilarity(\n-                    queryEmbedding,\n-                    doc.Embedding,\n-                    NumOps);\n-                results[doc.Id] = (doc, score);\n-                visited.Add(doc.Id);\n-            }\n+            foreach (var doc in initialDocs)\n+            {\n+                var score = NumOps.Zero;\n+                if (doc.Embedding is not null)\n+                {\n+                    score = StatisticsHelper.CosineSimilarity(\n+                        queryEmbedding,\n+                        doc.Embedding,\n+                        NumOps);\n+                }\n+\n+                results[doc.Id] = (doc, score);\n+                visited.Add(doc.Id);\n+            }\n@@\n-                if (neighbor != null)\n-                {\n-                    var score = StatisticsHelper.CosineSimilarity(\n-                        queryEmbedding,\n-                        neighbor.Embedding,\n-                        NumOps);\n-\n-                    var decayFactor = NumOps.FromDouble(Math.Pow(0.8, depth + 1));\n-                    var decayedScore = NumOps.Multiply(score, decayFactor);\n-\n-                    results[neighborId] = (neighbor, decayedScore);\n-                    await TraverseGraph(neighborId, queryEmbedding, depth + 1, visited, results);\n-                }\n+                if (neighbor != null)\n+                {\n+                    var score = NumOps.Zero;\n+                    if (neighbor.Embedding is not null)\n+                    {\n+                        score = StatisticsHelper.CosineSimilarity(\n+                            queryEmbedding,\n+                            neighbor.Embedding,\n+                            NumOps);\n+                    }\n+\n+                    var decayFactor = NumOps.FromDouble(Math.Pow(0.8, depth + 1));\n+                    var decayedScore = NumOps.Multiply(score, decayFactor);\n+\n+                    results[neighborId] = (neighbor, decayedScore);\n+                    await TraverseGraph(neighborId, queryEmbedding, depth + 1, visited, results);\n+                }\n```\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T13:49:54Z",
    "updated_at": "2025-11-04T13:49:57Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490596260",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596260"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490596260"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596260/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 52,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 104,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 104,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596278",
    "pull_request_review_id": 3416572453,
    "id": 2490596278,
    "node_id": "PRRC_kwDOKSXUF86Uc3u2",
    "diff_hunk": "@@ -0,0 +1,78 @@\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Threading.Tasks;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\n+{\n+    /// <summary>\n+    /// Multi-vector retriever that generates multiple embeddings per document\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations</typeparam>\n+    public class MultiVectorRetriever<T> : RetrieverBase<T> where T : struct, IComparable, IComparable<T>, IConvertible, IEquatable<T>, IFormattable\n+    {\n+        private readonly IEmbeddingModel<T> _embeddingModel;\n+        private readonly IDocumentStore<T> _documentStore;\n+        private readonly Dictionary<string, List<Vector<T>>> _documentVectors;\n+\n+        public MultiVectorRetriever(IEmbeddingModel<T> embeddingModel, IDocumentStore<T> documentStore)\n+        {\n+            _embeddingModel = embeddingModel ?? throw new ArgumentNullException(nameof(embeddingModel));\n+            _documentStore = documentStore ?? throw new ArgumentNullException(nameof(documentStore));\n+            _documentVectors = new Dictionary<string, List<Vector<T>>>();\n+        }\n+\n+        public async Task IndexDocumentAsync(Document<T> document)\n+        {\n+            if (document == null)\n+                throw new ArgumentNullException(nameof(document));\n+\n+            var sentences = document.Content.Split(new[] { '.', '!', '?' }, StringSplitOptions.RemoveEmptyEntries);\n+            var vectors = new List<Vector<T>>();\n+\n+            foreach (var sentence in sentences.Where(s => !string.IsNullOrWhiteSpace(s)))\n+            {\n+                var embedding = await _embeddingModel.GenerateEmbeddingAsync(sentence.Trim());\n+                vectors.Add(embedding);\n+            }\n+\n+            _documentVectors[document.Id] = vectors;\n+            await _documentStore.AddDocumentAsync(document);\n+        }\n+\n+        protected override async Task<List<Document<T>>> RetrieveCoreAsync(string query, int topK = 5)\n+        {\n+            var queryEmbedding = await _embeddingModel.GenerateEmbeddingAsync(query);\n+            var scoredDocuments = new List<(Document<T> doc, T score)>();\n+\n+            foreach (var (docId, vectors) in _documentVectors)\n+            {\n+                var maxScore = NumOps.FromDouble(-1.0);\n+\n+                foreach (var vector in vectors)\n+                {\n+                    var score = StatisticsHelper.CosineSimilarity(queryEmbedding, vector, NumOps);\n+                    if (NumOps.GreaterThan(score, maxScore))\n+                    {\n+                        maxScore = score;\n+                    }\n+                }\n+\n+                var document = await _documentStore.GetDocumentAsync(docId);\n+                if (document != null)\n+                {\n+                    scoredDocuments.Add((document, maxScore));\n+                }\n+            }\n+\n+            return scoredDocuments\n+                .OrderByDescending(x => x.score)\n+                .Take(topK)\n+                .Select(x => x.doc)",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/MultiVectorRetriever.cs",
    "commit_id": "33a79e89f3137dc80c940c48d7ab74f127dd4d91",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Guard _documentVectors against concurrent read/write**\n\n`IndexDocumentAsync` mutates `_documentVectors` while `RetrieveCoreAsync` enumerates it. Because `Dictionary` isnΓÇÖt thread-safe, concurrent indexing and retrieval will eventually throw `InvalidOperationException` (ΓÇ£Collection was modifiedΓÇ¥). Please protect access (e.g., switch to `ConcurrentDictionary<string, List<Vector<T>>>` or wrap reads/writes in a lock) so indexing and querying can run in parallel safely.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Retrievers/MultiVectorRetriever.cs lines\n18ΓÇô73, _documentVectors is a plain Dictionary that gets mutated in\nIndexDocumentAsync while being enumerated in RetrieveCoreAsync; make access\nthread-safe by either replacing Dictionary<string, List<Vector<T>>> with\nConcurrentDictionary<string, List<Vector<T>>> (add using\nSystem.Collections.Concurrent, use GetOrAdd/[] assignment and iterate over\n.ToArray() or .Values for a safe snapshot) or introduce a private readonly\nobject _docVectorsLock and wrap all reads/writes (assignments, enumerations) to\n_documentVectors in lock(_docVectorsLock) blocks so concurrent indexing and\nretrieval cannot cause a CollectionModified exception.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T13:49:54Z",
    "updated_at": "2025-11-04T13:49:57Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490596278",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596278"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490596278"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596278/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 18,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 73,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 73,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596294",
    "pull_request_review_id": 3416572453,
    "id": 2490596294,
    "node_id": "PRRC_kwDOKSXUF86Uc3vG",
    "diff_hunk": "@@ -0,0 +1,70 @@\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Threading.Tasks;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\n+{\n+    /// <summary>\n+    /// Parent document retriever that retrieves full documents from chunk matches\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations</typeparam>\n+    public class ParentDocumentRetriever<T> : RetrieverBase<T> where T : struct, IComparable, IComparable<T>, IConvertible, IEquatable<T>, IFormattable\n+    {\n+        private readonly IEmbeddingModel<T> _embeddingModel;\n+        private readonly IDocumentStore<T> _chunkStore;\n+        private readonly IDocumentStore<T> _parentStore;\n+\n+        public ParentDocumentRetriever(\n+            IEmbeddingModel<T> embeddingModel,\n+            IDocumentStore<T> chunkStore,\n+            IDocumentStore<T> parentStore)\n+        {\n+            _embeddingModel = embeddingModel ?? throw new ArgumentNullException(nameof(embeddingModel));\n+            _chunkStore = chunkStore ?? throw new ArgumentNullException(nameof(chunkStore));\n+            _parentStore = parentStore ?? throw new ArgumentNullException(nameof(parentStore));\n+        }\n+\n+        protected override async Task<List<Document<T>>> RetrieveCoreAsync(string query, int topK = 5)\n+        {\n+            var queryEmbedding = await _embeddingModel.GenerateEmbeddingAsync(query);\n+            \n+            var relevantChunks = await _chunkStore.SearchAsync(queryEmbedding, topK * 2);\n+\n+            var parentIds = new HashSet<string>();\n+            foreach (var chunk in relevantChunks)\n+            {\n+                if (chunk.Metadata.TryGetValue(\"parent_id\", out var parentId))\n+                {\n+                    parentIds.Add(parentId);\n+                }\n+            }",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/ParentDocumentRetriever.cs",
    "commit_id": "33a79e89f3137dc80c940c48d7ab74f127dd4d91",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Compile break: metadata value isnΓÇÖt a string**\n\n`TryGetValue` returns an `object`, but `HashSet<string>.Add` requires a string; this fails to build. Cast (and validate) the metadata before adding.\n\n\nApply this diff to fix the type handling:\n\n```diff\n-            foreach (var chunk in relevantChunks)\n-            {\n-                if (chunk.Metadata.TryGetValue(\"parent_id\", out var parentId))\n-                {\n-                    parentIds.Add(parentId);\n-                }\n-            }\n+            foreach (var chunk in relevantChunks)\n+            {\n+                if (!chunk.Metadata.TryGetValue(\"parent_id\", out var parentIdObj))\n+                    continue;\n+\n+                if (parentIdObj is string parentId && !string.IsNullOrWhiteSpace(parentId))\n+                {\n+                    parentIds.Add(parentId);\n+                }\n+            }\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Retrievers/ParentDocumentRetriever.cs around\nlines 36-43, the code calls chunk.Metadata.TryGetValue which yields an object\nbut then adds it to a HashSet<string>, causing a compile error; update the\nconditional to validate and cast the retrieved metadata to a string before\nadding (e.g., check that the returned object is a non-empty string via an \"is\nstring\" pattern or safe cast and null/empty check) and only call parentIds.Add\nwith that validated string.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T13:49:54Z",
    "updated_at": "2025-11-04T13:49:57Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490596294",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596294"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490596294"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596294/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 36,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 43,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 43,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596311",
    "pull_request_review_id": 3416572453,
    "id": 2490596311,
    "node_id": "PRRC_kwDOKSXUF86Uc3vX",
    "diff_hunk": "@@ -0,0 +1,70 @@\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Threading.Tasks;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\n+{\n+    /// <summary>\n+    /// Parent document retriever that retrieves full documents from chunk matches\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations</typeparam>\n+    public class ParentDocumentRetriever<T> : RetrieverBase<T> where T : struct, IComparable, IComparable<T>, IConvertible, IEquatable<T>, IFormattable\n+    {\n+        private readonly IEmbeddingModel<T> _embeddingModel;\n+        private readonly IDocumentStore<T> _chunkStore;\n+        private readonly IDocumentStore<T> _parentStore;\n+\n+        public ParentDocumentRetriever(\n+            IEmbeddingModel<T> embeddingModel,\n+            IDocumentStore<T> chunkStore,\n+            IDocumentStore<T> parentStore)\n+        {\n+            _embeddingModel = embeddingModel ?? throw new ArgumentNullException(nameof(embeddingModel));\n+            _chunkStore = chunkStore ?? throw new ArgumentNullException(nameof(chunkStore));\n+            _parentStore = parentStore ?? throw new ArgumentNullException(nameof(parentStore));\n+        }\n+\n+        protected override async Task<List<Document<T>>> RetrieveCoreAsync(string query, int topK = 5)\n+        {\n+            var queryEmbedding = await _embeddingModel.GenerateEmbeddingAsync(query);\n+            \n+            var relevantChunks = await _chunkStore.SearchAsync(queryEmbedding, topK * 2);\n+\n+            var parentIds = new HashSet<string>();\n+            foreach (var chunk in relevantChunks)\n+            {\n+                if (chunk.Metadata.TryGetValue(\"parent_id\", out var parentId))\n+                {\n+                    parentIds.Add(parentId);\n+                }\n+            }\n+\n+            var parentDocuments = new List<Document<T>>();\n+            foreach (var parentId in parentIds.Take(topK))\n+            {\n+                var parent = await _parentStore.GetDocumentAsync(parentId);\n+                if (parent != null)\n+                {\n+                    parentDocuments.Add(parent);\n+                }\n+            }\n+\n+            var scoredParents = new List<(Document<T> doc, T score)>();\n+            foreach (var parent in parentDocuments)\n+            {\n+                var score = StatisticsHelper.CosineSimilarity(queryEmbedding, parent.Embedding, NumOps);\n+                scoredParents.Add((parent, score));\n+            }",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/ParentDocumentRetriever.cs",
    "commit_id": "33a79e89f3137dc80c940c48d7ab74f127dd4d91",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Null embeddings trigger runtime faults**\n\nMost document stores (e.g., AzureSearchDocumentStore.SearchAsync) never hydrate `Document<T>.Embedding`, so `StatisticsHelper.CosineSimilarity(queryEmbedding, parent.Embedding, NumOps)` will throw as soon as a parent lacks an embedding. Either compute the embedding on the fly or skip scoring when itΓÇÖs missing. Based on repository patterns, regenerating the vector is the safest choice.\n\n\nApply this diff to harden the scoring:\n\n```diff\n-            foreach (var parent in parentDocuments)\n-            {\n-                var score = StatisticsHelper.CosineSimilarity(queryEmbedding, parent.Embedding, NumOps);\n-                scoredParents.Add((parent, score));\n-            }\n+            foreach (var parent in parentDocuments)\n+            {\n+                if (parent.Embedding is null)\n+                {\n+                    parent.Embedding = await _embeddingModel.GenerateEmbeddingAsync(parent.Content);\n+                }\n+\n+                var score = StatisticsHelper.CosineSimilarity(queryEmbedding, parent.Embedding, NumOps);\n+                parent.RelevanceScore = score;\n+                parent.HasRelevanceScore = true;\n+                scoredParents.Add((parent, score));\n+            }\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Retrievers/ParentDocumentRetriever.cs around\nlines 55 to 60, scoring calls StatisticsHelper.CosineSimilarity with\nparent.Embedding which can be null for many document stores; this causes runtime\nexceptions. Before computing cosine similarity, check if parent.Embedding is\nnull and if so regenerate the embedding (using the same embedding\nservice/provider used elsewhere in the repo ΓÇö e.g., call the class's\nembedding/encoder method or service async method to produce a vector) and assign\nit to parent.Embedding, then compute the cosine; alternatively, if regeneration\nis not possible, skip scoring that parent. Ensure any async embedding call is\nawaited and handle exceptions/logging so scoring proceeds safely.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T13:49:54Z",
    "updated_at": "2025-11-04T13:49:57Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490596311",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596311"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490596311"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490596311/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 55,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 60,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 60,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490601154",
    "pull_request_review_id": 3416579419,
    "id": 2490601154,
    "node_id": "PRRC_kwDOKSXUF86Uc47C",
    "diff_hunk": "@@ -0,0 +1,51 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Retrievers;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.AdvancedPatterns\n+{\n+    public class ChainOfThoughtRetriever<T> : RetrieverBase<T> where T : struct\n+    {\n+        private readonly IRetriever<T> _baseRetriever;\n+        private readonly int _reasoningSteps;\n+        \n+        protected override INumericOperations<T> NumOps { get; }\n+\n+        public ChainOfThoughtRetriever(IRetriever<T> baseRetriever, int reasoningSteps = 3)\n+        {\n+            _baseRetriever = baseRetriever ?? throw new System.ArgumentNullException(nameof(baseRetriever));\n+            _reasoningSteps = reasoningSteps > 0 ? reasoningSteps : throw new System.ArgumentOutOfRangeException(nameof(reasoningSteps));\n+            NumOps = NumericOperationsFactory.GetOperations<T>();\n+        }",
    "path": "src/RetrievalAugmentedGeneration/AdvancedPatterns/ChainOfThoughtRetriever.cs",
    "commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Remove the `NumOps` override.**\n\n`RetrieverBase<T>` already exposes a non-virtual `NumOps` property. Overriding it here produces the compiler error reported by the build. Drop the override (and the assignment in the ctor) and rely on the base property instead.\n\n```diff\n-        private readonly IRetriever<T> _baseRetriever;\n-        private readonly int _reasoningSteps;\n-        \n-        protected override INumericOperations<T> NumOps { get; }\n+        private readonly IRetriever<T> _baseRetriever;\n+        private readonly int _reasoningSteps;\n@@\n-            NumOps = NumericOperationsFactory.GetOperations<T>();\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        public ChainOfThoughtRetriever(IRetriever<T> baseRetriever, int reasoningSteps = 3)\n        {\n            _baseRetriever = baseRetriever ?? throw new System.ArgumentNullException(nameof(baseRetriever));\n            _reasoningSteps = reasoningSteps > 0 ? reasoningSteps : throw new System.ArgumentOutOfRangeException(nameof(reasoningSteps));\n        }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒº░ Tools</summary>\n\n<details>\n<summary>≡ƒ¬¢ GitHub Check: Build All Frameworks</summary>\n\n[failure] 15-15: \n'ChainOfThoughtRetriever<T>.NumOps': cannot override because 'RetrieverBase<T>.NumOps' is not a property\n\n</details>\n\n</details>\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/AdvancedPatterns/ChainOfThoughtRetriever.cs\naround lines 15 to 22, the class declares and overrides the NumOps property and\nassigns it in the constructor, but RetrieverBase<T> already exposes a\nnon-virtual NumOps property so this override causes a compiler error; remove the\nprotected override INumericOperations<T> NumOps declaration and also remove the\nassignment to NumOps in the constructor, relying on the base class's NumOps\nproperty instead.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits e352386 to 62aea1a",
    "created_at": "2025-11-04T13:51:06Z",
    "updated_at": "2025-11-04T13:53:23Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490601154",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490601154"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490601154"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490601154/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 15,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 22,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 22,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490601182",
    "pull_request_review_id": 3416579419,
    "id": 2490601182,
    "node_id": "PRRC_kwDOKSXUF86Uc47e",
    "diff_hunk": "@@ -0,0 +1,51 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Retrievers;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.AdvancedPatterns\n+{\n+    public class ChainOfThoughtRetriever<T> : RetrieverBase<T> where T : struct\n+    {\n+        private readonly IRetriever<T> _baseRetriever;\n+        private readonly int _reasoningSteps;\n+        \n+        protected override INumericOperations<T> NumOps { get; }\n+\n+        public ChainOfThoughtRetriever(IRetriever<T> baseRetriever, int reasoningSteps = 3)\n+        {\n+            _baseRetriever = baseRetriever ?? throw new System.ArgumentNullException(nameof(baseRetriever));\n+            _reasoningSteps = reasoningSteps > 0 ? reasoningSteps : throw new System.ArgumentOutOfRangeException(nameof(reasoningSteps));\n+            NumOps = NumericOperationsFactory.GetOperations<T>();\n+        }\n+\n+        protected override List<Document<T>> RetrieveCore(string query, int topK)\n+        {\n+            var results = new List<Document<T>>();\n+            var currentQuery = query;\n+\n+            for (int step = 0; step < _reasoningSteps; step++)\n+            {\n+                var stepResults = _baseRetriever.Retrieve(currentQuery, topK);\n+                \n+                foreach (var doc in stepResults)\n+                {\n+                    if (!results.Any(d => d.Id == doc.Id))\n+                    {\n+                        results.Add(doc);\n+                    }\n+                }\n+\n+                if (stepResults.Count > 0)\n+                {\n+                    currentQuery = $\"{query} Context: {stepResults[0].Content}\";\n+                }\n+            }\n+\n+            return results.OrderByDescending(d => NumOps.ToDouble(d.RelevanceScore)).Take(topK).ToList();\n+        }",
    "path": "src/RetrievalAugmentedGeneration/AdvancedPatterns/ChainOfThoughtRetriever.cs",
    "commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Match the updated `RetrieveCore` signature.**\n\n`RetrieverBase<T>` now calls into `protected abstract List<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)`. This overload without `metadataFilters` no longer exists, so the class fails to compile. Update the signature (and forward the filters to `_baseRetriever` using its filtered retrieval method) to satisfy the base contract.\n\n```diff\n-        protected override List<Document<T>> RetrieveCore(string query, int topK)\n+        protected override List<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\n         {\n             var results = new List<Document<T>>();\n             var currentQuery = query;\n \n             for (int step = 0; step < _reasoningSteps; step++)\n             {\n-                var stepResults = _baseRetriever.Retrieve(currentQuery, topK);\n+                var stepResults = _baseRetriever.RetrieveWithFilters(currentQuery, topK, metadataFilters);\n```\nIf `_baseRetriever` exposes a differently named filtered retrieval API, call that instead, but make sure the filters flow through.\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>≡ƒº░ Tools</summary>\n\n<details>\n<summary>≡ƒ¬¢ GitHub Check: Build All Frameworks</summary>\n\n[failure] 24-24: \n'ChainOfThoughtRetriever<T>.RetrieveCore(string, int)': no suitable method found to override\n\n</details>\n\n</details>\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/AdvancedPatterns/ChainOfThoughtRetriever.cs\naround lines 24 to 48, the RetrieveCore override signature no longer matches the\nbase: change the method to match protected override List<Document<T>>\nRetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\nand forward those filters when calling the underlying retriever (e.g., call\n_baseRetriever.Retrieve(currentQuery, topK, metadataFilters) or the filtered API\nyour _baseRetriever exposes), keeping the same dedup, query-update, and final\nsorting/Take(topK) logic.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T13:51:06Z",
    "updated_at": "2025-11-04T13:51:10Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490601182",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490601182"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490601182"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490601182/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 24,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 48,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 48,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490601205",
    "pull_request_review_id": 3416579419,
    "id": 2490601205,
    "node_id": "PRRC_kwDOKSXUF86Uc471",
    "diff_hunk": "@@ -0,0 +1,195 @@\n+#if NETCOREAPP || NETSTANDARD2_1_OR_GREATER\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Net.Http;\n+using System.Text;\n+using System.Text.Json;\n+using System.Threading.Tasks;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores\n+{\n+    /// <summary>\n+    /// Document store using Azure Cognitive Search\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations</typeparam>\n+    public class AzureSearchDocumentStore<T> : DocumentStoreBase<T> where T : struct, IComparable, IComparable<T>, IConvertible, IEquatable<T>, IFormattable\n+    {\n+        private readonly string _serviceName;\n+        private readonly string _apiKey;\n+        private readonly string _indexName;\n+        private readonly HttpClient _httpClient;\n+        private readonly string _baseUrl;\n+\n+        public AzureSearchDocumentStore(string serviceName, string apiKey, string indexName)\n+        {\n+            if (string.IsNullOrEmpty(serviceName))\n+                throw new ArgumentException(\"Service name cannot be null or empty\", nameof(serviceName));\n+            if (string.IsNullOrEmpty(apiKey))\n+                throw new ArgumentException(\"API key cannot be null or empty\", nameof(apiKey));\n+            if (string.IsNullOrEmpty(indexName))\n+                throw new ArgumentException(\"Index name cannot be null or empty\", nameof(indexName));\n+\n+            _serviceName = serviceName;\n+            _apiKey = apiKey;\n+            _indexName = indexName;\n+            _httpClient = new HttpClient();\n+            _httpClient.DefaultRequestHeaders.Add(\"api-key\", _apiKey);\n+            _baseUrl = $\"https://{_serviceName}.search.windows.net\";\n+        }\n+\n+        public override async Task AddDocumentAsync(Document<T> document)\n+        {\n+            if (document == null)\n+                throw new ArgumentNullException(nameof(document));\n+\n+            var uploadDoc = new\n+            {\n+                value = new[]\n+                {\n+                    new\n+                    {\n+                        id = document.Id,\n+                        content = document.Content,\n+                        embedding = ConvertVectorToDoubleArray(document.Embedding),\n+                        metadata = document.Metadata\n+                    }\n+                }\n+            };\n+\n+            var json = JsonSerializer.Serialize(uploadDoc);\n+            var content = new StringContent(json, Encoding.UTF8, \"application/json\");\n+\n+            var response = await _httpClient.PostAsync(\n+                $\"{_baseUrl}/indexes/{_indexName}/docs/index?api-version=2023-11-01\",\n+                content);\n+\n+            response.EnsureSuccessStatusCode();\n+        }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/AzureSearchDocumentStore.cs",
    "commit_id": "62aea1a1b376f23b0693eb1f0ecd24a268f42be2",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Include `@search.action` in the upload payload.**\n\nAzure Search's indexing API requires each item to set `@search.action` (e.g. `\"upload\"`). Without it, the service rejects the request. Serialize the payload with the correct action flag before calling the index endpoint.\n\n```diff\n-            var uploadDoc = new\n-            {\n-                value = new[]\n-                {\n-                    new\n-                    {\n-                        id = document.Id,\n-                        content = document.Content,\n-                        embedding = ConvertVectorToDoubleArray(document.Embedding),\n-                        metadata = document.Metadata\n-                    }\n-                }\n-            };\n+            var uploadDoc = new\n+            {\n+                value = new[]\n+                {\n+                    new Dictionary<string, object?>\n+                    {\n+                        [\"@search.action\"] = \"upload\",\n+                        [\"id\"] = document.Id,\n+                        [\"content\"] = document.Content,\n+                        [\"embedding\"] = ConvertVectorToDoubleArray(document.Embedding),\n+                        [\"metadata\"] = document.Metadata\n+                    }\n+                }\n+            };\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/AzureSearchDocumentStore.cs\naround lines 48 to 70, the uploaded document payload is missing the required\nAzure Search field \"@search.action\"; update the anonymous object used to build\nuploadDoc so each item includes \"@search.action\" = \"upload\" (alongside id,\ncontent, embedding, metadata) before serializing, so the index API accepts the\nrequest; keep the existing serialization and POST call but ensure the property\nname is exactly \"@search.action\" with the value \"upload\".\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T13:51:06Z",
    "updated_at": "2025-11-04T13:51:10Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490601205",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490601205"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490601205"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490601205/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 48,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 70,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 70,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490601213",
    "pull_request_review_id": 3416579419,
    "id": 2490601213,
    "node_id": "PRRC_kwDOKSXUF86Uc479",
    "diff_hunk": "@@ -0,0 +1,195 @@\n+#if NETCOREAPP || NETSTANDARD2_1_OR_GREATER\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Net.Http;\n+using System.Text;\n+using System.Text.Json;\n+using System.Threading.Tasks;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores\n+{\n+    /// <summary>\n+    /// Document store using Azure Cognitive Search\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations</typeparam>\n+    public class AzureSearchDocumentStore<T> : DocumentStoreBase<T> where T : struct, IComparable, IComparable<T>, IConvertible, IEquatable<T>, IFormattable\n+    {\n+        private readonly string _serviceName;\n+        private readonly string _apiKey;\n+        private readonly string _indexName;\n+        private readonly HttpClient _httpClient;\n+        private readonly string _baseUrl;\n+\n+        public AzureSearchDocumentStore(string serviceName, string apiKey, string indexName)\n+        {\n+            if (string.IsNullOrEmpty(serviceName))\n+                throw new ArgumentException(\"Service name cannot be null or empty\", nameof(serviceName));\n+            if (string.IsNullOrEmpty(apiKey))\n+                throw new ArgumentException(\"API key cannot be null or empty\", nameof(apiKey));\n+            if (string.IsNullOrEmpty(indexName))\n+                throw new ArgumentException(\"Index name cannot be null or empty\", nameof(indexName));\n+\n+            _serviceName = serviceName;\n+            _apiKey = apiKey;\n+            _indexName = indexName;\n+            _httpClient = new HttpClient();\n+            _httpClient.DefaultRequestHeaders.Add(\"api-key\", _apiKey);\n+            _baseUrl = $\"https://{_serviceName}.search.windows.net\";\n+        }\n+\n+        public override async Task AddDocumentAsync(Document<T> document)\n+        {\n+            if (document == null)\n+                throw new ArgumentNullException(nameof(document));\n+\n+            var uploadDoc = new\n+            {\n+                value = new[]\n+                {\n+                    new\n+                    {\n+                        id = document.Id,\n+                        content = document.Content,\n+                        embedding = ConvertVectorToDoubleArray(document.Embedding),\n+                        metadata = document.Metadata\n+                    }\n+                }\n+            };\n+\n+            var json = JsonSerializer.Serialize(uploadDoc);\n+            var content = new StringContent(json, Encoding.UTF8, \"application/json\");\n+\n+            var response = await _httpClient.PostAsync(\n+                $\"{_baseUrl}/indexes/{_indexName}/docs/index?api-version=2023-11-01\",\n+                content);\n+\n+            response.EnsureSuccessStatusCode();\n+        }\n+\n+        public override async Task<List<Document<T>>> SearchAsync(Vector<T> queryEmbedding, int topK = 5)\n+        {\n+            if (queryEmbedding == null)\n+                throw new ArgumentNullException(nameof(queryEmbedding));\n+\n+            var searchRequest = new\n+            {\n+                vector = new\n+                {\n+                    value = ConvertVectorToDoubleArray(queryEmbedding),\n+                    fields = \"embedding\",\n+                    k = topK\n+                },\n+                select = \"id,content,metadata\"\n+            };\n+\n+            var json = JsonSerializer.Serialize(searchRequest);\n+            var content = new StringContent(json, Encoding.UTF8, \"application/json\");\n+\n+            var response = await _httpClient.PostAsync(\n+                $\"{_baseUrl}/indexes/{_indexName}/docs/search?api-version=2023-11-01\",\n+                content);\n+\n+            response.EnsureSuccessStatusCode();\n+\n+            var responseJson = await response.Content.ReadAsStringAsync();\n+            var document = JsonDocument.Parse(responseJson);\n+            var results = new List<Document<T>>();\n+\n+            foreach (var result in document.RootElement.GetProperty(\"value\").EnumerateArray())\n+            {\n+                var doc = new Document<T>\n+                {\n+                    Id = result.GetProperty(\"id\").GetString() ?? string.Empty,\n+                    Content = result.GetProperty(\"content\").GetString() ?? string.Empty,\n+                    Metadata = JsonSerializer.Deserialize<Dictionary<string, string>>(\n+                        result.GetProperty(\"metadata\").GetRawText()) ?? new Dictionary<string, string>()\n+                };\n+\n+                results.Add(doc);\n+            }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/AzureSearchDocumentStore.cs",
    "commit_id": "62aea1a1b376f23b0693eb1f0ecd24a268f42be2",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Preserve arbitrary metadata types.**\n\nMetadata comes back as JSON with arbitrary value types. Deserializing into `Dictionary<string, string>` throws when the value is numeric/boolean. Swap the target type to `Dictionary<string, object?>` (and mirror that in `GetDocumentAsync`) so metadata survives intact.\n\n```diff\n-                    Metadata = JsonSerializer.Deserialize<Dictionary<string, string>>(\n-                        result.GetProperty(\"metadata\").GetRawText()) ?? new Dictionary<string, string>()\n+                    Metadata = JsonSerializer.Deserialize<Dictionary<string, object?>>(\n+                        result.GetProperty(\"metadata\").GetRawText()) ?? new Dictionary<string, object?>()\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/AzureSearchDocumentStore.cs\naround lines 101 to 112, the code deserializes metadata into Dictionary<string,\nstring> which fails for numeric/boolean JSON values; change the deserialization\ntarget to Dictionary<string, object?> so arbitrary JSON types are preserved,\nupdate the Document<T>.Metadata type accordingly (and mirror the same change in\nGetDocumentAsync signature/implementation), and ensure null-coalescing still\nprovides an empty Dictionary<string, object?> when deserialization returns null.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T13:51:07Z",
    "updated_at": "2025-11-04T13:51:10Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490601213",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490601213"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490601213"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490601213/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 101,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 112,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 112,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490601225",
    "pull_request_review_id": 3416579419,
    "id": 2490601225,
    "node_id": "PRRC_kwDOKSXUF86Uc48J",
    "diff_hunk": "@@ -0,0 +1,195 @@\n+#if NETCOREAPP || NETSTANDARD2_1_OR_GREATER\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Net.Http;\n+using System.Text;\n+using System.Text.Json;\n+using System.Threading.Tasks;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores\n+{\n+    /// <summary>\n+    /// Document store using Azure Cognitive Search\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations</typeparam>\n+    public class AzureSearchDocumentStore<T> : DocumentStoreBase<T> where T : struct, IComparable, IComparable<T>, IConvertible, IEquatable<T>, IFormattable\n+    {\n+        private readonly string _serviceName;\n+        private readonly string _apiKey;\n+        private readonly string _indexName;\n+        private readonly HttpClient _httpClient;\n+        private readonly string _baseUrl;\n+\n+        public AzureSearchDocumentStore(string serviceName, string apiKey, string indexName)\n+        {\n+            if (string.IsNullOrEmpty(serviceName))\n+                throw new ArgumentException(\"Service name cannot be null or empty\", nameof(serviceName));\n+            if (string.IsNullOrEmpty(apiKey))\n+                throw new ArgumentException(\"API key cannot be null or empty\", nameof(apiKey));\n+            if (string.IsNullOrEmpty(indexName))\n+                throw new ArgumentException(\"Index name cannot be null or empty\", nameof(indexName));\n+\n+            _serviceName = serviceName;\n+            _apiKey = apiKey;\n+            _indexName = indexName;\n+            _httpClient = new HttpClient();\n+            _httpClient.DefaultRequestHeaders.Add(\"api-key\", _apiKey);\n+            _baseUrl = $\"https://{_serviceName}.search.windows.net\";\n+        }\n+\n+        public override async Task AddDocumentAsync(Document<T> document)\n+        {\n+            if (document == null)\n+                throw new ArgumentNullException(nameof(document));\n+\n+            var uploadDoc = new\n+            {\n+                value = new[]\n+                {\n+                    new\n+                    {\n+                        id = document.Id,\n+                        content = document.Content,\n+                        embedding = ConvertVectorToDoubleArray(document.Embedding),\n+                        metadata = document.Metadata\n+                    }\n+                }\n+            };\n+\n+            var json = JsonSerializer.Serialize(uploadDoc);\n+            var content = new StringContent(json, Encoding.UTF8, \"application/json\");\n+\n+            var response = await _httpClient.PostAsync(\n+                $\"{_baseUrl}/indexes/{_indexName}/docs/index?api-version=2023-11-01\",\n+                content);\n+\n+            response.EnsureSuccessStatusCode();\n+        }\n+\n+        public override async Task<List<Document<T>>> SearchAsync(Vector<T> queryEmbedding, int topK = 5)\n+        {\n+            if (queryEmbedding == null)\n+                throw new ArgumentNullException(nameof(queryEmbedding));\n+\n+            var searchRequest = new\n+            {\n+                vector = new\n+                {\n+                    value = ConvertVectorToDoubleArray(queryEmbedding),\n+                    fields = \"embedding\",\n+                    k = topK\n+                },\n+                select = \"id,content,metadata\"\n+            };\n+\n+            var json = JsonSerializer.Serialize(searchRequest);\n+            var content = new StringContent(json, Encoding.UTF8, \"application/json\");\n+\n+            var response = await _httpClient.PostAsync(\n+                $\"{_baseUrl}/indexes/{_indexName}/docs/search?api-version=2023-11-01\",\n+                content);\n+\n+            response.EnsureSuccessStatusCode();\n+\n+            var responseJson = await response.Content.ReadAsStringAsync();\n+            var document = JsonDocument.Parse(responseJson);\n+            var results = new List<Document<T>>();\n+\n+            foreach (var result in document.RootElement.GetProperty(\"value\").EnumerateArray())\n+            {\n+                var doc = new Document<T>\n+                {\n+                    Id = result.GetProperty(\"id\").GetString() ?? string.Empty,\n+                    Content = result.GetProperty(\"content\").GetString() ?? string.Empty,\n+                    Metadata = JsonSerializer.Deserialize<Dictionary<string, string>>(\n+                        result.GetProperty(\"metadata\").GetRawText()) ?? new Dictionary<string, string>()\n+                };\n+\n+                results.Add(doc);\n+            }\n+\n+            return results;\n+        }\n+\n+        public override async Task DeleteDocumentAsync(string documentId)\n+        {\n+            if (string.IsNullOrEmpty(documentId))\n+                throw new ArgumentException(\"Document ID cannot be null or empty\", nameof(documentId));\n+\n+            var deleteDoc = new\n+            {\n+                value = new[]\n+                {\n+                    new\n+                    {\n+                        id = documentId,\n+                        atDelete = \"delete\"\n+                    }\n+                }\n+            };\n+\n+            var json = JsonSerializer.Serialize(deleteDoc);\n+            var content = new StringContent(json, Encoding.UTF8, \"application/json\");\n+\n+            var response = await _httpClient.PostAsync(\n+                $\"{_baseUrl}/indexes/{_indexName}/docs/index?api-version=2023-11-01\",\n+                content);\n+\n+            response.EnsureSuccessStatusCode();\n+        }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/AzureSearchDocumentStore.cs",
    "commit_id": "62aea1a1b376f23b0693eb1f0ecd24a268f42be2",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Fix the delete operation payload.**\n\nAzure Search expects `@search.action` with value `\"delete\"`. Sending `atDelete` does nothing, so documents never get removed. Use the same `Dictionary<string, object?>` pattern but set the action to `\"delete\"`.\n\n```diff\n-            var deleteDoc = new\n-            {\n-                value = new[]\n-                {\n-                    new\n-                    {\n-                        id = documentId,\n-                        atDelete = \"delete\"\n-                    }\n-                }\n-            };\n+            var deleteDoc = new\n+            {\n+                value = new[]\n+                {\n+                    new Dictionary<string, object?>\n+                    {\n+                        [\"@search.action\"] = \"delete\",\n+                        [\"id\"] = documentId\n+                    }\n+                }\n+            };\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/AzureSearchDocumentStore.cs\naround lines 122 to 142, the delete payload currently uses an \"atDelete\"\nproperty which Azure Search ignores; replace that anonymous object with the\nDictionary<string, object?> pattern used elsewhere so each document entry\ncontains the \"id\" and the key \"@search.action\" set to \"delete\" (e.g. new\nDictionary<string, object?> { [\"id\"]= documentId, [\"@search.action\"]=\"delete\"\n}), serialize that payload and send it as before so Azure Search will actually\nremove the document.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T13:51:07Z",
    "updated_at": "2025-11-04T13:51:10Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490601225",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490601225"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490601225"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490601225/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 122,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 142,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 142,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490601236",
    "pull_request_review_id": 3416579419,
    "id": 2490601236,
    "node_id": "PRRC_kwDOKSXUF86Uc48U",
    "diff_hunk": "@@ -0,0 +1,195 @@\n+#if NETCOREAPP || NETSTANDARD2_1_OR_GREATER\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Net.Http;\n+using System.Text;\n+using System.Text.Json;\n+using System.Threading.Tasks;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores\n+{\n+    /// <summary>\n+    /// Document store using Azure Cognitive Search\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations</typeparam>\n+    public class AzureSearchDocumentStore<T> : DocumentStoreBase<T> where T : struct, IComparable, IComparable<T>, IConvertible, IEquatable<T>, IFormattable\n+    {\n+        private readonly string _serviceName;\n+        private readonly string _apiKey;\n+        private readonly string _indexName;\n+        private readonly HttpClient _httpClient;\n+        private readonly string _baseUrl;\n+\n+        public AzureSearchDocumentStore(string serviceName, string apiKey, string indexName)\n+        {\n+            if (string.IsNullOrEmpty(serviceName))\n+                throw new ArgumentException(\"Service name cannot be null or empty\", nameof(serviceName));\n+            if (string.IsNullOrEmpty(apiKey))\n+                throw new ArgumentException(\"API key cannot be null or empty\", nameof(apiKey));\n+            if (string.IsNullOrEmpty(indexName))\n+                throw new ArgumentException(\"Index name cannot be null or empty\", nameof(indexName));\n+\n+            _serviceName = serviceName;\n+            _apiKey = apiKey;\n+            _indexName = indexName;\n+            _httpClient = new HttpClient();\n+            _httpClient.DefaultRequestHeaders.Add(\"api-key\", _apiKey);\n+            _baseUrl = $\"https://{_serviceName}.search.windows.net\";\n+        }\n+\n+        public override async Task AddDocumentAsync(Document<T> document)\n+        {\n+            if (document == null)\n+                throw new ArgumentNullException(nameof(document));\n+\n+            var uploadDoc = new\n+            {\n+                value = new[]\n+                {\n+                    new\n+                    {\n+                        id = document.Id,\n+                        content = document.Content,\n+                        embedding = ConvertVectorToDoubleArray(document.Embedding),\n+                        metadata = document.Metadata\n+                    }\n+                }\n+            };\n+\n+            var json = JsonSerializer.Serialize(uploadDoc);\n+            var content = new StringContent(json, Encoding.UTF8, \"application/json\");\n+\n+            var response = await _httpClient.PostAsync(\n+                $\"{_baseUrl}/indexes/{_indexName}/docs/index?api-version=2023-11-01\",\n+                content);\n+\n+            response.EnsureSuccessStatusCode();\n+        }\n+\n+        public override async Task<List<Document<T>>> SearchAsync(Vector<T> queryEmbedding, int topK = 5)\n+        {\n+            if (queryEmbedding == null)\n+                throw new ArgumentNullException(nameof(queryEmbedding));\n+\n+            var searchRequest = new\n+            {\n+                vector = new\n+                {\n+                    value = ConvertVectorToDoubleArray(queryEmbedding),\n+                    fields = \"embedding\",\n+                    k = topK\n+                },\n+                select = \"id,content,metadata\"\n+            };\n+\n+            var json = JsonSerializer.Serialize(searchRequest);\n+            var content = new StringContent(json, Encoding.UTF8, \"application/json\");\n+\n+            var response = await _httpClient.PostAsync(\n+                $\"{_baseUrl}/indexes/{_indexName}/docs/search?api-version=2023-11-01\",\n+                content);\n+\n+            response.EnsureSuccessStatusCode();\n+\n+            var responseJson = await response.Content.ReadAsStringAsync();\n+            var document = JsonDocument.Parse(responseJson);\n+            var results = new List<Document<T>>();\n+\n+            foreach (var result in document.RootElement.GetProperty(\"value\").EnumerateArray())\n+            {\n+                var doc = new Document<T>\n+                {\n+                    Id = result.GetProperty(\"id\").GetString() ?? string.Empty,\n+                    Content = result.GetProperty(\"content\").GetString() ?? string.Empty,\n+                    Metadata = JsonSerializer.Deserialize<Dictionary<string, string>>(\n+                        result.GetProperty(\"metadata\").GetRawText()) ?? new Dictionary<string, string>()\n+                };\n+\n+                results.Add(doc);\n+            }\n+\n+            return results;\n+        }\n+\n+        public override async Task DeleteDocumentAsync(string documentId)\n+        {\n+            if (string.IsNullOrEmpty(documentId))\n+                throw new ArgumentException(\"Document ID cannot be null or empty\", nameof(documentId));\n+\n+            var deleteDoc = new\n+            {\n+                value = new[]\n+                {\n+                    new\n+                    {\n+                        id = documentId,\n+                        atDelete = \"delete\"\n+                    }\n+                }\n+            };\n+\n+            var json = JsonSerializer.Serialize(deleteDoc);\n+            var content = new StringContent(json, Encoding.UTF8, \"application/json\");\n+\n+            var response = await _httpClient.PostAsync(\n+                $\"{_baseUrl}/indexes/{_indexName}/docs/index?api-version=2023-11-01\",\n+                content);\n+\n+            response.EnsureSuccessStatusCode();\n+        }\n+\n+        public override async Task<Document<T>?> GetDocumentAsync(string documentId)\n+        {\n+            if (string.IsNullOrEmpty(documentId))\n+                throw new ArgumentException(\"Document ID cannot be null or empty\", nameof(documentId));\n+\n+            try\n+            {\n+                var response = await _httpClient.GetAsync(\n+                    $\"{_baseUrl}/indexes/{_indexName}/docs/{documentId}?api-version=2023-11-01\");\n+\n+                if (!response.IsSuccessStatusCode)\n+                    return null;\n+\n+                var json = await response.Content.ReadAsStringAsync();\n+                var element = JsonDocument.Parse(json).RootElement;\n+\n+                return new Document<T>\n+                {\n+                    Id = element.GetProperty(\"id\").GetString() ?? string.Empty,\n+                    Content = element.GetProperty(\"content\").GetString() ?? string.Empty,\n+                    Metadata = JsonSerializer.Deserialize<Dictionary<string, string>>(\n+                        element.GetProperty(\"metadata\").GetRawText()) ?? new Dictionary<string, string>()\n+                };\n+            }\n+            catch\n+            {\n+                return null;\n+            }\n+        }\n+\n+        private double[] ConvertVectorToDoubleArray(Vector<T> vector)\n+        {\n+            var result = new double[vector.Length];\n+            for (int i = 0; i < vector.Length; i++)\n+            {\n+                result[i] = Convert.ToDouble(vector[i]);\n+            }\n+            return result;\n+        }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/AzureSearchDocumentStore.cs",
    "commit_id": "62aea1a1b376f23b0693eb1f0ecd24a268f42be2",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Null-check embeddings before conversion.**\n\n`ConvertVectorToDoubleArray(document.Embedding)` currently dereferences `null` and crashes. Add an explicit guard so callers get a clear exception.\n\n```diff\n-        private double[] ConvertVectorToDoubleArray(Vector<T> vector)\n+        private double[] ConvertVectorToDoubleArray(Vector<T> vector)\n         {\n+            if (vector == null)\n+                throw new ArgumentNullException(nameof(vector));\n+\n             var result = new double[vector.Length];\n             for (int i = 0; i < vector.Length; i++)\n             {\n                 result[i] = Convert.ToDouble(vector[i]);\n             }\n             return result;\n         }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        private double[] ConvertVectorToDoubleArray(Vector<T> vector)\n        {\n            if (vector == null)\n                throw new ArgumentNullException(nameof(vector));\n\n            var result = new double[vector.Length];\n            for (int i = 0; i < vector.Length; i++)\n            {\n                result[i] = Convert.ToDouble(vector[i]);\n            }\n            return result;\n        }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/AzureSearchDocumentStore.cs\naround lines 174 to 182, the method ConvertVectorToDoubleArray dereferences a\npotentially null Vector<T> (e.g., document.Embedding) and will throw a\nNullReferenceException; add an explicit null check at the top (if vector ==\nnull) and throw an ArgumentNullException with the parameter name (or\n\"vector\"/\"document.Embedding\") so callers receive a clear, descriptive\nexception; ensure the null check occurs before accessing vector.Length.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T13:51:07Z",
    "updated_at": "2025-11-04T13:51:10Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490601236",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490601236"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490601236"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490601236/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 174,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 182,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 182,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490601245",
    "pull_request_review_id": 3416579419,
    "id": 2490601245,
    "node_id": "PRRC_kwDOKSXUF86Uc48d",
    "diff_hunk": "@@ -0,0 +1,223 @@\n+#if NETCOREAPP || NETSTANDARD2_1_OR_GREATER\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Net.Http;\n+using System.Text;\n+using System.Text.Json;\n+using System.Threading.Tasks;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores\n+{\n+    /// <summary>\n+    /// Document store using ChromaDB vector database\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations</typeparam>\n+    public class ChromaDBDocumentStore<T> : DocumentStoreBase<T> where T : struct, IComparable, IComparable<T>, IConvertible, IEquatable<T>, IFormattable\n+    {\n+        private readonly string _host;\n+        private readonly int _port;\n+        private readonly string _collectionName;\n+        private readonly HttpClient _httpClient;\n+        private readonly string _baseUrl;\n+        private string? _collectionId;\n+\n+        public ChromaDBDocumentStore(string host = \"localhost\", int port = 8000, string collectionName = \"documents\")\n+        {\n+            _host = host;\n+            _port = port;\n+            _collectionName = collectionName;\n+            _httpClient = new HttpClient();\n+            _baseUrl = $\"http://{_host}:{_port}/api/v1\";\n+        }\n+\n+        private async Task EnsureCollectionExistsAsync()\n+        {\n+            if (_collectionId != null)\n+                return;\n+\n+            var createRequest = new\n+            {\n+                name = _collectionName,\n+                metadata = new { description = \"AiDotNet document collection\" }\n+            };\n+\n+            var json = JsonSerializer.Serialize(createRequest);\n+            var content = new StringContent(json, Encoding.UTF8, \"application/json\");\n+\n+            try\n+            {\n+                var response = await _httpClient.PostAsync($\"{_baseUrl}/collections\", content);\n+                var responseJson = await response.Content.ReadAsStringAsync();\n+                var document = JsonDocument.Parse(responseJson);\n+                _collectionId = document.RootElement.GetProperty(\"id\").GetString();\n+            }\n+            catch\n+            {\n+                var response = await _httpClient.GetAsync($\"{_baseUrl}/collections/{_collectionName}\");\n+                if (response.IsSuccessStatusCode)\n+                {\n+                    var responseJson = await response.Content.ReadAsStringAsync();\n+                    var document = JsonDocument.Parse(responseJson);\n+                    _collectionId = document.RootElement.GetProperty(\"id\").GetString();\n+                }\n+            }\n+        }\n+\n+        public override async Task AddDocumentAsync(Document<T> document)\n+        {\n+            if (document == null)\n+                throw new ArgumentNullException(nameof(document));\n+\n+            await EnsureCollectionExistsAsync();\n+\n+            var addRequest = new\n+            {\n+                ids = new[] { document.Id },\n+                embeddings = new[] { ConvertVectorToDoubleArray(document.Embedding) },\n+                documents = new[] { document.Content },\n+                metadatas = new[] { document.Metadata }\n+            };\n+\n+            var json = JsonSerializer.Serialize(addRequest);\n+            var content = new StringContent(json, Encoding.UTF8, \"application/json\");\n+\n+            var response = await _httpClient.PostAsync(\n+                $\"{_baseUrl}/collections/{_collectionId}/add\",\n+                content);\n+\n+            response.EnsureSuccessStatusCode();\n+        }\n+\n+        public override async Task<List<Document<T>>> SearchAsync(Vector<T> queryEmbedding, int topK = 5)\n+        {\n+            if (queryEmbedding == null)\n+                throw new ArgumentNullException(nameof(queryEmbedding));\n+\n+            await EnsureCollectionExistsAsync();\n+\n+            var queryRequest = new\n+            {\n+                query_embeddings = new[] { ConvertVectorToDoubleArray(queryEmbedding) },\n+                n_results = topK,\n+                include = new[] { \"documents\", \"metadatas\", \"distances\" }\n+            };\n+\n+            var json = JsonSerializer.Serialize(queryRequest);\n+            var content = new StringContent(json, Encoding.UTF8, \"application/json\");\n+\n+            var response = await _httpClient.PostAsync(\n+                $\"{_baseUrl}/collections/{_collectionId}/query\",\n+                content);\n+\n+            response.EnsureSuccessStatusCode();\n+\n+            var responseJson = await response.Content.ReadAsStringAsync();\n+            var document = JsonDocument.Parse(responseJson);\n+            var results = new List<Document<T>>();\n+\n+            var ids = document.RootElement.GetProperty(\"ids\")[0];\n+            var documents = document.RootElement.GetProperty(\"documents\")[0];\n+            var metadatas = document.RootElement.GetProperty(\"metadatas\")[0];\n+\n+            for (int i = 0; i < ids.GetArrayLength(); i++)\n+            {\n+                var doc = new Document<T>\n+                {\n+                    Id = ids[i].GetString() ?? string.Empty,\n+                    Content = documents[i].GetString() ?? string.Empty,\n+                    Metadata = JsonSerializer.Deserialize<Dictionary<string, string>>(\n+                        metadatas[i].GetRawText()) ?? new Dictionary<string, string>()\n+                };\n+\n+                results.Add(doc);\n+            }\n+",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/ChromaDBDocumentStore.cs",
    "commit_id": "62aea1a1b376f23b0693eb1f0ecd24a268f42be2",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Handle empty query responses safely.**\n\n`ids = ...[0]` and subsequent indexing assume Chroma always returns at least one inner array. When the collection has zero hits, the REST API returns `ids: [[]]` (or even an empty outer array), so these indexes throw instead of returning an empty list. Guard `GetArrayLength()` before indexing and shortΓÇæcircuit to an empty result set.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/ChromaDBDocumentStore.cs\naround lines 121 to 137, the code indexes into\ndocument.RootElement.GetProperty(\"ids\")[0] (and documents/metadatas) which\nthrows when the API returns empty arrays like ids: [[]] or an empty outer array;\nfix by first checking that the \"ids\", \"documents\", and \"metadatas\" properties\nexist and that their outer arrays have GetArrayLength() > 0 before accessing\n[0], and also verify the inner arrays have matching lengths; if any required\narray is missing or the outer/inner arrays are empty, shortΓÇæcircuit and return\nan empty results list to avoid exceptions.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 80630e8 to b6555ed",
    "created_at": "2025-11-04T13:51:07Z",
    "updated_at": "2025-11-05T02:31:15Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490601245",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490601245"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490601245"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490601245/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 121,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 137,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 137,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490601255",
    "pull_request_review_id": 3416579419,
    "id": 2490601255,
    "node_id": "PRRC_kwDOKSXUF86Uc48n",
    "diff_hunk": "@@ -0,0 +1,223 @@\n+#if NETCOREAPP || NETSTANDARD2_1_OR_GREATER\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Net.Http;\n+using System.Text;\n+using System.Text.Json;\n+using System.Threading.Tasks;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores\n+{\n+    /// <summary>\n+    /// Document store using ChromaDB vector database\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations</typeparam>\n+    public class ChromaDBDocumentStore<T> : DocumentStoreBase<T> where T : struct, IComparable, IComparable<T>, IConvertible, IEquatable<T>, IFormattable\n+    {\n+        private readonly string _host;\n+        private readonly int _port;\n+        private readonly string _collectionName;\n+        private readonly HttpClient _httpClient;\n+        private readonly string _baseUrl;\n+        private string? _collectionId;\n+\n+        public ChromaDBDocumentStore(string host = \"localhost\", int port = 8000, string collectionName = \"documents\")\n+        {\n+            _host = host;\n+            _port = port;\n+            _collectionName = collectionName;\n+            _httpClient = new HttpClient();\n+            _baseUrl = $\"http://{_host}:{_port}/api/v1\";\n+        }\n+\n+        private async Task EnsureCollectionExistsAsync()\n+        {\n+            if (_collectionId != null)\n+                return;\n+\n+            var createRequest = new\n+            {\n+                name = _collectionName,\n+                metadata = new { description = \"AiDotNet document collection\" }\n+            };\n+\n+            var json = JsonSerializer.Serialize(createRequest);\n+            var content = new StringContent(json, Encoding.UTF8, \"application/json\");\n+\n+            try\n+            {\n+                var response = await _httpClient.PostAsync($\"{_baseUrl}/collections\", content);\n+                var responseJson = await response.Content.ReadAsStringAsync();\n+                var document = JsonDocument.Parse(responseJson);\n+                _collectionId = document.RootElement.GetProperty(\"id\").GetString();\n+            }\n+            catch\n+            {\n+                var response = await _httpClient.GetAsync($\"{_baseUrl}/collections/{_collectionName}\");\n+                if (response.IsSuccessStatusCode)\n+                {\n+                    var responseJson = await response.Content.ReadAsStringAsync();\n+                    var document = JsonDocument.Parse(responseJson);\n+                    _collectionId = document.RootElement.GetProperty(\"id\").GetString();\n+                }\n+            }\n+        }\n+\n+        public override async Task AddDocumentAsync(Document<T> document)\n+        {\n+            if (document == null)\n+                throw new ArgumentNullException(nameof(document));\n+\n+            await EnsureCollectionExistsAsync();\n+\n+            var addRequest = new\n+            {\n+                ids = new[] { document.Id },\n+                embeddings = new[] { ConvertVectorToDoubleArray(document.Embedding) },\n+                documents = new[] { document.Content },\n+                metadatas = new[] { document.Metadata }\n+            };\n+\n+            var json = JsonSerializer.Serialize(addRequest);\n+            var content = new StringContent(json, Encoding.UTF8, \"application/json\");\n+\n+            var response = await _httpClient.PostAsync(\n+                $\"{_baseUrl}/collections/{_collectionId}/add\",\n+                content);\n+\n+            response.EnsureSuccessStatusCode();\n+        }\n+\n+        public override async Task<List<Document<T>>> SearchAsync(Vector<T> queryEmbedding, int topK = 5)\n+        {\n+            if (queryEmbedding == null)\n+                throw new ArgumentNullException(nameof(queryEmbedding));\n+\n+            await EnsureCollectionExistsAsync();\n+\n+            var queryRequest = new\n+            {\n+                query_embeddings = new[] { ConvertVectorToDoubleArray(queryEmbedding) },\n+                n_results = topK,\n+                include = new[] { \"documents\", \"metadatas\", \"distances\" }\n+            };\n+\n+            var json = JsonSerializer.Serialize(queryRequest);\n+            var content = new StringContent(json, Encoding.UTF8, \"application/json\");\n+\n+            var response = await _httpClient.PostAsync(\n+                $\"{_baseUrl}/collections/{_collectionId}/query\",\n+                content);\n+\n+            response.EnsureSuccessStatusCode();\n+\n+            var responseJson = await response.Content.ReadAsStringAsync();\n+            var document = JsonDocument.Parse(responseJson);\n+            var results = new List<Document<T>>();\n+\n+            var ids = document.RootElement.GetProperty(\"ids\")[0];\n+            var documents = document.RootElement.GetProperty(\"documents\")[0];\n+            var metadatas = document.RootElement.GetProperty(\"metadatas\")[0];\n+\n+            for (int i = 0; i < ids.GetArrayLength(); i++)\n+            {\n+                var doc = new Document<T>\n+                {\n+                    Id = ids[i].GetString() ?? string.Empty,\n+                    Content = documents[i].GetString() ?? string.Empty,\n+                    Metadata = JsonSerializer.Deserialize<Dictionary<string, string>>(\n+                        metadatas[i].GetRawText()) ?? new Dictionary<string, string>()\n+                };",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/ChromaDBDocumentStore.cs",
    "commit_id": "62aea1a1b376f23b0693eb1f0ecd24a268f42be2",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Keep metadata typed as `object`.**\n\nSame as the Elasticsearch store: deserializing to `Dictionary<string, string>` strips numerical/boolean types and breaks filter comparisons in `DocumentStoreBase<T>`. Materialize metadata as `Dictionary<string, object>` (or use `JsonElement` plus conversion) to preserve the original types.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/ChromaDBDocumentStore.cs\naround lines 131-133, metadata is being deserialized to\nDictionary<string,string>, which strips numeric/boolean types and breaks\nfilters; change the target to Dictionary<string,object> (or deserialize to\nDictionary<string,JsonElement> and convert JsonElement to appropriate CLR types)\nso original JSON types are preserved, and keep the null-coalescing fallback to a\nnew Dictionary<string,object> to avoid nulls.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits e0f1f5e to 6670ddf",
    "created_at": "2025-11-04T13:51:07Z",
    "updated_at": "2025-11-04T14:53:25Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490601255",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490601255"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490601255"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490601255/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 131,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 133,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 133,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490601264",
    "pull_request_review_id": 3416579419,
    "id": 2490601264,
    "node_id": "PRRC_kwDOKSXUF86Uc48w",
    "diff_hunk": "@@ -0,0 +1,205 @@\n+#if NETCOREAPP || NETSTANDARD2_1_OR_GREATER\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Net.Http;\n+using System.Text;\n+using System.Text.Json;\n+using System.Threading.Tasks;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores\n+{\n+    /// <summary>\n+    /// Document store using Elasticsearch with vector search\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations</typeparam>\n+    public class ElasticsearchDocumentStore<T> : DocumentStoreBase<T> where T : struct, IComparable, IComparable<T>, IConvertible, IEquatable<T>, IFormattable\n+    {\n+        private readonly string _host;\n+        private readonly int _port;\n+        private readonly string _indexName;\n+        private readonly HttpClient _httpClient;\n+        private readonly string _baseUrl;\n+\n+        public ElasticsearchDocumentStore(string host = \"localhost\", int port = 9200, string indexName = \"documents\")\n+        {\n+            _host = host;\n+            _port = port;\n+            _indexName = indexName;\n+            _httpClient = new HttpClient();\n+            _baseUrl = $\"http://{_host}:{_port}\";\n+        }\n+\n+        private async Task EnsureIndexExistsAsync()\n+        {\n+            var response = await _httpClient.GetAsync($\"{_baseUrl}/{_indexName}\");\n+            if (response.IsSuccessStatusCode)\n+                return;\n+\n+            var indexMapping = new\n+            {\n+                mappings = new\n+                {\n+                    properties = new\n+                    {\n+                        id = new { type = \"keyword\" },\n+                        content = new { type = \"text\" },\n+                        embedding = new\n+                        {\n+                            type = \"dense_vector\",\n+                            dims = 768,\n+                            index = true,\n+                            similarity = \"cosine\"\n+                        },",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/ElasticsearchDocumentStore.cs",
    "commit_id": "62aea1a1b376f23b0693eb1f0ecd24a268f42be2",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Don't hard-code the dense vector dimension.**\n\nThe mapping locks `embedding.dims` to 768, so any model that emits vectors of a different length (e.g., 512, 1024, 1536) will cause Elasticsearch to reject both indexing and search requests with 400 errors. The field mapping must either derive `dims` from the first vector you ingest or be configurable via constructor/argument so it always matches the actual embedding length.([elastic.co](https://www.elastic.co/getting-started/enterprise-search/vector-search/?utm_source=openai))\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/ElasticsearchDocumentStore.cs\naround lines 47ΓÇô55, the mapping hard-codes embedding.dims = 768 which will break\nindexing/search for models that emit other vector lengths; change the\nimplementation to accept the embedding dimension as a configurable parameter\n(constructor argument or factory option) or detect and set dims from the first\nembedding you index, then build the mapping using that dimension value; ensure\nany callers pass the correct dimension and validate it before creating the\nmapping so ES mapping dims always match actual embedding length.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits e0f1f5e to 6670ddf",
    "created_at": "2025-11-04T13:51:07Z",
    "updated_at": "2025-11-04T14:53:29Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490601264",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490601264"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490601264"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490601264/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 47,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 55,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 55,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490601269",
    "pull_request_review_id": 3416579419,
    "id": 2490601269,
    "node_id": "PRRC_kwDOKSXUF86Uc481",
    "diff_hunk": "@@ -0,0 +1,205 @@\n+#if NETCOREAPP || NETSTANDARD2_1_OR_GREATER\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Net.Http;\n+using System.Text;\n+using System.Text.Json;\n+using System.Threading.Tasks;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores\n+{\n+    /// <summary>\n+    /// Document store using Elasticsearch with vector search\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations</typeparam>\n+    public class ElasticsearchDocumentStore<T> : DocumentStoreBase<T> where T : struct, IComparable, IComparable<T>, IConvertible, IEquatable<T>, IFormattable\n+    {\n+        private readonly string _host;\n+        private readonly int _port;\n+        private readonly string _indexName;\n+        private readonly HttpClient _httpClient;\n+        private readonly string _baseUrl;\n+\n+        public ElasticsearchDocumentStore(string host = \"localhost\", int port = 9200, string indexName = \"documents\")\n+        {\n+            _host = host;\n+            _port = port;\n+            _indexName = indexName;\n+            _httpClient = new HttpClient();\n+            _baseUrl = $\"http://{_host}:{_port}\";\n+        }\n+\n+        private async Task EnsureIndexExistsAsync()\n+        {\n+            var response = await _httpClient.GetAsync($\"{_baseUrl}/{_indexName}\");\n+            if (response.IsSuccessStatusCode)\n+                return;\n+\n+            var indexMapping = new\n+            {\n+                mappings = new\n+                {\n+                    properties = new\n+                    {\n+                        id = new { type = \"keyword\" },\n+                        content = new { type = \"text\" },\n+                        embedding = new\n+                        {\n+                            type = \"dense_vector\",\n+                            dims = 768,\n+                            index = true,\n+                            similarity = \"cosine\"\n+                        },\n+                        metadata = new { type = \"object\", enabled = true }\n+                    }\n+                }\n+            };\n+\n+            var json = JsonSerializer.Serialize(indexMapping);\n+            var content = new StringContent(json, Encoding.UTF8, \"application/json\");\n+\n+            await _httpClient.PutAsync($\"{_baseUrl}/{_indexName}\", content);\n+        }\n+\n+        public override async Task AddDocumentAsync(Document<T> document)\n+        {\n+            if (document == null)\n+                throw new ArgumentNullException(nameof(document));\n+\n+            await EnsureIndexExistsAsync();\n+\n+            var doc = new\n+            {\n+                id = document.Id,\n+                content = document.Content,\n+                embedding = ConvertVectorToDoubleArray(document.Embedding),\n+                metadata = document.Metadata\n+            };\n+\n+            var json = JsonSerializer.Serialize(doc);\n+            var content = new StringContent(json, Encoding.UTF8, \"application/json\");\n+\n+            var response = await _httpClient.PostAsync(\n+                $\"{_baseUrl}/{_indexName}/_doc/{document.Id}\",\n+                content);\n+\n+            response.EnsureSuccessStatusCode();\n+        }\n+\n+        public override async Task<List<Document<T>>> SearchAsync(Vector<T> queryEmbedding, int topK = 5)\n+        {\n+            if (queryEmbedding == null)\n+                throw new ArgumentNullException(nameof(queryEmbedding));\n+\n+            await EnsureIndexExistsAsync();\n+\n+            var searchQuery = new\n+            {\n+                size = topK,\n+                query = new\n+                {\n+                    script_score = new\n+                    {\n+                        query = new { match_all = new { } },\n+                        script = new\n+                        {\n+                            source = \"cosineSimilarity(params.query_vector, 'embedding') + 1.0\",\n+                            @params = new\n+                            {\n+                                query_vector = ConvertVectorToDoubleArray(queryEmbedding)\n+                            }\n+                        }\n+                    }\n+                }\n+            };\n+\n+            var json = JsonSerializer.Serialize(searchQuery);\n+            var content = new StringContent(json, Encoding.UTF8, \"application/json\");\n+\n+            var response = await _httpClient.PostAsync(\n+                $\"{_baseUrl}/{_indexName}/_search\",\n+                content);\n+\n+            response.EnsureSuccessStatusCode();\n+\n+            var responseJson = await response.Content.ReadAsStringAsync();\n+            var document = JsonDocument.Parse(responseJson);\n+            var results = new List<Document<T>>();\n+\n+            foreach (var hit in document.RootElement.GetProperty(\"hits\").GetProperty(\"hits\").EnumerateArray())\n+            {\n+                var source = hit.GetProperty(\"_source\");\n+                var doc = new Document<T>\n+                {\n+                    Id = source.GetProperty(\"id\").GetString() ?? string.Empty,\n+                    Content = source.GetProperty(\"content\").GetString() ?? string.Empty,\n+                    Metadata = JsonSerializer.Deserialize<Dictionary<string, string>>(\n+                        source.GetProperty(\"metadata\").GetRawText()) ?? new Dictionary<string, string>()\n+                };",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/ElasticsearchDocumentStore.cs",
    "commit_id": "62aea1a1b376f23b0693eb1f0ecd24a268f42be2",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Preserve metadata types when materializing documents.**\n\n`Document<T>.Metadata` is `Dictionary<string, object>`, but this code deserializes into `Dictionary<string, string>`. When filters contain numbers/bools, `MatchesFilters` in `DocumentStoreBase<T>` ends up comparing incompatible types (`int` vs `string`) and throws. Deserialize into a `Dictionary<string, object>` (or `JsonElement` and convert) so metadata retains its original types.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/ElasticsearchDocumentStore.cs\naround lines 139-141, the code currently deserializes metadata into\nDictionary<string,string> which loses original types and causes type mismatches\nwhen filtering; change the deserialization to preserve types by deserializing\ninto Dictionary<string,object> (or into Dictionary<string,JsonElement> and then\nconvert JsonElements to CLR values) and assign that to Document<T>.Metadata,\nensuring you still fall back to new Dictionary<string,object>() when null.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T13:51:08Z",
    "updated_at": "2025-11-04T13:51:10Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490601269",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490601269"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490601269"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490601269/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 139,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 141,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 141,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490601280",
    "pull_request_review_id": 3416579419,
    "id": 2490601280,
    "node_id": "PRRC_kwDOKSXUF86Uc49A",
    "diff_hunk": "@@ -0,0 +1,200 @@\n+#if NETCOREAPP || NETSTANDARD2_1_OR_GREATER\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Net.Http;\n+using System.Text;\n+using System.Text.Json;\n+using System.Threading.Tasks;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores\n+{\n+    /// <summary>\n+    /// Document store using Qdrant vector database\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations</typeparam>\n+    public class QdrantDocumentStore<T> : DocumentStoreBase<T> where T : struct, IComparable, IComparable<T>, IConvertible, IEquatable<T>, IFormattable\n+    {\n+        private readonly string _host;\n+        private readonly int _port;\n+        private readonly string _collectionName;\n+        private readonly HttpClient _httpClient;\n+        private readonly string _baseUrl;\n+\n+        public QdrantDocumentStore(string host = \"localhost\", int port = 6333, string collectionName = \"documents\")\n+        {\n+            _host = host;\n+            _port = port;\n+            _collectionName = collectionName;\n+            _httpClient = new HttpClient();\n+            _baseUrl = $\"http://{_host}:{_port}\";\n+        }\n+\n+        private async Task EnsureCollectionExistsAsync()\n+        {\n+            var response = await _httpClient.GetAsync($\"{_baseUrl}/collections/{_collectionName}\");\n+            if (response.IsSuccessStatusCode)\n+                return;\n+\n+            var createRequest = new\n+            {\n+                vectors = new\n+                {\n+                    size = 768,\n+                    distance = \"Cosine\"\n+                }\n+            };\n+\n+            var json = JsonSerializer.Serialize(createRequest);\n+            var content = new StringContent(json, Encoding.UTF8, \"application/json\");\n+\n+            await _httpClient.PutAsync($\"{_baseUrl}/collections/{_collectionName}\", content);\n+        }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/QdrantDocumentStore.cs",
    "commit_id": "62aea1a1b376f23b0693eb1f0ecd24a268f42be2",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Don't hard-code Qdrant vector size.**\n\nThis always creates the collection with 768 dimensions. Any embedding model that isn't 768-wide will make every write/search fail with a 400 from Qdrant. Pass the actual vector length from the call site when ensuring the collection instead.\n\n```diff\n-        private async Task EnsureCollectionExistsAsync()\n+        private async Task EnsureCollectionExistsAsync(int vectorDimension)\n@@\n             var createRequest = new\n             {\n                 vectors = new\n                 {\n-                    size = 768,\n+                    size = vectorDimension,\n                     distance = \"Cosine\"\n                 }\n             };\n@@\n-            await _httpClient.PutAsync($\"{_baseUrl}/collections/{_collectionName}\", content);\n+            await _httpClient.PutAsync($\"{_baseUrl}/collections/{_collectionName}\", content);\n```\nIn `AddDocumentAsync` and `SearchAsync`, call the updated helper with the actual embedding length so the schema matches the stored data.\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/QdrantDocumentStore.cs around\nlines 41-54, the collection creation JSON hard-codes vectors.size = 768 which\nbreaks writes/searches for other embedding sizes; change the EnsureCollection\nhelper to accept an int vectorSize parameter and use that value when building\nthe createRequest payload, then update AddDocumentAsync and SearchAsync to call\nEnsureCollection passing the actual embedding.Length from their call sites so\nthe Qdrant collection schema matches the embeddings stored/searched.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T13:51:08Z",
    "updated_at": "2025-11-04T13:51:10Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490601280",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490601280"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490601280"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490601280/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 41,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 54,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 54,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490601295",
    "pull_request_review_id": 3416579419,
    "id": 2490601295,
    "node_id": "PRRC_kwDOKSXUF86Uc49P",
    "diff_hunk": "@@ -0,0 +1,200 @@\n+#if NETCOREAPP || NETSTANDARD2_1_OR_GREATER\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Net.Http;\n+using System.Text;\n+using System.Text.Json;\n+using System.Threading.Tasks;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores\n+{\n+    /// <summary>\n+    /// Document store using Qdrant vector database\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations</typeparam>\n+    public class QdrantDocumentStore<T> : DocumentStoreBase<T> where T : struct, IComparable, IComparable<T>, IConvertible, IEquatable<T>, IFormattable\n+    {\n+        private readonly string _host;\n+        private readonly int _port;\n+        private readonly string _collectionName;\n+        private readonly HttpClient _httpClient;\n+        private readonly string _baseUrl;\n+\n+        public QdrantDocumentStore(string host = \"localhost\", int port = 6333, string collectionName = \"documents\")\n+        {\n+            _host = host;\n+            _port = port;\n+            _collectionName = collectionName;\n+            _httpClient = new HttpClient();\n+            _baseUrl = $\"http://{_host}:{_port}\";\n+        }\n+\n+        private async Task EnsureCollectionExistsAsync()\n+        {\n+            var response = await _httpClient.GetAsync($\"{_baseUrl}/collections/{_collectionName}\");\n+            if (response.IsSuccessStatusCode)\n+                return;\n+\n+            var createRequest = new\n+            {\n+                vectors = new\n+                {\n+                    size = 768,\n+                    distance = \"Cosine\"\n+                }\n+            };\n+\n+            var json = JsonSerializer.Serialize(createRequest);\n+            var content = new StringContent(json, Encoding.UTF8, \"application/json\");\n+\n+            await _httpClient.PutAsync($\"{_baseUrl}/collections/{_collectionName}\", content);\n+        }\n+\n+        public override async Task AddDocumentAsync(Document<T> document)\n+        {\n+            if (document == null)\n+                throw new ArgumentNullException(nameof(document));\n+\n+            await EnsureCollectionExistsAsync();\n+\n+            var point = new\n+            {\n+                points = new[]\n+                {\n+                    new\n+                    {\n+                        id = document.Id.GetHashCode(),\n+                        vector = ConvertVectorToDoubleArray(document.Embedding),\n+                        payload = new\n+                        {\n+                            id = document.Id,\n+                            content = document.Content,\n+                            metadata = document.Metadata\n+                        }\n+                    }\n+                }\n+            };\n+\n+            var json = JsonSerializer.Serialize(point);\n+            var content = new StringContent(json, Encoding.UTF8, \"application/json\");\n+\n+            var response = await _httpClient.PutAsync(\n+                $\"{_baseUrl}/collections/{_collectionName}/points\",\n+                content);\n+\n+            response.EnsureSuccessStatusCode();\n+        }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/QdrantDocumentStore.cs",
    "commit_id": "62aea1a1b376f23b0693eb1f0ecd24a268f42be2",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Stop hashing document IDs.**\n\n`string.GetHashCode()` is process-randomized in .NET, so the same document ID hashes differently between runs. That corrupts the key space (collisions) and makes deletes/gets fail after a restart. Qdrant accepts string IDs directlyΓÇöstore the canonical ID instead.\n\n```diff\n-                        id = document.Id.GetHashCode(),\n+                        id = document.Id,\n@@\n-            var deleteRequest = new\n-            {\n-                points = new[] { documentId.GetHashCode() }\n-            };\n+            var deleteRequest = new\n+            {\n+                points = new[] { documentId }\n+            };\n@@\n-                $\"{_baseUrl}/collections/{_collectionName}/points/{documentId.GetHashCode()}\");\n+                $\"{_baseUrl}/collections/{_collectionName}/points/{documentId}\");\n```\nMake the same change anywhere else the hashed value is used.\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 80630e8 to b6555ed",
    "created_at": "2025-11-04T13:51:08Z",
    "updated_at": "2025-11-05T02:31:19Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490601295",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490601295"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490601295"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490601295/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 63,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 89,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 89,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490601309",
    "pull_request_review_id": 3416579419,
    "id": 2490601309,
    "node_id": "PRRC_kwDOKSXUF86Uc49d",
    "diff_hunk": "@@ -0,0 +1,200 @@\n+#if NETCOREAPP || NETSTANDARD2_1_OR_GREATER\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Net.Http;\n+using System.Text;\n+using System.Text.Json;\n+using System.Threading.Tasks;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores\n+{\n+    /// <summary>\n+    /// Document store using Qdrant vector database\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations</typeparam>\n+    public class QdrantDocumentStore<T> : DocumentStoreBase<T> where T : struct, IComparable, IComparable<T>, IConvertible, IEquatable<T>, IFormattable\n+    {\n+        private readonly string _host;\n+        private readonly int _port;\n+        private readonly string _collectionName;\n+        private readonly HttpClient _httpClient;\n+        private readonly string _baseUrl;\n+\n+        public QdrantDocumentStore(string host = \"localhost\", int port = 6333, string collectionName = \"documents\")\n+        {\n+            _host = host;\n+            _port = port;\n+            _collectionName = collectionName;\n+            _httpClient = new HttpClient();\n+            _baseUrl = $\"http://{_host}:{_port}\";\n+        }\n+\n+        private async Task EnsureCollectionExistsAsync()\n+        {\n+            var response = await _httpClient.GetAsync($\"{_baseUrl}/collections/{_collectionName}\");\n+            if (response.IsSuccessStatusCode)\n+                return;\n+\n+            var createRequest = new\n+            {\n+                vectors = new\n+                {\n+                    size = 768,\n+                    distance = \"Cosine\"\n+                }\n+            };\n+\n+            var json = JsonSerializer.Serialize(createRequest);\n+            var content = new StringContent(json, Encoding.UTF8, \"application/json\");\n+\n+            await _httpClient.PutAsync($\"{_baseUrl}/collections/{_collectionName}\", content);\n+        }\n+\n+        public override async Task AddDocumentAsync(Document<T> document)\n+        {\n+            if (document == null)\n+                throw new ArgumentNullException(nameof(document));\n+\n+            await EnsureCollectionExistsAsync();\n+\n+            var point = new\n+            {\n+                points = new[]\n+                {\n+                    new\n+                    {\n+                        id = document.Id.GetHashCode(),\n+                        vector = ConvertVectorToDoubleArray(document.Embedding),\n+                        payload = new\n+                        {\n+                            id = document.Id,\n+                            content = document.Content,\n+                            metadata = document.Metadata\n+                        }\n+                    }\n+                }\n+            };\n+\n+            var json = JsonSerializer.Serialize(point);\n+            var content = new StringContent(json, Encoding.UTF8, \"application/json\");\n+\n+            var response = await _httpClient.PutAsync(\n+                $\"{_baseUrl}/collections/{_collectionName}/points\",\n+                content);\n+\n+            response.EnsureSuccessStatusCode();\n+        }\n+\n+        public override async Task<List<Document<T>>> SearchAsync(Vector<T> queryEmbedding, int topK = 5)\n+        {\n+            if (queryEmbedding == null)\n+                throw new ArgumentNullException(nameof(queryEmbedding));\n+\n+            await EnsureCollectionExistsAsync();\n+\n+            var searchRequest = new\n+            {\n+                vector = ConvertVectorToDoubleArray(queryEmbedding),\n+                limit = topK,\n+                with_payload = true\n+            };\n+\n+            var json = JsonSerializer.Serialize(searchRequest);\n+            var content = new StringContent(json, Encoding.UTF8, \"application/json\");\n+\n+            var response = await _httpClient.PostAsync(\n+                $\"{_baseUrl}/collections/{_collectionName}/points/search\",\n+                content);\n+\n+            response.EnsureSuccessStatusCode();\n+\n+            var responseJson = await response.Content.ReadAsStringAsync();\n+            var document = JsonDocument.Parse(responseJson);\n+            var results = new List<Document<T>>();\n+\n+            foreach (var result in document.RootElement.GetProperty(\"result\").EnumerateArray())\n+            {\n+                var payload = result.GetProperty(\"payload\");\n+                var doc = new Document<T>\n+                {\n+                    Id = payload.GetProperty(\"id\").GetString() ?? string.Empty,\n+                    Content = payload.GetProperty(\"content\").GetString() ?? string.Empty,\n+                    Metadata = JsonSerializer.Deserialize<Dictionary<string, string>>(\n+                        payload.GetProperty(\"metadata\").GetRawText()) ?? new Dictionary<string, string>()\n+                };\n+\n+                results.Add(doc);\n+            }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/QdrantDocumentStore.cs",
    "commit_id": "62aea1a1b376f23b0693eb1f0ecd24a268f42be2",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Handle non-string metadata correctly.**\n\n`Document.Metadata` is `Dictionary<string, object>`. Deserializing into `Dictionary<string, string>` blows up as soon as a value is numeric/boolean/array. Deserialize into a JSON element or `Dictionary<string, object?>` so heterogeneous metadata survives the round trip. Apply the same fix in `GetDocumentAsync`.\n\n```diff\n-                    Metadata = JsonSerializer.Deserialize<Dictionary<string, string>>(\n-                        payload.GetProperty(\"metadata\").GetRawText()) ?? new Dictionary<string, string>()\n+                    Metadata = JsonSerializer.Deserialize<Dictionary<string, object?>>(\n+                        payload.GetProperty(\"metadata\").GetRawText()) ?? new Dictionary<string, object?>()\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/QdrantDocumentStore.cs around\nlines 118 to 130, the code deserializes payload metadata into\nDictionary<string,string> which fails for non-string values; change the\ndeserialization target to Dictionary<string, object?> or Dictionary<string,\nJsonElement> (or deserialize to JsonDocument/JsonElement and convert) so\nheterogeneous types (numbers, booleans, arrays, objects) are preserved, update\nthe Metadata property assignment accordingly, and apply the identical change in\nGetDocumentAsync to ensure both bulk and single-document paths handle mixed-type\nmetadata.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 80630e8 to b6555ed",
    "created_at": "2025-11-04T13:51:08Z",
    "updated_at": "2025-11-05T02:31:22Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490601309",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490601309"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490601309"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490601309/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 118,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 130,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 130,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490601316",
    "pull_request_review_id": 3416579419,
    "id": 2490601316,
    "node_id": "PRRC_kwDOKSXUF86Uc49k",
    "diff_hunk": "@@ -0,0 +1,200 @@\n+#if NETCOREAPP || NETSTANDARD2_1_OR_GREATER\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Net.Http;\n+using System.Text;\n+using System.Text.Json;\n+using System.Threading.Tasks;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores\n+{\n+    /// <summary>\n+    /// Document store using Qdrant vector database\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations</typeparam>\n+    public class QdrantDocumentStore<T> : DocumentStoreBase<T> where T : struct, IComparable, IComparable<T>, IConvertible, IEquatable<T>, IFormattable\n+    {\n+        private readonly string _host;\n+        private readonly int _port;\n+        private readonly string _collectionName;\n+        private readonly HttpClient _httpClient;\n+        private readonly string _baseUrl;\n+\n+        public QdrantDocumentStore(string host = \"localhost\", int port = 6333, string collectionName = \"documents\")\n+        {\n+            _host = host;\n+            _port = port;\n+            _collectionName = collectionName;\n+            _httpClient = new HttpClient();\n+            _baseUrl = $\"http://{_host}:{_port}\";\n+        }\n+\n+        private async Task EnsureCollectionExistsAsync()\n+        {\n+            var response = await _httpClient.GetAsync($\"{_baseUrl}/collections/{_collectionName}\");\n+            if (response.IsSuccessStatusCode)\n+                return;\n+\n+            var createRequest = new\n+            {\n+                vectors = new\n+                {\n+                    size = 768,\n+                    distance = \"Cosine\"\n+                }\n+            };\n+\n+            var json = JsonSerializer.Serialize(createRequest);\n+            var content = new StringContent(json, Encoding.UTF8, \"application/json\");\n+\n+            await _httpClient.PutAsync($\"{_baseUrl}/collections/{_collectionName}\", content);\n+        }\n+\n+        public override async Task AddDocumentAsync(Document<T> document)\n+        {\n+            if (document == null)\n+                throw new ArgumentNullException(nameof(document));\n+\n+            await EnsureCollectionExistsAsync();\n+\n+            var point = new\n+            {\n+                points = new[]\n+                {\n+                    new\n+                    {\n+                        id = document.Id.GetHashCode(),\n+                        vector = ConvertVectorToDoubleArray(document.Embedding),\n+                        payload = new\n+                        {\n+                            id = document.Id,\n+                            content = document.Content,\n+                            metadata = document.Metadata\n+                        }\n+                    }\n+                }\n+            };\n+\n+            var json = JsonSerializer.Serialize(point);\n+            var content = new StringContent(json, Encoding.UTF8, \"application/json\");\n+\n+            var response = await _httpClient.PutAsync(\n+                $\"{_baseUrl}/collections/{_collectionName}/points\",\n+                content);\n+\n+            response.EnsureSuccessStatusCode();\n+        }\n+\n+        public override async Task<List<Document<T>>> SearchAsync(Vector<T> queryEmbedding, int topK = 5)\n+        {\n+            if (queryEmbedding == null)\n+                throw new ArgumentNullException(nameof(queryEmbedding));\n+\n+            await EnsureCollectionExistsAsync();\n+\n+            var searchRequest = new\n+            {\n+                vector = ConvertVectorToDoubleArray(queryEmbedding),\n+                limit = topK,\n+                with_payload = true\n+            };\n+\n+            var json = JsonSerializer.Serialize(searchRequest);\n+            var content = new StringContent(json, Encoding.UTF8, \"application/json\");\n+\n+            var response = await _httpClient.PostAsync(\n+                $\"{_baseUrl}/collections/{_collectionName}/points/search\",\n+                content);\n+\n+            response.EnsureSuccessStatusCode();\n+\n+            var responseJson = await response.Content.ReadAsStringAsync();\n+            var document = JsonDocument.Parse(responseJson);\n+            var results = new List<Document<T>>();\n+\n+            foreach (var result in document.RootElement.GetProperty(\"result\").EnumerateArray())\n+            {\n+                var payload = result.GetProperty(\"payload\");\n+                var doc = new Document<T>\n+                {\n+                    Id = payload.GetProperty(\"id\").GetString() ?? string.Empty,\n+                    Content = payload.GetProperty(\"content\").GetString() ?? string.Empty,\n+                    Metadata = JsonSerializer.Deserialize<Dictionary<string, string>>(\n+                        payload.GetProperty(\"metadata\").GetRawText()) ?? new Dictionary<string, string>()\n+                };\n+\n+                results.Add(doc);\n+            }\n+\n+            return results;\n+        }\n+\n+        public override async Task DeleteDocumentAsync(string documentId)\n+        {\n+            if (string.IsNullOrEmpty(documentId))\n+                throw new ArgumentException(\"Document ID cannot be null or empty\", nameof(documentId));\n+\n+            var deleteRequest = new\n+            {\n+                points = new[] { documentId.GetHashCode() }\n+            };\n+\n+            var json = JsonSerializer.Serialize(deleteRequest);\n+            var content = new StringContent(json, Encoding.UTF8, \"application/json\");\n+\n+            var response = await _httpClient.PostAsync(\n+                $\"{_baseUrl}/collections/{_collectionName}/points/delete\",\n+                content);\n+\n+            response.EnsureSuccessStatusCode();\n+        }\n+\n+        public override async Task<Document<T>?> GetDocumentAsync(string documentId)\n+        {\n+            if (string.IsNullOrEmpty(documentId))\n+                throw new ArgumentException(\"Document ID cannot be null or empty\", nameof(documentId));\n+\n+            var response = await _httpClient.GetAsync(\n+                $\"{_baseUrl}/collections/{_collectionName}/points/{documentId.GetHashCode()}\");\n+\n+            if (!response.IsSuccessStatusCode)\n+                return null;\n+\n+            var json = await response.Content.ReadAsStringAsync();\n+            var element = JsonDocument.Parse(json).RootElement;\n+            \n+            var payload = element.GetProperty(\"result\").GetProperty(\"payload\");\n+            return new Document<T>\n+            {\n+                Id = payload.GetProperty(\"id\").GetString() ?? string.Empty,\n+                Content = payload.GetProperty(\"content\").GetString() ?? string.Empty,\n+                Metadata = JsonSerializer.Deserialize<Dictionary<string, string>>(\n+                    payload.GetProperty(\"metadata\").GetRawText()) ?? new Dictionary<string, string>()\n+            };\n+        }\n+\n+        private double[] ConvertVectorToDoubleArray(Vector<T> vector)\n+        {\n+            var result = new double[vector.Length];\n+            for (int i = 0; i < vector.Length; i++)\n+            {\n+                result[i] = Convert.ToDouble(vector[i]);\n+            }\n+            return result;\n+        }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/QdrantDocumentStore.cs",
    "commit_id": "62aea1a1b376f23b0693eb1f0ecd24a268f42be2",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Guard against missing embeddings.**\n\n`ConvertVectorToDoubleArray(document.Embedding)` will throw a `NullReferenceException` whenever a document arrives without an embedding (which the public API still allows). Fail fast with a clear argument check before touching `vector.Length`.\n\n```diff\n-        private double[] ConvertVectorToDoubleArray(Vector<T> vector)\n+        private double[] ConvertVectorToDoubleArray(Vector<T> vector)\n         {\n+            if (vector == null)\n+                throw new ArgumentNullException(nameof(vector));\n+\n             var result = new double[vector.Length];\n             for (int i = 0; i < vector.Length; i++)\n             {\n                 result[i] = Convert.ToDouble(vector[i]);\n             }\n             return result;\n         }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        private double[] ConvertVectorToDoubleArray(Vector<T> vector)\n        {\n            if (vector == null)\n                throw new ArgumentNullException(nameof(vector));\n\n            var result = new double[vector.Length];\n            for (int i = 0; i < vector.Length; i++)\n            {\n                result[i] = Convert.ToDouble(vector[i]);\n            }\n            return result;\n        }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/QdrantDocumentStore.cs around\nlines 179 to 187, add a guard at the top of ConvertVectorToDoubleArray to check\nfor a null vector and throw an ArgumentNullException (using nameof(vector))\nbefore accessing vector.Length; then proceed with the existing conversion loop.\nEnsure the exception message is clear (use the standard ArgumentNullException\nbehavior) so callers fail fast when a document has no embedding.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T13:51:09Z",
    "updated_at": "2025-11-04T13:51:10Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490601316",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490601316"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490601316"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490601316/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 179,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 187,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 187,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490601329",
    "pull_request_review_id": 3416579419,
    "id": 2490601329,
    "node_id": "PRRC_kwDOKSXUF86Uc49x",
    "diff_hunk": "@@ -0,0 +1,96 @@\n+using AiDotNet.LinearAlgebra;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Threading.Tasks;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels\n+{\n+    /// <summary>\n+    /// Multi-modal embedding model that combines text and other modalities\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for embeddings</typeparam>\n+    public class MultiModalEmbeddingModel<T> : EmbeddingModelBase<T> where T : struct, IComparable, IComparable<T>, IConvertible, IEquatable<T>, IFormattable\n+    {\n+        private readonly IEmbeddingModel<T> _textEmbedder;\n+        private readonly Dictionary<string, Func<object, Task<Vector<T>>>> _modalityEmbedders;\n+        private readonly Func<List<Vector<T>>, Vector<T>> _fusionStrategy;\n+\n+        public MultiModalEmbeddingModel(\n+            IEmbeddingModel<T> textEmbedder,\n+            Func<List<Vector<T>>, Vector<T>>? fusionStrategy = null,\n+            INormalizer<T>? normalizer = null)\n+            : base(normalizer)\n+        {\n+            _textEmbedder = textEmbedder ?? throw new ArgumentNullException(nameof(textEmbedder));\n+            _modalityEmbedders = new Dictionary<string, Func<object, Task<Vector<T>>>>();\n+            _fusionStrategy = fusionStrategy ?? DefaultFusion;\n+        }",
    "path": "src/RetrievalAugmentedGeneration/EmbeddingModels/MultiModalEmbeddingModel.cs",
    "commit_id": "33a79e89f3137dc80c940c48d7ab74f127dd4d91",
    "original_commit_id": "602cbe2c5622c74ada0ae3dc576d5d45f9130d35",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Restore the base embedding contract.**\n\n`EmbeddingModelBase<T>` declares `EmbeddingDimension` and `MaxTokens` as abstract. Without overriding them here, the class remains abstract and the build fails. Mirror the underlying text embedder so this concrete model compiles.\n\n```diff\n@@\n         public MultiModalEmbeddingModel(\n             IEmbeddingModel<T> textEmbedder,\n             Func<List<Vector<T>>, Vector<T>>? fusionStrategy = null,\n             INormalizer<T>? normalizer = null)\n             : base(normalizer)\n         {\n             _textEmbedder = textEmbedder ?? throw new ArgumentNullException(nameof(textEmbedder));\n             _modalityEmbedders = new Dictionary<string, Func<object, Task<Vector<T>>>>();\n             _fusionStrategy = fusionStrategy ?? DefaultFusion;\n         }\n+\n+        public override int EmbeddingDimension => _textEmbedder.EmbeddingDimension;\n+\n+        public override int MaxTokens => _textEmbedder.MaxTokens;\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        public MultiModalEmbeddingModel(\n            IEmbeddingModel<T> textEmbedder,\n            Func<List<Vector<T>>, Vector<T>>? fusionStrategy = null,\n            INormalizer<T>? normalizer = null)\n            : base(normalizer)\n        {\n            _textEmbedder = textEmbedder ?? throw new ArgumentNullException(nameof(textEmbedder));\n            _modalityEmbedders = new Dictionary<string, Func<object, Task<Vector<T>>>>();\n            _fusionStrategy = fusionStrategy ?? DefaultFusion;\n        }\n\n        public override int EmbeddingDimension => _textEmbedder.EmbeddingDimension;\n\n        public override int MaxTokens => _textEmbedder.MaxTokens;\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/EmbeddingModels/MultiModalEmbeddingModel.cs\naround lines 19 to 28, the class is missing overrides for the abstract\nproperties EmbeddingDimension and MaxTokens from EmbeddingModelBase<T>, leaving\nthe class abstract; implement public override int EmbeddingDimension =>\n_textEmbedder.EmbeddingDimension and public override int MaxTokens =>\n_textEmbedder.MaxTokens (mirror the underlying text embedder), ensuring the\nproperties use the existing _textEmbedder field and proper override signatures\nso the class becomes concrete and compiles.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits e0f1f5e to bc2e366",
    "created_at": "2025-11-04T13:51:09Z",
    "updated_at": "2025-11-04T14:51:07Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490601329",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490601329"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490601329"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490601329/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 19,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 28,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 28,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490648970",
    "pull_request_review_id": 3416647316,
    "id": 2490648970,
    "node_id": "PRRC_kwDOKSXUF86UdEmK",
    "diff_hunk": "@@ -0,0 +1,49 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Retrievers;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.AdvancedPatterns\n+{\n+    public class FLARERetriever<T> : RetrieverBase<T>\n+    {\n+        private readonly IRetriever<T> _baseRetriever;\n+        private readonly int _activeRetrievalSteps;\n+\n+        public FLARERetriever(IRetriever<T> baseRetriever, int activeRetrievalSteps = 3)\n+        {\n+            _baseRetriever = baseRetriever ?? throw new System.ArgumentNullException(nameof(baseRetriever));\n+            _activeRetrievalSteps = activeRetrievalSteps > 0 ? activeRetrievalSteps : throw new System.ArgumentOutOfRangeException(nameof(activeRetrievalSteps));\n+        }\n+\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\n+        {\n+            var allResults = new List<Document<T>>();\n+            var currentQuery = query;\n+\n+            for (int step = 0; step < _activeRetrievalSteps; step++)\n+            {\n+                var stepResults = _baseRetriever.Retrieve(currentQuery, topK, metadataFilters).ToList();\n+                \n+                foreach (var doc in stepResults)\n+                {\n+                    if (!allResults.Any(d => d.Id == doc.Id))\n+                    {\n+                        allResults.Add(doc);\n+                    }\n+                }\n+\n+                if (stepResults.Count > 0)\n+                {\n+                    var topDoc = stepResults.OrderByDescending(d => Convert.ToDouble(d.RelevanceScore)).First();\n+                    currentQuery = $\"{query} {topDoc.Content.Substring(0, System.Math.Min(200, topDoc.Content.Length))}\";\n+                }\n+            }\n+\n+            return allResults.OrderByDescending(d => Convert.ToDouble(d.RelevanceScore)).Take(topK).ToList();",
    "path": "src/RetrievalAugmentedGeneration/AdvancedPatterns/FLARERetriever.cs",
    "commit_id": "33a79e89f3137dc80c940c48d7ab74f127dd4d91",
    "original_commit_id": "33a79e89f3137dc80c940c48d7ab74f127dd4d91",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Stop using `Convert.ToDouble` on generic scores.**\n\n`Convert.ToDouble` will throw for numeric types we just enabled (e.g., `Complex<T>`), breaking FLARE retrieval at runtime. Use the numeric-ops adapter exposed by `RetrieverBase<T>`.\n\n```diff\n-                if (stepResults.Count > 0)\n-                {\n-                    var topDoc = stepResults.OrderByDescending(d => Convert.ToDouble(d.RelevanceScore)).First();\n-                    currentQuery = $\"{query} {topDoc.Content.Substring(0, System.Math.Min(200, topDoc.Content.Length))}\";\n-                }\n+                if (stepResults.Count > 0)\n+                {\n+                    var topDoc = stepResults\n+                        .OrderByDescending(d => NumOps.ToDouble(d.RelevanceScore))\n+                        .First();\n+                    currentQuery = $\"{query} {topDoc.Content.Substring(0, System.Math.Min(200, topDoc.Content.Length))}\";\n+                }\n@@\n-            return allResults.OrderByDescending(d => Convert.ToDouble(d.RelevanceScore)).Take(topK).ToList();\n+            return allResults\n+                .OrderByDescending(d => NumOps.ToDouble(d.RelevanceScore))\n+                .Take(topK)\n+                .ToList();\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/AdvancedPatterns/FLARERetriever.cs around\nlines 38 to 45, don't call Convert.ToDouble on the generic RelevanceScore (it\nfails for new numeric types like Complex<T>); instead use the numeric-ops\nadapter exposed by RetrieverBase<T> to convert/compare scores (e.g., use the\nRetrieverBase<T>.NumericOps/NumberOps helper to get a double or to compare\nvalues) when ordering and selecting top documents so the code works for all\nsupported numeric types.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T14:03:04Z",
    "updated_at": "2025-11-04T14:03:05Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490648970",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490648970"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490648970"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490648970/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 38,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 45,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 45,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490648984",
    "pull_request_review_id": 3416647316,
    "id": 2490648984,
    "node_id": "PRRC_kwDOKSXUF86UdEmY",
    "diff_hunk": "@@ -0,0 +1,75 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Retrievers;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.AdvancedPatterns\n+{\n+    public class GraphRAG<T> : RetrieverBase<T>\n+    {\n+        private readonly IRetriever<T> _baseRetriever;\n+        private readonly Dictionary<string, List<string>> _documentGraph;\n+        private readonly int _traversalDepth;\n+\n+        public GraphRAG(IRetriever<T> baseRetriever, int traversalDepth = 2)\n+        {\n+            _baseRetriever = baseRetriever ?? throw new System.ArgumentNullException(nameof(baseRetriever));\n+            _documentGraph = new Dictionary<string, List<string>>();\n+            _traversalDepth = traversalDepth > 0 ? traversalDepth : throw new System.ArgumentOutOfRangeException(nameof(traversalDepth));\n+        }\n+\n+        public void AddEdge(string sourceDocId, string targetDocId)\n+        {\n+            if (!_documentGraph.ContainsKey(sourceDocId))\n+            {\n+                _documentGraph[sourceDocId] = new List<string>();\n+            }\n+            \n+            if (!_documentGraph[sourceDocId].Contains(targetDocId))\n+            {\n+                _documentGraph[sourceDocId].Add(targetDocId);\n+            }\n+        }\n+\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\n+        {\n+            var initialResults = _baseRetriever.Retrieve(query, topK, metadataFilters).ToList();\n+            var expandedResults = new Dictionary<string, Document<T>>();\n+\n+            foreach (var doc in initialResults)\n+            {\n+                expandedResults[doc.Id] = doc;\n+                TraverseGraph(doc.Id, _traversalDepth, expandedResults, metadataFilters);\n+            }\n+\n+            return expandedResults.Values\n+                .OrderByDescending(d => Convert.ToDouble(d.RelevanceScore))\n+                .Take(topK)",
    "path": "src/RetrievalAugmentedGeneration/AdvancedPatterns/GraphRAG.cs",
    "commit_id": "33a79e89f3137dc80c940c48d7ab74f127dd4d91",
    "original_commit_id": "33a79e89f3137dc80c940c48d7ab74f127dd4d91",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Use `NumOps.ToDouble` when ordering graph results.**\n\nDirect `Convert.ToDouble` calls will crash for non-`IConvertible` score types. Leverage the numeric operations facade.\n\n```diff\n-            return expandedResults.Values\n-                .OrderByDescending(d => Convert.ToDouble(d.RelevanceScore))\n+            return expandedResults.Values\n+                .OrderByDescending(d => NumOps.ToDouble(d.RelevanceScore))\n                 .Take(topK)\n                 .ToList();\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n            return expandedResults.Values\n                .OrderByDescending(d => NumOps.ToDouble(d.RelevanceScore))\n                .Take(topK)\n                .ToList();\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/AdvancedPatterns/GraphRAG.cs around lines\n47-49, replace the Convert.ToDouble call used for ordering with the numeric\nfacade NumOps.ToDouble to avoid crashes for non-IConvertible score types; update\nthe LINQ to OrderByDescending(d => NumOps.ToDouble(d.RelevanceScore)), and add\nthe required using or fully-qualified reference to NumOps if not already\nimported so the code compiles.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T14:03:04Z",
    "updated_at": "2025-11-04T14:03:05Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490648984",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490648984"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490648984"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490648984/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 47,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 49,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 49,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490648998",
    "pull_request_review_id": 3416647316,
    "id": 2490648998,
    "node_id": "PRRC_kwDOKSXUF86UdEmm",
    "diff_hunk": "@@ -0,0 +1,75 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Retrievers;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.AdvancedPatterns\n+{\n+    public class GraphRAG<T> : RetrieverBase<T>\n+    {\n+        private readonly IRetriever<T> _baseRetriever;\n+        private readonly Dictionary<string, List<string>> _documentGraph;\n+        private readonly int _traversalDepth;\n+\n+        public GraphRAG(IRetriever<T> baseRetriever, int traversalDepth = 2)\n+        {\n+            _baseRetriever = baseRetriever ?? throw new System.ArgumentNullException(nameof(baseRetriever));\n+            _documentGraph = new Dictionary<string, List<string>>();\n+            _traversalDepth = traversalDepth > 0 ? traversalDepth : throw new System.ArgumentOutOfRangeException(nameof(traversalDepth));\n+        }\n+\n+        public void AddEdge(string sourceDocId, string targetDocId)\n+        {\n+            if (!_documentGraph.ContainsKey(sourceDocId))\n+            {\n+                _documentGraph[sourceDocId] = new List<string>();\n+            }\n+            \n+            if (!_documentGraph[sourceDocId].Contains(targetDocId))\n+            {\n+                _documentGraph[sourceDocId].Add(targetDocId);\n+            }\n+        }\n+\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\n+        {\n+            var initialResults = _baseRetriever.Retrieve(query, topK, metadataFilters).ToList();\n+            var expandedResults = new Dictionary<string, Document<T>>();\n+\n+            foreach (var doc in initialResults)\n+            {\n+                expandedResults[doc.Id] = doc;\n+                TraverseGraph(doc.Id, _traversalDepth, expandedResults, metadataFilters);\n+            }\n+\n+            return expandedResults.Values\n+                .OrderByDescending(d => Convert.ToDouble(d.RelevanceScore))\n+                .Take(topK)\n+                .ToList();\n+        }\n+\n+        private void TraverseGraph(string docId, int depth, Dictionary<string, Document<T>> results, Dictionary<string, object> metadataFilters)\n+        {\n+            if (depth <= 0 || !_documentGraph.ContainsKey(docId))\n+            {\n+                return;\n+            }\n+\n+            foreach (var neighborId in _documentGraph[docId])\n+            {\n+                if (!results.ContainsKey(neighborId))\n+                {\n+                    var neighborDocs = _baseRetriever.Retrieve(neighborId, 1, metadataFilters).ToList();\n+                    if (neighborDocs.Count > 0)\n+                    {\n+                        results[neighborId] = neighborDocs[0];\n+                        TraverseGraph(neighborId, depth - 1, results, metadataFilters);",
    "path": "src/RetrievalAugmentedGeneration/AdvancedPatterns/GraphRAG.cs",
    "commit_id": "33a79e89f3137dc80c940c48d7ab74f127dd4d91",
    "original_commit_id": "33a79e89f3137dc80c940c48d7ab74f127dd4d91",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Fetching neighbors by document ID is incorrect.**\n\nDuring expansion you call `_baseRetriever.Retrieve(neighborId, ΓÇª)` with the neighborΓÇÖs ID as the query. Retrievers expect natural-language queries, so this almost never returns the intended neighbor document and can even introduce unrelated results. Pass an `IDocumentStore<T>` (or similar) into `GraphRAG<T>` and look up neighbors by ID, or cache the full `Document<T>` objects when edges are created. Until then the traversal logic canΓÇÖt reliably surface linked documents.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/AdvancedPatterns/GraphRAG.cs around lines 60\nto 68, the traversal currently calls _baseRetriever.Retrieve(neighborId, ...)\nusing the neighbor document ID as a natural-language query which is incorrect;\nchange the implementation to retrieve neighbors by ID from a document store or\ncache instead of using the retriever: add an IDocumentStore<T> (or similar)\ndependency to GraphRAG<T> (or accept a document cache populated when edges are\ncreated), use that store/cache to look up the full Document<T> by neighborId\ninside TraverseGraph, and only fall back to the retriever for content-based\nqueries; update constructors and tests accordingly so traversal surfaces the\nactual linked Document<T> objects rather than passing IDs as queries.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T14:03:04Z",
    "updated_at": "2025-11-04T14:03:05Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490648998",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490648998"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490648998"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490648998/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 60,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 68,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 68,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490649008",
    "pull_request_review_id": 3416647316,
    "id": 2490649008,
    "node_id": "PRRC_kwDOKSXUF86UdEmw",
    "diff_hunk": "@@ -0,0 +1,67 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Retrievers;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.AdvancedPatterns\n+{\n+    public class SelfCorrectingRetriever<T> : RetrieverBase<T>\n+    {\n+        private readonly IRetriever<T> _baseRetriever;\n+        private readonly int _maxCorrectionAttempts;\n+        private readonly T _relevanceThreshold;\n+\n+        public SelfCorrectingRetriever(IRetriever<T> baseRetriever, T relevanceThreshold, int maxCorrectionAttempts = 3)\n+        {\n+            _baseRetriever = baseRetriever ?? throw new System.ArgumentNullException(nameof(baseRetriever));\n+            _relevanceThreshold = relevanceThreshold;\n+            _maxCorrectionAttempts = maxCorrectionAttempts > 0 ? maxCorrectionAttempts : throw new System.ArgumentOutOfRangeException(nameof(maxCorrectionAttempts));\n+        }\n+\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\n+        {\n+            var results = _baseRetriever.Retrieve(query, topK, metadataFilters).ToList();\n+            var attempts = 0;\n+\n+            while (attempts < _maxCorrectionAttempts && NeedsCorrection(results))\n+            {\n+                var refinedQuery = RefineQuery(query, results);\n+                results = _baseRetriever.Retrieve(refinedQuery, topK, metadataFilters).ToList();\n+                attempts++;\n+            }\n+\n+            return results;\n+        }\n+\n+        private bool NeedsCorrection(List<Document<T>> documents)\n+        {\n+            if (documents == null || documents.Count == 0)\n+            {\n+                return true;\n+            }\n+\n+            var avgScore = NumOps.Divide(\n+                documents.Aggregate(NumOps.Zero, (acc, doc) => NumOps.Add(acc, doc.RelevanceScore)),\n+                NumOps.FromDouble(documents.Count)\n+            );\n+\n+            return NumOps.LessThan(avgScore, _relevanceThreshold);\n+        }\n+\n+        private string RefineQuery(string originalQuery, List<Document<T>> previousResults)\n+        {\n+            if (previousResults == null || previousResults.Count == 0)\n+            {\n+                return originalQuery;\n+            }\n+\n+            var topDoc = previousResults.OrderByDescending(d => Convert.ToDouble(d.RelevanceScore)).First();\n+            var snippet = topDoc.Content.Substring(0, System.Math.Min(100, topDoc.Content.Length));\n+            \n+            return $\"{originalQuery} {snippet}\";\n+        }",
    "path": "src/RetrievalAugmentedGeneration/AdvancedPatterns/SelfCorrectingRetriever.cs",
    "commit_id": "33a79e89f3137dc80c940c48d7ab74f127dd4d91",
    "original_commit_id": "33a79e89f3137dc80c940c48d7ab74f127dd4d91",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Use `NumOps.ToDouble` when ranking corrections.**\n\n`Convert.ToDouble` fails for supported numeric types such as `Complex<T>`, so the self-correction loop explodes at runtime. Stick to the numeric-operations abstraction.\n\n```diff\n-            var topDoc = previousResults.OrderByDescending(d => Convert.ToDouble(d.RelevanceScore)).First();\n+            var topDoc = previousResults\n+                .OrderByDescending(d => NumOps.ToDouble(d.RelevanceScore))\n+                .First();\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        private string RefineQuery(string originalQuery, List<Document<T>> previousResults)\n        {\n            if (previousResults == null || previousResults.Count == 0)\n            {\n                return originalQuery;\n            }\n\n            var topDoc = previousResults\n                .OrderByDescending(d => NumOps.ToDouble(d.RelevanceScore))\n                .First();\n            var snippet = topDoc.Content.Substring(0, System.Math.Min(100, topDoc.Content.Length));\n            \n            return $\"{originalQuery} {snippet}\";\n        }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/AdvancedPatterns/SelfCorrectingRetriever.cs\naround lines 53 to 64, the code uses Convert.ToDouble(d.RelevanceScore) to rank\ndocuments which fails for numeric types like Complex<T>; replace\nConvert.ToDouble with the numeric-operations abstraction call\nNumOps.ToDouble(d.RelevanceScore) (or the proper fully-qualified NumOps method\nin scope) so all supported numeric types convert safely, and add the necessary\nusing or qualifier if NumOps is not currently imported.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T14:03:05Z",
    "updated_at": "2025-11-04T14:03:05Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490649008",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490649008"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490649008"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490649008/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 53,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 64,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 64,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721618",
    "pull_request_review_id": 3416751163,
    "id": 2490721618,
    "node_id": "PRRC_kwDOKSXUF86UdWVS",
    "diff_hunk": "@@ -41,11 +41,20 @@\n \t</ItemGroup>\n \n \t<ItemGroup>\n+\t  <PackageReference Include=\"Azure.Search.Documents\" Version=\"11.7.0\" />\n+\t  <PackageReference Include=\"Elastic.Clients.Elasticsearch\" Version=\"9.2.1\" />\n \t  <PackageReference Include=\"Microsoft.CSharp\" Version=\"4.7.0\" />\n \t  <PackageReference Include=\"Newtonsoft.Json\" Version=\"13.0.4\" />\n+\t  <PackageReference Include=\"Pinecone.Client\" Version=\"4.0.2\" />\n+\t  <PackageReference Include=\"StackExchange.Redis\" Version=\"2.9.32\" />\n \t  <PackageReference Include=\"System.ValueTuple\" Version=\"4.6.1\" />\n \t</ItemGroup>\n \n+\t<!-- PostgreSQL package only for .NET 8.0+ -->\n+\t<ItemGroup Condition=\"'$(TargetFramework)' == 'net8.0'\">\n+\t  <PackageReference Include=\"Npgsql.EntityFrameworkCore.PostgreSQL\" Version=\"9.0.4\" />\n+\t</ItemGroup>",
    "path": "src/AiDotNet.csproj",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "5e2879cb116daba6f58fb4a68fc7f8d434e60eb4",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n<details>\n<summary>≡ƒº⌐ Analysis chain</summary>\n\n**Verify dependency versions for compatibility, security, and completeness.**\n\nThe added packages support key RAG implementations (Pinecone, PostgreSQL pgvector, Azure, Elasticsearch, Redis). However, several considerations require verification:\n\n1. **Version currency and security**: Given the knowledge cutoffΓÇôcurrent date gap (March 2025 ΓåÆ November 2025), these versions may be outdated or have known security advisories.\n2. **Framework compatibility**: Verify that all added dependencies are compatible with both `net8.0` and `net462` targets, especially the new packages.\n3. **Missing vector database clients**: The PR implements FAISSDocumentStore, MilvusDocumentStore, and WeaviateDocumentStore, but no corresponding NuGet packages are referenced. Confirm whether these are intentional omissions (e.g., wrapped internally, external implementations, or deferred).\n4. **Conditional targeting**: The PostgreSQL package is appropriately scoped to `net8.0` only (line 54), which is correct since newer EF Core versions target .NET 8+.\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Verify that all added packages support multi-targeting (net8.0 and net462)\n# Search for any package-specific target framework constraints or compatibility notes\n\nfd --type f '\\.cs$' --exec grep -l 'FAISS\\|Milvus\\|Weaviate' {} \\;\n```\n\nLength of output: 294\n\n---\n\n≡ƒîÉ Web query:\n\n```web\nWhat are the latest stable versions of Pinecone.Client, Elastic.Clients.Elasticsearch, Azure.Search.Documents, and Npgsql.EntityFrameworkCore.PostgreSQL as of November 2025?\n```\n\n≡ƒÆí Result:\n\nLatest stable versions as of 2025-11-04:\n\n- Pinecone.Client ΓÇö 4.0.2. [1]  \n- Elastic.Clients.Elasticsearch ΓÇö 9.2.0. [2]  \n- Azure.Search.Documents ΓÇö 11.6.1. [3]  \n- Npgsql.EntityFrameworkCore.PostgreSQL ΓÇö 9.0.4. [4]\n\nSources:  \n[1] nuget.org/Pinecone.Client (package profile). ([nuget.org](https://www.nuget.org/profiles/Pinecone.io?utm_source=openai))  \n[2] nuget.org/Elastic.Clients.Elasticsearch (package page showing latest). ([nuget.org](https://www.nuget.org/packages/Elastic.Clients.Elasticsearch?utm_source=openai))  \n[3] nuget.org/Azure.Search.Documents (package page for 11.6.1). ([nuget.org](https://www.nuget.org/packages/Azure.Search.Documents/11.6.1?utm_source=openai))  \n[4] nuget.org/Npgsql.EntityFrameworkCore.PostgreSQL (package page; 9.0.4 stable; newer prereleases listed). ([nuget.org](https://www.nuget.org/packages/Npgsql.EntityFrameworkCore.PostgreSQL))\n\nCitations:\n\n- 1: https://www.nuget.org/profiles/Pinecone.io?utm_source=openai\n- 2: https://www.nuget.org/packages/Elastic.Clients.Elasticsearch?utm_source=openai\n- 3: https://www.nuget.org/packages/Azure.Search.Documents/11.6.1?utm_source=openai\n- 4: https://www.nuget.org/packages/Npgsql.EntityFrameworkCore.PostgreSQL\n\n---\n\n≡ƒîÉ Web query:\n\n```web\nAre there known security vulnerabilities or compatibility issues with Pinecone.Client 4.0.2, Elastic.Clients.Elasticsearch 9.2.1, Azure.Search.Documents 11.7.0, or StackExchange.Redis 2.9.32?\n```\n\n≡ƒÆí Result:\n\nShort answer ΓÇö no widely reported CVEs specific to those client libraries themselves, but there are compatibility notes and important server/service-side vulnerabilities or breaking changes you should watch:\n\n- Pinecone.Client 4.0.2: No public CVE entries found for the .NET client. Mind API/behavior changes between 3.xΓåÆ4.x (and occasional required params/workarounds reported by users). Verify youΓÇÖre using the client version that matches Pinecone service/API expectations. [Pinecone NuGet; Pinecone community]. [1][2]\n\n- Elastic.Clients.Elasticsearch 9.2.1: No client-specific CVEs located. ElasticΓÇÖs client/compatibility guidance: 9.x clients are intended for Elasticsearch 9.x (and forward-compatible to later minor Elasticsearch 9/10 in many cases); consult Elastic known-issues/release notes for Elasticsearch server-side bugs that may affect behavior. [Elastic client NuGet compatibility; Elasticsearch known issues]. [3][4]\n\n- Azure.Search.Documents 11.7.0: No CVEs found for the package itself, but the Azure SDK changelogs list breaking changes and behavioral changes in the 11.7.x line (AOT/serialization/options and some semantic-search related relocations). Test for API/serialization/breaking-change impact when upgrading. [Azure SDK / Azure.Search.Documents release notes]. [5][6]\n\n- StackExchange.Redis 2.9.32: No public CVE for the client version; the 2.9.32 release notes show bug fixes (e.g., SSUBSCRIBE routing). However, Redis server and ecosystem have multiple highΓÇæseverity vulnerabilities (Lua useΓÇæafterΓÇæfree / RCE, DoS, etc.) ΓÇö these are server-side issues that can be exploited by authenticated or misconfigured clients. Keep Redis servers patched and follow hardening guidance. [StackExchange.Redis release notes; Redis security advisories]. [7][8][9]\n\nIf you want, I can:\n- search for any newly disclosed CVEs (give me permission to re-scan live), or\n- produce a short checklist of tests/configs to run before upgrading each library.\n\n---\n\nI need additional information to finalize the review. The critical gap is the missing NuGet packages for FAISS, Milvus, and Weaviate implementations. Let me verify:\n\n\n\n\n\n```\n\n```\n\n\n\n```\n\n\n\n```\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check if FAISS, Milvus, Weaviate implementations reference external packages or are self-contained\nrg -n -C2 'using.*FAISS|using.*Milvus|using.*Weaviate|new.*FAISS|new.*Milvus|new.*Weaviate' src/RetrievalAugmentedGeneration/DocumentStores/\n```\n\nLength of output: 0\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Verify net462 compatibility concerns ΓÇö check for any .NET version-specific usages in new code\nfd --type f '\\.cs$' -x grep -l 'net8\\|TargetFramework\\|#if.*NET' {} \\; | head -20\n```\n\nLength of output: 0\n\n---\n\n≡ƒîÉ Web query:\n\n```web\nFAISS vector database .NET client NuGet package\n```\n\n≡ƒÆí Result:\n\nHere are the main FAISS .NET NuGet clients you can use:\n\n1) FaissNet ΓÇö actively maintained .NET wrapper (supports .NET 6+). Install: dotnet add package FaissNet. [1]  \n2) FaissSharp ΓÇö older .NET Standard wrapper (last updated 2019). Install: dotnet add package FaissSharp. [2]\n\nSources:  \n[1] FaissNet NuGet page.  \n[2] FaissSharp NuGet page.\n\n---\n\n≡ƒîÉ Web query:\n\n```web\nMilvus vector database .NET client NuGet package\n```\n\n≡ƒÆí Result:\n\nYes ΓÇö Milvus has a .NET client on NuGet named Milvus.Client. Install with:\n\ndotnet add package Milvus.Client --version 2.3.0-preview.1\n\n(Latest prerelease shown on NuGet; source and docs are on the milvus-sdk-csharp GitHub repo and Milvus docs.) [1][2][3]\n\nAlternative/DI wrapper: Aspire.Milvus.Client (NuGet / Microsoft docs). [4][5]\n\n---\n\n≡ƒîÉ Web query:\n\n```web\nWeaviate vector database .NET client NuGet package\n```\n\n≡ƒÆí Result:\n\nOptions for a .NET (C#) Weaviate client on NuGet:\n\n1) Official/Weaviate organization package (Weaviate.Client ΓÇö prerelease on NuGet). Install: dotnet add package Weaviate.Client --version 0.0.1-beta.2. [1]\n\n2) Community/auto-generated package named \"Weaviate\" (dev prereleases available). Example install: dotnet add package Weaviate --version 0.0.0-dev.38. [2]\n\n3) Weaviate.NET ΓÇö a long-running community wrapper: dotnet add package WeaviateNET. (See latest versions on NuGet; e.g. 1.20.x/1.23.x series). [3][6]\n\n4) SearchPioneer.Weaviate.Client ΓÇö another community client on NuGet: dotnet add package SearchPioneer.Weaviate.Client. [4]\n\nAdditional resources:\n- Official Weaviate GitHub has a csharp-client repo (official C# client work). Check the repo for source/docs. [5]\n\nIf you want, tell me which one you prefer (official beta vs community) and IΓÇÖll show example code for connecting and doing a vector search.\n\n---\n\n```\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Inspect the FAISS, Milvus, Weaviate DocumentStore implementations to assess completeness\nfor file in src/RetrievalAugmentedGeneration/DocumentStores/FAISSDocumentStore.cs src/RetrievalAugmentedGeneration/DocumentStores/MilvusDocumentStore.cs src/RetrievalAugmentedGeneration/DocumentStores/WeaviateDocumentStore.cs; do\n  echo \"=== $file ===\"\n  head -30 \"$file\"\n  echo \"\"\ndone\n```\n\nLength of output: 3978\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check if Elastic.Clients.Elasticsearch and Azure.Search.Documents are actually used\nrg -n 'using.*Elastic|using.*Azure.Search|new.*Elastic|new.*Azure.Search' src/ | head -20\n```\n\nLength of output: 2320\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Verify what's actually implemented in the new DocumentStore classes\nrg -n 'public.*class.*DocumentStore' src/RetrievalAugmentedGeneration/DocumentStores/ -A5\n```\n\nLength of output: 9456\n\n---\n\n≡ƒîÉ Web query:\n\n```web\nElastic.Clients.Elasticsearch 9.2.1 NuGet version stable\n```\n\n≡ƒÆí Result:\n\nYes ΓÇö Elastic.Clients.Elasticsearch version 9.2.1 is published as a stable NuGet package. Install with:\ndotnet add package Elastic.Clients.Elasticsearch --version 9.2.1. [1][2]\n\nSources:\n[1] NuGet package page for Elastic.Clients.Elasticsearch 9.2.1.  \n[2] Elastic .NET client installation docs (install latest/stable via dotnet add package).\n\n---\n\n≡ƒîÉ Web query:\n\n```web\nAzure.Search.Documents 11.7.0 NuGet version release date\n```\n\n≡ƒÆí Result:\n\nThere is no stable NuGet release 11.7.0 ΓÇö only prerelease beta versions (e.g. 11.7.0-beta.6). See the NuGet prerelease page and the Azure SDK releases listing. [1][2]\n\nSources:\n- NuGet package page for Azure.Search.Documents 11.7.0-beta.6. [1]\n- Azure SDK releases listing showing NuGet 11.6.1 (stable) and 11.7.0-beta.* (prereleases). [2]\n\n---\n\n\n\n</details>\n\n**Fix Azure.Search.Documents version and address incomplete Elasticsearch/Azure integrations.**\n\nAzure.Search.Documents 11.7.0 is a prerelease version only (11.7.0-beta.* variants exist); the stable version is 11.6.1. Downgrade to `11.6.1` (line 44).\n\nAdditionally, the implementations for these newly added packages are incomplete:\n- **ElasticsearchDocumentStore** (line 45): Throws `NotImplementedException(\"Elasticsearch integration requires HTTP client implementation\")` ΓÇö not ready for use\n- **AzureSearchDocumentStore** (line 44): Implementation incomplete with only basic field initialization\n\nEither complete these integrations before merge or remove the packages and stub implementations to defer this work to a future PR. Using prerelease versions and incomplete integrations risks production instability.\n\nPinecone.Client 4.0.2, StackExchange.Redis 2.9.32, and Npgsql.EntityFrameworkCore.PostgreSQL 9.0.4 are stable and implementations are functional.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/AiDotNet.csproj around lines 44 to 56, the Azure.Search.Documents package\nis set to a prerelease (11.7.0) and the newly added Elasticsearch/Azure search\nintegrations are incomplete; change the Azure.Search.Documents reference to the\nstable 11.6.1 version, and either complete the ElasticsearchDocumentStore and\nAzureSearchDocumentStore implementations (remove the NotImplementedException and\nimplement HTTP client calls, mapping and error handling, plus write unit tests)\nor remove the corresponding PackageReference entries and their incomplete stub\nclasses so the PR does not introduce prerelease deps or non-functional\nintegrations; if you remove packages, also remove any using/imports and project\nreferences to those stub classes to keep the build green.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 80630e8 to 17e2d69",
    "created_at": "2025-11-04T14:21:41Z",
    "updated_at": "2025-11-05T02:38:04Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721618",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721618"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721618"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721618/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 44,
    "original_start_line": 44,
    "start_side": "RIGHT",
    "line": 56,
    "original_line": 56,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 16,
    "position": 16,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721637",
    "pull_request_review_id": 3416751163,
    "id": 2490721637,
    "node_id": "PRRC_kwDOKSXUF86UdWVl",
    "diff_hunk": "@@ -0,0 +1,133 @@\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies;\n+\n+/// <summary>\n+/// Multi-modal splitter for documents containing both text and images.\n+/// </summary>\n+/// <remarks>\n+/// Creates chunks that keep text and related images together, preserving the relationship\n+/// between visual and textual content for better context preservation.\n+/// </remarks>\n+public class MultiModalTextSplitter : ChunkingStrategyBase\n+{\n+    private readonly bool _preserveImageContext;\n+    private readonly int _contextWindowSize;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"MultiModalTextSplitter\"/> class.\n+    /// </summary>\n+    /// <param name=\"chunkSize\">Maximum size of text portion in each chunk.</param>\n+    /// <param name=\"chunkOverlap\">The number of characters that should overlap between consecutive chunks.</param>\n+    /// <param name=\"contextWindowSize\">Number of characters before/after image to include.</param>\n+    /// <param name=\"preserveImageContext\">Whether to keep surrounding text with images.</param>\n+    public MultiModalTextSplitter(\n+        int chunkSize,\n+        int chunkOverlap = 0,\n+        int contextWindowSize = 200,\n+        bool preserveImageContext = true)\n+        : base(chunkSize, chunkOverlap)\n+    {\n+        if (contextWindowSize < 0)\n+            throw new ArgumentOutOfRangeException(nameof(contextWindowSize), \"Context window size cannot be negative\");\n+            \n+        _contextWindowSize = contextWindowSize;\n+        _preserveImageContext = preserveImageContext;\n+    }\n+\n+    /// <summary>\n+    /// Core chunking logic that splits text while preserving text-image relationships.\n+    /// </summary>\n+    protected override IEnumerable<(string Chunk, int StartPosition, int EndPosition)> ChunkCore(string text)\n+    {\n+        var chunks = new List<(string, int, int)>();\n+        var lines = text.Split(new[] { \"\\r\\n\", \"\\r\", \"\\n\" }, StringSplitOptions.None);\n+        \n+        var currentChunk = new List<string>();\n+        var chunkStart = 0;\n+        var position = 0;\n+\n+        for (var i = 0; i < lines.Length; i++)\n+        {\n+            var line = lines[i];\n+            var lineLength = line.Length + Environment.NewLine.Length;\n+\n+            // Detect image references (Markdown ![alt](url) or HTML <img>)\n+            if (IsImageReference(line))\n+            {\n+                if (_preserveImageContext)\n+                {\n+                    // Include context before image\n+                    var contextStart = Math.Max(0, i - (_contextWindowSize / 50)); // Approximate lines\n+                    for (var j = contextStart; j < i; j++)\n+                    {\n+                        if (!currentChunk.Contains(lines[j]))\n+                        {\n+                            currentChunk.Add(lines[j]);\n+                        }\n+                    }\n+                }",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/MultiModalTextSplitter.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "5e2879cb116daba6f58fb4a68fc7f8d434e60eb4",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Position tracking inconsistency and inefficient Contains check.**\n\nTwo issues with the context-before logic:\n\n1. **Inefficient Contains check (line 64)**: `currentChunk.Contains(lines[j])` performs O(n) string comparisons and can yield false positives if identical line text appears multiple times. Consider using a `HashSet<int>` to track line indices instead.\n\n2. **Position tracking inconsistency**: Lines from `contextStart` to `i-1` were already processed in previous iterations and accounted for in `position`. Adding them again to `currentChunk` without adjusting `chunkStart` creates a mismatchΓÇöthe chunk now contains lines whose positions fall outside the `[chunkStart, endPosition)` range reported for this chunk.\n\n\n\nConsider restructuring to avoid re-adding already-processed lines, or adjust `chunkStart` to reflect the actual start of the earliest context line included.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/ChunkingStrategies/MultiModalTextSplitter.cs\naround lines 58 to 69, the context-before logic currently uses\ncurrentChunk.Contains(lines[j]) (inefficient and can mis-detect duplicates) and\nre-adds already-processed lines causing chunkStart/position to be inconsistent\nwith the returned chunk range; replace the string-based membership check with a\nHashSet<int> that tracks added line indices, and either skip adding context\nlines whose index < chunkStart (to avoid re-adding processed lines) or, if you\nmust include earlier context, set chunkStart = Math.Min(chunkStart,\ncontextStart) and adjust the computed position/endPosition accordingly so the\nreported range matches the actual earliest included line; ensure you add indices\nto the HashSet when adding lines and use index-based checks instead of string\ncomparisons.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T14:21:41Z",
    "updated_at": "2025-11-04T14:21:49Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721637",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721637"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721637"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721637/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 62,
    "original_start_line": 58,
    "start_side": "RIGHT",
    "line": 73,
    "original_line": 69,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 69,
    "position": 73,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721642",
    "pull_request_review_id": 3416751163,
    "id": 2490721642,
    "node_id": "PRRC_kwDOKSXUF86UdWVq",
    "diff_hunk": "@@ -0,0 +1,133 @@\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies;\n+\n+/// <summary>\n+/// Multi-modal splitter for documents containing both text and images.\n+/// </summary>\n+/// <remarks>\n+/// Creates chunks that keep text and related images together, preserving the relationship\n+/// between visual and textual content for better context preservation.\n+/// </remarks>\n+public class MultiModalTextSplitter : ChunkingStrategyBase\n+{\n+    private readonly bool _preserveImageContext;\n+    private readonly int _contextWindowSize;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"MultiModalTextSplitter\"/> class.\n+    /// </summary>\n+    /// <param name=\"chunkSize\">Maximum size of text portion in each chunk.</param>\n+    /// <param name=\"chunkOverlap\">The number of characters that should overlap between consecutive chunks.</param>\n+    /// <param name=\"contextWindowSize\">Number of characters before/after image to include.</param>\n+    /// <param name=\"preserveImageContext\">Whether to keep surrounding text with images.</param>\n+    public MultiModalTextSplitter(\n+        int chunkSize,\n+        int chunkOverlap = 0,\n+        int contextWindowSize = 200,\n+        bool preserveImageContext = true)\n+        : base(chunkSize, chunkOverlap)\n+    {\n+        if (contextWindowSize < 0)\n+            throw new ArgumentOutOfRangeException(nameof(contextWindowSize), \"Context window size cannot be negative\");\n+            \n+        _contextWindowSize = contextWindowSize;\n+        _preserveImageContext = preserveImageContext;\n+    }\n+\n+    /// <summary>\n+    /// Core chunking logic that splits text while preserving text-image relationships.\n+    /// </summary>\n+    protected override IEnumerable<(string Chunk, int StartPosition, int EndPosition)> ChunkCore(string text)\n+    {\n+        var chunks = new List<(string, int, int)>();\n+        var lines = text.Split(new[] { \"\\r\\n\", \"\\r\", \"\\n\" }, StringSplitOptions.None);\n+        \n+        var currentChunk = new List<string>();\n+        var chunkStart = 0;\n+        var position = 0;\n+\n+        for (var i = 0; i < lines.Length; i++)\n+        {\n+            var line = lines[i];\n+            var lineLength = line.Length + Environment.NewLine.Length;\n+\n+            // Detect image references (Markdown ![alt](url) or HTML <img>)\n+            if (IsImageReference(line))\n+            {\n+                if (_preserveImageContext)\n+                {\n+                    // Include context before image\n+                    var contextStart = Math.Max(0, i - (_contextWindowSize / 50)); // Approximate lines\n+                    for (var j = contextStart; j < i; j++)\n+                    {\n+                        if (!currentChunk.Contains(lines[j]))\n+                        {\n+                            currentChunk.Add(lines[j]);\n+                        }\n+                    }\n+                }\n+\n+                currentChunk.Add(line);\n+\n+                if (_preserveImageContext)\n+                {\n+                    // Include context after image\n+                    var contextEnd = Math.Min(lines.Length, i + (_contextWindowSize / 50));\n+                    for (var j = i + 1; j < contextEnd; j++)\n+                    {\n+                        currentChunk.Add(lines[j]);\n+                    }\n+                    i = contextEnd - 1; // Skip ahead\n+                }\n+\n+                // Create chunk with image\n+                var content = string.Join(Environment.NewLine, currentChunk);\n+                chunks.Add((content, chunkStart, position + lineLength));\n+\n+                currentChunk.Clear();\n+                chunkStart = position + lineLength;\n+            }",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/MultiModalTextSplitter.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "5e2879cb116daba6f58fb4a68fc7f8d434e60eb4",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Position tracking broken when adding context after image.**\n\nWhen context lines are added after an image (lines 77-80) and the loop skips ahead (line 81), the position tracking becomes incorrect:\n\n1. **Missing position updates for skipped lines**: Lines `i+1` through `contextEnd-1` are added to the chunk, but only line `i`'s length is added to `position` (line 106). The skipped lines' lengths are never accounted for in the position counter.\n\n2. **Incorrect end position**: Line 86 calculates the chunk's end position as `position + lineLength`, which only accounts for line `i` (the image line), not the context lines added in lines 77-80. The reported end position will be too small.\n\n**Result**: All subsequent chunks will have incorrect `StartPosition` and `EndPosition` values, and position-sensitive downstream processing will fail.\n\n\n\nApply this diff to fix position tracking:\n\n```diff\n                 if (_preserveImageContext)\n                 {\n                     // Include context after image\n                     var contextEnd = Math.Min(lines.Length, i + (_contextWindowSize / 50));\n+                    var contextLength = 0;\n                     for (var j = i + 1; j < contextEnd; j++)\n                     {\n                         currentChunk.Add(lines[j]);\n+                        contextLength += lines[j].Length + Environment.NewLine.Length;\n                     }\n                     i = contextEnd - 1; // Skip ahead\n+                    lineLength += contextLength; // Include context lines in length calculation\n                 }\n\n                 // Create chunk with image\n                 var content = string.Join(Environment.NewLine, currentChunk);\n                 chunks.Add((content, chunkStart, position + lineLength));\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/ChunkingStrategies/MultiModalTextSplitter.cs\naround lines 71 to 90, position tracking is broken when adding context lines\nafter an image: you must update the running position to include the lengths\n(plus newline lengths) of every context line you append and compute the chunk\nend position using the accumulated position after those lines; specifically,\nwhen iterating j from i+1 to contextEnd-1, add each lines[j] length (and the\nEnvironment.NewLine length if applicable) to position, then set i = contextEnd -\n1 and use the updated position (or position + lastLineLength if you prefer\nexplicit end) when creating the chunk and setting chunkStart so subsequent\nchunks have correct StartPosition and EndPosition.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T14:21:41Z",
    "updated_at": "2025-11-04T14:21:49Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721642",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721642"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721642"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721642/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 75,
    "original_start_line": 71,
    "start_side": "RIGHT",
    "line": 95,
    "original_line": 90,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 90,
    "position": 95,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721653",
    "pull_request_review_id": 3416751163,
    "id": 2490721653,
    "node_id": "PRRC_kwDOKSXUF86UdWV1",
    "diff_hunk": "@@ -0,0 +1,226 @@\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies;\n+\n+/// <summary>\n+/// Recursively splits text using a hierarchy of separators to preserve document structure.\n+/// </summary>\n+/// <remarks>\n+/// <para>\n+/// This advanced chunking strategy tries to split text using the most semantically meaningful\n+/// separators first (e.g., double newlines for paragraphs), falling back to less meaningful\n+/// separators (single newlines, spaces) only when necessary. This preserves the natural\n+/// structure of documents and keeps related content together.\n+/// </para>\n+/// <para><b>For Beginners:</b> This is a smart splitter that keeps related text together.\n+/// \n+/// Think of it like organizing a document by trying the best splits first:\n+/// \n+/// Priority 1: Split by double newlines (paragraphs)\n+///   \"Paragraph 1\\n\\nParagraph 2\\n\\nParagraph 3\"\n+///   ΓåÆ Keeps each paragraph whole\n+/// \n+/// Priority 2: If paragraphs are too big, split by single newlines (sentences/lines)\n+///   \"Long paragraph with\\nmultiple lines\\nthat need splitting\"\n+///   ΓåÆ Splits at line breaks\n+/// \n+/// Priority 3: If lines are too big, split by periods (sentences)\n+///   \"First sentence. Second sentence. Third sentence.\"\n+///   ΓåÆ Splits at sentences\n+/// \n+/// Priority 4: If sentences are too big, split by spaces (words)\n+///   \"This is a very long sentence without periods\"\n+///   ΓåÆ Splits at words\n+/// \n+/// Priority 5: Last resort, split by characters\n+///   \"ReallyLongWordWithNoSpaces\"\n+///   ΓåÆ Splits anywhere\n+/// \n+/// Why this is better than simple splitting:\n+/// - Keeps paragraphs together when possible (best semantic unity)\n+/// - Falls back gracefully when content is too large\n+/// - Preserves natural document structure\n+/// - Works well with various document formats (code, articles, books)\n+/// \n+/// Example with chunkSize=100, overlap=20:\n+/// \n+/// Input: \"First paragraph.\\n\\nSecond paragraph that is very long and needs to be split into multiple chunks.\\n\\nThird paragraph.\"\n+/// \n+/// 1. Try splitting by \"\\n\\n\" ΓåÆ Second paragraph too large\n+/// 2. Split second paragraph by \" \" ΓåÆ Gets multiple chunks\n+/// 3. Add overlap between chunks\n+/// \n+/// Result:\n+/// - Chunk 1: \"First paragraph.\"\n+/// - Chunk 2: \"Second paragraph that is very long and\" (overlap from chunk 1)\n+/// - Chunk 3: \"very long and needs to be split into\" (overlap from chunk 2)\n+/// - Chunk 4: \"split into multiple chunks.\"\n+/// - Chunk 5: \"Third paragraph.\"\n+/// </para>\n+/// </remarks>\n+public class RecursiveCharacterChunkingStrategy : ChunkingStrategyBase\n+{\n+    private readonly int _chunkSize;\n+    private readonly int _chunkOverlap;\n+    private readonly string[] _separators;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the RecursiveCharacterChunkingStrategy class.\n+    /// </summary>\n+    /// <param name=\"chunkSize\">Maximum size for each chunk in characters (default: 1000).</param>\n+    /// <param name=\"chunkOverlap\">Number of characters to overlap between chunks (default: 200).</param>\n+    /// <param name=\"separators\">Ordered list of separators to try (default: paragraph, newline, period, space, character).</param>\n+    public RecursiveCharacterChunkingStrategy(\n+        int chunkSize = 1000,\n+        int chunkOverlap = 200,\n+        string[]? separators = null)\n+    {\n+        if (chunkSize <= 0)\n+        {\n+            throw new ArgumentException(\"Chunk size must be positive.\", nameof(chunkSize));\n+        }\n+\n+        if (chunkOverlap < 0)\n+        {\n+            throw new ArgumentException(\"Chunk overlap cannot be negative.\", nameof(chunkOverlap));\n+        }\n+\n+        if (chunkOverlap >= chunkSize)\n+        {\n+            throw new ArgumentException(\"Chunk overlap must be less than chunk size.\", nameof(chunkOverlap));\n+        }\n+\n+        _chunkSize = chunkSize;\n+        _chunkOverlap = chunkOverlap;\n+        _separators = separators ?? new[] { \"\\n\\n\", \"\\n\", \". \", \" \", \"\" };\n+    }\n+\n+    /// <summary>\n+    /// Recursively splits text using the separator hierarchy.\n+    /// </summary>\n+    /// <param name=\"text\">The validated text to split.</param>\n+    /// <returns>Chunks with position information.</returns>\n+    protected override IEnumerable<(string Chunk, int StartPosition, int EndPosition)> ChunkCore(string text)\n+    {\n+        var chunks = SplitTextRecursively(text, _separators);\n+        var results = new List<(string, int, int)>();\n+        var position = 0;\n+\n+        foreach (var chunk in chunks)\n+        {\n+            var endPos = position + chunk.Length;\n+            results.Add((chunk, position, endPos));\n+            position = endPos;\n+        }\n+\n+        return results;\n+    }",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/RecursiveCharacterChunkingStrategy.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "5e2879cb116daba6f58fb4a68fc7f8d434e60eb4",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Position tracking doesn't account for overlapping content.**\n\nThe current position tracking accumulates chunk lengths sequentially (`position = endPos`), which means overlapping content gets assigned different positions in different chunks. For example, if chunk 1 is \"ABC\" (positions 0-3) and chunk 2 is \"CDEF\" with overlap \"C\", the current logic assigns chunk 2 positions 3-7, but \"C\" (which appeared at position 2-3 in chunk 1) now appears at position 3 in chunk 2.\n\nThis makes positions represent sequential locations in the chunked output rather than positions in the original text, which may confuse consumers who expect to map chunks back to the source document.\n\n\n\nConsider one of these approaches:\n\n1. **Document the behavior**: Add XML remarks explaining that positions are sequential in the output and overlapping content appears at multiple positions.\n\n2. **Track original positions**: Modify the algorithm to track where each chunk starts in the original text (accounting for re-inserted separators), though this is complex with recursive splitting.\n\n3. **Use zero-based sequential indices**: If positions don't represent original text, consider using simple sequential indices instead (chunk 0, chunk 1, etc.) to avoid confusion.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn\nsrc/RetrievalAugmentedGeneration/ChunkingStrategies/RecursiveCharacterChunkingStrategy.cs\naround lines 103 to 117, the current position tracking assigns sequential output\npositions instead of original-text offsets, causing overlapping content to get\nincorrect start/end indices; update the implementation so positions reflect\nindices in the original source text by propagating and computing offsets during\nrecursive splitting: modify SplitTextRecursively (or create an internal helper)\nto return tuples that include each chunk's start index in the original text\n(accounting for any reinserted separators) or accept a currentOffset parameter\nthat is updated as recursion proceeds, then change ChunkCore to consume those\noriginal offsets when building results (EndPosition = StartPosition +\nchunk.Length); ensure overlaps preserve the original shared indices rather than\nadvancing a cumulative sequential position.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T14:21:42Z",
    "updated_at": "2025-11-04T14:21:49Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721653",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721653"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721653"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721653/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 85,
    "original_start_line": 103,
    "start_side": "RIGHT",
    "line": 99,
    "original_line": 117,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 117,
    "position": 99,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721672",
    "pull_request_review_id": 3416751163,
    "id": 2490721672,
    "node_id": "PRRC_kwDOKSXUF86UdWWI",
    "diff_hunk": "@@ -0,0 +1,226 @@\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies;\n+\n+/// <summary>\n+/// Recursively splits text using a hierarchy of separators to preserve document structure.\n+/// </summary>\n+/// <remarks>\n+/// <para>\n+/// This advanced chunking strategy tries to split text using the most semantically meaningful\n+/// separators first (e.g., double newlines for paragraphs), falling back to less meaningful\n+/// separators (single newlines, spaces) only when necessary. This preserves the natural\n+/// structure of documents and keeps related content together.\n+/// </para>\n+/// <para><b>For Beginners:</b> This is a smart splitter that keeps related text together.\n+/// \n+/// Think of it like organizing a document by trying the best splits first:\n+/// \n+/// Priority 1: Split by double newlines (paragraphs)\n+///   \"Paragraph 1\\n\\nParagraph 2\\n\\nParagraph 3\"\n+///   ΓåÆ Keeps each paragraph whole\n+/// \n+/// Priority 2: If paragraphs are too big, split by single newlines (sentences/lines)\n+///   \"Long paragraph with\\nmultiple lines\\nthat need splitting\"\n+///   ΓåÆ Splits at line breaks\n+/// \n+/// Priority 3: If lines are too big, split by periods (sentences)\n+///   \"First sentence. Second sentence. Third sentence.\"\n+///   ΓåÆ Splits at sentences\n+/// \n+/// Priority 4: If sentences are too big, split by spaces (words)\n+///   \"This is a very long sentence without periods\"\n+///   ΓåÆ Splits at words\n+/// \n+/// Priority 5: Last resort, split by characters\n+///   \"ReallyLongWordWithNoSpaces\"\n+///   ΓåÆ Splits anywhere\n+/// \n+/// Why this is better than simple splitting:\n+/// - Keeps paragraphs together when possible (best semantic unity)\n+/// - Falls back gracefully when content is too large\n+/// - Preserves natural document structure\n+/// - Works well with various document formats (code, articles, books)\n+/// \n+/// Example with chunkSize=100, overlap=20:\n+/// \n+/// Input: \"First paragraph.\\n\\nSecond paragraph that is very long and needs to be split into multiple chunks.\\n\\nThird paragraph.\"\n+/// \n+/// 1. Try splitting by \"\\n\\n\" ΓåÆ Second paragraph too large\n+/// 2. Split second paragraph by \" \" ΓåÆ Gets multiple chunks\n+/// 3. Add overlap between chunks\n+/// \n+/// Result:\n+/// - Chunk 1: \"First paragraph.\"\n+/// - Chunk 2: \"Second paragraph that is very long and\" (overlap from chunk 1)\n+/// - Chunk 3: \"very long and needs to be split into\" (overlap from chunk 2)\n+/// - Chunk 4: \"split into multiple chunks.\"\n+/// - Chunk 5: \"Third paragraph.\"\n+/// </para>\n+/// </remarks>\n+public class RecursiveCharacterChunkingStrategy : ChunkingStrategyBase\n+{\n+    private readonly int _chunkSize;\n+    private readonly int _chunkOverlap;\n+    private readonly string[] _separators;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the RecursiveCharacterChunkingStrategy class.\n+    /// </summary>\n+    /// <param name=\"chunkSize\">Maximum size for each chunk in characters (default: 1000).</param>\n+    /// <param name=\"chunkOverlap\">Number of characters to overlap between chunks (default: 200).</param>\n+    /// <param name=\"separators\">Ordered list of separators to try (default: paragraph, newline, period, space, character).</param>\n+    public RecursiveCharacterChunkingStrategy(\n+        int chunkSize = 1000,\n+        int chunkOverlap = 200,\n+        string[]? separators = null)\n+    {\n+        if (chunkSize <= 0)\n+        {\n+            throw new ArgumentException(\"Chunk size must be positive.\", nameof(chunkSize));\n+        }\n+\n+        if (chunkOverlap < 0)\n+        {\n+            throw new ArgumentException(\"Chunk overlap cannot be negative.\", nameof(chunkOverlap));\n+        }\n+\n+        if (chunkOverlap >= chunkSize)\n+        {\n+            throw new ArgumentException(\"Chunk overlap must be less than chunk size.\", nameof(chunkOverlap));\n+        }\n+\n+        _chunkSize = chunkSize;\n+        _chunkOverlap = chunkOverlap;\n+        _separators = separators ?? new[] { \"\\n\\n\", \"\\n\", \". \", \" \", \"\" };\n+    }\n+\n+    /// <summary>\n+    /// Recursively splits text using the separator hierarchy.\n+    /// </summary>\n+    /// <param name=\"text\">The validated text to split.</param>\n+    /// <returns>Chunks with position information.</returns>\n+    protected override IEnumerable<(string Chunk, int StartPosition, int EndPosition)> ChunkCore(string text)\n+    {\n+        var chunks = SplitTextRecursively(text, _separators);\n+        var results = new List<(string, int, int)>();\n+        var position = 0;\n+\n+        foreach (var chunk in chunks)\n+        {\n+            var endPos = position + chunk.Length;\n+            results.Add((chunk, position, endPos));\n+            position = endPos;\n+        }\n+\n+        return results;\n+    }\n+\n+    /// <summary>\n+    /// Recursively splits text, trying each separator in order.\n+    /// </summary>\n+    /// <param name=\"text\">The text to split.</param>\n+    /// <param name=\"separators\">The ordered list of separators to try.</param>\n+    /// <returns>A list of text chunks.</returns>\n+    private List<string> SplitTextRecursively(string text, string[] separators)\n+    {\n+        var finalChunks = new List<string>();\n+\n+        // Base case: if text is small enough, return it as a single chunk\n+        if (text.Length <= _chunkSize)\n+        {\n+            if (!string.IsNullOrWhiteSpace(text))\n+            {\n+                finalChunks.Add(text.Trim());\n+            }\n+            return finalChunks;\n+        }\n+\n+        // Get the current separator\n+        var separator = separators[0];\n+        var nextSeparators = separators.Length > 1 ? separators[1..] : new[] { \"\" };\n+\n+        // Split by the current separator\n+        var splits = !string.IsNullOrEmpty(separator)\n+            ? text.Split(new[] { separator }, StringSplitOptions.None)\n+            : new[] { text };\n+\n+        var currentChunk = new System.Text.StringBuilder();\n+\n+        foreach (var split in splits)\n+        {\n+            // If this split would make the chunk too large\n+            if (currentChunk.Length + split.Length + separator.Length > _chunkSize)\n+            {\n+                // If we have accumulated content, save it\n+                if (currentChunk.Length > 0)\n+                {\n+                    finalChunks.Add(currentChunk.ToString().Trim());\n+                    currentChunk.Clear();\n+\n+                    // Add overlap from the end of the last chunk\n+                    if (finalChunks.Count > 0 && _chunkOverlap > 0)\n+                    {\n+                        var lastChunk = finalChunks[finalChunks.Count - 1];\n+                        var overlapStart = Math.Max(0, lastChunk.Length - _chunkOverlap);\n+                        var overlap = lastChunk.Substring(overlapStart);\n+                        currentChunk.Append(overlap);\n+                    }\n+                }\n+\n+                // If the split itself is too large, recursively split it with next separator\n+                if (split.Length > _chunkSize)\n+                {\n+                    var subChunks = SplitTextRecursively(split, nextSeparators);\n+                    foreach (var subChunk in subChunks)\n+                    {\n+                        if (currentChunk.Length + subChunk.Length > _chunkSize && currentChunk.Length > 0)\n+                        {\n+                            finalChunks.Add(currentChunk.ToString().Trim());\n+                            currentChunk.Clear();\n+\n+                            // Add overlap\n+                            if (finalChunks.Count > 0 && _chunkOverlap > 0)\n+                            {\n+                                var lastChunk = finalChunks[finalChunks.Count - 1];\n+                                var overlapStart = Math.Max(0, lastChunk.Length - _chunkOverlap);\n+                                var overlap = lastChunk.Substring(overlapStart);\n+                                currentChunk.Append(overlap);\n+                            }\n+                        }\n+\n+                        if (currentChunk.Length > 0)\n+                        {\n+                            currentChunk.Append(separator);\n+                        }\n+                        currentChunk.Append(subChunk);\n+                    }",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/RecursiveCharacterChunkingStrategy.cs",
    "commit_id": "5e2879cb116daba6f58fb4a68fc7f8d434e60eb4",
    "original_commit_id": "5e2879cb116daba6f58fb4a68fc7f8d434e60eb4",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Critical: Incorrect separator insertion in recursive subchunks.**\n\nWhen a split exceeds `_chunkSize` and is recursively split with `nextSeparators` (line 174), the returned `subChunks` are then joined using the current-level separator (line 194). This creates incorrect output when recursion reaches character-level splitting.\n\n**Example scenario:**\n```\nText: \"VeryLongWordThatExceedsChunkSize\" (no natural breaks)\nCurrent separator: \"\\n\\n\" (paragraph)\nΓåÆ Recursively splits down to character level\nΓåÆ Returns subChunks: [\"VeryLongWord\", \"ThatExceeds\", \"ChunkSize\"]\nΓåÆ Line 194 joins with \"\\n\\n\"\nΓåÆ Result: \"VeryLongWord\\n\\nThatExceeds\\n\\nChunkSize\"\n```\n\nThis inserts paragraph breaks in the middle of a word, corrupting the document structure.\n\n\n\n**Fix:** Don't add separators between recursively-split subChunks:\n\n```diff\n                    var subChunks = SplitTextRecursively(split, nextSeparators);\n                    foreach (var subChunk in subChunks)\n                    {\n                        if (currentChunk.Length + subChunk.Length > _chunkSize && currentChunk.Length > 0)\n                        {\n                            finalChunks.Add(currentChunk.ToString().Trim());\n                            currentChunk.Clear();\n\n                            // Add overlap\n                            if (finalChunks.Count > 0 && _chunkOverlap > 0)\n                            {\n                                var lastChunk = finalChunks[finalChunks.Count - 1];\n                                var overlapStart = Math.Max(0, lastChunk.Length - _chunkOverlap);\n                                var overlap = lastChunk.Substring(overlapStart);\n                                currentChunk.Append(overlap);\n                            }\n                        }\n\n-                       if (currentChunk.Length > 0)\n-                       {\n-                           currentChunk.Append(separator);\n-                       }\n                        currentChunk.Append(subChunk);\n                    }\n```\n\nThe subChunks from recursion are already complete segments that shouldn't be re-separated with the current level's delimiter.\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n                if (split.Length > _chunkSize)\n                {\n                    var subChunks = SplitTextRecursively(split, nextSeparators);\n                    foreach (var subChunk in subChunks)\n                    {\n                        if (currentChunk.Length + subChunk.Length > _chunkSize && currentChunk.Length > 0)\n                        {\n                            finalChunks.Add(currentChunk.ToString().Trim());\n                            currentChunk.Clear();\n\n                            // Add overlap\n                            if (finalChunks.Count > 0 && _chunkOverlap > 0)\n                            {\n                                var lastChunk = finalChunks[finalChunks.Count - 1];\n                                var overlapStart = Math.Max(0, lastChunk.Length - _chunkOverlap);\n                                var overlap = lastChunk.Substring(overlapStart);\n                                currentChunk.Append(overlap);\n                            }\n                        }\n\n                        currentChunk.Append(subChunk);\n                    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn\nsrc/RetrievalAugmentedGeneration/ChunkingStrategies/RecursiveCharacterChunkingStrategy.cs\naround lines 172ΓÇô197, the code erroneously inserts the current-level separator\nwhen iterating over subChunks returned from recursive SplitTextRecursively\ncalls; remove the insertion of separator between recursively-produced subChunks\n(i.e., do not call currentChunk.Append(separator) when handling subChunks\nreturned by recursion) so those subChunks remain intact, while keeping the\nexisting chunk-size checks, overlap logic, and the separator behavior only for\nthe non-recursive/top-level splits.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commit 841164e",
    "created_at": "2025-11-04T14:21:42Z",
    "updated_at": "2025-11-04T14:30:05Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721672",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721672"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721672"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721672/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 172,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 197,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 197,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721687",
    "pull_request_review_id": 3416751163,
    "id": 2490721687,
    "node_id": "PRRC_kwDOKSXUF86UdWWX",
    "diff_hunk": "@@ -0,0 +1,190 @@\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies;\n+\n+/// <summary>\n+/// Splits text into chunks based on sentence boundaries to preserve semantic coherence.\n+/// </summary>\n+/// <remarks>\n+/// <para>\n+/// This chunking strategy splits text at sentence boundaries (periods, question marks,\n+/// exclamation points) and combines sentences until reaching the target chunk size.\n+/// This approach preserves complete thoughts and improves retrieval quality compared\n+/// to arbitrary character-based splitting.\n+/// </para>\n+/// <para><b>For Beginners:</b> This keeps complete sentences together in each chunk.\n+/// \n+/// Think of it like organizing a book:\n+/// - Bad way: Cut every 500 characters, even mid-sentence\n+///   \"The cat sat on the m|at. The dog ran ar|ound the yard.\"\n+/// - Good way: Keep sentences whole\n+///   Chunk 1: \"The cat sat on the mat. The dog ran around the yard.\"\n+///   Chunk 2: \"The bird flew over the fence. The fish swam in the pond.\"\n+/// \n+/// Why this matters:\n+/// - Retrieval works better when searching complete thoughts\n+/// - Generators get more coherent context\n+/// - No weird sentence fragments that confuse the model\n+/// \n+/// Parameters:\n+/// - targetChunkSize: Aim for this many characters per chunk\n+/// - maxChunkSize: Never exceed this size (may break sentences if needed)\n+/// - overlapSentences: Number of sentences to repeat between chunks for context\n+/// \n+/// Example with targetChunkSize=100, overlapSentences=1:\n+/// \"First sentence. Second sentence. Third sentence. Fourth sentence.\"\n+/// \n+/// Chunk 1: \"First sentence. Second sentence. Third sentence.\"\n+/// Chunk 2: \"Third sentence. Fourth sentence.\" (overlap: \"Third sentence\")\n+/// </para>\n+/// </remarks>\n+public class SentenceChunkingStrategy : ChunkingStrategyBase\n+{\n+    private readonly int _targetChunkSize;\n+    private readonly int _maxChunkSize;\n+    private readonly int _overlapSentences;\n+    private static readonly char[] SentenceEnders = { '.', '!', '?' };\n+\n+    /// <summary>\n+    /// Initializes a new instance of the SentenceChunkingStrategy class.\n+    /// </summary>\n+    /// <param name=\"targetChunkSize\">Target size for each chunk in characters (default: 500).</param>\n+    /// <param name=\"maxChunkSize\">Maximum allowed chunk size in characters (default: 1000).</param>\n+    /// <param name=\"overlapSentences\">Number of sentences to overlap between chunks (default: 1).</param>\n+    public SentenceChunkingStrategy(int targetChunkSize = 500, int maxChunkSize = 1000, int overlapSentences = 1)\n+    {\n+        if (targetChunkSize <= 0)\n+        {\n+            throw new ArgumentException(\"Target chunk size must be positive.\", nameof(targetChunkSize));\n+        }\n+\n+        if (maxChunkSize < targetChunkSize)\n+        {\n+            throw new ArgumentException(\"Maximum chunk size must be greater than or equal to target chunk size.\", nameof(maxChunkSize));\n+        }\n+\n+        if (overlapSentences < 0)\n+        {\n+            throw new ArgumentException(\"Overlap sentences cannot be negative.\", nameof(overlapSentences));\n+        }\n+\n+        _targetChunkSize = targetChunkSize;\n+        _maxChunkSize = maxChunkSize;\n+        _overlapSentences = overlapSentences;\n+    }",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/SentenceChunkingStrategy.cs",
    "commit_id": "5e2879cb116daba6f58fb4a68fc7f8d434e60eb4",
    "original_commit_id": "5e2879cb116daba6f58fb4a68fc7f8d434e60eb4",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Missing base class constructor call.**\n\nThe constructor does not invoke the base class `ChunkingStrategyBase` constructor, which requires `chunkSize` and `chunkOverlap` parameters. This means the base class properties `ChunkSize` and `ChunkOverlap` are either uninitialized or set to default values, breaking the inheritance contract.\n\n\n\nApply this diff to properly initialize the base class:\n\n```diff\n-    public SentenceChunkingStrategy(int targetChunkSize = 500, int maxChunkSize = 1000, int overlapSentences = 1)\n+    public SentenceChunkingStrategy(int targetChunkSize = 500, int maxChunkSize = 1000, int overlapSentences = 1)\n+        : base(targetChunkSize, 0) // Pass targetChunkSize as chunkSize, 0 for overlap since we handle it differently\n     {\n```\n\nAlternatively, if sentence-based overlap differs fundamentally from character-based overlap, consider whether this class should inherit from `ChunkingStrategyBase` at all, or whether the base class needs a parameterless constructor for strategies that manage sizing differently.\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    public SentenceChunkingStrategy(int targetChunkSize = 500, int maxChunkSize = 1000, int overlapSentences = 1)\n        : base(targetChunkSize, 0) // Pass targetChunkSize as chunkSize, 0 for overlap since we handle it differently\n    {\n        if (targetChunkSize <= 0)\n        {\n            throw new ArgumentException(\"Target chunk size must be positive.\", nameof(targetChunkSize));\n        }\n\n        if (maxChunkSize < targetChunkSize)\n        {\n            throw new ArgumentException(\"Maximum chunk size must be greater than or equal to target chunk size.\", nameof(maxChunkSize));\n        }\n\n        if (overlapSentences < 0)\n        {\n            throw new ArgumentException(\"Overlap sentences cannot be negative.\", nameof(overlapSentences));\n        }\n\n        _targetChunkSize = targetChunkSize;\n        _maxChunkSize = maxChunkSize;\n        _overlapSentences = overlapSentences;\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn\nsrc/RetrievalAugmentedGeneration/ChunkingStrategies/SentenceChunkingStrategy.cs\naround lines 54 to 74, the constructor must invoke the base ChunkingStrategyBase\nconstructor to initialize its required chunkSize and chunkOverlap fields; update\nthe constructor signature to call the base constructor (e.g.,\nbase(targetChunkSize, overlapSentences)) so ChunkSize and ChunkOverlap are set\ncorrectly, or if sentence-based overlap is incompatible with the base class\nsemantics, refactor by either removing inheritance or adding a\nparameterless/alternative base constructor and adjust the class accordingly.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T14:21:42Z",
    "updated_at": "2025-11-04T14:21:49Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721687",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721687"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721687"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721687/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 54,
    "original_start_line": 54,
    "start_side": "RIGHT",
    "line": 70,
    "original_line": 74,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 74,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721717",
    "pull_request_review_id": 3416751163,
    "id": 2490721717,
    "node_id": "PRRC_kwDOKSXUF86UdWW1",
    "diff_hunk": "@@ -0,0 +1,78 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// ChromaDB-based document store designed for simplicity and developer experience.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// ChromaDB is an open-source vector database that emphasizes ease of use while maintaining\n+/// high performance for similarity search operations.\n+/// </remarks>\n+public class ChromaDBDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly string _endpoint;\n+    private readonly string _collectionName;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"ChromaDBDocumentStore{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"endpoint\">The ChromaDB endpoint URL.</param>\n+    /// <param name=\"collectionName\">The name of the collection to use.</param>\n+    /// <param name=\"vectorDimension\">The dimensionality of document vectors.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public ChromaDBDocumentStore(\n+        string endpoint,\n+        string collectionName,\n+        int vectorDimension,\n+        INumericOperations<T> numericOperations)\n+        : base(vectorDimension, numericOperations)\n+    {\n+        _endpoint = endpoint ?? throw new ArgumentNullException(nameof(endpoint));\n+        _collectionName = collectionName ?? throw new ArgumentNullException(nameof(collectionName));\n+    }\n+\n+    /// <summary>\n+    /// Adds a document to the ChromaDB collection.\n+    /// </summary>\n+    public override void AddDocument(Document<T> document)\n+    {\n+        if (document == null)\n+            throw new ArgumentNullException(nameof(document));\n+\n+        // TODO: Implement ChromaDB add via REST API\n+        throw new NotImplementedException(\"ChromaDB integration requires HTTP client implementation\");\n+    }\n+\n+    /// <summary>\n+    /// Retrieves documents similar to the query vector.\n+    /// </summary>\n+    public override IEnumerable<Document<T>> GetSimilar(Vector<T> queryVector, int topK)\n+    {\n+        if (queryVector == null)\n+            throw new ArgumentNullException(nameof(queryVector));\n+\n+        if (topK <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(topK), \"topK must be positive\");\n+\n+        // TODO: Implement ChromaDB query via REST API\n+        throw new NotImplementedException(\"ChromaDB integration requires HTTP client implementation\");\n+    }\n+\n+    /// <summary>\n+    /// Gets all documents from the collection.\n+    /// </summary>\n+    public override IEnumerable<Document<T>> GetAllDocuments()\n+    {\n+        // TODO: Implement ChromaDB get all documents\n+        throw new NotImplementedException(\"ChromaDB integration requires HTTP client implementation\");\n+    }\n+\n+    /// <summary>\n+    /// Gets the total number of documents in the collection.\n+    /// </summary>\n+    public override int DocumentCount => 0; // TODO: Implement via ChromaDB API",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/ChromaDBDocumentStore.cs",
    "commit_id": "7b870fd485b3e1ca43e80aec3854cec198669882",
    "original_commit_id": "5e2879cb116daba6f58fb4a68fc7f8d434e60eb4",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**DocumentCount cannot always return zero.**\n\n`DocumentStoreBase<T>.ValidateVectorDocument` relies on `DocumentCount` to enforce consistent embedding dimensions after the first insert. Returning a constant zero permanently disables that guard, allowing mixed dimensions to slip in. Implement this by querying ChromaDB for the collection size (or throw until the REST call is wired up), but do not return a hard-coded zero.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/ChromaDBDocumentStore.cs\naround line 77, replace the hard-coded DocumentCount => 0 with a real\nimplementation: call the ChromaDB client / collection API to return the current\ncollection/document count (or, if the REST client isn't available yet, throw a\nNotImplementedException/InvalidOperationException instead of returning 0) so\nDocumentStoreBase<T>.ValidateVectorDocument can enforce consistent embedding\ndimensions; ensure the call surfaces any client errors clearly and does not\nswallow exceptions.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 80630e8 to 17e2d69",
    "created_at": "2025-11-04T14:21:43Z",
    "updated_at": "2025-11-05T02:38:11Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721717",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721717"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721717"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721717/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": null,
    "original_line": 77,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 77,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721725",
    "pull_request_review_id": 3416751163,
    "id": 2490721725,
    "node_id": "PRRC_kwDOKSXUF86UdWW9",
    "diff_hunk": "@@ -0,0 +1,83 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// Elasticsearch-based document store providing hybrid search capabilities (BM25 + dense vectors).\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Elasticsearch combines traditional full-text search (BM25) with vector similarity search,\n+/// making it ideal for hybrid retrieval scenarios where both keyword matching and semantic\n+/// similarity are important.\n+/// </remarks>\n+public class ElasticsearchDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly string _endpoint;\n+    private readonly string _indexName;\n+    private readonly string _apiKey;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"ElasticsearchDocumentStore{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"endpoint\">The Elasticsearch endpoint URL.</param>\n+    /// <param name=\"indexName\">The name of the index to use.</param>\n+    /// <param name=\"apiKey\">The API key for authentication.</param>\n+    /// <param name=\"vectorDimension\">The dimensionality of document vectors.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public ElasticsearchDocumentStore(\n+        string endpoint,\n+        string indexName,\n+        string apiKey,\n+        int vectorDimension,\n+        INumericOperations<T> numericOperations)\n+        : base(vectorDimension, numericOperations)\n+    {\n+        _endpoint = endpoint ?? throw new ArgumentNullException(nameof(endpoint));\n+        _indexName = indexName ?? throw new ArgumentNullException(nameof(indexName));\n+        _apiKey = apiKey ?? throw new ArgumentNullException(nameof(apiKey));\n+    }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/ElasticsearchDocumentStore.cs",
    "commit_id": "80630e82c71aabc214615014532fd5063336af16",
    "original_commit_id": "5e2879cb116daba6f58fb4a68fc7f8d434e60eb4",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Critical: Remove invalid base constructor call and implement all required abstract members.**\n\nThe `base(vectorDimension, numericOperations)` call at line 36 will fail to compile because `DocumentStoreBase<T>` has no constructor with that signatureΓÇöit only provides an implicit parameterless constructor. Additionally, this class is missing all required abstract member implementations.\n\nThe corrected `AzureSearchDocumentStore.cs` in this same PR demonstrates the correct pattern. You must:\n\n1. Remove the `base(...)` call (use the implicit base constructor)\n2. Store `vectorDimension` in a private field `_vectorDimension`\n3. Store `numericOperations` if needed (though the base provides `NumOps` static member)\n4. Implement the abstract property: `public override int VectorDimension => _vectorDimension;`\n5. Implement the abstract property: `public override int DocumentCount` (track count in a field)\n6. Implement the abstract methods: `Clear()`, `AddCore(VectorDocument<T>)`, `GetSimilarCore(Vector<T>, int, Dictionary<string,object>)`, `GetByIdCore(string)`, `RemoveCore(string)`\n\n\n\nApply these changes to match the pattern in `AzureSearchDocumentStore.cs` (lines 30-107):\n\n```diff\n     public ElasticsearchDocumentStore(\n         string endpoint,\n         string indexName,\n         string apiKey,\n         int vectorDimension,\n         INumericOperations<T> numericOperations)\n-        : base(vectorDimension, numericOperations)\n     {\n         _endpoint = endpoint ?? throw new ArgumentNullException(nameof(endpoint));\n         _indexName = indexName ?? throw new ArgumentNullException(nameof(indexName));\n         _apiKey = apiKey ?? throw new ArgumentNullException(nameof(apiKey));\n+        \n+        if (vectorDimension <= 0)\n+            throw new ArgumentException(\"Vector dimension must be positive\", nameof(vectorDimension));\n+        \n+        _vectorDimension = vectorDimension;\n+        _documentCount = 0;\n     }\n```\n\nThen add the missing fields at line 21:\n\n```diff\n     private readonly string _apiKey;\n+    private readonly int _vectorDimension;\n+    private int _documentCount;\n```\n\nAnd add the missing abstract member implementations after line 82:\n\n```csharp\n    public override int VectorDimension => _vectorDimension;\n\n    public override void Clear()\n    {\n        // TODO: Implement Elasticsearch index clearing via REST API\n        _documentCount = 0;\n    }\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/ElasticsearchDocumentStore.cs\naround lines 30-41, remove the invalid base(vectorDimension, numericOperations)\nconstructor call (use the implicit base ctor), add private fields at ~line 21\nfor _vectorDimension, _documentCount and any needed numericOperations reference,\nassign _vectorDimension (and store numericOperations if required) in the\nconstructor, and implement the required abstract members after ~line 82:\noverride int VectorDimension => _vectorDimension; override int DocumentCount\nreturning _documentCount; and implement Clear(), AddCore(VectorDocument<T>),\nGetSimilarCore(Vector<T>, int, Dictionary<string,object>), GetByIdCore(string),\nand RemoveCore(string) following the pattern used in AzureSearchDocumentStore.cs\n(use TODOs for unimplemented REST logic and update _documentCount accordingly).\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits e0f1f5e to 6670ddf",
    "created_at": "2025-11-04T14:21:43Z",
    "updated_at": "2025-11-04T14:53:46Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721725",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721725"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721725"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721725/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 30,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 41,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 41,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721739",
    "pull_request_review_id": 3416751163,
    "id": 2490721739,
    "node_id": "PRRC_kwDOKSXUF86UdWXL",
    "diff_hunk": "@@ -0,0 +1,119 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores\n+{\n+    /// <summary>\n+    /// FAISS-inspired document store with indexed vectors for efficient similarity search.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class FAISSDocumentStore<T> : DocumentStoreBase<T>\n+    {\n+        private readonly Dictionary<string, VectorDocument<T>> _documents;\n+        private readonly Dictionary<int, Vector<T>> _indexedVectors;\n+        private int _vectorDimension;\n+        private int _currentIndex;\n+\n+        public override int DocumentCount => _documents.Count;\n+        public override int VectorDimension => _vectorDimension;\n+\n+        public FAISSDocumentStore(int initialCapacity = 1000)\n+        {\n+            if (initialCapacity <= 0)\n+                throw new ArgumentException(\"Initial capacity must be greater than zero\", nameof(initialCapacity));\n+\n+            _documents = new Dictionary<string, VectorDocument<T>>(initialCapacity);\n+            _indexedVectors = new Dictionary<int, Vector<T>>(initialCapacity);\n+            _vectorDimension = 0;\n+            _currentIndex = 0;\n+        }\n+\n+        protected override void AddCore(VectorDocument<T> vectorDocument)\n+        {\n+            if (_documents.Count == 0)\n+            {\n+                _vectorDimension = vectorDocument.Embedding.Length;\n+            }\n+\n+            var index = _currentIndex++;\n+            _documents[vectorDocument.Document.Id] = vectorDocument;\n+            _indexedVectors[index] = vectorDocument.Embedding;\n+        }\n+\n+        protected override void AddBatchCore(IList<VectorDocument<T>> vectorDocuments)\n+        {\n+            if (_vectorDimension == 0 && vectorDocuments.Count > 0)\n+            {\n+                _vectorDimension = vectorDocuments[0].Embedding.Length;\n+            }\n+\n+            foreach (var vectorDocument in vectorDocuments)\n+            {\n+                if (vectorDocument.Embedding.Length != _vectorDimension)\n+                    throw new ArgumentException(\n+                        $\"Vector dimension mismatch in batch. Expected {_vectorDimension}, got {vectorDocument.Embedding.Length}\",\n+                        nameof(vectorDocuments));\n+\n+                var index = _currentIndex++;\n+                _documents[vectorDocument.Document.Id] = vectorDocument;\n+                _indexedVectors[index] = vectorDocument.Embedding;\n+            }\n+        }\n+\n+        protected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)\n+        {\n+            var scoredDocuments = new List<(Document<T> Document, T Score)>();\n+\n+            var matchingDocuments = _documents.Values\n+                .Where(vectorDoc => MatchesFilters(vectorDoc.Document, metadataFilters));\n+\n+            foreach (var vectorDoc in matchingDocuments)\n+            {\n+                var similarity = StatisticsHelper<T>.CosineSimilarity(queryVector, vectorDoc.Embedding);\n+                scoredDocuments.Add((vectorDoc.Document, similarity));\n+            }\n+\n+            var results = scoredDocuments\n+                .OrderByDescending(x => x.Score)\n+                .Take(topK)\n+                .Select(x =>\n+                {\n+                    x.Document.RelevanceScore = x.Score;\n+                    x.Document.HasRelevanceScore = true;\n+                    return x.Document;\n+                })\n+                .ToList();\n+\n+            return results;\n+        }\n+\n+        protected override Document<T>? GetByIdCore(string documentId)\n+        {\n+            return _documents.TryGetValue(documentId, out var vectorDoc) ? vectorDoc.Document : null;\n+        }\n+\n+        protected override bool RemoveCore(string documentId)\n+        {\n+            var removed = _documents.Remove(documentId);\n+            if (removed && _documents.Count == 0)\n+            {\n+                _vectorDimension = 0;\n+                _currentIndex = 0;\n+                _indexedVectors.Clear();\n+            }\n+            return removed;\n+        }\n+\n+        public override void Clear()\n+        {\n+            _documents.Clear();\n+            _indexedVectors.Clear();\n+            _vectorDimension = 0;\n+            _currentIndex = 0;\n+        }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/FAISSDocumentStore.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "5e2879cb116daba6f58fb4a68fc7f8d434e60eb4",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Clear stale FAISS index entries on removal**\n\n`RemoveCore` only deletes the `VectorDocument` entry; the vector remains in `_indexedVectors`, so every delete leaks an embedding and leaves the FAISS index out of sync with `_documents`. A longΓÇærunning service that ingests and prunes documents will accumulate unbounded vectors and risk serving stale results if future code ever reads `_indexedVectors`. Please keep the index in lockstep with the stored documents.\n\nSuggested fix:\n\n```diff\n@@\n-        private readonly Dictionary<int, Vector<T>> _indexedVectors;\n+        private readonly Dictionary<int, Vector<T>> _indexedVectors;\n+        private readonly Dictionary<string, int> _documentIndices;\n@@\n-            _indexedVectors = new Dictionary<int, Vector<T>>(initialCapacity);\n+            _indexedVectors = new Dictionary<int, Vector<T>>(initialCapacity);\n+            _documentIndices = new Dictionary<string, int>(initialCapacity);\n@@\n             var index = _currentIndex++;\n             _documents[vectorDocument.Document.Id] = vectorDocument;\n             _indexedVectors[index] = vectorDocument.Embedding;\n+            _documentIndices[vectorDocument.Document.Id] = index;\n@@\n                 var index = _currentIndex++;\n                 _documents[vectorDocument.Document.Id] = vectorDocument;\n                 _indexedVectors[index] = vectorDocument.Embedding;\n+                _documentIndices[vectorDocument.Document.Id] = index;\n@@\n-            var removed = _documents.Remove(documentId);\n-            if (removed && _documents.Count == 0)\n+            if (!_documents.Remove(documentId))\n+            {\n+                return false;\n+            }\n+\n+            if (_documentIndices.TryGetValue(documentId, out var vectorIndex))\n+            {\n+                _indexedVectors.Remove(vectorIndex);\n+                _documentIndices.Remove(documentId);\n+            }\n+\n+            if (_documents.Count == 0)\n             {\n                 _vectorDimension = 0;\n                 _currentIndex = 0;\n                 _indexedVectors.Clear();\n+                _documentIndices.Clear();\n             }\n-            return removed;\n+            return true;\n@@\n             _indexedVectors.Clear();\n+            _documentIndices.Clear();\n             _vectorDimension = 0;\n             _currentIndex = 0;\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        private readonly Dictionary<string, VectorDocument<T>> _documents;\n        private readonly Dictionary<int, Vector<T>> _indexedVectors;\n        private readonly Dictionary<string, int> _documentIndices;\n        private int _vectorDimension;\n        private int _currentIndex;\n\n        public override int DocumentCount => _documents.Count;\n        public override int VectorDimension => _vectorDimension;\n\n        public FAISSDocumentStore(int initialCapacity = 1000)\n        {\n            if (initialCapacity <= 0)\n                throw new ArgumentException(\"Initial capacity must be greater than zero\", nameof(initialCapacity));\n\n            _documents = new Dictionary<string, VectorDocument<T>>(initialCapacity);\n            _indexedVectors = new Dictionary<int, Vector<T>>(initialCapacity);\n            _documentIndices = new Dictionary<string, int>(initialCapacity);\n            _vectorDimension = 0;\n            _currentIndex = 0;\n        }\n\n        protected override void AddCore(VectorDocument<T> vectorDocument)\n        {\n            if (_documents.Count == 0)\n            {\n                _vectorDimension = vectorDocument.Embedding.Length;\n            }\n\n            var index = _currentIndex++;\n            _documents[vectorDocument.Document.Id] = vectorDocument;\n            _indexedVectors[index] = vectorDocument.Embedding;\n            _documentIndices[vectorDocument.Document.Id] = index;\n        }\n\n        protected override void AddBatchCore(IList<VectorDocument<T>> vectorDocuments)\n        {\n            if (_vectorDimension == 0 && vectorDocuments.Count > 0)\n            {\n                _vectorDimension = vectorDocuments[0].Embedding.Length;\n            }\n\n            foreach (var vectorDocument in vectorDocuments)\n            {\n                if (vectorDocument.Embedding.Length != _vectorDimension)\n                    throw new ArgumentException(\n                        $\"Vector dimension mismatch in batch. Expected {_vectorDimension}, got {vectorDocument.Embedding.Length}\",\n                        nameof(vectorDocuments));\n\n                var index = _currentIndex++;\n                _documents[vectorDocument.Document.Id] = vectorDocument;\n                _indexedVectors[index] = vectorDocument.Embedding;\n                _documentIndices[vectorDocument.Document.Id] = index;\n            }\n        }\n\n        protected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)\n        {\n            var scoredDocuments = new List<(Document<T> Document, T Score)>();\n\n            var matchingDocuments = _documents.Values\n                .Where(vectorDoc => MatchesFilters(vectorDoc.Document, metadataFilters));\n\n            foreach (var vectorDoc in matchingDocuments)\n            {\n                var similarity = StatisticsHelper<T>.CosineSimilarity(queryVector, vectorDoc.Embedding);\n                scoredDocuments.Add((vectorDoc.Document, similarity));\n            }\n\n            var results = scoredDocuments\n                .OrderByDescending(x => x.Score)\n                .Take(topK)\n                .Select(x =>\n                {\n                    x.Document.RelevanceScore = x.Score;\n                    x.Document.HasRelevanceScore = true;\n                    return x.Document;\n                })\n                .ToList();\n\n            return results;\n        }\n\n        protected override Document<T>? GetByIdCore(string documentId)\n        {\n            return _documents.TryGetValue(documentId, out var vectorDoc) ? vectorDoc.Document : null;\n        }\n\n        protected override bool RemoveCore(string documentId)\n        {\n            if (!_documents.Remove(documentId))\n            {\n                return false;\n            }\n\n            if (_documentIndices.TryGetValue(documentId, out var vectorIndex))\n            {\n                _indexedVectors.Remove(vectorIndex);\n                _documentIndices.Remove(documentId);\n            }\n\n            if (_documents.Count == 0)\n            {\n                _vectorDimension = 0;\n                _currentIndex = 0;\n                _indexedVectors.Clear();\n                _documentIndices.Clear();\n            }\n            return true;\n        }\n\n        public override void Clear()\n        {\n            _documents.Clear();\n            _indexedVectors.Clear();\n            _documentIndices.Clear();\n            _vectorDimension = 0;\n            _currentIndex = 0;\n        }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/FAISSDocumentStore.cs around\nlines 16 to 117, RemoveCore currently only removes the VectorDocument from\n_documents leaving its vector in _indexedVectors, which leaks embeddings and\ndesynchronizes the index; to fix, introduce and maintain a reverse map (e.g.\nDictionary<string,int> _documentIndexMap) updated when adding documents (set\nmap[documentId]=index in AddCore and AddBatchCore), and in RemoveCore use that\nmap to look up the index and remove the corresponding entry from _indexedVectors\nand the reverse map; also ensure Clear resets and clears the reverse map (and\nany other related state).\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T14:21:43Z",
    "updated_at": "2025-11-04T14:21:49Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721739",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721739"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721739"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721739/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 40,
    "original_start_line": 16,
    "start_side": "RIGHT",
    "line": 348,
    "original_line": 117,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 117,
    "position": 348,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721755",
    "pull_request_review_id": 3416751163,
    "id": 2490721755,
    "node_id": "PRRC_kwDOKSXUF86UdWXb",
    "diff_hunk": "@@ -0,0 +1,93 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores\n+{\n+    /// <summary>\n+    /// Milvus-inspired document store with collection-based vector organization.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class MilvusDocumentStore<T> : DocumentStoreBase<T>\n+    {\n+        private readonly Dictionary<string, VectorDocument<T>> _documents;\n+        private readonly string _collectionName;\n+        private int _vectorDimension;\n+\n+        public override int DocumentCount => _documents.Count;\n+        public override int VectorDimension => _vectorDimension;\n+\n+        public MilvusDocumentStore(string collectionName, int initialCapacity = 1000)\n+        {\n+            if (string.IsNullOrWhiteSpace(collectionName))\n+                throw new ArgumentException(\"Collection name cannot be empty\", nameof(collectionName));\n+            if (initialCapacity <= 0)\n+                throw new ArgumentException(\"Initial capacity must be greater than zero\", nameof(initialCapacity));\n+\n+            _collectionName = collectionName;\n+            _documents = new Dictionary<string, VectorDocument<T>>(initialCapacity);\n+            _vectorDimension = 0;\n+        }\n+\n+        protected override void AddCore(VectorDocument<T> vectorDocument)\n+        {\n+            if (_documents.Count == 0)\n+            {\n+                _vectorDimension = vectorDocument.Embedding.Length;\n+            }\n+\n+            _documents[vectorDocument.Document.Id] = vectorDocument;\n+        }\n+\n+        protected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)\n+        {\n+            var scoredDocuments = new List<(Document<T> Document, T Score)>();\n+\n+            var matchingDocuments = _documents.Values\n+                .Where(vectorDoc => MatchesFilters(vectorDoc.Document, metadataFilters));\n+\n+            foreach (var vectorDoc in matchingDocuments)\n+            {\n+                var similarity = StatisticsHelper<T>.CosineSimilarity(queryVector, vectorDoc.Embedding);\n+                scoredDocuments.Add((vectorDoc.Document, similarity));\n+            }\n+\n+            var results = scoredDocuments\n+                .OrderByDescending(x => x.Score)\n+                .Take(topK)\n+                .Select(x =>\n+                {\n+                    x.Document.RelevanceScore = x.Score;\n+                    x.Document.HasRelevanceScore = true;\n+                    return x.Document;\n+                })\n+                .ToList();\n+\n+            return results;\n+        }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/MilvusDocumentStore.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "5e2879cb116daba6f58fb4a68fc7f8d434e60eb4",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Avoid mutating stored document state with query-specific relevance scores.**\n\nLines 63-64 mutate the `RelevanceScore` and `HasRelevanceScore` properties of documents stored in the `_documents` dictionary. This causes state pollution: relevance scores from one query persist and will appear on the same document when retrieved via `GetById` or subsequent `GetSimilar` calls. Query-specific metadata should not leak into stored document state.\n\n\n\nApply this diff to return a shallow copy with relevance score instead of mutating the original:\n\n```diff\n             var results = scoredDocuments\n                 .OrderByDescending(x => x.Score)\n                 .Take(topK)\n                 .Select(x =>\n                 {\n-                    x.Document.RelevanceScore = x.Score;\n-                    x.Document.HasRelevanceScore = true;\n-                    return x.Document;\n+                    var doc = new Document<T>(x.Document.Id, x.Document.Content, x.Document.Metadata)\n+                    {\n+                        RelevanceScore = x.Score,\n+                        HasRelevanceScore = true\n+                    };\n+                    return doc;\n                 })\n                 .ToList();\n```\n\nNote: This assumes `Document<T>` has a copy constructor or properties that allow initialization. Adjust the copy pattern based on the actual `Document<T>` API.\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/MilvusDocumentStore.cs around\nlines 45 to 70, the code currently mutates stored documents by setting\nRelevanceScore and HasRelevanceScore on the original objects when building query\nresults; instead create and return shallow copies of each Document<T>, set\nRelevanceScore and HasRelevanceScore on the copies only, and leave the objects\nin the _documents dictionary unchanged. Use the Document<T> copy constructor or\nconstruct a new Document<T> duplicating all relevant properties (Id, Content,\nMetadata, Embedding, etc.), assign the computed score and HasRelevanceScore =\ntrue on the copy, and return those copies in the results; ensure the original\nstored documents are never modified so query-specific metadata does not persist.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T14:21:43Z",
    "updated_at": "2025-11-04T14:21:49Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721755",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721755"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721755"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721755/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 132,
    "original_start_line": 45,
    "start_side": "RIGHT",
    "line": 157,
    "original_line": 70,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 70,
    "position": 157,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721765",
    "pull_request_review_id": 3416751163,
    "id": 2490721765,
    "node_id": "PRRC_kwDOKSXUF86UdWXl",
    "diff_hunk": "@@ -0,0 +1,93 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores\n+{\n+    /// <summary>\n+    /// Pinecone-inspired document store with index-based vector organization.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class PineconeDocumentStore<T> : DocumentStoreBase<T>\n+    {\n+        private readonly Dictionary<string, VectorDocument<T>> _documents;\n+        private readonly string _indexName;\n+        private int _vectorDimension;\n+\n+        public override int DocumentCount => _documents.Count;\n+        public override int VectorDimension => _vectorDimension;\n+\n+        public PineconeDocumentStore(string indexName, int initialCapacity = 1000)\n+        {\n+            if (string.IsNullOrWhiteSpace(indexName))\n+                throw new ArgumentException(\"Index name cannot be empty\", nameof(indexName));\n+            if (initialCapacity <= 0)\n+                throw new ArgumentException(\"Initial capacity must be greater than zero\", nameof(initialCapacity));\n+\n+            _indexName = indexName;\n+            _documents = new Dictionary<string, VectorDocument<T>>(initialCapacity);\n+            _vectorDimension = 0;\n+        }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/PineconeDocumentStore.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "5e2879cb116daba6f58fb4a68fc7f8d434e60eb4",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**The `indexName` parameter is stored but never used.**\n\nThe `_indexName` field is set in the constructor but is not referenced anywhere else in the class. Consider either using this field for logging/diagnostics, or removing it if not needed for the current implementation.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T14:21:43Z",
    "updated_at": "2025-11-04T14:21:49Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721765",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721765"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721765"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721765/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 75,
    "original_start_line": 23,
    "start_side": "RIGHT",
    "line": 85,
    "original_line": 33,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 33,
    "position": 85,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721778",
    "pull_request_review_id": 3416751163,
    "id": 2490721778,
    "node_id": "PRRC_kwDOKSXUF86UdWXy",
    "diff_hunk": "@@ -0,0 +1,93 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores\n+{\n+    /// <summary>\n+    /// PostgreSQL with pgvector-inspired document store for relational database vector storage.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class PostgresVectorDocumentStore<T> : DocumentStoreBase<T>",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/PostgresVectorDocumentStore.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "5e2879cb116daba6f58fb4a68fc7f8d434e60eb4",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**CRITICAL: Misleading class name and documentation - no actual PostgreSQL integration.**\n\nThe class is named `PostgresVectorDocumentStore` and documented as \"PostgreSQL with pgvector-inspired document store for relational database vector storage,\" but the implementation is just an in-memory `Dictionary` with no database connection whatsoever. This creates a false expectation that data will be persisted to PostgreSQL with pgvector, when in reality all data is held in memory and lost when the process ends.\n\nEither:\n1. Implement actual PostgreSQL/pgvector integration with database connections, schema creation, and vector operations, OR\n2. Rename to `InMemoryVectorDocumentStore` or similar and update the documentation to clarify it's an in-memory implementation.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T14:21:44Z",
    "updated_at": "2025-11-04T14:21:49Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721778",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721778"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721778"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721778/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 10,
    "original_start_line": 10,
    "start_side": "RIGHT",
    "line": 39,
    "original_line": 14,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 14,
    "position": 39,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721789",
    "pull_request_review_id": 3416751163,
    "id": 2490721789,
    "node_id": "PRRC_kwDOKSXUF86UdWX9",
    "diff_hunk": "@@ -0,0 +1,93 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores\n+{\n+    /// <summary>\n+    /// PostgreSQL with pgvector-inspired document store for relational database vector storage.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class PostgresVectorDocumentStore<T> : DocumentStoreBase<T>\n+    {\n+        private readonly Dictionary<string, VectorDocument<T>> _documents;\n+        private readonly string _tableName;\n+        private int _vectorDimension;\n+\n+        public override int DocumentCount => _documents.Count;\n+        public override int VectorDimension => _vectorDimension;\n+\n+        public PostgresVectorDocumentStore(string tableName, int initialCapacity = 1000)\n+        {\n+            if (string.IsNullOrWhiteSpace(tableName))\n+                throw new ArgumentException(\"Table name cannot be empty\", nameof(tableName));\n+            if (initialCapacity <= 0)\n+                throw new ArgumentException(\"Initial capacity must be greater than zero\", nameof(initialCapacity));\n+\n+            _tableName = tableName;\n+            _documents = new Dictionary<string, VectorDocument<T>>(initialCapacity);\n+            _vectorDimension = 0;\n+        }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/PostgresVectorDocumentStore.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "5e2879cb116daba6f58fb4a68fc7f8d434e60eb4",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**The `tableName` parameter is stored but never used.**\n\nThe constructor requires a `tableName` parameter and validates it, but the field is never referenced after initialization. Since this is an in-memory store with no actual database table, this parameter serves no purpose and should be removed (along with the `_tableName` field at line 17).\n\n\n\nApply this diff to remove the unused parameter and field:\n\n```diff\n-        private readonly string _tableName;\n         private int _vectorDimension;\n \n         public override int DocumentCount => _documents.Count;\n         public override int VectorDimension => _vectorDimension;\n \n-        public PostgresVectorDocumentStore(string tableName, int initialCapacity = 1000)\n+        public PostgresVectorDocumentStore(int initialCapacity = 1000)\n         {\n-            if (string.IsNullOrWhiteSpace(tableName))\n-                throw new ArgumentException(\"Table name cannot be empty\", nameof(tableName));\n             if (initialCapacity <= 0)\n                 throw new ArgumentException(\"Initial capacity must be greater than zero\", nameof(initialCapacity));\n \n-            _tableName = tableName;\n             _documents = new Dictionary<string, VectorDocument<T>>(initialCapacity);\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn\nsrc/RetrievalAugmentedGeneration/DocumentStores/PostgresVectorDocumentStore.cs\naround lines 23 to 33, the constructor accepts and validates a tableName\nparameter but the private field _tableName (at line 17) and the parameter are\nnever used; remove the unused field declaration and change the constructor to no\nlonger take tableName (remove its validation checks), update any call sites to\nuse the new constructor signature (or provide default parameters if necessary),\nand ensure the rest of the class compiles without references to _tableName.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T14:21:44Z",
    "updated_at": "2025-11-04T14:21:49Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721789",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721789"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721789"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721789/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 76,
    "original_start_line": 23,
    "start_side": "RIGHT",
    "line": 86,
    "original_line": 33,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 33,
    "position": 86,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721799",
    "pull_request_review_id": 3416751163,
    "id": 2490721799,
    "node_id": "PRRC_kwDOKSXUF86UdWYH",
    "diff_hunk": "@@ -0,0 +1,93 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores\n+{\n+    /// <summary>\n+    /// PostgreSQL with pgvector-inspired document store for relational database vector storage.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class PostgresVectorDocumentStore<T> : DocumentStoreBase<T>\n+    {\n+        private readonly Dictionary<string, VectorDocument<T>> _documents;\n+        private readonly string _tableName;\n+        private int _vectorDimension;\n+\n+        public override int DocumentCount => _documents.Count;\n+        public override int VectorDimension => _vectorDimension;\n+\n+        public PostgresVectorDocumentStore(string tableName, int initialCapacity = 1000)\n+        {\n+            if (string.IsNullOrWhiteSpace(tableName))\n+                throw new ArgumentException(\"Table name cannot be empty\", nameof(tableName));\n+            if (initialCapacity <= 0)\n+                throw new ArgumentException(\"Initial capacity must be greater than zero\", nameof(initialCapacity));\n+\n+            _tableName = tableName;\n+            _documents = new Dictionary<string, VectorDocument<T>>(initialCapacity);\n+            _vectorDimension = 0;\n+        }\n+\n+        protected override void AddCore(VectorDocument<T> vectorDocument)\n+        {\n+            if (_documents.Count == 0)\n+            {\n+                _vectorDimension = vectorDocument.Embedding.Length;\n+            }\n+\n+            _documents[vectorDocument.Document.Id] = vectorDocument;\n+        }\n+\n+        protected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)\n+        {\n+            var scoredDocuments = new List<(Document<T> Document, T Score)>();\n+\n+            var matchingDocuments = _documents.Values\n+                .Where(vectorDoc => MatchesFilters(vectorDoc.Document, metadataFilters));\n+\n+            foreach (var vectorDoc in matchingDocuments)\n+            {\n+                var similarity = StatisticsHelper<T>.CosineSimilarity(queryVector, vectorDoc.Embedding);\n+                scoredDocuments.Add((vectorDoc.Document, similarity));\n+            }\n+\n+            var results = scoredDocuments\n+                .OrderByDescending(x => x.Score)\n+                .Take(topK)\n+                .Select(x =>\n+                {\n+                    x.Document.RelevanceScore = x.Score;\n+                    x.Document.HasRelevanceScore = true;\n+                    return x.Document;\n+                })\n+                .ToList();\n+\n+            return results;\n+        }\n+\n+        protected override Document<T>? GetByIdCore(string documentId)\n+        {\n+            return _documents.TryGetValue(documentId, out var vectorDoc) ? vectorDoc.Document : null;\n+        }\n+\n+        protected override bool RemoveCore(string documentId)\n+        {\n+            var removed = _documents.Remove(documentId);\n+            if (removed && _documents.Count == 0)\n+            {\n+                _vectorDimension = 0;\n+            }\n+            return removed;\n+        }\n+\n+        public override void Clear()\n+        {\n+            _documents.Clear();\n+            _vectorDimension = 0;\n+        }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/PostgresVectorDocumentStore.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "5e2879cb116daba6f58fb4a68fc7f8d434e60eb4",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_≡ƒ¢á∩╕Å Refactor suggestion_ | _≡ƒƒá Major_\n\n**Essential refactor: Eliminate code duplication with FAISSDocumentStore.**\n\nThis entire implementation is nearly identical to `FAISSDocumentStore<T>` (see relevant code snippets). Both use the same Dictionary-based in-memory storage, identical similarity search logic, and the same metadata filtering. The only differences are:\n- FAISS has unused `_indexedVectors` and `_currentIndex` fields\n- This class has an unused `_tableName` field\n\nThis violates the DRY principle and creates a maintenance burden.\n\nConsider one of these approaches:\n1. Extract shared logic into a common `InMemoryDocumentStore<T>` base class that both can extend\n2. Eliminate this class entirely and use a properly-named in-memory store\n3. If this class is meant to have PostgreSQL integration, implement the actual database functionality\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn\nsrc/RetrievalAugmentedGeneration/DocumentStores/PostgresVectorDocumentStore.cs\naround lines 35 to 91, the class reimplements in-memory dictionary storage and\nsimilarity/filtering logic already present in FAISSDocumentStore<T>; refactor to\nremove duplication by extracting the shared behavior into a new\nInMemoryDocumentStore<T> base class (move _documents, _vectorDimension and\nimplementations of AddCore, GetSimilarCore, GetByIdCore, RemoveCore, Clear and\nMatchesFilters there), have both PostgresVectorDocumentStore<T> and\nFAISSDocumentStore<T> inherit from it, delete the duplicated implementations in\nboth children, and either remove the unused _tableName field from\nPostgresVectorDocumentStore or replace the in-memory implementations with actual\nPostgreSQL-backed logic if Postgres integration is required.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T14:21:44Z",
    "updated_at": "2025-11-04T14:21:50Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721799",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721799"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721799"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721799/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 98,
    "original_start_line": 35,
    "start_side": "RIGHT",
    "line": 328,
    "original_line": 91,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 91,
    "position": 328,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721811",
    "pull_request_review_id": 3416751163,
    "id": 2490721811,
    "node_id": "PRRC_kwDOKSXUF86UdWYT",
    "diff_hunk": "@@ -0,0 +1,65 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels;\n+\n+/// <summary>\n+/// Cohere embedding model integration for high-performance embeddings.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Cohere provides state-of-the-art embeddings with multiple model sizes optimized\n+/// for different use cases (English, multilingual, search, classification).\n+/// </remarks>\n+public class CohereEmbeddingModel<T> : EmbeddingModelBase<T>\n+{\n+    private readonly string _apiKey;\n+    private readonly string _model;\n+    private readonly string _inputType;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"CohereEmbeddingModel{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"apiKey\">The Cohere API key.</param>\n+    /// <param name=\"model\">The model name (e.g., \"embed-english-v3.0\").</param>\n+    /// <param name=\"inputType\">The input type (\"search_document\" or \"search_query\").</param>\n+    /// <param name=\"dimension\">The embedding dimension.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public CohereEmbeddingModel(\n+        string apiKey,\n+        string model,\n+        string inputType,\n+        int dimension,\n+        INumericOperations<T> numericOperations)\n+        : base(dimension, numericOperations)\n+    {\n+        _apiKey = apiKey ?? throw new ArgumentNullException(nameof(apiKey));\n+        _model = model ?? throw new ArgumentNullException(nameof(model));\n+        _inputType = inputType ?? throw new ArgumentNullException(nameof(inputType));\n+    }\n+\n+    /// <summary>\n+    /// Generates embeddings using Cohere API.\n+    /// </summary>\n+    public override Vector<T> Embed(string text)\n+    {\n+        if (string.IsNullOrWhiteSpace(text))\n+            throw new ArgumentException(\"Text cannot be null or whitespace\", nameof(text));\n+\n+        // TODO: Implement Cohere API call\n+        throw new NotImplementedException(\"Cohere integration requires HTTP client implementation\");\n+    }\n+\n+    /// <summary>\n+    /// Batch embedding generation.\n+    /// </summary>\n+    public override IEnumerable<Vector<T>> EmbedBatch(IEnumerable<string> texts)\n+    {\n+        if (texts == null)\n+            throw new ArgumentNullException(nameof(texts));\n+\n+        // TODO: Implement Cohere batch API call\n+        throw new NotImplementedException(\"Cohere integration requires HTTP client implementation\");\n+    }",
    "path": "src/RetrievalAugmentedGeneration/EmbeddingModels/CohereEmbeddingModel.cs",
    "commit_id": "e0f1f5e5a5a165cae3753c9567d549f34e91daf5",
    "original_commit_id": "5e2879cb116daba6f58fb4a68fc7f8d434e60eb4",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Non-functional implementation contradicts PR completion claims.**\n\nBoth embedding methods throw `NotImplementedException`, making this model unusable in production. The PR summary states \"Implementations completed\" and closes #303, but this class provides no actual embedding functionality.\n\nRecommend either:\n1. Complete the Cohere API integration before merging\n2. Mark the class as `internal` until implementation is ready\n3. Update PR description to clarify these are scaffolding/stubs, not complete implementations\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/EmbeddingModels/CohereEmbeddingModel.cs\naround lines 45 to 64, both Embed and EmbedBatch currently throw\nNotImplementedException which contradicts the PR claim of completed\nimplementations; replace these stubs with a real Cohere API integration (use the\nconfigured HTTP client, call Cohere embed endpoint, handle authentication,\ndeserialize vectors into Vector<T> and return them; implement batching and\nretry/error handling for EmbedBatch) or, if you cannot implement now, change the\nclass accessibility to internal and update the PR description to state these are\nscaffolds, ensuring tests and consumers are updated accordingly.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 80630e8 to 17e2d69",
    "created_at": "2025-11-04T14:21:44Z",
    "updated_at": "2025-11-05T02:38:09Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721811",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721811"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721811"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721811/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 45,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 64,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 64,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721823",
    "pull_request_review_id": 3416751163,
    "id": 2490721823,
    "node_id": "PRRC_kwDOKSXUF86UdWYf",
    "diff_hunk": "@@ -0,0 +1,69 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels;\n+\n+/// <summary>\n+/// Google PaLM embedding model integration.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Provides access to Google's PaLM (Pathways Language Model) embedding capabilities\n+/// through the Google Cloud Vertex AI platform.\n+/// </remarks>\n+public class GooglePalmEmbeddingModel<T> : EmbeddingModelBase<T>\n+{\n+    private readonly string _projectId;\n+    private readonly string _location;\n+    private readonly string _model;\n+    private readonly string _apiKey;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"GooglePalmEmbeddingModel{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"projectId\">The Google Cloud project ID.</param>\n+    /// <param name=\"location\">The Google Cloud location.</param>\n+    /// <param name=\"model\">The PaLM model name.</param>\n+    /// <param name=\"apiKey\">The API key for authentication.</param>\n+    /// <param name=\"dimension\">The embedding dimension.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public GooglePalmEmbeddingModel(\n+        string projectId,\n+        string location,\n+        string model,\n+        string apiKey,\n+        int dimension,\n+        INumericOperations<T> numericOperations)\n+        : base(dimension, numericOperations)\n+    {\n+        _projectId = projectId ?? throw new ArgumentNullException(nameof(projectId));\n+        _location = location ?? throw new ArgumentNullException(nameof(location));\n+        _model = model ?? throw new ArgumentNullException(nameof(model));\n+        _apiKey = apiKey ?? throw new ArgumentNullException(nameof(apiKey));\n+    }\n+\n+    /// <summary>\n+    /// Generates embeddings using Google PaLM API.\n+    /// </summary>\n+    public override Vector<T> Embed(string text)\n+    {\n+        if (string.IsNullOrWhiteSpace(text))\n+            throw new ArgumentException(\"Text cannot be null or whitespace\", nameof(text));\n+\n+        // TODO: Implement Google PaLM API call\n+        throw new NotImplementedException(\"Google PaLM integration requires HTTP client implementation\");\n+    }\n+\n+    /// <summary>\n+    /// Batch embedding generation.\n+    /// </summary>\n+    public override IEnumerable<Vector<T>> EmbedBatch(IEnumerable<string> texts)\n+    {\n+        if (texts == null)\n+            throw new ArgumentNullException(nameof(texts));\n+\n+        // TODO: Implement Google PaLM batch API call\n+        throw new NotImplementedException(\"Google PaLM integration requires HTTP client implementation\");\n+    }",
    "path": "src/RetrievalAugmentedGeneration/EmbeddingModels/GooglePalmEmbeddingModel.cs",
    "commit_id": "e0f1f5e5a5a165cae3753c9567d549f34e91daf5",
    "original_commit_id": "5e2879cb116daba6f58fb4a68fc7f8d434e60eb4",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Non-functional implementation blocks production use.**\n\nBoth `Embed` and `EmbedBatch` throw `NotImplementedException`, making this class unusable. While the PR summary claims \"Implementations completed\" and marks #303 as closed, this embedding model provides no actual functionality.\n\nIf HTTP client integration is pending, consider marking the class `internal` until the implementation is complete, or update the PR description to clarify that this is scaffolding rather than a complete implementation.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/EmbeddingModels/GooglePalmEmbeddingModel.cs\naround lines 49ΓÇô68, both Embed and EmbedBatch currently throw\nNotImplementedException making the class unusable; either mark the class\ninternal until a full implementation is provided, or implement the embedding\ncalls: inject an HttpClient (or IHttpClientFactory) via the constructor,\nvalidate inputs (null/whitespace checks already present), call the Google PaLM\nembeddings endpoint with proper auth and request payload, parse the response\ninto Vector<T> instances, implement batching by sending multiple inputs in one\nrequest (or chunking when needed), handle HTTP errors/retries and map exceptions\nto meaningful exceptions, add unit/integration tests, and update the PR\ndescription to reflect whether this is scaffolding or a complete implementation.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 80630e8 to b6555ed",
    "created_at": "2025-11-04T14:21:44Z",
    "updated_at": "2025-11-05T02:31:37Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721823",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721823"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721823"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721823/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 49,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 68,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 68,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721846",
    "pull_request_review_id": 3416751163,
    "id": 2490721846,
    "node_id": "PRRC_kwDOKSXUF86UdWY2",
    "diff_hunk": "@@ -0,0 +1,90 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels;\n+\n+/// <summary>\n+/// Fine-tuner for sentence transformer models on domain-specific data.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Enables fine-tuning of pre-trained sentence transformer models on custom datasets\n+/// to improve embedding quality for specific domains or tasks.\n+/// </remarks>\n+public class SentenceTransformersFineTuner<T> : EmbeddingModelBase<T>\n+{\n+    private readonly string _baseModelPath;\n+    private readonly string _outputModelPath;\n+    private readonly int _epochs;\n+    private readonly T _learningRate;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"SentenceTransformersFineTuner{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"baseModelPath\">Path to the base model to fine-tune.</param>\n+    /// <param name=\"outputModelPath\">Path where fine-tuned model will be saved.</param>\n+    /// <param name=\"epochs\">Number of training epochs.</param>\n+    /// <param name=\"learningRate\">Learning rate for fine-tuning.</param>\n+    /// <param name=\"dimension\">The embedding dimension.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public SentenceTransformersFineTuner(\n+        string baseModelPath,\n+        string outputModelPath,\n+        int epochs,\n+        T learningRate,\n+        int dimension,\n+        INumericOperations<T> numericOperations)\n+        : base(dimension, numericOperations)\n+    {\n+        _baseModelPath = baseModelPath ?? throw new ArgumentNullException(nameof(baseModelPath));\n+        _outputModelPath = outputModelPath ?? throw new ArgumentNullException(nameof(outputModelPath));\n+        \n+        if (epochs <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(epochs), \"Epochs must be positive\");\n+            \n+        _epochs = epochs;\n+        _learningRate = learningRate;\n+    }\n+\n+    /// <summary>\n+    /// Fine-tunes the model on provided training data.\n+    /// </summary>\n+    /// <param name=\"trainingPairs\">Training pairs of (anchor, positive, negative) texts.</param>\n+    public void FineTune(IEnumerable<(string anchor, string positive, string negative)> trainingPairs)\n+    {\n+        if (trainingPairs == null)\n+            throw new ArgumentNullException(nameof(trainingPairs));\n+\n+        // TODO: Implement model fine-tuning\n+        // 1. Load base model\n+        // 2. Create training dataset from pairs\n+        // 3. Train using triplet loss or similar\n+        // 4. Save fine-tuned model\n+        throw new NotImplementedException(\"Fine-tuning requires ML framework integration\");\n+    }\n+\n+    /// <summary>\n+    /// Generates embeddings using the fine-tuned model.\n+    /// </summary>\n+    public override Vector<T> Embed(string text)\n+    {\n+        if (string.IsNullOrWhiteSpace(text))\n+            throw new ArgumentException(\"Text cannot be null or whitespace\", nameof(text));\n+\n+        // TODO: Implement embedding with fine-tuned model\n+        throw new NotImplementedException(\"Fine-tuned model embedding requires model loading implementation\");\n+    }\n+\n+    /// <summary>\n+    /// Batch embedding generation.\n+    /// </summary>\n+    public override IEnumerable<Vector<T>> EmbedBatch(IEnumerable<string> texts)\n+    {\n+        if (texts == null)\n+            throw new ArgumentNullException(nameof(texts));\n+\n+        // TODO: Implement batch embedding\n+        throw new NotImplementedException(\"Fine-tuned model embedding requires model loading implementation\");\n+    }",
    "path": "src/RetrievalAugmentedGeneration/EmbeddingModels/SentenceTransformersFineTuner.cs",
    "commit_id": "6670ddf78227de3c8c09d915b54d9d78a797dc7e",
    "original_commit_id": "5e2879cb116daba6f58fb4a68fc7f8d434e60eb4",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**No functional implementation across all methods.**\n\nAll three core methods (`FineTune`, `Embed`, and `EmbedBatch`) throw `NotImplementedException`. This class provides no actual fine-tuning or embedding functionality despite being listed as a completed deliverable in the PR objectives.\n\nThis is particularly problematic since:\n1. The PR summary states \"Implementations completed\" and closes #303\n2. This class is marked `public`, exposing non-functional APIs\n3. The class name promises fine-tuning capabilities that don't exist\n\nRecommend:\n1. Complete the ML framework integration before merging, OR\n2. Mark the class as `internal` until implementation is ready, OR  \n3. Reclassify this PR as scaffolding/interface work and update #303 tracking to reflect incomplete implementation\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn\nsrc/RetrievalAugmentedGeneration/EmbeddingModels/SentenceTransformersFineTuner.cs\naround lines 54 to 89, the three public methods FineTune, Embed and EmbedBatch\nare unimplemented and currently throw NotImplementedException while the class is\npublicly exposed and the PR claims the feature is complete; either implement the\nfunctionality by integrating the chosen ML framework (load base model, build\ntriplet dataset, train, save model for FineTune; load model and return\nembeddings for Embed and EmbedBatch, handling null/empty inputs and batching\nefficiently), or if implementation cannot be completed now change the class\nvisibility from public to internal and update the PR/issue (#303) and PR summary\nto mark this as scaffolding/incomplete, ensuring no public nonfunctional APIs\nare merged.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 80630e8 to b6555ed",
    "created_at": "2025-11-04T14:21:45Z",
    "updated_at": "2025-11-05T02:31:38Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721846",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721846"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721846"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721846/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 54,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 89,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 89,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721861",
    "pull_request_review_id": 3416751163,
    "id": 2490721861,
    "node_id": "PRRC_kwDOKSXUF86UdWZF",
    "diff_hunk": "@@ -0,0 +1,65 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels;\n+\n+/// <summary>\n+/// Voyage AI embedding model integration for high-performance embeddings.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Voyage AI provides specialized embedding models optimized for retrieval tasks\n+/// with industry-leading performance on benchmark datasets.\n+/// </remarks>\n+public class VoyageAIEmbeddingModel<T> : EmbeddingModelBase<T>\n+{\n+    private readonly string _apiKey;\n+    private readonly string _model;\n+    private readonly string _inputType;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"VoyageAIEmbeddingModel{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"apiKey\">The Voyage AI API key.</param>\n+    /// <param name=\"model\">The model name (e.g., \"voyage-02\").</param>\n+    /// <param name=\"inputType\">The input type (\"document\" or \"query\").</param>\n+    /// <param name=\"dimension\">The embedding dimension.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public VoyageAIEmbeddingModel(\n+        string apiKey,\n+        string model,\n+        string inputType,\n+        int dimension,\n+        INumericOperations<T> numericOperations)\n+        : base(dimension, numericOperations)\n+    {\n+        _apiKey = apiKey ?? throw new ArgumentNullException(nameof(apiKey));\n+        _model = model ?? throw new ArgumentNullException(nameof(model));\n+        _inputType = inputType ?? throw new ArgumentNullException(nameof(inputType));\n+    }",
    "path": "src/RetrievalAugmentedGeneration/EmbeddingModels/VoyageAIEmbeddingModel.cs",
    "commit_id": "80630e82c71aabc214615014532fd5063336af16",
    "original_commit_id": "5e2879cb116daba6f58fb4a68fc7f8d434e60eb4",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_≡ƒ¢á∩╕Å Refactor suggestion_ | _≡ƒƒá Major_\n\n**Add validation for empty strings, dimension range, and inputType values.**\n\nThe constructor validates for null but misses other invalid states:\n- `apiKey` and `model` can be empty strings\n- `dimension` can be zero or negative\n- `inputType` is documented as \"document\" or \"query\" but accepts any value\n\n\n\nApply this diff to strengthen validation:\n\n```diff\n     public VoyageAIEmbeddingModel(\n         string apiKey,\n         string model,\n         string inputType,\n         int dimension,\n         INumericOperations<T> numericOperations)\n         : base(dimension, numericOperations)\n     {\n-        _apiKey = apiKey ?? throw new ArgumentNullException(nameof(apiKey));\n-        _model = model ?? throw new ArgumentNullException(nameof(model));\n-        _inputType = inputType ?? throw new ArgumentNullException(nameof(inputType));\n+        if (string.IsNullOrWhiteSpace(apiKey))\n+            throw new ArgumentException(\"API key cannot be null or empty\", nameof(apiKey));\n+        if (string.IsNullOrWhiteSpace(model))\n+            throw new ArgumentException(\"Model name cannot be null or empty\", nameof(model));\n+        if (string.IsNullOrWhiteSpace(inputType))\n+            throw new ArgumentException(\"Input type cannot be null or empty\", nameof(inputType));\n+        if (inputType != \"document\" && inputType != \"query\")\n+            throw new ArgumentException(\"Input type must be 'document' or 'query'\", nameof(inputType));\n+        if (dimension <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(dimension), \"Dimension must be positive\");\n+\n+        _apiKey = apiKey;\n+        _model = model;\n+        _inputType = inputType;\n     }\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/EmbeddingModels/VoyageAIEmbeddingModel.cs\naround lines 29 to 40, the constructor currently only checks for nulls; update\nvalidation to reject empty/whitespace apiKey and model (use\nstring.IsNullOrWhiteSpace and throw ArgumentException), ensure dimension is a\npositive non-zero integer (throw ArgumentOutOfRangeException if <= 0), and\nvalidate inputType only accepts \"document\" or \"query\" (case-insensitive)\nthrowing ArgumentException for invalid values; keep existing parameter names in\nexceptions for clarity.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T14:21:45Z",
    "updated_at": "2025-11-04T14:21:50Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721861",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721861"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721861"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721861/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 29,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 40,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 40,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721874",
    "pull_request_review_id": 3416751163,
    "id": 2490721874,
    "node_id": "PRRC_kwDOKSXUF86UdWZS",
    "diff_hunk": "@@ -0,0 +1,65 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels;\n+\n+/// <summary>\n+/// Voyage AI embedding model integration for high-performance embeddings.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Voyage AI provides specialized embedding models optimized for retrieval tasks\n+/// with industry-leading performance on benchmark datasets.\n+/// </remarks>\n+public class VoyageAIEmbeddingModel<T> : EmbeddingModelBase<T>\n+{\n+    private readonly string _apiKey;\n+    private readonly string _model;\n+    private readonly string _inputType;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"VoyageAIEmbeddingModel{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"apiKey\">The Voyage AI API key.</param>\n+    /// <param name=\"model\">The model name (e.g., \"voyage-02\").</param>\n+    /// <param name=\"inputType\">The input type (\"document\" or \"query\").</param>\n+    /// <param name=\"dimension\">The embedding dimension.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public VoyageAIEmbeddingModel(\n+        string apiKey,\n+        string model,\n+        string inputType,\n+        int dimension,\n+        INumericOperations<T> numericOperations)\n+        : base(dimension, numericOperations)\n+    {\n+        _apiKey = apiKey ?? throw new ArgumentNullException(nameof(apiKey));\n+        _model = model ?? throw new ArgumentNullException(nameof(model));\n+        _inputType = inputType ?? throw new ArgumentNullException(nameof(inputType));\n+    }\n+\n+    /// <summary>\n+    /// Generates embeddings using Voyage AI API.\n+    /// </summary>\n+    public override Vector<T> Embed(string text)\n+    {\n+        if (string.IsNullOrWhiteSpace(text))\n+            throw new ArgumentException(\"Text cannot be null or whitespace\", nameof(text));\n+\n+        // TODO: Implement Voyage AI API call\n+        throw new NotImplementedException(\"Voyage AI integration requires HTTP client implementation\");\n+    }\n+\n+    /// <summary>\n+    /// Batch embedding generation.\n+    /// </summary>\n+    public override IEnumerable<Vector<T>> EmbedBatch(IEnumerable<string> texts)\n+    {\n+        if (texts == null)\n+            throw new ArgumentNullException(nameof(texts));\n+\n+        // TODO: Implement Voyage AI batch API call\n+        throw new NotImplementedException(\"Voyage AI integration requires HTTP client implementation\");\n+    }",
    "path": "src/RetrievalAugmentedGeneration/EmbeddingModels/VoyageAIEmbeddingModel.cs",
    "commit_id": "71413132e7592b3fd1e86c3d8bc32c7e2fdc1430",
    "original_commit_id": "5e2879cb116daba6f58fb4a68fc7f8d434e60eb4",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_≡ƒ¢á∩╕Å Refactor suggestion_ | _≡ƒƒá Major_\n\n**Implement `EmbedCore` and `EmbedBatchCore` instead of overriding public methods.**\n\nThe base class `EmbeddingModelBase<T>` uses a template method pattern: public `Embed` validates input via `ValidateText`, then calls the protected abstract `EmbedCore`. By overriding the public methods instead of implementing the protected core methods, this class bypasses the base class's architectural pattern.\n\n\n\nApply this diff to align with the base class pattern:\n\n```diff\n     /// <summary>\n-    /// Generates embeddings using Voyage AI API.\n+    /// Core embedding logic using Voyage AI API.\n     /// </summary>\n-    public override Vector<T> Embed(string text)\n+    protected override Vector<T> EmbedCore(string text)\n     {\n-        if (string.IsNullOrWhiteSpace(text))\n-            throw new ArgumentException(\"Text cannot be null or whitespace\", nameof(text));\n-\n         // TODO: Implement Voyage AI API call\n         throw new NotImplementedException(\"Voyage AI integration requires HTTP client implementation\");\n     }\n\n     /// <summary>\n-    /// Batch embedding generation.\n+    /// Core batch embedding logic using Voyage AI API.\n     /// </summary>\n-    public override IEnumerable<Vector<T>> EmbedBatch(IEnumerable<string> texts)\n+    protected override Matrix<T> EmbedBatchCore(IList<string> texts)\n     {\n-        if (texts == null)\n-            throw new ArgumentNullException(nameof(texts));\n-\n         // TODO: Implement Voyage AI batch API call\n         throw new NotImplementedException(\"Voyage AI integration requires HTTP client implementation\");\n     }\n```\n\nNote: The base class already provides input validation in `Embed` and `EmbedBatch`, so `EmbedCore` implementations don't need redundant checks.\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits e0f1f5e to 0147db6",
    "created_at": "2025-11-04T14:21:45Z",
    "updated_at": "2025-11-04T14:59:06Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721874",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721874"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721874"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721874/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 45,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 64,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 64,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721887",
    "pull_request_review_id": 3416751163,
    "id": 2490721887,
    "node_id": "PRRC_kwDOKSXUF86UdWZf",
    "diff_hunk": "@@ -0,0 +1,34 @@\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Evaluation;\n+\n+/// <summary>\n+/// Evaluates the model's robustness to noisy or irrelevant documents in the context.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// Tests how well the RAG system performs when irrelevant documents are included\n+/// in the retrieved context, measuring ability to filter signal from noise.\n+/// </remarks>\n+public class NoiseRobustnessMetric<T> : RAGMetricBase<T>\n+{\n+    private readonly T _noiseRatio;\n+\n+    public override string Name => \"Noise Robustness\";\n+    public override string Description => \"Evaluates robustness to noisy or irrelevant documents in the context\";\n+    protected override bool RequiresGroundTruth => false;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"NoiseRobustnessMetric{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"noiseRatio\">Ratio of noise documents to inject (0-1).</param>\n+    public NoiseRobustnessMetric(T noiseRatio)\n+    {\n+        _noiseRatio = noiseRatio;\n+    }\n+\n+    protected override T EvaluateCore(GroundedAnswer<T> answer, string? groundTruth)\n+    {\n+        throw new NotImplementedException(\"Noise robustness evaluation requires answer generation capability\");\n+    }",
    "path": "src/RetrievalAugmentedGeneration/Evaluation/NoiseRobustnessMetric.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "5e2879cb116daba6f58fb4a68fc7f8d434e60eb4",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n<details>\n<summary>≡ƒº⌐ Analysis chain</summary>\n\n**Critical: Incomplete implementation contradicts PR objectives.**\n\nThe `EvaluateCore` method throws `NotImplementedException`, leaving this metric non-functional. This contradicts the PR description which states \"Implementations completed\" and marks components as \"ready for integration testing.\"\n\nIf this metric requires additional infrastructure (answer generation capability) that isn't yet available, either:\n1. Implement the evaluation logic using currently available components, or\n2. Remove this class from the PR and track it separately until dependencies are ready.\n\nMerging incomplete stubs defeats the purpose of delivering \"concrete implementations\" per Issue #303.\n\n\n\n\nCan you clarify the plan for completing this implementation? If it depends on missing infrastructure, please provide details so we can assess whether it should be included in this PR.\n\n---\n\nI need to verify the claims in the review comment. Let me generate shell scripts to investigate:\n\n\n\n\n\n\n\n\n\n\n\n```\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Find the file and examine the complete class structure\ncat \"src/RetrievalAugmentedGeneration/Evaluation/NoiseRobustnessMetric.cs\"\n```\n\nLength of output: 1334\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Find the base class to understand the contract\nfind . -name \"*.cs\" -type f | xargs grep -l \"class.*Metric\" | head -20\n```\n\nLength of output: 587\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Search for all EvaluateCore implementations to compare completeness\nrg -n \"EvaluateCore\" --type cs -A 5\n```\n\nLength of output: 5184\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Search for TODOs or FIXMEs related to NoiseRobustness\nrg -n \"NoiseRobustness|noise.robustness\" --type cs -i -B 2 -A 2\n```\n\nLength of output: 2508\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check PR description or issue references in the repository\nfind . -name \"*.md\" -o -name \"*.txt\" | xargs grep -l \"Issue #303\\|concrete implementations\" 2>/dev/null | head -10\n```\n\nLength of output: 0\n\n---\n\nI can confirm the implementation is incomplete, but I need to verify the PR description claims. Let me search for PR metadata:\n\n\n\n\n\n\n\n\n\n```\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Look for PR or project metadata files\nfind . -name \"*.json\" -o -name \"*.yaml\" -o -name \"*.yml\" -o -name \".github\" -type d | head -20\n```\n\nLength of output: 552\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Search for CHANGELOG or similar documentation\nfind . -name \"CHANGELOG*\" -o -name \"changelog*\" -o -name \"CHANGES*\" -o -name \"*.md\" -type f | xargs ls -la 2>/dev/null | head -20\n```\n\nLength of output: 903\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Look for any references to Issue 303 or PR objectives more broadly\nrg -i \"issue.*303|implementations.*complete|integration.*test\" --type md --type txt --type json -C 2 | head -40\n```\n\nLength of output: 0\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check if there are GitHub workflow files or PR templates\nfind . -path \"*/.github/*\" -type f 2>/dev/null | head -20\n```\n\nLength of output: 654\n\n---\n\n\n\n</details>\n\n**Incomplete implementation requires resolution before merge.**\n\nThe `EvaluateCore` method in `NoiseRobustnessMetric.cs` (lines 30-33) throws `NotImplementedException`. This is the only stub implementation in the metrics moduleΓÇöall other metrics (`AnswerCorrectnessMetric`, `FaithfulnessMetric`, `ContextRelevanceMetric`, `AnswerSimilarityMetric`, `ContextCoverageMetric`) have functional implementations.\n\nAdditionally, the `_noiseRatio` field initialized in the constructor is never used, further confirming this is incomplete.\n\nEither implement the evaluation logic or remove this class from the PR. Merging incomplete stubs undermines the reliability of the codebase.\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 80630e8 to b6555ed",
    "created_at": "2025-11-04T14:21:45Z",
    "updated_at": "2025-11-05T02:31:40Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721887",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721887"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721887"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721887/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 114,
    "original_start_line": 30,
    "start_side": "RIGHT",
    "line": 149,
    "original_line": 33,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 33,
    "position": 149,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721897",
    "pull_request_review_id": 3416751163,
    "id": 2490721897,
    "node_id": "PRRC_kwDOKSXUF86UdWZp",
    "diff_hunk": "@@ -0,0 +1,60 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.QueryExpansion;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.QueryExpansion;\n+\n+/// <summary>\n+/// Learned sparse encoder expansion using models like SPLADE.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// Uses a learned sparse model (e.g., SPLADE) to expand queries with relevant terms\n+/// weighted by their importance, combining benefits of sparse and dense retrieval.\n+/// </remarks>\n+public class LearnedSparseEncoderExpansion : QueryExpansionBase",
    "path": "src/RetrievalAugmentedGeneration/QueryExpansion/LearnedSparseEncoderExpansion.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "5e2879cb116daba6f58fb4a68fc7f8d434e60eb4",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Critical: Missing generic type parameter causes compilation failure.**\n\nThe class uses generic type `T` (line 18 field, line 31 constructor parameter) but does not declare it as a generic parameter. The base class also requires the generic parameter.\n\n\n\nApply this diff to add the generic parameter with the appropriate constraint:\n\n```diff\n-public class LearnedSparseEncoderExpansion : QueryExpansionBase\n+public class LearnedSparseEncoderExpansion<T> : QueryExpansionBase<T>\n+    where T : struct, IComparable, IConvertible, IFormattable\n {\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\npublic class LearnedSparseEncoderExpansion<T> : QueryExpansionBase<T>\n    where T : struct, IComparable, IConvertible, IFormattable\n{\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn\nsrc/RetrievalAugmentedGeneration/QueryExpansion/LearnedSparseEncoderExpansion.cs\naround line 14, the class declaration is missing the generic type parameter T\nused elsewhere and required by the base type; change the declaration from\n\"public class LearnedSparseEncoderExpansion : QueryExpansionBase\" to \"public\nclass LearnedSparseEncoderExpansion<T> : QueryExpansionBase<T>\" and, if\nQueryExpansionBase imposes any constraints on T (e.g., where T : class or an\ninterface), add the same constraint to this declaration so the field and\nconstructor usages of T compile.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T14:21:45Z",
    "updated_at": "2025-11-04T14:21:50Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721897",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721897"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721897"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721897/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": 61,
    "original_line": 14,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 14,
    "position": 61,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721908",
    "pull_request_review_id": 3416751163,
    "id": 2490721908,
    "node_id": "PRRC_kwDOKSXUF86UdWZ0",
    "diff_hunk": "@@ -0,0 +1,59 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.QueryExpansion;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.QueryExpansion;\n+\n+/// <summary>\n+/// Sub-query expansion that decomposes complex queries into simpler sub-queries.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// Breaks down complex multi-part queries into individual sub-queries that are\n+/// easier to retrieve for, then combines the results.\n+/// </remarks>\n+public class SubQueryExpansion : QueryExpansionBase",
    "path": "src/RetrievalAugmentedGeneration/QueryExpansion/SubQueryExpansion.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "5e2879cb116daba6f58fb4a68fc7f8d434e60eb4",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Add missing generic parameter to class declaration.**\n\nThe class references generic type `T` in its XML documentation (line 9), constructor parameter `INumericOperations<T>` (line 31), and base class, but the declaration on line 14 omits the generic parameter. This will cause compilation errors.\n\nApply this diff to add the generic parameter with the appropriate constraint:\n\n```diff\n-public class SubQueryExpansion : QueryExpansionBase\n+public class SubQueryExpansion<T> : QueryExpansionBase<T>\n+    where T : struct, IComparable, IConvertible, IFormattable\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\npublic class SubQueryExpansion<T> : QueryExpansionBase<T>\n    where T : struct, IComparable, IConvertible, IFormattable\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/QueryExpansion/SubQueryExpansion.cs around\nline 14, the class declaration is missing the generic parameter T referenced\nelsewhere; update the declaration to include the generic parameter and match the\nbase-class generic usage (e.g., change to SubQueryExpansion<T> :\nQueryExpansionBase<T>) and add the same generic constraint(s) used by\nINumericOperations<T> elsewhere in the codebase (for example: where T : struct,\nINumber<T> or whatever constraint is defined on\nQueryExpansionBase/INumericOperations) so the constructor parameter\nINumericOperations<T> and XML docs compile.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T14:21:46Z",
    "updated_at": "2025-11-04T14:21:50Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721908",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721908"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721908"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721908/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": 61,
    "original_line": 14,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 14,
    "position": 61,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721920",
    "pull_request_review_id": 3416751163,
    "id": 2490721920,
    "node_id": "PRRC_kwDOKSXUF86UdWaA",
    "diff_hunk": "@@ -0,0 +1,59 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.QueryExpansion;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.QueryExpansion;\n+\n+/// <summary>\n+/// Sub-query expansion that decomposes complex queries into simpler sub-queries.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// Breaks down complex multi-part queries into individual sub-queries that are\n+/// easier to retrieve for, then combines the results.\n+/// </remarks>\n+public class SubQueryExpansion : QueryExpansionBase\n+{\n+    private readonly string _llmEndpoint;\n+    private readonly string _llmApiKey;\n+    private readonly int _maxSubQueries;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"SubQueryExpansion{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"llmEndpoint\">The LLM API endpoint.</param>\n+    /// <param name=\"llmApiKey\">The API key for the LLM service.</param>\n+    /// <param name=\"maxSubQueries\">Maximum number of sub-queries to generate.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public SubQueryExpansion(\n+        string llmEndpoint,\n+        string llmApiKey,\n+        int maxSubQueries,\n+        INumericOperations<T> numericOperations)\n+        : base(numericOperations)\n+    {\n+        _llmEndpoint = llmEndpoint ?? throw new ArgumentNullException(nameof(llmEndpoint));\n+        _llmApiKey = llmApiKey ?? throw new ArgumentNullException(nameof(llmApiKey));\n+        \n+        if (maxSubQueries <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(maxSubQueries), \"Max sub-queries must be positive\");\n+            \n+        _maxSubQueries = maxSubQueries;\n+    }\n+\n+    /// <summary>\n+    /// Decomposes the query into sub-queries.\n+    /// </summary>\n+    public override IEnumerable<string> ExpandQuery(string query)\n+    {\n+        if (string.IsNullOrWhiteSpace(query))\n+            throw new ArgumentException(\"Query cannot be null or whitespace\", nameof(query));\n+\n+        // TODO: Implement sub-query decomposition\n+        // 1. Analyze query complexity\n+        // 2. If complex, send to LLM to decompose into sub-queries\n+        // 3. Return list of sub-queries\n+        // 4. If simple, return original query\n+        throw new NotImplementedException(\"Sub-query expansion requires LLM integration\");\n+    }",
    "path": "src/RetrievalAugmentedGeneration/QueryExpansion/SubQueryExpansion.cs",
    "commit_id": "6670ddf78227de3c8c09d915b54d9d78a797dc7e",
    "original_commit_id": "5e2879cb116daba6f58fb4a68fc7f8d434e60eb4",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n<details>\n<summary>≡ƒº⌐ Analysis chain</summary>\n\n**Verify implementation status against PR objectives.**\n\nThe method contains only a `NotImplementedException` with a TODO. The PR summary states \"implementations marked ready for integration testing,\" but this class lacks the core decomposition logic outlined in lines 51-55.\n\nRun the following script to check if other query expansion classes are similarly incomplete:\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check for NotImplementedException in query expansion implementations\nrg -n \"NotImplementedException\" --type cs -g '**/QueryExpansion/*.cs' -C 2\n```\n\nLength of output: 1779\n\n---\n\n\n\n</details>\n\n**All query expansion implementations are incomplete and block integration testing readiness.**\n\nVerification confirms SubQueryExpansion, MultiQueryExpansion, and LearnedSparseEncoderExpansion all contain only `NotImplementedException` (lines 56, 55, and 57 respectively). If the PR claims implementations are \"ready for integration testing,\" this is incorrectΓÇönone have the core decomposition, variation, or encoding logic outlined in their TODOs. All three must be implemented or the PR's integration testing objective cannot be met.\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 80630e8 to 17e2d69",
    "created_at": "2025-11-04T14:21:46Z",
    "updated_at": "2025-11-05T02:38:16Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721920",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721920"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721920"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721920/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 46,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 57,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 57,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721927",
    "pull_request_review_id": 3416751163,
    "id": 2490721927,
    "node_id": "PRRC_kwDOKSXUF86UdWaH",
    "diff_hunk": "@@ -0,0 +1,85 @@\n+using System;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.RerankingStrategies\n+{\n+    /// <summary>\n+    /// Cross-encoder based reranking for improved relevance scoring.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class CrossEncoderReranker<T> : RerankingStrategyBase<T>\n+    {\n+        private readonly string _modelPath;\n+\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"CrossEncoderReranker{T}\"/> class.\n+        /// </summary>\n+        /// <param name=\"modelPath\">The path to the cross-encoder model.</param>\n+        public CrossEncoderReranker(string? modelPath = null)\n+        {\n+            _modelPath = modelPath ?? string.Empty;\n+        }\n+\n+        /// <summary>\n+        /// Reranks documents using cross-encoder scoring.\n+        /// </summary>\n+        /// <param name=\"query\">The query string.</param>\n+        /// <param name=\"documents\">The documents to rerank.</param>\n+        /// <param name=\"topK\">The number of top documents to return.</param>\n+        /// <returns>A reranked list of documents.</returns>\n+        public override List<Document<T>> Rerank(string query, List<Document<T>> documents, int topK)\n+        {\n+            if (string.IsNullOrEmpty(query)) throw new ArgumentNullException(nameof(query));\n+            if (documents == null) throw new ArgumentNullException(nameof(documents));\n+            if (topK <= 0) throw new ArgumentOutOfRangeException(nameof(topK));\n+\n+            var scoredDocs = new List<(Document<T> doc, T score)>();\n+\n+            foreach (var doc in documents)\n+            {\n+                var score = ComputeCrossEncoderScore(query, doc.Content);\n+                scoredDocs.Add((doc, score));\n+            }\n+\n+            var reranked = scoredDocs\n+                .OrderByDescending(x => Convert.ToDouble(x.score))\n+                .Take(topK)\n+                .Select(x =>\n+                {\n+                    x.doc.RelevanceScore = x.score;\n+                    x.doc.HasRelevanceScore = true;\n+                    return x.doc;\n+                })\n+                .ToList();\n+\n+            return reranked;",
    "path": "src/RetrievalAugmentedGeneration/RerankingStrategies/CrossEncoderReranker.cs",
    "commit_id": "80630e82c71aabc214615014532fd5063336af16",
    "original_commit_id": "5e2879cb116daba6f58fb4a68fc7f8d434e60eb4",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Avoid `Convert.ToDouble` on generic scores.**\n\n`MathHelper.GetNumericOperations<T>()` allows `Complex<T>` and other non-`IConvertible` numeric types. Invoking `Convert.ToDouble(x.score)` will throw for those types, breaking reranking at runtime. Use the injected `INumericOperations<T>` (e.g., compare via `NumOps`) or stash the double you already compute before converting back to `T`, and order by that instead of re-converting.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/RerankingStrategies/CrossEncoderReranker.cs\naround lines 32 to 57, the code uses Convert.ToDouble(x.score) which will fail\nfor non-IConvertible numeric types (e.g., Complex<T>) from\nMathHelper.GetNumericOperations<T>(); replace this by using the injected\nINumericOperations<T> (NumOps) to order the scoredDocs (either call\nNumOps.Compare(x.score, y.score) or store the already-computed double score\nalongside the original T score and OrderByDescending on that double), then\nassign the original T score back to doc.RelevanceScore and set HasRelevanceScore\n= true before returning the topK documents.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T14:21:46Z",
    "updated_at": "2025-11-04T14:21:50Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721927",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721927"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721927"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721927/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 32,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 57,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 57,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721939",
    "pull_request_review_id": 3416751163,
    "id": 2490721939,
    "node_id": "PRRC_kwDOKSXUF86UdWaT",
    "diff_hunk": "@@ -0,0 +1,105 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using AiDotNet.RetrievalAugmentedGeneration.Rerankers;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.RerankingStrategies;\n+\n+/// <summary>\n+/// Diversity-based reranker that prioritizes variety among retrieved documents.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// Reduces redundancy by ensuring retrieved documents cover diverse aspects of the topic\n+/// rather than returning multiple similar documents.\n+/// </remarks>\n+public class DiversityReranker<T> : RerankerBase<T>\n+{\n+    private readonly T _diversityWeight;\n+    private readonly T _relevanceWeight;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"DiversityReranker{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"diversityWeight\">Weight for diversity component (0-1).</param>\n+    /// <param name=\"relevanceWeight\">Weight for relevance component (0-1).</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public DiversityReranker(\n+        T diversityWeight,\n+        T relevanceWeight,\n+        INumericOperations<T> numericOperations)\n+        : base(numericOperations)\n+    {\n+        _diversityWeight = diversityWeight;\n+        _relevanceWeight = relevanceWeight;\n+    }\n+\n+    /// <summary>\n+    /// Reranks documents balancing relevance and diversity.\n+    /// </summary>\n+    public override IEnumerable<Document<T>> Rerank(string query, IEnumerable<Document<T>> documents, int topK)\n+    {\n+        if (string.IsNullOrWhiteSpace(query))\n+            throw new ArgumentException(\"Query cannot be null or whitespace\", nameof(query));\n+\n+        if (documents == null)\n+            throw new ArgumentNullException(nameof(documents));\n+\n+        if (topK <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(topK), \"topK must be positive\");\n+\n+        var docList = documents.ToList();\n+        if (docList.Count == 0)\n+            return Enumerable.Empty<Document<T>>();\n+\n+        var selected = new List<Document<T>>();\n+        var remaining = new List<Document<T>>(docList);\n+\n+        // Select first document by relevance\n+        var first = remaining.OrderByDescending(d => d.HasRelevanceScore ? d.RelevanceScore : NumOps.Zero).First();\n+        selected.Add(first);\n+        remaining.Remove(first);\n+\n+        // Iteratively select documents balancing relevance and diversity\n+        while (selected.Count < topK && remaining.Count > 0)\n+        {\n+            var bestDoc = remaining[0];\n+            var bestScore = NumOps.FromDouble(double.NegativeInfinity);\n+\n+            foreach (var doc in remaining)\n+            {\n+                // Relevance score\n+                var relevance = doc.HasRelevanceScore ? doc.RelevanceScore : NumOps.Zero;\n+\n+                // Diversity score (minimum similarity to selected documents)\n+                var minSimilarity = NumOps.One;\n+                foreach (var selectedDoc in selected)\n+                {\n+                    var similarity = StatisticsHelper.JaccardSimilarity(doc.Content, selectedDoc.Content);\n+                    var simT = NumOps.FromDouble(similarity);\n+                    if (NumOps.LessThan(simT, minSimilarity))\n+                    {\n+                        minSimilarity = simT;\n+                    }\n+                }\n+\n+                // Combined score\n+                var score = NumOps.Add(\n+                    NumOps.Multiply(_relevanceWeight, relevance),\n+                    NumOps.Multiply(_diversityWeight, NumOps.Subtract(NumOps.One, minSimilarity))\n+                );\n+\n+                if (NumOps.GreaterThan(score, bestScore))\n+                {\n+                    bestScore = score;\n+                    bestDoc = doc;\n+                }\n+            }\n+\n+            selected.Add(bestDoc);\n+            remaining.Remove(bestDoc);\n+        }\n+\n+        return selected;\n+    }",
    "path": "src/RetrievalAugmentedGeneration/RerankingStrategies/DiversityReranker.cs",
    "commit_id": "80630e82c71aabc214615014532fd5063336af16",
    "original_commit_id": "5e2879cb116daba6f58fb4a68fc7f8d434e60eb4",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Restore the base-class contract so this type compiles.**\n\n`RerankerBase<T>` still exposes the abstract `ModifiesScores` property and `RerankCore(...)`. This class neither overrides them nor can it legally override `Rerank(...)` (the base method isn't virtual), so the project will not build. Please expose `ModifiesScores` and move your logic into `RerankCore` in order to satisfy the contract and keep the greedy selection behaviour.\n\n```diff\n@@\n-public override IEnumerable<Document<T>> Rerank(string query, IEnumerable<Document<T>> documents, int topK)\n-{\n-    if (string.IsNullOrWhiteSpace(query))\n-        throw new ArgumentException(\"Query cannot be null or whitespace\", nameof(query));\n-\n-    if (documents == null)\n-        throw new ArgumentNullException(nameof(documents));\n-\n-    if (topK <= 0)\n-        throw new ArgumentOutOfRangeException(nameof(topK), \"topK must be positive\");\n-\n-    var docList = documents.ToList();\n-    if (docList.Count == 0)\n-        return Enumerable.Empty<Document<T>>();\n-\n-    var selected = new List<Document<T>>();\n-    var remaining = new List<Document<T>>(docList);\n+public override bool ModifiesScores => true;\n+\n+protected override IEnumerable<Document<T>> RerankCore(string query, IList<Document<T>> documents)\n+{\n+    var selected = new List<Document<T>>();\n+    var remaining = new List<Document<T>>(documents);\n+\n+    if (remaining.Count == 0)\n+        return selected;\n@@\n-    return selected;\n+    return selected;\n }\n+\n+private T CalculateCombinedScore(Document<T> doc, IList<Document<T>> selected)\n+{\n+    var relevance = doc.HasRelevanceScore ? doc.RelevanceScore : NumOps.Zero;\n+    var minSimilarity = NumOps.One;\n+    foreach (var selectedDoc in selected)\n+    {\n+        var similarity = StatisticsHelper.JaccardSimilarity(doc.Content, selectedDoc.Content);\n+        var simT = NumOps.FromDouble(similarity);\n+        if (NumOps.LessThan(simT, minSimilarity))\n+        {\n+            minSimilarity = simT;\n+        }\n+    }\n+\n+    return NumOps.Add(\n+        NumOps.Multiply(_relevanceWeight, relevance),\n+        NumOps.Multiply(_diversityWeight, NumOps.Subtract(NumOps.One, minSimilarity))\n+    );\n+}\n```\n\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits e0f1f5e to b63203e",
    "created_at": "2025-11-04T14:21:46Z",
    "updated_at": "2025-11-04T14:57:18Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721939",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721939"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721939"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721939/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 16,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 104,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 104,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721953",
    "pull_request_review_id": 3416751163,
    "id": 2490721953,
    "node_id": "PRRC_kwDOKSXUF86UdWah",
    "diff_hunk": "@@ -0,0 +1,105 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using AiDotNet.RetrievalAugmentedGeneration.Rerankers;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.RerankingStrategies;\n+\n+/// <summary>\n+/// Diversity-based reranker that prioritizes variety among retrieved documents.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// Reduces redundancy by ensuring retrieved documents cover diverse aspects of the topic\n+/// rather than returning multiple similar documents.\n+/// </remarks>\n+public class DiversityReranker<T> : RerankerBase<T>\n+{\n+    private readonly T _diversityWeight;\n+    private readonly T _relevanceWeight;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"DiversityReranker{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"diversityWeight\">Weight for diversity component (0-1).</param>\n+    /// <param name=\"relevanceWeight\">Weight for relevance component (0-1).</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public DiversityReranker(\n+        T diversityWeight,\n+        T relevanceWeight,\n+        INumericOperations<T> numericOperations)\n+        : base(numericOperations)\n+    {\n+        _diversityWeight = diversityWeight;\n+        _relevanceWeight = relevanceWeight;\n+    }\n+\n+    /// <summary>\n+    /// Reranks documents balancing relevance and diversity.\n+    /// </summary>\n+    public override IEnumerable<Document<T>> Rerank(string query, IEnumerable<Document<T>> documents, int topK)\n+    {\n+        if (string.IsNullOrWhiteSpace(query))\n+            throw new ArgumentException(\"Query cannot be null or whitespace\", nameof(query));\n+\n+        if (documents == null)\n+            throw new ArgumentNullException(nameof(documents));\n+\n+        if (topK <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(topK), \"topK must be positive\");\n+\n+        var docList = documents.ToList();\n+        if (docList.Count == 0)\n+            return Enumerable.Empty<Document<T>>();\n+\n+        var selected = new List<Document<T>>();\n+        var remaining = new List<Document<T>>(docList);\n+\n+        // Select first document by relevance\n+        var first = remaining.OrderByDescending(d => d.HasRelevanceScore ? d.RelevanceScore : NumOps.Zero).First();\n+        selected.Add(first);\n+        remaining.Remove(first);\n+\n+        // Iteratively select documents balancing relevance and diversity\n+        while (selected.Count < topK && remaining.Count > 0)\n+        {\n+            var bestDoc = remaining[0];\n+            var bestScore = NumOps.FromDouble(double.NegativeInfinity);\n+\n+            foreach (var doc in remaining)\n+            {\n+                // Relevance score\n+                var relevance = doc.HasRelevanceScore ? doc.RelevanceScore : NumOps.Zero;\n+\n+                // Diversity score (minimum similarity to selected documents)\n+                var minSimilarity = NumOps.One;\n+                foreach (var selectedDoc in selected)\n+                {\n+                    var similarity = StatisticsHelper.JaccardSimilarity(doc.Content, selectedDoc.Content);\n+                    var simT = NumOps.FromDouble(similarity);\n+                    if (NumOps.LessThan(simT, minSimilarity))\n+                    {\n+                        minSimilarity = simT;\n+                    }\n+                }\n+\n+                // Combined score\n+                var score = NumOps.Add(\n+                    NumOps.Multiply(_relevanceWeight, relevance),\n+                    NumOps.Multiply(_diversityWeight, NumOps.Subtract(NumOps.One, minSimilarity))\n+                );\n+\n+                if (NumOps.GreaterThan(score, bestScore))\n+                {\n+                    bestScore = score;\n+                    bestDoc = doc;\n+                }\n+            }\n+\n+            selected.Add(bestDoc);\n+            remaining.Remove(bestDoc);\n+        }",
    "path": "src/RetrievalAugmentedGeneration/RerankingStrategies/DiversityReranker.cs",
    "commit_id": "6670ddf78227de3c8c09d915b54d9d78a797dc7e",
    "original_commit_id": "5e2879cb116daba6f58fb4a68fc7f8d434e60eb4",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Replace the `double.NegativeInfinity` sentinel with a safe guard.**\n\n`NumOps.FromDouble(double.NegativeInfinity)` throws for supported non-floating numeric types (e.g. `decimal`), so the reranker crashes before ranking starts. At the same time, the chosen document never has its updated score persisted, which leaves `HasRelevanceScore` false for downstream consumers. Track ΓÇ£bestΓÇ¥ via a boolean sentinel, guard against the empty case, and persist the score.\n\n```diff\n-        while (selected.Count < topK && remaining.Count > 0)\n+        while (selected.Count < documents.Count && remaining.Count > 0)\n         {\n-            var bestDoc = remaining[0];\n-            var bestScore = NumOps.FromDouble(double.NegativeInfinity);\n+            Document<T>? bestDoc = null;\n+            var hasBestScore = false;\n+            T bestScore = default!;\n@@\n-                if (NumOps.GreaterThan(score, bestScore))\n+                if (!hasBestScore || NumOps.GreaterThan(score, bestScore))\n                 {\n                     bestScore = score;\n                     bestDoc = doc;\n+                    hasBestScore = true;\n                 }\n             }\n \n-            selected.Add(bestDoc);\n-            remaining.Remove(bestDoc);\n+            if (!hasBestScore || bestDoc is null)\n+                break;\n+\n+            bestDoc.RelevanceScore = bestScore;\n+            bestDoc.HasRelevanceScore = true;\n+\n+            selected.Add(bestDoc);\n+            remaining.Remove(bestDoc);\n         }\n```\n\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits e0f1f5e to 0147db6",
    "created_at": "2025-11-04T14:21:46Z",
    "updated_at": "2025-11-04T14:59:18Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721953",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721953"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721953"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721953/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 64,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 101,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 101,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721958",
    "pull_request_review_id": 3416751163,
    "id": 2490721958,
    "node_id": "PRRC_kwDOKSXUF86UdWam",
    "diff_hunk": "@@ -0,0 +1,123 @@\n+using System;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System.Collections.Generic;\n+using System.Linq;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.RerankingStrategies\n+{\n+    /// <summary>\n+    /// Reciprocal Rank Fusion for combining multiple ranking lists.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class ReciprocalRankFusion<T> : RerankingStrategyBase<T>\n+    {\n+        private readonly INumericOperations<T> _numOps;\n+        private readonly int _k;\n+\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"ReciprocalRankFusion{T}\"/> class.\n+        /// </summary>\n+        /// <param name=\"numericOperations\">The numeric operations for type T.</param>\n+        /// <param name=\"k\">The constant k for reciprocal rank formula (default: 60).</param>\n+        public ReciprocalRankFusion(INumericOperations<T> numericOperations, int k = 60) : base(numericOperations)\n+        {\n+            _numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\n+            _k = k > 0 ? k : throw new ArgumentOutOfRangeException(nameof(k));\n+        }",
    "path": "src/RetrievalAugmentedGeneration/RerankingStrategies/ReciprocalRankFusion.cs",
    "commit_id": "841164e577273bc2ab7077b392c98869a9db522d",
    "original_commit_id": "5e2879cb116daba6f58fb4a68fc7f8d434e60eb4",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Fix the base-constructor call.**\n\n`RerankingStrategyBase<T>` exposes only the implicit parameterless constructor. Calling `base(numericOperations)` does not compile. Drop the base call and keep the guard on `numericOperations`.\n\n```diff\n-        public ReciprocalRankFusion(INumericOperations<T> numericOperations, int k = 60) : base(numericOperations)\n+        public ReciprocalRankFusion(INumericOperations<T> numericOperations, int k = 60)\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        public ReciprocalRankFusion(INumericOperations<T> numericOperations, int k = 60)\n        {\n            _numOps = numericOperations ?? throw new ArgumentNullException(nameof(numericOperations));\n            _k = k > 0 ? k : throw new ArgumentOutOfRangeException(nameof(k));\n        }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/RerankingStrategies/ReciprocalRankFusion.cs\naround lines 23 to 27, the constructor is incorrectly calling\nbase(numericOperations) though RerankingStrategyBase<T> only has a parameterless\nconstructor; remove the base(...) call from the constructor signature and retain\nthe existing null-guard for numericOperations and the k validation so the\nconstructor becomes parameterless on the base and still sets _numOps and _k\nafter validating inputs.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commit e0f1f5e",
    "created_at": "2025-11-04T14:21:46Z",
    "updated_at": "2025-11-04T14:32:51Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721958",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721958"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721958"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721958/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 23,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 27,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 27,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721971",
    "pull_request_review_id": 3416751163,
    "id": 2490721971,
    "node_id": "PRRC_kwDOKSXUF86UdWaz",
    "diff_hunk": "@@ -0,0 +1,76 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers;\n+\n+/// <summary>\n+/// ColBERT (Contextualized Late Interaction over BERT) retriever using token-level embeddings.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// ColBERT represents documents and queries as sets of token embeddings, enabling more precise\n+/// matching through contextualized token-level interactions. This provides better retrieval\n+/// quality than single-vector approaches while maintaining reasonable efficiency.\n+/// </remarks>\n+public class ColBERTRetriever<T> : RetrieverBase<T>\n+{\n+    private readonly string _modelPath;\n+    private readonly int _maxDocLength;\n+    private readonly int _maxQueryLength;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"ColBERTRetriever{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"documentStore\">The document store to retrieve from.</param>\n+    /// <param name=\"modelPath\">Path to the ColBERT model.</param>\n+    /// <param name=\"maxDocLength\">Maximum document length in tokens.</param>\n+    /// <param name=\"maxQueryLength\">Maximum query length in tokens.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public ColBERTRetriever(\n+        IDocumentStore<T> documentStore,\n+        string modelPath,\n+        int maxDocLength,\n+        int maxQueryLength,\n+        INumericOperations<T> numericOperations)\n+        : base(documentStore, numericOperations)\n+    {\n+        _modelPath = modelPath ?? throw new ArgumentNullException(nameof(modelPath));",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/ColBERTRetriever.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "5e2879cb116daba6f58fb4a68fc7f8d434e60eb4",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_≡ƒ¢á∩╕Å Refactor suggestion_ | _≡ƒƒá Major_\n\n**Enhance modelPath validation to check for empty/whitespace.**\n\nThe current validation only checks for null, but an empty or whitespace string would pass validation and likely cause issues later.\n\n\n\nApply this diff:\n\n```diff\n-        _modelPath = modelPath ?? throw new ArgumentNullException(nameof(modelPath));\n+        if (string.IsNullOrWhiteSpace(modelPath))\n+            throw new ArgumentException(\"Model path cannot be null or whitespace\", nameof(modelPath));\n+        \n+        _modelPath = modelPath;\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        if (string.IsNullOrWhiteSpace(modelPath))\n            throw new ArgumentException(\"Model path cannot be null or whitespace\", nameof(modelPath));\n        \n        _modelPath = modelPath;\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Retrievers/ColBERTRetriever.cs around line\n40, the constructor only checks modelPath for null but not for empty or\nwhitespace; update the validation to use string.IsNullOrWhiteSpace(modelPath)\nand throw an ArgumentException (or ArgumentNullException with a clear message)\nwhen the value is empty/whitespace so _modelPath is only assigned a non-empty\npath; ensure the exception uses nameof(modelPath) and preserves current behavior\nfor non-null, non-empty values.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T14:21:47Z",
    "updated_at": "2025-11-04T14:21:50Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721971",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721971"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721971"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721971/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": 98,
    "original_line": 40,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 40,
    "position": 98,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721982",
    "pull_request_review_id": 3416751163,
    "id": 2490721982,
    "node_id": "PRRC_kwDOKSXUF86UdWa-",
    "diff_hunk": "@@ -0,0 +1,67 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers;\n+\n+/// <summary>\n+/// Multi-vector retriever that assigns multiple vectors to each document.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// Instead of representing each document with a single vector, this retriever uses\n+/// multiple vectors to capture different aspects of the document's content, enabling\n+/// more nuanced similarity matching.\n+/// </remarks>\n+public class MultiVectorRetriever<T> : RetrieverBase<T>\n+{\n+    private readonly int _vectorsPerDocument;\n+    private readonly string _aggregationMethod;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"MultiVectorRetriever{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"documentStore\">The document store to retrieve from.</param>\n+    /// <param name=\"vectorsPerDocument\">Number of vectors per document.</param>\n+    /// <param name=\"aggregationMethod\">Method for aggregating scores (\"max\", \"mean\", \"weighted\").</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public MultiVectorRetriever(\n+        IDocumentStore<T> documentStore,\n+        int vectorsPerDocument,\n+        string aggregationMethod,\n+        INumericOperations<T> numericOperations)\n+        : base(documentStore, numericOperations)\n+    {\n+        if (vectorsPerDocument <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(vectorsPerDocument), \"Vectors per document must be positive\");\n+            \n+        _vectorsPerDocument = vectorsPerDocument;\n+        _aggregationMethod = aggregationMethod ?? throw new ArgumentNullException(nameof(aggregationMethod));\n+    }\n+\n+    /// <summary>\n+    /// Retrieves documents using multi-vector matching.\n+    /// </summary>\n+    protected override IEnumerable<Document<T>> RetrieveCore(\n+        string query,\n+        int topK,\n+        Dictionary<string, object> metadataFilters)\n+    {\n+        if (string.IsNullOrWhiteSpace(query))\n+            throw new ArgumentException(\"Query cannot be null or whitespace\", nameof(query));\n+\n+        if (topK <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(topK), \"topK must be positive\");\n+\n+        // TODO: Implement multi-vector retrieval\n+        // 1. Generate query vector(s)\n+        // 2. For each document:\n+        //    a. Compute similarity between query and each document vector\n+        //    b. Aggregate similarities using specified method (max/mean/weighted)\n+        // 3. Rank documents by aggregated similarity\n+        // 4. Return top-K documents\n+        throw new NotImplementedException(\"Multi-vector retrieval requires implementation\");\n+    }",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/MultiVectorRetriever.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "5e2879cb116daba6f58fb4a68fc7f8d434e60eb4",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Implement the RetrieveCore method before merging.**\n\nThe `RetrieveCore` method throws `NotImplementedException`, making this class non-functional. This contradicts the PR objectives, which state \"implementations marked ready for integration testing\" and \"Project builds with 0 errors.\" While the code compiles, this class cannot be used at runtime.\n\nThe TODO outlines the implementation steps, but the work must be completed. If this is intentional scaffolding for future work, consider:\n1. Making the class `internal` or marking it with `[Obsolete]` to prevent accidental usage\n2. Updating the PR description to clarify which implementations are complete vs. planned\n3. Moving incomplete implementations to a separate PR\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Retrievers/MultiVectorRetriever.cs around\nlines 47 to 66, the RetrieveCore method currently throws\nNotImplementedException; implement it by generating one or more query vectors\nfrom the input query, applying metadataFilters to limit candidate documents,\niterating each candidate document to compute similarity between the query\nvectors and each document vector, aggregating per-document similarities\naccording to the configured aggregation method (e.g., max, mean, weighted),\nsorting documents by aggregated similarity, and returning the topK Document<T>\nresults; ensure input validation remains, remove the NotImplementedException,\nand if you cannot implement now either make the class internal or mark it\n[Obsolete] and update the PR description to avoid accidental runtime use.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 80630e8 to b6555ed",
    "created_at": "2025-11-04T14:21:47Z",
    "updated_at": "2025-11-05T02:31:48Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721982",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721982"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2490721982"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2490721982/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 149,
    "original_start_line": 47,
    "start_side": "RIGHT",
    "line": 256,
    "original_line": 66,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 66,
    "position": 256,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051697",
    "pull_request_review_id": 3417217555,
    "id": 2491051697,
    "node_id": "PRRC_kwDOKSXUF86Uem6x",
    "diff_hunk": "@@ -0,0 +1,208 @@\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies;\n+\n+/// <summary>\n+/// Recursively splits text using a hierarchy of separators to preserve document structure.\n+/// </summary>\n+/// <remarks>\n+/// <para>\n+/// This advanced chunking strategy tries to split text using the most semantically meaningful\n+/// separators first (e.g., double newlines for paragraphs), falling back to less meaningful\n+/// separators (single newlines, spaces) only when necessary. This preserves the natural\n+/// structure of documents and keeps related content together.\n+/// </para>\n+/// <para><b>For Beginners:</b> This is a smart splitter that keeps related text together.\n+/// \n+/// Think of it like organizing a document by trying the best splits first:\n+/// \n+/// Priority 1: Split by double newlines (paragraphs)\n+///   \"Paragraph 1\\n\\nParagraph 2\\n\\nParagraph 3\"\n+///   ΓåÆ Keeps each paragraph whole\n+/// \n+/// Priority 2: If paragraphs are too big, split by single newlines (sentences/lines)\n+///   \"Long paragraph with\\nmultiple lines\\nthat need splitting\"\n+///   ΓåÆ Splits at line breaks\n+/// \n+/// Priority 3: If lines are too big, split by periods (sentences)\n+///   \"First sentence. Second sentence. Third sentence.\"\n+///   ΓåÆ Splits at sentences\n+/// \n+/// Priority 4: If sentences are too big, split by spaces (words)\n+///   \"This is a very long sentence without periods\"\n+///   ΓåÆ Splits at words\n+/// \n+/// Priority 5: Last resort, split by characters\n+///   \"ReallyLongWordWithNoSpaces\"\n+///   ΓåÆ Splits anywhere\n+/// \n+/// Why this is better than simple splitting:\n+/// - Keeps paragraphs together when possible (best semantic unity)\n+/// - Falls back gracefully when content is too large\n+/// - Preserves natural document structure\n+/// - Works well with various document formats (code, articles, books)\n+/// \n+/// Example with chunkSize=100, overlap=20:\n+/// \n+/// Input: \"First paragraph.\\n\\nSecond paragraph that is very long and needs to be split into multiple chunks.\\n\\nThird paragraph.\"\n+/// \n+/// 1. Try splitting by \"\\n\\n\" ΓåÆ Second paragraph too large\n+/// 2. Split second paragraph by \" \" ΓåÆ Gets multiple chunks\n+/// 3. Add overlap between chunks\n+/// \n+/// Result:\n+/// - Chunk 1: \"First paragraph.\"\n+/// - Chunk 2: \"Second paragraph that is very long and\" (overlap from chunk 1)\n+/// - Chunk 3: \"very long and needs to be split into\" (overlap from chunk 2)\n+/// - Chunk 4: \"split into multiple chunks.\"\n+/// - Chunk 5: \"Third paragraph.\"\n+/// </para>\n+/// </remarks>\n+public class RecursiveCharacterChunkingStrategy : ChunkingStrategyBase\n+{\n+    private readonly string[] _separators;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the RecursiveCharacterChunkingStrategy class.\n+    /// </summary>\n+    /// <param name=\"chunkSize\">Maximum size for each chunk in characters (default: 1000).</param>\n+    /// <param name=\"chunkOverlap\">Number of characters to overlap between chunks (default: 200).</param>\n+    /// <param name=\"separators\">Ordered list of separators to try (default: paragraph, newline, period, space, character).</param>\n+    public RecursiveCharacterChunkingStrategy(\n+        int chunkSize = 1000,\n+        int chunkOverlap = 200,\n+        string[]? separators = null)\n+        : base(chunkSize, chunkOverlap)\n+    {\n+        _separators = separators ?? new[] { \"\\n\\n\", \"\\n\", \". \", \" \", \"\" };\n+    }\n+\n+    /// <summary>\n+    /// Recursively splits text using the separator hierarchy.\n+    /// </summary>\n+    /// <param name=\"text\">The validated text to split.</param>\n+    /// <returns>Chunks with position information.</returns>\n+    protected override IEnumerable<(string Chunk, int StartPosition, int EndPosition)> ChunkCore(string text)\n+    {\n+        var chunks = SplitTextRecursively(text, _separators);\n+        var results = new List<(string, int, int)>();\n+        var position = 0;\n+\n+        foreach (var chunk in chunks)\n+        {\n+            var endPos = position + chunk.Length;\n+            results.Add((chunk, position, endPos));\n+            position = endPos;\n+        }\n+\n+        return results;\n+    }\n+\n+    /// <summary>\n+    /// Recursively splits text, trying each separator in order.\n+    /// </summary>\n+    /// <param name=\"text\">The text to split.</param>\n+    /// <param name=\"separators\">The ordered list of separators to try.</param>\n+    /// <returns>A list of text chunks.</returns>\n+    private List<string> SplitTextRecursively(string text, string[] separators)\n+    {\n+        var finalChunks = new List<string>();\n+\n+        // Base case: if text is small enough, return it as a single chunk\n+        if (text.Length <= ChunkSize)\n+        {\n+            if (!string.IsNullOrWhiteSpace(text))\n+            {\n+                finalChunks.Add(text.Trim());\n+            }\n+            return finalChunks;\n+        }\n+\n+        // Get the current separator\n+        var separator = separators[0];\n+        var nextSeparators = separators.Length > 1 ? separators.Skip(1).ToArray() : new[] { \"\" };\n+\n+        // Split by the current separator\n+        var splits = !string.IsNullOrEmpty(separator)\n+            ? text.Split(new[] { separator }, StringSplitOptions.None)\n+            : new[] { text };",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/RecursiveCharacterChunkingStrategy.cs",
    "commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "original_commit_id": "2e85607917a4ef53ec4ed06ecd3d2cbc46bd3626",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Critical: Character-level splitting causes infinite recursion.**\n\nWhen the separator is `\"\"` (empty string, the last resort in the default hierarchy), line 128 returns `new[] { text }` instead of splitting by character. Combined with line 123's fallback to `new[] { \"\" }`, this creates infinite recursion when text exceeds `ChunkSize`.\n\n**Trace:**\n1. Text \"VeryLongWord\" with separator `\"\"` exceeds `ChunkSize`\n2. Line 128: splits = `new[] { \"VeryLongWord\" }` (no actual split)\n3. Loop: split.Length > ChunkSize, so recurse at line 156\n4. Line 123: nextSeparators = `new[] { \"\" }` (same separator again!)\n5. Recursion uses same empty separator ΓåÆ infinite loop\n\n\n\nApply this diff to implement proper character-level splitting:\n\n```diff\n         // Get the current separator\n         var separator = separators[0];\n-        var nextSeparators = separators.Length > 1 ? separators.Skip(1).ToArray() : new[] { \"\" };\n+        var nextSeparators = separators.Length > 1 ? separators.Skip(1).ToArray() : Array.Empty<string>();\n\n         // Split by the current separator\n-        var splits = !string.IsNullOrEmpty(separator)\n-            ? text.Split(new[] { separator }, StringSplitOptions.None)\n-            : new[] { text };\n+        string[] splits;\n+        if (!string.IsNullOrEmpty(separator))\n+        {\n+            splits = text.Split(new[] { separator }, StringSplitOptions.None);\n+        }\n+        else if (nextSeparators.Length == 0)\n+        {\n+            // Final fallback: split by character\n+            splits = text.Select(c => c.ToString()).ToArray();\n+        }\n+        else\n+        {\n+            splits = new[] { text };\n+        }\n```\n\nThis ensures that when we've exhausted all separators, we split character-by-character, and the empty nextSeparators array prevents further recursion.\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        var separator = separators[0];\n        var nextSeparators = separators.Length > 1 ? separators.Skip(1).ToArray() : Array.Empty<string>();\n\n        // Split by the current separator\n        string[] splits;\n        if (!string.IsNullOrEmpty(separator))\n        {\n            splits = text.Split(new[] { separator }, StringSplitOptions.None);\n        }\n        else if (nextSeparators.Length == 0)\n        {\n            // Final fallback: split by character\n            splits = text.Select(c => c.ToString()).ToArray();\n        }\n        else\n        {\n            splits = new[] { text };\n        }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T15:48:47Z",
    "updated_at": "2025-11-04T15:48:53Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491051697",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051697"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491051697"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051697/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 122,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 128,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 128,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051717",
    "pull_request_review_id": 3417217555,
    "id": 2491051717,
    "node_id": "PRRC_kwDOKSXUF86Uem7F",
    "diff_hunk": "@@ -0,0 +1,27 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System.Collections.Generic;\n+\n+namespace AiDotNet.RAG.ContextCompression",
    "path": "src/RetrievalAugmentedGeneration/ContextCompression/ContextCompressorBase.cs",
    "commit_id": "2e85607917a4ef53ec4ed06ecd3d2cbc46bd3626",
    "original_commit_id": "2e85607917a4ef53ec4ed06ecd3d2cbc46bd3626",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Fix the namespace to match the folder structure.**\n\nThe namespace `AiDotNet.RAG.ContextCompression` doesn't match the file path `src/RetrievalAugmentedGeneration/ContextCompression/`. The namespace should be `AiDotNet.RetrievalAugmentedGeneration.ContextCompression` to maintain consistency with the codebase structure and avoid confusion.\n\n\n\nApply this diff:\n\n```diff\n-namespace AiDotNet.RAG.ContextCompression\n+namespace AiDotNet.RetrievalAugmentedGeneration.ContextCompression\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\nnamespace AiDotNet.RetrievalAugmentedGeneration.ContextCompression\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/ContextCompression/ContextCompressorBase.cs\naround line 5, the namespace is currently declared as\nAiDotNet.RAG.ContextCompression which does not match the folder path; update the\nnamespace declaration to\nAiDotNet.RetrievalAugmentedGeneration.ContextCompression so it reflects the\ndirectory structure and project naming conventions.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 05b10e8 to d54edda",
    "created_at": "2025-11-04T15:48:47Z",
    "updated_at": "2025-11-04T15:57:40Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491051717",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051717"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491051717"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051717/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": null,
    "original_line": 5,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 5,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051736",
    "pull_request_review_id": 3417217555,
    "id": 2491051736,
    "node_id": "PRRC_kwDOKSXUF86Uem7Y",
    "diff_hunk": "@@ -0,0 +1,27 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System.Collections.Generic;\n+\n+namespace AiDotNet.RAG.ContextCompression\n+{\n+    public abstract class ContextCompressorBase<T>",
    "path": "src/RetrievalAugmentedGeneration/ContextCompression/ContextCompressorBase.cs",
    "commit_id": "2e85607917a4ef53ec4ed06ecd3d2cbc46bd3626",
    "original_commit_id": "2e85607917a4ef53ec4ed06ecd3d2cbc46bd3626",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_≡ƒ¢á∩╕Å Refactor suggestion_ | _≡ƒƒá Major_\n\n**Add XML documentation to the public abstract class.**\n\nPublic APIs should include XML documentation to help consumers understand the purpose and usage of the class.\n\n\n\nApply this diff:\n\n```diff\n+    /// <summary>\n+    /// Base class for context compression strategies that reduce the size of retrieved documents\n+    /// while preserving relevant information for a given query.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type used in document embeddings and operations.</typeparam>\n     public abstract class ContextCompressorBase<T>\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    /// <summary>\n    /// Base class for context compression strategies that reduce the size of retrieved documents\n    /// while preserving relevant information for a given query.\n    /// </summary>\n    /// <typeparam name=\"T\">The numeric type used in document embeddings and operations.</typeparam>\n    public abstract class ContextCompressorBase<T>\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/ContextCompression/ContextCompressorBase.cs\naround line 7, the public abstract class declaration lacks XML documentation;\nadd a triple-slash XML comment block immediately above the class that includes a\n<summary> describing the purpose of the class, a <typeparam name=\"T\"> explaining\nthe type parameter, and optional <remarks> or <example> tags to clarify usage\nand behavior for consumers; ensure the comment is complete and builds without\nwarnings.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 05b10e8 to d54edda",
    "created_at": "2025-11-04T15:48:47Z",
    "updated_at": "2025-11-04T15:57:30Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491051736",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051736"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491051736"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051736/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": null,
    "original_line": 7,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 7,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051739",
    "pull_request_review_id": 3417217555,
    "id": 2491051739,
    "node_id": "PRRC_kwDOKSXUF86Uem7b",
    "diff_hunk": "@@ -0,0 +1,27 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System.Collections.Generic;\n+\n+namespace AiDotNet.RAG.ContextCompression\n+{\n+    public abstract class ContextCompressorBase<T>\n+    {\n+        public List<Document<T>> Compress(\n+            List<Document<T>> documents,\n+            string query,\n+            Dictionary<string, object>? options = null)\n+        {\n+            if (documents == null || documents.Count == 0)\n+            {\n+                return new List<Document<T>>();\n+            }\n+\n+            return CompressCore(documents, query, options);\n+        }\n+\n+        protected abstract List<Document<T>> CompressCore(\n+            List<Document<T>> documents,\n+            string query,\n+            Dictionary<string, object>? options = null);",
    "path": "src/RetrievalAugmentedGeneration/ContextCompression/ContextCompressorBase.cs",
    "commit_id": "2e85607917a4ef53ec4ed06ecd3d2cbc46bd3626",
    "original_commit_id": "2e85607917a4ef53ec4ed06ecd3d2cbc46bd3626",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_≡ƒ¢á∩╕Å Refactor suggestion_ | _≡ƒƒá Major_\n\n**Add XML documentation to the abstract method.**\n\nProtected abstract methods should include XML documentation to guide implementers of derived classes.\n\n\n\nApply this diff:\n\n```diff\n+        /// <summary>\n+        /// Implements the core compression logic for the specific strategy.\n+        /// </summary>\n+        /// <param name=\"documents\">The list of documents to compress.</param>\n+        /// <param name=\"query\">The query string used to guide compression.</param>\n+        /// <param name=\"options\">Optional configuration parameters for compression.</param>\n+        /// <returns>A list of compressed documents.</returns>\n         protected abstract List<Document<T>> CompressCore(\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        /// <summary>\n        /// Implements the core compression logic for the specific strategy.\n        /// </summary>\n        /// <param name=\"documents\">The list of documents to compress.</param>\n        /// <param name=\"query\">The query string used to guide compression.</param>\n        /// <param name=\"options\">Optional configuration parameters for compression.</param>\n        /// <returns>A list of compressed documents.</returns>\n        protected abstract List<Document<T>> CompressCore(\n            List<Document<T>> documents,\n            string query,\n            Dictionary<string, object>? options = null);\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/ContextCompression/ContextCompressorBase.cs\naround lines 22 to 25, the protected abstract method CompressCore lacks XML\ndocumentation; add an XML doc comment block above the method including a\n<summary> describing that it compresses or reduces a list of documents based on\na query, <typeparam name=\"T\"> describing the document content type, <param\nname=\"documents\"> explaining the input documents to be compressed, <param\nname=\"query\"> describing the user query or context used to guide compression,\n<param name=\"options\"> noting optional parameters and that it may be null, and a\n<returns> describing the returned compressed list of Document<T>; keep wording\nconcise and actionable for implementers.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 05b10e8 to d54edda",
    "created_at": "2025-11-04T15:48:47Z",
    "updated_at": "2025-11-04T15:57:31Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491051739",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051739"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491051739"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051739/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 22,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 25,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 25,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051750",
    "pull_request_review_id": 3417217555,
    "id": 2491051750,
    "node_id": "PRRC_kwDOKSXUF86Uem7m",
    "diff_hunk": "@@ -0,0 +1,109 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// Azure Cognitive Search document store providing fully managed search capabilities.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Azure Cognitive Search combines full-text search, semantic search, and vector search\n+/// in a fully managed cloud service with enterprise-grade security and compliance.\n+/// </remarks>\n+public class AzureSearchDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly string _serviceName;\n+    private readonly string _indexName;\n+    private readonly string _apiKey;\n+    private readonly int _vectorDimension;\n+    private int _documentCount;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"AzureSearchDocumentStore{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"serviceName\">The Azure Search service name.</param>\n+    /// <param name=\"indexName\">The name of the index to use.</param>\n+    /// <param name=\"apiKey\">The admin API key for authentication.</param>\n+    /// <param name=\"vectorDimension\">The dimensionality of document vectors.</param>\n+    public AzureSearchDocumentStore(\n+        string serviceName,\n+        string indexName,\n+        string apiKey,\n+        int vectorDimension)\n+    {\n+        _serviceName = serviceName ?? throw new ArgumentNullException(nameof(serviceName));\n+        _indexName = indexName ?? throw new ArgumentNullException(nameof(indexName));\n+        _apiKey = apiKey ?? throw new ArgumentNullException(nameof(apiKey));\n+        \n+        if (vectorDimension <= 0)\n+            throw new ArgumentException(\"Vector dimension must be positive\", nameof(vectorDimension));\n+        \n+        _vectorDimension = vectorDimension;\n+        _documentCount = 0;\n+    }\n+\n+    /// <summary>\n+    /// Gets the total number of documents in the index.\n+    /// </summary>\n+    public override int DocumentCount => _documentCount;\n+\n+    /// <summary>\n+    /// Gets the dimensionality of vectors in this store.\n+    /// </summary>\n+    public override int VectorDimension => _vectorDimension;\n+\n+    /// <summary>\n+    /// Removes all documents from the store.\n+    /// </summary>\n+    public override void Clear()\n+    {\n+        // TODO: Implement Azure Search index clearing via REST API\n+        _documentCount = 0;\n+    }\n+\n+    /// <summary>\n+    /// Core logic for adding a single vector document.\n+    /// </summary>\n+    protected override void AddCore(VectorDocument<T> vectorDocument)\n+    {\n+        // TODO: Implement Azure Search indexing via REST API\n+        // This would send HTTP POST request to Azure Search index endpoint\n+        _documentCount++;\n+    }\n+\n+    /// <summary>\n+    /// Core logic for similarity search with optional filtering.\n+    /// </summary>\n+    protected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)\n+    {\n+        // TODO: Implement Azure Search vector search via REST API\n+        // This would send HTTP POST request to Azure Search search endpoint with vector query\n+        return Enumerable.Empty<Document<T>>();\n+    }\n+\n+    /// <summary>\n+    /// Core logic for retrieving a document by ID.\n+    /// </summary>\n+    protected override Document<T>? GetByIdCore(string documentId)\n+    {\n+        // TODO: Implement Azure Search document retrieval by ID via REST API\n+        return null;\n+    }\n+\n+    /// <summary>\n+    /// Core logic for removing a document by ID.\n+    /// </summary>\n+    protected override bool RemoveCore(string documentId)\n+    {\n+        // TODO: Implement Azure Search document deletion via REST API\n+        if (_documentCount > 0)\n+        {\n+            _documentCount--;\n+            return true;\n+        }\n+        return false;\n+    }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/AzureSearchDocumentStore.cs",
    "commit_id": "80630e82c71aabc214615014532fd5063336af16",
    "original_commit_id": "2e85607917a4ef53ec4ed06ecd3d2cbc46bd3626",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Fake DocumentCount tracking misleads base class validation.**\n\n`AddCore` increments `_documentCount` without storing any document, and `RemoveCore` decrements it without verifying existence. This creates phantom state that:\n- Triggers base class dimension validation (`DocumentCount > 0`) when no documents actually exist\n- Returns `true` from `RemoveCore` for non-existent documents\n- Breaks the contract expected by callers\n\n\n\nFor stub implementations, either:\n1. Keep `DocumentCount => 0` constant until real storage is added\n2. Add a comment explaining the counter is a placeholder\n3. Make methods throw `NotImplementedException` consistently\n\n```diff\n     public override int DocumentCount => _documentCount;\n+    // NOTE: DocumentCount is a placeholder. Actual count will be queried from Azure Search once implemented.\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/AzureSearchDocumentStore.cs\naround lines 69 to 107: the stub AddCore/RemoveCore/others currently mutates\n_documentCount (incrementing/decrementing) without storing documents, which\nmisleads base-class validation and callers; remove the fake counter updates and\ninstead make the stub implementations consistent -- either return safe\nempty/default values (DocumentCount remains 0 and GetSimilarCore returns\nEnumerable.Empty, GetByIdCore returns null, RemoveCore returns false) or throw\nNotImplementedException from AddCore/GetSimilarCore/GetByIdCore/RemoveCore; pick\none approach and apply it consistently, and add a short comment indicating these\nmethods are intentionally unimplemented until Azure Search REST integration is\nadded.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 80630e8 to b6555ed",
    "created_at": "2025-11-04T15:48:48Z",
    "updated_at": "2025-11-05T02:31:50Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491051750",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051750"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491051750"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051750/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 69,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 107,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 107,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051763",
    "pull_request_review_id": 3417217555,
    "id": 2491051763,
    "node_id": "PRRC_kwDOKSXUF86Uem7z",
    "diff_hunk": "@@ -0,0 +1,120 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores\n+{\n+    /// <summary>\n+    /// FAISS-inspired document store with indexed vectors for efficient similarity search.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class FAISSDocumentStore<T> : DocumentStoreBase<T>\n+    {\n+        private readonly Dictionary<string, VectorDocument<T>> _documents;\n+        private readonly Dictionary<int, Vector<T>> _indexedVectors;\n+        private int _vectorDimension;\n+        private int _currentIndex;\n+\n+        public override int DocumentCount => _documents.Count;\n+        public override int VectorDimension => _vectorDimension;\n+\n+        public FAISSDocumentStore(int initialCapacity = 1000)\n+        {\n+            if (initialCapacity <= 0)\n+                throw new ArgumentException(\"Initial capacity must be greater than zero\", nameof(initialCapacity));\n+\n+            _documents = new Dictionary<string, VectorDocument<T>>(initialCapacity);\n+            _indexedVectors = new Dictionary<int, Vector<T>>(initialCapacity);\n+            _vectorDimension = 0;\n+            _currentIndex = 0;\n+        }\n+\n+        protected override void AddCore(VectorDocument<T> vectorDocument)\n+        {\n+            if (_documents.Count == 0)\n+            {\n+                _vectorDimension = vectorDocument.Embedding.Length;\n+            }\n+\n+            var index = _currentIndex++;\n+            _documents[vectorDocument.Document.Id] = vectorDocument;\n+            _indexedVectors[index] = vectorDocument.Embedding;\n+        }\n+\n+        protected override void AddBatchCore(IList<VectorDocument<T>> vectorDocuments)\n+        {\n+            if (_vectorDimension == 0 && vectorDocuments.Count > 0)\n+            {\n+                _vectorDimension = vectorDocuments[0].Embedding.Length;\n+            }\n+\n+            foreach (var vectorDocument in vectorDocuments)\n+            {\n+                if (vectorDocument.Embedding.Length != _vectorDimension)\n+                    throw new ArgumentException(\n+                        $\"Vector dimension mismatch in batch. Expected {_vectorDimension}, got {vectorDocument.Embedding.Length}\",\n+                        nameof(vectorDocuments));\n+\n+                var index = _currentIndex++;\n+                _documents[vectorDocument.Document.Id] = vectorDocument;\n+                _indexedVectors[index] = vectorDocument.Embedding;\n+            }\n+        }\n+\n+        protected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)\n+        {\n+            var scoredDocuments = new List<(Document<T> Document, T Score)>();\n+\n+            var matchingDocuments = _documents.Values\n+                .Where(vectorDoc => MatchesFilters(vectorDoc.Document, metadataFilters));\n+\n+            foreach (var vectorDoc in matchingDocuments)\n+            {\n+                var similarity = StatisticsHelper<T>.CosineSimilarity(queryVector, vectorDoc.Embedding);\n+                scoredDocuments.Add((vectorDoc.Document, similarity));\n+            }\n+\n+            var results = scoredDocuments\n+                .OrderByDescending(x => x.Score)\n+                .Take(topK)\n+                .Select(x =>\n+                {\n+                    x.Document.RelevanceScore = x.Score;\n+                    x.Document.HasRelevanceScore = true;\n+                    return x.Document;\n+                })",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/FAISSDocumentStore.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "2e85607917a4ef53ec4ed06ecd3d2cbc46bd3626",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Avoid mutating persisted documents with transient relevance scores.**  \nJust like in the Milvus store, this block updates the stored `Document<T>` instance. Those mutations persist beyond the query and surface through `GetById` or later searches. Please return a copy with the score instead.\n\n\n```diff\n             var results = scoredDocuments\n                 .OrderByDescending(x => x.Score)\n                 .Take(topK)\n                 .Select(x =>\n                 {\n-                    x.Document.RelevanceScore = x.Score;\n-                    x.Document.HasRelevanceScore = true;\n-                    return x.Document;\n+                    var source = x.Document;\n+                    var docCopy = new Document<T>(source.Id, source.Content, new Dictionary<string, object>(source.Metadata))\n+                    {\n+                        Embedding = source.Embedding,\n+                        RelevanceScore = x.Score,\n+                        HasRelevanceScore = true\n+                    };\n+                    return docCopy;\n                 })\n                 .ToList();\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n            var results = scoredDocuments\n                .OrderByDescending(x => x.Score)\n                .Take(topK)\n                .Select(x =>\n                {\n                    var source = x.Document;\n                    var docCopy = new Document<T>(source.Id, source.Content, new Dictionary<string, object>(source.Metadata))\n                    {\n                        Embedding = source.Embedding,\n                        RelevanceScore = x.Score,\n                        HasRelevanceScore = true\n                    };\n                    return docCopy;\n                })\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/FAISSDocumentStore.cs around\nlines 80ΓÇô88, the code mutates the stored Document by setting\nRelevanceScore/HasRelevanceScore on the persisted instance; instead create and\nreturn a new Document instance (a shallow copy) for each scored result, copy\nacross the original document fields (Id, Content, Metadata, etc.), set\nRelevanceScore and HasRelevanceScore on that copy, and return the copy so the\npersisted document remains unchanged.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T15:48:48Z",
    "updated_at": "2025-11-04T15:48:54Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491051763",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051763"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491051763"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051763/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 198,
    "original_start_line": 80,
    "start_side": "RIGHT",
    "line": 206,
    "original_line": 88,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 88,
    "position": 206,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051775",
    "pull_request_review_id": 3417217555,
    "id": 2491051775,
    "node_id": "PRRC_kwDOKSXUF86Uem7_",
    "diff_hunk": "@@ -0,0 +1,94 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores\n+{\n+    /// <summary>\n+    /// PostgreSQL with pgvector-inspired document store for relational database vector storage.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class PostgresVectorDocumentStore<T> : DocumentStoreBase<T>\n+    {\n+        private readonly Dictionary<string, VectorDocument<T>> _documents;\n+        private readonly string _tableName;\n+        private int _vectorDimension;\n+\n+        public override int DocumentCount => _documents.Count;\n+        public override int VectorDimension => _vectorDimension;\n+\n+        public PostgresVectorDocumentStore(string tableName, int initialCapacity = 1000)\n+        {\n+            if (string.IsNullOrWhiteSpace(tableName))\n+                throw new ArgumentException(\"Table name cannot be empty\", nameof(tableName));\n+            if (initialCapacity <= 0)\n+                throw new ArgumentException(\"Initial capacity must be greater than zero\", nameof(initialCapacity));\n+\n+            _tableName = tableName;\n+            _documents = new Dictionary<string, VectorDocument<T>>(initialCapacity);\n+            _vectorDimension = 0;\n+        }\n+\n+        protected override void AddCore(VectorDocument<T> vectorDocument)\n+        {\n+            if (_documents.Count == 0)\n+            {\n+                _vectorDimension = vectorDocument.Embedding.Length;\n+            }\n+\n+            _documents[vectorDocument.Document.Id] = vectorDocument;\n+        }\n+\n+        protected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)\n+        {\n+            var scoredDocuments = new List<(Document<T> Document, T Score)>();\n+\n+            var matchingDocuments = _documents.Values\n+                .Where(vectorDoc => MatchesFilters(vectorDoc.Document, metadataFilters));\n+\n+            foreach (var vectorDoc in matchingDocuments)\n+            {\n+                var similarity = StatisticsHelper<T>.CosineSimilarity(queryVector, vectorDoc.Embedding);\n+                scoredDocuments.Add((vectorDoc.Document, similarity));\n+            }\n+\n+            var results = scoredDocuments\n+                .OrderByDescending(x => x.Score)\n+                .Take(topK)\n+                .Select(x =>\n+                {\n+                    x.Document.RelevanceScore = x.Score;\n+                    x.Document.HasRelevanceScore = true;\n+                    return x.Document;\n+                })\n+                .ToList();\n+\n+            return results;\n+        }\n+\n+        protected override Document<T>? GetByIdCore(string documentId)\n+        {\n+            return _documents.TryGetValue(documentId, out var vectorDoc) ? vectorDoc.Document : null;\n+        }\n+\n+        protected override bool RemoveCore(string documentId)\n+        {\n+            var removed = _documents.Remove(documentId);\n+            if (removed && _documents.Count == 0)\n+            {\n+                _vectorDimension = 0;\n+            }\n+            return removed;\n+        }\n+\n+        public override void Clear()\n+        {\n+            _documents.Clear();\n+            _vectorDimension = 0;\n+        }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/PostgresVectorDocumentStore.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "2e85607917a4ef53ec4ed06ecd3d2cbc46bd3626",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Inconsistent stub pattern compared to other placeholder implementations.**\n\nThe PR objectives note that \"Many implementations are scaffolded with NotImplementedException placeholders pending external service integration.\" However, this class provides a fully functional in-memory implementation instead of throwing `NotImplementedException` for methods requiring PostgreSQL integration.\n\nThis creates an inconsistency: developers might believe PostgreSQL integration is complete, when it's actually just an in-memory fallback. Consider either:\n1. Throwing `NotImplementedException` in core methods to clearly signal pending PostgreSQL integration, OR\n2. Implementing actual PostgreSQL/pgvector functionality to match the class name and documentation\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T15:48:48Z",
    "updated_at": "2025-11-04T15:48:54Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491051775",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051775"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491051775"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051775/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 10,
    "original_start_line": 10,
    "start_side": "RIGHT",
    "line": 328,
    "original_line": 91,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 91,
    "position": 328,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051783",
    "pull_request_review_id": 3417217555,
    "id": 2491051783,
    "node_id": "PRRC_kwDOKSXUF86Uem8H",
    "diff_hunk": "@@ -0,0 +1,86 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// Lightweight SQLite-based vector store using the SQLite-VSS extension.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// SQLite-VSS provides vector similarity search in a serverless, file-based database,\n+/// ideal for development, testing, and edge deployments.\n+/// </remarks>\n+public class SQLiteVSSDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly string _databasePath;\n+    private readonly string _tableName;\n+\n+    private readonly int _vectorDimension;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"SQLiteVSSDocumentStore{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"databasePath\">The path to the SQLite database file.</param>\n+    /// <param name=\"tableName\">The name of the table to use.</param>\n+    /// <param name=\"vectorDimension\">The dimensionality of document vectors.</param>\n+    public SQLiteVSSDocumentStore(\n+        string databasePath,\n+        string tableName,\n+        int vectorDimension)\n+    {\n+        _databasePath = databasePath ?? throw new ArgumentNullException(nameof(databasePath));\n+        _tableName = tableName ?? throw new ArgumentNullException(nameof(tableName));\n+        _vectorDimension = vectorDimension;\n+    }\n+\n+    /// <inheritdoc />\n+    public override int DocumentCount => 0;",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/SQLiteVSSDocumentStore.cs",
    "commit_id": "80630e82c71aabc214615014532fd5063336af16",
    "original_commit_id": "2e85607917a4ef53ec4ed06ecd3d2cbc46bd3626",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Hardcoded DocumentCount breaks dimension validation.**\n\nThe base class `ValidateVectorDocument` only enforces dimension checks when `DocumentCount > 0`. Returning a hardcoded `0` means dimension mismatches will never be caught, allowing inconsistent vector dimensions to be added.\n\n\n\nOnce the store implementation is added, track the actual count or compute it from the underlying SQLite table.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/SQLiteVSSDocumentStore.cs\naround line 39, the DocumentCount property is hardcoded to 0 which disables\ndimension validation; replace it with a real count computed from the underlying\nSQLite storage (either query the documents table for COUNT(*) or maintain an\naccurate counter updated on add/remove operations) and return that value; ensure\nthe implementation handles errors (fallback to 0 if store not initialized), is\nefficient (cache or async query if needed), and is thread-safe so validation\nlogic in the base class works correctly.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 80630e8 to 17e2d69",
    "created_at": "2025-11-04T15:48:48Z",
    "updated_at": "2025-11-05T02:38:23Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491051783",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051783"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491051783"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051783/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": null,
    "original_line": 39,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 39,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051788",
    "pull_request_review_id": 3417217555,
    "id": 2491051788,
    "node_id": "PRRC_kwDOKSXUF86Uem8M",
    "diff_hunk": "@@ -0,0 +1,86 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// Lightweight SQLite-based vector store using the SQLite-VSS extension.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// SQLite-VSS provides vector similarity search in a serverless, file-based database,\n+/// ideal for development, testing, and edge deployments.\n+/// </remarks>\n+public class SQLiteVSSDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly string _databasePath;\n+    private readonly string _tableName;\n+\n+    private readonly int _vectorDimension;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"SQLiteVSSDocumentStore{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"databasePath\">The path to the SQLite database file.</param>\n+    /// <param name=\"tableName\">The name of the table to use.</param>\n+    /// <param name=\"vectorDimension\">The dimensionality of document vectors.</param>\n+    public SQLiteVSSDocumentStore(\n+        string databasePath,\n+        string tableName,\n+        int vectorDimension)\n+    {\n+        _databasePath = databasePath ?? throw new ArgumentNullException(nameof(databasePath));\n+        _tableName = tableName ?? throw new ArgumentNullException(nameof(tableName));\n+        _vectorDimension = vectorDimension;\n+    }\n+\n+    /// <inheritdoc />\n+    public override int DocumentCount => 0;\n+\n+    /// <inheritdoc />\n+    public override int VectorDimension => _vectorDimension;\n+\n+    /// <inheritdoc />\n+    protected override void AddCore(VectorDocument<T> vectorDocument)\n+    {\n+        // TODO: Implement SQLite-VSS insert\n+        throw new NotImplementedException(\"SQLite-VSS integration requires System.Data.SQLite implementation\");\n+    }\n+\n+    /// <inheritdoc />\n+    protected override void AddBatchCore(IList<VectorDocument<T>> vectorDocuments)\n+    {\n+        // TODO: Implement SQLite-VSS batch insert\n+        throw new NotImplementedException(\"SQLite-VSS integration requires System.Data.SQLite implementation\");\n+    }\n+\n+    /// <inheritdoc />\n+    protected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)\n+    {\n+        // TODO: Implement SQLite-VSS vector search\n+        throw new NotImplementedException(\"SQLite-VSS integration requires System.Data.SQLite implementation\");\n+    }\n+\n+    /// <inheritdoc />\n+    protected override Document<T>? GetByIdCore(string documentId)\n+    {\n+        // TODO: Implement SQLite document retrieval\n+        throw new NotImplementedException(\"SQLite-VSS integration requires System.Data.SQLite implementation\");\n+    }\n+\n+    /// <inheritdoc />\n+    protected override bool RemoveCore(string documentId)\n+    {\n+        // TODO: Implement SQLite document deletion\n+        throw new NotImplementedException(\"SQLite-VSS integration requires System.Data.SQLite implementation\");\n+    }\n+\n+    /// <inheritdoc />\n+    public override void Clear()\n+    {\n+        // TODO: Implement SQLite table clearing\n+        throw new NotImplementedException(\"SQLite-VSS integration requires System.Data.SQLite implementation\");\n+    }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/SQLiteVSSDocumentStore.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "2e85607917a4ef53ec4ed06ecd3d2cbc46bd3626",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n<details>\n<summary>≡ƒº⌐ Analysis chain</summary>\n\n**All operations throw NotImplementedException - store is non-functional.**\n\nEvery core method throws `NotImplementedException`, making this store completely unusable. Past reviews flagged these as critical blockers that were marked \"Addressed,\" yet the exceptions remain.\n\n\n\n\nIf SQLite-VSS integration is deferred, consider:\n1. Marking the class as `abstract` or `internal` to prevent misuse\n2. Adding `[Obsolete(\"Not yet implemented\")]` attributes\n3. Documenting in XML that this is a placeholder awaiting external package integration\n\nOtherwise, implement the SQLite-VSS operations before release.\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check if the file exists and examine its full structure\nfd -t f SQLiteVSSDocumentStore.cs --exec cat {}\n```\n\nLength of output: 3245\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Search for class declaration and attributes\nrg -n \"class SQLiteVSSDocumentStore\" -A 10 -B 5\n```\n\nLength of output: 1890\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Search for System.Data.SQLite references across the codebase\nrg -n \"System\\.Data\\.SQLite\" --type=cs -C 2\n```\n\nLength of output: 3377\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check project files for dependency configuration\nfd -t f '\\.(csproj|nuspec|packages\\.config)$' --exec grep -l \"SQLite\" {}\n```\n\nLength of output: 0\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Look for NotImplementedException patterns in the same file\nrg -n \"NotImplementedException\" src/RetrievalAugmentedGeneration/DocumentStores/SQLiteVSSDocumentStore.cs -B 2 -A 1\n```\n\nLength of output: 1132\n\n---\n\n\n\n</details>\n\n**All core operations throw NotImplementedExceptionΓÇöclass is non-functional and publicly instantiable.**\n\nEvery method that would perform store operations throws `NotImplementedException`, yet the class is public and can be instantiated. This violates the Liskov Substitution Principle and creates a trap for consumers who may attempt to use it.\n\n**Required actions:**\n\n1. Mark the class `abstract` or `internal` to prevent instantiation and misuse.\n2. Add `[Obsolete(\"Not yet implemented. Awaiting System.Data.SQLite integration.\")]` attribute to all stub methods.\n3. If integration is deferred, add XML documentation clarifying this is a placeholder and linking to tracking issue.\n\nOtherwise, implement the SQLite-VSS operations (lines 48, 55, 62, 69, 76, 83) before merging.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T15:48:48Z",
    "updated_at": "2025-11-04T15:48:54Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491051788",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051788"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491051788"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051788/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 102,
    "original_start_line": 45,
    "start_side": "RIGHT",
    "line": 333,
    "original_line": 84,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 84,
    "position": 333,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051798",
    "pull_request_review_id": 3417217555,
    "id": 2491051798,
    "node_id": "PRRC_kwDOKSXUF86Uem8W",
    "diff_hunk": "@@ -0,0 +1,52 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels;\n+\n+/// <summary>\n+/// Cohere embedding model integration for high-performance embeddings.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Cohere provides state-of-the-art embeddings with multiple model sizes optimized\n+/// for different use cases (English, multilingual, search, classification).\n+/// </remarks>\n+public class CohereEmbeddingModel<T> : EmbeddingModelBase<T>\n+{\n+    private readonly string _apiKey;\n+    private readonly string _model;\n+    private readonly string _inputType;\n+    private readonly int _dimension;\n+\n+    public override int EmbeddingDimension => _dimension;\n+    public override int MaxTokens => 512;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"CohereEmbeddingModel{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"apiKey\">The Cohere API key.</param>\n+    /// <param name=\"model\">The model name (e.g., \"embed-english-v3.0\").</param>\n+    /// <param name=\"inputType\">The input type (\"search_document\" or \"search_query\").</param>\n+    /// <param name=\"dimension\">The embedding dimension.</param>\n+    public CohereEmbeddingModel(\n+        string apiKey,\n+        string model,\n+        string inputType,\n+        int dimension = 1024)\n+    {\n+        _apiKey = apiKey ?? throw new ArgumentNullException(nameof(apiKey));\n+        _model = model ?? throw new ArgumentNullException(nameof(model));\n+        _inputType = inputType ?? throw new ArgumentNullException(nameof(inputType));\n+        _dimension = dimension;",
    "path": "src/RetrievalAugmentedGeneration/EmbeddingModels/CohereEmbeddingModel.cs",
    "commit_id": "80630e82c71aabc214615014532fd5063336af16",
    "original_commit_id": "2e85607917a4ef53ec4ed06ecd3d2cbc46bd3626",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Harden constructor argument validation.**\n\nAllowing whitespace API keys/models or non-positive dimensions produces unusable instances that fail later at runtime. Reject these cases up front (e.g., `string.IsNullOrWhiteSpace` checks and `dimension <= 0` guard) so consumers get immediate feedback.\n\n```diff\n-        _apiKey = apiKey ?? throw new ArgumentNullException(nameof(apiKey));\n-        _model = model ?? throw new ArgumentNullException(nameof(model));\n-        _inputType = inputType ?? throw new ArgumentNullException(nameof(inputType));\n-        _dimension = dimension;\n+        if (string.IsNullOrWhiteSpace(apiKey))\n+            throw new ArgumentException(\"API key cannot be null or whitespace.\", nameof(apiKey));\n+        if (string.IsNullOrWhiteSpace(model))\n+            throw new ArgumentException(\"Model name cannot be null or whitespace.\", nameof(model));\n+        if (string.IsNullOrWhiteSpace(inputType))\n+            throw new ArgumentException(\"Input type cannot be null or whitespace.\", nameof(inputType));\n+        if (dimension <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(dimension), \"Embedding dimension must be positive.\");\n+\n+        _apiKey = apiKey;\n+        _model = model;\n+        _inputType = inputType;\n+        _dimension = dimension;\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    public CohereEmbeddingModel(\n        string apiKey,\n        string model,\n        string inputType,\n        int dimension = 1024)\n    {\n        if (string.IsNullOrWhiteSpace(apiKey))\n            throw new ArgumentException(\"API key cannot be null or whitespace.\", nameof(apiKey));\n        if (string.IsNullOrWhiteSpace(model))\n            throw new ArgumentException(\"Model name cannot be null or whitespace.\", nameof(model));\n        if (string.IsNullOrWhiteSpace(inputType))\n            throw new ArgumentException(\"Input type cannot be null or whitespace.\", nameof(inputType));\n        if (dimension <= 0)\n            throw new ArgumentOutOfRangeException(nameof(dimension), \"Embedding dimension must be positive.\");\n\n        _apiKey = apiKey;\n        _model = model;\n        _inputType = inputType;\n        _dimension = dimension;\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/EmbeddingModels/CohereEmbeddingModel.cs\naround lines 32 to 41, the constructor currently only checks for nulls but\nallows whitespace strings and non-positive dimensions; update validation to\nreject empty/whitespace strings for apiKey, model, and inputType (use\nstring.IsNullOrWhiteSpace and throw ArgumentException or ArgumentNullException\nwith the parameter name) and validate dimension > 0 (throw\nArgumentOutOfRangeException with a clear message) so invalid inputs fail fast\nwith meaningful errors.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T15:48:49Z",
    "updated_at": "2025-11-04T15:48:54Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491051798",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051798"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491051798"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051798/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 32,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 41,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 41,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051815",
    "pull_request_review_id": 3417217555,
    "id": 2491051815,
    "node_id": "PRRC_kwDOKSXUF86Uem8n",
    "diff_hunk": "@@ -0,0 +1,65 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\n+using System;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels\n+{\n+    /// <summary>\n+    /// Local transformer embedding model for generating embeddings without external API calls.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class LocalTransformerEmbedding<T> : EmbeddingModelBase<T>\n+    {\n+        private readonly string _modelPath;\n+        private readonly int _dimension;\n+        private readonly int _maxTokens;\n+\n+        public override int EmbeddingDimension => _dimension;\n+        public override int MaxTokens => _maxTokens;\n+\n+        public LocalTransformerEmbedding(string modelPath, int dimension = 384, int maxTokens = 512)\n+        {\n+            if (string.IsNullOrWhiteSpace(modelPath))\n+                throw new ArgumentException(\"Model path cannot be empty\", nameof(modelPath));\n+            if (dimension <= 0)\n+                throw new ArgumentException(\"Dimension must be positive\", nameof(dimension));\n+            if (maxTokens <= 0)\n+                throw new ArgumentException(\"Max tokens must be positive\", nameof(maxTokens));\n+\n+            _modelPath = modelPath;\n+            _dimension = dimension;\n+            _maxTokens = maxTokens;\n+        }\n+\n+        protected override Vector<T> EmbedCore(string text)\n+        {\n+            var values = new T[_dimension];\n+            var hash = GetDeterministicHash(text);\n+            \n+            for (int i = 0; i < _dimension; i++)\n+            {\n+                var val = NumOps.FromDouble(Math.Sin((double)hash * (i + 1) * 0.003));\n+                values[i] = val;\n+            }\n+\n+            return new Vector<T>(values).Normalize();\n+        }",
    "path": "src/RetrievalAugmentedGeneration/Embeddings/LocalTransformerEmbedding.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "2e85607917a4ef53ec4ed06ecd3d2cbc46bd3626",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Implement the actual local transformer inference.**\n\nThis model never loads `_modelPath` or runs a transformer; it just hashes text into a sine wave, yielding garbage embeddings. Please wire up real local model inference (e.g., load the torch/ONNX weights, run forward pass, and normalize outputs) before calling this complete.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/EmbeddingModels/LocalTransformerEmbedding.cs\naround lines 35 to 47, the EmbedCore currently returns a deterministic\nsine-based fake vector; replace it with real local model inference: load the\nmodel weights from _modelPath during initialization (using the chosen runtime:\nTorchSharp, ONNX Runtime or similar), build/instantiate a tokenizer and\npreprocessing pipeline, run the tokenized input through the model forward pass\nto obtain the raw embedding tensor, convert/clip the tensor values to type T and\npopulate the values array, then normalize and return the Vector<T>. Ensure you\nhandle model/device selection (CPU/GPU), dispose runtime resources safely,\nsupport batching or short-circuit for empty inputs, and log/throw meaningful\nerrors if model loading or inference fails.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T15:48:49Z",
    "updated_at": "2025-11-04T15:48:54Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491051815",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051815"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491051815"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051815/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 35,
    "original_start_line": 35,
    "start_side": "RIGHT",
    "line": 47,
    "original_line": 47,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 47,
    "position": 47,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051828",
    "pull_request_review_id": 3417217555,
    "id": 2491051828,
    "node_id": "PRRC_kwDOKSXUF86Uem80",
    "diff_hunk": "@@ -0,0 +1,83 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels;\n+\n+/// <summary>\n+/// Multi-modal embedding model supporting both text and images (e.g., CLIP).\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Enables creation of unified embeddings for text and images in the same vector space,\n+/// allowing cross-modal similarity search and retrieval.\n+/// </remarks>\n+public class MultiModalEmbeddingModel<T> : EmbeddingModelBase<T>\n+{\n+    private readonly string _modelPath;\n+    private readonly bool _normalizeEmbeddings;\n+\n+    private readonly int _dimension;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"MultiModalEmbeddingModel{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"modelPath\">Path to the multi-modal model (e.g., CLIP).</param>\n+    /// <param name=\"normalizeEmbeddings\">Whether to normalize embeddings to unit length.</param>\n+    /// <param name=\"dimension\">The embedding dimension.</param>\n+    public MultiModalEmbeddingModel(\n+        string modelPath,\n+        bool normalizeEmbeddings,\n+        int dimension)\n+    {\n+        _modelPath = modelPath ?? throw new ArgumentNullException(nameof(modelPath));\n+        _normalizeEmbeddings = normalizeEmbeddings;\n+        _dimension = dimension;\n+    }",
    "path": "src/RetrievalAugmentedGeneration/EmbeddingModels/MultiModalEmbeddingModel.cs",
    "commit_id": "80630e82c71aabc214615014532fd5063336af16",
    "original_commit_id": "2e85607917a4ef53ec4ed06ecd3d2cbc46bd3626",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Add validation for `modelPath` and `dimension` parameters.**\n\nThe constructor should validate that `modelPath` is not empty/whitespace and that `dimension` is positive to prevent invalid object states.\n\n\n\nApply this diff to add the missing validations:\n\n```diff\n     public MultiModalEmbeddingModel(\n         string modelPath,\n         bool normalizeEmbeddings,\n         int dimension)\n     {\n-        _modelPath = modelPath ?? throw new ArgumentNullException(nameof(modelPath));\n+        if (string.IsNullOrWhiteSpace(modelPath))\n+            throw new ArgumentException(\"Model path cannot be null or whitespace\", nameof(modelPath));\n+        if (dimension <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(dimension), \"Dimension must be positive\");\n+            \n+        _modelPath = modelPath;\n         _normalizeEmbeddings = normalizeEmbeddings;\n         _dimension = dimension;\n     }\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/EmbeddingModels/MultiModalEmbeddingModel.cs\naround lines 28 to 36, the constructor currently only null-checks modelPath and\ndoes not validate that modelPath is not empty or whitespace nor that dimension\nis positive; update the constructor to throw an ArgumentException (or\nArgumentNullException with message) if modelPath is empty or whitespace (use\nstring.IsNullOrWhiteSpace) and throw an ArgumentOutOfRangeException if dimension\nis less than or equal to zero, keeping existing assignments when inputs are\nvalid.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T15:48:49Z",
    "updated_at": "2025-11-04T15:48:54Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491051828",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051828"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491051828"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051828/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 28,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 36,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 36,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051838",
    "pull_request_review_id": 3417217555,
    "id": 2491051838,
    "node_id": "PRRC_kwDOKSXUF86Uem8-",
    "diff_hunk": "@@ -0,0 +1,83 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels;\n+\n+/// <summary>\n+/// Multi-modal embedding model supporting both text and images (e.g., CLIP).\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Enables creation of unified embeddings for text and images in the same vector space,\n+/// allowing cross-modal similarity search and retrieval.\n+/// </remarks>\n+public class MultiModalEmbeddingModel<T> : EmbeddingModelBase<T>\n+{\n+    private readonly string _modelPath;\n+    private readonly bool _normalizeEmbeddings;\n+\n+    private readonly int _dimension;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"MultiModalEmbeddingModel{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"modelPath\">Path to the multi-modal model (e.g., CLIP).</param>\n+    /// <param name=\"normalizeEmbeddings\">Whether to normalize embeddings to unit length.</param>\n+    /// <param name=\"dimension\">The embedding dimension.</param>\n+    public MultiModalEmbeddingModel(\n+        string modelPath,\n+        bool normalizeEmbeddings,\n+        int dimension)\n+    {\n+        _modelPath = modelPath ?? throw new ArgumentNullException(nameof(modelPath));\n+        _normalizeEmbeddings = normalizeEmbeddings;\n+        _dimension = dimension;\n+    }\n+\n+    /// <inheritdoc />\n+    public override int EmbeddingDimension => _dimension;\n+\n+    /// <inheritdoc />\n+    public override int MaxTokens => 512;",
    "path": "src/RetrievalAugmentedGeneration/EmbeddingModels/MultiModalEmbeddingModel.cs",
    "commit_id": "80630e82c71aabc214615014532fd5063336af16",
    "original_commit_id": "2e85607917a4ef53ec4ed06ecd3d2cbc46bd3626",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n<details>\n<summary>≡ƒº⌐ Analysis chain</summary>\n\n**Verify the MaxTokens default value for multi-modal models.**\n\nThe hardcoded value of 512 may not be accurate for CLIP-style models, which typically use 77 tokens for the text encoder. Consider either using the correct default for CLIP or making this value configurable via a constructor parameter.\n\n---\n\n≡ƒîÉ Web query:\n\n```web\nWhat is the maximum token length for CLIP text encoder?\n```\n\n≡ƒÆí Result:\n\nThe CLIP text encoder's maximum input length is 77 tokens (context_length = 77). [1][2]\n\nSources:\n- OpenAI CLIP repository (clip.tokenize context_length=77). [1]\n- open-source implementations/docs noting CLIP uses 77-token context length (e.g., Towhee/open_clip). [2]\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Inspect the MultiModalEmbeddingModel class definition and constructor\nrg -n -A 20 \"class MultiModalEmbeddingModel\" src/RetrievalAugmentedGeneration/EmbeddingModels/MultiModalEmbeddingModel.cs\n```\n\nLength of output: 927\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check for any configuration or constructor parameters\nrg -n -B 5 -A 10 \"public.*MaxTokens\" src/RetrievalAugmentedGeneration/EmbeddingModels/MultiModalEmbeddingModel.cs\n```\n\nLength of output: 525\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Look for comments or documentation about the model type\nrg -n -B 10 \"class MultiModalEmbeddingModel|MaxTokens\" src/RetrievalAugmentedGeneration/EmbeddingModels/MultiModalEmbeddingModel.cs | head -30\n```\n\nLength of output: 893\n\n---\n\n\n\n</details>\n\n**Use the correct MaxTokens value for CLIP models.**\n\nThe CLIP text encoder's maximum input length is 77 tokens, but the hardcoded value of 512 does not match this. Since the class is documented as CLIP-based, update `MaxTokens` to 77 or make it configurable via a constructor parameter. The TODO comment indicates implementation is incomplete, so address this when integrating the actual CLIP model.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/EmbeddingModels/MultiModalEmbeddingModel.cs\naround lines 41 to 42, the MaxTokens property is hardcoded to 512 but this class\nis CLIP-based and should use the CLIP text encoder limit of 77; change MaxTokens\nto return 77 or add a constructor parameter and backing field to make the max\ntokens configurable (default 77), and update any TODO comments to reflect that\nthe correct CLIP limit is implemented when the actual CLIP model is integrated.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T15:48:49Z",
    "updated_at": "2025-11-04T15:48:54Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491051838",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051838"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491051838"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051838/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 41,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 42,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 42,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051849",
    "pull_request_review_id": 3417217555,
    "id": 2491051849,
    "node_id": "PRRC_kwDOKSXUF86Uem9J",
    "diff_hunk": "@@ -0,0 +1,65 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\n+using System;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels\n+{\n+    /// <summary>\n+    /// ONNX-based sentence transformer for generating embeddings.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class ONNXSentenceTransformer<T> : EmbeddingModelBase<T>\n+    {\n+        private readonly string _modelPath;",
    "path": "src/RetrievalAugmentedGeneration/Embeddings/ONNXSentenceTransformer.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "2e85607917a4ef53ec4ed06ecd3d2cbc46bd3626",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n<details>\n<summary>≡ƒº⌐ Analysis chain</summary>\n\n**Flag unused field or document stub status.**\n\nThe `_modelPath` field is stored but never used in the implementation. This is misleading to callers who expect the ONNX model to be loaded from the specified path. Since this is a stub implementation using hash-based synthetic embeddings, consider one of the following:\n\n1. Add an XML comment to the class documenting that this is a placeholder implementation pending ONNX Runtime integration.\n2. Throw `NotImplementedException` in the constructor with a message indicating ONNX support is not yet implemented.\n3. Actually implement ONNX model loading (if ready).\n\n\n\n\nDo you want me to help generate code that throws `NotImplementedException` or adds appropriate documentation?\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Examine the ONNXSentenceTransformer.cs file to verify _modelPath usage\nfind . -name \"ONNXSentenceTransformer.cs\" -type f | head -20\n```\n\nLength of output: 77\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Display the complete file content\ncat \"src/RetrievalAugmentedGeneration/EmbeddingModels/ONNXSentenceTransformer.cs\"\n```\n\nLength of output: 2144\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Search for all references to _modelPath in the file\nrg \"_modelPath\" \"src/RetrievalAugmentedGeneration/EmbeddingModels/ONNXSentenceTransformer.cs\" -n\n```\n\nLength of output: 85\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check related embedding model files for comparison\nfind src/RetrievalAugmentedGeneration/EmbeddingModels -name \"*.cs\" -type f | head -10\n```\n\nLength of output: 688\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check if there are any similar stub implementations or patterns\nrg \"NotImplementedException|placeholder|stub\" src/RetrievalAugmentedGeneration/EmbeddingModels -n --type cs\n```\n\nLength of output: 1472\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Examine one of the other embedding model implementations for comparison\ncat \"src/RetrievalAugmentedGeneration/EmbeddingModels/HuggingFaceEmbeddingModel.cs\" 2>/dev/null | head -100\n```\n\nLength of output: 2257\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check for any documentation or comments about ONNX implementation status\nrg -i \"onnx|not implemented|placeholder|pending\" src/RetrievalAugmentedGeneration -n --type cs | head -30\n```\n\nLength of output: 2550\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Look for any constructor that accepts modelPath to understand intent\nrg \"modelPath|_modelPath\" src/RetrievalAugmentedGeneration/EmbeddingModels -B 3 -A 3 -n --type cs\n```\n\nLength of output: 8389\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check the base class to understand the contract\ncat \"src/RetrievalAugmentedGeneration/EmbeddingModels/EmbeddingModelBase.cs\" 2>/dev/null\n```\n\nLength of output: 0\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Look at the full namespace context and class hierarchy\nrg \"class ONNXSentenceTransformer|class EmbeddingModelBase\" src/RetrievalAugmentedGeneration -B 2 -A 5 -n --type cs\n```\n\nLength of output: 1861\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check if _modelPath might be used via reflection or dynamic invocation\nrg \"GetField|Invoke|Reflection|_modelPath\" src/RetrievalAugmentedGeneration -n --type cs\n```\n\nLength of output: 1575\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Examine the constructor implementation to see how _modelPath is assigned\nast-grep --pattern 'class ONNXSentenceTransformer {\n  $$$\n  public ONNXSentenceTransformer($_) {\n    $$$\n  }\n  $$$\n}'\n```\n\nLength of output: 0\n\n---\n\n\n\n</details>\n\n**Unused _modelPath field creates misleading API contract.**\n\nThe `_modelPath` field (line 14) is validated in the constructor but never used in `EmbedCore()`. This contradicts the class documentation claiming \"ONNX-based\" and misleads callers expecting model loading. The implementation silently falls back to hash-based synthetic embeddings instead.\n\nAdopt one of these approaches:\n1. **Throw `NotImplementedException`** in the constructor (matches pattern in `MultiModalEmbeddingModel`):\n   ```csharp\n   throw new NotImplementedException(\"ONNX support requires Runtime integration\");\n   ```\n2. **Remove the `modelPath` parameter** and document this as a synthetic embedding provider.\n3. **Implement ONNX loading** (if ready).\n\nAlso review `LocalTransformerEmbedding.cs` which has the identical issue.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/EmbeddingModels/ONNXSentenceTransformer.cs\naround line 14, the private field _modelPath is validated in the constructor but\nnever used in EmbedCore(), misleading callers; fix by picking one approach: (1)\nmirror MultiModalEmbeddingModel and throw a NotImplementedException in the\nconstructor indicating ONNX runtime integration is required, (2) remove the\nmodelPath parameter and _modelPath field and update class/docs to state this is\na synthetic/hash-based embedding provider, or (3) implement actual ONNX model\nloading/usage in EmbedCore so _modelPath is consumed; after choosing, update\nconstructor, class summary, and related tests, and apply the same fix to\nLocalTransformerEmbedding.cs.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T15:48:49Z",
    "updated_at": "2025-11-04T15:48:54Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491051849",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051849"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491051849"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051849/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": 15,
    "original_line": 14,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 14,
    "position": 15,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051857",
    "pull_request_review_id": 3417217555,
    "id": 2491051857,
    "node_id": "PRRC_kwDOKSXUF86Uem9R",
    "diff_hunk": "@@ -0,0 +1,65 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\n+using System;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels\n+{\n+    /// <summary>\n+    /// ONNX-based sentence transformer for generating embeddings.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class ONNXSentenceTransformer<T> : EmbeddingModelBase<T>\n+    {\n+        private readonly string _modelPath;\n+        private readonly int _dimension;\n+        private readonly int _maxTokens;\n+\n+        public override int EmbeddingDimension => _dimension;\n+        public override int MaxTokens => _maxTokens;\n+\n+        public ONNXSentenceTransformer(string modelPath, int dimension = 384, int maxTokens = 512)\n+        {\n+            if (string.IsNullOrWhiteSpace(modelPath))\n+                throw new ArgumentException(\"Model path cannot be empty\", nameof(modelPath));\n+            if (dimension <= 0)\n+                throw new ArgumentException(\"Dimension must be positive\", nameof(dimension));\n+            if (maxTokens <= 0)\n+                throw new ArgumentException(\"Max tokens must be positive\", nameof(maxTokens));\n+\n+            _modelPath = modelPath;\n+            _dimension = dimension;\n+            _maxTokens = maxTokens;\n+        }",
    "path": "src/RetrievalAugmentedGeneration/Embeddings/ONNXSentenceTransformer.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "2e85607917a4ef53ec4ed06ecd3d2cbc46bd3626",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n<details>\n<summary>≡ƒº⌐ Analysis chain</summary>\n\n**Verify design intent for modelPath parameter.**\n\nThe constructor validates and stores `modelPath`, but the current implementation uses a deterministic hash-based approach rather than loading an actual ONNX model. Ensure that downstream code doesn't incorrectly assume this model loads and uses the specified ONNX model file.\n\n\n\nRun the following script to check if any code attempts to use ONNXSentenceTransformer with actual model files:\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Description: Find usages of ONNXSentenceTransformer to verify how modelPath is being used\n\n# Search for instantiations and check what paths are being passed\nrg -n -C3 'new ONNXSentenceTransformer' --type=cs\n```\n\nLength of output: 0\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Search for broader usages of ONNXSentenceTransformer class\nrg -n -C2 'ONNXSentenceTransformer' --type=cs\n```\n\nLength of output: 1226\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check the implementation of ONNXSentenceTransformer class to see how modelPath is used\nrg -n -A20 'class ONNXSentenceTransformer' --type=cs\n```\n\nLength of output: 2594\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Search for test files that might instantiate ONNXSentenceTransformer\nfd -e 'Test.cs' -o -e 'Tests.cs' --exec grep -l 'ONNXSentenceTransformer' {} \\;\n```\n\nLength of output: 116\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Get the complete ONNXSentenceTransformer.cs file to see full implementation\ncat src/RetrievalAugmentedGeneration/EmbeddingModels/ONNXSentenceTransformer.cs\n```\n\nLength of output: 2144\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Search for usages of _modelPath field in the implementation\nrg -n '_modelPath' --type=cs\n```\n\nLength of output: 1575\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Search for instantiations with generic type parameter\nrg -n 'ONNXSentenceTransformer<' --type=cs\n```\n\nLength of output: 146\n\n---\n\n\n\n</details>\n\n**Remove unused `modelPath` parameter or implement actual ONNX model loading.**\n\nThe `_modelPath` field is validated and stored but never used in the `EmbedCore` method, which uses only a deterministic hash-based approach. Either remove the parameter entirely, or implement actual ONNX model loading. The current design creates false expectations that the model path is being utilized.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/EmbeddingModels/ONNXSentenceTransformer.cs\naround lines 21 to 33, the constructor accepts and stores modelPath but the rest\nof the class never uses it (EmbedCore currently computes embeddings via a hash),\nso either remove the modelPath parameter and associated _modelPath\nfield/validation or implement real ONNX model loading: if removing, delete the\nmodelPath param, _modelPath field, and related ArgumentException check and\nupdate callers/tests; if implementing, add ONNX runtime (or existing project\nruntime) initialization in the constructor (load the model from modelPath into\nan inference session or equivalent), store the session as a private field, and\nmodify EmbedCore to run the ONNX model to produce embeddings (respecting\ndimension and maxTokens), plus add error handling for load failures and update\nunit tests to reflect the real model usage.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T15:48:50Z",
    "updated_at": "2025-11-04T15:48:54Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491051857",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051857"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491051857"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051857/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 22,
    "original_start_line": 21,
    "start_side": "RIGHT",
    "line": 34,
    "original_line": 33,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 33,
    "position": 34,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051865",
    "pull_request_review_id": 3417217555,
    "id": 2491051865,
    "node_id": "PRRC_kwDOKSXUF86Uem9Z",
    "diff_hunk": "@@ -0,0 +1,68 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using AiDotNet.LinearAlgebra;",
    "path": "src/RetrievalAugmentedGeneration/QueryExpansion/HyDEQueryExpansion.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "2e85607917a4ef53ec4ed06ecd3d2cbc46bd3626",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Remove unused using statement.**\n\nThe `AiDotNet.LinearAlgebra` namespace is imported but never used. No `Vector<T>` or other LinearAlgebra types appear in the implementation.\n\nApply this diff:\n\n```diff\n using System;\n using System.Collections.Generic;\n using System.Linq;\n-using AiDotNet.LinearAlgebra;\n \n namespace AiDotNet.RetrievalAugmentedGeneration.QueryExpansion\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/QueryExpansion/HyDEQueryExpansion.cs around\nline 4, the using directive for AiDotNet.LinearAlgebra is unused; remove the\nunused using statement (delete the line \"using AiDotNet.LinearAlgebra;\") so the\nfile no longer imports an unreferenced namespace and then run a build/IDE\ncleanup to ensure no other unused usings remain.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T15:48:50Z",
    "updated_at": "2025-11-04T15:48:54Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491051865",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051865"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491051865"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051865/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": 4,
    "original_line": 4,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 4,
    "position": 4,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051889",
    "pull_request_review_id": 3417217555,
    "id": 2491051889,
    "node_id": "PRRC_kwDOKSXUF86Uem9x",
    "diff_hunk": "@@ -0,0 +1,68 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using AiDotNet.LinearAlgebra;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.QueryExpansion\n+{\n+    /// <summary>\n+    /// Hypothetical Document Embeddings (HyDE) query expansion strategy.\n+    /// </summary>\n+    public class HyDEQueryExpansion : QueryExpansionBase\n+    {\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"HyDEQueryExpansion\"/> class.\n+        /// </summary>\n+        public HyDEQueryExpansion()\n+        {\n+        }",
    "path": "src/RetrievalAugmentedGeneration/QueryExpansion/HyDEQueryExpansion.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "2e85607917a4ef53ec4ed06ecd3d2cbc46bd3626",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Parameterless constructor inconsistent with LLM integration requirements.**\n\nUnlike `SubQueryExpansion` and `MultiQueryExpansion` which accept `llmEndpoint` and `llmApiKey` parameters, this constructor has no configuration. Past reviews flagged that HyDE requires LLM-based document generation, but without endpoint/key fields, there's no path to add that integration. The class cannot fulfill its documented purpose of generating hypothetical documents via an LLM.\n\nConsider aligning with the pattern used by other expansion classes:\n\n```diff\n+    private readonly string _llmEndpoint;\n+    private readonly string _llmApiKey;\n+\n-    public HyDEQueryExpansion()\n+    public HyDEQueryExpansion(string llmEndpoint, string llmApiKey)\n     {\n+        _llmEndpoint = llmEndpoint ?? throw new ArgumentNullException(nameof(llmEndpoint));\n+        _llmApiKey = llmApiKey ?? throw new ArgumentNullException(nameof(llmApiKey));\n     }\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/QueryExpansion/HyDEQueryExpansion.cs around\nlines 16 to 18, the parameterless constructor prevents injecting LLM\nconfiguration required for HyDE; add a constructor that accepts the same\nllmEndpoint and llmApiKey parameters used by SubQueryExpansion and\nMultiQueryExpansion, assign them to private readonly fields (or call into an\nexisting primary constructor), and remove or delegate the parameterless ctor so\ninstances must be created with LLM config; update any call sites/tests to pass\nthe endpoint and key.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T15:48:50Z",
    "updated_at": "2025-11-04T15:48:54Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491051889",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051889"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491051889"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051889/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 16,
    "original_start_line": 16,
    "start_side": "RIGHT",
    "line": 18,
    "original_line": 18,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 18,
    "position": 18,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051901",
    "pull_request_review_id": 3417217555,
    "id": 2491051901,
    "node_id": "PRRC_kwDOKSXUF86Uem99",
    "diff_hunk": "@@ -0,0 +1,52 @@\n+using AiDotNet.RetrievalAugmentedGeneration.QueryExpansion;",
    "path": "src/RetrievalAugmentedGeneration/QueryExpansion/LearnedSparseEncoderExpansion.cs",
    "commit_id": "80630e82c71aabc214615014532fd5063336af16",
    "original_commit_id": "2e85607917a4ef53ec4ed06ecd3d2cbc46bd3626",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Remove redundant using statement.**\n\nLine 1 imports the same namespace declared on line 3. This is redundant.\n\nApply this diff:\n\n```diff\n-using AiDotNet.RetrievalAugmentedGeneration.QueryExpansion;\n-\n namespace AiDotNet.RetrievalAugmentedGeneration.QueryExpansion;\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\nnamespace AiDotNet.RetrievalAugmentedGeneration.QueryExpansion;\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn\nsrc/RetrievalAugmentedGeneration/QueryExpansion/LearnedSparseEncoderExpansion.cs\naround line 1, there's a redundant using statement importing\nAiDotNet.RetrievalAugmentedGeneration.QueryExpansion which is the same namespace\ndeclared further down; remove the duplicate using (delete the using on line 1)\nso the file only keeps the namespace declaration and run a quick build/format to\nensure no unused-usings remain.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 80630e8 to b6555ed",
    "created_at": "2025-11-04T15:48:50Z",
    "updated_at": "2025-11-05T02:32:04Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491051901",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051901"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491051901"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051901/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": null,
    "original_line": 1,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 1,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051918",
    "pull_request_review_id": 3417217555,
    "id": 2491051918,
    "node_id": "PRRC_kwDOKSXUF86Uem-O",
    "diff_hunk": "@@ -0,0 +1,100 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using AiDotNet.RetrievalAugmentedGeneration.Rerankers;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.RerankingStrategies;\n+\n+/// <summary>\n+/// Diversity-based reranker that prioritizes variety among retrieved documents.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// Reduces redundancy by ensuring retrieved documents cover diverse aspects of the topic\n+/// rather than returning multiple similar documents.\n+/// </remarks>\n+public class DiversityReranker<T> : RerankerBase<T>\n+{\n+    private readonly T _diversityWeight;\n+    private readonly T _relevanceWeight;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"DiversityReranker{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"diversityWeight\">Weight for diversity component (0-1).</param>\n+    /// <param name=\"relevanceWeight\">Weight for relevance component (0-1).</param>\n+    public DiversityReranker(\n+        T diversityWeight,\n+        T relevanceWeight)\n+    {\n+        _diversityWeight = diversityWeight;\n+        _relevanceWeight = relevanceWeight;\n+    }\n+\n+    /// <inheritdoc />\n+    public override bool ModifiesScores => true;\n+\n+    /// <inheritdoc />\n+    protected override IEnumerable<Document<T>> RerankCore(string query, IList<Document<T>> documents)\n+    {\n+        if (documents.Count == 0)\n+            return Enumerable.Empty<Document<T>>();\n+\n+        var selected = new List<Document<T>>();\n+        var remaining = new List<Document<T>>(documents);\n+\n+        // Select first document by relevance\n+        var first = remaining.OrderByDescending(d => d.HasRelevanceScore ? d.RelevanceScore : NumOps.Zero).First();",
    "path": "src/RetrievalAugmentedGeneration/RerankingStrategies/DiversityReranker.cs",
    "commit_id": "80630e82c71aabc214615014532fd5063336af16",
    "original_commit_id": "2e85607917a4ef53ec4ed06ecd3d2cbc46bd3626",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Replace LINQ ordering with NumOps-based comparison.**\n\n`OrderByDescending` requires `T` to implement `IComparable<T>`, but the framework doesn't constrain generic numeric types to that interfaceΓÇöit uses `INumericOperations<T>` for comparisons instead. This line will throw at runtime for numeric types that don't implement `IComparable`.\n\n\n\nApply this diff to use a safe manual selection:\n\n```diff\n-    // Select first document by relevance\n-    var first = remaining.OrderByDescending(d => d.HasRelevanceScore ? d.RelevanceScore : NumOps.Zero).First();\n-    selected.Add(first);\n-    remaining.Remove(first);\n+    // Select first document by relevance\n+    var first = remaining[0];\n+    var firstScore = first.HasRelevanceScore ? first.RelevanceScore : NumOps.Zero;\n+    for (int i = 1; i < remaining.Count; i++)\n+    {\n+        var candidateScore = remaining[i].HasRelevanceScore ? remaining[i].RelevanceScore : NumOps.Zero;\n+        if (NumOps.GreaterThan(candidateScore, firstScore))\n+        {\n+            first = remaining[i];\n+            firstScore = candidateScore;\n+        }\n+    }\n+    selected.Add(first);\n+    remaining.Remove(first);\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T15:48:50Z",
    "updated_at": "2025-11-04T15:48:54Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491051918",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051918"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491051918"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051918/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": null,
    "original_line": 47,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 47,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051943",
    "pull_request_review_id": 3417217555,
    "id": 2491051943,
    "node_id": "PRRC_kwDOKSXUF86Uem-n",
    "diff_hunk": "@@ -0,0 +1,100 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using AiDotNet.RetrievalAugmentedGeneration.Rerankers;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.RerankingStrategies;\n+\n+/// <summary>\n+/// Diversity-based reranker that prioritizes variety among retrieved documents.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// Reduces redundancy by ensuring retrieved documents cover diverse aspects of the topic\n+/// rather than returning multiple similar documents.\n+/// </remarks>\n+public class DiversityReranker<T> : RerankerBase<T>\n+{\n+    private readonly T _diversityWeight;\n+    private readonly T _relevanceWeight;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"DiversityReranker{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"diversityWeight\">Weight for diversity component (0-1).</param>\n+    /// <param name=\"relevanceWeight\">Weight for relevance component (0-1).</param>\n+    public DiversityReranker(\n+        T diversityWeight,\n+        T relevanceWeight)\n+    {\n+        _diversityWeight = diversityWeight;\n+        _relevanceWeight = relevanceWeight;\n+    }\n+\n+    /// <inheritdoc />\n+    public override bool ModifiesScores => true;\n+\n+    /// <inheritdoc />\n+    protected override IEnumerable<Document<T>> RerankCore(string query, IList<Document<T>> documents)\n+    {\n+        if (documents.Count == 0)\n+            return Enumerable.Empty<Document<T>>();\n+\n+        var selected = new List<Document<T>>();\n+        var remaining = new List<Document<T>>(documents);\n+\n+        // Select first document by relevance\n+        var first = remaining.OrderByDescending(d => d.HasRelevanceScore ? d.RelevanceScore : NumOps.Zero).First();\n+        selected.Add(first);\n+        remaining.Remove(first);\n+\n+        // Iteratively select documents balancing relevance and diversity\n+        while (remaining.Count > 0)\n+        {\n+            var bestDoc = remaining[0];\n+            var bestScore = NumOps.FromDouble(double.NegativeInfinity);\n+\n+            foreach (var doc in remaining)\n+            {\n+                // Relevance score\n+                var relevance = doc.HasRelevanceScore ? doc.RelevanceScore : NumOps.Zero;\n+\n+                // Diversity score (based on token overlap similarity)\n+                var minSimilarity = NumOps.One;\n+                foreach (var selectedDoc in selected)\n+                {\n+                    // Simple token overlap-based similarity\n+                    var tokens1 = doc.Content.Split(new[] { ' ', '\\t', '\\n', '\\r' }, StringSplitOptions.RemoveEmptyEntries);\n+                    var tokens2 = selectedDoc.Content.Split(new[] { ' ', '\\t', '\\n', '\\r' }, StringSplitOptions.RemoveEmptyEntries);",
    "path": "src/RetrievalAugmentedGeneration/RerankingStrategies/DiversityReranker.cs",
    "commit_id": "80630e82c71aabc214615014532fd5063336af16",
    "original_commit_id": "2e85607917a4ef53ec4ed06ecd3d2cbc46bd3626",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Add null checks for document content before tokenization.**\n\nThe code calls `Split` on `doc.Content` and `selectedDoc.Content` without null checks. Although `Content` defaults to `string.Empty`, it can be set to `null` by user code, which would throw `NullReferenceException` here.\n\n\n\nApply this diff to add validation:\n\n```diff\n+                if (string.IsNullOrEmpty(doc.Content) || string.IsNullOrEmpty(selectedDoc.Content))\n+                    continue;\n+\n                 // Simple token overlap-based similarity\n                 var tokens1 = doc.Content.Split(new[] { ' ', '\\t', '\\n', '\\r' }, StringSplitOptions.RemoveEmptyEntries);\n                 var tokens2 = selectedDoc.Content.Split(new[] { ' ', '\\t', '\\n', '\\r' }, StringSplitOptions.RemoveEmptyEntries);\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/RerankingStrategies/DiversityReranker.cs\naround lines 67-68, the code calls Split on doc.Content and selectedDoc.Content\nwithout null checks which can throw NullReferenceException; fix by normalizing\neach content to a non-null string (e.g. var content1 = doc.Content ??\nstring.Empty; var content2 = selectedDoc.Content ?? string.Empty;) and then call\nSplit on those variables, or explicitly check for null and handle/skip\naccordingly so tokenization never operates on a null reference.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T15:48:51Z",
    "updated_at": "2025-11-04T15:48:54Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491051943",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051943"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491051943"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051943/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 67,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 68,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 68,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051957",
    "pull_request_review_id": 3417217555,
    "id": 2491051957,
    "node_id": "PRRC_kwDOKSXUF86Uem-1",
    "diff_hunk": "@@ -0,0 +1,126 @@\n+using System;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.RerankingStrategies\n+{\n+    /// <summary>\n+    /// LLM-based reranking using language model relevance assessment.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class LLMBasedReranker<T> : Rerankers.RerankerBase<T>\n+    {\n+        private readonly string _llmEndpoint;\n+        private readonly string _apiKey;\n+\n+        /// <summary>\n+        /// Gets a value indicating whether this reranker modifies relevance scores.\n+        /// </summary>\n+        public override bool ModifiesScores => true;\n+\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"LLMBasedReranker{T}\"/> class.\n+        /// </summary>\n+        /// <param name=\"llmEndpoint\">The LLM API endpoint.</param>\n+        /// <param name=\"apiKey\">The API key for the LLM service.</param>\n+        public LLMBasedReranker(string? llmEndpoint = null, string? apiKey = null)\n+        {\n+            _llmEndpoint = llmEndpoint ?? string.Empty;\n+            _apiKey = apiKey ?? string.Empty;\n+        }\n+\n+        /// <summary>\n+        /// Reranks documents using LLM-based relevance scoring.\n+        /// </summary>\n+        /// <param name=\"query\">The query string.</param>\n+        /// <param name=\"documents\">The documents to rerank.</param>\n+        /// <returns>A reranked list of documents.</returns>\n+        protected override IEnumerable<Document<T>> RerankCore(string query, IList<Document<T>> documents)\n+        {\n+            var queryTokens = Tokenize(query);\n+            if (queryTokens.Count == 0)\n+            {\n+                return documents;\n+            }\n+\n+            var scoredDocs = new List<(Document<T> doc, T score)>();\n+\n+            foreach (var doc in documents)\n+            {\n+                var score = AssessRelevance(query, doc.Content);\n+                scoredDocs.Add((doc, score));\n+            }\n+\n+            var reranked = scoredDocs\n+                .OrderByDescending(x => Convert.ToDouble(x.score))\n+                .Select(x =>\n+                {\n+                    x.doc.RelevanceScore = x.score;\n+                    x.doc.HasRelevanceScore = true;\n+                    return x.doc;\n+                })\n+                .ToList();\n+\n+            return reranked;\n+        }\n+\n+        private T AssessRelevance(string query, string document)\n+        {\n+            var queryWords = Tokenize(query);\n+            var docWords = Tokenize(document);\n+\n+            var exactMatches = queryWords.Count(w => docWords.Contains(w));\n+            var proximityScore = ComputeProximityScore(queryWords, document);\n+            var semanticScore = ComputeSemanticSimilarity(query, document);\n+\n+            var totalScore = (exactMatches * 0.4 + proximityScore * 0.3 + semanticScore * 0.3);\n+            totalScore = Math.Min(1.0, totalScore / queryWords.Count);",
    "path": "src/RetrievalAugmentedGeneration/Rerankers/LLMBasedReranker.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "2e85607917a4ef53ec4ed06ecd3d2cbc46bd3626",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Prevent null document content from crashing proximity scoring.**\n\n`Document<T>.Content` is publicly settable and nothing in `Document<T>` guarantees it stays non-null. When a store yields a document with `Content == null`, this path dereferences it inside `ComputeProximityScore` (`document.ToLowerInvariant()`), throwing a `NullReferenceException` and killing the rerank. Please sanitize the content before scoring.\n\n\nApply this diff to guard against null content:\n\n```diff\n-        private T AssessRelevance(string query, string document)\n+        private T AssessRelevance(string query, string document)\n         {\n-            var queryWords = Tokenize(query);\n-            var docWords = Tokenize(document);\n+            var safeDocument = document ?? string.Empty;\n+\n+            var queryWords = Tokenize(query);\n+            var docWords = Tokenize(safeDocument);\n \n             var exactMatches = queryWords.Count(w => docWords.Contains(w));\n-            var proximityScore = ComputeProximityScore(queryWords, document);\n-            var semanticScore = ComputeSemanticSimilarity(query, document);\n+            var proximityScore = ComputeProximityScore(queryWords, safeDocument);\n+            var semanticScore = ComputeSemanticSimilarity(query, safeDocument);\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        private T AssessRelevance(string query, string document)\n        {\n            var safeDocument = document ?? string.Empty;\n\n            var queryWords = Tokenize(query);\n            var docWords = Tokenize(safeDocument);\n\n            var exactMatches = queryWords.Count(w => docWords.Contains(w));\n            var proximityScore = ComputeProximityScore(queryWords, safeDocument);\n            var semanticScore = ComputeSemanticSimilarity(query, safeDocument);\n\n            var totalScore = (exactMatches * 0.4 + proximityScore * 0.3 + semanticScore * 0.3);\n            totalScore = Math.Min(1.0, totalScore / queryWords.Count);\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/RerankingStrategies/LLMBasedReranker.cs\naround lines 68 to 78, the code assumes document is non-null and calls\nComputeProximityScore which dereferences document (causing a\nNullReferenceException if Content is null); sanitize the document before scoring\nby replacing null with an empty string (e.g., var safeDocument = document ??\nstring.Empty) and pass safeDocument into ComputeProximityScore and\nComputeSemanticSimilarity, and also guard the division by queryWords.Count by\nusing Math.Max(1, queryWords.Count) when normalizing totalScore to avoid\ndivide-by-zero.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T15:48:51Z",
    "updated_at": "2025-11-04T15:48:54Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491051957",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051957"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491051957"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051957/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 68,
    "original_start_line": 68,
    "start_side": "RIGHT",
    "line": 78,
    "original_line": 78,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 78,
    "position": 78,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051969",
    "pull_request_review_id": 3417217555,
    "id": 2491051969,
    "node_id": "PRRC_kwDOKSXUF86Uem_B",
    "diff_hunk": "@@ -0,0 +1,119 @@\n+using System;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System.Collections.Generic;\n+using System.Linq;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.RerankingStrategies\n+{\n+    /// <summary>\n+    /// Reciprocal Rank Fusion for combining multiple ranking lists.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class ReciprocalRankFusion<T> : Rerankers.RerankerBase<T>\n+    {\n+        private readonly int _k;\n+\n+        /// <summary>\n+        /// Gets a value indicating whether this reranker modifies relevance scores.\n+        /// </summary>\n+        public override bool ModifiesScores => true;\n+\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"ReciprocalRankFusion{T}\"/> class.\n+        /// </summary>\n+        /// <param name=\"k\">The constant k for reciprocal rank formula (default: 60).</param>\n+        public ReciprocalRankFusion(int k = 60)\n+        {\n+            _k = k > 0 ? k : throw new ArgumentOutOfRangeException(nameof(k));\n+        }\n+\n+        /// <summary>\n+        /// Reranks documents using reciprocal rank fusion.\n+        /// </summary>\n+        /// <param name=\"query\">The query string.</param>\n+        /// <param name=\"documents\">The documents to rerank.</param>\n+        /// <returns>A reranked list of documents.</returns>\n+        protected override IEnumerable<Document<T>> RerankCore(string query, IList<Document<T>> documents)\n+        {\n+            var scores = new Dictionary<string, T>();\n+\n+            for (int rank = 0; rank < documents.Count; rank++)\n+            {\n+                var doc = documents[rank];\n+                var rrfScore = NumOps.FromDouble(1.0 / (_k + rank + 1));\n+\n+                if (scores.TryGetValue(doc.Id, out var existingScore))\n+                {\n+                    scores[doc.Id] = NumOps.Add(existingScore, rrfScore);\n+                }\n+                else\n+                {\n+                    scores[doc.Id] = rrfScore;\n+                }\n+            }\n+\n+            var reranked = documents\n+                .OrderByDescending(d => Convert.ToDouble(scores.ContainsKey(d.Id) ? scores[d.Id] : NumOps.Zero))\n+                .ToList();\n+\n+            foreach (var doc in reranked)\n+            {\n+                if (scores.ContainsKey(doc.Id))\n+                {\n+                    doc.RelevanceScore = scores[doc.Id];\n+                    doc.HasRelevanceScore = true;\n+                }\n+            }\n+\n+            return reranked;\n+        }",
    "path": "src/RetrievalAugmentedGeneration/Rerankers/ReciprocalRankFusion.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "2e85607917a4ef53ec4ed06ecd3d2cbc46bd3626",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_≡ƒ¢á∩╕Å Refactor suggestion_ | _≡ƒƒá Major_\n\n**Remove redundant ContainsKey checks.**\n\nSince lines 41-54 iterate through all documents and add each `doc.Id` to the scores dictionary, every document is guaranteed to be in `scores` by line 56. The `ContainsKey` checks on lines 57 and 62 are redundant and add unnecessary overhead.\n\n\n\nApply this diff to remove the redundant checks:\n\n```diff\n         var reranked = documents\n-            .OrderByDescending(d => Convert.ToDouble(scores.ContainsKey(d.Id) ? scores[d.Id] : NumOps.Zero))\n+            .OrderByDescending(d => Convert.ToDouble(scores[d.Id]))\n             .ToList();\n\n         foreach (var doc in reranked)\n         {\n-            if (scores.ContainsKey(doc.Id))\n-            {\n-                doc.RelevanceScore = scores[doc.Id];\n-                doc.HasRelevanceScore = true;\n-            }\n+            doc.RelevanceScore = scores[doc.Id];\n+            doc.HasRelevanceScore = true;\n         }\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/RerankingStrategies/ReciprocalRankFusion.cs\naround lines 37 to 70, remove the redundant Dictionary.ContainsKey checks: when\nbuilding `scores` you already add every document id, so in the OrderByDescending\nreplace the ternary that checks `scores.ContainsKey(d.Id)` with direct access\n`scores[d.Id]`, and in the final foreach remove the `if\n(scores.ContainsKey(doc.Id))` guard and unconditionally assign\n`doc.RelevanceScore = scores[doc.Id]; doc.HasRelevanceScore = true;`. Ensure no\nother behavior changes.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 80630e8 to b6555ed",
    "created_at": "2025-11-04T15:48:51Z",
    "updated_at": "2025-11-05T02:32:10Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491051969",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051969"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491051969"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051969/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 37,
    "original_start_line": 37,
    "start_side": "RIGHT",
    "line": 62,
    "original_line": 70,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 70,
    "position": 62,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051984",
    "pull_request_review_id": 3417217555,
    "id": 2491051984,
    "node_id": "PRRC_kwDOKSXUF86Uem_Q",
    "diff_hunk": "@@ -0,0 +1,197 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\n+{\n+    /// <summary>\n+    /// BM25 (Best Matching 25) retrieval algorithm for sparse keyword-based search.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class BM25Retriever<T> : RetrieverBase<T>\n+    {\n+        private readonly IDocumentStore<T> _documentStore;\n+        private readonly T _k1;\n+        private readonly T _b;\n+        private readonly Dictionary<string, Dictionary<string, T>> _termFrequencies;\n+        private readonly Dictionary<string, T> _documentLengths;\n+        private readonly Dictionary<string, int> _documentFrequencies;\n+        private int _totalDocuments;\n+        private T _avgDocLength;\n+\n+        public BM25Retriever(IDocumentStore<T> documentStore, int defaultTopK = 5, double k1 = 1.5, double b = 0.75) \n+            : base(defaultTopK)\n+        {\n+            if (documentStore == null)\n+                throw new ArgumentNullException(nameof(documentStore));\n+\n+            _documentStore = documentStore;\n+            _k1 = NumOps.FromDouble(k1);\n+            _b = NumOps.FromDouble(b);\n+            _termFrequencies = new Dictionary<string, Dictionary<string, T>>();\n+            _documentLengths = new Dictionary<string, T>();\n+            _documentFrequencies = new Dictionary<string, int>();\n+            _avgDocLength = NumOps.Zero;\n+            _totalDocuments = 0;\n+        }\n+\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\n+        {\n+            var queryTerms = Tokenize(query);\n+            var scores = new Dictionary<string, T>();\n+\n+            var candidates = _documentStore.GetSimilar(\n+                new AiDotNet.LinearAlgebra.Vector<T>(new T[Math.Max(1, _documentStore.VectorDimension)]),\n+                _documentStore.DocumentCount\n+            );",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/BM25Retriever.cs",
    "commit_id": "2c4fd73b96810e83f9069d03400dd1dc73e157af",
    "original_commit_id": "2e85607917a4ef53ec4ed06ecd3d2cbc46bd3626",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_≡ƒ¢á∩╕Å Refactor suggestion_ | _≡ƒƒá Major_\n\n<details>\n<summary>≡ƒº⌐ Analysis chain</summary>\n\n**Fragile document retrieval relies on vector similarity API for keyword search.**\n\nBM25 is a keyword-based algorithm that needs the full corpus, but this code uses `GetSimilar` with a dummy zero vector to retrieve all documents. This approach:\n- Assumes the vector store returns all documents for a zero-vector query, which isn't guaranteed\n- Could fail if the store applies similarity thresholds or returns only nearest neighbors\n- Misuses an API designed for dense vector retrieval for sparse keyword retrieval\n\nConsider adding a `GetAllDocuments()` or `GetDocumentsByIds()` method to `IDocumentStore<T>` specifically for retrievers that need full corpus access.\n\n\n\n\nRun this script to check if other retrievers face the same issue:\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check how other retrievers access documents from the store\nrg -n -A 3 'GetSimilar' src/RetrievalAugmentedGeneration/Retrievers/\n```\n\nLength of output: 1625\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Find IDocumentStore interface and examine available methods\nfind . -name \"*.cs\" -type f | xargs grep -l \"interface IDocumentStore\" | head -1 | xargs cat\n```\n\nLength of output: 8545\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check if there are methods like GetAll, GetAllDocuments, or GetDocumentsByIds\nrg -n \"GetAll|GetDocumentsByIds|GetDocuments\\(\" src/ -A 2\n```\n\nLength of output: 1400\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Look at TFIDFRetriever to confirm identical pattern and check context\ncat src/RetrievalAugmentedGeneration/Retrievers/TFIDFRetriever.cs\n```\n\nLength of output: 5605\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check BM25Retriever full context to understand if there are tests or comments explaining workaround\ncat src/RetrievalAugmentedGeneration/Retrievers/BM25Retriever.cs\n```\n\nLength of output: 7191\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check if tests exist for BM25Retriever that might show expected behavior\nfind . -name \"*BM25*Test*.cs\" -o -name \"*Test*BM25*.cs\" | head -3 | xargs cat 2>/dev/null\n```\n\nLength of output: 0\n\n---\n\n\n\n</details>\n\n**Both BM25 and TFIDF retrievers rely on fragile zero-vector workaround to access full corpus.**\n\nThe zero-vector workaround at lines 46-49 misuses the `GetSimilar` API for a purpose it wasn't designed for. This same pattern appears in `TFIDFRetriever.cs:35-38`. Both keyword-based algorithms require the full document corpus to compute statistics (IDF calculations, term frequencies), but the `IDocumentStore<T>` interface lacks a method to retrieve all documents.\n\nThe `InMemoryDocumentStore` already implements `GetAllDocuments()`, but it's not part of the interface contract, making it inaccessible to retrievers. Add `GetAllDocuments()` to the `IDocumentStore<T>` interface so both retrievers can use it directly instead of the zero-vector workaround.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T15:48:51Z",
    "updated_at": "2025-11-04T15:48:55Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491051984",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051984"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491051984"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051984/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 46,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 49,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 49,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051995",
    "pull_request_review_id": 3417217555,
    "id": 2491051995,
    "node_id": "PRRC_kwDOKSXUF86Uem_b",
    "diff_hunk": "@@ -0,0 +1,79 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\n+{\n+    /// <summary>\n+    /// Multi-query retriever that generates multiple query variations and merges results.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class MultiQueryRetriever<T> : RetrieverBase<T>\n+    {\n+        private readonly IRetriever<T> _baseRetriever;\n+        private readonly int _numQueries;\n+\n+        public MultiQueryRetriever(IRetriever<T> baseRetriever, int numQueries = 3, int defaultTopK = 5) : base(defaultTopK)\n+        {\n+            if (baseRetriever == null)\n+                throw new ArgumentNullException(nameof(baseRetriever));\n+            if (numQueries <= 0)\n+                throw new ArgumentException(\"Number of queries must be positive\", nameof(numQueries));\n+\n+            _baseRetriever = baseRetriever;\n+            _numQueries = numQueries;\n+        }\n+\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\n+        {\n+            var queries = GenerateQueries(query);\n+            var allResults = new Dictionary<string, (Document<T> doc, T score)>();\n+\n+            foreach (var q in queries)\n+            {\n+                var results = _baseRetriever.Retrieve(q, topK, metadataFilters);\n+\n+                foreach (var doc in results.Where(d => d.HasRelevanceScore))\n+                {\n+                    if (allResults.TryGetValue(doc.Id, out var existing))\n+                    {\n+                        var newScore = NumOps.Add(existing.score, doc.RelevanceScore);\n+                        allResults[doc.Id] = (doc, newScore);\n+                    }\n+                    else\n+                    {\n+                        allResults[doc.Id] = (doc, doc.RelevanceScore);\n+                    }\n+                }\n+            }\n+\n+            var merged = allResults.Values\n+                .OrderByDescending(x => x.score)\n+                .Take(topK)\n+                .Select(x =>\n+                {\n+                    x.doc.RelevanceScore = x.score;\n+                    x.doc.HasRelevanceScore = true;\n+                    return x.doc;\n+                })\n+                .ToList();\n+\n+            return merged;\n+        }\n+\n+        private List<string> GenerateQueries(string originalQuery)\n+        {\n+            var queries = new List<string> { originalQuery };\n+\n+            for (int i = 1; i < _numQueries; i++)\n+            {\n+                queries.Add($\"{originalQuery} variation {i}\");\n+            }\n+\n+            return queries;\n+        }",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/MultiQueryRetriever.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "2e85607917a4ef53ec4ed06ecd3d2cbc46bd3626",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Stub implementation defeats the purpose of multi-query retrieval.**\n\nThe current implementation simply appends \"variation {i}\" to the original query, which produces meaningless query variations. This defeats the entire purpose of multi-query retrieval, as the generated queries are not semantically diverse and will likely return nearly identical results.\n\n\nBased on the PR objectives mentioning `LLMQueryExpansion` and `HyDEQueryExpansion`, consider injecting an `IQueryExpansion` strategy as a dependency instead of hardcoding query generation:\n\n```diff\n-    public class MultiQueryRetriever<T> : RetrieverBase<T>\n-    {\n-        private readonly IRetriever<T> _baseRetriever;\n-        private readonly int _numQueries;\n+    public class MultiQueryRetriever<T> : RetrieverBase<T>\n+    {\n+        private readonly IRetriever<T> _baseRetriever;\n+        private readonly IQueryExpansion _queryExpansion;\n+        private readonly int _numQueries;\n\n-        public MultiQueryRetriever(IRetriever<T> baseRetriever, int numQueries = 3, int defaultTopK = 5) : base(defaultTopK)\n+        public MultiQueryRetriever(IRetriever<T> baseRetriever, IQueryExpansion queryExpansion, int numQueries = 3, int defaultTopK = 5) : base(defaultTopK)\n         {\n             if (baseRetriever == null)\n                 throw new ArgumentNullException(nameof(baseRetriever));\n+            if (queryExpansion == null)\n+                throw new ArgumentNullException(nameof(queryExpansion));\n             if (numQueries <= 0)\n                 throw new ArgumentException(\"Number of queries must be positive\", nameof(numQueries));\n\n             _baseRetriever = baseRetriever;\n+            _queryExpansion = queryExpansion;\n             _numQueries = numQueries;\n         }\n```\n\nThen update `GenerateQueries` to delegate to the injected strategy:\n\n```diff\n-        private List<string> GenerateQueries(string originalQuery)\n-        {\n-            var queries = new List<string> { originalQuery };\n-\n-            for (int i = 1; i < _numQueries; i++)\n-            {\n-                queries.Add($\"{originalQuery} variation {i}\");\n-            }\n-\n-            return queries;\n-        }\n+        private List<string> GenerateQueries(string originalQuery)\n+        {\n+            return _queryExpansion.ExpandQuery(originalQuery, _numQueries);\n+        }\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T15:48:52Z",
    "updated_at": "2025-11-04T15:48:55Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491051995",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051995"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491051995"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491051995/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 67,
    "original_start_line": 67,
    "start_side": "RIGHT",
    "line": 77,
    "original_line": 77,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 77,
    "position": 77,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491052008",
    "pull_request_review_id": 3417217555,
    "id": 2491052008,
    "node_id": "PRRC_kwDOKSXUF86Uem_o",
    "diff_hunk": "@@ -0,0 +1,61 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.QueryExpansion;",
    "path": "temp_file.txt",
    "commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "original_commit_id": "2e85607917a4ef53ec4ed06ecd3d2cbc46bd3626",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Remove redundant using statement.**\n\nLine 2 imports the same namespace declared on line 4. A file doesn't need to import its own namespace.\n\nApply this diff:\n\n```diff\n using AiDotNet.Interfaces;\n-using AiDotNet.RetrievalAugmentedGeneration.QueryExpansion;\n \n namespace AiDotNet.RetrievalAugmentedGeneration.QueryExpansion;\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\nusing AiDotNet.Interfaces;\n\nnamespace AiDotNet.RetrievalAugmentedGeneration.QueryExpansion;\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn temp_file.txt around line 2, the using statement \"using\nAiDotNet.RetrievalAugmentedGeneration.QueryExpansion;\" is redundant because the\nfile already declares that same namespace later (line 4); remove the duplicate\nusing on line 2 so the file does not import its own namespace.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T15:48:52Z",
    "updated_at": "2025-11-04T15:48:55Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491052008",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491052008"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491052008"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491052008/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": null,
    "original_line": 2,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 2,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491052017",
    "pull_request_review_id": 3417217555,
    "id": 2491052017,
    "node_id": "PRRC_kwDOKSXUF86Uem_x",
    "diff_hunk": "@@ -0,0 +1,61 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.QueryExpansion;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.QueryExpansion;\n+\n+/// <summary>\n+/// Learned sparse encoder expansion using models like SPLADE.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// Uses a learned sparse model (e.g., SPLADE) to expand queries with relevant terms\n+/// weighted by their importance, combining benefits of sparse and dense retrieval.\n+/// </remarks>\n+public class LearnedSparseEncoderExpansion : QueryExpansionBase\n+{\n+    private readonly string _modelPath;\n+    private readonly int _maxExpansionTerms;\n+    private readonly T _minTermWeight;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"LearnedSparseEncoderExpansion{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"modelPath\">Path to the SPLADE or similar model.</param>\n+    /// <param name=\"maxExpansionTerms\">Maximum number of expansion terms to add.</param>\n+    /// <param name=\"minTermWeight\">Minimum weight threshold for including a term.</param>\n+    /// <param name=\"numericOperations\">The numeric operations provider.</param>\n+    public LearnedSparseEncoderExpansion(\n+        string modelPath,\n+        int maxExpansionTerms,\n+        T minTermWeight,\n+        INumericOperations<T> numericOperations)\n+        : base(numericOperations)\n+    {\n+        _modelPath = modelPath ?? throw new ArgumentNullException(nameof(modelPath));\n+        \n+        if (maxExpansionTerms <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(maxExpansionTerms), \"Max expansion terms must be positive\");\n+            \n+        _maxExpansionTerms = maxExpansionTerms;\n+        _minTermWeight = minTermWeight;\n+    }\n+\n+    /// <summary>\n+    /// Expands the query using learned sparse encoding.\n+    /// </summary>\n+    public override IEnumerable<string> ExpandQuery(string query)",
    "path": "temp_file.txt",
    "commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "original_commit_id": "2e85607917a4ef53ec4ed06ecd3d2cbc46bd3626",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Critical: Return type mismatch with base class.**\n\nThe method returns `IEnumerable<string>` but the base class `QueryExpansionBase.ExpandQuery` requires `List<string>` (see QueryExpansionBase.cs lines 7-15). This breaks the override contract.\n\nApply this diff to fix the return type:\n\n```diff\n-    public override IEnumerable<string> ExpandQuery(string query)\n+    public override List<string> ExpandQuery(string query)\n```\n\nThen update the return statement on line 57 to return a List instead of throwing:\n\n```diff\n-        throw new NotImplementedException(\"Learned sparse encoder expansion requires SPLADE/ONNX model integration\");\n+        return new List<string> { query };\n+        // TODO: Implement learned sparse expansion (see TODO comments above)\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn temp_file.txt around line 46, the method signature currently declares \"public\noverride IEnumerable<string> ExpandQuery(string query)\" but the base class\nQueryExpansionBase.ExpandQuery requires \"List<string>\" causing an override\nmismatch; change the method signature to \"public override List<string>\nExpandQuery(string query)\" and update the return on line 57 to return a\nList<string> (e.g., new List<string> { /* results */ } or convert any existing\nIEnumerable to a List via .ToList()) instead of throwing; ensure System.Linq is\nimported if you use .ToList().\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T15:48:52Z",
    "updated_at": "2025-11-04T15:48:55Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491052017",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491052017"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491052017"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491052017/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": null,
    "original_line": 46,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 46,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491104324",
    "pull_request_review_id": 3417293438,
    "id": 2491104324,
    "node_id": "PRRC_kwDOKSXUF86UezxE",
    "diff_hunk": "@@ -0,0 +1,58 @@\n+using AiDotNet.NumericOperations;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using AiDotNet.RetrievalAugmentedGeneration.Retrievers;\n+using System;\n+using System.Collections.Generic;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.AdvancedPatterns;\n+\n+/// <summary>\n+/// Chain-of-Thought retriever that generates reasoning steps before retrieving documents.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// Generates a chain of thought reasoning process based on the query, then uses\n+/// that reasoning to inform retrieval, leading to more targeted and relevant results.\n+/// </remarks>\n+public class ChainOfThoughtRetriever<T>",
    "path": "src/RetrievalAugmentedGeneration/AdvancedPatterns/ChainOfThoughtRetriever.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "d54eddaa48a8017826434db69a3ca8defd40e974",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n<details>\n<summary>≡ƒº⌐ Analysis chain</summary>\n\n**Implement `IRetriever<T>` or inherit from `RetrieverBase<T>` to follow the framework pattern.**\n\nThe class is currently standalone but should follow the \"interface ΓåÆ base class ΓåÆ concrete implementation\" pattern described in the PR objectives. Other retrievers (e.g., `ParentDocumentRetriever<T>`) inherit from `RetrieverBase<T>` and override `RetrieveCore`. Consider either:\n\n1. Inheriting from `RetrieverBase<T>` and implementing `RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)`, or\n2. Directly implementing `IRetriever<T>` if the base class doesn't suit the chain-of-thought pattern.\n\nThis ensures consistency with the framework architecture and allows the retriever to be used polymorphically.\n\n\n\n\nRun the following script to verify the existence of `IRetriever<T>` interface and examine the pattern used by other retrievers:\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check if IRetriever interface exists and examine retriever patterns\nast-grep --pattern $'interface IRetriever<$_> {\n  $$$\n}'\n\n# Check other retriever implementations for inheritance patterns\nrg -n -A3 -B1 --type=cs 'class.*Retriever.*<T>.*:.*RetrieverBase'\n```\n\nLength of output: 5576\n\n---\n\n```\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Verify ChainOfThoughtRetriever class declaration\nsed -n '17,20p' src/RetrievalAugmentedGeneration/AdvancedPatterns/ChainOfThoughtRetriever.cs\n```\n\nLength of output: 119\n\n---\n\n```\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Search for ChainOfThoughtRetriever file anywhere in the repository\nfd -type f -name '*ChainOfThought*'\n```\n\nLength of output: 170\n\n---\n\n```\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Search for ChainOfThoughtRetriever file with correct fd syntax\nfd 'ChainOfThought' --type file\n```\n\nLength of output: 76\n\n---\n\n```\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Read the class declaration from ChainOfThoughtRetriever.cs\ncat src/RetrievalAugmentedGeneration/AdvancedPatterns/ChainOfThoughtRetriever.cs | sed -n '17,20p'\n```\n\nLength of output: 123\n\n---\n\n\n\n</details>\n\n**Inherit from `RetrieverBase<T>` to match framework architecture.**\n\nThe `ChainOfThoughtRetriever<T>` class must inherit from `RetrieverBase<T>` to follow the established pattern used by all 10 other retrievers in the codebase (ColBERTRetriever, ParentDocumentRetriever, TFIDFRetriever, BM25Retriever, MultiVectorRetriever, VectorRetriever, MultiQueryRetriever, DenseRetriever, GraphRetriever, HybridRetriever). Update line 17 from:\n\n```\npublic class ChainOfThoughtRetriever<T>\n```\n\nto:\n\n```\npublic class ChainOfThoughtRetriever<T> : RetrieverBase<T>\n```\n\nThen implement the `RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)` method required by the base class.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/AdvancedPatterns/ChainOfThoughtRetriever.cs\naround line 17, change the class declaration to inherit from RetrieverBase<T>\nand implement the required abstract method: override RetrieveCore(string query,\nint topK, Dictionary<string, object> metadataFilters). Update the line to\n\"public class ChainOfThoughtRetriever<T> : RetrieverBase<T>\" and add an override\nfor RetrieveCore that accepts the query, topK and metadataFilters, performs the\nretriever's existing retrieval logic (filtering, scoring, and selecting topK\nitems) and returns the expected collection/type the base requires; ensure method\nsignature and return type exactly match RetrieverBase<T>'s abstract definition\nand preserve existing class behavior inside the new method body.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T16:02:08Z",
    "updated_at": "2025-11-04T16:02:11Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491104324",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491104324"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491104324"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491104324/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": 89,
    "original_line": 17,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 17,
    "position": 89,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491104341",
    "pull_request_review_id": 3417293438,
    "id": 2491104341,
    "node_id": "PRRC_kwDOKSXUF86UezxV",
    "diff_hunk": "@@ -0,0 +1,188 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ContextCompression\n+{\n+    /// <summary>\n+    /// Document summarizer for creating concise summaries of retrieved content.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class DocumentSummarizer<T> : ContextCompressorBase<T>\n+    {\n+        private readonly int _maxSummaryLength;\n+        private readonly string _llmEndpoint;\n+        private readonly string _apiKey;\n+\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"DocumentSummarizer{T}\"/> class.\n+        /// </summary>\n+        /// <param name=\"numericOperations\">The numeric operations for type T.</param>\n+        /// <param name=\"maxSummaryLength\">The maximum length of the summary in characters.</param>\n+        /// <param name=\"llmEndpoint\">The LLM API endpoint.</param>\n+        /// <param name=\"apiKey\">The API key for the LLM service.</param>\n+        public DocumentSummarizer(\n+            INumericOperations<T> numericOperations,\n+            int maxSummaryLength = 500,\n+            string llmEndpoint = \"\",\n+            string apiKey = \"\")\n+        {\n+            if (numericOperations == null)\n+                throw new ArgumentNullException(nameof(numericOperations));\n+                \n+            _maxSummaryLength = maxSummaryLength > 0\n+                ? maxSummaryLength\n+                : throw new ArgumentOutOfRangeException(nameof(maxSummaryLength));\n+            _llmEndpoint = llmEndpoint;\n+            _apiKey = apiKey;\n+        }",
    "path": "src/RetrievalAugmentedGeneration/ContextCompression/DocumentSummarizer.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "d54eddaa48a8017826434db69a3ca8defd40e974",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Unused constructor parameter suggests incomplete implementation.**\n\nThe `numericOperations` parameter is validated but never stored or used. While the base class `ContextCompressorBase<T>` provides a static `NumOps` field, requiring this parameter without using it is misleading and suggests either incomplete implementation or unnecessary validation.\n\n\n\nConsider one of these approaches:\n\n```diff\n public DocumentSummarizer(\n-    INumericOperations<T> numericOperations,\n     int maxSummaryLength = 500,\n     string llmEndpoint = \"\",\n     string apiKey = \"\")\n {\n-    if (numericOperations == null)\n-        throw new ArgumentNullException(nameof(numericOperations));\n-        \n     _maxSummaryLength = maxSummaryLength > 0\n         ? maxSummaryLength\n         : throw new ArgumentOutOfRangeException(nameof(maxSummaryLength));\n```\n\nOr if future numeric operations are planned, store it:\n\n```diff\n+private readonly INumericOperations<T> _numericOperations;\n+\n public DocumentSummarizer(\n     INumericOperations<T> numericOperations,\n     int maxSummaryLength = 500,\n     string llmEndpoint = \"\",\n     string apiKey = \"\")\n {\n     if (numericOperations == null)\n         throw new ArgumentNullException(nameof(numericOperations));\n+    _numericOperations = numericOperations;\n         \n     _maxSummaryLength = maxSummaryLength > 0\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        public DocumentSummarizer(\n            int maxSummaryLength = 500,\n            string llmEndpoint = \"\",\n            string apiKey = \"\")\n        {\n            _maxSummaryLength = maxSummaryLength > 0\n                ? maxSummaryLength\n                : throw new ArgumentOutOfRangeException(nameof(maxSummaryLength));\n            _llmEndpoint = llmEndpoint;\n            _apiKey = apiKey;\n        }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/ContextCompression/DocumentSummarizer.cs\naround lines 26 to 40, the constructor validates the numericOperations parameter\nbut never stores or uses it; update the constructor to assign the provided\nnumericOperations to the base class static NumOps (e.g.,\nContextCompressorBase<T>.NumOps = numericOperations) or to a private readonly\nfield so the parameter is actually retained for use; keep the existing null\ncheck and ArgumentOutOfRange handling, and remove the parameter instead only if\nyou intentionally want to drop numericOperations from the API.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T16:02:08Z",
    "updated_at": "2025-11-04T16:02:11Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491104341",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491104341"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491104341"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491104341/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 26,
    "original_start_line": 26,
    "start_side": "RIGHT",
    "line": 40,
    "original_line": 40,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 40,
    "position": 40,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491104379",
    "pull_request_review_id": 3417293438,
    "id": 2491104379,
    "node_id": "PRRC_kwDOKSXUF86Uezx7",
    "diff_hunk": "@@ -0,0 +1,188 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ContextCompression\n+{\n+    /// <summary>\n+    /// Document summarizer for creating concise summaries of retrieved content.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class DocumentSummarizer<T> : ContextCompressorBase<T>\n+    {\n+        private readonly int _maxSummaryLength;\n+        private readonly string _llmEndpoint;\n+        private readonly string _apiKey;\n+\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"DocumentSummarizer{T}\"/> class.\n+        /// </summary>\n+        /// <param name=\"numericOperations\">The numeric operations for type T.</param>\n+        /// <param name=\"maxSummaryLength\">The maximum length of the summary in characters.</param>\n+        /// <param name=\"llmEndpoint\">The LLM API endpoint.</param>\n+        /// <param name=\"apiKey\">The API key for the LLM service.</param>\n+        public DocumentSummarizer(\n+            INumericOperations<T> numericOperations,\n+            int maxSummaryLength = 500,\n+            string llmEndpoint = \"\",\n+            string apiKey = \"\")\n+        {\n+            if (numericOperations == null)\n+                throw new ArgumentNullException(nameof(numericOperations));\n+                \n+            _maxSummaryLength = maxSummaryLength > 0\n+                ? maxSummaryLength\n+                : throw new ArgumentOutOfRangeException(nameof(maxSummaryLength));\n+            _llmEndpoint = llmEndpoint;\n+            _apiKey = apiKey;\n+        }\n+\n+        /// <summary>\n+        /// Compresses documents by summarizing their content.\n+        /// </summary>\n+        protected override List<Document<T>> CompressCore(\n+            List<Document<T>> documents,\n+            string query,\n+            Dictionary<string, object>? options = null)\n+        {\n+            var summarized = new List<Document<T>>();\n+\n+            foreach (var doc in documents)\n+            {\n+                var summary = SummarizeText(doc.Content);\n+                var summarizedDoc = new Document<T>(doc.Id, summary)\n+                {\n+                    Metadata = doc.Metadata,\n+                    RelevanceScore = doc.RelevanceScore,\n+                    HasRelevanceScore = doc.HasRelevanceScore\n+                };\n+                summarized.Add(summarizedDoc);\n+            }\n+\n+            return summarized;\n+        }\n+\n+        /// <summary>\n+        /// Summarizes a list of documents.\n+        /// </summary>\n+        /// <param name=\"documents\">The documents to summarize.</param>\n+        /// <returns>A list of summarized documents.</returns>\n+        public List<Document<T>> Summarize(List<Document<T>> documents)\n+        {\n+            if (documents == null) throw new ArgumentNullException(nameof(documents));\n+\n+            var summarized = new List<Document<T>>();\n+\n+            foreach (var doc in documents)\n+            {\n+                var summary = SummarizeText(doc.Content);\n+                var summarizedDoc = new Document<T>(doc.Id, summary)\n+                {\n+                    Metadata = doc.Metadata,\n+                    RelevanceScore = doc.RelevanceScore,\n+                    HasRelevanceScore = doc.HasRelevanceScore\n+                };\n+                summarized.Add(summarizedDoc);\n+            }\n+\n+            return summarized;\n+        }",
    "path": "src/RetrievalAugmentedGeneration/ContextCompression/DocumentSummarizer.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "d54eddaa48a8017826434db69a3ca8defd40e974",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_≡ƒ¢á∩╕Å Refactor suggestion_ | _≡ƒƒá Major_\n\n**Eliminate duplicate compression logic.**\n\nThe `CompressCore` (lines 45-65) and `Summarize` (lines 72-91) methods contain identical logic. This violates DRY principles and creates maintenance burden.\n\n\n\nRefactor to eliminate duplication:\n\n```diff\n protected override List<Document<T>> CompressCore(\n     List<Document<T>> documents,\n     string query,\n     Dictionary<string, object>? options = null)\n {\n-    var summarized = new List<Document<T>>();\n-\n-    foreach (var doc in documents)\n-    {\n-        var summary = SummarizeText(doc.Content);\n-        var summarizedDoc = new Document<T>(doc.Id, summary)\n-        {\n-            Metadata = doc.Metadata,\n-            RelevanceScore = doc.RelevanceScore,\n-            HasRelevanceScore = doc.HasRelevanceScore\n-        };\n-        summarized.Add(summarizedDoc);\n-    }\n-\n-    return summarized;\n+    return Summarize(documents);\n }\n\n public List<Document<T>> Summarize(List<Document<T>> documents)\n {\n     if (documents == null) throw new ArgumentNullException(nameof(documents));\n\n     var summarized = new List<Document<T>>();\n\n     foreach (var doc in documents)\n     {\n         var summary = SummarizeText(doc.Content);\n         var summarizedDoc = new Document<T>(doc.Id, summary)\n         {\n             Metadata = doc.Metadata,\n             RelevanceScore = doc.RelevanceScore,\n             HasRelevanceScore = doc.HasRelevanceScore\n         };\n         summarized.Add(summarizedDoc);\n     }\n\n     return summarized;\n }\n```\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T16:02:09Z",
    "updated_at": "2025-11-04T16:02:11Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491104379",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491104379"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491104379"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491104379/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 45,
    "original_start_line": 45,
    "start_side": "RIGHT",
    "line": 92,
    "original_line": 91,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 91,
    "position": 92,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491104399",
    "pull_request_review_id": 3417293438,
    "id": 2491104399,
    "node_id": "PRRC_kwDOKSXUF86UezyP",
    "diff_hunk": "@@ -0,0 +1,172 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ContextCompression\n+{\n+    /// <summary>\n+    /// LLM-based context compression to reduce token usage while preserving key information.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class LLMContextCompressor<T> : ContextCompressorBase<T>\n+    {\n+        private readonly string _llmEndpoint;\n+        private readonly string _apiKey;\n+        private readonly double _compressionRatio;\n+\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"LLMContextCompressor{T}\"/> class.\n+        /// </summary>\n+        /// <param name=\"numericOperations\">The numeric operations for type T.</param>\n+        /// <param name=\"compressionRatio\">The target compression ratio (0.0 to 1.0).</param>\n+        /// <param name=\"llmEndpoint\">The LLM API endpoint.</param>\n+        /// <param name=\"apiKey\">The API key for the LLM service.</param>\n+        public LLMContextCompressor(\n+            INumericOperations<T> numericOperations,\n+            double compressionRatio = 0.5,\n+            string llmEndpoint = \"\",\n+            string apiKey = \"\")\n+        {\n+            if (numericOperations == null)\n+                throw new ArgumentNullException(nameof(numericOperations));\n+                \n+            _compressionRatio = compressionRatio >= 0 && compressionRatio <= 1\n+                ? compressionRatio\n+                : throw new ArgumentOutOfRangeException(nameof(compressionRatio), \"Compression ratio must be between 0 and 1\");\n+            _llmEndpoint = llmEndpoint;\n+            _apiKey = apiKey;\n+        }",
    "path": "src/RetrievalAugmentedGeneration/ContextCompression/LLMContextCompressor.cs",
    "commit_id": "d54eddaa48a8017826434db69a3ca8defd40e974",
    "original_commit_id": "d54eddaa48a8017826434db69a3ca8defd40e974",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Unused constructor parameter suggests incomplete implementation.**\n\nThe `numericOperations` parameter is validated but never stored or used, identical to the issue in `DocumentSummarizer.cs`. This is misleading and suggests either incomplete implementation or unnecessary validation.\n\n\n\nRemove the unused parameter:\n\n```diff\n public LLMContextCompressor(\n-    INumericOperations<T> numericOperations,\n     double compressionRatio = 0.5,\n     string llmEndpoint = \"\",\n     string apiKey = \"\")\n {\n-    if (numericOperations == null)\n-        throw new ArgumentNullException(nameof(numericOperations));\n-        \n     _compressionRatio = compressionRatio >= 0 && compressionRatio <= 1\n         ? compressionRatio\n         : throw new ArgumentOutOfRangeException(nameof(compressionRatio), \"Compression ratio must be between 0 and 1\");\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        public LLMContextCompressor(\n            double compressionRatio = 0.5,\n            string llmEndpoint = \"\",\n            string apiKey = \"\")\n        {\n            _compressionRatio = compressionRatio >= 0 && compressionRatio <= 1\n                ? compressionRatio\n                : throw new ArgumentOutOfRangeException(nameof(compressionRatio), \"Compression ratio must be between 0 and 1\");\n            _llmEndpoint = llmEndpoint;\n            _apiKey = apiKey;\n        }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/ContextCompression/LLMContextCompressor.cs\naround lines 26 to 40, the constructor accepts and validates the\nINumericOperations<T> numericOperations parameter but never stores or uses it;\nremove the unused parameter and its related null check from the constructor\nsignature and body, and update any call sites to the constructor accordingly (or\nalternatively, if numeric operations are intended to be used, assign\nnumericOperations to a private readonly field and use it where needed)ΓÇöensure\nconstructor parameter list, validations, and assignments remain consistent after\nthe change.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commit 72f2fd6",
    "created_at": "2025-11-04T16:02:09Z",
    "updated_at": "2025-11-04T16:14:08Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491104399",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491104399"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491104399"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491104399/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 25,
    "original_start_line": 26,
    "start_side": "RIGHT",
    "line": 35,
    "original_line": 40,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 40,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491104427",
    "pull_request_review_id": 3417293438,
    "id": 2491104427,
    "node_id": "PRRC_kwDOKSXUF86Uezyr",
    "diff_hunk": "@@ -0,0 +1,172 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ContextCompression\n+{\n+    /// <summary>\n+    /// LLM-based context compression to reduce token usage while preserving key information.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class LLMContextCompressor<T> : ContextCompressorBase<T>\n+    {\n+        private readonly string _llmEndpoint;\n+        private readonly string _apiKey;\n+        private readonly double _compressionRatio;\n+\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"LLMContextCompressor{T}\"/> class.\n+        /// </summary>\n+        /// <param name=\"numericOperations\">The numeric operations for type T.</param>\n+        /// <param name=\"compressionRatio\">The target compression ratio (0.0 to 1.0).</param>\n+        /// <param name=\"llmEndpoint\">The LLM API endpoint.</param>\n+        /// <param name=\"apiKey\">The API key for the LLM service.</param>\n+        public LLMContextCompressor(\n+            INumericOperations<T> numericOperations,\n+            double compressionRatio = 0.5,\n+            string llmEndpoint = \"\",\n+            string apiKey = \"\")\n+        {\n+            if (numericOperations == null)\n+                throw new ArgumentNullException(nameof(numericOperations));\n+                \n+            _compressionRatio = compressionRatio >= 0 && compressionRatio <= 1\n+                ? compressionRatio\n+                : throw new ArgumentOutOfRangeException(nameof(compressionRatio), \"Compression ratio must be between 0 and 1\");\n+            _llmEndpoint = llmEndpoint;\n+            _apiKey = apiKey;\n+        }\n+\n+        /// <summary>\n+        /// Compresses documents while preserving relevance to the query.\n+        /// </summary>\n+        protected override List<Document<T>> CompressCore(\n+            List<Document<T>> documents,\n+            string query,\n+            Dictionary<string, object>? options = null)\n+        {\n+            var compressed = new List<Document<T>>();\n+\n+            foreach (var doc in documents)\n+            {\n+                var compressedContent = CompressText(query, doc.Content);\n+                var compressedDoc = new Document<T>(doc.Id, compressedContent)\n+                {\n+                    Metadata = doc.Metadata,\n+                    RelevanceScore = doc.RelevanceScore,\n+                    HasRelevanceScore = doc.HasRelevanceScore\n+                };\n+                compressed.Add(compressedDoc);\n+            }\n+\n+            return compressed;\n+        }\n+\n+        /// <summary>\n+        /// Compresses a list of documents while preserving relevance to the query.\n+        /// </summary>\n+        /// <param name=\"query\">The query context.</param>\n+        /// <param name=\"documents\">The documents to compress.</param>\n+        /// <returns>A list of compressed documents.</returns>\n+        public List<Document<T>> Compress(string query, List<Document<T>> documents)\n+        {\n+            if (string.IsNullOrEmpty(query)) throw new ArgumentNullException(nameof(query));\n+            if (documents == null) throw new ArgumentNullException(nameof(documents));\n+\n+            var compressed = new List<Document<T>>();\n+\n+            foreach (var doc in documents)\n+            {\n+                var compressedContent = CompressText(query, doc.Content);\n+                var compressedDoc = new Document<T>\n+                {\n+                    Id = doc.Id,\n+                    Content = compressedContent,\n+                    Metadata = doc.Metadata,\n+                    RelevanceScore = doc.RelevanceScore,\n+                    HasRelevanceScore = doc.HasRelevanceScore\n+                };\n+                compressed.Add(compressedDoc);\n+            }\n+\n+            return compressed;\n+        }",
    "path": "src/RetrievalAugmentedGeneration/ContextCompression/LLMContextCompressor.cs",
    "commit_id": "d54eddaa48a8017826434db69a3ca8defd40e974",
    "original_commit_id": "d54eddaa48a8017826434db69a3ca8defd40e974",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_≡ƒ¢á∩╕Å Refactor suggestion_ | _≡ƒƒá Major_\n\n**Eliminate duplicate compression logic.**\n\nThe `CompressCore` (lines 45-65) and `Compress` (lines 73-95) methods contain identical logic, violating DRY principles. This is the same pattern as in `DocumentSummarizer.cs`.\n\n\n\nRefactor to reuse logic:\n\n```diff\n protected override List<Document<T>> CompressCore(\n     List<Document<T>> documents,\n     string query,\n     Dictionary<string, object>? options = null)\n {\n-    var compressed = new List<Document<T>>();\n-\n-    foreach (var doc in documents)\n-    {\n-        var compressedContent = CompressText(query, doc.Content);\n-        var compressedDoc = new Document<T>(doc.Id, compressedContent)\n-        {\n-            Metadata = doc.Metadata,\n-            RelevanceScore = doc.RelevanceScore,\n-            HasRelevanceScore = doc.HasRelevanceScore\n-        };\n-        compressed.Add(compressedDoc);\n-    }\n-\n-    return compressed;\n+    return Compress(query, documents);\n }\n\n public List<Document<T>> Compress(string query, List<Document<T>> documents)\n {\n     if (string.IsNullOrEmpty(query)) throw new ArgumentNullException(nameof(query));\n     if (documents == null) throw new ArgumentNullException(nameof(documents));\n\n     var compressed = new List<Document<T>>();\n\n     foreach (var doc in documents)\n     {\n         var compressedContent = CompressText(query, doc.Content);\n-        var compressedDoc = new Document<T>\n-        {\n-            Id = doc.Id,\n-            Content = compressedContent,\n+        var compressedDoc = new Document<T>(doc.Id, compressedContent)\n+        {\n             Metadata = doc.Metadata,\n             RelevanceScore = doc.RelevanceScore,\n             HasRelevanceScore = doc.HasRelevanceScore\n         };\n         compressed.Add(compressedDoc);\n     }\n\n     return compressed;\n }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        protected override List<Document<T>> CompressCore(\n            List<Document<T>> documents,\n            string query,\n            Dictionary<string, object>? options = null)\n        {\n            return Compress(query, documents);\n        }\n\n        /// <summary>\n        /// Compresses a list of documents while preserving relevance to the query.\n        /// </summary>\n        /// <param name=\"query\">The query context.</param>\n        /// <param name=\"documents\">The documents to compress.</param>\n        /// <returns>A list of compressed documents.</returns>\n        public List<Document<T>> Compress(string query, List<Document<T>> documents)\n        {\n            if (string.IsNullOrEmpty(query)) throw new ArgumentNullException(nameof(query));\n            if (documents == null) throw new ArgumentNullException(nameof(documents));\n\n            var compressed = new List<Document<T>>();\n\n            foreach (var doc in documents)\n            {\n                var compressedContent = CompressText(query, doc.Content);\n                var compressedDoc = new Document<T>(doc.Id, compressedContent)\n                {\n                    Metadata = doc.Metadata,\n                    RelevanceScore = doc.RelevanceScore,\n                    HasRelevanceScore = doc.HasRelevanceScore\n                };\n                compressed.Add(compressedDoc);\n            }\n\n            return compressed;\n        }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T16:02:09Z",
    "updated_at": "2025-11-04T16:02:11Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491104427",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491104427"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491104427"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491104427/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 45,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 95,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 95,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491163318",
    "pull_request_review_id": 3417383784,
    "id": 2491163318,
    "node_id": "PRRC_kwDOKSXUF86UfCK2",
    "diff_hunk": "@@ -0,0 +1,137 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ContextCompression\n+{\n+    /// <summary>\n+    /// LLM-based context compression to reduce token usage while preserving key information.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class LLMContextCompressor<T> : ContextCompressorBase<T>\n+    {\n+        private readonly string _llmEndpoint;\n+        private readonly string _apiKey;\n+        private readonly double _compressionRatio;\n+\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"LLMContextCompressor{T}\"/> class.\n+        /// </summary>\n+        /// <param name=\"compressionRatio\">The target compression ratio (0.0 to 1.0).</param>\n+        /// <param name=\"llmEndpoint\">The LLM API endpoint.</param>\n+        /// <param name=\"apiKey\">The API key for the LLM service.</param>\n+        public LLMContextCompressor(\n+            double compressionRatio = 0.5,\n+            string llmEndpoint = \"\",\n+            string apiKey = \"\")\n+        {\n+            _compressionRatio = compressionRatio >= 0 && compressionRatio <= 1\n+                ? compressionRatio\n+                : throw new ArgumentOutOfRangeException(nameof(compressionRatio), \"Compression ratio must be between 0 and 1\");\n+            _llmEndpoint = llmEndpoint;\n+            _apiKey = apiKey;\n+        }\n+\n+        /// <summary>\n+        /// Compresses documents while preserving relevance to the query.\n+        /// </summary>\n+        protected override List<Document<T>> CompressCore(\n+            List<Document<T>> documents,\n+            string query,\n+            Dictionary<string, object>? options = null)\n+        {\n+            var compressed = new List<Document<T>>();\n+\n+            foreach (var doc in documents)\n+            {\n+                var compressedContent = CompressText(query, doc.Content);\n+                var compressedDoc = new Document<T>(doc.Id, compressedContent)\n+                {\n+                    Metadata = doc.Metadata,\n+                    RelevanceScore = doc.RelevanceScore,\n+                    HasRelevanceScore = doc.HasRelevanceScore\n+                };\n+                compressed.Add(compressedDoc);\n+            }\n+\n+            return compressed;\n+        }\n+\n+        /// <summary>\n+        /// Compresses text based on relevance to the query.\n+        /// </summary>\n+        /// <param name=\"query\">The query context.</param>\n+        /// <param name=\"text\">The text to compress.</param>\n+        /// <returns>The compressed text.</returns>\n+        public string CompressText(string query, string text)\n+        {\n+            if (string.IsNullOrEmpty(text)) return text;\n+\n+            var sentences = SplitIntoSentences(text);\n+            var scoredSentences = new List<(string sentence, double score)>();\n+\n+            foreach (var sentence in sentences)\n+            {\n+                var relevance = ComputeRelevance(query, sentence);\n+                scoredSentences.Add((sentence, relevance));\n+            }\n+\n+            var targetCount = Math.Max(1, (int)(sentences.Count * _compressionRatio));\n+            var topSentences = scoredSentences\n+                .OrderByDescending(x => x.score)\n+                .Take(targetCount)\n+                .OrderBy(x => sentences.IndexOf(x.sentence))\n+                .Select(x => x.sentence);\n+\n+            return string.Join(\" \", topSentences);\n+        }",
    "path": "src/RetrievalAugmentedGeneration/ContextCompression/LLMContextCompressor.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "72f2fd674bf9265ffd09a3923f36851f474d2c8c",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Optimize sentence reordering to avoid O(n┬▓) complexity.**\n\nLine 85 calls `sentences.IndexOf(x.sentence)` for each selected sentence, resulting in O(n┬▓) time complexity. For large documents, this could be slow.\n\nApply this diff to use index-based lookup:\n\n```diff\n public string CompressText(string query, string text)\n {\n     if (string.IsNullOrEmpty(text)) return text;\n\n     var sentences = SplitIntoSentences(text);\n-    var scoredSentences = new List<(string sentence, double score)>();\n+    var scoredSentences = new List<(string sentence, double score, int index)>();\n\n-    foreach (var sentence in sentences)\n+    for (int i = 0; i < sentences.Count; i++)\n     {\n-        var relevance = ComputeRelevance(query, sentence);\n-        scoredSentences.Add((sentence, relevance));\n+        var relevance = ComputeRelevance(query, sentences[i]);\n+        scoredSentences.Add((sentences[i], relevance, i));\n     }\n\n     var targetCount = Math.Max(1, (int)(sentences.Count * _compressionRatio));\n     var topSentences = scoredSentences\n         .OrderByDescending(x => x.score)\n         .Take(targetCount)\n-        .OrderBy(x => sentences.IndexOf(x.sentence))\n+        .OrderBy(x => x.index)\n         .Select(x => x.sentence);\n\n     return string.Join(\" \", topSentences);\n }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        public string CompressText(string query, string text)\n        {\n            if (string.IsNullOrEmpty(text)) return text;\n\n            var sentences = SplitIntoSentences(text);\n            var scoredSentences = new List<(string sentence, double score, int index)>();\n\n            for (int i = 0; i < sentences.Count; i++)\n            {\n                var relevance = ComputeRelevance(query, sentences[i]);\n                scoredSentences.Add((sentences[i], relevance, i));\n            }\n\n            var targetCount = Math.Max(1, (int)(sentences.Count * _compressionRatio));\n            var topSentences = scoredSentences\n                .OrderByDescending(x => x.score)\n                .Take(targetCount)\n                .OrderBy(x => x.index)\n                .Select(x => x.sentence);\n\n            return string.Join(\" \", topSentences);\n        }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/ContextCompression/LLMContextCompressor.cs\naround lines 68 to 89, the code calls sentences.IndexOf(x.sentence) for each\nselected sentence causing O(n┬▓) behavior; replace that with index-aware data so\nlookups are O(1). When building scoredSentences, include the original index\n(e.g., enumerate sentences with their index or build a dictionary mapping\nsentence->index), store (sentence, score, index), then OrderByDescending by\nscore, Take targetCount, and finally OrderBy by the stored index to restore\noriginal order before joining; this removes the repeated IndexOf calls and\nreduces complexity to O(n log n).\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T16:17:56Z",
    "updated_at": "2025-11-04T16:17:57Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491163318",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491163318"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491163318"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491163318/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 68,
    "original_start_line": 68,
    "start_side": "RIGHT",
    "line": 89,
    "original_line": 89,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 89,
    "position": 89,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491225665",
    "pull_request_review_id": 3417481514,
    "id": 2491225665,
    "node_id": "PRRC_kwDOKSXUF86UfRZB",
    "diff_hunk": "@@ -0,0 +1,206 @@\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Generators;\n+\n+/// <summary>\n+/// Base class for generator implementations providing common functionality and validation.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for relevance scoring.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// This base class provides standard validation, prompt construction, and citation handling\n+/// for generator implementations. It defines the template for implementing custom generation logic.\n+/// </para>\n+/// <para><b>For Beginners:</b> This is the foundation for all text generators in RAG systems.\n+/// \n+/// It handles common tasks so you don't have to repeat them:\n+/// - Checking that inputs aren't null or empty\n+/// - Building prompts that combine the query and retrieved documents\n+/// - Extracting citations from generated text\n+/// - Creating properly formatted answers\n+/// \n+/// When you create a new generator (like OpenAIGenerator or OnnxGenerator):\n+/// 1. Inherit from this class\n+/// 2. Set MaxContextTokens and MaxGenerationTokens in the constructor\n+/// 3. Implement GenerateCore with your specific generation logic\n+/// 4. Everything else (validation, prompt formatting, citations) is handled automatically\n+/// </para>\n+/// </remarks>\n+public abstract class GeneratorBase<T> : IGenerator<T>\n+{\n+    /// <summary>\n+    /// Gets the maximum number of tokens this generator can process in a single request.\n+    /// </summary>\n+    public int MaxContextTokens { get; protected set; }\n+\n+    /// <summary>\n+    /// Gets the maximum number of tokens this generator can generate in a response.\n+    /// </summary>\n+    public int MaxGenerationTokens { get; protected set; }\n+\n+    /// <summary>\n+    /// Initializes a new instance of the GeneratorBase class.\n+    /// </summary>\n+    /// <param name=\"maxContextTokens\">The maximum context window size in tokens.</param>\n+    /// <param name=\"maxGenerationTokens\">The maximum number of tokens to generate.</param>\n+    protected GeneratorBase(int maxContextTokens, int maxGenerationTokens)\n+    {\n+        if (maxContextTokens <= 0)\n+        {\n+            throw new ArgumentException(\"Maximum context tokens must be positive.\", nameof(maxContextTokens));\n+        }\n+\n+        if (maxGenerationTokens <= 0)\n+        {\n+            throw new ArgumentException(\"Maximum generation tokens must be positive.\", nameof(maxGenerationTokens));\n+        }\n+\n+        MaxContextTokens = maxContextTokens;\n+        MaxGenerationTokens = maxGenerationTokens;\n+    }\n+\n+    /// <summary>\n+    /// Generates a text response based on a prompt with validation.\n+    /// </summary>\n+    /// <param name=\"prompt\">The input prompt or question.</param>\n+    /// <returns>The generated text response.</returns>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when prompt is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when prompt is empty or whitespace.</exception>\n+    public string Generate(string prompt)\n+    {\n+        if (prompt == null)\n+        {\n+            throw new ArgumentNullException(nameof(prompt), \"Prompt cannot be null.\");\n+        }\n+\n+        if (string.IsNullOrWhiteSpace(prompt))\n+        {\n+            throw new ArgumentException(\"Prompt cannot be empty or whitespace.\", nameof(prompt));\n+        }\n+\n+        return GenerateCore(prompt);\n+    }\n+\n+    /// <summary>\n+    /// Generates a grounded answer using provided context documents.\n+    /// </summary>\n+    /// <param name=\"query\">The user's original query or question.</param>\n+    /// <param name=\"context\">The retrieved documents providing context for the answer.</param>\n+    /// <returns>A grounded answer with the generated text, source documents, and extracted citations.</returns>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when query or context is null.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when query is empty or context has no documents.</exception>\n+    public GroundedAnswer<T> GenerateGrounded(string query, IEnumerable<Document<T>> context)\n+    {\n+        if (query == null)\n+        {\n+            throw new ArgumentNullException(nameof(query), \"Query cannot be null.\");\n+        }\n+\n+        if (string.IsNullOrWhiteSpace(query))\n+        {\n+            throw new ArgumentException(\"Query cannot be empty or whitespace.\", nameof(query));\n+        }\n+\n+        if (context == null)\n+        {\n+            throw new ArgumentNullException(nameof(context), \"Context cannot be null.\");\n+        }\n+\n+        var contextList = context.ToList();\n+        if (contextList.Count == 0)\n+        {\n+            throw new ArgumentException(\"Context must contain at least one document.\", nameof(context));\n+        }\n+\n+        // Build the prompt with context\n+        var prompt = BuildPromptWithContext(query, contextList);\n+\n+        // Generate the answer\n+        var generatedText = GenerateCore(prompt);",
    "path": "src/RetrievalAugmentedGeneration/Generators/GeneratorBase.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "351c3dea4d2ca16823303d522d76f76de8960678",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Token limits are not enforced when building prompts.**\n\nThe class defines `MaxContextTokens` (line 35) but never validates that the constructed prompt stays within this limit. When many or lengthy documents are provided in the context, the resulting prompt could exceed the model's context window, leading to runtime failures or unexpected truncation.\n\n\nConsider adding token counting and enforcement logic:\n\n```diff\n         // Build the prompt with context\n         var prompt = BuildPromptWithContext(query, contextList);\n+        \n+        // Validate prompt length doesn't exceed context window\n+        var estimatedTokens = EstimateTokenCount(prompt);\n+        if (estimatedTokens > MaxContextTokens)\n+        {\n+            throw new InvalidOperationException(\n+                $\"Constructed prompt ({estimatedTokens} tokens) exceeds maximum context tokens ({MaxContextTokens}). \" +\n+                \"Consider reducing the number of context documents or their length.\");\n+        }\n```\n\nYou'll need to add a token counting method (either exact via a tokenizer or an approximation like word count ├ù 1.3).\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T16:33:48Z",
    "updated_at": "2025-11-04T16:33:49Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491225665",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491225665"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491225665"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491225665/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 116,
    "original_start_line": 116,
    "start_side": "RIGHT",
    "line": 120,
    "original_line": 120,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 120,
    "position": 120,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491434853",
    "pull_request_review_id": 3417778725,
    "id": 2491434853,
    "node_id": "PRRC_kwDOKSXUF86UgEdl",
    "diff_hunk": "@@ -0,0 +1,92 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Text;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies\n+{\n+    /// <summary>\n+    /// Markdown-aware text splitter that respects markdown structure.\n+    /// </summary>\n+    public class MarkdownTextSplitter : ChunkingStrategyBase\n+    {\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"MarkdownTextSplitter\"/> class.\n+        /// </summary>\n+        /// <param name=\"chunkSize\">The maximum size of each chunk.</param>\n+        /// <param name=\"chunkOverlap\">The overlap between consecutive chunks.</param>\n+        public MarkdownTextSplitter(int chunkSize = 1000, int chunkOverlap = 200)\n+            : base(chunkSize, chunkOverlap)\n+        {\n+        }\n+\n+        /// <summary>\n+        /// Chunks markdown text while preserving structure.\n+        /// </summary>\n+        /// <param name=\"text\">The markdown text to chunk.</param>\n+        /// <returns>A collection of markdown chunks with positions.</returns>\n+        protected override IEnumerable<(string Chunk, int StartPosition, int EndPosition)> ChunkCore(string text)\n+        {\n+            var separators = new[] { \"\\n## \", \"\\n### \", \"\\n#### \", \"\\n\\n\", \"\\n\", \". \", \" \" };\n+            var chunks = SplitTextRecursive(text, separators);\n+            \n+            var position = 0;\n+            foreach (var chunk in chunks)\n+            {\n+                var endPos = position + chunk.Length;\n+                yield return (chunk, position, endPos);\n+                position = endPos - ChunkOverlap;\n+            }\n+        }\n+\n+        private List<string> SplitTextRecursive(string text, string[] separators)\n+        {\n+            var chunks = new List<string>();\n+\n+if (text.Length <= ChunkSize)\n+            {\n+                chunks.Add(text);\n+                return chunks;\n+            }\n+\n+\n+            var currentChunk = new StringBuilder();\n+            \n+            foreach (var separator in separators)\n+            {\n+                var splits = text.Split(new[] { separator }, StringSplitOptions.None);\n+                currentChunk.Clear();\n+\n+                foreach (var split in splits)\n+                {\n+                    if (currentChunk.Length + split.Length + separator.Length > ChunkSize)\n+                    {\n+                        if (currentChunk.Length > 0)\n+                        {\n+                            chunks.Add(currentChunk.ToString());\n+                            var overlap = Math.Min(ChunkOverlap, currentChunk.Length);\n+                            currentChunk = new StringBuilder(currentChunk.ToString(currentChunk.Length - overlap, overlap));\n+                        }\n+                    }\n+\n+                    if (currentChunk.Length > 0)\n+                    {\n+                        currentChunk.Append(separator);\n+                    }\n+                    currentChunk.Append(split);\n+                }\n+\n+                if (currentChunk.Length > 0)\n+                {\n+                    chunks.Add(currentChunk.ToString());\n+                }\n+\n+                if (chunks.Count > 1)\n+                {\n+                    return chunks;\n+                }\n+            }\n+\n+            return chunks;\n+        }",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/MarkdownTextSplitter.cs",
    "commit_id": "5389cc536ebb509f6c373d578d6191f52e74986b",
    "original_commit_id": "5389cc536ebb509f6c373d578d6191f52e74986b",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_≡ƒ¢á∩╕Å Refactor suggestion_ | _≡ƒƒá Major_\n\n**Extract duplicated SplitTextRecursive into a shared base or helper.**\n\nThis method is nearly identical to the implementations in `CodeAwareTextSplitter.cs` (lines 61-109) and `RecursiveCharacterTextSplitter.cs` (lines 47-103). Extract the common logic into `ChunkingStrategyBase` or a static helper to eliminate duplication and ensure consistent behavior across all recursive splitting strategies.\n\n\n\nApply this pattern:\n\n```diff\n-        private List<string> SplitTextRecursive(string text, string[] separators)\n-        {\n-            var chunks = new List<string>();\n-            // ... (48 lines of duplicated logic)\n-        }\n+        private List<string> SplitTextRecursive(string text, string[] separators)\n+        {\n+            return ChunkingStrategyBase.SplitTextRecursive(text, separators, ChunkSize, ChunkOverlap);\n+        }\n```\n\nThen move the implementation to `ChunkingStrategyBase` as a protected static method.\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T17:25:37Z",
    "updated_at": "2025-11-04T17:25:38Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491434853",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491434853"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491434853"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491434853/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 41,
    "original_start_line": 41,
    "start_side": "RIGHT",
    "line": 89,
    "original_line": 90,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 90,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491434865",
    "pull_request_review_id": 3417778725,
    "id": 2491434865,
    "node_id": "PRRC_kwDOKSXUF86UgEdx",
    "diff_hunk": "@@ -0,0 +1,88 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies\n+{\n+    /// <summary>\n+    /// Semantic-based text chunking that uses embeddings to group related content.",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/SemanticChunkingStrategy.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "5389cc536ebb509f6c373d578d6191f52e74986b",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Update documentation to reflect actual implementation.**\n\nThe XML documentation claims this class \"uses embeddings to group related content\" (line 9) and \"chunks text based on semantic similarity\" (line 26), but the implementation simply groups sentences by size without any embedding or similarity calculations. Either implement actual semantic analysis or update the documentation to accurately describe sentence-based size chunking.\n\n\n\nApply this diff:\n\n```diff\n     /// <summary>\n-    /// Semantic-based text chunking that uses embeddings to group related content.\n+    /// Sentence-based text chunking that groups sentences up to the maximum chunk size.\n     /// </summary>\n     public class SemanticChunkingStrategy : ChunkingStrategyBase\n     {\n         /// <summary>\n-        /// Chunks text based on semantic similarity between sentences.\n+        /// Chunks text by grouping sentences up to the size limit.\n         /// </summary>\n```\n\n\nAlso applies to: 25-26\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn\nsrc/RetrievalAugmentedGeneration/ChunkingStrategies/SemanticChunkingStrategy.cs\naround lines 8-9 and 25-26, the XML documentation falsely states the class uses\nembeddings and semantic similarity while the implementation only groups\nsentences by size; update the XML comments to accurately describe that this\nstrategy performs sentence-based size chunking (e.g., groups/accumulates\nsentences into chunks up to a max token/character size) or, if you prefer\nsemantic behavior, implement embeddings/similarity computation and\nclusteringΓÇöpick one approach and make the code and docs consistent (either\nchange the docs to describe size-based sentence chunking or add embedding\ngeneration + similarity logic and update docs accordingly).\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T17:25:37Z",
    "updated_at": "2025-11-04T17:25:38Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491434865",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491434865"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491434865"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491434865/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 8,
    "original_start_line": 8,
    "start_side": "RIGHT",
    "line": 9,
    "original_line": 9,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 9,
    "position": 9,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491434877",
    "pull_request_review_id": 3417778725,
    "id": 2491434877,
    "node_id": "PRRC_kwDOKSXUF86UgEd9",
    "diff_hunk": "@@ -0,0 +1,88 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies\n+{\n+    /// <summary>\n+    /// Semantic-based text chunking that uses embeddings to group related content.\n+    /// </summary>\n+    public class SemanticChunkingStrategy : ChunkingStrategyBase\n+    {\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"SemanticChunkingStrategy\"/> class.\n+        /// </summary>\n+        /// <param name=\"maxChunkSize\">The maximum chunk size in characters.</param>\n+        /// <param name=\"chunkOverlap\">The chunk overlap in characters.</param>\n+        public SemanticChunkingStrategy(\n+            int maxChunkSize = 1000,\n+            int chunkOverlap = 200)\n+            : base(maxChunkSize, chunkOverlap)\n+        {\n+        }\n+\n+        /// <summary>\n+        /// Chunks text based on semantic similarity between sentences.\n+        /// </summary>\n+        /// <param name=\"text\">The text to chunk.</param>\n+        /// <returns>A collection of semantically coherent chunks with positions.</returns>\n+        protected override IEnumerable<(string Chunk, int StartPosition, int EndPosition)> ChunkCore(string text)\n+        {\n+            var sentences = SplitIntoSentences(text);\n+            var currentChunk = new List<string>();\n+            var currentSize = 0;\n+            var position = 0;\n+\n+            foreach (var sentence in sentences)\n+            {\n+                if (currentSize + sentence.Length > ChunkSize && currentChunk.Count > 0)\n+                {\n+                    var chunkText = string.Join(\" \", currentChunk);\n+                    var endPos = position + chunkText.Length;\n+                    yield return (chunkText, position, endPos);\n+                    \n+                    position = endPos - ChunkOverlap;\n+                    currentChunk.Clear();\n+                    currentSize = 0;\n+                }\n+\n+                currentChunk.Add(sentence);\n+                currentSize += sentence.Length;\n+            }\n+\n+            if (currentChunk.Count > 0)\n+            {\n+                var chunkText = string.Join(\" \", currentChunk);\n+                yield return (chunkText, position, position + chunkText.Length);\n+            }\n+        }\n+\n+        private List<string> SplitIntoSentences(string text)\n+        {\n+            var sentences = new List<string>();\n+            var sentenceEndings = new[] { \". \", \"! \", \"? \", \".\\n\", \"!\\n\", \"?\\n\" };\n+            var currentSentence = new System.Text.StringBuilder();\n+\n+            for (int i = 0; i < text.Length; i++)\n+            {\n+                currentSentence.Append(text[i]);\n+\n+                var matchedEnding = sentenceEndings.FirstOrDefault(ending => currentSentence.ToString().EndsWith(ending));\n+                if (matchedEnding != null)\n+                {\n+                    sentences.Add(currentSentence.ToString().Trim());\n+                    currentSentence.Clear();\n+                }\n+            }\n+\n+            if (currentSentence.Length > 0 && !string.IsNullOrWhiteSpace(currentSentence.ToString()))\n+            {\n+                sentences.Add(currentSentence.ToString().Trim());\n+            }\n+\n+            return sentences;\n+        }",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/SemanticChunkingStrategy.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "5389cc536ebb509f6c373d578d6191f52e74986b",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_≡ƒ¢á∩╕Å Refactor suggestion_ | _≡ƒƒá Major_\n\n**Extract duplicated SplitIntoSentences into a shared helper.**\n\nThis method duplicates the implementation in `DocumentSummarizer.cs` (lines 152-176). Extract it into a shared utility class (e.g., `TextProcessingHelpers` or `StringHelper`) to eliminate duplication and ensure consistent sentence-splitting behavior across the codebase.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn\nsrc/RetrievalAugmentedGeneration/ChunkingStrategies/SemanticChunkingStrategy.cs\nlines 61-85, the SplitIntoSentences implementation is duplicated (same code\nexists in DocumentSummarizer.cs lines 152-176); extract this logic into a shared\nstatic helper (e.g., TextProcessingHelpers.SplitIntoSentences or\nStringHelper.SplitIntoSentences) in a common namespace, make the method public\nstatic and reuse it from both SemanticChunkingStrategy.cs and\nDocumentSummarizer.cs, remove the duplicated implementations, update\nusing/imports as needed, and run tests to ensure behavior remains identical.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T17:25:37Z",
    "updated_at": "2025-11-04T17:25:38Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491434877",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491434877"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491434877"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491434877/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 61,
    "original_start_line": 61,
    "start_side": "RIGHT",
    "line": 85,
    "original_line": 85,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 85,
    "position": 85,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491434886",
    "pull_request_review_id": 3417778725,
    "id": 2491434886,
    "node_id": "PRRC_kwDOKSXUF86UgEeG",
    "diff_hunk": "@@ -0,0 +1,188 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ContextCompression\n+{\n+    /// <summary>\n+    /// Document summarizer for creating concise summaries of retrieved content.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class DocumentSummarizer<T> : ContextCompressorBase<T>\n+    {\n+        private readonly int _maxSummaryLength;\n+        private readonly string _llmEndpoint;\n+        private readonly string _apiKey;",
    "path": "src/RetrievalAugmentedGeneration/ContextCompression/DocumentSummarizer.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "5389cc536ebb509f6c373d578d6191f52e74986b",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n<details>\n<summary>≡ƒº⌐ Analysis chain</summary>\n\n**LLM endpoint and API key are stored but never used.**\n\nThe `_llmEndpoint` and `_apiKey` fields are initialized in the constructor but are not referenced anywhere in the implementation. The current summarization logic is purely extractive and doesn't integrate with any LLM service.\n\n\n\n\nEither implement the LLM-based summarization feature or remove these unused fields and constructor parameters to avoid misleading the API consumers.\n\n\n\n\nAlso applies to: 29-30, 38-39\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Verify that _llmEndpoint and _apiKey are not used elsewhere in the codebase\nrg -n --type cs '_llmEndpoint|_apiKey' -g '!DocumentSummarizer.cs'\n```\n\nLength of output: 5782\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# View the full DocumentSummarizer.cs file to verify field usage\ncat src/RetrievalAugmentedGeneration/ContextCompression/DocumentSummarizer.cs\n```\n\nLength of output: 6718\n\n---\n\n\n\n</details>\n\n**Remove unused LLM fields and constructor parameters from DocumentSummarizer.**\n\nThe verification confirms that `_llmEndpoint` and `_apiKey` (lines 16-17, assigned at lines 38-39) are never referenced in the class implementation. The entire summarization logic is purely extractiveΓÇöusing sentence importance scoring without any LLM service integration. Remove these unused fields and their constructor parameters (lines 31-32) to eliminate misleading API contracts.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/ContextCompression/DocumentSummarizer.cs\naround lines 16-17 (fields) and constructor parameters at lines 31-32 with\nassignments at lines 38-39: remove the unused private fields _llmEndpoint and\n_apiKey, and delete the corresponding constructor parameters and their\nassignments; update the constructor signature to only accept parameters actually\nused by the class and remove any related XML/doc comments or tests referencing\nthose parameters to keep the API contract accurate.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T17:25:37Z",
    "updated_at": "2025-11-04T17:25:38Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491434886",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491434886"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491434886"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491434886/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 16,
    "original_start_line": 16,
    "start_side": "RIGHT",
    "line": 17,
    "original_line": 17,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 17,
    "position": 17,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491434899",
    "pull_request_review_id": 3417778725,
    "id": 2491434899,
    "node_id": "PRRC_kwDOKSXUF86UgEeT",
    "diff_hunk": "@@ -0,0 +1,137 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ContextCompression\n+{\n+    /// <summary>\n+    /// LLM-based context compression to reduce token usage while preserving key information.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class LLMContextCompressor<T> : ContextCompressorBase<T>\n+    {\n+        private readonly string _llmEndpoint;\n+        private readonly string _apiKey;\n+        private readonly double _compressionRatio;",
    "path": "src/RetrievalAugmentedGeneration/ContextCompression/LLMContextCompressor.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "5389cc536ebb509f6c373d578d6191f52e74986b",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Misleading class name: LLM fields unused (unresolved from past review).**\n\nThe `_llmEndpoint` and `_apiKey` fields are stored but never used. The class name implies LLM-based compression, yet the implementation relies solely on token overlap scoring (lines 91-100) without any LLM API calls. This is misleading to users expecting LLM integration.\n\n\n\nChoose one of two options:\n\n**Option A (Recommended):** Remove unused fields and rename to reflect actual behavior:\n\n```diff\n-    /// LLM-based context compression to reduce token usage while preserving key information.\n+    /// Token overlap-based context compression to reduce token usage while preserving key information.\n     /// </summary>\n     /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n-    public class LLMContextCompressor<T> : ContextCompressorBase<T>\n+    public class TokenOverlapCompressor<T> : ContextCompressorBase<T>\n     {\n-        private readonly string _llmEndpoint;\n-        private readonly string _apiKey;\n         private readonly double _compressionRatio;\n\n         /// <summary>\n-        /// Initializes a new instance of the <see cref=\"LLMContextCompressor{T}\"/> class.\n+        /// Initializes a new instance of the <see cref=\"TokenOverlapCompressor{T}\"/> class.\n         /// </summary>\n         /// <param name=\"compressionRatio\">The target compression ratio (0.0 to 1.0).</param>\n-        /// <param name=\"llmEndpoint\">The LLM API endpoint.</param>\n-        /// <param name=\"apiKey\">The API key for the LLM service.</param>\n-        public LLMContextCompressor(\n+        public TokenOverlapCompressor(\n             double compressionRatio = 0.5,\n-            string llmEndpoint = \"\",\n-            string apiKey = \"\")\n+        )\n         {\n             _compressionRatio = compressionRatio >= 0 && compressionRatio <= 1\n                 ? compressionRatio\n                 : throw new ArgumentOutOfRangeException(nameof(compressionRatio), \"Compression ratio must be between 0 and 1\");\n-            _llmEndpoint = llmEndpoint;\n-            _apiKey = apiKey;\n         }\n```\n\n**Option B:** Complete the LLM integration by implementing actual LLM API calls in `CompressText` or `ComputeRelevance` methods, using the stored endpoint and API key.\n\n\nAlso applies to: 25-35\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T17:25:37Z",
    "updated_at": "2025-11-04T17:25:38Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491434899",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491434899"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491434899"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491434899/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 15,
    "original_start_line": 15,
    "start_side": "RIGHT",
    "line": 17,
    "original_line": 17,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 17,
    "position": 17,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491529421",
    "pull_request_review_id": 3417918577,
    "id": 2491529421,
    "node_id": "PRRC_kwDOKSXUF86UgbjN",
    "diff_hunk": "@@ -0,0 +1,91 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Text;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies\n+{\n+    /// <summary>\n+    /// Markdown-aware text splitter that respects markdown structure.\n+    /// </summary>\n+    public class MarkdownTextSplitter : ChunkingStrategyBase\n+    {\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"MarkdownTextSplitter\"/> class.\n+        /// </summary>\n+        /// <param name=\"chunkSize\">The maximum size of each chunk.</param>\n+        /// <param name=\"chunkOverlap\">The overlap between consecutive chunks.</param>\n+        public MarkdownTextSplitter(int chunkSize = 1000, int chunkOverlap = 200)\n+            : base(chunkSize, chunkOverlap)\n+        {\n+        }\n+\n+        /// <summary>\n+        /// Chunks markdown text while preserving structure.\n+        /// </summary>\n+        /// <param name=\"text\">The markdown text to chunk.</param>\n+        /// <returns>A collection of markdown chunks with positions.</returns>\n+        protected override IEnumerable<(string Chunk, int StartPosition, int EndPosition)> ChunkCore(string text)\n+        {\n+            var separators = new[] { \"\\n## \", \"\\n### \", \"\\n#### \", \"\\n\\n\", \"\\n\", \". \", \" \" };\n+            var chunks = SplitTextRecursive(text, separators);\n+            \n+            var position = 0;\n+            foreach (var chunk in chunks)\n+            {\n+                var endPos = position + chunk.Length;\n+                yield return (chunk, position, endPos);\n+                position = endPos - ChunkOverlap;\n+            }",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/MarkdownTextSplitter.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "21d8da8115e66a3f5de77dd822141f95e95cb167",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Position tracking is incorrect for overlapping chunks.**\n\nThe position calculation assumes chunks are contiguous segments of the original text, but `SplitTextRecursive` produces chunks with artificial overlap (lines 66-67 copy the tail of one chunk into the start of the next). This means the `StartPosition` and `EndPosition` values do not accurately map to positions in the original input text.\n\nConsider either:\n1. Tracking actual positions during splitting to reflect where each chunk's content originates in the source text, or\n2. Removing overlap from the splitting logic and letting the consumer handle overlap if needed, or\n3. Documenting that positions are logical offsets in the processed chunk sequence, not the original text\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/ChunkingStrategies/MarkdownTextSplitter.cs\naround lines 32ΓÇô38, the StartPosition/EndPosition are computed assuming chunks\nare contiguous, but SplitTextRecursive creates overlapping chunks so those\npositions are incorrect; fix by changing the splitter to track and return each\nchunk's actual start index in the original input (propagate an index parameter\nthrough SplitTextRecursive so when you create a chunk you record its original\nstart and end), then yield (chunk, originalStart, originalEnd) here instead of\nderiving positions from the streaming position; alternatively, if you\nintentionally want logical positions, update the API/docs and remove misleading\nStart/End fields or remove overlap at split time so positions match contiguous\noffsets.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T17:46:17Z",
    "updated_at": "2025-11-04T17:46:19Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491529421",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491529421"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491529421"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491529421/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 32,
    "original_start_line": 32,
    "start_side": "RIGHT",
    "line": 38,
    "original_line": 38,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 38,
    "position": 38,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491529440",
    "pull_request_review_id": 3417918577,
    "id": 2491529440,
    "node_id": "PRRC_kwDOKSXUF86Ugbjg",
    "diff_hunk": "@@ -0,0 +1,91 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Text;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies\n+{\n+    /// <summary>\n+    /// Markdown-aware text splitter that respects markdown structure.\n+    /// </summary>\n+    public class MarkdownTextSplitter : ChunkingStrategyBase\n+    {\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"MarkdownTextSplitter\"/> class.\n+        /// </summary>\n+        /// <param name=\"chunkSize\">The maximum size of each chunk.</param>\n+        /// <param name=\"chunkOverlap\">The overlap between consecutive chunks.</param>\n+        public MarkdownTextSplitter(int chunkSize = 1000, int chunkOverlap = 200)\n+            : base(chunkSize, chunkOverlap)\n+        {\n+        }\n+\n+        /// <summary>\n+        /// Chunks markdown text while preserving structure.\n+        /// </summary>\n+        /// <param name=\"text\">The markdown text to chunk.</param>\n+        /// <returns>A collection of markdown chunks with positions.</returns>\n+        protected override IEnumerable<(string Chunk, int StartPosition, int EndPosition)> ChunkCore(string text)\n+        {\n+            var separators = new[] { \"\\n## \", \"\\n### \", \"\\n#### \", \"\\n\\n\", \"\\n\", \". \", \" \" };\n+            var chunks = SplitTextRecursive(text, separators);\n+            \n+            var position = 0;\n+            foreach (var chunk in chunks)\n+            {\n+                var endPos = position + chunk.Length;\n+                yield return (chunk, position, endPos);\n+                position = endPos - ChunkOverlap;\n+            }\n+        }\n+\n+        private List<string> SplitTextRecursive(string text, string[] separators)\n+        {\n+            var chunks = new List<string>();\n+\n+if (text.Length <= ChunkSize)",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/MarkdownTextSplitter.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "21d8da8115e66a3f5de77dd822141f95e95cb167",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Fix indentation.**\n\nLine 45 has inconsistent indentation (missing leading whitespace).\n\n\nApply this diff:\n\n```diff\n-if (text.Length <= ChunkSize)\n+            if (text.Length <= ChunkSize)\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n            if (text.Length <= ChunkSize)\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/ChunkingStrategies/MarkdownTextSplitter.cs\naround line 45, the if statement has inconsistent indentation (missing leading\nwhitespace); fix it by adding the same indentation used by the surrounding\nmethod/block (use the file's existing tab/space style) so the line aligns with\nthe other statements in the block.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T17:46:17Z",
    "updated_at": "2025-11-04T17:46:19Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491529440",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491529440"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491529440"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491529440/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": 45,
    "original_line": 45,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 45,
    "position": 45,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491529463",
    "pull_request_review_id": 3417918577,
    "id": 2491529463,
    "node_id": "PRRC_kwDOKSXUF86Ugbj3",
    "diff_hunk": "@@ -0,0 +1,105 @@\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using System;\n+using System.Collections.Generic;\n+using System.Text;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies\n+{\n+    /// <summary>\n+    /// Recursive character-based text splitting that preserves semantic meaning.\n+    /// </summary>\n+    public class RecursiveCharacterTextSplitter : ChunkingStrategyBase\n+    {\n+        private readonly string[] _separators;\n+\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"RecursiveCharacterTextSplitter\"/> class.\n+        /// </summary>\n+        /// <param name=\"chunkSize\">The maximum size of each chunk.</param>\n+        /// <param name=\"chunkOverlap\">The overlap between consecutive chunks.</param>\n+        /// <param name=\"separators\">The separators to use for splitting, in order of preference.</param>\n+        public RecursiveCharacterTextSplitter(\n+            int chunkSize = 1000,\n+            int chunkOverlap = 200,\n+            string[]? separators = null)\n+            : base(chunkSize, chunkOverlap)\n+        {\n+            _separators = separators ?? new[] { \"\\n\\n\", \"\\n\", \". \", \" \", \"\" };\n+        }\n+\n+        /// <summary>\n+        /// Splits the input text into chunks recursively.\n+        /// </summary>\n+        /// <param name=\"text\">The text to split.</param>\n+        /// <returns>A collection of text chunks with positions.</returns>\n+        protected override IEnumerable<(string Chunk, int StartPosition, int EndPosition)> ChunkCore(string text)\n+        {\n+            var chunks = SplitTextRecursive(text, _separators);\n+            \n+            var position = 0;\n+            foreach (var chunk in chunks)\n+            {\n+                var endPos = position + chunk.Length;\n+                yield return (chunk, position, endPos);\n+                position = endPos - ChunkOverlap;\n+            }",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/RecursiveCharacterTextSplitter.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "21d8da8115e66a3f5de77dd822141f95e95cb167",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Position tracking is incorrect for overlapping chunks.**\n\nSame issue as in MarkdownTextSplitter: the position calculation assumes chunks are contiguous segments of the original text, but `SplitTextRecursive` produces chunks with artificial overlap (lines 80-81 copy the tail of one chunk into the start of the next). The `StartPosition` and `EndPosition` values do not accurately represent positions in the original input text.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T17:46:18Z",
    "updated_at": "2025-11-04T17:46:19Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491529463",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491529463"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491529463"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491529463/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 39,
    "original_start_line": 39,
    "start_side": "RIGHT",
    "line": 45,
    "original_line": 45,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 45,
    "position": 45,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491529474",
    "pull_request_review_id": 3417918577,
    "id": 2491529474,
    "node_id": "PRRC_kwDOKSXUF86UgbkC",
    "diff_hunk": "@@ -0,0 +1,105 @@\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using System;\n+using System.Collections.Generic;\n+using System.Text;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies\n+{\n+    /// <summary>\n+    /// Recursive character-based text splitting that preserves semantic meaning.\n+    /// </summary>\n+    public class RecursiveCharacterTextSplitter : ChunkingStrategyBase\n+    {\n+        private readonly string[] _separators;\n+\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"RecursiveCharacterTextSplitter\"/> class.\n+        /// </summary>\n+        /// <param name=\"chunkSize\">The maximum size of each chunk.</param>\n+        /// <param name=\"chunkOverlap\">The overlap between consecutive chunks.</param>\n+        /// <param name=\"separators\">The separators to use for splitting, in order of preference.</param>\n+        public RecursiveCharacterTextSplitter(\n+            int chunkSize = 1000,\n+            int chunkOverlap = 200,\n+            string[]? separators = null)\n+            : base(chunkSize, chunkOverlap)\n+        {\n+            _separators = separators ?? new[] { \"\\n\\n\", \"\\n\", \". \", \" \", \"\" };\n+        }\n+\n+        /// <summary>\n+        /// Splits the input text into chunks recursively.\n+        /// </summary>\n+        /// <param name=\"text\">The text to split.</param>\n+        /// <returns>A collection of text chunks with positions.</returns>\n+        protected override IEnumerable<(string Chunk, int StartPosition, int EndPosition)> ChunkCore(string text)\n+        {\n+            var chunks = SplitTextRecursive(text, _separators);\n+            \n+            var position = 0;\n+            foreach (var chunk in chunks)\n+            {\n+                var endPos = position + chunk.Length;\n+                yield return (chunk, position, endPos);\n+                position = endPos - ChunkOverlap;\n+            }\n+        }\n+\n+        private List<string> SplitTextRecursive(string text, string[] separators)\n+        {\n+            var chunks = new List<string>();\n+\n+            if (text.Length <= ChunkSize)\n+            {\n+                chunks.Add(text);\n+                return chunks;\n+            }\n+\n+            foreach (var separator in separators)\n+            {\n+                if (string.IsNullOrEmpty(separator))\n+                {\n+                    for (int i = 0; i < text.Length; i += ChunkSize - ChunkOverlap)\n+                    {\n+                        var length = Math.Min(ChunkSize, text.Length - i);\n+                        chunks.Add(text.Substring(i, length));\n+                    }\n+                    return chunks;\n+                }\n+\n+                var splits = text.Split(new[] { separator }, StringSplitOptions.None);\n+                var currentChunk = new StringBuilder();\n+\n+                foreach (var split in splits)\n+                {\n+                    if (currentChunk.Length + split.Length + separator.Length > ChunkSize)\n+                    {\n+                        if (currentChunk.Length > 0)\n+                        {\n+                            chunks.Add(currentChunk.ToString());\n+                            var overlap = Math.Min(ChunkOverlap, currentChunk.Length);\n+                            currentChunk = new StringBuilder(currentChunk.ToString(currentChunk.Length - overlap, overlap));\n+                        }\n+                    }\n+\n+                    if (currentChunk.Length > 0)\n+                        currentChunk.Append(separator);\n+                    \n+                    currentChunk.Append(split);\n+                }",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/RecursiveCharacterTextSplitter.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "21d8da8115e66a3f5de77dd822141f95e95cb167",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n<details>\n<summary>≡ƒº⌐ Analysis chain</summary>\n\n**Handle case where a single split exceeds ChunkSize.**\n\nIf a split segment is longer than `ChunkSize` and `currentChunk` is empty (lines 75-83), the segment is skipped entirely and never added to any chunk. This could result in data loss for large unsplittable segments.\n\n\n\n\nRun this script to check if there are tests covering this edge case:\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Description: Search for tests that verify handling of segments larger than ChunkSize\n\n# Search for test methods related to RecursiveCharacterTextSplitter\nrg -n -A 10 \"RecursiveCharacterTextSplitter\" --type=cs -g '*Test*.cs'\n\n# Search for test cases with large text segments\nrg -n \"ChunkSize.*split.*exceed|exceed.*ChunkSize|segment.*larger.*chunk\" --type=cs -g '*Test*.cs'\n```\n\nLength of output: 0\n\n---\n\nI need to gather more information about the codebase structure and examine the complete implementation. Let me search more broadly.\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Description: Find test files and examine the complete RecursiveCharacterTextSplitter implementation\n\n# Find all test-related C# files with various naming patterns\nfind . -type f -name \"*.cs\" | grep -iE \"(test|spec)\" | head -20\n\n# Examine the complete RecursiveCharacterTextSplitter.cs file\nfind . -type f -name \"RecursiveCharacterTextSplitter.cs\" -exec cat {} \\;\n```\n\nLength of output: 4733\n\n---\n\nNow let me search for test coverage of this chunking strategy:\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Description: Search for tests of RecursiveCharacterTextSplitter\n\n# Find test files and search for RecursiveCharacterTextSplitter tests\nfind ./tests -type f -name \"*.cs\" 2>/dev/null | head -20\n\n# Search for RecursiveCharacterTextSplitter in all test files\nrg -n \"RecursiveCharacterTextSplitter\" --type=cs\n```\n\nLength of output: 1553\n\n---\n\n\n\n</details>\n\n**Review comment characterization is incorrect; reanalyze the actual constraint violation.**\n\nThe original claim that segments are \"skipped entirely and never added to any chunk\" is factually incorrect. Code analysis shows the split is appended to `currentChunk` at line 82 and subsequently added to chunks after the loop (line 88). \n\nHowever, there is a legitimate issue: when a single split segment exceeds `ChunkSize`, it is added to chunks without further subdivision, violating the `ChunkSize` constraint. This differs from \"data loss\"ΓÇöthe segment is retained but oversized. The code has proper fallback logic for the empty separator case (using `Math.Min` to chunk large segments), but this safety net is absent for non-empty separators. No existing tests cover this edge case for `RecursiveCharacterTextSplitter`.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T17:46:18Z",
    "updated_at": "2025-11-04T17:46:19Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491529474",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491529474"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491529474"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491529474/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 73,
    "original_start_line": 73,
    "start_side": "RIGHT",
    "line": 89,
    "original_line": 89,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 89,
    "position": 89,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491529486",
    "pull_request_review_id": 3417918577,
    "id": 2491529486,
    "node_id": "PRRC_kwDOKSXUF86UgbkO",
    "diff_hunk": "@@ -0,0 +1,86 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\n+{\n+    /// <summary>\n+    /// Hybrid retriever combining dense and sparse retrieval strategies.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class HybridRetriever<T> : RetrieverBase<T>\n+    {\n+        private readonly IRetriever<T> _denseRetriever;\n+        private readonly IRetriever<T> _sparseRetriever;\n+        private readonly T _denseWeight;\n+        private readonly T _sparseWeight;\n+\n+        public HybridRetriever(\n+            IRetriever<T> denseRetriever, \n+            IRetriever<T> sparseRetriever, \n+            double denseWeight = 0.7, \n+            double sparseWeight = 0.3,\n+            int defaultTopK = 5) \n+            : base(defaultTopK)\n+        {\n+            if (denseRetriever == null)\n+                throw new ArgumentNullException(nameof(denseRetriever));\n+            if (sparseRetriever == null)\n+                throw new ArgumentNullException(nameof(sparseRetriever));\n+\n+            _denseRetriever = denseRetriever;\n+            _sparseRetriever = sparseRetriever;\n+            _denseWeight = NumOps.FromDouble(denseWeight);\n+            _sparseWeight = NumOps.FromDouble(sparseWeight);\n+        }",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/HybridRetriever.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "21d8da8115e66a3f5de77dd822141f95e95cb167",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Validate weight parameters before converting them**  \nConstructor currently accepts negative weights or a total weight of zero, which silently produces inverted or all-zero relevance scores. For example, `denseWeight = -0.4` yields negative relevance, while both weights set to `0` produce a blank score map even when the underlying retrievers return hits. Please reject invalid combinations up front (nonΓÇænegative weights with at least one strictly positive and optionally normalizing if required) before calling `NumOps.FromDouble(...)`.  \n\n\n```diff\n         _denseRetriever = denseRetriever;\n         _sparseRetriever = sparseRetriever;\n+\n+        if (denseWeight < 0)\n+            throw new ArgumentOutOfRangeException(nameof(denseWeight), \"denseWeight must be non-negative.\");\n+        if (sparseWeight < 0)\n+            throw new ArgumentOutOfRangeException(nameof(sparseWeight), \"sparseWeight must be non-negative.\");\n+        if (denseWeight + sparseWeight <= 0)\n+            throw new ArgumentException(\"At least one weight must be greater than zero.\", nameof(denseWeight));\n+\n         _denseWeight = NumOps.FromDouble(denseWeight);\n         _sparseWeight = NumOps.FromDouble(sparseWeight);\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        public HybridRetriever(\n            IRetriever<T> denseRetriever, \n            IRetriever<T> sparseRetriever, \n            double denseWeight = 0.7, \n            double sparseWeight = 0.3,\n            int defaultTopK = 5) \n            : base(defaultTopK)\n        {\n            if (denseRetriever == null)\n                throw new ArgumentNullException(nameof(denseRetriever));\n            if (sparseRetriever == null)\n                throw new ArgumentNullException(nameof(sparseRetriever));\n\n            _denseRetriever = denseRetriever;\n            _sparseRetriever = sparseRetriever;\n\n            if (denseWeight < 0)\n                throw new ArgumentOutOfRangeException(nameof(denseWeight), \"denseWeight must be non-negative.\");\n            if (sparseWeight < 0)\n                throw new ArgumentOutOfRangeException(nameof(sparseWeight), \"sparseWeight must be non-negative.\");\n            if (denseWeight + sparseWeight <= 0)\n                throw new ArgumentException(\"At least one weight must be greater than zero.\", nameof(denseWeight));\n\n            _denseWeight = NumOps.FromDouble(denseWeight);\n            _sparseWeight = NumOps.FromDouble(sparseWeight);\n        }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Retrievers/HybridRetriever.cs around lines\n21 to 38, the constructor currently converts any denseWeight/sparseWeight to\nNumOps without validation; validate inputs first by ensuring both weights are\nnonΓÇænegative and at least one is strictly greater than zero (throw\nArgumentException/ArgumentOutOfRangeException for invalid values), optionally\nnormalize the two weights so they sum to 1 (e.g., divide each by (denseWeight +\nsparseWeight) when sum > 0), then call NumOps.FromDouble(...) with the\nvalidated/normalized values and assign to _denseWeight/_sparseWeight.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-04T17:46:18Z",
    "updated_at": "2025-11-04T17:46:19Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491529486",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491529486"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2491529486"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2491529486/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 21,
    "original_start_line": 21,
    "start_side": "RIGHT",
    "line": 38,
    "original_line": 38,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 38,
    "position": 38,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669615",
    "pull_request_review_id": 3419457803,
    "id": 2492669615,
    "node_id": "PRRC_kwDOKSXUF86Ukx6v",
    "diff_hunk": "@@ -0,0 +1,91 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// Lightweight SQLite-based vector store using the SQLite-VSS extension.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// SQLite-VSS provides vector similarity search in a serverless, file-based database,\n+/// ideal for development, testing, and edge deployments.\n+/// </remarks>\n+public class SQLiteVSSDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly Dictionary<string, VectorDocument<T>> _store;\n+    private int _vectorDimension;\n+\n+    public override int DocumentCount => _store.Count;\n+    public override int VectorDimension => _vectorDimension;\n+\n+    public SQLiteVSSDocumentStore(string databasePath, string tableName, int vectorDimension)\n+    {\n+        if (string.IsNullOrWhiteSpace(databasePath))\n+            throw new ArgumentException(\"Database path cannot be empty\", nameof(databasePath));\n+        if (string.IsNullOrWhiteSpace(tableName))\n+            throw new ArgumentException(\"Table name cannot be empty\", nameof(tableName));\n+        if (vectorDimension <= 0)\n+            throw new ArgumentException(\"Vector dimension must be positive\", nameof(vectorDimension));\n+\n+        _store = new Dictionary<string, VectorDocument<T>>();\n+        _vectorDimension = vectorDimension;\n+    }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/SQLiteVSSDocumentStore.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Constructor parameters `databasePath` and `tableName` are validated but never used.**\n\nThe constructor accepts and validates `databasePath` and `tableName` (lines 29-32), suggesting they configure the SQLite database location, but they are never stored or used. This misleading API creates false expectations about persistence configuration.\n\nAdditionally, the `vectorDimension` parameter is set at line 37 but is immediately overridden by the first document added (lines 42-43 in `AddCore`, lines 52-53 in `AddBatchCore`), making the constructor parameter effectively ignored.\n\n\n\n**Fix:**\n\nEither remove the unused parameters or store them for future SQLite integration. If the in-memory implementation is intentional, update the constructor signature to match the actual behavior:\n\n```diff\n-public SQLiteVSSDocumentStore(string databasePath, string tableName, int vectorDimension)\n+public SQLiteVSSDocumentStore()\n {\n-    if (string.IsNullOrWhiteSpace(databasePath))\n-        throw new ArgumentException(\"Database path cannot be empty\", nameof(databasePath));\n-    if (string.IsNullOrWhiteSpace(tableName))\n-        throw new ArgumentException(\"Table name cannot be empty\", nameof(tableName));\n-    if (vectorDimension <= 0)\n-        throw new ArgumentException(\"Vector dimension must be positive\", nameof(vectorDimension));\n-\n     _store = new Dictionary<string, VectorDocument<T>>();\n-    _vectorDimension = vectorDimension;\n+    _vectorDimension = 0;\n }\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/SQLiteVSSDocumentStore.cs\naround lines 27 to 38, the constructor validates databasePath and tableName but\nnever stores or uses them, and the vectorDimension parameter is set but later\noverridden by first added document; either remove the unused databasePath and\ntableName parameters from the constructor (and related callers) to reflect the\ncurrent in-memory implementation, or persist them to private readonly fields\n(e.g., _databasePath, _tableName) for future SQLite integration and ensure they\nare used where appropriate; additionally, make vectorDimension authoritative by\nassigning it to _vectorDimension and prevent AddCore/AddBatchCore from\noverwriting it (only set _vectorDimension if it is unset/null/0, or remove that\nfallback logic), and update any XML/comments and unit tests to match the chosen\napproach.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:49:50Z",
    "updated_at": "2025-11-05T02:49:57Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669615",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669615"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669615"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669615/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 79,
    "original_start_line": 27,
    "start_side": "RIGHT",
    "line": 90,
    "original_line": 38,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 38,
    "position": 90,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669628",
    "pull_request_review_id": 3419457803,
    "id": 2492669628,
    "node_id": "PRRC_kwDOKSXUF86Ukx68",
    "diff_hunk": "@@ -0,0 +1,88 @@\n+global using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// Weaviate-inspired document store with class-based schema organization.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+public class WeaviateDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly Dictionary<string, VectorDocument<T>> _documents;\n+    private readonly string _className;",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/WeaviateDocumentStore.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**The `_className` field is never used.**\n\nThe constructor accepts and validates a `className` parameter, storing it in the `_className` field. However, this field is never referenced anywhere in the implementation. The XML documentation mentions \"class-based schema organization,\" but no schema functionality is implemented.\n\n\n\nConsider either:\n- Implementing the class-based schema organization if planned\n- Removing the unused field and parameter to simplify the API\n- Adding a public property to expose the class name for future use\n\n```diff\n-    private readonly string _className;\n-\n     public override int DocumentCount => _documents.Count;\n     public override int VectorDimension => _vectorDimension;\n\n-    public WeaviateDocumentStore(string className, int initialCapacity = 1000)\n+    public WeaviateDocumentStore(int initialCapacity = 1000)\n     {\n-        if (string.IsNullOrWhiteSpace(className))\n-            throw new ArgumentException(\"Class name cannot be empty\", nameof(className));\n         if (initialCapacity <= 0)\n             throw new ArgumentException(\"Initial capacity must be greater than zero\", nameof(initialCapacity));\n\n-        _className = className;\n         _documents = new Dictionary<string, VectorDocument<T>>(initialCapacity);\n```\n\n\nAlso applies to: 18-28\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/WeaviateDocumentStore.cs\naround lines 11-12 (and related constructor code at 18-28), the private field\n_className is assigned but never used; either remove the unused field and its\nconstructor parameter/validation and update XML docs and any callers to simplify\nthe API, or expose it as a public read-only property and keep the constructor\nparameter so callers can read the class name (and update XML docs to reflect the\nproperty); implement the chosen change consistently (remove all mentions/usages\nof _className and its parameter when removing, or add a public getter and\nreplace any direct field references with the property if exposing).\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:49:50Z",
    "updated_at": "2025-11-05T02:49:58Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669628",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669628"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669628"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669628/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 35,
    "original_start_line": 11,
    "start_side": "RIGHT",
    "line": 36,
    "original_line": 12,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 12,
    "position": 36,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669636",
    "pull_request_review_id": 3419457803,
    "id": 2492669636,
    "node_id": "PRRC_kwDOKSXUF86Ukx7E",
    "diff_hunk": "@@ -0,0 +1,88 @@\n+global using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// Weaviate-inspired document store with class-based schema organization.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+public class WeaviateDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly Dictionary<string, VectorDocument<T>> _documents;\n+    private readonly string _className;\n+    private int _vectorDimension;\n+\n+    public override int DocumentCount => _documents.Count;\n+    public override int VectorDimension => _vectorDimension;\n+\n+    public WeaviateDocumentStore(string className, int initialCapacity = 1000)\n+    {\n+        if (string.IsNullOrWhiteSpace(className))\n+            throw new ArgumentException(\"Class name cannot be empty\", nameof(className));\n+        if (initialCapacity <= 0)\n+            throw new ArgumentException(\"Initial capacity must be greater than zero\", nameof(initialCapacity));\n+\n+        _className = className;\n+        _documents = new Dictionary<string, VectorDocument<T>>(initialCapacity);\n+        _vectorDimension = 0;\n+    }\n+\n+    protected override void AddCore(VectorDocument<T> vectorDocument)\n+    {\n+        if (_documents.Count == 0)\n+        {\n+            _vectorDimension = vectorDocument.Embedding.Length;\n+        }\n+\n+        _documents[vectorDocument.Document.Id] = vectorDocument;\n+    }\n+\n+    protected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)\n+    {\n+        var scoredDocuments = new List<(Document<T> Document, T Score)>();\n+\n+        var matchingDocuments = _documents.Values\n+            .Where(vectorDoc => MatchesFilters(vectorDoc.Document, metadataFilters));\n+\n+        foreach (var vectorDoc in matchingDocuments)\n+        {\n+            var similarity = StatisticsHelper<T>.CosineSimilarity(queryVector, vectorDoc.Embedding);\n+            scoredDocuments.Add((vectorDoc.Document, similarity));\n+        }\n+\n+        var results = scoredDocuments\n+            .OrderByDescending(x => x.Score)\n+            .Take(topK)\n+            .Select(x =>\n+            {\n+                x.Document.RelevanceScore = x.Score;\n+                x.Document.HasRelevanceScore = true;\n+                return x.Document;\n+            })\n+            .ToList();\n+\n+        return results;\n+    }\n+\n+    protected override Document<T>? GetByIdCore(string documentId)\n+    {\n+        return _documents.TryGetValue(documentId, out var vectorDoc) ? vectorDoc.Document : null;\n+    }\n+\n+    protected override bool RemoveCore(string documentId)\n+    {\n+        var removed = _documents.Remove(documentId);\n+        if (removed && _documents.Count == 0)\n+        {\n+            _vectorDimension = 0;\n+        }\n+        return removed;\n+    }\n+\n+    public override void Clear()\n+    {\n+        _documents.Clear();\n+        _vectorDimension = 0;\n+    }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/WeaviateDocumentStore.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_≡ƒ¢á∩╕Å Refactor suggestion_ | _≡ƒƒá Major_\n\n**Eliminate code duplication with PineconeDocumentStore.**\n\nThe entire implementation of `WeaviateDocumentStore<T>` is identical to `PineconeDocumentStore<T>` (except for the unused `_className` field). All five methodsΓÇö`AddCore`, `GetSimilarCore`, `GetByIdCore`, `RemoveCore`, and `Clear`ΓÇöshare the same logic, which violates DRY principles.\n\n\n\nConsider consolidating these implementations:\n\n**Option 1 (Recommended):** Create a shared `InMemoryDocumentStore<T>` base class:\n\n```csharp\npublic class InMemoryDocumentStore<T> : DocumentStoreBase<T>\n{\n    protected readonly Dictionary<string, VectorDocument<T>> _documents;\n    protected int _vectorDimension;\n    \n    public override int DocumentCount => _documents.Count;\n    public override int VectorDimension => _vectorDimension;\n    \n    public InMemoryDocumentStore(int initialCapacity = 1000)\n    {\n        if (initialCapacity <= 0)\n            throw new ArgumentException(\"Initial capacity must be greater than zero\", nameof(initialCapacity));\n        _documents = new Dictionary<string, VectorDocument<T>>(initialCapacity);\n        _vectorDimension = 0;\n    }\n    \n    // ... implement all methods once ...\n}\n```\n\nThen both stores can inherit from it:\n\n```csharp\npublic class WeaviateDocumentStore<T> : InMemoryDocumentStore<T>\n{\n    public WeaviateDocumentStore(int initialCapacity = 1000) : base(initialCapacity) { }\n}\n\npublic class PineconeDocumentStore<T> : InMemoryDocumentStore<T>\n{\n    public PineconeDocumentStore(string namespace, int initialCapacity = 1000) : base(initialCapacity) { }\n}\n```\n\n**Option 2:** If these stores will diverge with unique features later, add TODO comments explaining the future differentiation strategy.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/WeaviateDocumentStore.cs\naround lines 30-86 the five methods (AddCore, GetSimilarCore, GetByIdCore,\nRemoveCore, Clear) duplicate the implementation in PineconeDocumentStore<T>;\nextract this shared logic into a new generic InMemoryDocumentStore<T> base class\nthat owns _documents and _vectorDimension, implements those five methods once,\nexposes DocumentCount and VectorDimension, and validates initialCapacity in its\nconstructor; then have WeaviateDocumentStore<T> and PineconeDocumentStore<T>\ninherit from InMemoryDocumentStore<T> (keeping any provider-specific fields like\n_className in the subclass) or, if divergence is expected, add TODO comments in\nboth stores explaining why they currently duplicate behavior and when they\nshould be refactored.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:49:50Z",
    "updated_at": "2025-11-05T02:49:58Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669636",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669636"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669636"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669636/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 92,
    "original_start_line": 30,
    "start_side": "RIGHT",
    "line": 317,
    "original_line": 86,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 86,
    "position": 317,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669641",
    "pull_request_review_id": 3419457803,
    "id": 2492669641,
    "node_id": "PRRC_kwDOKSXUF86Ukx7J",
    "diff_hunk": "@@ -0,0 +1,73 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\n+using System;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels;\n+\n+/// <summary>\n+/// Cohere embedding model integration for high-performance embeddings.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Cohere provides state-of-the-art embeddings with multiple model sizes optimized\n+/// for different use cases (English, multilingual, search, classification).\n+/// </remarks>",
    "path": "src/RetrievalAugmentedGeneration/Embeddings/CohereEmbeddingModel.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Misleading documentation: Claims API integration but implements deterministic stub.**\n\nThe XML documentation claims this is a \"Cohere embedding model integration\" providing \"state-of-the-art embeddings,\" but the actual implementation (lines 44-56) generates deterministic hash-based vectors without calling the Cohere API. This misleads users into believing they have a working Cohere integration.\n\nThe documentation should clearly indicate this is a stub/mock implementation for testing, or the implementation should be completed to actually call the Cohere API.\n\n\n</parameter>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:49:50Z",
    "updated_at": "2025-11-05T02:49:58Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669641",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669641"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669641"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669641/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 9,
    "original_start_line": 9,
    "start_side": "RIGHT",
    "line": 16,
    "original_line": 16,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 16,
    "position": 16,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669648",
    "pull_request_review_id": 3419457803,
    "id": 2492669648,
    "node_id": "PRRC_kwDOKSXUF86Ukx7Q",
    "diff_hunk": "@@ -0,0 +1,73 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\n+using System;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels;\n+\n+/// <summary>\n+/// Cohere embedding model integration for high-performance embeddings.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Cohere provides state-of-the-art embeddings with multiple model sizes optimized\n+/// for different use cases (English, multilingual, search, classification).\n+/// </remarks>\n+public class CohereEmbeddingModel<T> : EmbeddingModelBase<T>\n+{\n+    private readonly string _apiKey;\n+    private readonly string _model;\n+    private readonly string _inputType;\n+    private readonly int _dimension;\n+\n+    public override int EmbeddingDimension => _dimension;\n+    public override int MaxTokens => 512;\n+\n+    public CohereEmbeddingModel(string apiKey, string model, string inputType, int dimension = 1024)\n+    {\n+        if (string.IsNullOrWhiteSpace(apiKey))\n+            throw new ArgumentException(\"API key cannot be empty\", nameof(apiKey));\n+        if (string.IsNullOrWhiteSpace(model))\n+            throw new ArgumentException(\"Model cannot be empty\", nameof(model));\n+        if (string.IsNullOrWhiteSpace(inputType))\n+            throw new ArgumentException(\"Input type cannot be empty\", nameof(inputType));\n+        if (dimension <= 0)\n+            throw new ArgumentException(\"Dimension must be positive\", nameof(dimension));\n+\n+        _apiKey = apiKey;\n+        _model = model;\n+        _inputType = inputType;\n+        _dimension = dimension;\n+    }",
    "path": "src/RetrievalAugmentedGeneration/Embeddings/CohereEmbeddingModel.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n<details>\n<summary>≡ƒº⌐ Analysis chain</summary>\n\n**API key and model parameters are stored but never used.**\n\nThe constructor validates and stores `_apiKey` (line 19) and `_model` (line 20), but these fields are never used in the `EmbedCore` implementation (lines 44-56). This is misleadingΓÇöusers will assume their API credentials are being used to call the Cohere API, when in reality the implementation generates deterministic embeddings without any external API call.\n\nIf this is intentionally a stub implementation, either:\n- Remove unused parameters from the constructor, or  \n- Add a TODO comment and throw `NotImplementedException` to signal incomplete implementation, or  \n- Actually implement the Cohere API integration\n\n\n\n\nRun this script to verify if other embedding models follow the same pattern of storing but not using API parameters:\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Description: Check if other embedding models also store API keys without using them\n\n# Search for API key fields in embedding models\nrg -n -A 3 -B 3 'private.*string.*_apiKey' src/RetrievalAugmentedGeneration/Embeddings/\n\n# Check how these API keys are used (should only appear in constructor if unused)\nrg -n '_apiKey' src/RetrievalAugmentedGeneration/Embeddings/\n```\n\nLength of output: 4987\n\n---\n\n```\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check if _apiKey is used anywhere OUTSIDE the constructor (field assignment)\n# in CohereEmbeddingModel and other models\n\necho \"=== CohereEmbeddingModel _apiKey usage ===\"\nrg -n '_apiKey' src/RetrievalAugmentedGeneration/Embeddings/CohereEmbeddingModel.cs -A 2\n\necho \"\"\necho \"=== OpenAIEmbeddingModel _apiKey usage ===\"\nrg -n '_apiKey' src/RetrievalAugmentedGeneration/Embeddings/OpenAIEmbeddingModel.cs -A 2\n\necho \"\"\necho \"=== VoyageAIEmbeddingModel _apiKey usage ===\"\nrg -n '_apiKey' src/RetrievalAugmentedGeneration/Embeddings/VoyageAIEmbeddingModel.cs -A 2\n\necho \"\"\necho \"=== HuggingFaceEmbeddingModel _apiKey usage ===\"\nrg -n '_apiKey' src/RetrievalAugmentedGeneration/Embeddings/HuggingFaceEmbeddingModel.cs -A 2\n\necho \"\"\necho \"=== GooglePalmEmbeddingModel _apiKey usage ===\"\nrg -n '_apiKey' src/RetrievalAugmentedGeneration/Embeddings/GooglePalmEmbeddingModel.cs -A 2\n```\n\nLength of output: 1541\n\n---\n\n```\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check EmbedCore implementations to see if any model actually makes API calls\n\necho \"=== CohereEmbeddingModel.EmbedCore implementation ===\"\nrg -n 'EmbedCore\\|HttpClient\\|ApiRequest\\|Cohere' src/RetrievalAugmentedGeneration/Embeddings/CohereEmbeddingModel.cs -A 5\n\necho \"\"\necho \"=== OpenAIEmbeddingModel.EmbedCore implementation ===\"\nrg -n 'EmbedCore\\|HttpClient\\|ApiRequest\\|OpenAI' src/RetrievalAugmentedGeneration/Embeddings/OpenAIEmbeddingModel.cs -A 5\n\necho \"\"\necho \"=== VoyageAIEmbeddingModel.EmbedCore implementation ===\"\nrg -n 'EmbedCore\\|HttpClient\\|ApiRequest\\|Voyage' src/RetrievalAugmentedGeneration/Embeddings/VoyageAIEmbeddingModel.cs -A 5\n```\n\nLength of output: 165\n\n---\n\n```\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Look for EmbedCore method definition in CohereEmbeddingModel\necho \"=== CohereEmbeddingModel methods ===\"\nrg -n 'protected\\|public\\|private' src/RetrievalAugmentedGeneration/Embeddings/CohereEmbeddingModel.cs -A 3 | head -40\n\necho \"\"\necho \"=== Checking for HTTP/REST calls in CohereEmbeddingModel ===\"\nrg -n 'HttpClient\\|HttpRequest\\|new Http\\|\\.Post\\|\\.Get\\|\\.Put' src/RetrievalAugmentedGeneration/Embeddings/CohereEmbeddingModel.cs\n\necho \"\"\necho \"=== Checking for hardcoded/stub embeddings in CohereEmbeddingModel ===\"\nrg -n 'new float\\|new double\\|Vector\\|Enumerable.Range\\|for.*int' src/RetrievalAugmentedGeneration/Embeddings/CohereEmbeddingModel.cs -A 2\n```\n\nLength of output: 170\n\n---\n\n```\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Read the entire CohereEmbeddingModel.cs file to see full implementation\ncat src/RetrievalAugmentedGeneration/Embeddings/CohereEmbeddingModel.cs\n```\n\nLength of output: 2382\n\n---\n\n```\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check if other embedding models actually use their API keys or are also stubs\necho \"=== OpenAIEmbeddingModel.cs full content ===\"\ncat src/RetrievalAugmentedGeneration/Embeddings/OpenAIEmbeddingModel.cs | head -80\n```\n\nLength of output: 2445\n\n---\n\n\n\n</details>\n\n**API key and model parameters are stored but never used ΓÇö this is a systemic issue across all embedding models.**\n\nThe constructor validates and stores `_apiKey` and `_model`, but both are never referenced in `EmbedCore` (lines 44-56). The implementation generates deterministic embeddings using a hash function instead. This same pattern occurs in OpenAIEmbeddingModel, VoyageAIEmbeddingModel, and other embedding modelsΓÇöall accept credentials but ignore them, creating a misleading API.\n\nUsers will assume their API credentials enable real API calls, but the implementations are stubs generating deterministic embeddings locally.\n\nTo resolve, either:\n- Remove unused `apiKey` and `model` parameters from constructors\n- Add `NotImplementedException` with a TODO to signal incomplete implementation\n- Implement actual Cohere API integration\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:49:50Z",
    "updated_at": "2025-11-05T02:49:58Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669648",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669648"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669648"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669648/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 19,
    "original_start_line": 19,
    "start_side": "RIGHT",
    "line": 42,
    "original_line": 42,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 42,
    "position": 42,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669655",
    "pull_request_review_id": 3419457803,
    "id": 2492669655,
    "node_id": "PRRC_kwDOKSXUF86Ukx7X",
    "diff_hunk": "@@ -0,0 +1,65 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\n+using System;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels\n+{\n+    /// <summary>\n+    /// Local transformer embedding model for generating embeddings without external API calls.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class LocalTransformerEmbedding<T> : EmbeddingModelBase<T>\n+    {\n+        private readonly string _modelPath;\n+        private readonly int _dimension;\n+        private readonly int _maxTokens;\n+\n+        public override int EmbeddingDimension => _dimension;\n+        public override int MaxTokens => _maxTokens;\n+\n+        public LocalTransformerEmbedding(string modelPath, int dimension = 384, int maxTokens = 512)\n+        {\n+            if (string.IsNullOrWhiteSpace(modelPath))\n+                throw new ArgumentException(\"Model path cannot be empty\", nameof(modelPath));\n+            if (dimension <= 0)\n+                throw new ArgumentException(\"Dimension must be positive\", nameof(dimension));\n+            if (maxTokens <= 0)\n+                throw new ArgumentException(\"Max tokens must be positive\", nameof(maxTokens));\n+\n+            _modelPath = modelPath;\n+            _dimension = dimension;\n+            _maxTokens = maxTokens;\n+        }",
    "path": "src/RetrievalAugmentedGeneration/Embeddings/LocalTransformerEmbedding.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**The `modelPath` parameter is never used.**\n\nThe constructor accepts and validates `modelPath`, but the implementation never loads or uses any model file. This is misleading - users will expect the class to load a transformer model from this path, but the embedding logic (lines 35-47) only generates deterministic sine-based vectors without any model inference.\n\n\n\nEither:\n1. Remove the `modelPath` parameter entirely if this is intended as a mock/stub, or\n2. Implement actual model loading and inference (see next comment)\n\n```diff\n-public LocalTransformerEmbedding(string modelPath, int dimension = 384, int maxTokens = 512)\n+public LocalTransformerEmbedding(int dimension = 384, int maxTokens = 512)\n {\n-    if (string.IsNullOrWhiteSpace(modelPath))\n-        throw new ArgumentException(\"Model path cannot be empty\", nameof(modelPath));\n     if (dimension <= 0)\n         throw new ArgumentException(\"Dimension must be positive\", nameof(dimension));\n     if (maxTokens <= 0)\n         throw new ArgumentException(\"Max tokens must be positive\", nameof(maxTokens));\n\n-    _modelPath = modelPath;\n     _dimension = dimension;\n     _maxTokens = maxTokens;\n }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        public LocalTransformerEmbedding(int dimension = 384, int maxTokens = 512)\n        {\n            if (dimension <= 0)\n                throw new ArgumentException(\"Dimension must be positive\", nameof(dimension));\n            if (maxTokens <= 0)\n                throw new ArgumentException(\"Max tokens must be positive\", nameof(maxTokens));\n\n            _dimension = dimension;\n            _maxTokens = maxTokens;\n        }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Embeddings/LocalTransformerEmbedding.cs\naround lines 21 to 33, the constructor accepts and validates modelPath but never\nuses it; either remove the modelPath parameter and corresponding\nvalidation/field if this is intended to be a pure deterministic stub, or\nimplement proper model loading: store the path to a private field, load the\ntransformer model (or an inference client) during construction or lazily, and\nuse it during embedding generation instead of the current sine-based stub;\nupdate XML/comments to reflect whether this class is a mock or a real\nmodel-backed embedding and adjust unit tests accordingly.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:49:51Z",
    "updated_at": "2025-11-05T02:49:58Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669655",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669655"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669655"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669655/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 21,
    "original_start_line": 21,
    "start_side": "RIGHT",
    "line": 33,
    "original_line": 33,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 33,
    "position": 33,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669664",
    "pull_request_review_id": 3419457803,
    "id": 2492669664,
    "node_id": "PRRC_kwDOKSXUF86Ukx7g",
    "diff_hunk": "@@ -0,0 +1,204 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels;\n+\n+/// <summary>\n+/// Multi-modal embedding model that creates unified vector representations for both text and images in the same embedding space.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// This model enables cross-modal similarity search by embedding text and images into the same vector space,\n+/// allowing you to search for images using text queries, find similar images, or retrieve text documents\n+/// related to images. Based on models like CLIP (Contrastive Language-Image Pre-training).\n+/// </para>\n+/// <para><b>For Beginners:</b> Imagine a search engine where you can find pictures using words, or vice versa.\n+/// \n+/// Normal embeddings:\n+/// - Text embedding: \"a cat\" ΓåÆ [0.2, 0.5, 0.1, ...]\n+/// - Image embedding: (cat.jpg) ΓåÆ [0.9, 0.1, 0.3, ...] (different space, can't compare!)\n+/// \n+/// Multi-modal embeddings (same space):\n+/// - Text: \"a cat\" ΓåÆ [0.2, 0.5, 0.1, ...]\n+/// - Image: (cat.jpg) ΓåÆ [0.21, 0.48, 0.12, ...] (similar values = same meaning!)\n+/// - Result: Can calculate similarity between text and image!\n+/// \n+/// Real-world uses:\n+/// 1. Search images with text: \"sunset over mountains\" finds mountain sunset photos\n+/// 2. Search text with images: Upload product photo, find product descriptions\n+/// 3. Visual Q&A: \"What's in this image?\" matched against image embeddings\n+/// 4. Content recommendation: Show images similar to text user likes\n+/// \n+/// How it works:\n+/// The model was trained with millions of (image, text) pairs to learn that:\n+/// - Image of dog + text \"dog\" should have similar embeddings\n+/// - Image of dog + text \"cat\" should have different embeddings\n+/// - This creates a \"shared understanding\" between vision and language\n+/// </para>\n+/// <para><b>Example Usage:</b>\n+/// <code>\n+/// // Initialize with CLIP model\n+/// var model = new MultiModalEmbeddingModel&lt;double&gt;(\n+///     modelPath: \"path/to/clip-model.onnx\",\n+///     normalizeEmbeddings: true,  // Important for cosine similarity\n+///     dimension: 512\n+/// );\n+/// \n+/// // Embed text queries\n+/// var textEmbedding = model.Embed(\"a photo of a golden retriever\");\n+/// \n+/// // Embed images\n+/// var imageEmbedding = model.EmbedImage(\"photos/dog.jpg\");\n+/// \n+/// // Calculate cross-modal similarity\n+/// var similarity = Vector.CosineSimilarity(textEmbedding, imageEmbedding);\n+/// // High similarity means image matches text description!\n+/// \n+/// // Batch process images\n+/// var imagePaths = Directory.GetFiles(\"photos/\", \"*.jpg\");\n+/// var imageEmbeddings = model.EmbedImageBatch(imagePaths).ToList();\n+/// \n+/// // Find best matching image for a text query\n+/// var query = model.Embed(\"brown dog playing fetch\");\n+/// var bestMatch = imageEmbeddings\n+///     .Select((emb, idx) => (similarity: Vector.CosineSimilarity(query, emb), path: imagePaths[idx]))\n+///     .OrderByDescending(x => x.similarity)\n+///     .First();\n+/// </code>\n+/// </para>\n+/// <para><b>How It Works:</b>\n+/// Architecture (simplified CLIP):\n+/// \n+/// Text Path:\n+/// 1. Text ΓåÆ Tokenizer ΓåÆ Token IDs\n+/// 2. Token IDs ΓåÆ Text Encoder (Transformer) ΓåÆ Text Features\n+/// 3. Text Features ΓåÆ Projection Head ΓåÆ Text Embedding (512D)\n+/// \n+/// Image Path:\n+/// 1. Image ΓåÆ Resize/Normalize ΓåÆ Pixel Array\n+/// 2. Pixels ΓåÆ Image Encoder (CNN/Vision Transformer) ΓåÆ Image Features\n+/// 3. Image Features ΓåÆ Projection Head ΓåÆ Image Embedding (512D)\n+/// \n+/// Both paths output embeddings in the same 512-dimensional space, enabling direct comparison.\n+/// \n+/// Current implementation uses ONNX for text encoding and simulated image encoding.\n+/// Production should use full CLIP ONNX model with both encoders.\n+/// </para>\n+/// <para><b>Benefits:</b>\n+/// - Cross-modal search - Find images with text, text with images\n+/// - Zero-shot classification - Classify images without task-specific training\n+/// - Unified representation - Single embedding space for all content types\n+/// - Flexible retrieval - Mix text and image documents in same index\n+/// - Pre-trained - No custom training needed for basic use cases\n+/// </para>\n+/// <para><b>Limitations:</b>\n+/// - Image encoding currently simulated (hash-based) - needs real CLIP encoder\n+/// - Model file size can be large (1-4GB for CLIP variants)\n+/// - Image processing slower than text (convolutional layers are compute-intensive)\n+/// - Works best with natural images (struggles with abstract diagrams, charts)\n+/// - Limited to English text in most CLIP models (multilingual versions available)\n+/// </para>\n+/// </remarks>\n+public class MultiModalEmbeddingModel<T> : EmbeddingModelBase<T>\n+{\n+    private readonly string _modelPath;\n+    private readonly bool _normalizeEmbeddings;\n+\n+    private readonly int _dimension;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"MultiModalEmbeddingModel{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"modelPath\">Path to the multi-modal model (e.g., CLIP).</param>\n+    /// <param name=\"normalizeEmbeddings\">Whether to normalize embeddings to unit length.</param>\n+    /// <param name=\"dimension\">The embedding dimension.</param>\n+    public MultiModalEmbeddingModel(\n+        string modelPath,\n+        bool normalizeEmbeddings,\n+        int dimension)\n+    {\n+        _modelPath = modelPath ?? throw new ArgumentNullException(nameof(modelPath));\n+        _normalizeEmbeddings = normalizeEmbeddings;\n+        _dimension = dimension;\n+    }",
    "path": "src/RetrievalAugmentedGeneration/Embeddings/MultiModalEmbeddingModel.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Add validation for modelPath existence and dimension value.**\n\nThe constructor validates `modelPath` for null but doesn't check if the file exists, which will cause a late failure when `EmbedCore` is called. Additionally, `dimension` should be validated to ensure it's positive, similar to the validation in `ONNXSentenceTransformer`.\n\n\n\nApply this diff to add the missing validations:\n\n```diff\n public MultiModalEmbeddingModel(\n     string modelPath,\n     bool normalizeEmbeddings,\n     int dimension)\n {\n     _modelPath = modelPath ?? throw new ArgumentNullException(nameof(modelPath));\n+    \n+    if (!File.Exists(modelPath))\n+        throw new FileNotFoundException($\"Model file not found: {modelPath}\");\n+    \n+    if (dimension <= 0)\n+        throw new ArgumentException(\"Dimension must be positive\", nameof(dimension));\n+    \n     _normalizeEmbeddings = normalizeEmbeddings;\n     _dimension = dimension;\n }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    public MultiModalEmbeddingModel(\n        string modelPath,\n        bool normalizeEmbeddings,\n        int dimension)\n    {\n        _modelPath = modelPath ?? throw new ArgumentNullException(nameof(modelPath));\n        \n        if (!File.Exists(modelPath))\n            throw new FileNotFoundException($\"Model file not found: {modelPath}\");\n        \n        if (dimension <= 0)\n            throw new ArgumentException(\"Dimension must be positive\", nameof(dimension));\n        \n        _normalizeEmbeddings = normalizeEmbeddings;\n        _dimension = dimension;\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Embeddings/MultiModalEmbeddingModel.cs\naround lines 117 to 125, the constructor currently only checks modelPath for\nnull and doesn't validate that the file actually exists nor that dimension is\npositive; add a check that File.Exists(modelPath) (or Directory/URI check as\nappropriate) and throw a FileNotFoundException or ArgumentException with a clear\nmessage if it doesn't, and validate that dimension > 0 and throw an\nArgumentOutOfRangeException (include nameof(dimension) in the exception);\nperform these validations before assigning fields.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:49:51Z",
    "updated_at": "2025-11-05T02:49:58Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669664",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669664"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669664"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669664/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 117,
    "original_start_line": 117,
    "start_side": "RIGHT",
    "line": 125,
    "original_line": 125,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 125,
    "position": 125,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669673",
    "pull_request_review_id": 3419457803,
    "id": 2492669673,
    "node_id": "PRRC_kwDOKSXUF86Ukx7p",
    "diff_hunk": "@@ -0,0 +1,204 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels;\n+\n+/// <summary>\n+/// Multi-modal embedding model that creates unified vector representations for both text and images in the same embedding space.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// This model enables cross-modal similarity search by embedding text and images into the same vector space,\n+/// allowing you to search for images using text queries, find similar images, or retrieve text documents\n+/// related to images. Based on models like CLIP (Contrastive Language-Image Pre-training).\n+/// </para>\n+/// <para><b>For Beginners:</b> Imagine a search engine where you can find pictures using words, or vice versa.\n+/// \n+/// Normal embeddings:\n+/// - Text embedding: \"a cat\" ΓåÆ [0.2, 0.5, 0.1, ...]\n+/// - Image embedding: (cat.jpg) ΓåÆ [0.9, 0.1, 0.3, ...] (different space, can't compare!)\n+/// \n+/// Multi-modal embeddings (same space):\n+/// - Text: \"a cat\" ΓåÆ [0.2, 0.5, 0.1, ...]\n+/// - Image: (cat.jpg) ΓåÆ [0.21, 0.48, 0.12, ...] (similar values = same meaning!)\n+/// - Result: Can calculate similarity between text and image!\n+/// \n+/// Real-world uses:\n+/// 1. Search images with text: \"sunset over mountains\" finds mountain sunset photos\n+/// 2. Search text with images: Upload product photo, find product descriptions\n+/// 3. Visual Q&A: \"What's in this image?\" matched against image embeddings\n+/// 4. Content recommendation: Show images similar to text user likes\n+/// \n+/// How it works:\n+/// The model was trained with millions of (image, text) pairs to learn that:\n+/// - Image of dog + text \"dog\" should have similar embeddings\n+/// - Image of dog + text \"cat\" should have different embeddings\n+/// - This creates a \"shared understanding\" between vision and language\n+/// </para>\n+/// <para><b>Example Usage:</b>\n+/// <code>\n+/// // Initialize with CLIP model\n+/// var model = new MultiModalEmbeddingModel&lt;double&gt;(\n+///     modelPath: \"path/to/clip-model.onnx\",\n+///     normalizeEmbeddings: true,  // Important for cosine similarity\n+///     dimension: 512\n+/// );\n+/// \n+/// // Embed text queries\n+/// var textEmbedding = model.Embed(\"a photo of a golden retriever\");\n+/// \n+/// // Embed images\n+/// var imageEmbedding = model.EmbedImage(\"photos/dog.jpg\");\n+/// \n+/// // Calculate cross-modal similarity\n+/// var similarity = Vector.CosineSimilarity(textEmbedding, imageEmbedding);\n+/// // High similarity means image matches text description!\n+/// \n+/// // Batch process images\n+/// var imagePaths = Directory.GetFiles(\"photos/\", \"*.jpg\");\n+/// var imageEmbeddings = model.EmbedImageBatch(imagePaths).ToList();\n+/// \n+/// // Find best matching image for a text query\n+/// var query = model.Embed(\"brown dog playing fetch\");\n+/// var bestMatch = imageEmbeddings\n+///     .Select((emb, idx) => (similarity: Vector.CosineSimilarity(query, emb), path: imagePaths[idx]))\n+///     .OrderByDescending(x => x.similarity)\n+///     .First();\n+/// </code>\n+/// </para>\n+/// <para><b>How It Works:</b>\n+/// Architecture (simplified CLIP):\n+/// \n+/// Text Path:\n+/// 1. Text ΓåÆ Tokenizer ΓåÆ Token IDs\n+/// 2. Token IDs ΓåÆ Text Encoder (Transformer) ΓåÆ Text Features\n+/// 3. Text Features ΓåÆ Projection Head ΓåÆ Text Embedding (512D)\n+/// \n+/// Image Path:\n+/// 1. Image ΓåÆ Resize/Normalize ΓåÆ Pixel Array\n+/// 2. Pixels ΓåÆ Image Encoder (CNN/Vision Transformer) ΓåÆ Image Features\n+/// 3. Image Features ΓåÆ Projection Head ΓåÆ Image Embedding (512D)\n+/// \n+/// Both paths output embeddings in the same 512-dimensional space, enabling direct comparison.\n+/// \n+/// Current implementation uses ONNX for text encoding and simulated image encoding.\n+/// Production should use full CLIP ONNX model with both encoders.\n+/// </para>\n+/// <para><b>Benefits:</b>\n+/// - Cross-modal search - Find images with text, text with images\n+/// - Zero-shot classification - Classify images without task-specific training\n+/// - Unified representation - Single embedding space for all content types\n+/// - Flexible retrieval - Mix text and image documents in same index\n+/// - Pre-trained - No custom training needed for basic use cases\n+/// </para>\n+/// <para><b>Limitations:</b>\n+/// - Image encoding currently simulated (hash-based) - needs real CLIP encoder\n+/// - Model file size can be large (1-4GB for CLIP variants)\n+/// - Image processing slower than text (convolutional layers are compute-intensive)\n+/// - Works best with natural images (struggles with abstract diagrams, charts)\n+/// - Limited to English text in most CLIP models (multilingual versions available)\n+/// </para>\n+/// </remarks>\n+public class MultiModalEmbeddingModel<T> : EmbeddingModelBase<T>\n+{\n+    private readonly string _modelPath;\n+    private readonly bool _normalizeEmbeddings;\n+\n+    private readonly int _dimension;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"MultiModalEmbeddingModel{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"modelPath\">Path to the multi-modal model (e.g., CLIP).</param>\n+    /// <param name=\"normalizeEmbeddings\">Whether to normalize embeddings to unit length.</param>\n+    /// <param name=\"dimension\">The embedding dimension.</param>\n+    public MultiModalEmbeddingModel(\n+        string modelPath,\n+        bool normalizeEmbeddings,\n+        int dimension)\n+    {\n+        _modelPath = modelPath ?? throw new ArgumentNullException(nameof(modelPath));\n+        _normalizeEmbeddings = normalizeEmbeddings;\n+        _dimension = dimension;\n+    }\n+\n+    /// <inheritdoc />\n+    public override int EmbeddingDimension => _dimension;\n+\n+    /// <inheritdoc />\n+    public override int MaxTokens => 512;\n+\n+    /// <inheritdoc />\n+    protected override Vector<T> EmbedCore(string text)\n+    {\n+        // Use ONNX-based sentence transformer for text embeddings\n+        var textModel = new ONNXSentenceTransformer<T>(_modelPath, _dimension, MaxTokens);\n+        var embedding = textModel.Embed(text);\n+        \n+        return _normalizeEmbeddings ? embedding.Normalize() : embedding;\n+    }",
    "path": "src/RetrievalAugmentedGeneration/Embeddings/MultiModalEmbeddingModel.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Critical: Instantiation inefficiency and normalization inconsistency.**\n\nTwo significant issues:\n\n1. **Performance**: A new `ONNXSentenceTransformer` instance is created on *every* call to `EmbedCore`. This is extremely inefficient and wasteful. The instance should be created once in the constructor and reused.\n\n2. **Normalization**: `ONNXSentenceTransformer.EmbedCore` already returns a normalized vector (see line 59 in ONNXSentenceTransformer.cs: `return new Vector<T>(values).Normalize()`). Line 140 then conditionally normalizes again, resulting in:\n   - If `_normalizeEmbeddings = true`: Double normalization (redundant, though idempotent)\n   - If `_normalizeEmbeddings = false`: The vector is still normalized from the inner call, making this flag misleading\n\n\n\nApply this diff to fix both issues:\n\n```diff\n+private readonly ONNXSentenceTransformer<T> _textModel;\n+\n public MultiModalEmbeddingModel(\n     string modelPath,\n     bool normalizeEmbeddings,\n     int dimension)\n {\n     _modelPath = modelPath ?? throw new ArgumentNullException(nameof(modelPath));\n     _normalizeEmbeddings = normalizeEmbeddings;\n     _dimension = dimension;\n+    _textModel = new ONNXSentenceTransformer<T>(modelPath, dimension, MaxTokens);\n }\n\n protected override Vector<T> EmbedCore(string text)\n {\n-    // Use ONNX-based sentence transformer for text embeddings\n-    var textModel = new ONNXSentenceTransformer<T>(_modelPath, _dimension, MaxTokens);\n-    var embedding = textModel.Embed(text);\n-    \n-    return _normalizeEmbeddings ? embedding.Normalize() : embedding;\n+    // ONNXSentenceTransformer already normalizes internally\n+    return _textModel.Embed(text);\n }\n```\n\nNote: If you need to control normalization, consider modifying `ONNXSentenceTransformer` to accept a normalization flag, or create a version that doesn't normalize.\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Embeddings/MultiModalEmbeddingModel.cs\naround lines 134 to 141, avoid creating a new ONNXSentenceTransformer on every\nEmbedCore call and fix the normalization mismatch: instantiate the\nONNXSentenceTransformer<T> once (store as a private readonly field) in the class\nconstructor and reuse it inside EmbedCore, and remove the conditional\nNormalize() call there so you do not double-normalize or make the\n_normalizeEmbeddings flag misleading; if you need runtime control over\nnormalization, update ONNXSentenceTransformer to accept a normalization flag or\nadd a non-normalizing embedding method and call that from EmbedCore instead.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:49:51Z",
    "updated_at": "2025-11-05T02:49:58Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669673",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669673"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669673"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669673/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 134,
    "original_start_line": 134,
    "start_side": "RIGHT",
    "line": 141,
    "original_line": 141,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 141,
    "position": 141,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669681",
    "pull_request_review_id": 3419457803,
    "id": 2492669681,
    "node_id": "PRRC_kwDOKSXUF86Ukx7x",
    "diff_hunk": "@@ -0,0 +1,204 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels;\n+\n+/// <summary>\n+/// Multi-modal embedding model that creates unified vector representations for both text and images in the same embedding space.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// This model enables cross-modal similarity search by embedding text and images into the same vector space,\n+/// allowing you to search for images using text queries, find similar images, or retrieve text documents\n+/// related to images. Based on models like CLIP (Contrastive Language-Image Pre-training).\n+/// </para>\n+/// <para><b>For Beginners:</b> Imagine a search engine where you can find pictures using words, or vice versa.\n+/// \n+/// Normal embeddings:\n+/// - Text embedding: \"a cat\" ΓåÆ [0.2, 0.5, 0.1, ...]\n+/// - Image embedding: (cat.jpg) ΓåÆ [0.9, 0.1, 0.3, ...] (different space, can't compare!)\n+/// \n+/// Multi-modal embeddings (same space):\n+/// - Text: \"a cat\" ΓåÆ [0.2, 0.5, 0.1, ...]\n+/// - Image: (cat.jpg) ΓåÆ [0.21, 0.48, 0.12, ...] (similar values = same meaning!)\n+/// - Result: Can calculate similarity between text and image!\n+/// \n+/// Real-world uses:\n+/// 1. Search images with text: \"sunset over mountains\" finds mountain sunset photos\n+/// 2. Search text with images: Upload product photo, find product descriptions\n+/// 3. Visual Q&A: \"What's in this image?\" matched against image embeddings\n+/// 4. Content recommendation: Show images similar to text user likes\n+/// \n+/// How it works:\n+/// The model was trained with millions of (image, text) pairs to learn that:\n+/// - Image of dog + text \"dog\" should have similar embeddings\n+/// - Image of dog + text \"cat\" should have different embeddings\n+/// - This creates a \"shared understanding\" between vision and language\n+/// </para>\n+/// <para><b>Example Usage:</b>\n+/// <code>\n+/// // Initialize with CLIP model\n+/// var model = new MultiModalEmbeddingModel&lt;double&gt;(\n+///     modelPath: \"path/to/clip-model.onnx\",\n+///     normalizeEmbeddings: true,  // Important for cosine similarity\n+///     dimension: 512\n+/// );\n+/// \n+/// // Embed text queries\n+/// var textEmbedding = model.Embed(\"a photo of a golden retriever\");\n+/// \n+/// // Embed images\n+/// var imageEmbedding = model.EmbedImage(\"photos/dog.jpg\");\n+/// \n+/// // Calculate cross-modal similarity\n+/// var similarity = Vector.CosineSimilarity(textEmbedding, imageEmbedding);\n+/// // High similarity means image matches text description!\n+/// \n+/// // Batch process images\n+/// var imagePaths = Directory.GetFiles(\"photos/\", \"*.jpg\");\n+/// var imageEmbeddings = model.EmbedImageBatch(imagePaths).ToList();\n+/// \n+/// // Find best matching image for a text query\n+/// var query = model.Embed(\"brown dog playing fetch\");\n+/// var bestMatch = imageEmbeddings\n+///     .Select((emb, idx) => (similarity: Vector.CosineSimilarity(query, emb), path: imagePaths[idx]))\n+///     .OrderByDescending(x => x.similarity)\n+///     .First();\n+/// </code>\n+/// </para>\n+/// <para><b>How It Works:</b>\n+/// Architecture (simplified CLIP):\n+/// \n+/// Text Path:\n+/// 1. Text ΓåÆ Tokenizer ΓåÆ Token IDs\n+/// 2. Token IDs ΓåÆ Text Encoder (Transformer) ΓåÆ Text Features\n+/// 3. Text Features ΓåÆ Projection Head ΓåÆ Text Embedding (512D)\n+/// \n+/// Image Path:\n+/// 1. Image ΓåÆ Resize/Normalize ΓåÆ Pixel Array\n+/// 2. Pixels ΓåÆ Image Encoder (CNN/Vision Transformer) ΓåÆ Image Features\n+/// 3. Image Features ΓåÆ Projection Head ΓåÆ Image Embedding (512D)\n+/// \n+/// Both paths output embeddings in the same 512-dimensional space, enabling direct comparison.\n+/// \n+/// Current implementation uses ONNX for text encoding and simulated image encoding.\n+/// Production should use full CLIP ONNX model with both encoders.\n+/// </para>\n+/// <para><b>Benefits:</b>\n+/// - Cross-modal search - Find images with text, text with images\n+/// - Zero-shot classification - Classify images without task-specific training\n+/// - Unified representation - Single embedding space for all content types\n+/// - Flexible retrieval - Mix text and image documents in same index\n+/// - Pre-trained - No custom training needed for basic use cases\n+/// </para>\n+/// <para><b>Limitations:</b>\n+/// - Image encoding currently simulated (hash-based) - needs real CLIP encoder\n+/// - Model file size can be large (1-4GB for CLIP variants)\n+/// - Image processing slower than text (convolutional layers are compute-intensive)\n+/// - Works best with natural images (struggles with abstract diagrams, charts)\n+/// - Limited to English text in most CLIP models (multilingual versions available)\n+/// </para>\n+/// </remarks>\n+public class MultiModalEmbeddingModel<T> : EmbeddingModelBase<T>\n+{\n+    private readonly string _modelPath;\n+    private readonly bool _normalizeEmbeddings;\n+\n+    private readonly int _dimension;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"MultiModalEmbeddingModel{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"modelPath\">Path to the multi-modal model (e.g., CLIP).</param>\n+    /// <param name=\"normalizeEmbeddings\">Whether to normalize embeddings to unit length.</param>\n+    /// <param name=\"dimension\">The embedding dimension.</param>\n+    public MultiModalEmbeddingModel(\n+        string modelPath,\n+        bool normalizeEmbeddings,\n+        int dimension)\n+    {\n+        _modelPath = modelPath ?? throw new ArgumentNullException(nameof(modelPath));\n+        _normalizeEmbeddings = normalizeEmbeddings;\n+        _dimension = dimension;\n+    }\n+\n+    /// <inheritdoc />\n+    public override int EmbeddingDimension => _dimension;\n+\n+    /// <inheritdoc />\n+    public override int MaxTokens => 512;\n+\n+    /// <inheritdoc />\n+    protected override Vector<T> EmbedCore(string text)\n+    {\n+        // Use ONNX-based sentence transformer for text embeddings\n+        var textModel = new ONNXSentenceTransformer<T>(_modelPath, _dimension, MaxTokens);\n+        var embedding = textModel.Embed(text);\n+        \n+        return _normalizeEmbeddings ? embedding.Normalize() : embedding;\n+    }\n+\n+    /// <summary>\n+    /// Generates image embeddings from file path.\n+    /// </summary>\n+    /// <param name=\"imagePath\">Path to the image file.</param>\n+    /// <returns>The embedding vector for the image.</returns>\n+    public Vector<T> EmbedImage(string imagePath)\n+    {\n+        if (string.IsNullOrWhiteSpace(imagePath))\n+            throw new ArgumentException(\"Image path cannot be null or whitespace\", nameof(imagePath));\n+\n+        if (!File.Exists(imagePath))\n+            throw new FileNotFoundException($\"Image file not found: {imagePath}\");\n+\n+        // Generate embedding using image path hash\n+        // In production, this would use CLIP's image encoder with convolutional layers\n+        var values = new T[_dimension];\n+        var hash = GetImageHash(imagePath);\n+        \n+        for (int i = 0; i < _dimension; i++)\n+        {\n+            var val = NumOps.FromDouble(Math.Sin((double)hash * (i + 1) * 0.003));\n+            values[i] = val;\n+        }\n+\n+        var embedding = new Vector<T>(values);\n+        return _normalizeEmbeddings ? embedding.Normalize() : embedding;\n+    }\n+\n+    /// <summary>\n+    /// Batch embedding generation for images.\n+    /// </summary>\n+    /// <param name=\"imagePaths\">Paths to image files.</param>\n+    /// <returns>Embedding vectors for all images.</returns>\n+    public IEnumerable<Vector<T>> EmbedImageBatch(IEnumerable<string> imagePaths)\n+    {\n+        if (imagePaths == null)\n+            throw new ArgumentNullException(nameof(imagePaths));\n+\n+        return imagePaths.Select(path => EmbedImage(path));\n+    }\n+\n+    private int GetImageHash(string imagePath)\n+    {\n+        // Simple hash based on file path and length\n+        // In production, would hash image content after preprocessing\n+        unchecked\n+        {\n+            int hash = 17;\n+            hash = (hash * 31) + imagePath.GetHashCode();\n+            \n+            if (File.Exists(imagePath))\n+            {\n+                var fileInfo = new FileInfo(imagePath);\n+                hash = (hash * 31) + fileInfo.Length.GetHashCode();\n+            }\n+            \n+            return hash;\n+        }\n+    }",
    "path": "src/RetrievalAugmentedGeneration/Embeddings/MultiModalEmbeddingModel.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Non-deterministic hash across platforms.**\n\nThe use of `imagePath.GetHashCode()` at line 191 produces non-deterministic results across different .NET platforms and versions. This means the same image path will generate different embeddings on different systems, breaking reproducibility.\n\n\n\nFor consistent hashing across platforms, use a deterministic hash algorithm:\n\n```diff\n private int GetImageHash(string imagePath)\n {\n-    // Simple hash based on file path and length\n-    // In production, would hash image content after preprocessing\n     unchecked\n     {\n         int hash = 17;\n-        hash = (hash * 31) + imagePath.GetHashCode();\n+        // Deterministic hash of file path\n+        foreach (char c in imagePath)\n+        {\n+            hash = (hash * 31) + c;\n+        }\n         \n         if (File.Exists(imagePath))\n         {\n             var fileInfo = new FileInfo(imagePath);\n             hash = (hash * 31) + fileInfo.Length.GetHashCode();\n         }\n         \n         return hash;\n     }\n }\n```\n\nNote: The `File.Exists` check at lines 193-196 is redundant since `EmbedImage` already validates file existence at line 153-154.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Embeddings/MultiModalEmbeddingModel.cs\naround lines 184 to 201, the method uses imagePath.GetHashCode() which is\nnon-deterministic across .NET platforms; replace it with a deterministic hash\n(e.g., compute a SHA256 or MD5 over the UTF8 bytes of the imagePath and convert\na portion to an int) and incorporate the file length deterministically by\nreading FileInfo.Length (no File.Exists check since EmbedImage already validated\nexistence); ensure you use unchecked arithmetic if needed and return a stable\nint derived from the cryptographic hash so the same path/length produce\nidentical results across platforms.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:49:51Z",
    "updated_at": "2025-11-05T02:49:58Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669681",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669681"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669681"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669681/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 184,
    "original_start_line": 184,
    "start_side": "RIGHT",
    "line": 201,
    "original_line": 201,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 201,
    "position": 201,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669686",
    "pull_request_review_id": 3419457803,
    "id": 2492669686,
    "node_id": "PRRC_kwDOKSXUF86Ukx72",
    "diff_hunk": "@@ -0,0 +1,150 @@\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Evaluation;\n+\n+/// <summary>\n+/// Evaluates how well a RAG system handles noisy or irrelevant documents in the retrieved context.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for scoring (typically float or double).</typeparam>\n+/// <remarks>\n+/// <para>\n+/// This metric measures the robustness of a RAG system when the retrieved document set contains\n+/// irrelevant or noisy documents. A robust system should focus on high-quality documents and\n+/// maintain answer quality even when some retrieved documents are not relevant. The metric\n+/// analyzes the distribution of relevance scores to determine if the system can distinguish\n+/// signal from noise.\n+/// </para>\n+/// <para><b>For Beginners:</b> This checks if your RAG system can ignore \"junk\" documents.\n+/// \n+/// Think of it like reading a research paper:\n+/// - You have 10 sources\n+/// - 5 are highly relevant to your topic\n+/// - 5 are somewhat related but not very useful\n+/// - A good researcher focuses on the 5 good sources\n+/// - A bad researcher treats all 10 equally\n+/// \n+/// This metric checks if your RAG system acts like a good researcher!\n+/// \n+/// Score interpretation:\n+/// - Score near 1.0: System focuses on relevant documents, ignores noise\n+/// - Score near 0.5: System treats all documents equally (not robust)\n+/// - Score near 0.0: System focuses on wrong documents\n+/// \n+/// For example:\n+/// ```csharp\n+/// var metric = new NoiseRobustnessMetric<double>(NumOps.FromDouble(0.3));\n+/// var answer = new GroundedAnswer<double>(\n+///     query: \"What is AI?\",\n+///     answer: \"AI is artificial intelligence...\",\n+///     sourceDocuments: retrievedDocs,\n+///     citations: new List<string>(),\n+///     confidenceScore: 0.8\n+/// );\n+/// var score = metric.Evaluate(answer);\n+/// Console.WriteLine($\"Robustness: {score} (higher means better at filtering noise)\");\n+/// ```\n+/// \n+/// Why this matters:\n+/// - Real-world retrieval often includes some irrelevant documents\n+/// - Robust systems maintain quality despite imperfect retrieval\n+/// - Helps evaluate production reliability\n+/// </para>\n+/// </remarks>\n+public class NoiseRobustnessMetric<T> : RAGMetricBase<T>\n+{\n+    private readonly T _noiseRatio;",
    "path": "src/RetrievalAugmentedGeneration/Evaluation/NoiseRobustnessMetric.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Critical: Constructor parameter `noiseRatio` is never used.**\n\nThe `_noiseRatio` field is initialized in the constructor but never referenced in `EvaluateCore` (lines 114-149). The current implementation calculates robustness solely from the distribution of relevance scores, completely ignoring the expected noise ratio provided by the user.\n\nEither:\n1. Remove the parameter and field if they're not needed, or\n2. Incorporate `_noiseRatio` into the calculation (e.g., adjust the scoring threshold based on expected noise level, or penalize deviations from the expected noise distribution)\n\n\n\nApply this diff if you decide to remove the unused parameter:\n\n```diff\n-    private readonly T _noiseRatio;\n-\n     /// <summary>\n     /// Gets the name of this metric.\n     /// </summary>\n     public override string Name => \"Noise Robustness\";\n \n     /// <summary>\n     /// Gets the description of what this metric measures.\n     /// </summary>\n     public override string Description => \"Evaluates robustness to noisy or irrelevant documents in the context\";\n \n     /// <summary>\n     /// Gets a value indicating whether this metric requires ground truth for evaluation.\n     /// </summary>\n     protected override bool RequiresGroundTruth => false;\n \n     /// <summary>\n     /// Initializes a new instance of the NoiseRobustnessMetric class.\n     /// </summary>\n-    /// <param name=\"noiseRatio\">The expected ratio of noise documents in the context (0-1).</param>\n-    /// <remarks>\n-    /// <para><b>For Beginners:</b> The noise ratio tells the metric what percentage of\n-    /// documents you expect to be irrelevant. For example, 0.3 means you expect about 30%\n-    /// of retrieved documents to be noise.\n-    /// </para>\n-    /// </remarks>\n-    public NoiseRobustnessMetric(T noiseRatio)\n+    public NoiseRobustnessMetric()\n     {\n-        _noiseRatio = noiseRatio;\n     }\n```\n\nAlso update the documentation example on line 36 to remove the parameter:\n```diff\n-var metric = new NoiseRobustnessMetric<double>(NumOps.FromDouble(0.3));\n+var metric = new NoiseRobustnessMetric<double>();\n```\n\n\nAlso applies to: 83-86\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:49:51Z",
    "updated_at": "2025-11-05T02:49:58Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669686",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669686"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669686"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669686/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": 56,
    "original_line": 56,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 56,
    "position": 56,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669693",
    "pull_request_review_id": 3419457803,
    "id": 2492669693,
    "node_id": "PRRC_kwDOKSXUF86Ukx79",
    "diff_hunk": "@@ -0,0 +1,301 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.NER;\n+\n+/// <summary>\n+/// Production-ready Named Entity Recognition model using pattern matching and heuristics.\n+/// </summary>\n+/// <remarks>\n+/// <para>\n+/// This NER model uses intelligent pattern matching, capitalization analysis, and context clues\n+/// to identify named entities. While not ML-based (yet), it provides production-ready accuracy\n+/// for common entity types through carefully crafted rules.\n+/// </para>\n+/// <para><b>For Beginners:</b> This model identifies entities in text (people, places, organizations, etc.).\n+/// \n+/// Think of it like highlighting important information in a document:\n+/// - Input: \"Albert Einstein worked at Princeton University in New Jersey.\"\n+/// - Output: [Albert Einstein]=PERSON, [Princeton University]=ORGANIZATION, [New Jersey]=LOCATION\n+/// \n+/// How it works:\n+/// 1. Analyzes capitalization patterns\n+/// 2. Looks for entity indicators (Corp, University, City, etc.)\n+/// 3. Uses word lists of common names/places\n+/// 4. Detects multi-word entities (first + last names, etc.)\n+/// 5. Applies context rules (person names before \"works at\", etc.)\n+/// \n+/// Entity types supported:\n+/// - PERSON: Names of people\n+/// - ORGANIZATION: Companies, institutions\n+/// - LOCATION: Cities, countries, places\n+/// - DATE: Temporal expressions",
    "path": "src/RetrievalAugmentedGeneration/NER/BiLSTMCRF_NER.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Remove DATE from documentation or implement DATE entity detection.**\n\nThe documentation claims DATE entity type support (lines 29-33), but the `PredictEntityType` method has no date detection logic. This creates a documentation-implementation mismatch.\n\nEither implement DATE detection or update the documentation to note it as \"future work.\"\n\n\n\nApply this diff to clarify:\n\n```diff\n /// Entity types supported:\n /// - PERSON: Names of people\n /// - ORGANIZATION: Companies, institutions\n /// - LOCATION: Cities, countries, places\n-/// - DATE: Temporal expressions\n+/// - DATE: Temporal expressions (planned for future implementation)\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/NER/BiLSTMCRF_NER.cs around lines 29 to 33,\nthe file's header lists DATE as a supported entity but the PredictEntityType\nmethod contains no date detection, causing a docs/implementation mismatch;\neither remove the DATE bullet or mark it as \"future work\" in the header, or\nimplement DATE detection by adding regex/date-parsing logic to PredictEntityType\n(e.g., patterns for YYYY-MM-DD, MM/DD/YYYY, month names, relative dates) and\nupdate unit tests and comments accordingly so the documentation and code stay\nconsistent.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:49:52Z",
    "updated_at": "2025-11-05T02:49:58Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669693",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669693"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669693"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669693/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 29,
    "original_start_line": 29,
    "start_side": "RIGHT",
    "line": 33,
    "original_line": 33,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 33,
    "position": 33,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669701",
    "pull_request_review_id": 3419457803,
    "id": 2492669701,
    "node_id": "PRRC_kwDOKSXUF86Ukx8F",
    "diff_hunk": "@@ -0,0 +1,301 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.NER;\n+\n+/// <summary>\n+/// Production-ready Named Entity Recognition model using pattern matching and heuristics.\n+/// </summary>\n+/// <remarks>\n+/// <para>\n+/// This NER model uses intelligent pattern matching, capitalization analysis, and context clues\n+/// to identify named entities. While not ML-based (yet), it provides production-ready accuracy\n+/// for common entity types through carefully crafted rules.\n+/// </para>\n+/// <para><b>For Beginners:</b> This model identifies entities in text (people, places, organizations, etc.).\n+/// \n+/// Think of it like highlighting important information in a document:\n+/// - Input: \"Albert Einstein worked at Princeton University in New Jersey.\"\n+/// - Output: [Albert Einstein]=PERSON, [Princeton University]=ORGANIZATION, [New Jersey]=LOCATION\n+/// \n+/// How it works:\n+/// 1. Analyzes capitalization patterns\n+/// 2. Looks for entity indicators (Corp, University, City, etc.)\n+/// 3. Uses word lists of common names/places\n+/// 4. Detects multi-word entities (first + last names, etc.)\n+/// 5. Applies context rules (person names before \"works at\", etc.)\n+/// \n+/// Entity types supported:\n+/// - PERSON: Names of people\n+/// - ORGANIZATION: Companies, institutions\n+/// - LOCATION: Cities, countries, places\n+/// - DATE: Temporal expressions\n+/// \n+/// Future: Will be upgraded to BiLSTM-CRF neural network model for higher accuracy.\n+/// </para>\n+/// </remarks>\n+public class NamedEntityRecognizer\n+{\n+    private readonly Dictionary<string, string> _commonNames;\n+    private readonly Dictionary<string, string> _commonLocations;\n+    private readonly Dictionary<string, string> _commonOrganizations;\n+    \n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"NamedEntityRecognizer\"/> class.\n+    /// </summary>\n+    public NamedEntityRecognizer()\n+    {\n+        _commonNames = InitializeCommonNames();\n+        _commonLocations = InitializeCommonLocations();\n+        _commonOrganizations = InitializeCommonOrganizations();\n+    }\n+    \n+    private Dictionary<string, string> InitializeCommonNames()\n+    {\n+        var names = new Dictionary<string, string>(StringComparer.OrdinalIgnoreCase);\n+        foreach (var name in new[] {\n+            \"John\", \"Mary\", \"James\", \"Patricia\", \"Robert\", \"Jennifer\", \"Michael\", \"Linda\",\n+            \"William\", \"Elizabeth\", \"David\", \"Barbara\", \"Richard\", \"Susan\", \"Joseph\", \"Jessica\",\n+            \"Thomas\", \"Sarah\", \"Charles\", \"Karen\", \"Christopher\", \"Nancy\", \"Daniel\", \"Lisa\",\n+            \"Albert\", \"Isaac\", \"Marie\", \"Stephen\", \"Alan\", \"Richard\", \"Marie\", \"Ada\",\n+            \"Grace\", \"Linus\", \"Dennis\", \"Ken\", \"Brian\", \"Donald\", \"Elon\", \"Jeff\", \"Bill\"\n+        })\n+        {\n+            names[name] = \"PERSON\";\n+        }\n+        return names;\n+    }\n+    \n+    private Dictionary<string, string> InitializeCommonLocations()\n+    {\n+        var locations = new Dictionary<string, string>(StringComparer.OrdinalIgnoreCase);\n+        foreach (var location in new[] {\n+            \"America\", \"Europe\", \"Asia\", \"Africa\", \"Australia\", \"Antarctica\",\n+            \"California\", \"Texas\", \"Florida\", \"NewYork\", \"Illinois\", \"Pennsylvania\",\n+            \"Seattle\", \"Boston\", \"Chicago\", \"Houston\", \"Phoenix\", \"Philadelphia\",\n+            \"Denver\", \"Washington\", \"Atlanta\", \"Miami\", \"Dallas\", \"Austin\",\n+            \"London\", \"Paris\", \"Tokyo\", \"Beijing\", \"Moscow\", \"Berlin\", \"Rome\",\n+            \"Princeton\", \"Cambridge\", \"Oxford\", \"Stanford\", \"Harvard\", \"MIT\"\n+        })\n+        {\n+            locations[location] = \"LOCATION\";\n+        }\n+        return locations;\n+    }\n+    \n+    private Dictionary<string, string> InitializeCommonOrganizations()\n+    {\n+        var orgs = new Dictionary<string, string>(StringComparer.OrdinalIgnoreCase);\n+        foreach (var org in new[] {\n+            \"Microsoft\", \"Apple\", \"Google\", \"Amazon\", \"Facebook\", \"Tesla\",\n+            \"IBM\", \"Intel\", \"Oracle\", \"Adobe\", \"Netflix\", \"Uber\", \"Twitter\",\n+            \"NASA\", \"MIT\", \"Harvard\", \"Stanford\", \"Princeton\", \"Yale\", \"Oxford\"\n+        })\n+        {\n+            orgs[org] = \"ORGANIZATION\";\n+        }\n+        return orgs;\n+    }\n+    \n+    /// <summary>\n+    /// Extracts named entities from text.\n+    /// </summary>\n+    /// <param name=\"text\">The input text to process.</param>\n+    /// <returns>List of extracted entities with their types and spans.</returns>\n+    public List<ExtractedEntity> ExtractEntities(string text)\n+    {\n+        if (string.IsNullOrWhiteSpace(text))\n+            return new List<ExtractedEntity>();\n+            \n+        var tokens = TokenizeText(text);\n+        if (tokens.Count == 0)\n+            return new List<ExtractedEntity>();\n+        \n+        return ExtractEntitiesFromTokens(tokens, text);\n+    }\n+    \n+    private List<string> TokenizeText(string text)\n+    {\n+        return text.Split(new[] { ' ', '\\t', '\\n', '\\r' }, StringSplitOptions.RemoveEmptyEntries)\n+                   .ToList();\n+    }\n+    \n+    private List<ExtractedEntity> ExtractEntitiesFromTokens(List<string> tokens, string originalText)\n+    {\n+        var entities = new List<ExtractedEntity>();\n+        int i = 0;\n+        \n+        while (i < tokens.Count)\n+        {\n+            var token = tokens[i];\n+            \n+            // Check for multi-word entities first\n+            if (i < tokens.Count - 1 && char.IsUpper(token[0]))\n+            {\n+                var multiWordEntity = TryExtractMultiWordEntity(tokens, i, out int consumed);\n+                if (multiWordEntity != null)\n+                {\n+                    entities.Add(multiWordEntity);\n+                    i += consumed;\n+                    continue;\n+                }\n+            }\n+            \n+            // Check for single-word entities\n+            var entity = TryExtractSingleEntity(token, i);\n+            if (entity != null)\n+            {\n+                entities.Add(entity);\n+            }\n+            \n+            i++;\n+        }\n+        \n+        return entities;\n+    }",
    "path": "src/RetrievalAugmentedGeneration/NER/BiLSTMCRF_NER.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Add bounds check before accessing `token[0]`.**\n\nLine 134 accesses `token[0]` without verifying the token is non-empty. While the current tokenization with `StringSplitOptions.RemoveEmptyEntries` should prevent empty tokens, this creates a fragile dependency. An empty token would cause an `IndexOutOfRangeException`.\n\n\n\nApply this diff to add defensive bounds checking:\n\n```diff\n         while (i < tokens.Count)\n         {\n             var token = tokens[i];\n+            \n+            if (string.IsNullOrEmpty(token))\n+            {\n+                i++;\n+                continue;\n+            }\n             \n             // Check for multi-word entities first\n             if (i < tokens.Count - 1 && char.IsUpper(token[0]))\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    private List<ExtractedEntity> ExtractEntitiesFromTokens(List<string> tokens, string originalText)\n    {\n        var entities = new List<ExtractedEntity>();\n        int i = 0;\n        \n        while (i < tokens.Count)\n        {\n            var token = tokens[i];\n            \n            if (string.IsNullOrEmpty(token))\n            {\n                i++;\n                continue;\n            }\n            \n            // Check for multi-word entities first\n            if (i < tokens.Count - 1 && char.IsUpper(token[0]))\n            {\n                var multiWordEntity = TryExtractMultiWordEntity(tokens, i, out int consumed);\n                if (multiWordEntity != null)\n                {\n                    entities.Add(multiWordEntity);\n                    i += consumed;\n                    continue;\n                }\n            }\n            \n            // Check for single-word entities\n            var entity = TryExtractSingleEntity(token, i);\n            if (entity != null)\n            {\n                entities.Add(entity);\n            }\n            \n            i++;\n        }\n        \n        return entities;\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/NER/BiLSTMCRF_NER.cs around lines 124 to\n156, the code accesses token[0] on line 134 without ensuring the token is\nnon-empty; add a defensive bounds check (e.g., verify token is not null or empty\nand token.Length > 0 or use !string.IsNullOrEmpty(token)) before calling\nchar.IsUpper(token[0]) so empty tokens cannot cause an IndexOutOfRangeException,\nand keep the rest of the logic unchanged.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:49:52Z",
    "updated_at": "2025-11-05T02:49:58Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669701",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669701"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669701"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669701/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 124,
    "original_start_line": 124,
    "start_side": "RIGHT",
    "line": 156,
    "original_line": 156,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 156,
    "position": 156,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669706",
    "pull_request_review_id": 3419457803,
    "id": 2492669706,
    "node_id": "PRRC_kwDOKSXUF86Ukx8K",
    "diff_hunk": "@@ -0,0 +1,301 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.NER;\n+\n+/// <summary>\n+/// Production-ready Named Entity Recognition model using pattern matching and heuristics.\n+/// </summary>\n+/// <remarks>\n+/// <para>\n+/// This NER model uses intelligent pattern matching, capitalization analysis, and context clues\n+/// to identify named entities. While not ML-based (yet), it provides production-ready accuracy\n+/// for common entity types through carefully crafted rules.\n+/// </para>\n+/// <para><b>For Beginners:</b> This model identifies entities in text (people, places, organizations, etc.).\n+/// \n+/// Think of it like highlighting important information in a document:\n+/// - Input: \"Albert Einstein worked at Princeton University in New Jersey.\"\n+/// - Output: [Albert Einstein]=PERSON, [Princeton University]=ORGANIZATION, [New Jersey]=LOCATION\n+/// \n+/// How it works:\n+/// 1. Analyzes capitalization patterns\n+/// 2. Looks for entity indicators (Corp, University, City, etc.)\n+/// 3. Uses word lists of common names/places\n+/// 4. Detects multi-word entities (first + last names, etc.)\n+/// 5. Applies context rules (person names before \"works at\", etc.)\n+/// \n+/// Entity types supported:\n+/// - PERSON: Names of people\n+/// - ORGANIZATION: Companies, institutions\n+/// - LOCATION: Cities, countries, places\n+/// - DATE: Temporal expressions\n+/// \n+/// Future: Will be upgraded to BiLSTM-CRF neural network model for higher accuracy.\n+/// </para>\n+/// </remarks>\n+public class NamedEntityRecognizer\n+{\n+    private readonly Dictionary<string, string> _commonNames;\n+    private readonly Dictionary<string, string> _commonLocations;\n+    private readonly Dictionary<string, string> _commonOrganizations;\n+    \n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"NamedEntityRecognizer\"/> class.\n+    /// </summary>\n+    public NamedEntityRecognizer()\n+    {\n+        _commonNames = InitializeCommonNames();\n+        _commonLocations = InitializeCommonLocations();\n+        _commonOrganizations = InitializeCommonOrganizations();\n+    }\n+    \n+    private Dictionary<string, string> InitializeCommonNames()\n+    {\n+        var names = new Dictionary<string, string>(StringComparer.OrdinalIgnoreCase);\n+        foreach (var name in new[] {\n+            \"John\", \"Mary\", \"James\", \"Patricia\", \"Robert\", \"Jennifer\", \"Michael\", \"Linda\",\n+            \"William\", \"Elizabeth\", \"David\", \"Barbara\", \"Richard\", \"Susan\", \"Joseph\", \"Jessica\",\n+            \"Thomas\", \"Sarah\", \"Charles\", \"Karen\", \"Christopher\", \"Nancy\", \"Daniel\", \"Lisa\",\n+            \"Albert\", \"Isaac\", \"Marie\", \"Stephen\", \"Alan\", \"Richard\", \"Marie\", \"Ada\",\n+            \"Grace\", \"Linus\", \"Dennis\", \"Ken\", \"Brian\", \"Donald\", \"Elon\", \"Jeff\", \"Bill\"\n+        })\n+        {\n+            names[name] = \"PERSON\";\n+        }\n+        return names;\n+    }\n+    \n+    private Dictionary<string, string> InitializeCommonLocations()\n+    {\n+        var locations = new Dictionary<string, string>(StringComparer.OrdinalIgnoreCase);\n+        foreach (var location in new[] {\n+            \"America\", \"Europe\", \"Asia\", \"Africa\", \"Australia\", \"Antarctica\",\n+            \"California\", \"Texas\", \"Florida\", \"NewYork\", \"Illinois\", \"Pennsylvania\",\n+            \"Seattle\", \"Boston\", \"Chicago\", \"Houston\", \"Phoenix\", \"Philadelphia\",\n+            \"Denver\", \"Washington\", \"Atlanta\", \"Miami\", \"Dallas\", \"Austin\",\n+            \"London\", \"Paris\", \"Tokyo\", \"Beijing\", \"Moscow\", \"Berlin\", \"Rome\",\n+            \"Princeton\", \"Cambridge\", \"Oxford\", \"Stanford\", \"Harvard\", \"MIT\"\n+        })\n+        {\n+            locations[location] = \"LOCATION\";\n+        }\n+        return locations;\n+    }\n+    \n+    private Dictionary<string, string> InitializeCommonOrganizations()\n+    {\n+        var orgs = new Dictionary<string, string>(StringComparer.OrdinalIgnoreCase);\n+        foreach (var org in new[] {\n+            \"Microsoft\", \"Apple\", \"Google\", \"Amazon\", \"Facebook\", \"Tesla\",\n+            \"IBM\", \"Intel\", \"Oracle\", \"Adobe\", \"Netflix\", \"Uber\", \"Twitter\",\n+            \"NASA\", \"MIT\", \"Harvard\", \"Stanford\", \"Princeton\", \"Yale\", \"Oxford\"\n+        })\n+        {\n+            orgs[org] = \"ORGANIZATION\";\n+        }\n+        return orgs;\n+    }\n+    \n+    /// <summary>\n+    /// Extracts named entities from text.\n+    /// </summary>\n+    /// <param name=\"text\">The input text to process.</param>\n+    /// <returns>List of extracted entities with their types and spans.</returns>\n+    public List<ExtractedEntity> ExtractEntities(string text)\n+    {\n+        if (string.IsNullOrWhiteSpace(text))\n+            return new List<ExtractedEntity>();\n+            \n+        var tokens = TokenizeText(text);\n+        if (tokens.Count == 0)\n+            return new List<ExtractedEntity>();\n+        \n+        return ExtractEntitiesFromTokens(tokens, text);\n+    }\n+    \n+    private List<string> TokenizeText(string text)\n+    {\n+        return text.Split(new[] { ' ', '\\t', '\\n', '\\r' }, StringSplitOptions.RemoveEmptyEntries)\n+                   .ToList();\n+    }\n+    \n+    private List<ExtractedEntity> ExtractEntitiesFromTokens(List<string> tokens, string originalText)\n+    {\n+        var entities = new List<ExtractedEntity>();\n+        int i = 0;\n+        \n+        while (i < tokens.Count)\n+        {\n+            var token = tokens[i];\n+            \n+            // Check for multi-word entities first\n+            if (i < tokens.Count - 1 && char.IsUpper(token[0]))\n+            {\n+                var multiWordEntity = TryExtractMultiWordEntity(tokens, i, out int consumed);\n+                if (multiWordEntity != null)\n+                {\n+                    entities.Add(multiWordEntity);\n+                    i += consumed;\n+                    continue;\n+                }\n+            }\n+            \n+            // Check for single-word entities\n+            var entity = TryExtractSingleEntity(token, i);\n+            if (entity != null)\n+            {\n+                entities.Add(entity);\n+            }\n+            \n+            i++;\n+        }\n+        \n+        return entities;\n+    }\n+    \n+    private ExtractedEntity? TryExtractMultiWordEntity(List<string> tokens, int startIndex, out int consumed)\n+    {\n+        consumed = 1;\n+        var entityTokens = new List<string> { tokens[startIndex] };\n+        var entityType = PredictEntityType(tokens[startIndex]);\n+        \n+        // Look ahead for continuation\n+        for (int i = startIndex + 1; i < Math.Min(startIndex + 4, tokens.Count); i++)\n+        {\n+            var nextToken = tokens[i];\n+            \n+            // Stop at lowercase words (unless connectors)\n+            if (!char.IsUpper(nextToken[0]) && !IsConnector(nextToken))\n+                break;\n+                \n+            // Stop at different entity type indicators\n+            var nextType = PredictEntityType(nextToken);\n+            if (nextType != entityType && nextType != \"UNKNOWN\")\n+                break;\n+                \n+            entityTokens.Add(nextToken);\n+            consumed++;\n+        }\n+        \n+        if (entityTokens.Count > 1 || entityType != \"UNKNOWN\")\n+        {\n+            return new ExtractedEntity\n+            {\n+                Text = string.Join(\" \", entityTokens),\n+                Type = entityType,\n+                StartIndex = startIndex,\n+                EndIndex = startIndex + consumed - 1,\n+                Confidence = 0.85\n+            };\n+        }\n+        \n+        consumed = 1;\n+        return null;\n+    }",
    "path": "src/RetrievalAugmentedGeneration/NER/BiLSTMCRF_NER.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Add bounds check before accessing `nextToken[0]`.**\n\nLine 170 accesses `nextToken[0]` without verifying it's non-empty, risking an `IndexOutOfRangeException`. Apply the same defensive check as recommended for the main loop.\n\n\n\nApply this diff:\n\n```diff\n         for (int i = startIndex + 1; i < Math.Min(startIndex + 4, tokens.Count); i++)\n         {\n             var nextToken = tokens[i];\n+            \n+            if (string.IsNullOrEmpty(nextToken))\n+                break;\n             \n             // Stop at lowercase words (unless connectors)\n             if (!char.IsUpper(nextToken[0]) && !IsConnector(nextToken))\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/NER/BiLSTMCRF_NER.cs around lines 158 to\n196, the lookΓÇæahead loop accesses nextToken[0] without ensuring nextToken is\nnon-empty; add a defensive bounds check (e.g., if\nstring.IsNullOrEmpty(nextToken) or nextToken.Length == 0) before calling\nnextToken[0], and shortΓÇæcircuit to break the loop (or treat empty as\nnonΓÇæconnector) so IsConnector(nextToken) and char.IsUpper are only evaluated on\nnonΓÇæempty strings, mirroring the same defensive check used in the main loop.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:49:52Z",
    "updated_at": "2025-11-05T02:49:58Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669706",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669706"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669706"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669706/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 158,
    "original_start_line": 158,
    "start_side": "RIGHT",
    "line": 196,
    "original_line": 196,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 196,
    "position": 196,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669710",
    "pull_request_review_id": 3419457803,
    "id": 2492669710,
    "node_id": "PRRC_kwDOKSXUF86Ukx8O",
    "diff_hunk": "@@ -0,0 +1,301 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.NER;\n+\n+/// <summary>\n+/// Production-ready Named Entity Recognition model using pattern matching and heuristics.\n+/// </summary>\n+/// <remarks>\n+/// <para>\n+/// This NER model uses intelligent pattern matching, capitalization analysis, and context clues\n+/// to identify named entities. While not ML-based (yet), it provides production-ready accuracy\n+/// for common entity types through carefully crafted rules.\n+/// </para>\n+/// <para><b>For Beginners:</b> This model identifies entities in text (people, places, organizations, etc.).\n+/// \n+/// Think of it like highlighting important information in a document:\n+/// - Input: \"Albert Einstein worked at Princeton University in New Jersey.\"\n+/// - Output: [Albert Einstein]=PERSON, [Princeton University]=ORGANIZATION, [New Jersey]=LOCATION\n+/// \n+/// How it works:\n+/// 1. Analyzes capitalization patterns\n+/// 2. Looks for entity indicators (Corp, University, City, etc.)\n+/// 3. Uses word lists of common names/places\n+/// 4. Detects multi-word entities (first + last names, etc.)\n+/// 5. Applies context rules (person names before \"works at\", etc.)\n+/// \n+/// Entity types supported:\n+/// - PERSON: Names of people\n+/// - ORGANIZATION: Companies, institutions\n+/// - LOCATION: Cities, countries, places\n+/// - DATE: Temporal expressions\n+/// \n+/// Future: Will be upgraded to BiLSTM-CRF neural network model for higher accuracy.\n+/// </para>\n+/// </remarks>\n+public class NamedEntityRecognizer\n+{\n+    private readonly Dictionary<string, string> _commonNames;\n+    private readonly Dictionary<string, string> _commonLocations;\n+    private readonly Dictionary<string, string> _commonOrganizations;\n+    \n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"NamedEntityRecognizer\"/> class.\n+    /// </summary>\n+    public NamedEntityRecognizer()\n+    {\n+        _commonNames = InitializeCommonNames();\n+        _commonLocations = InitializeCommonLocations();\n+        _commonOrganizations = InitializeCommonOrganizations();\n+    }\n+    \n+    private Dictionary<string, string> InitializeCommonNames()\n+    {\n+        var names = new Dictionary<string, string>(StringComparer.OrdinalIgnoreCase);\n+        foreach (var name in new[] {\n+            \"John\", \"Mary\", \"James\", \"Patricia\", \"Robert\", \"Jennifer\", \"Michael\", \"Linda\",\n+            \"William\", \"Elizabeth\", \"David\", \"Barbara\", \"Richard\", \"Susan\", \"Joseph\", \"Jessica\",\n+            \"Thomas\", \"Sarah\", \"Charles\", \"Karen\", \"Christopher\", \"Nancy\", \"Daniel\", \"Lisa\",\n+            \"Albert\", \"Isaac\", \"Marie\", \"Stephen\", \"Alan\", \"Richard\", \"Marie\", \"Ada\",\n+            \"Grace\", \"Linus\", \"Dennis\", \"Ken\", \"Brian\", \"Donald\", \"Elon\", \"Jeff\", \"Bill\"\n+        })\n+        {\n+            names[name] = \"PERSON\";\n+        }\n+        return names;\n+    }\n+    \n+    private Dictionary<string, string> InitializeCommonLocations()\n+    {\n+        var locations = new Dictionary<string, string>(StringComparer.OrdinalIgnoreCase);\n+        foreach (var location in new[] {\n+            \"America\", \"Europe\", \"Asia\", \"Africa\", \"Australia\", \"Antarctica\",\n+            \"California\", \"Texas\", \"Florida\", \"NewYork\", \"Illinois\", \"Pennsylvania\",\n+            \"Seattle\", \"Boston\", \"Chicago\", \"Houston\", \"Phoenix\", \"Philadelphia\",\n+            \"Denver\", \"Washington\", \"Atlanta\", \"Miami\", \"Dallas\", \"Austin\",\n+            \"London\", \"Paris\", \"Tokyo\", \"Beijing\", \"Moscow\", \"Berlin\", \"Rome\",\n+            \"Princeton\", \"Cambridge\", \"Oxford\", \"Stanford\", \"Harvard\", \"MIT\"\n+        })\n+        {\n+            locations[location] = \"LOCATION\";\n+        }\n+        return locations;\n+    }\n+    \n+    private Dictionary<string, string> InitializeCommonOrganizations()\n+    {\n+        var orgs = new Dictionary<string, string>(StringComparer.OrdinalIgnoreCase);\n+        foreach (var org in new[] {\n+            \"Microsoft\", \"Apple\", \"Google\", \"Amazon\", \"Facebook\", \"Tesla\",\n+            \"IBM\", \"Intel\", \"Oracle\", \"Adobe\", \"Netflix\", \"Uber\", \"Twitter\",\n+            \"NASA\", \"MIT\", \"Harvard\", \"Stanford\", \"Princeton\", \"Yale\", \"Oxford\"\n+        })\n+        {\n+            orgs[org] = \"ORGANIZATION\";\n+        }\n+        return orgs;\n+    }\n+    \n+    /// <summary>\n+    /// Extracts named entities from text.\n+    /// </summary>\n+    /// <param name=\"text\">The input text to process.</param>\n+    /// <returns>List of extracted entities with their types and spans.</returns>\n+    public List<ExtractedEntity> ExtractEntities(string text)\n+    {\n+        if (string.IsNullOrWhiteSpace(text))\n+            return new List<ExtractedEntity>();\n+            \n+        var tokens = TokenizeText(text);\n+        if (tokens.Count == 0)\n+            return new List<ExtractedEntity>();\n+        \n+        return ExtractEntitiesFromTokens(tokens, text);\n+    }\n+    \n+    private List<string> TokenizeText(string text)\n+    {\n+        return text.Split(new[] { ' ', '\\t', '\\n', '\\r' }, StringSplitOptions.RemoveEmptyEntries)\n+                   .ToList();\n+    }\n+    \n+    private List<ExtractedEntity> ExtractEntitiesFromTokens(List<string> tokens, string originalText)\n+    {\n+        var entities = new List<ExtractedEntity>();\n+        int i = 0;\n+        \n+        while (i < tokens.Count)\n+        {\n+            var token = tokens[i];\n+            \n+            // Check for multi-word entities first\n+            if (i < tokens.Count - 1 && char.IsUpper(token[0]))\n+            {\n+                var multiWordEntity = TryExtractMultiWordEntity(tokens, i, out int consumed);\n+                if (multiWordEntity != null)\n+                {\n+                    entities.Add(multiWordEntity);\n+                    i += consumed;\n+                    continue;\n+                }\n+            }\n+            \n+            // Check for single-word entities\n+            var entity = TryExtractSingleEntity(token, i);\n+            if (entity != null)\n+            {\n+                entities.Add(entity);\n+            }\n+            \n+            i++;\n+        }\n+        \n+        return entities;\n+    }\n+    \n+    private ExtractedEntity? TryExtractMultiWordEntity(List<string> tokens, int startIndex, out int consumed)\n+    {\n+        consumed = 1;\n+        var entityTokens = new List<string> { tokens[startIndex] };\n+        var entityType = PredictEntityType(tokens[startIndex]);\n+        \n+        // Look ahead for continuation\n+        for (int i = startIndex + 1; i < Math.Min(startIndex + 4, tokens.Count); i++)\n+        {\n+            var nextToken = tokens[i];\n+            \n+            // Stop at lowercase words (unless connectors)\n+            if (!char.IsUpper(nextToken[0]) && !IsConnector(nextToken))\n+                break;\n+                \n+            // Stop at different entity type indicators\n+            var nextType = PredictEntityType(nextToken);\n+            if (nextType != entityType && nextType != \"UNKNOWN\")\n+                break;\n+                \n+            entityTokens.Add(nextToken);\n+            consumed++;\n+        }\n+        \n+        if (entityTokens.Count > 1 || entityType != \"UNKNOWN\")\n+        {\n+            return new ExtractedEntity\n+            {\n+                Text = string.Join(\" \", entityTokens),\n+                Type = entityType,\n+                StartIndex = startIndex,\n+                EndIndex = startIndex + consumed - 1,\n+                Confidence = 0.85\n+            };\n+        }\n+        \n+        consumed = 1;\n+        return null;\n+    }\n+    \n+    private bool IsConnector(string token)\n+    {\n+        return token.Equals(\"of\", StringComparison.OrdinalIgnoreCase) ||\n+               token.Equals(\"the\", StringComparison.OrdinalIgnoreCase) ||\n+               token.Equals(\"and\", StringComparison.OrdinalIgnoreCase);\n+    }\n+    \n+    private ExtractedEntity? TryExtractSingleEntity(string token, int index)\n+    {\n+        var entityType = PredictEntityType(token);\n+        \n+        if (entityType != \"UNKNOWN\")\n+        {\n+            return new ExtractedEntity\n+            {\n+                Text = token,\n+                Type = entityType,\n+                StartIndex = index,\n+                EndIndex = index,\n+                Confidence = 0.75\n+            };\n+        }\n+        \n+        return null;\n+    }\n+    \n+    private string PredictEntityType(string token)\n+    {\n+        // Check dictionaries first\n+        if (_commonNames.ContainsKey(token))\n+            return \"PERSON\";\n+        if (_commonLocations.ContainsKey(token))\n+            return \"LOCATION\";\n+        if (_commonOrganizations.ContainsKey(token))\n+            return \"ORGANIZATION\";\n+        \n+        // Pattern-based detection\n+        if (!char.IsUpper(token[0]))\n+            return \"UNKNOWN\";\n+        \n+        // Organization indicators\n+        if (token.EndsWith(\"Corp\") || token.EndsWith(\"Inc\") || token.EndsWith(\"LLC\") ||\n+            token.EndsWith(\"Ltd\") || token.EndsWith(\"GmbH\") || token.EndsWith(\"SA\") ||\n+            token.Contains(\"University\") || token.Contains(\"Institute\") || token.Contains(\"College\") ||\n+            token.Contains(\"Foundation\") || token.Contains(\"Association\"))\n+        {\n+            return \"ORGANIZATION\";\n+        }\n+        \n+        // Location indicators\n+        if (token.EndsWith(\"City\") || token.EndsWith(\"Town\") || token.EndsWith(\"Village\") ||\n+            token.EndsWith(\"County\") || token.EndsWith(\"State\") || token.EndsWith(\"Province\") ||\n+            token.EndsWith(\"Country\") || token.EndsWith(\"Island\") || token.EndsWith(\"Mountain\"))\n+        {\n+            return \"LOCATION\";\n+        }\n+        \n+        // Person name patterns (capital first letter, common suffixes)\n+        if (token.EndsWith(\"son\") || token.EndsWith(\"sen\") || token.EndsWith(\"berg\") ||\n+            token.EndsWith(\"stein\") || token.EndsWith(\"man\") || token.EndsWith(\"ton\"))\n+        {\n+            return \"PERSON\";\n+        }\n+        \n+        // Default to PERSON for capitalized words (most common)\n+        return token.Length > 2 ? \"PERSON\" : \"UNKNOWN\";\n+    }",
    "path": "src/RetrievalAugmentedGeneration/NER/BiLSTMCRF_NER.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Add bounds check before accessing `token[0]` and reconsider default heuristic.**\n\nLine 235 accesses `token[0]` without checking if the token is empty, risking an `IndexOutOfRangeException`.\n\nAdditionally, the default heuristic on line 263 classifies all capitalized words longer than 2 characters as \"PERSON\", which incorrectly labels acronyms and abbreviations (e.g., \"USA\", \"API\", \"HTTP\", \"SQL\").\n\n\n\n\nApply this diff to add bounds checking:\n\n```diff\n     private string PredictEntityType(string token)\n     {\n+        if (string.IsNullOrEmpty(token))\n+            return \"UNKNOWN\";\n+            \n         // Check dictionaries first\n         if (_commonNames.ContainsKey(token))\n```\n\nConsider refining the default heuristic:\n\n```diff\n         // Default to PERSON for capitalized words (most common)\n-        return token.Length > 2 ? \"PERSON\" : \"UNKNOWN\";\n+        // Avoid classifying short all-caps words (likely acronyms) as PERSON\n+        if (token.Length <= 3 && token.All(char.IsUpper))\n+            return \"UNKNOWN\";\n+        return token.Length > 2 ? \"PERSON\" : \"UNKNOWN\";\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    private string PredictEntityType(string token)\n    {\n        if (string.IsNullOrEmpty(token))\n            return \"UNKNOWN\";\n            \n        // Check dictionaries first\n        if (_commonNames.ContainsKey(token))\n            return \"PERSON\";\n        if (_commonLocations.ContainsKey(token))\n            return \"LOCATION\";\n        if (_commonOrganizations.ContainsKey(token))\n            return \"ORGANIZATION\";\n        \n        // Pattern-based detection\n        if (!char.IsUpper(token[0]))\n            return \"UNKNOWN\";\n        \n        // Organization indicators\n        if (token.EndsWith(\"Corp\") || token.EndsWith(\"Inc\") || token.EndsWith(\"LLC\") ||\n            token.EndsWith(\"Ltd\") || token.EndsWith(\"GmbH\") || token.EndsWith(\"SA\") ||\n            token.Contains(\"University\") || token.Contains(\"Institute\") || token.Contains(\"College\") ||\n            token.Contains(\"Foundation\") || token.Contains(\"Association\"))\n        {\n            return \"ORGANIZATION\";\n        }\n        \n        // Location indicators\n        if (token.EndsWith(\"City\") || token.EndsWith(\"Town\") || token.EndsWith(\"Village\") ||\n            token.EndsWith(\"County\") || token.EndsWith(\"State\") || token.EndsWith(\"Province\") ||\n            token.EndsWith(\"Country\") || token.EndsWith(\"Island\") || token.EndsWith(\"Mountain\"))\n        {\n            return \"LOCATION\";\n        }\n        \n        // Person name patterns (capital first letter, common suffixes)\n        if (token.EndsWith(\"son\") || token.EndsWith(\"sen\") || token.EndsWith(\"berg\") ||\n            token.EndsWith(\"stein\") || token.EndsWith(\"man\") || token.EndsWith(\"ton\"))\n        {\n            return \"PERSON\";\n        }\n        \n        // Default to PERSON for capitalized words (most common)\n        // Avoid classifying short all-caps words (likely acronyms) as PERSON\n        if (token.Length <= 3 && token.All(char.IsUpper))\n            return \"UNKNOWN\";\n        return token.Length > 2 ? \"PERSON\" : \"UNKNOWN\";\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/NER/BiLSTMCRF_NER.cs around lines 224 to\n264, add a null/empty and trim guard at the top of PredictEntityType (return\n\"UNKNOWN\" for null/empty tokens) and only access token[0] after confirming\ntoken.Length>0; replace the simple capitalized-word default by excluding\nall-uppercase acronyms, tokens containing digits or punctuation, and short\nacronyms: after the pattern checks, treat tokens as PERSON only if length>2,\nfirst letter is uppercase, and the token is not all upper-case, does not contain\ndigits or '.'/'-' etc.; otherwise return \"UNKNOWN\" (or \"ORGANIZATION\" for tokens\nmatching organization-specific patterns). Ensure you trim the token before\nchecks.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:49:52Z",
    "updated_at": "2025-11-05T02:49:58Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669710",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669710"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669710"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669710/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 224,
    "original_start_line": 224,
    "start_side": "RIGHT",
    "line": 264,
    "original_line": 264,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 264,
    "position": 264,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669719",
    "pull_request_review_id": 3419457803,
    "id": 2492669719,
    "node_id": "PRRC_kwDOKSXUF86Ukx8X",
    "diff_hunk": "@@ -0,0 +1,301 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.NER;\n+\n+/// <summary>\n+/// Production-ready Named Entity Recognition model using pattern matching and heuristics.\n+/// </summary>\n+/// <remarks>\n+/// <para>\n+/// This NER model uses intelligent pattern matching, capitalization analysis, and context clues\n+/// to identify named entities. While not ML-based (yet), it provides production-ready accuracy\n+/// for common entity types through carefully crafted rules.\n+/// </para>\n+/// <para><b>For Beginners:</b> This model identifies entities in text (people, places, organizations, etc.).\n+/// \n+/// Think of it like highlighting important information in a document:\n+/// - Input: \"Albert Einstein worked at Princeton University in New Jersey.\"\n+/// - Output: [Albert Einstein]=PERSON, [Princeton University]=ORGANIZATION, [New Jersey]=LOCATION\n+/// \n+/// How it works:\n+/// 1. Analyzes capitalization patterns\n+/// 2. Looks for entity indicators (Corp, University, City, etc.)\n+/// 3. Uses word lists of common names/places\n+/// 4. Detects multi-word entities (first + last names, etc.)\n+/// 5. Applies context rules (person names before \"works at\", etc.)\n+/// \n+/// Entity types supported:\n+/// - PERSON: Names of people\n+/// - ORGANIZATION: Companies, institutions\n+/// - LOCATION: Cities, countries, places\n+/// - DATE: Temporal expressions\n+/// \n+/// Future: Will be upgraded to BiLSTM-CRF neural network model for higher accuracy.\n+/// </para>\n+/// </remarks>\n+public class NamedEntityRecognizer\n+{\n+    private readonly Dictionary<string, string> _commonNames;\n+    private readonly Dictionary<string, string> _commonLocations;\n+    private readonly Dictionary<string, string> _commonOrganizations;\n+    \n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"NamedEntityRecognizer\"/> class.\n+    /// </summary>\n+    public NamedEntityRecognizer()\n+    {\n+        _commonNames = InitializeCommonNames();\n+        _commonLocations = InitializeCommonLocations();\n+        _commonOrganizations = InitializeCommonOrganizations();\n+    }\n+    \n+    private Dictionary<string, string> InitializeCommonNames()\n+    {\n+        var names = new Dictionary<string, string>(StringComparer.OrdinalIgnoreCase);\n+        foreach (var name in new[] {\n+            \"John\", \"Mary\", \"James\", \"Patricia\", \"Robert\", \"Jennifer\", \"Michael\", \"Linda\",\n+            \"William\", \"Elizabeth\", \"David\", \"Barbara\", \"Richard\", \"Susan\", \"Joseph\", \"Jessica\",\n+            \"Thomas\", \"Sarah\", \"Charles\", \"Karen\", \"Christopher\", \"Nancy\", \"Daniel\", \"Lisa\",\n+            \"Albert\", \"Isaac\", \"Marie\", \"Stephen\", \"Alan\", \"Richard\", \"Marie\", \"Ada\",\n+            \"Grace\", \"Linus\", \"Dennis\", \"Ken\", \"Brian\", \"Donald\", \"Elon\", \"Jeff\", \"Bill\"\n+        })\n+        {\n+            names[name] = \"PERSON\";\n+        }\n+        return names;\n+    }\n+    \n+    private Dictionary<string, string> InitializeCommonLocations()\n+    {\n+        var locations = new Dictionary<string, string>(StringComparer.OrdinalIgnoreCase);\n+        foreach (var location in new[] {\n+            \"America\", \"Europe\", \"Asia\", \"Africa\", \"Australia\", \"Antarctica\",\n+            \"California\", \"Texas\", \"Florida\", \"NewYork\", \"Illinois\", \"Pennsylvania\",\n+            \"Seattle\", \"Boston\", \"Chicago\", \"Houston\", \"Phoenix\", \"Philadelphia\",\n+            \"Denver\", \"Washington\", \"Atlanta\", \"Miami\", \"Dallas\", \"Austin\",\n+            \"London\", \"Paris\", \"Tokyo\", \"Beijing\", \"Moscow\", \"Berlin\", \"Rome\",\n+            \"Princeton\", \"Cambridge\", \"Oxford\", \"Stanford\", \"Harvard\", \"MIT\"\n+        })\n+        {\n+            locations[location] = \"LOCATION\";\n+        }\n+        return locations;\n+    }\n+    \n+    private Dictionary<string, string> InitializeCommonOrganizations()\n+    {\n+        var orgs = new Dictionary<string, string>(StringComparer.OrdinalIgnoreCase);\n+        foreach (var org in new[] {\n+            \"Microsoft\", \"Apple\", \"Google\", \"Amazon\", \"Facebook\", \"Tesla\",\n+            \"IBM\", \"Intel\", \"Oracle\", \"Adobe\", \"Netflix\", \"Uber\", \"Twitter\",\n+            \"NASA\", \"MIT\", \"Harvard\", \"Stanford\", \"Princeton\", \"Yale\", \"Oxford\"\n+        })\n+        {\n+            orgs[org] = \"ORGANIZATION\";\n+        }\n+        return orgs;\n+    }\n+    \n+    /// <summary>\n+    /// Extracts named entities from text.\n+    /// </summary>\n+    /// <param name=\"text\">The input text to process.</param>\n+    /// <returns>List of extracted entities with their types and spans.</returns>\n+    public List<ExtractedEntity> ExtractEntities(string text)\n+    {\n+        if (string.IsNullOrWhiteSpace(text))\n+            return new List<ExtractedEntity>();\n+            \n+        var tokens = TokenizeText(text);\n+        if (tokens.Count == 0)\n+            return new List<ExtractedEntity>();\n+        \n+        return ExtractEntitiesFromTokens(tokens, text);\n+    }\n+    \n+    private List<string> TokenizeText(string text)\n+    {\n+        return text.Split(new[] { ' ', '\\t', '\\n', '\\r' }, StringSplitOptions.RemoveEmptyEntries)\n+                   .ToList();\n+    }\n+    \n+    private List<ExtractedEntity> ExtractEntitiesFromTokens(List<string> tokens, string originalText)\n+    {\n+        var entities = new List<ExtractedEntity>();\n+        int i = 0;\n+        \n+        while (i < tokens.Count)\n+        {\n+            var token = tokens[i];\n+            \n+            // Check for multi-word entities first\n+            if (i < tokens.Count - 1 && char.IsUpper(token[0]))\n+            {\n+                var multiWordEntity = TryExtractMultiWordEntity(tokens, i, out int consumed);\n+                if (multiWordEntity != null)\n+                {\n+                    entities.Add(multiWordEntity);\n+                    i += consumed;\n+                    continue;\n+                }\n+            }\n+            \n+            // Check for single-word entities\n+            var entity = TryExtractSingleEntity(token, i);\n+            if (entity != null)\n+            {\n+                entities.Add(entity);\n+            }\n+            \n+            i++;\n+        }\n+        \n+        return entities;\n+    }\n+    \n+    private ExtractedEntity? TryExtractMultiWordEntity(List<string> tokens, int startIndex, out int consumed)\n+    {\n+        consumed = 1;\n+        var entityTokens = new List<string> { tokens[startIndex] };\n+        var entityType = PredictEntityType(tokens[startIndex]);\n+        \n+        // Look ahead for continuation\n+        for (int i = startIndex + 1; i < Math.Min(startIndex + 4, tokens.Count); i++)\n+        {\n+            var nextToken = tokens[i];\n+            \n+            // Stop at lowercase words (unless connectors)\n+            if (!char.IsUpper(nextToken[0]) && !IsConnector(nextToken))\n+                break;\n+                \n+            // Stop at different entity type indicators\n+            var nextType = PredictEntityType(nextToken);\n+            if (nextType != entityType && nextType != \"UNKNOWN\")\n+                break;\n+                \n+            entityTokens.Add(nextToken);\n+            consumed++;\n+        }\n+        \n+        if (entityTokens.Count > 1 || entityType != \"UNKNOWN\")\n+        {\n+            return new ExtractedEntity\n+            {\n+                Text = string.Join(\" \", entityTokens),\n+                Type = entityType,\n+                StartIndex = startIndex,\n+                EndIndex = startIndex + consumed - 1,\n+                Confidence = 0.85\n+            };\n+        }\n+        \n+        consumed = 1;\n+        return null;\n+    }\n+    \n+    private bool IsConnector(string token)\n+    {\n+        return token.Equals(\"of\", StringComparison.OrdinalIgnoreCase) ||\n+               token.Equals(\"the\", StringComparison.OrdinalIgnoreCase) ||\n+               token.Equals(\"and\", StringComparison.OrdinalIgnoreCase);\n+    }\n+    \n+    private ExtractedEntity? TryExtractSingleEntity(string token, int index)\n+    {\n+        var entityType = PredictEntityType(token);\n+        \n+        if (entityType != \"UNKNOWN\")\n+        {\n+            return new ExtractedEntity\n+            {\n+                Text = token,\n+                Type = entityType,\n+                StartIndex = index,\n+                EndIndex = index,\n+                Confidence = 0.75\n+            };\n+        }\n+        \n+        return null;\n+    }\n+    \n+    private string PredictEntityType(string token)\n+    {\n+        // Check dictionaries first\n+        if (_commonNames.ContainsKey(token))\n+            return \"PERSON\";\n+        if (_commonLocations.ContainsKey(token))\n+            return \"LOCATION\";\n+        if (_commonOrganizations.ContainsKey(token))\n+            return \"ORGANIZATION\";\n+        \n+        // Pattern-based detection\n+        if (!char.IsUpper(token[0]))\n+            return \"UNKNOWN\";\n+        \n+        // Organization indicators\n+        if (token.EndsWith(\"Corp\") || token.EndsWith(\"Inc\") || token.EndsWith(\"LLC\") ||\n+            token.EndsWith(\"Ltd\") || token.EndsWith(\"GmbH\") || token.EndsWith(\"SA\") ||\n+            token.Contains(\"University\") || token.Contains(\"Institute\") || token.Contains(\"College\") ||\n+            token.Contains(\"Foundation\") || token.Contains(\"Association\"))\n+        {\n+            return \"ORGANIZATION\";\n+        }\n+        \n+        // Location indicators\n+        if (token.EndsWith(\"City\") || token.EndsWith(\"Town\") || token.EndsWith(\"Village\") ||\n+            token.EndsWith(\"County\") || token.EndsWith(\"State\") || token.EndsWith(\"Province\") ||\n+            token.EndsWith(\"Country\") || token.EndsWith(\"Island\") || token.EndsWith(\"Mountain\"))\n+        {\n+            return \"LOCATION\";\n+        }\n+        \n+        // Person name patterns (capital first letter, common suffixes)\n+        if (token.EndsWith(\"son\") || token.EndsWith(\"sen\") || token.EndsWith(\"berg\") ||\n+            token.EndsWith(\"stein\") || token.EndsWith(\"man\") || token.EndsWith(\"ton\"))\n+        {\n+            return \"PERSON\";\n+        }\n+        \n+        // Default to PERSON for capitalized words (most common)\n+        return token.Length > 2 ? \"PERSON\" : \"UNKNOWN\";\n+    }\n+}\n+\n+/// <summary>\n+/// Represents an entity extracted from text.\n+/// </summary>\n+public class ExtractedEntity\n+{\n+    /// <summary>\n+    /// The entity text.\n+    /// </summary>\n+    public string Text { get; set; } = string.Empty;\n+    \n+    /// <summary>\n+    /// The entity type (PERSON, ORGANIZATION, LOCATION, DATE).\n+    /// </summary>\n+    public string Type { get; set; } = string.Empty;\n+    \n+    /// <summary>\n+    /// Starting token index in the original text.\n+    /// </summary>\n+    public int StartIndex { get; set; }\n+    \n+    /// <summary>\n+    /// Ending token index in the original text.\n+    /// </summary>\n+    public int EndIndex { get; set; }",
    "path": "src/RetrievalAugmentedGeneration/NER/BiLSTMCRF_NER.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Fix index semantics: use character positions instead of token indices.**\n\nThe `StartIndex` and `EndIndex` properties are documented as \"token indices\" but standard NER practice requires character positions in the original text for entity highlighting and extraction. Token indices are not useful without retaining the token array.\n\nTo make the API usable, track character offsets during tokenization and assign actual character positions from the original text.\n\n\n\nExample approach:\n\n1. Modify tokenization to return tokens with character offsets:\n```csharp\nprivate List<(string token, int start, int end)> TokenizeWithOffsets(string text)\n{\n    var tokens = new List<(string, int, int)>();\n    int pos = 0;\n    foreach (var match in Regex.Matches(text, @\"\\S+\"))\n    {\n        var m = (Match)match;\n        tokens.Add((m.Value, m.Index, m.Index + m.Length - 1));\n    }\n    return tokens;\n}\n```\n\n2. Update entity extraction to use these character positions instead of token indices.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/NER/BiLSTMCRF_NER.cs around lines 282-290,\nthe StartIndex/EndIndex properties are documented and implemented as token\nindices but must use character offsets; update tokenization to produce token\nstart/end character positions, propagate those offsets through entity\nextraction, and set StartIndex/EndIndex to the token character start and end\npositions (inclusive or exclusive consistently) instead of token indices; ensure\nany callers/tests expecting token indices are updated to use character offsets\nand add unit tests validating offsets map back to original text.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:49:52Z",
    "updated_at": "2025-11-05T02:49:58Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669719",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669719"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669719"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669719/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 282,
    "original_start_line": 282,
    "start_side": "RIGHT",
    "line": 290,
    "original_line": 290,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 290,
    "position": 290,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669729",
    "pull_request_review_id": 3419457803,
    "id": 2492669729,
    "node_id": "PRRC_kwDOKSXUF86Ukx8h",
    "diff_hunk": "@@ -0,0 +1,191 @@\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Text.RegularExpressions;\n+",
    "path": "src/RetrievalAugmentedGeneration/QueryExpansion/MultiQueryExpansion.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Add missing `using System;` directive.**\n\nThe code uses `ArgumentNullException` (lines 74-75), `ArgumentOutOfRangeException` (line 78), and `ArgumentException` (line 87) without importing the `System` namespace, which will cause compilation errors.\n\n\n\nApply this diff:\n\n```diff\n+using System;\n using System.Collections.Generic;\n using System.Linq;\n using System.Text.RegularExpressions;\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/QueryExpansion/MultiQueryExpansion.cs around\nlines 1 to 4, the file is missing the System namespace import which causes usage\nof ArgumentNullException, ArgumentOutOfRangeException, and ArgumentException to\nfail; add the line \"using System;\" to the top of the file (alongside the\nexisting using directives) so those exception types resolve and the file\ncompiles.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:49:52Z",
    "updated_at": "2025-11-05T02:49:58Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669729",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669729"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669729"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669729/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 1,
    "original_start_line": 1,
    "start_side": "RIGHT",
    "line": 4,
    "original_line": 4,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 4,
    "position": 4,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669740",
    "pull_request_review_id": 3419457803,
    "id": 2492669740,
    "node_id": "PRRC_kwDOKSXUF86Ukx8s",
    "diff_hunk": "@@ -0,0 +1,191 @@\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Text.RegularExpressions;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.QueryExpansion;\n+\n+/// <summary>\n+/// Expands queries by generating multiple query variations from different perspectives using LLM-based reformulation.\n+/// </summary>\n+/// <remarks>\n+/// <para>\n+/// MultiQueryExpansion addresses the vocabulary mismatch problem in retrieval by creating diverse phrasings of the same\n+/// information need. Instead of searching with a single query, it generates 3-5 reformulations (questions, statements,\n+/// contextual phrases, synonyms) and retrieves documents for each variation, then merges and deduplicates results.\n+/// This approach significantly improves recall by capturing documents that use different terminology than the original\n+/// query. The implementation uses pattern-based transformations as a fallback but is designed to integrate with LLMs\n+/// for more sophisticated reformulations (e.g., technical ΓåÆ layman, abstract ΓåÆ concrete, general ΓåÆ specific perspectives).\n+/// </para>\n+/// <para><b>For Beginners:</b> Think of this like asking the same question in different ways:\n+/// \n+/// Regular search: \"machine learning algorithms\"\n+/// \n+/// Multi-query expansion generates:\n+/// 1. \"What are machine learning algorithms?\" (question form)\n+/// 2. \"information about machine learning algorithms\" (contextual)\n+/// 3. \"artificial intelligence techniques\" (synonym expansion)\n+/// 4. \"ML algorithms\" (simplified)\n+/// 5. \"machine learning methods\" (variation)\n+/// \n+/// Then searches using ALL variations and combines results!\n+/// \n+/// ```csharp\n+/// var expander = new MultiQueryExpansion(\n+///     llmEndpoint: \"http://localhost:1234/v1\",\n+///     llmApiKey: \"your-key\",\n+///     numVariations: 5\n+/// );\n+/// \n+/// var queries = expander.ExpandQuery(\"deep learning optimization\");\n+/// // Returns: [\"deep learning optimization\", \"What is deep learning optimization?\", \n+/// //           \"details about deep learning optimization\", \"neural network training\", ...]\n+/// ```\n+/// \n+/// Why use MultiQueryExpansion:\n+/// - Finds documents using different terminology (e.g., \"car\" vs \"automobile\")\n+/// - Improves recall without sacrificing precision\n+/// - Handles ambiguous queries (explores multiple interpretations)\n+/// - Effective for cross-domain search (technical Γåö layman terms)\n+/// \n+/// When NOT to use it:\n+/// - Very specific queries with clear terminology (wastes compute)\n+/// - High-latency systems (multiplies retrieval cost by numVariations)\n+/// - When you need ONLY exact matches\n+/// - Extremely short queries (no room for variation)\n+/// </para>\n+/// </remarks>\n+public class MultiQueryExpansion : QueryExpansionBase\n+{\n+    private readonly string _llmEndpoint;\n+    private readonly string _llmApiKey;\n+    private readonly int _numVariations;",
    "path": "src/RetrievalAugmentedGeneration/QueryExpansion/MultiQueryExpansion.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Constructor stores unused LLM parameters.**\n\nThe constructor validates and stores `_llmEndpoint` and `_llmApiKey` (lines 59-60, 74-75), but these fields are never used in the implementation. While the comment on line 92 indicates this is for future LLM integration, consider whether it's better to:\n\n1. Remove these parameters until LLM integration is implemented, or\n2. Add a code comment near the field declarations explaining they're reserved for future LLM calls\n\nThis avoids confusion about dead code and makes the intention clearer.\n\n\n\n\nAlso applies to: 69-81\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/QueryExpansion/MultiQueryExpansion.cs around\nlines 59-61 and 69-81, the constructor stores _llmEndpoint and _llmApiKey but\nthey are never used; either remove these parameters and their backing fields\n(and adjust constructor signature/validation) to eliminate dead code, or add a\nconcise inline comment on the field declarations stating they are reserved for\nfuture LLM integration and intentionally unused (and suppress unused-variable\nwarnings if necessary); pick one approach and update constructor, fields, and\nXML comments accordingly to keep intent clear.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:49:52Z",
    "updated_at": "2025-11-05T02:49:58Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669740",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669740"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669740"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669740/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 59,
    "original_start_line": 59,
    "start_side": "RIGHT",
    "line": 61,
    "original_line": 61,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 61,
    "position": 61,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669746",
    "pull_request_review_id": 3419457803,
    "id": 2492669746,
    "node_id": "PRRC_kwDOKSXUF86Ukx8y",
    "diff_hunk": "@@ -0,0 +1,191 @@\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Text.RegularExpressions;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.QueryExpansion;\n+\n+/// <summary>\n+/// Expands queries by generating multiple query variations from different perspectives using LLM-based reformulation.\n+/// </summary>\n+/// <remarks>\n+/// <para>\n+/// MultiQueryExpansion addresses the vocabulary mismatch problem in retrieval by creating diverse phrasings of the same\n+/// information need. Instead of searching with a single query, it generates 3-5 reformulations (questions, statements,\n+/// contextual phrases, synonyms) and retrieves documents for each variation, then merges and deduplicates results.\n+/// This approach significantly improves recall by capturing documents that use different terminology than the original\n+/// query. The implementation uses pattern-based transformations as a fallback but is designed to integrate with LLMs\n+/// for more sophisticated reformulations (e.g., technical ΓåÆ layman, abstract ΓåÆ concrete, general ΓåÆ specific perspectives).\n+/// </para>\n+/// <para><b>For Beginners:</b> Think of this like asking the same question in different ways:\n+/// \n+/// Regular search: \"machine learning algorithms\"\n+/// \n+/// Multi-query expansion generates:\n+/// 1. \"What are machine learning algorithms?\" (question form)\n+/// 2. \"information about machine learning algorithms\" (contextual)\n+/// 3. \"artificial intelligence techniques\" (synonym expansion)\n+/// 4. \"ML algorithms\" (simplified)\n+/// 5. \"machine learning methods\" (variation)\n+/// \n+/// Then searches using ALL variations and combines results!\n+/// \n+/// ```csharp\n+/// var expander = new MultiQueryExpansion(\n+///     llmEndpoint: \"http://localhost:1234/v1\",\n+///     llmApiKey: \"your-key\",\n+///     numVariations: 5\n+/// );\n+/// \n+/// var queries = expander.ExpandQuery(\"deep learning optimization\");\n+/// // Returns: [\"deep learning optimization\", \"What is deep learning optimization?\", \n+/// //           \"details about deep learning optimization\", \"neural network training\", ...]\n+/// ```\n+/// \n+/// Why use MultiQueryExpansion:\n+/// - Finds documents using different terminology (e.g., \"car\" vs \"automobile\")\n+/// - Improves recall without sacrificing precision\n+/// - Handles ambiguous queries (explores multiple interpretations)\n+/// - Effective for cross-domain search (technical Γåö layman terms)\n+/// \n+/// When NOT to use it:\n+/// - Very specific queries with clear terminology (wastes compute)\n+/// - High-latency systems (multiplies retrieval cost by numVariations)\n+/// - When you need ONLY exact matches\n+/// - Extremely short queries (no room for variation)\n+/// </para>\n+/// </remarks>\n+public class MultiQueryExpansion : QueryExpansionBase\n+{\n+    private readonly string _llmEndpoint;\n+    private readonly string _llmApiKey;\n+    private readonly int _numVariations;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"MultiQueryExpansion\"/> class.\n+    /// </summary>\n+    /// <param name=\"llmEndpoint\">The LLM API endpoint.</param>\n+    /// <param name=\"llmApiKey\">The API key for the LLM service.</param>\n+    /// <param name=\"numVariations\">Number of query variations to generate.</param>\n+    public MultiQueryExpansion(\n+        string llmEndpoint,\n+        string llmApiKey,\n+        int numVariations)\n+    {\n+        _llmEndpoint = llmEndpoint ?? throw new ArgumentNullException(nameof(llmEndpoint));\n+        _llmApiKey = llmApiKey ?? throw new ArgumentNullException(nameof(llmApiKey));\n+        \n+        if (numVariations <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(numVariations), \"Number of variations must be positive\");\n+            \n+        _numVariations = numVariations;\n+    }\n+\n+    /// <inheritdoc />\n+    public override List<string> ExpandQuery(string query)\n+    {\n+        if (string.IsNullOrWhiteSpace(query))\n+            throw new ArgumentException(\"Query cannot be null or whitespace\", nameof(query));\n+\n+        var expandedQueries = new List<string> { query };\n+\n+        // Generate variations using pattern-based transformations\n+        // In production, this would call an LLM, but we provide rule-based fallback\n+        var variations = GenerateVariations(query);\n+        expandedQueries.AddRange(variations.Take(_numVariations - 1));\n+\n+        return expandedQueries;\n+    }\n+\n+    private List<string> GenerateVariations(string query)\n+    {\n+        var variations = new List<string>();\n+\n+        // Question reformulation patterns\n+        variations.Add(ReformulateAsQuestion(query));\n+        variations.Add(ReformulateAsStatement(query));\n+        variations.Add(AddContextualPhrases(query));\n+        variations.Add(SimplifyQuery(query));\n+        variations.Add(ExpandWithSynonyms(query));\n+\n+        return variations.Where(v => !string.IsNullOrWhiteSpace(v) && v != query).Distinct().ToList();\n+    }\n+\n+    private string ReformulateAsQuestion(string query)\n+    {\n+        if (query.TrimEnd().EndsWith(\"?\"))\n+            return query;\n+\n+        var questionWords = new[] { \"what\", \"how\", \"why\", \"when\", \"where\", \"who\", \"which\" };\n+        var lowerQuery = query.ToLower();\n+        \n+        if (questionWords.Any(w => lowerQuery.StartsWith(w)))\n+            return query.TrimEnd('.', ' ') + \"?\";\n+\n+        return $\"What is {query.TrimEnd('.', ' ')}?\";\n+    }\n+\n+    private string ReformulateAsStatement(string query)\n+    {\n+        var trimmed = query.TrimEnd('?', '.', ' ');\n+        var lowerQuery = trimmed.ToLower();\n+\n+        if (lowerQuery.StartsWith(\"what is \"))\n+            return trimmed.Substring(8);\n+        if (lowerQuery.StartsWith(\"how to \"))\n+            return $\"information about {trimmed.Substring(7)}\";\n+        if (lowerQuery.StartsWith(\"why \"))\n+            return $\"reason for {trimmed.Substring(4)}\";\n+\n+        return trimmed;\n+    }\n+\n+    private string AddContextualPhrases(string query)\n+    {\n+        var phrases = new[]\n+        {\n+            $\"details about {query}\",\n+            $\"information regarding {query}\",\n+            $\"explain {query}\",\n+            $\"describe {query}\"\n+        };\n+\n+        return phrases[query.GetHashCode() % phrases.Length];",
    "path": "src/RetrievalAugmentedGeneration/QueryExpansion/MultiQueryExpansion.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Replace `GetHashCode()` with a stable hash for deterministic selection.**\n\nUsing `query.GetHashCode()` (lines 152 and 184) for index selection is non-deterministicΓÇöthe same query can produce different variations across:\n- Different .NET runtime versions\n- Different platforms (x86/x64)  \n- Separate application runs\n\nThis violates the expectation that identical queries yield identical expansions.\n\n\n\nConsider one of these stable alternatives:\n\n**Option 1: Use the query's absolute hash code (simplest)**\n```diff\n-        return phrases[query.GetHashCode() % phrases.Length];\n+        return phrases[Math.Abs(query.GetHashCode()) % phrases.Length];\n```\nNote: Even `Math.Abs(GetHashCode())` is not guaranteed stable across platforms, but it's slightly better.\n\n**Option 2: Use string length (fully deterministic)**\n```diff\n-        return phrases[query.GetHashCode() % phrases.Length];\n+        return phrases[query.Length % phrases.Length];\n```\n\n**Option 3: Use a stable hash function**\n```diff\n+        private int GetStableHashCode(string str)\n+        {\n+            unchecked\n+            {\n+                int hash = 23;\n+                foreach (char c in str)\n+                    hash = hash * 31 + c;\n+                return Math.Abs(hash);\n+            }\n+        }\n\n-        return phrases[query.GetHashCode() % phrases.Length];\n+        return phrases[GetStableHashCode(query) % phrases.Length];\n```\n\nApply the same fix to line 184 in `ExpandWithSynonyms`.\n\n\nAlso applies to: 184-184\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/QueryExpansion/MultiQueryExpansion.cs around\nlines 152 and 184, using query.GetHashCode() for index selection is\nnon-deterministic across runtimes and platforms; replace it with a stable\ndeterministic hash (e.g., compute a stable hash from the query string using a\ncryptographic or stable non-cryptographic hash like SHA256/MD5/xxHash, convert\nthe hash bytes to a non-negative integer, then use integer % phrases.Length) or\nuse a deterministic fallback like query.Length if you need simplicity; apply the\nsame replacement at both occurrences (line 152 and line 184 in\nExpandWithSynonyms) and ensure the resulting index is within bounds\n(0..phrases.Length-1).\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:49:53Z",
    "updated_at": "2025-11-05T02:49:58Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669746",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669746"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669746"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669746/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": 152,
    "original_line": 152,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 152,
    "position": 152,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669757",
    "pull_request_review_id": 3419457803,
    "id": 2492669757,
    "node_id": "PRRC_kwDOKSXUF86Ukx89",
    "diff_hunk": "@@ -0,0 +1,203 @@\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Text.RegularExpressions;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.QueryExpansion;\n+\n+/// <summary>\n+/// Expands complex queries by decomposing them into simpler, focused sub-queries for parallel retrieval.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for relevance scoring (typically float or double).</typeparam>",
    "path": "src/RetrievalAugmentedGeneration/QueryExpansion/SubQueryExpansion.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Remove incorrect generic type parameter from XML documentation.**\n\nThe XML documentation on line 10 references a generic type parameter `T`, but the class declaration on line 61 is not generic. This creates misleading documentation that doesn't match the actual signature.\n\n\n\nApply this diff to remove the incorrect documentation:\n\n```diff\n /// <summary>\n /// Expands complex queries by decomposing them into simpler, focused sub-queries for parallel retrieval.\n /// </summary>\n-/// <typeparam name=\"T\">The numeric data type used for relevance scoring (typically float or double).</typeparam>\n /// <remarks>\n```\n\n\nAlso applies to: 61-61\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/QueryExpansion/SubQueryExpansion.cs around\nlines 10 and 61, the XML doc contains an incorrect generic type parameter <T>\neven though the class at line 61 is not generic; remove the erroneous <typeparam\nname=\"T\">...</typeparam> element (and any references to T in that doc block) so\nthe XML documentation matches the non-generic class signature, keeping any\nremaining relevant summary/remarks intact.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:49:53Z",
    "updated_at": "2025-11-05T02:49:58Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669757",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669757"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669757"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669757/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": 10,
    "original_line": 10,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 10,
    "position": 10,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669762",
    "pull_request_review_id": 3419457803,
    "id": 2492669762,
    "node_id": "PRRC_kwDOKSXUF86Ukx9C",
    "diff_hunk": "@@ -0,0 +1,203 @@\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Text.RegularExpressions;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.QueryExpansion;\n+\n+/// <summary>\n+/// Expands complex queries by decomposing them into simpler, focused sub-queries for parallel retrieval.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for relevance scoring (typically float or double).</typeparam>\n+/// <remarks>\n+/// <para>\n+/// SubQueryExpansion solves the \"complex query problem\" where a single query contains multiple information needs.\n+/// It intelligently detects complexity indicators (conjunctions, multiple questions, comma-separated concepts) and\n+/// decomposes the query into independent sub-queries that are easier to retrieve for. For example, \"How does climate\n+/// change affect polar bears and what conservation efforts exist?\" becomes two focused queries. This approach improves\n+/// both precision (each sub-query is more specific) and recall (broader topic coverage). The implementation uses\n+/// linguistic patterns to identify sub-queries but can integrate with LLMs for more sophisticated decomposition.\n+/// Results from all sub-queries are retrieved independently and merged to provide comprehensive coverage.\n+/// </para>\n+/// <para><b>For Beginners:</b> Think of this like breaking a big question into smaller, easier ones:\n+/// \n+/// Complex query: \"Explain machine learning, deep learning, and reinforcement learning\"\n+/// \n+/// Sub-query decomposition:\n+/// 1. \"Explain machine learning\"\n+/// 2. \"Explain deep learning\"\n+/// 3. \"Explain reinforcement learning\"\n+/// 4. \"information about machine learning\" (key concept)\n+/// 5. \"information about deep learning\" (key concept)\n+/// \n+/// Each sub-query finds specific documents, then combines all results!\n+/// \n+/// ```csharp\n+/// var expander = new SubQueryExpansion(\n+///     llmEndpoint: \"http://localhost:1234/v1\",\n+///     llmApiKey: \"your-key\",\n+///     maxSubQueries: 4\n+/// );\n+/// \n+/// var queries = expander.ExpandQuery(\n+///     \"What is photosynthesis and how do plants use it for energy production?\"\n+/// );\n+/// // Returns: [\"What is photosynthesis?\", \"how do plants use it for energy production?\", \n+/// //           \"information about photosynthesis\", \"information about energy production\"]\n+/// ```\n+/// \n+/// Why use SubQueryExpansion:\n+/// - Handles multi-part questions effectively\n+/// - Each sub-query is more precise (better matches)\n+/// - Covers all aspects of complex information needs\n+/// - Ideal for research questions, comprehensive queries\n+/// \n+/// When NOT to use it:\n+/// - Simple, single-concept queries (unnecessary overhead)\n+/// - When you need documents covering ALL aspects together (decomposition loses connections)\n+/// - Very short queries (nothing to decompose)\n+/// - When retrieval latency is critical (multiple searches = slower)\n+/// </para>\n+/// </remarks>\n+public class SubQueryExpansion : QueryExpansionBase\n+{\n+    private readonly string _llmEndpoint;\n+    private readonly string _llmApiKey;",
    "path": "src/RetrievalAugmentedGeneration/QueryExpansion/SubQueryExpansion.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Remove unused LLM parameters or implement LLM-based decomposition.**\n\nThe constructor accepts `llmEndpoint` and `llmApiKey` parameters (lines 74-75) and stores them in fields (lines 63-64), but these values are never used anywhere in the implementation. The entire decomposition logic is rule-based using regex and string operations. This creates a misleading API where users configure LLM credentials that serve no purpose.\n\n\n\nEither implement LLM-based decomposition that uses these parameters, or remove them:\n\n```diff\n-    private readonly string _llmEndpoint;\n-    private readonly string _llmApiKey;\n     private readonly int _maxSubQueries;\n\n     /// <summary>\n     /// Initializes a new instance of the <see cref=\"SubQueryExpansion\"/> class.\n     /// </summary>\n-    /// <param name=\"llmEndpoint\">The LLM API endpoint.</param>\n-    /// <param name=\"llmApiKey\">The API key for the LLM service.</param>\n     /// <param name=\"maxSubQueries\">Maximum number of sub-queries to generate.</param>\n     public SubQueryExpansion(\n-        string llmEndpoint,\n-        string llmApiKey,\n         int maxSubQueries)\n     {\n-        _llmEndpoint = llmEndpoint ?? throw new ArgumentNullException(nameof(llmEndpoint));\n-        _llmApiKey = llmApiKey ?? throw new ArgumentNullException(nameof(llmApiKey));\n-        \n         if (maxSubQueries <= 0)\n             throw new ArgumentOutOfRangeException(nameof(maxSubQueries), \"Max sub-queries must be positive\");\n```\n\n\nAlso applies to: 73-79\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/QueryExpansion/SubQueryExpansion.cs around\nlines 63-64 (and constructor area lines 73-79), the _llmEndpoint and _llmApiKey\nfields are stored but never used; either implement LLM-based decomposition or\nremove the unused parameters/fields. To fix: either (A) implement a private\nmethod that calls the configured LLM endpoint using _llmEndpoint and _llmApiKey\nto perform query decomposition and wire it into the existing Decompose method\n(handle async/network errors, timeouts, and fall back to the current regex\nrules), or (B) remove the llmEndpoint and llmApiKey parameters from the\nconstructor, delete the corresponding private fields and any references, and\nupdate all call sites, tests and documentation to reflect the simplified API.\nEnsure compile-time changes are applied consistently across the project.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:49:53Z",
    "updated_at": "2025-11-05T02:49:58Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669762",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669762"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669762"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669762/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 63,
    "original_start_line": 63,
    "start_side": "RIGHT",
    "line": 64,
    "original_line": 64,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 64,
    "position": 64,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669769",
    "pull_request_review_id": 3419457803,
    "id": 2492669769,
    "node_id": "PRRC_kwDOKSXUF86Ukx9J",
    "diff_hunk": "@@ -0,0 +1,203 @@\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Text.RegularExpressions;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.QueryExpansion;\n+\n+/// <summary>\n+/// Expands complex queries by decomposing them into simpler, focused sub-queries for parallel retrieval.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for relevance scoring (typically float or double).</typeparam>\n+/// <remarks>\n+/// <para>\n+/// SubQueryExpansion solves the \"complex query problem\" where a single query contains multiple information needs.\n+/// It intelligently detects complexity indicators (conjunctions, multiple questions, comma-separated concepts) and\n+/// decomposes the query into independent sub-queries that are easier to retrieve for. For example, \"How does climate\n+/// change affect polar bears and what conservation efforts exist?\" becomes two focused queries. This approach improves\n+/// both precision (each sub-query is more specific) and recall (broader topic coverage). The implementation uses\n+/// linguistic patterns to identify sub-queries but can integrate with LLMs for more sophisticated decomposition.\n+/// Results from all sub-queries are retrieved independently and merged to provide comprehensive coverage.\n+/// </para>\n+/// <para><b>For Beginners:</b> Think of this like breaking a big question into smaller, easier ones:\n+/// \n+/// Complex query: \"Explain machine learning, deep learning, and reinforcement learning\"\n+/// \n+/// Sub-query decomposition:\n+/// 1. \"Explain machine learning\"\n+/// 2. \"Explain deep learning\"\n+/// 3. \"Explain reinforcement learning\"\n+/// 4. \"information about machine learning\" (key concept)\n+/// 5. \"information about deep learning\" (key concept)\n+/// \n+/// Each sub-query finds specific documents, then combines all results!\n+/// \n+/// ```csharp\n+/// var expander = new SubQueryExpansion(\n+///     llmEndpoint: \"http://localhost:1234/v1\",\n+///     llmApiKey: \"your-key\",\n+///     maxSubQueries: 4\n+/// );\n+/// \n+/// var queries = expander.ExpandQuery(\n+///     \"What is photosynthesis and how do plants use it for energy production?\"\n+/// );\n+/// // Returns: [\"What is photosynthesis?\", \"how do plants use it for energy production?\", \n+/// //           \"information about photosynthesis\", \"information about energy production\"]\n+/// ```\n+/// \n+/// Why use SubQueryExpansion:\n+/// - Handles multi-part questions effectively\n+/// - Each sub-query is more precise (better matches)\n+/// - Covers all aspects of complex information needs\n+/// - Ideal for research questions, comprehensive queries\n+/// \n+/// When NOT to use it:\n+/// - Simple, single-concept queries (unnecessary overhead)\n+/// - When you need documents covering ALL aspects together (decomposition loses connections)\n+/// - Very short queries (nothing to decompose)\n+/// - When retrieval latency is critical (multiple searches = slower)\n+/// </para>\n+/// </remarks>\n+public class SubQueryExpansion : QueryExpansionBase\n+{\n+    private readonly string _llmEndpoint;\n+    private readonly string _llmApiKey;\n+    private readonly int _maxSubQueries;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"SubQueryExpansion\"/> class.\n+    /// </summary>\n+    /// <param name=\"llmEndpoint\">The LLM API endpoint.</param>\n+    /// <param name=\"llmApiKey\">The API key for the LLM service.</param>\n+    /// <param name=\"maxSubQueries\">Maximum number of sub-queries to generate.</param>\n+    public SubQueryExpansion(\n+        string llmEndpoint,\n+        string llmApiKey,\n+        int maxSubQueries)\n+    {\n+        _llmEndpoint = llmEndpoint ?? throw new ArgumentNullException(nameof(llmEndpoint));\n+        _llmApiKey = llmApiKey ?? throw new ArgumentNullException(nameof(llmApiKey));\n+        \n+        if (maxSubQueries <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(maxSubQueries), \"Max sub-queries must be positive\");\n+            \n+        _maxSubQueries = maxSubQueries;\n+    }\n+\n+    /// <inheritdoc />\n+    public override List<string> ExpandQuery(string query)\n+    {\n+        if (string.IsNullOrWhiteSpace(query))\n+            throw new ArgumentException(\"Query cannot be null or whitespace\", nameof(query));\n+\n+        var subQueries = new List<string> { query };\n+\n+        // Detect and decompose complex queries\n+        if (IsComplexQuery(query))\n+        {\n+            var decomposed = DecomposeQuery(query);\n+            subQueries.AddRange(decomposed.Take(_maxSubQueries - 1));\n+        }\n+\n+        return subQueries;\n+    }\n+\n+    private bool IsComplexQuery(string query)\n+    {\n+        // Check for complexity indicators\n+        var complexityIndicators = new[]\n+        {\n+            \" and \", \" or \", \" also \", \" as well as \",\n+            \" furthermore \", \" moreover \", \" additionally \",\n+            \",\", \";\", \" versus \", \" vs \", \" compared to \"\n+        };\n+\n+        var lowerQuery = query.ToLower();\n+        return complexityIndicators.Any(indicator => lowerQuery.Contains(indicator));\n+    }\n+\n+    private List<string> DecomposeQuery(string query)\n+    {\n+        var subQueries = new List<string>();\n+\n+        // Split on common conjunctions and separators\n+        var parts = Regex.Split(query, @\"\\s+(?:and|or|also|as well as|furthermore|moreover|additionally)\\s+\", RegexOptions.IgnoreCase)\n+            .Concat(query.Split(new[] { ',', ';' }, StringSplitOptions.RemoveEmptyEntries))\n+            .Select(p => p.Trim())\n+            .Where(p => p.Length > 10)\n+            .Distinct()\n+            .ToList();\n+\n+        if (parts.Count > 1)\n+        {\n+            subQueries.AddRange(parts);\n+        }\n+\n+        // Extract questions if multiple are present\n+        var questions = ExtractQuestions(query);\n+        if (questions.Count > 1)\n+        {\n+            subQueries.AddRange(questions);\n+        }\n+\n+        // Identify key concepts for focused sub-queries\n+        var concepts = ExtractKeyConcepts(query);\n+        foreach (var concept in concepts.Take(Math.Min(3, _maxSubQueries)))",
    "path": "src/RetrievalAugmentedGeneration/QueryExpansion/SubQueryExpansion.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Add missing using directive for Math.Min.**\n\nLine 145 uses `Math.Min` but the `System` namespace is not imported at the top of the file. This will cause a compilation error.\n\n\n\nAdd the using directive at the top of the file:\n\n```diff\n using System.Collections.Generic;\n using System.Linq;\n using System.Text.RegularExpressions;\n+using System;\n\n namespace AiDotNet.RetrievalAugmentedGeneration.QueryExpansion;\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Text.RegularExpressions;\nusing System;\n\nnamespace AiDotNet.RetrievalAugmentedGeneration.QueryExpansion;\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/QueryExpansion/SubQueryExpansion.cs around\nline 145, the code calls Math.Min but the System namespace is not imported; add\nthe missing using directive for System at the top of the file (alongside other\nusing statements) so Math.Min resolves and the file compiles.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:49:53Z",
    "updated_at": "2025-11-05T02:49:58Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669769",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669769"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669769"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669769/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": 145,
    "original_line": 145,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 145,
    "position": 145,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669772",
    "pull_request_review_id": 3419457803,
    "id": 2492669772,
    "node_id": "PRRC_kwDOKSXUF86Ukx9M",
    "diff_hunk": "@@ -0,0 +1,203 @@\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Text.RegularExpressions;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.QueryExpansion;\n+\n+/// <summary>\n+/// Expands complex queries by decomposing them into simpler, focused sub-queries for parallel retrieval.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for relevance scoring (typically float or double).</typeparam>\n+/// <remarks>\n+/// <para>\n+/// SubQueryExpansion solves the \"complex query problem\" where a single query contains multiple information needs.\n+/// It intelligently detects complexity indicators (conjunctions, multiple questions, comma-separated concepts) and\n+/// decomposes the query into independent sub-queries that are easier to retrieve for. For example, \"How does climate\n+/// change affect polar bears and what conservation efforts exist?\" becomes two focused queries. This approach improves\n+/// both precision (each sub-query is more specific) and recall (broader topic coverage). The implementation uses\n+/// linguistic patterns to identify sub-queries but can integrate with LLMs for more sophisticated decomposition.\n+/// Results from all sub-queries are retrieved independently and merged to provide comprehensive coverage.\n+/// </para>\n+/// <para><b>For Beginners:</b> Think of this like breaking a big question into smaller, easier ones:\n+/// \n+/// Complex query: \"Explain machine learning, deep learning, and reinforcement learning\"\n+/// \n+/// Sub-query decomposition:\n+/// 1. \"Explain machine learning\"\n+/// 2. \"Explain deep learning\"\n+/// 3. \"Explain reinforcement learning\"\n+/// 4. \"information about machine learning\" (key concept)\n+/// 5. \"information about deep learning\" (key concept)\n+/// \n+/// Each sub-query finds specific documents, then combines all results!\n+/// \n+/// ```csharp\n+/// var expander = new SubQueryExpansion(\n+///     llmEndpoint: \"http://localhost:1234/v1\",\n+///     llmApiKey: \"your-key\",\n+///     maxSubQueries: 4\n+/// );\n+/// \n+/// var queries = expander.ExpandQuery(\n+///     \"What is photosynthesis and how do plants use it for energy production?\"\n+/// );\n+/// // Returns: [\"What is photosynthesis?\", \"how do plants use it for energy production?\", \n+/// //           \"information about photosynthesis\", \"information about energy production\"]\n+/// ```\n+/// \n+/// Why use SubQueryExpansion:\n+/// - Handles multi-part questions effectively\n+/// - Each sub-query is more precise (better matches)\n+/// - Covers all aspects of complex information needs\n+/// - Ideal for research questions, comprehensive queries\n+/// \n+/// When NOT to use it:\n+/// - Simple, single-concept queries (unnecessary overhead)\n+/// - When you need documents covering ALL aspects together (decomposition loses connections)\n+/// - Very short queries (nothing to decompose)\n+/// - When retrieval latency is critical (multiple searches = slower)\n+/// </para>\n+/// </remarks>\n+public class SubQueryExpansion : QueryExpansionBase\n+{\n+    private readonly string _llmEndpoint;\n+    private readonly string _llmApiKey;\n+    private readonly int _maxSubQueries;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"SubQueryExpansion\"/> class.\n+    /// </summary>\n+    /// <param name=\"llmEndpoint\">The LLM API endpoint.</param>\n+    /// <param name=\"llmApiKey\">The API key for the LLM service.</param>\n+    /// <param name=\"maxSubQueries\">Maximum number of sub-queries to generate.</param>\n+    public SubQueryExpansion(\n+        string llmEndpoint,\n+        string llmApiKey,\n+        int maxSubQueries)\n+    {\n+        _llmEndpoint = llmEndpoint ?? throw new ArgumentNullException(nameof(llmEndpoint));\n+        _llmApiKey = llmApiKey ?? throw new ArgumentNullException(nameof(llmApiKey));\n+        \n+        if (maxSubQueries <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(maxSubQueries), \"Max sub-queries must be positive\");\n+            \n+        _maxSubQueries = maxSubQueries;\n+    }\n+\n+    /// <inheritdoc />\n+    public override List<string> ExpandQuery(string query)\n+    {\n+        if (string.IsNullOrWhiteSpace(query))\n+            throw new ArgumentException(\"Query cannot be null or whitespace\", nameof(query));\n+\n+        var subQueries = new List<string> { query };\n+\n+        // Detect and decompose complex queries\n+        if (IsComplexQuery(query))\n+        {\n+            var decomposed = DecomposeQuery(query);\n+            subQueries.AddRange(decomposed.Take(_maxSubQueries - 1));\n+        }\n+\n+        return subQueries;\n+    }\n+\n+    private bool IsComplexQuery(string query)\n+    {\n+        // Check for complexity indicators\n+        var complexityIndicators = new[]\n+        {\n+            \" and \", \" or \", \" also \", \" as well as \",\n+            \" furthermore \", \" moreover \", \" additionally \",\n+            \",\", \";\", \" versus \", \" vs \", \" compared to \"\n+        };\n+\n+        var lowerQuery = query.ToLower();\n+        return complexityIndicators.Any(indicator => lowerQuery.Contains(indicator));\n+    }\n+\n+    private List<string> DecomposeQuery(string query)\n+    {\n+        var subQueries = new List<string>();\n+\n+        // Split on common conjunctions and separators\n+        var parts = Regex.Split(query, @\"\\s+(?:and|or|also|as well as|furthermore|moreover|additionally)\\s+\", RegexOptions.IgnoreCase)\n+            .Concat(query.Split(new[] { ',', ';' }, StringSplitOptions.RemoveEmptyEntries))\n+            .Select(p => p.Trim())\n+            .Where(p => p.Length > 10)\n+            .Distinct()\n+            .ToList();\n+\n+        if (parts.Count > 1)\n+        {\n+            subQueries.AddRange(parts);\n+        }\n+\n+        // Extract questions if multiple are present\n+        var questions = ExtractQuestions(query);\n+        if (questions.Count > 1)\n+        {\n+            subQueries.AddRange(questions);\n+        }\n+\n+        // Identify key concepts for focused sub-queries\n+        var concepts = ExtractKeyConcepts(query);\n+        foreach (var concept in concepts.Take(Math.Min(3, _maxSubQueries)))\n+        {\n+            subQueries.Add($\"information about {concept}\");\n+        }\n+\n+        return subQueries.Distinct().Where(s => s != query).Take(_maxSubQueries - 1).ToList();\n+    }\n+\n+    private List<string> ExtractQuestions(string query)\n+    {\n+        // Split on question marks but preserve the questions\n+        var questions = new List<string>();\n+        var parts = query.Split('?');\n+\n+        for (int i = 0; i < parts.Length - 1; i++)\n+        {\n+            var question = parts[i].Trim() + \"?\";\n+            if (question.Length > 10)\n+                questions.Add(question);\n+        }\n+\n+        return questions;\n+    }\n+\n+    private List<string> ExtractKeyConcepts(string query)\n+    {\n+        // Remove common stop words and extract meaningful terms\n+        var stopWords = new HashSet<string>\n+        {\n+            \"the\", \"a\", \"an\", \"and\", \"or\", \"but\", \"in\", \"on\", \"at\", \"to\", \"for\",\n+            \"of\", \"with\", \"by\", \"from\", \"as\", \"is\", \"was\", \"are\", \"were\", \"been\",\n+            \"be\", \"have\", \"has\", \"had\", \"do\", \"does\", \"did\", \"will\", \"would\",\n+            \"could\", \"should\", \"may\", \"might\", \"can\", \"what\", \"how\", \"why\", \"when\",\n+            \"where\", \"who\", \"which\"\n+        };\n+\n+        var words = Regex.Split(query.ToLower(), @\"\\W+\")\n+            .Where(w => w.Length > 3 && !stopWords.Contains(w))\n+            .ToList();\n+\n+        // Extract noun phrases (simple heuristic: sequences of capitalized words or long words)\n+        var concepts = new List<string>();\n+        for (int i = 0; i < words.Count; i++)\n+        {\n+            if (words[i].Length >= 5)\n+            {\n+                concepts.Add(words[i]);\n+                \n+                // Check for multi-word concepts\n+                if (i < words.Count - 1 && words[i + 1].Length >= 5)\n+                {\n+                    concepts.Add($\"{words[i]} {words[i + 1]}\");\n+                }\n+            }\n+        }\n+\n+        return concepts.Distinct().ToList();\n+    }}",
    "path": "src/RetrievalAugmentedGeneration/QueryExpansion/SubQueryExpansion.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Fix syntax error: extra closing brace.**\n\nLine 202 has two closing braces (`}}`), but only one is needed to close the class. This will cause a compilation error.\n\n\n\nApply this diff:\n\n```diff\n         return concepts.Distinct().ToList();\n-    }}\n+    }\n+}\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    }\n}\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/QueryExpansion/SubQueryExpansion.cs around\nline 202 there is an extra closing brace (\"}}\") causing a syntax/compilation\nerror; remove the redundant brace so only a single closing brace remains to\nproperly close the class (ensure surrounding indentation remains correct and no\nother braces are altered).\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:49:53Z",
    "updated_at": "2025-11-05T02:49:58Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669772",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669772"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669772"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669772/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": 202,
    "original_line": 202,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 202,
    "position": 202,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669780",
    "pull_request_review_id": 3419457803,
    "id": 2492669780,
    "node_id": "PRRC_kwDOKSXUF86Ukx9U",
    "diff_hunk": "@@ -0,0 +1,266 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using AiDotNet.RetrievalAugmentedGeneration.Rerankers;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.RerankingStrategies;\n+\n+/// <summary>\n+/// Reranks documents using Cohere's specialized reranking API for state-of-the-art relevance scoring.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for relevance scoring (typically float or double).</typeparam>\n+/// <remarks>\n+/// <para>\n+/// CohereReranker integrates with Cohere's dedicated reranking models (e.g., rerank-english-v3.0) which are specifically\n+/// trained for relevance scoring rather than general language understanding. Unlike bi-encoder models that compare query\n+/// and document embeddings independently, Cohere's reranker uses cross-encoder architecture to analyze query-document pairs\n+/// jointly, producing more accurate relevance scores. This two-stage retrieval approach (fast bi-encoder retrieval ΓåÆ \n+/// precise cross-encoder reranking) is industry best practice, significantly improving answer quality at manageable cost.\n+/// The implementation provides a heuristic fallback using term overlap, proximity, and length penalties, but is designed\n+/// to call Cohere's API in production for superior performance. Reranking is particularly effective when applied to the\n+/// top 20-100 results from initial retrieval, balancing accuracy gains against API costs.\n+/// </para>\n+/// <para><b>For Beginners:</b> Think of this as getting a second, expert opinion on search results:\n+/// \n+/// Without reranking:\n+/// - Fast vector search finds 100 \"similar\" documents\n+/// - Returns top 5 based on vector distance\n+/// - Good enough, but not perfect\n+/// \n+/// With Cohere reranking:\n+/// - Fast vector search finds 100 candidates\n+/// - Cohere expert looks at each one carefully\n+/// - Reorders them by true relevance\n+/// - Returns truly best 5\n+/// \n+/// Example:\n+/// Query: \"How to prevent overfitting in neural networks?\"\n+/// \n+/// Initial retrieval (vector search):\n+/// 1. \"Neural networks architecture guide\" (score: 0.82)\n+/// 2. \"Overfitting in machine learning\" (score: 0.80)\n+/// 3. \"Regularization techniques for deep learning\" (score: 0.78)\n+/// \n+/// After Cohere reranking:\n+/// 1. \"Regularization techniques for deep learning\" (NEW score: 0.95) ΓåÉ Actually best match!\n+/// 2. \"Overfitting in machine learning\" (NEW score: 0.88)\n+/// 3. \"Neural networks architecture guide\" (NEW score: 0.72) ΓåÉ Less relevant\n+/// \n+/// ```csharp\n+/// var reranker = new CohereReranker<double>(\n+///     apiKey: \"your-cohere-api-key\",\n+///     model: \"rerank-english-v3.0\"\n+/// );\n+/// \n+/// var initialResults = retriever.Retrieve(query, topK: 20);  // Get 20 candidates\n+/// var reranked = reranker.Rerank(query, initialResults);      // Rerank to improve order\n+/// var final = reranked.Take(5);                               // Return best 5\n+/// ```\n+/// \n+/// Why use CohereReranker:\n+/// - Significantly improves answer quality (10-30% better precision)\n+/// - Cohere models are specifically trained for reranking (not repurposed)\n+/// - Handles complex queries better than vector search alone\n+/// - Industry-standard approach used by major search systems\n+/// \n+/// When NOT to use it:\n+/// - Simple keyword queries (not worth the API cost)\n+/// - Real-time systems with strict latency requirements (adds ~100-300ms)\n+/// - Very high query volume (API costs add up)\n+/// - When initial retrieval is already highly accurate\n+/// </para>\n+/// </remarks>\n+public class CohereReranker<T> : RerankerBase<T>\n+{\n+    private readonly string _apiKey;\n+    private readonly string _model;\n+\n+    /// <summary>\n+    /// Gets a value indicating whether this reranker modifies relevance scores.\n+    /// </summary>\n+    public override bool ModifiesScores => true;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"CohereReranker{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"apiKey\">The Cohere API key for authenticating requests to the reranking service.</param>\n+    /// <param name=\"model\">The Cohere reranking model identifier (e.g., \"rerank-english-v3.0\" for latest, \"rerank-multilingual-v2.0\" for non-English).</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when apiKey or model is null.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This sets up the connection to Cohere's reranking service.\n+    /// \n+    /// Available Cohere models:\n+    /// - \"rerank-english-v3.0\": Best for English queries (recommended, most accurate)\n+    /// - \"rerank-multilingual-v2.0\": Supports 100+ languages\n+    /// - \"rerank-english-v2.0\": Previous generation (faster but less accurate)\n+    /// \n+    /// You'll need a Cohere API key from https://cohere.ai\n+    /// \n+    /// Typical usage:\n+    /// ```csharp\n+    /// var reranker = new CohereReranker<double>(\n+    ///     apiKey: Environment.GetEnvironmentVariable(\"COHERE_API_KEY\"),\n+    ///     model: \"rerank-english-v3.0\"\n+    /// );\n+    /// ```\n+    /// </para>\n+    /// </remarks>\n+    public CohereReranker(string apiKey, string model)\n+    {\n+        _apiKey = apiKey ?? throw new ArgumentNullException(nameof(apiKey));\n+        _model = model ?? throw new ArgumentNullException(nameof(model));\n+    }\n+\n+    /// <summary>\n+    /// Reranks documents by analyzing query-document relevance using Cohere's cross-encoder model.\n+    /// </summary>\n+    /// <param name=\"query\">The search query to compare documents against.</param>\n+    /// <param name=\"documents\">The candidate documents from initial retrieval (typically 20-100 documents).</param>\n+    /// <returns>Documents reordered by relevance score (highest first) with updated RelevanceScore values.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// This method implements sophisticated relevance scoring through:\n+    /// 1. Term Overlap (50% weight): Jaccard similarity between query and document terms\n+    /// 2. Term Proximity (30% weight): Measures how close query terms appear in document (within 100 chars)\n+    /// 3. Length Penalty (20% weight): Favors moderate-length documents (around 500 chars ideal)\n+    /// 4. Original Score Blending: If documents have existing scores, blends 70% new score + 30% original\n+    /// \n+    /// The fallback implementation uses heuristics, but production deployments should integrate with Cohere's API\n+    /// for cross-encoder scoring which jointly analyzes query-document pairs for superior accuracy. The cross-encoder\n+    /// architecture allows the model to learn interaction patterns between query and document tokens, producing\n+    /// more nuanced relevance judgments than independent embeddings can achieve.\n+    /// \n+    /// Recommended usage: Rerank top 20-100 results from initial retrieval, then select top 5-10 for LLM context.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is where the actual reranking magic happens:\n+    /// \n+    /// Step 1: Analyze each document carefully\n+    /// - Does it contain query terms? (overlap score)\n+    /// - Do query terms appear close together? (proximity score)\n+    /// - Is the document a good length? (length penalty)\n+    /// \n+    /// Step 2: Combine scores\n+    /// - Term overlap: 50% of final score\n+    /// - Proximity: 30% of final score\n+    /// - Length: 20% of final score\n+    /// \n+    /// Step 3: Blend with original score if available\n+    /// - New score (70%) + Original vector score (30%)\n+    /// \n+    /// Step 4: Sort by new scores and return\n+    /// \n+    /// Example scoring:\n+    /// Document A: overlap=0.8, proximity=0.9, length=1.0 ΓåÆ Final: 0.85\n+    /// Document B: overlap=0.9, proximity=0.3, length=0.7 ΓåÆ Final: 0.68\n+    /// ΓåÆ Document A ranks higher despite lower term overlap!\n+    /// \n+    /// This finds documents that truly answer your question, not just contain keywords.\n+    /// </para>\n+    /// </remarks>\n+    protected override IEnumerable<Document<T>> RerankCore(string query, IList<Document<T>> documents)\n+    {\n+        // For production, this would call Cohere Rerank API\n+        // Fallback: Use cross-encoder-like scoring with term overlap and relevance\n+        \n+        var queryTerms = ExtractTerms(query);\n+        var scoredDocuments = new List<(Document<T> doc, double score)>();\n+\n+        foreach (var doc in documents)\n+        {\n+            var docTerms = ExtractTerms(doc.Content);\n+            \n+            // Calculate relevance score based on multiple factors\n+            var termOverlapScore = CalculateTermOverlap(queryTerms, docTerms);\n+            var proximityScore = CalculateTermProximity(query, doc.Content);\n+            var lengthPenalty = CalculateLengthPenalty(doc.Content.Length);\n+            \n+            // Combine scores\n+            var rerankScore = (termOverlapScore * 0.5 + proximityScore * 0.3 + lengthPenalty * 0.2);\n+            \n+            // Blend with original score if available\n+            if (doc.HasRelevanceScore)\n+            {\n+                var originalScore = Convert.ToDouble(doc.RelevanceScore);\n+                rerankScore = rerankScore * 0.7 + originalScore * 0.3;\n+            }\n+\n+            scoredDocuments.Add((doc, rerankScore));\n+        }\n+\n+        // Return reranked documents\n+        return scoredDocuments\n+            .OrderByDescending(x => x.score)\n+            .Select(x =>\n+            {\n+                x.doc.RelevanceScore = NumOps.FromDouble(x.score);\n+                x.doc.HasRelevanceScore = true;\n+                return x.doc;\n+            });",
    "path": "src/RetrievalAugmentedGeneration/Rerankers/CohereReranker.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Implement the actual Cohere API call (currently just heuristic fallback).**\n\nThe class accepts a Cohere API key/model but never invokes CohereΓÇÖs rerank endpointΓÇö`RerankCore` only runs local heuristics. In production this means callers believe theyΓÇÖre getting Cohere cross-encoder scores while actually receiving the fallback, which undermines accuracy and defeats the purpose of adding this concrete implementation. Please wire up the real API request (e.g., via `HttpClient` to CohereΓÇÖs `/v1/rerank`), handle response parsing, and fall back to heuristics only on failure; otherwise this class does not meet the stated objectives.\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:49:53Z",
    "updated_at": "2025-11-05T02:49:58Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669780",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669780"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669780"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669780/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 164,
    "original_start_line": 164,
    "start_side": "RIGHT",
    "line": 200,
    "original_line": 200,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 200,
    "position": 200,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669786",
    "pull_request_review_id": 3419457803,
    "id": 2492669786,
    "node_id": "PRRC_kwDOKSXUF86Ukx9a",
    "diff_hunk": "@@ -0,0 +1,126 @@\n+using System;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.RerankingStrategies\n+{\n+    /// <summary>\n+    /// LLM-based reranking using language model relevance assessment.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class LLMBasedReranker<T> : Rerankers.RerankerBase<T>\n+    {\n+        private readonly string _llmEndpoint;\n+        private readonly string _apiKey;",
    "path": "src/RetrievalAugmentedGeneration/Rerankers/LLMBasedReranker.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Critical: Stored LLM credentials are never used; implementation doesn't call any LLM.**\n\nThe constructor accepts `llmEndpoint` and `apiKey` parameters and stores them in fields, but these fields are never referenced in the implementation. The actual reranking logic uses local heuristic scoring (exact matches, proximity, Jaccard similarity) without any LLM API calls. This is misleading because the class name \"LLMBasedReranker\" and its constructor signature promise LLM-based relevance assessment, but the implementation doesn't deliver on that promise.\n\n\n\nEither implement actual LLM API calls for relevance assessment, or rename the class to reflect its heuristic nature (e.g., `HeuristicReranker` or `LocalRelevanceReranker`) and remove the unused parameters.\n\n\n\nAlso applies to: 27-31\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Rerankers/LLMBasedReranker.cs around lines\n14-15 (and similarly lines 27-31), the constructor stores llmEndpoint and apiKey\nbut they are never used; either implement actual LLM calls or remove/rename to\nreflect a heuristic reranker. To fix: choose one of two optionsΓÇö(A) Implement\nLLM-based scoring by using the stored _llmEndpoint and _apiKey: inject or create\nan HttpClient, build request payloads with the candidate document/query context,\ninclude the apiKey in Authorization or required header, call the endpoint, parse\nand use the returned relevance scores to replace the current heuristic scoring,\nand add error handling/timeouts and unit tests for the external call; or (B) If\nyou prefer local heuristics, rename the class (e.g., HeuristicReranker or\nLocalRelevanceReranker), remove the unused llmEndpoint and apiKey parameters and\nprivate fields, update constructor signature and all references/tests to the new\nclass name, and adjust XML/docs to reflect the heuristic behavior.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:49:54Z",
    "updated_at": "2025-11-05T02:49:58Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669786",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669786"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669786"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669786/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 14,
    "original_start_line": 14,
    "start_side": "RIGHT",
    "line": 15,
    "original_line": 15,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 15,
    "position": 15,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669789",
    "pull_request_review_id": 3419457803,
    "id": 2492669789,
    "node_id": "PRRC_kwDOKSXUF86Ukx9d",
    "diff_hunk": "@@ -0,0 +1,68 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using AiDotNet.RetrievalAugmentedGeneration.Rerankers;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.RerankingStrategies;\n+\n+/// <summary>\n+/// Addresses the \"lost in the middle\" problem by strategically reordering documents.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// Research shows LLMs often ignore information in the middle of long contexts.\n+/// This reranker places most relevant documents at the beginning and end of the context.\n+/// </remarks>\n+public class LostInTheMiddleReranker<T> : RerankerBase<T>\n+{\n+    /// <summary>\n+    /// Gets a value indicating whether this reranker modifies relevance scores.\n+    /// </summary>\n+    public override bool ModifiesScores => false;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"LostInTheMiddleReranker{T}\"/> class.\n+    /// </summary>\n+    public LostInTheMiddleReranker()\n+    {\n+    }\n+\n+    /// <summary>\n+    /// Reranks documents to avoid the \"lost in the middle\" problem.\n+    /// </summary>\n+    /// <remarks>\n+    /// Strategy: Place most relevant at start, 2nd most relevant at end, 3rd in middle,\n+    /// alternating to distribute important documents to positions LLMs pay attention to.\n+    /// </remarks>",
    "path": "src/RetrievalAugmentedGeneration/Rerankers/LostInTheMiddleReranker.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_≡ƒ¢á∩╕Å Refactor suggestion_ | _≡ƒƒá Major_\n\n**Clarify documentation to match actual algorithm behavior.**\n\nThe documentation states \"3rd in middle,\" but the algorithm actually places items alternately from the start and end. For a 5-document list, the order is: [1st, 3rd, 5th, 4th, 2nd], meaning the 3rd document goes to position 1 (not the middle), and the middle position receives the 5th (least relevant of the top 5) document.\n\n\n\n```diff\n     /// <remarks>\n-    /// Strategy: Place most relevant at start, 2nd most relevant at end, 3rd in middle,\n-    /// alternating to distribute important documents to positions LLMs pay attention to.\n+    /// Strategy: Alternates placing documents from start and end positions.\n+    /// Most relevant at position 0, 2nd at last position, 3rd at position 1, 4th at second-to-last, etc.\n+    /// This distributes the most important documents to the edges where LLMs pay attention,\n+    /// while less relevant documents occupy the middle positions.\n     /// </remarks>\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    /// <summary>\n    /// Reranks documents to avoid the \"lost in the middle\" problem.\n    /// </summary>\n    /// <remarks>\n    /// Strategy: Alternates placing documents from start and end positions.\n    /// Most relevant at position 0, 2nd at last position, 3rd at position 1, 4th at second-to-last, etc.\n    /// This distributes the most important documents to the edges where LLMs pay attention,\n    /// while less relevant documents occupy the middle positions.\n    /// </remarks>\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Rerankers/LostInTheMiddleReranker.cs around\nlines 30 to 36, the XML documentation incorrectly states \"3rd in middle\" while\nthe implementation alternates placing items from the start and end (e.g., for\nfive docs the order is [1st, 3rd, 5th, 4th, 2nd]). Update the summary/remarks to\ndescribe the actual algorithm: highest relevance placed first, second placed\nlast, then alternate placing the remaining items from the front and back inward;\ninclude a brief example (5-document case) to make the behavior explicit and\nremove the incorrect \"3rd in middle\" phrasing so the docs match the code.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:49:54Z",
    "updated_at": "2025-11-05T02:49:58Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669789",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669789"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669789"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669789/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 30,
    "original_start_line": 30,
    "start_side": "RIGHT",
    "line": 36,
    "original_line": 36,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 36,
    "position": 36,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669795",
    "pull_request_review_id": 3419457803,
    "id": 2492669795,
    "node_id": "PRRC_kwDOKSXUF86Ukx9j",
    "diff_hunk": "@@ -0,0 +1,68 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using AiDotNet.RetrievalAugmentedGeneration.Rerankers;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.RerankingStrategies;\n+\n+/// <summary>\n+/// Addresses the \"lost in the middle\" problem by strategically reordering documents.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// Research shows LLMs often ignore information in the middle of long contexts.\n+/// This reranker places most relevant documents at the beginning and end of the context.\n+/// </remarks>\n+public class LostInTheMiddleReranker<T> : RerankerBase<T>\n+{\n+    /// <summary>\n+    /// Gets a value indicating whether this reranker modifies relevance scores.\n+    /// </summary>\n+    public override bool ModifiesScores => false;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"LostInTheMiddleReranker{T}\"/> class.\n+    /// </summary>\n+    public LostInTheMiddleReranker()\n+    {\n+    }\n+\n+    /// <summary>\n+    /// Reranks documents to avoid the \"lost in the middle\" problem.\n+    /// </summary>\n+    /// <remarks>\n+    /// Strategy: Place most relevant at start, 2nd most relevant at end, 3rd in middle,\n+    /// alternating to distribute important documents to positions LLMs pay attention to.\n+    /// </remarks>\n+    protected override IEnumerable<Document<T>> RerankCore(string query, IList<Document<T>> documents)\n+    {\n+        if (documents.Count <= 2)\n+            return documents;\n+\n+        var sorted = documents\n+            .OrderByDescending(d => d.HasRelevanceScore ? d.RelevanceScore : NumOps.Zero)\n+            .ToList();",
    "path": "src/RetrievalAugmentedGeneration/Rerankers/LostInTheMiddleReranker.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Fix runtime error: generic type T comparison requires explicit conversion.**\n\n`OrderByDescending` on generic type `T` requires `T` to implement `IComparable<T>`, which is not guaranteed by the current constraints. This will fail at runtime for types that don't implement comparison operators.\n\n\n\nFollowing the pattern from `LLMBasedReranker.cs` (line 67), convert to `double` for comparison:\n\n```diff\n         var sorted = documents\n-            .OrderByDescending(d => d.HasRelevanceScore ? d.RelevanceScore : NumOps.Zero)\n+            .OrderByDescending(d => d.HasRelevanceScore ? Convert.ToDouble(d.RelevanceScore) : 0.0)\n             .ToList();\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        var sorted = documents\n            .OrderByDescending(d => d.HasRelevanceScore ? Convert.ToDouble(d.RelevanceScore) : 0.0)\n            .ToList();\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Rerankers/LostInTheMiddleReranker.cs around\nlines 42 to 44, the OrderByDescending uses the generic RelevanceScore directly\nwhich requires IComparable<T> and can fail at runtime; change the key selector\nto convert the relevance score to a double (e.g., cast or call ToDouble/Value)\nand fall back to NumOps.Zero converted to double, mirroring the\nLLMBasedReranker.cs pattern so sorting uses a numeric double key rather than a\ngeneric T.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:49:54Z",
    "updated_at": "2025-11-05T02:49:58Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669795",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669795"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669795"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669795/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 42,
    "original_start_line": 42,
    "start_side": "RIGHT",
    "line": 44,
    "original_line": 44,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 44,
    "position": 44,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669803",
    "pull_request_review_id": 3419457803,
    "id": 2492669803,
    "node_id": "PRRC_kwDOKSXUF86Ukx9r",
    "diff_hunk": "@@ -0,0 +1,111 @@\n+using System;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System.Collections.Generic;\n+using System.Linq;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.RerankingStrategies\n+{\n+    /// <summary>\n+    /// Reciprocal Rank Fusion for combining multiple ranking lists.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class ReciprocalRankFusion<T> : Rerankers.RerankerBase<T>\n+    {\n+        private readonly int _k;\n+\n+        /// <summary>\n+        /// Gets a value indicating whether this reranker modifies relevance scores.\n+        /// </summary>\n+        public override bool ModifiesScores => true;\n+\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"ReciprocalRankFusion{T}\"/> class.\n+        /// </summary>\n+        /// <param name=\"k\">The constant k for reciprocal rank formula (default: 60).</param>\n+        public ReciprocalRankFusion(int k = 60)\n+        {\n+            _k = k > 0 ? k : throw new ArgumentOutOfRangeException(nameof(k));\n+        }\n+\n+        /// <summary>\n+        /// Reranks documents using reciprocal rank fusion.\n+        /// </summary>\n+        /// <param name=\"query\">The query string.</param>\n+        /// <param name=\"documents\">The documents to rerank.</param>\n+        /// <returns>A reranked list of documents.</returns>\n+        protected override IEnumerable<Document<T>> RerankCore(string query, IList<Document<T>> documents)\n+        {\n+            var scores = new Dictionary<string, T>();\n+\n+            for (int rank = 0; rank < documents.Count; rank++)\n+            {\n+                var doc = documents[rank];\n+                var rrfScore = NumOps.FromDouble(1.0 / (_k + rank + 1));\n+\n+                scores[doc.Id] = scores.TryGetValue(doc.Id, out var existingScore)\n+                    ? NumOps.Add(existingScore, rrfScore)\n+                    : rrfScore;\n+            }\n+\n+            var reranked = documents\n+                .OrderByDescending(d => scores.TryGetValue(d.Id, out var score) ? Convert.ToDouble(score) : 0.0)\n+                .ToList();\n+\n+            foreach (var doc in reranked.Where(d => scores.ContainsKey(d.Id)))\n+            {\n+                doc.RelevanceScore = scores[doc.Id];\n+                doc.HasRelevanceScore = true;\n+            }\n+\n+            return reranked;\n+        }\n+\n+        /// <summary>\n+        /// Fuses multiple ranking lists using reciprocal rank fusion.\n+        /// </summary>\n+        /// <param name=\"rankingLists\">Multiple lists of ranked documents.</param>\n+        /// <param name=\"topK\">The number of top documents to return.</param>\n+        /// <returns>A fused and reranked list of documents.</returns>\n+        public List<Document<T>> FuseRankings(List<List<Document<T>>> rankingLists, int topK)\n+        {\n+            if (rankingLists == null || rankingLists.Count == 0)\n+                throw new ArgumentException(\"Ranking lists cannot be null or empty\", nameof(rankingLists));\n+            if (topK <= 0) throw new ArgumentOutOfRangeException(nameof(topK));\n+\n+            var scores = new Dictionary<string, T>();\n+            var allDocs = new Dictionary<string, Document<T>>();\n+\n+            foreach (var rankingList in rankingLists)\n+            {\n+                for (int rank = 0; rank < rankingList.Count; rank++)\n+                {\n+                    var doc = rankingList[rank];\n+                    var rrfScore = NumOps.FromDouble(1.0 / (_k + rank + 1));\n+\n+                    if (scores.TryGetValue(doc.Id, out var existingScore))\n+                    {\n+                        scores[doc.Id] = NumOps.Add(existingScore, rrfScore);\n+                    }\n+                    else\n+                    {\n+                        scores[doc.Id] = rrfScore;\n+                        allDocs[doc.Id] = doc;\n+                    }\n+                }\n+            }\n+\n+            var reranked = allDocs.Values\n+                .OrderByDescending(d => Convert.ToDouble(scores[d.Id]))\n+                .Take(topK)\n+                .ToList();\n+\n+            foreach (var doc in reranked)\n+            {\n+                doc.RelevanceScore = scores[doc.Id];\n+            }",
    "path": "src/RetrievalAugmentedGeneration/Rerankers/ReciprocalRankFusion.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Missing `HasRelevanceScore` flag assignment.**\n\nThe `FuseRankings` method sets `RelevanceScore` but doesn't set `HasRelevanceScore = true`, unlike `RerankCore` (line 58). This inconsistency may cause issues if downstream code checks this flag to validate score presence.\n\n\nApply this diff:\n\n```diff\n         foreach (var doc in reranked)\n         {\n             doc.RelevanceScore = scores[doc.Id];\n+            doc.HasRelevanceScore = true;\n         }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n            foreach (var doc in reranked)\n            {\n                doc.RelevanceScore = scores[doc.Id];\n                doc.HasRelevanceScore = true;\n            }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Rerankers/ReciprocalRankFusion.cs around\nlines 103 to 106, the loop assigns RelevanceScore but does not set\nHasRelevanceScore = true; update the loop so that for each doc you set both\ndoc.RelevanceScore = scores[doc.Id] and doc.HasRelevanceScore = true (mirroring\nRerankCore at line 58) so downstream checks correctly detect that a relevance\nscore exists.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:49:54Z",
    "updated_at": "2025-11-05T02:49:58Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669803",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669803"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669803"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669803/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 103,
    "original_start_line": 103,
    "start_side": "RIGHT",
    "line": 106,
    "original_line": 106,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 106,
    "position": 106,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669807",
    "pull_request_review_id": 3419457803,
    "id": 2492669807,
    "node_id": "PRRC_kwDOKSXUF86Ukx9v",
    "diff_hunk": "@@ -0,0 +1,29 @@\n+using System;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System.Collections.Generic;\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.RerankingStrategies",
    "path": "src/RetrievalAugmentedGeneration/Rerankers/RerankingStrategyBase.cs",
    "commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Namespace doesn't match file location.**\n\nThe file is located in `src/RetrievalAugmentedGeneration/Rerankers/` but declares namespace `RerankingStrategies`. This violates C# conventions where namespace structure should mirror folder structure.\n\n\n\nConsider aligning the namespace with the file path:\n```diff\n-namespace AiDotNet.RetrievalAugmentedGeneration.RerankingStrategies\n+namespace AiDotNet.RetrievalAugmentedGeneration.Rerankers\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\nnamespace AiDotNet.RetrievalAugmentedGeneration.Rerankers\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Rerankers/RerankingStrategyBase.cs around\nline 7, the declared namespace\nAiDotNet.RetrievalAugmentedGeneration.RerankingStrategies does not match the\nfile path folder Rerankers; update the namespace to mirror the folder structure\n(e.g., AiDotNet.RetrievalAugmentedGeneration.Rerankers) so it aligns with C#\nconventions and project layout, and adjust any using/imports or references if\nnecessary to reflect the new namespace.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:49:54Z",
    "updated_at": "2025-11-05T02:49:59Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669807",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669807"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669807"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669807/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": null,
    "original_line": 7,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 7,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669815",
    "pull_request_review_id": 3419457803,
    "id": 2492669815,
    "node_id": "PRRC_kwDOKSXUF86Ukx93",
    "diff_hunk": "@@ -0,0 +1,29 @@\n+using System;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System.Collections.Generic;\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.RerankingStrategies\n+{\n+    /// <summary>\n+    /// Base class for reranking strategies.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public abstract class RerankingStrategyBase<T>\n+    {\n+        /// <summary>\n+        /// Provides mathematical operations for the numeric type T.\n+        /// </summary>\n+        protected static readonly INumericOperations<T> NumOps = MathHelper.GetNumericOperations<T>();\n+\n+        /// <summary>\n+        /// Reranks a list of documents based on their relevance to the query.\n+        /// </summary>\n+        /// <param name=\"query\">The query string.</param>\n+        /// <param name=\"documents\">The documents to rerank.</param>\n+        /// <param name=\"topK\">The number of top documents to return.</param>\n+        /// <returns>A reranked list of documents.</returns>\n+        public abstract List<Document<T>> Rerank(string query, List<Document<T>> documents, int topK);",
    "path": "src/RetrievalAugmentedGeneration/Rerankers/RerankingStrategyBase.cs",
    "commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n<details>\n<summary>≡ƒº⌐ Analysis chain</summary>\n\n**Verify if this base class is actually used.**\n\nThe file introduces `RerankingStrategyBase<T>`, but the concrete implementation in this PR (`LostInTheMiddleReranker<T>`) extends `RerankerBase<T>` from the `Rerankers` namespace instead. This suggests `RerankingStrategyBase<T>` might be unused or intended for a different purpose.\n\nAdditionally, the return type `List<Document<T>>` is less flexible than `IEnumerable<Document<T>>` used by `RerankerBase<T>` (see RerankerBase.cs lines 26-46).\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Verify if RerankingStrategyBase is used anywhere in the codebase\n\n# Search for classes inheriting from RerankingStrategyBase\nast-grep --pattern 'class $_ : RerankingStrategyBase<$_>'\n\n# Search for any references to RerankingStrategyBase\nrg -n 'RerankingStrategyBase' --type=cs -C3\n```\n\nLength of output: 788\n\n---\n\n\n\n</details>\n\n**RerankingStrategyBase<T> appears to be unused and should be removed.**\n\nBased on code analysis, `RerankingStrategyBase<T>` is never inherited by any class in the codebase. The concrete reranker implementation (`LostInTheMiddleReranker<T>`) extends `RerankerBase<T>` instead. This class has no implementations and no references anywhere else in the repository.\n\nIf this class is intended for future use, consider:\n- Adding a concrete implementation that uses it, or\n- Removing it until needed to avoid dead code\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Rerankers/RerankingStrategyBase.cs around\nlines 13 to 27, the generic abstract class RerankingStrategyBase<T> is unused\nand should be removed; delete the entire file or remove the class definition,\nupdate any project/namespace references if present, run the build and unit tests\nto ensure nothing else depends on it, and if this was intended for future use\ninstead add a concrete implementation or document the intended extension point\nbefore keeping it in the repo.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:49:54Z",
    "updated_at": "2025-11-05T02:49:59Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669815",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669815"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669815"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669815/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 13,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 27,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 27,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669821",
    "pull_request_review_id": 3419457803,
    "id": 2492669821,
    "node_id": "PRRC_kwDOKSXUF86Ukx99",
    "diff_hunk": "@@ -0,0 +1,257 @@\n+global using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+global using System.Text.RegularExpressions;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers;\n+\n+/// <summary>\n+/// Retrieves documents by identifying entities and their relationships in knowledge graph structures.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for relevance scoring (typically float or double).</typeparam>\n+/// <remarks>\n+/// <para>\n+/// GraphRetriever enhances traditional vector search by leveraging entity recognition and relationship scoring.\n+/// It extracts entities from the query (proper nouns, quoted terms, numbers), then boosts documents that contain\n+/// these entities and demonstrate relationships between them (co-occurrence within proximity). This approach is\n+/// particularly effective for queries requiring structured information or multi-hop reasoning across connected facts.\n+/// The retriever uses a hybrid scoring strategy combining base vector similarity with entity match scores (30% weight)\n+/// and relationship scores (20% weight), making it superior to plain vector search for graph-structured data.\n+/// </para>\n+/// <para><b>For Beginners:</b> Think of this like a detective searching a case file:\n+/// \n+/// A regular search finds documents by overall similarity. GraphRetriever is smarterΓÇöit:\n+/// 1. Identifies key people, places, and things in your question (entities)\n+/// 2. Looks for documents mentioning those entities\n+/// 3. Gives extra points when entities appear close together (showing relationships)\n+/// \n+/// For example, asking \"How does John know Mary?\" works better than \"John Mary relationship\" because:\n+/// - It finds documents mentioning both John AND Mary\n+/// - It prioritizes documents where John and Mary appear near each other\n+/// - It understands you're looking for connections, not just mentions\n+/// \n+/// ```csharp\n+/// var graphRetriever = new GraphRetriever<double>(\n+///     documentStore,\n+///     graphEndpoint: \"http://localhost:7200\",\n+///     graphQueryLanguage: \"SPARQL\",\n+///     maxHops: 3\n+/// );\n+/// \n+/// var results = graphRetriever.Retrieve(\"How does Einstein relate to quantum physics?\", topK: 5);\n+/// // Finds documents with both \"Einstein\" and \"quantum physics\" appearing together\n+/// ```\n+/// \n+/// Why use GraphRetriever:\n+/// - Better at finding connected facts (multi-hop questions)\n+/// - Understands relationships between entities\n+/// - Ideal for knowledge-intensive queries (science, history, technical domains)\n+/// - Works well with structured or semi-structured data\n+/// \n+/// When NOT to use it:\n+/// - Simple factual lookups (basic vector search is faster)\n+/// - Queries without clear entities (abstract concepts, opinions)\n+/// - Documents lacking entity mentions or relationships\n+/// </para>\n+/// </remarks>\n+public class GraphRetriever<T> : RetrieverBase<T>\n+{\n+    private readonly string _graphEndpoint;\n+    private readonly string _graphQueryLanguage;\n+    private readonly int _maxHops;",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/GraphRetriever.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Unused constructor parameters indicate incomplete graph integration.**\n\nThe constructor accepts and validates `graphEndpoint`, `graphQueryLanguage`, and `maxHops` parameters (lines 85-87, 90-94), but these fields are never used in the implementation. The comment at line 154 acknowledges this is a fallback implementation, but accepting parameters that aren't used creates a misleading API contract.\n\n\nConsider one of these approaches:\n\n**Option 1: Remove unused parameters until graph integration is implemented**\n```diff\n public GraphRetriever(\n-    IDocumentStore<T> documentStore,\n-    string graphEndpoint,\n-    string graphQueryLanguage,\n-    int maxHops)\n+    IDocumentStore<T> documentStore)\n {\n     _documentStore = documentStore ?? throw new ArgumentNullException(nameof(documentStore));\n-    _graphEndpoint = graphEndpoint ?? throw new ArgumentNullException(nameof(graphEndpoint));\n-    _graphQueryLanguage = graphQueryLanguage ?? throw new ArgumentNullException(nameof(graphQueryLanguage));\n-    \n-    if (maxHops <= 0)\n-        throw new ArgumentOutOfRangeException(nameof(maxHops), \"Max hops must be positive\");\n-        \n-    _maxHops = maxHops;\n }\n```\n\n**Option 2: Add TODO comment and mark class as incomplete**\n```diff\n+/// <remarks>\n+/// TODO: Graph database integration not yet implemented. Currently uses metadata-enhanced vector retrieval as fallback.\n+/// </remarks>\n public class GraphRetriever<T> : RetrieverBase<T>\n```\n\n\nAlso applies to: 83-97\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Retrievers/GraphRetriever.cs around lines\n57-59 and 83-97 the private fields _graphEndpoint, _graphQueryLanguage and\n_maxHops and their constructor parameters are validated but never used, creating\na misleading API; either remove these unused fields and constructor parameters\n(and corresponding validation) and update all call sites to stop supplying them,\nor if you intend to implement graph integration soon, keep the parameters but\nadd a clear TODO comment on the class and mark it as incomplete (e.g., /// TODO:\nimplement graph-based retrieval; currently fallback to vector DB) and annotate\nthe unused fields with a justification comment or [Obsolete] to warn callers;\npick one approach and apply it consistently across the constructor, fields,\nvalidation logic, and any XML docs or tests.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:49:55Z",
    "updated_at": "2025-11-05T02:49:59Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669821",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669821"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669821"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669821/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 57,
    "original_start_line": 57,
    "start_side": "RIGHT",
    "line": 59,
    "original_line": 59,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 59,
    "position": 59,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669830",
    "pull_request_review_id": 3419457803,
    "id": 2492669830,
    "node_id": "PRRC_kwDOKSXUF86Ukx-G",
    "diff_hunk": "@@ -0,0 +1,257 @@\n+global using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+global using System.Text.RegularExpressions;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers;\n+\n+/// <summary>\n+/// Retrieves documents by identifying entities and their relationships in knowledge graph structures.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for relevance scoring (typically float or double).</typeparam>\n+/// <remarks>\n+/// <para>\n+/// GraphRetriever enhances traditional vector search by leveraging entity recognition and relationship scoring.\n+/// It extracts entities from the query (proper nouns, quoted terms, numbers), then boosts documents that contain\n+/// these entities and demonstrate relationships between them (co-occurrence within proximity). This approach is\n+/// particularly effective for queries requiring structured information or multi-hop reasoning across connected facts.\n+/// The retriever uses a hybrid scoring strategy combining base vector similarity with entity match scores (30% weight)\n+/// and relationship scores (20% weight), making it superior to plain vector search for graph-structured data.\n+/// </para>\n+/// <para><b>For Beginners:</b> Think of this like a detective searching a case file:\n+/// \n+/// A regular search finds documents by overall similarity. GraphRetriever is smarterΓÇöit:\n+/// 1. Identifies key people, places, and things in your question (entities)\n+/// 2. Looks for documents mentioning those entities\n+/// 3. Gives extra points when entities appear close together (showing relationships)\n+/// \n+/// For example, asking \"How does John know Mary?\" works better than \"John Mary relationship\" because:\n+/// - It finds documents mentioning both John AND Mary\n+/// - It prioritizes documents where John and Mary appear near each other\n+/// - It understands you're looking for connections, not just mentions\n+/// \n+/// ```csharp\n+/// var graphRetriever = new GraphRetriever<double>(\n+///     documentStore,\n+///     graphEndpoint: \"http://localhost:7200\",\n+///     graphQueryLanguage: \"SPARQL\",\n+///     maxHops: 3\n+/// );\n+/// \n+/// var results = graphRetriever.Retrieve(\"How does Einstein relate to quantum physics?\", topK: 5);\n+/// // Finds documents with both \"Einstein\" and \"quantum physics\" appearing together\n+/// ```\n+/// \n+/// Why use GraphRetriever:\n+/// - Better at finding connected facts (multi-hop questions)\n+/// - Understands relationships between entities\n+/// - Ideal for knowledge-intensive queries (science, history, technical domains)\n+/// - Works well with structured or semi-structured data\n+/// \n+/// When NOT to use it:\n+/// - Simple factual lookups (basic vector search is faster)\n+/// - Queries without clear entities (abstract concepts, opinions)\n+/// - Documents lacking entity mentions or relationships\n+/// </para>\n+/// </remarks>\n+public class GraphRetriever<T> : RetrieverBase<T>\n+{\n+    private readonly string _graphEndpoint;\n+    private readonly string _graphQueryLanguage;\n+    private readonly int _maxHops;\n+\n+    private readonly IDocumentStore<T> _documentStore;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"GraphRetriever{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"documentStore\">The document store containing indexed documents with entity metadata.</param>\n+    /// <param name=\"graphEndpoint\">The knowledge graph database endpoint URL (e.g., \"http://localhost:7200\" for GraphDB).</param>\n+    /// <param name=\"graphQueryLanguage\">The graph query language used by the endpoint (e.g., \"SPARQL\" for RDF stores, \"Cypher\" for Neo4j).</param>\n+    /// <param name=\"maxHops\">Maximum number of relationship hops to traverse when exploring the graph (1-3 recommended for performance).</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when documentStore, graphEndpoint, or graphQueryLanguage is null.</exception>\n+    /// <exception cref=\"ArgumentOutOfRangeException\">Thrown when maxHops is less than or equal to zero.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This constructor sets up the retriever with your document storage and graph database connection.\n+    /// \n+    /// The maxHops parameter controls how far the retriever looks for connectionsΓÇöthink of it like \"degrees of separation\":\n+    /// - maxHops = 1: Only direct connections (e.g., Einstein ΓåÆ relativity)\n+    /// - maxHops = 2: Friends of friends (e.g., Einstein ΓåÆ relativity ΓåÆ quantum mechanics)\n+    /// - maxHops = 3: Extended network (rarely needed, slower)\n+    /// \n+    /// Most queries work well with maxHops = 2.\n+    /// </para>\n+    /// </remarks>\n+    public GraphRetriever(\n+        IDocumentStore<T> documentStore,\n+        string graphEndpoint,\n+        string graphQueryLanguage,\n+        int maxHops)\n+    {\n+        _documentStore = documentStore ?? throw new ArgumentNullException(nameof(documentStore));\n+        _graphEndpoint = graphEndpoint ?? throw new ArgumentNullException(nameof(graphEndpoint));\n+        _graphQueryLanguage = graphQueryLanguage ?? throw new ArgumentNullException(nameof(graphQueryLanguage));\n+        \n+        if (maxHops <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(maxHops), \"Max hops must be positive\");\n+            \n+        _maxHops = maxHops;\n+    }\n+\n+    /// <summary>\n+    /// Retrieves relevant documents by extracting entities from the query and scoring based on entity matches and relationships.\n+    /// </summary>\n+    /// <param name=\"query\">The validated search query (non-empty).</param>\n+    /// <param name=\"topK\">The validated number of documents to return (positive integer).</param>\n+    /// <param name=\"metadataFilters\">The validated metadata filters for document selection.</param>\n+    /// <returns>Documents ordered by enhanced relevance score (combining vector similarity, entity matches, and relationship proximity).</returns>\n+    /// <exception cref=\"ArgumentException\">Thrown when query is null or whitespace.</exception>\n+    /// <exception cref=\"ArgumentOutOfRangeException\">Thrown when topK is less than or equal to zero.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// This method implements a three-stage retrieval pipeline:\n+    /// 1. Entity Extraction: Uses regex patterns to identify proper nouns, quoted terms, and numbers from the query\n+    /// 2. Enhanced Retrieval: Oversamples documents (topK * 2) with entity filters for comprehensive coverage\n+    /// 3. Relationship Scoring: Calculates entity co-occurrence scores within 200-character windows, then combines:\n+    ///    - Base vector similarity (100% weight)\n+    ///    - Entity match score (30% boost)\n+    ///    - Relationship score (20% boost)\n+    /// \n+    /// The algorithm prioritizes documents where query entities appear together, indicating factual relationships.\n+    /// For production systems, this would integrate with graph databases (e.g., Neo4j, GraphDB) via SPARQL/Cypher queries.\n+    /// The current implementation uses metadata-enhanced vector retrieval as a fallback.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This method does the actual searching. Here's what happens:\n+    /// \n+    /// Step 1: Extract entities from your query\n+    /// - Query: \"How did Marie Curie discover radium?\"\n+    /// - Entities found: [\"Marie Curie\", \"radium\"]\n+    /// \n+    /// Step 2: Find documents mentioning these entities\n+    /// - Gets more documents than needed (topK * 2) to ensure good coverage\n+    /// \n+    /// Step 3: Score documents based on:\n+    /// - Do they mention the entities? (+30% boost)\n+    /// - Do the entities appear near each other? (+20% boost if within ~200 characters)\n+    /// \n+    /// Step 4: Return the best topK documents\n+    /// \n+    /// The result is documents that don't just mention your keywords, but actually connect them together.\n+    /// </para>\n+    /// </remarks>\n+    protected override IEnumerable<Document<T>> RetrieveCore(\n+        string query,\n+        int topK,\n+        Dictionary<string, object> metadataFilters)\n+    {\n+        if (string.IsNullOrWhiteSpace(query))\n+            throw new ArgumentException(\"Query cannot be null or whitespace\", nameof(query));\n+\n+        if (topK <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(topK), \"topK must be positive\");\n+\n+        // Extract entities from query (simple NER)\n+        var entities = ExtractEntities(query);\n+\n+        // For production, this would query a graph database\n+        // Fallback: Use metadata-enhanced vector retrieval\n+        var enhancedFilters = new Dictionary<string, object>(metadataFilters ?? new Dictionary<string, object>());\n+        \n+        // Add entity filters if entities were found\n+        if (entities.Count > 0)\n+        {\n+            enhancedFilters[\"entities\"] = entities;\n+        }\n+\n+        // Retrieve documents with entity context\n+        var documents = _documentStore.GetSimilarWithFilters(\n+            new Vector<T>(new T[0]), // Placeholder\n+            topK * 2, // Oversample for filtering\n+            enhancedFilters\n+        ).ToList();",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/GraphRetriever.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Placeholder vector breaks the retrieval contract.**\n\nLine 166 creates an empty vector `new Vector<T>(new T[0])` as a placeholder because the implementation doesn't actually use vector similarity. This contradicts:\n1. The documentation claiming \"base vector similarity (100% weight)\" (line 114)\n2. The method signature of `GetSimilarWithFilters` which expects a meaningful query vector\n3. The architectural pattern shown in `VectorRetriever.cs` which embeds the query first\n\nThe document store will likely ignore the empty vector or produce incorrect results.\n\n\nIf vector similarity is not needed, use a different retrieval method. If it is needed, embed the query first:\n\n```diff\n+    // This retriever needs an embedding model to create query vectors\n+    private readonly IEmbeddingModel<T> _embeddingModel;\n+    \n+    public GraphRetriever(\n+        IDocumentStore<T> documentStore,\n+        IEmbeddingModel<T> embeddingModel)  // Add embedding model parameter\n+    {\n+        _documentStore = documentStore ?? throw new ArgumentNullException(nameof(documentStore));\n+        _embeddingModel = embeddingModel ?? throw new ArgumentNullException(nameof(embeddingModel));\n+    }\n```\n\nThen in `RetrieveCore`:\n```diff\n+    // Embed the query for vector similarity\n+    var queryVector = _embeddingModel.Embed(query);\n+    \n     // Retrieve documents with entity context\n     var documents = _documentStore.GetSimilarWithFilters(\n-        new Vector<T>(new T[0]), // Placeholder\n+        queryVector,\n         topK * 2,\n         enhancedFilters\n     ).ToList();\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:49:55Z",
    "updated_at": "2025-11-05T02:49:59Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669830",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669830"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669830"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669830/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 165,
    "original_start_line": 165,
    "start_side": "RIGHT",
    "line": 169,
    "original_line": 169,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 169,
    "position": 169,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669835",
    "pull_request_review_id": 3419457803,
    "id": 2492669835,
    "node_id": "PRRC_kwDOKSXUF86Ukx-L",
    "diff_hunk": "@@ -0,0 +1,257 @@\n+global using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+global using System.Text.RegularExpressions;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers;\n+\n+/// <summary>\n+/// Retrieves documents by identifying entities and their relationships in knowledge graph structures.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for relevance scoring (typically float or double).</typeparam>\n+/// <remarks>\n+/// <para>\n+/// GraphRetriever enhances traditional vector search by leveraging entity recognition and relationship scoring.\n+/// It extracts entities from the query (proper nouns, quoted terms, numbers), then boosts documents that contain\n+/// these entities and demonstrate relationships between them (co-occurrence within proximity). This approach is\n+/// particularly effective for queries requiring structured information or multi-hop reasoning across connected facts.\n+/// The retriever uses a hybrid scoring strategy combining base vector similarity with entity match scores (30% weight)\n+/// and relationship scores (20% weight), making it superior to plain vector search for graph-structured data.\n+/// </para>\n+/// <para><b>For Beginners:</b> Think of this like a detective searching a case file:\n+/// \n+/// A regular search finds documents by overall similarity. GraphRetriever is smarterΓÇöit:\n+/// 1. Identifies key people, places, and things in your question (entities)\n+/// 2. Looks for documents mentioning those entities\n+/// 3. Gives extra points when entities appear close together (showing relationships)\n+/// \n+/// For example, asking \"How does John know Mary?\" works better than \"John Mary relationship\" because:\n+/// - It finds documents mentioning both John AND Mary\n+/// - It prioritizes documents where John and Mary appear near each other\n+/// - It understands you're looking for connections, not just mentions\n+/// \n+/// ```csharp\n+/// var graphRetriever = new GraphRetriever<double>(\n+///     documentStore,\n+///     graphEndpoint: \"http://localhost:7200\",\n+///     graphQueryLanguage: \"SPARQL\",\n+///     maxHops: 3\n+/// );\n+/// \n+/// var results = graphRetriever.Retrieve(\"How does Einstein relate to quantum physics?\", topK: 5);\n+/// // Finds documents with both \"Einstein\" and \"quantum physics\" appearing together\n+/// ```\n+/// \n+/// Why use GraphRetriever:\n+/// - Better at finding connected facts (multi-hop questions)\n+/// - Understands relationships between entities\n+/// - Ideal for knowledge-intensive queries (science, history, technical domains)\n+/// - Works well with structured or semi-structured data\n+/// \n+/// When NOT to use it:\n+/// - Simple factual lookups (basic vector search is faster)\n+/// - Queries without clear entities (abstract concepts, opinions)\n+/// - Documents lacking entity mentions or relationships\n+/// </para>\n+/// </remarks>\n+public class GraphRetriever<T> : RetrieverBase<T>\n+{\n+    private readonly string _graphEndpoint;\n+    private readonly string _graphQueryLanguage;\n+    private readonly int _maxHops;\n+\n+    private readonly IDocumentStore<T> _documentStore;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"GraphRetriever{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"documentStore\">The document store containing indexed documents with entity metadata.</param>\n+    /// <param name=\"graphEndpoint\">The knowledge graph database endpoint URL (e.g., \"http://localhost:7200\" for GraphDB).</param>\n+    /// <param name=\"graphQueryLanguage\">The graph query language used by the endpoint (e.g., \"SPARQL\" for RDF stores, \"Cypher\" for Neo4j).</param>\n+    /// <param name=\"maxHops\">Maximum number of relationship hops to traverse when exploring the graph (1-3 recommended for performance).</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when documentStore, graphEndpoint, or graphQueryLanguage is null.</exception>\n+    /// <exception cref=\"ArgumentOutOfRangeException\">Thrown when maxHops is less than or equal to zero.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This constructor sets up the retriever with your document storage and graph database connection.\n+    /// \n+    /// The maxHops parameter controls how far the retriever looks for connectionsΓÇöthink of it like \"degrees of separation\":\n+    /// - maxHops = 1: Only direct connections (e.g., Einstein ΓåÆ relativity)\n+    /// - maxHops = 2: Friends of friends (e.g., Einstein ΓåÆ relativity ΓåÆ quantum mechanics)\n+    /// - maxHops = 3: Extended network (rarely needed, slower)\n+    /// \n+    /// Most queries work well with maxHops = 2.\n+    /// </para>\n+    /// </remarks>\n+    public GraphRetriever(\n+        IDocumentStore<T> documentStore,\n+        string graphEndpoint,\n+        string graphQueryLanguage,\n+        int maxHops)\n+    {\n+        _documentStore = documentStore ?? throw new ArgumentNullException(nameof(documentStore));\n+        _graphEndpoint = graphEndpoint ?? throw new ArgumentNullException(nameof(graphEndpoint));\n+        _graphQueryLanguage = graphQueryLanguage ?? throw new ArgumentNullException(nameof(graphQueryLanguage));\n+        \n+        if (maxHops <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(maxHops), \"Max hops must be positive\");\n+            \n+        _maxHops = maxHops;\n+    }\n+\n+    /// <summary>\n+    /// Retrieves relevant documents by extracting entities from the query and scoring based on entity matches and relationships.\n+    /// </summary>\n+    /// <param name=\"query\">The validated search query (non-empty).</param>\n+    /// <param name=\"topK\">The validated number of documents to return (positive integer).</param>\n+    /// <param name=\"metadataFilters\">The validated metadata filters for document selection.</param>\n+    /// <returns>Documents ordered by enhanced relevance score (combining vector similarity, entity matches, and relationship proximity).</returns>\n+    /// <exception cref=\"ArgumentException\">Thrown when query is null or whitespace.</exception>\n+    /// <exception cref=\"ArgumentOutOfRangeException\">Thrown when topK is less than or equal to zero.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// This method implements a three-stage retrieval pipeline:\n+    /// 1. Entity Extraction: Uses regex patterns to identify proper nouns, quoted terms, and numbers from the query\n+    /// 2. Enhanced Retrieval: Oversamples documents (topK * 2) with entity filters for comprehensive coverage\n+    /// 3. Relationship Scoring: Calculates entity co-occurrence scores within 200-character windows, then combines:\n+    ///    - Base vector similarity (100% weight)\n+    ///    - Entity match score (30% boost)\n+    ///    - Relationship score (20% boost)\n+    /// \n+    /// The algorithm prioritizes documents where query entities appear together, indicating factual relationships.\n+    /// For production systems, this would integrate with graph databases (e.g., Neo4j, GraphDB) via SPARQL/Cypher queries.\n+    /// The current implementation uses metadata-enhanced vector retrieval as a fallback.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This method does the actual searching. Here's what happens:\n+    /// \n+    /// Step 1: Extract entities from your query\n+    /// - Query: \"How did Marie Curie discover radium?\"\n+    /// - Entities found: [\"Marie Curie\", \"radium\"]\n+    /// \n+    /// Step 2: Find documents mentioning these entities\n+    /// - Gets more documents than needed (topK * 2) to ensure good coverage\n+    /// \n+    /// Step 3: Score documents based on:\n+    /// - Do they mention the entities? (+30% boost)\n+    /// - Do the entities appear near each other? (+20% boost if within ~200 characters)\n+    /// \n+    /// Step 4: Return the best topK documents\n+    /// \n+    /// The result is documents that don't just mention your keywords, but actually connect them together.\n+    /// </para>\n+    /// </remarks>\n+    protected override IEnumerable<Document<T>> RetrieveCore(\n+        string query,\n+        int topK,\n+        Dictionary<string, object> metadataFilters)\n+    {\n+        if (string.IsNullOrWhiteSpace(query))\n+            throw new ArgumentException(\"Query cannot be null or whitespace\", nameof(query));\n+\n+        if (topK <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(topK), \"topK must be positive\");\n+\n+        // Extract entities from query (simple NER)\n+        var entities = ExtractEntities(query);\n+\n+        // For production, this would query a graph database\n+        // Fallback: Use metadata-enhanced vector retrieval\n+        var enhancedFilters = new Dictionary<string, object>(metadataFilters ?? new Dictionary<string, object>());\n+        \n+        // Add entity filters if entities were found\n+        if (entities.Count > 0)\n+        {\n+            enhancedFilters[\"entities\"] = entities;\n+        }\n+\n+        // Retrieve documents with entity context\n+        var documents = _documentStore.GetSimilarWithFilters(\n+            new Vector<T>(new T[0]), // Placeholder\n+            topK * 2, // Oversample for filtering\n+            enhancedFilters\n+        ).ToList();\n+\n+        // Score documents based on entity matches and relationships\n+        var scoredDocuments = documents.Select(doc =>\n+        {\n+            var entityScore = CalculateEntityMatchScore(doc, entities);\n+            var relationshipScore = CalculateRelationshipScore(doc, entities);\n+            \n+            // Combine scores\n+            var baseScore = Convert.ToDouble(doc.RelevanceScore);\n+            var enhancedScore = baseScore * (1.0 + entityScore * 0.3 + relationshipScore * 0.2);\n+            \n+            return (doc, NumOps.FromDouble(enhancedScore));",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/GraphRetriever.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Use NumOps throughout for generic numeric operations.**\n\nLines 178-179 use `Convert.ToDouble` and double arithmetic, then convert back with `NumOps.FromDouble`. This breaks the generic abstraction and can fail for custom numeric types that aren't convertible to/from double.\n\n\nApply this diff to use `NumOps` consistently:\n\n```diff\n     var scoredDocuments = documents.Select(doc =>\n     {\n         var entityScore = CalculateEntityMatchScore(doc, entities);\n         var relationshipScore = CalculateRelationshipScore(doc, entities);\n         \n         // Combine scores\n-        var baseScore = Convert.ToDouble(doc.RelevanceScore);\n-        var enhancedScore = baseScore * (1.0 + entityScore * 0.3 + relationshipScore * 0.2);\n+        var entityBoost = NumOps.FromDouble(1.0 + entityScore * 0.3 + relationshipScore * 0.2);\n+        var enhancedScore = NumOps.Multiply(doc.RelevanceScore, entityBoost);\n         \n-        return (doc, NumOps.FromDouble(enhancedScore));\n+        return (doc, enhancedScore);\n     }).ToList();\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Retrievers/GraphRetriever.cs around lines\n178 to 181, the code converts doc.RelevanceScore to double and performs\narithmetic, then wraps the result with NumOps.FromDouble which breaks the\ngeneric numeric abstraction; replace the Convert.ToDouble and raw double math\nwith NumOps operations: obtain a numeric representation of doc.RelevanceScore\nvia the appropriate NumOps conversion, compute the weighted factors and enhanced\nscore using NumOps.Add/NumOps.Multiply (and any other NumOps helpers needed) so\nall arithmetic stays in the generic numeric type, and return the resulting\nNumOps value directly.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:49:55Z",
    "updated_at": "2025-11-05T02:49:59Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669835",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669835"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669835"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669835/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 178,
    "original_start_line": 178,
    "start_side": "RIGHT",
    "line": 181,
    "original_line": 181,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 181,
    "position": 181,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669839",
    "pull_request_review_id": 3419457803,
    "id": 2492669839,
    "node_id": "PRRC_kwDOKSXUF86Ukx-P",
    "diff_hunk": "@@ -0,0 +1,257 @@\n+global using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+global using System.Text.RegularExpressions;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers;\n+\n+/// <summary>\n+/// Retrieves documents by identifying entities and their relationships in knowledge graph structures.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for relevance scoring (typically float or double).</typeparam>\n+/// <remarks>\n+/// <para>\n+/// GraphRetriever enhances traditional vector search by leveraging entity recognition and relationship scoring.\n+/// It extracts entities from the query (proper nouns, quoted terms, numbers), then boosts documents that contain\n+/// these entities and demonstrate relationships between them (co-occurrence within proximity). This approach is\n+/// particularly effective for queries requiring structured information or multi-hop reasoning across connected facts.\n+/// The retriever uses a hybrid scoring strategy combining base vector similarity with entity match scores (30% weight)\n+/// and relationship scores (20% weight), making it superior to plain vector search for graph-structured data.\n+/// </para>\n+/// <para><b>For Beginners:</b> Think of this like a detective searching a case file:\n+/// \n+/// A regular search finds documents by overall similarity. GraphRetriever is smarterΓÇöit:\n+/// 1. Identifies key people, places, and things in your question (entities)\n+/// 2. Looks for documents mentioning those entities\n+/// 3. Gives extra points when entities appear close together (showing relationships)\n+/// \n+/// For example, asking \"How does John know Mary?\" works better than \"John Mary relationship\" because:\n+/// - It finds documents mentioning both John AND Mary\n+/// - It prioritizes documents where John and Mary appear near each other\n+/// - It understands you're looking for connections, not just mentions\n+/// \n+/// ```csharp\n+/// var graphRetriever = new GraphRetriever<double>(\n+///     documentStore,\n+///     graphEndpoint: \"http://localhost:7200\",\n+///     graphQueryLanguage: \"SPARQL\",\n+///     maxHops: 3\n+/// );\n+/// \n+/// var results = graphRetriever.Retrieve(\"How does Einstein relate to quantum physics?\", topK: 5);\n+/// // Finds documents with both \"Einstein\" and \"quantum physics\" appearing together\n+/// ```\n+/// \n+/// Why use GraphRetriever:\n+/// - Better at finding connected facts (multi-hop questions)\n+/// - Understands relationships between entities\n+/// - Ideal for knowledge-intensive queries (science, history, technical domains)\n+/// - Works well with structured or semi-structured data\n+/// \n+/// When NOT to use it:\n+/// - Simple factual lookups (basic vector search is faster)\n+/// - Queries without clear entities (abstract concepts, opinions)\n+/// - Documents lacking entity mentions or relationships\n+/// </para>\n+/// </remarks>\n+public class GraphRetriever<T> : RetrieverBase<T>\n+{\n+    private readonly string _graphEndpoint;\n+    private readonly string _graphQueryLanguage;\n+    private readonly int _maxHops;\n+\n+    private readonly IDocumentStore<T> _documentStore;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"GraphRetriever{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"documentStore\">The document store containing indexed documents with entity metadata.</param>\n+    /// <param name=\"graphEndpoint\">The knowledge graph database endpoint URL (e.g., \"http://localhost:7200\" for GraphDB).</param>\n+    /// <param name=\"graphQueryLanguage\">The graph query language used by the endpoint (e.g., \"SPARQL\" for RDF stores, \"Cypher\" for Neo4j).</param>\n+    /// <param name=\"maxHops\">Maximum number of relationship hops to traverse when exploring the graph (1-3 recommended for performance).</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when documentStore, graphEndpoint, or graphQueryLanguage is null.</exception>\n+    /// <exception cref=\"ArgumentOutOfRangeException\">Thrown when maxHops is less than or equal to zero.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This constructor sets up the retriever with your document storage and graph database connection.\n+    /// \n+    /// The maxHops parameter controls how far the retriever looks for connectionsΓÇöthink of it like \"degrees of separation\":\n+    /// - maxHops = 1: Only direct connections (e.g., Einstein ΓåÆ relativity)\n+    /// - maxHops = 2: Friends of friends (e.g., Einstein ΓåÆ relativity ΓåÆ quantum mechanics)\n+    /// - maxHops = 3: Extended network (rarely needed, slower)\n+    /// \n+    /// Most queries work well with maxHops = 2.\n+    /// </para>\n+    /// </remarks>\n+    public GraphRetriever(\n+        IDocumentStore<T> documentStore,\n+        string graphEndpoint,\n+        string graphQueryLanguage,\n+        int maxHops)\n+    {\n+        _documentStore = documentStore ?? throw new ArgumentNullException(nameof(documentStore));\n+        _graphEndpoint = graphEndpoint ?? throw new ArgumentNullException(nameof(graphEndpoint));\n+        _graphQueryLanguage = graphQueryLanguage ?? throw new ArgumentNullException(nameof(graphQueryLanguage));\n+        \n+        if (maxHops <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(maxHops), \"Max hops must be positive\");\n+            \n+        _maxHops = maxHops;\n+    }\n+\n+    /// <summary>\n+    /// Retrieves relevant documents by extracting entities from the query and scoring based on entity matches and relationships.\n+    /// </summary>\n+    /// <param name=\"query\">The validated search query (non-empty).</param>\n+    /// <param name=\"topK\">The validated number of documents to return (positive integer).</param>\n+    /// <param name=\"metadataFilters\">The validated metadata filters for document selection.</param>\n+    /// <returns>Documents ordered by enhanced relevance score (combining vector similarity, entity matches, and relationship proximity).</returns>\n+    /// <exception cref=\"ArgumentException\">Thrown when query is null or whitespace.</exception>\n+    /// <exception cref=\"ArgumentOutOfRangeException\">Thrown when topK is less than or equal to zero.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// This method implements a three-stage retrieval pipeline:\n+    /// 1. Entity Extraction: Uses regex patterns to identify proper nouns, quoted terms, and numbers from the query\n+    /// 2. Enhanced Retrieval: Oversamples documents (topK * 2) with entity filters for comprehensive coverage\n+    /// 3. Relationship Scoring: Calculates entity co-occurrence scores within 200-character windows, then combines:\n+    ///    - Base vector similarity (100% weight)\n+    ///    - Entity match score (30% boost)\n+    ///    - Relationship score (20% boost)\n+    /// \n+    /// The algorithm prioritizes documents where query entities appear together, indicating factual relationships.\n+    /// For production systems, this would integrate with graph databases (e.g., Neo4j, GraphDB) via SPARQL/Cypher queries.\n+    /// The current implementation uses metadata-enhanced vector retrieval as a fallback.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This method does the actual searching. Here's what happens:\n+    /// \n+    /// Step 1: Extract entities from your query\n+    /// - Query: \"How did Marie Curie discover radium?\"\n+    /// - Entities found: [\"Marie Curie\", \"radium\"]\n+    /// \n+    /// Step 2: Find documents mentioning these entities\n+    /// - Gets more documents than needed (topK * 2) to ensure good coverage\n+    /// \n+    /// Step 3: Score documents based on:\n+    /// - Do they mention the entities? (+30% boost)\n+    /// - Do the entities appear near each other? (+20% boost if within ~200 characters)\n+    /// \n+    /// Step 4: Return the best topK documents\n+    /// \n+    /// The result is documents that don't just mention your keywords, but actually connect them together.\n+    /// </para>\n+    /// </remarks>\n+    protected override IEnumerable<Document<T>> RetrieveCore(\n+        string query,\n+        int topK,\n+        Dictionary<string, object> metadataFilters)\n+    {\n+        if (string.IsNullOrWhiteSpace(query))\n+            throw new ArgumentException(\"Query cannot be null or whitespace\", nameof(query));\n+\n+        if (topK <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(topK), \"topK must be positive\");\n+\n+        // Extract entities from query (simple NER)\n+        var entities = ExtractEntities(query);\n+\n+        // For production, this would query a graph database\n+        // Fallback: Use metadata-enhanced vector retrieval\n+        var enhancedFilters = new Dictionary<string, object>(metadataFilters ?? new Dictionary<string, object>());\n+        \n+        // Add entity filters if entities were found\n+        if (entities.Count > 0)\n+        {\n+            enhancedFilters[\"entities\"] = entities;\n+        }\n+\n+        // Retrieve documents with entity context\n+        var documents = _documentStore.GetSimilarWithFilters(\n+            new Vector<T>(new T[0]), // Placeholder\n+            topK * 2, // Oversample for filtering\n+            enhancedFilters\n+        ).ToList();\n+\n+        // Score documents based on entity matches and relationships\n+        var scoredDocuments = documents.Select(doc =>\n+        {\n+            var entityScore = CalculateEntityMatchScore(doc, entities);\n+            var relationshipScore = CalculateRelationshipScore(doc, entities);\n+            \n+            // Combine scores\n+            var baseScore = Convert.ToDouble(doc.RelevanceScore);\n+            var enhancedScore = baseScore * (1.0 + entityScore * 0.3 + relationshipScore * 0.2);\n+            \n+            return (doc, NumOps.FromDouble(enhancedScore));\n+        }).ToList();\n+\n+        // Return top-K documents sorted by enhanced score\n+        return scoredDocuments\n+            .OrderByDescending(x => x.Item2)\n+            .Take(topK)\n+            .Select(x =>\n+            {\n+                x.doc.RelevanceScore = x.Item2;\n+                x.doc.HasRelevanceScore = true;\n+                return x.doc;\n+            });\n+    }\n+\n+    private List<string> ExtractEntities(string text)\n+    {\n+        var entities = new List<string>();\n+\n+        // Extract capitalized phrases (simple proper noun detection)\n+        var capitalizedMatches = Regex.Matches(text, @\"\\b[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*\\b\");\n+        entities.AddRange(capitalizedMatches.Cast<Match>().Select(m => m.Value));\n+\n+        // Extract quoted terms\n+        var quotedMatches = Regex.Matches(text, @\"\"\"([^\"\"]+)\"\"\");\n+        entities.AddRange(quotedMatches.Cast<Match>().Select(m => m.Groups[1].Value));\n+\n+        // Extract numbers and dates\n+        var numberMatches = Regex.Matches(text, @\"\\b\\d+(?:\\.\\d+)?\\b\");\n+        entities.AddRange(numberMatches.Cast<Match>().Select(m => m.Value));\n+\n+        return entities.Distinct().ToList();\n+    }\n+\n+    private double CalculateEntityMatchScore(Document<T> document, List<string> entities)\n+    {\n+        if (entities.Count == 0)\n+            return 0.0;\n+\n+        var contentLower = document.Content.ToLower();\n+        var matchCount = entities.Count(e => contentLower.Contains(e.ToLower()));\n+\n+        return (double)matchCount / entities.Count;\n+    }\n+\n+    private double CalculateRelationshipScore(Document<T> document, List<string> entities)\n+    {\n+        if (entities.Count < 2)\n+            return 0.0;\n+\n+        // Check for co-occurrence of entities (indicating relationships)\n+        var relationshipCount = 0;\n+        var content = document.Content;\n+\n+        for (int i = 0; i < entities.Count - 1; i++)\n+        {\n+            for (int j = i + 1; j < entities.Count; j++)\n+            {\n+                // Check if both entities appear close to each other\n+                var entity1Pos = content.IndexOf(entities[i], StringComparison.OrdinalIgnoreCase);\n+                var entity2Pos = content.IndexOf(entities[j], StringComparison.OrdinalIgnoreCase);\n+\n+                if (entity1Pos >= 0 && entity2Pos >= 0)\n+                {\n+                    var distance = Math.Abs(entity1Pos - entity2Pos);\n+                    if (distance < 200) // Within 200 characters",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/GraphRetriever.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**IndexOf limitation may undercount relationships.**\n\nLines 240-241 use `IndexOf` which only finds the *first* occurrence of each entity. If entities appear multiple times in the document, this approach misses additional relationship opportunities.\n\nExample:\n```\nContent: \"Einstein studied physics. Later, Einstein published papers on quantum physics.\"\nEntities: [\"Einstein\", \"physics\"]\n```\n\nCurrent logic finds only the first \"Einstein\" (position 0) and first \"physics\" (position 17), missing the second co-occurrence of \"Einstein\" and \"physics\" near the end where they're even closer together.\n\n\nFor more accurate relationship detection, consider finding all occurrences:\n\n```diff\n private double CalculateRelationshipScore(Document<T> document, List<string> entities)\n {\n     if (entities.Count < 2)\n         return 0.0;\n \n     var relationshipCount = 0;\n     var content = document.Content;\n \n     for (int i = 0; i < entities.Count - 1; i++)\n     {\n         for (int j = i + 1; j < entities.Count; j++)\n         {\n-            // Check if both entities appear close to each other\n-            var entity1Pos = content.IndexOf(entities[i], StringComparison.OrdinalIgnoreCase);\n-            var entity2Pos = content.IndexOf(entities[j], StringComparison.OrdinalIgnoreCase);\n-\n-            if (entity1Pos >= 0 && entity2Pos >= 0)\n+            // Find all occurrences of both entities\n+            var positions1 = FindAllOccurrences(content, entities[i]);\n+            var positions2 = FindAllOccurrences(content, entities[j]);\n+            \n+            // Check if any pair appears within proximity\n+            foreach (var pos1 in positions1)\n             {\n-                var distance = Math.Abs(entity1Pos - entity2Pos);\n-                if (distance < 200)\n+                foreach (var pos2 in positions2)\n                 {\n-                    relationshipCount++;\n+                    var distance = Math.Abs(pos1 - pos2);\n+                    if (distance < 200)\n+                    {\n+                        relationshipCount++;\n+                        break; // Count this pair once\n+                    }\n                 }\n             }\n         }\n     }\n \n     var maxPossibleRelationships = (entities.Count * (entities.Count - 1)) / 2;\n     return maxPossibleRelationships > 0 ? (double)relationshipCount / maxPossibleRelationships : 0.0;\n }\n+\n+private List<int> FindAllOccurrences(string content, string entity)\n+{\n+    var positions = new List<int>();\n+    int index = 0;\n+    while ((index = content.IndexOf(entity, index, StringComparison.OrdinalIgnoreCase)) >= 0)\n+    {\n+        positions.Add(index);\n+        index += entity.Length;\n+    }\n+    return positions;\n+}\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\nprivate double CalculateRelationshipScore(Document<T> document, List<string> entities)\n{\n    if (entities.Count < 2)\n        return 0.0;\n\n    var relationshipCount = 0;\n    var content = document.Content;\n\n    for (int i = 0; i < entities.Count - 1; i++)\n    {\n        for (int j = i + 1; j < entities.Count; j++)\n        {\n            // Find all occurrences of both entities\n            var positions1 = FindAllOccurrences(content, entities[i]);\n            var positions2 = FindAllOccurrences(content, entities[j]);\n            \n            // Check if any pair appears within proximity\n            foreach (var pos1 in positions1)\n            {\n                foreach (var pos2 in positions2)\n                {\n                    var distance = Math.Abs(pos1 - pos2);\n                    if (distance < 200)\n                    {\n                        relationshipCount++;\n                        break; // Count this pair once\n                    }\n                }\n            }\n        }\n    }\n\n    var maxPossibleRelationships = (entities.Count * (entities.Count - 1)) / 2;\n    return maxPossibleRelationships > 0 ? (double)relationshipCount / maxPossibleRelationships : 0.0;\n}\n\nprivate List<int> FindAllOccurrences(string content, string entity)\n{\n    var positions = new List<int>();\n    int index = 0;\n    while ((index = content.IndexOf(entity, index, StringComparison.OrdinalIgnoreCase)) >= 0)\n    {\n        positions.Add(index);\n        index += entity.Length;\n    }\n    return positions;\n}\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Retrievers/GraphRetriever.cs around lines\n240 to 246, the code uses IndexOf once per entity which only finds the first\noccurrence and therefore undercounts relationships; update the logic to find all\ncase-insensitive occurrences for each entity (e.g., loop using IndexOf with a\nstart index or use a case-insensitive Regex/Matches) collect their positions\ninto lists, then compute distances between every occurrence pair and apply the\nsame proximity check (<200 chars) to create relationships; ensure you handle\noverlapping matches and advance the search index to avoid infinite loops.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:49:55Z",
    "updated_at": "2025-11-05T02:49:59Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669839",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669839"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669839"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669839/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 240,
    "original_start_line": 240,
    "start_side": "RIGHT",
    "line": 246,
    "original_line": 246,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 246,
    "position": 246,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669845",
    "pull_request_review_id": 3419457803,
    "id": 2492669845,
    "node_id": "PRRC_kwDOKSXUF86Ukx-V",
    "diff_hunk": "@@ -0,0 +1,263 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers;\n+\n+/// <summary>\n+/// Retrieves precise small chunks for matching but returns complete parent documents for comprehensive context.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for relevance scoring (typically float or double).</typeparam>\n+/// <remarks>\n+/// <para>\n+/// ParentDocumentRetriever solves the \"chunk size dilemma\" in RAG systems: small chunks enable precise matching\n+/// but lack context, while large chunks provide context but reduce precision. This retriever uses a two-tier\n+/// approachΓÇösearch against small chunks (e.g., paragraphs) for accuracy, then return their larger parent documents\n+/// (e.g., full sections or pages) for complete context. This ensures the LLM receives sufficient information to\n+/// generate accurate answers while maintaining high retrieval precision. The retriever can optionally include\n+/// neighboring chunks to expand context boundaries. This pattern is particularly effective for structured content\n+/// (technical docs, research papers, legal documents) where individual paragraphs are meaningful but answers require\n+/// broader context.\n+/// </para>\n+/// <para><b>For Beginners:</b> Think of this like a two-step library search:\n+/// \n+/// The Problem:\n+/// - Small chunks (paragraphs): Easy to match precisely, BUT not enough context\n+/// - Large chunks (whole pages): Plenty of context, BUT hard to match precisely\n+/// \n+/// The Solution - Parent Document Retrieval:\n+/// 1. Search using SMALL chunks for precision\n+/// 2. Return the LARGE parent document for context\n+/// \n+/// Real-world example:\n+/// Query: \"How does photosynthesis work?\"\n+/// \n+/// Small chunk matches: \"...chlorophyll absorbs light energy...\" (paragraph 3)\n+/// Returns: Full page containing introduction + detailed process + diagram\n+/// \n+/// ```csharp\n+/// var retriever = new ParentDocumentRetriever<double>(\n+///     documentStore,\n+///     chunkSize: 256,                    // Small chunks for precision\n+///     parentSize: 2048,                  // Large parents for context\n+///     includeNeighboringChunks: true     // Add nearby chunks too\n+/// );\n+/// \n+/// var results = retriever.Retrieve(\"explain quantum entanglement\", topK: 3);\n+/// // Finds precise paragraphs but returns full sections with complete explanation\n+/// ```\n+/// \n+/// Why use ParentDocumentRetriever:\n+/// - Best of both worlds: precise matching + complete context\n+/// - Ideal for technical documentation and research papers\n+/// - Reduces LLM hallucinations (more context = better answers)\n+/// - Works great with structured content (headings, sections, chapters)\n+/// \n+/// When NOT to use it:\n+/// - Very short documents (chunks = parents already)\n+/// - Documents with redundant content (wastes context window)\n+/// - When you need ONLY the matching excerpt (use regular retrieval)\n+/// - Memory-constrained systems (returns more content per match)\n+/// </para>\n+/// </remarks>\n+public class ParentDocumentRetriever<T> : RetrieverBase<T>\n+{\n+    private readonly IDocumentStore<T> _documentStore;\n+    private readonly int _chunkSize;\n+    private readonly int _parentSize;\n+    private readonly bool _includeNeighboringChunks;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"ParentDocumentRetriever{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"documentStore\">The document store containing chunked documents with parent metadata.</param>\n+    /// <param name=\"chunkSize\">Character length of child chunks used for matching (typically 128-512 characters).</param>\n+    /// <param name=\"parentSize\">Character length of parent documents returned (typically 1024-4096 characters).</param>\n+    /// <param name=\"includeNeighboringChunks\">Whether to include adjacent chunks around the matched chunk (expands context boundaries).</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when documentStore is null.</exception>\n+    /// <exception cref=\"ArgumentOutOfRangeException\">Thrown when chunkSize or parentSize is less than or equal to zero.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when parentSize is less than chunkSize.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This configures how the retriever balances precision vs. context.\n+    /// \n+    /// Recommended configurations:\n+    /// \n+    /// Technical Documentation:\n+    /// - chunkSize: 256 (1-2 paragraphs)\n+    /// - parentSize: 2048 (full section)\n+    /// - includeNeighboringChunks: true (add context before/after)\n+    /// \n+    /// Research Papers:\n+    /// - chunkSize: 512 (paragraph or two)\n+    /// - parentSize: 4096 (entire subsection)\n+    /// - includeNeighboringChunks: false (rely on section boundaries)\n+    /// \n+    /// General Content:\n+    /// - chunkSize: 128 (few sentences)\n+    /// - parentSize: 1024 (multiple paragraphs)\n+    /// - includeNeighboringChunks: true (smooth transitions)\n+    /// \n+    /// The includeNeighboringChunks parameter is helpful when chunk boundaries might split important context.\n+    /// </para>\n+    /// </remarks>\n+    public ParentDocumentRetriever(\n+        IDocumentStore<T> documentStore,\n+        int chunkSize,\n+        int parentSize,\n+        bool includeNeighboringChunks)\n+    {\n+        _documentStore = documentStore ?? throw new ArgumentNullException(nameof(documentStore));\n+        \n+        if (chunkSize <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(chunkSize), \"Chunk size must be positive\");\n+            \n+        if (parentSize <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(parentSize), \"Parent size must be positive\");\n+            \n+        if (parentSize < chunkSize)\n+            throw new ArgumentException(\"Parent size must be greater than or equal to chunk size\");\n+            \n+        _chunkSize = chunkSize;\n+        _parentSize = parentSize;\n+        _includeNeighboringChunks = includeNeighboringChunks;\n+    }",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/ParentDocumentRetriever.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Critical: Missing embedding model dependency**\n\nThe constructor lacks an `IEmbeddingModel<T>` parameter. Without an embedding model, the retriever cannot convert the query string into a vector for similarity search (see line 193 issue). Compare with `VectorRetriever.cs` (lines 61-68), which embeds the query before calling `GetSimilarWithFilters`.\n\n\n\nApply this diff to add the embedding model dependency:\n\n```diff\n     public ParentDocumentRetriever(\n         IDocumentStore<T> documentStore,\n+        IEmbeddingModel<T> embeddingModel,\n         int chunkSize,\n         int parentSize,\n         bool includeNeighboringChunks)\n     {\n         _documentStore = documentStore ?? throw new ArgumentNullException(nameof(documentStore));\n+        _embeddingModel = embeddingModel ?? throw new ArgumentNullException(nameof(embeddingModel));\n         \n         if (chunkSize <= 0)\n             throw new ArgumentOutOfRangeException(nameof(chunkSize), \"Chunk size must be positive\");\n```\n\nThen add the field declaration after line 70:\n\n```diff\n     private readonly IDocumentStore<T> _documentStore;\n+    private readonly IEmbeddingModel<T> _embeddingModel;\n     private readonly int _chunkSize;\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Retrievers/ParentDocumentRetriever.cs around\nlines 108 to 128, the constructor is missing an IEmbeddingModel<T> dependency\nrequired to embed queries before similarity search; add an IEmbeddingModel<T>\nembeddingModel parameter to the constructor, validate it with\nArgumentNullException, and assign it to a private readonly field; also add the\nprivate readonly IEmbeddingModel<T> _embeddingModel field declaration after line\n70 so the class can call the embedding model where the query is converted to a\nvector before calling GetSimilarWithFilters.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:49:55Z",
    "updated_at": "2025-11-05T02:49:59Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669845",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669845"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669845"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669845/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 108,
    "original_start_line": 108,
    "start_side": "RIGHT",
    "line": 128,
    "original_line": 128,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 128,
    "position": 128,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669853",
    "pull_request_review_id": 3419457803,
    "id": 2492669853,
    "node_id": "PRRC_kwDOKSXUF86Ukx-d",
    "diff_hunk": "@@ -0,0 +1,263 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers;\n+\n+/// <summary>\n+/// Retrieves precise small chunks for matching but returns complete parent documents for comprehensive context.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for relevance scoring (typically float or double).</typeparam>\n+/// <remarks>\n+/// <para>\n+/// ParentDocumentRetriever solves the \"chunk size dilemma\" in RAG systems: small chunks enable precise matching\n+/// but lack context, while large chunks provide context but reduce precision. This retriever uses a two-tier\n+/// approachΓÇösearch against small chunks (e.g., paragraphs) for accuracy, then return their larger parent documents\n+/// (e.g., full sections or pages) for complete context. This ensures the LLM receives sufficient information to\n+/// generate accurate answers while maintaining high retrieval precision. The retriever can optionally include\n+/// neighboring chunks to expand context boundaries. This pattern is particularly effective for structured content\n+/// (technical docs, research papers, legal documents) where individual paragraphs are meaningful but answers require\n+/// broader context.\n+/// </para>\n+/// <para><b>For Beginners:</b> Think of this like a two-step library search:\n+/// \n+/// The Problem:\n+/// - Small chunks (paragraphs): Easy to match precisely, BUT not enough context\n+/// - Large chunks (whole pages): Plenty of context, BUT hard to match precisely\n+/// \n+/// The Solution - Parent Document Retrieval:\n+/// 1. Search using SMALL chunks for precision\n+/// 2. Return the LARGE parent document for context\n+/// \n+/// Real-world example:\n+/// Query: \"How does photosynthesis work?\"\n+/// \n+/// Small chunk matches: \"...chlorophyll absorbs light energy...\" (paragraph 3)\n+/// Returns: Full page containing introduction + detailed process + diagram\n+/// \n+/// ```csharp\n+/// var retriever = new ParentDocumentRetriever<double>(\n+///     documentStore,\n+///     chunkSize: 256,                    // Small chunks for precision\n+///     parentSize: 2048,                  // Large parents for context\n+///     includeNeighboringChunks: true     // Add nearby chunks too\n+/// );\n+/// \n+/// var results = retriever.Retrieve(\"explain quantum entanglement\", topK: 3);\n+/// // Finds precise paragraphs but returns full sections with complete explanation\n+/// ```\n+/// \n+/// Why use ParentDocumentRetriever:\n+/// - Best of both worlds: precise matching + complete context\n+/// - Ideal for technical documentation and research papers\n+/// - Reduces LLM hallucinations (more context = better answers)\n+/// - Works great with structured content (headings, sections, chapters)\n+/// \n+/// When NOT to use it:\n+/// - Very short documents (chunks = parents already)\n+/// - Documents with redundant content (wastes context window)\n+/// - When you need ONLY the matching excerpt (use regular retrieval)\n+/// - Memory-constrained systems (returns more content per match)\n+/// </para>\n+/// </remarks>\n+public class ParentDocumentRetriever<T> : RetrieverBase<T>\n+{\n+    private readonly IDocumentStore<T> _documentStore;\n+    private readonly int _chunkSize;\n+    private readonly int _parentSize;\n+    private readonly bool _includeNeighboringChunks;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"ParentDocumentRetriever{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"documentStore\">The document store containing chunked documents with parent metadata.</param>\n+    /// <param name=\"chunkSize\">Character length of child chunks used for matching (typically 128-512 characters).</param>\n+    /// <param name=\"parentSize\">Character length of parent documents returned (typically 1024-4096 characters).</param>\n+    /// <param name=\"includeNeighboringChunks\">Whether to include adjacent chunks around the matched chunk (expands context boundaries).</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when documentStore is null.</exception>\n+    /// <exception cref=\"ArgumentOutOfRangeException\">Thrown when chunkSize or parentSize is less than or equal to zero.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when parentSize is less than chunkSize.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This configures how the retriever balances precision vs. context.\n+    /// \n+    /// Recommended configurations:\n+    /// \n+    /// Technical Documentation:\n+    /// - chunkSize: 256 (1-2 paragraphs)\n+    /// - parentSize: 2048 (full section)\n+    /// - includeNeighboringChunks: true (add context before/after)\n+    /// \n+    /// Research Papers:\n+    /// - chunkSize: 512 (paragraph or two)\n+    /// - parentSize: 4096 (entire subsection)\n+    /// - includeNeighboringChunks: false (rely on section boundaries)\n+    /// \n+    /// General Content:\n+    /// - chunkSize: 128 (few sentences)\n+    /// - parentSize: 1024 (multiple paragraphs)\n+    /// - includeNeighboringChunks: true (smooth transitions)\n+    /// \n+    /// The includeNeighboringChunks parameter is helpful when chunk boundaries might split important context.\n+    /// </para>\n+    /// </remarks>\n+    public ParentDocumentRetriever(\n+        IDocumentStore<T> documentStore,\n+        int chunkSize,\n+        int parentSize,\n+        bool includeNeighboringChunks)\n+    {\n+        _documentStore = documentStore ?? throw new ArgumentNullException(nameof(documentStore));\n+        \n+        if (chunkSize <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(chunkSize), \"Chunk size must be positive\");\n+            \n+        if (parentSize <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(parentSize), \"Parent size must be positive\");\n+            \n+        if (parentSize < chunkSize)\n+            throw new ArgumentException(\"Parent size must be greater than or equal to chunk size\");\n+            \n+        _chunkSize = chunkSize;\n+        _parentSize = parentSize;\n+        _includeNeighboringChunks = includeNeighboringChunks;\n+    }\n+\n+    /// <summary>\n+    /// Retrieves parent documents by matching against child chunks and reconstructing full parent context.\n+    /// </summary>\n+    /// <param name=\"query\">The validated search query (non-empty).</param>\n+    /// <param name=\"topK\">The validated number of parent documents to return (positive integer).</param>\n+    /// <param name=\"metadataFilters\">The validated metadata filters for document selection.</param>\n+    /// <returns>Parent documents ordered by their best child chunk's relevance score (highest first).</returns>\n+    /// <exception cref=\"ArgumentException\">Thrown when query is null or whitespace.</exception>\n+    /// <exception cref=\"ArgumentOutOfRangeException\">Thrown when topK is less than or equal to zero.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// This method implements a hierarchical retrieval pipeline:\n+    /// 1. Oversampling: Retrieves topK * 3 child chunks to ensure sufficient parent document coverage\n+    /// 2. Parent Grouping: Groups chunks by parent_id metadata field\n+    /// 3. Score Aggregation: Assigns each parent the MAXIMUM score of its child chunks (best match wins)\n+    /// 4. Optional Expansion: If includeNeighboringChunks=true, concatenates all matching chunks for each parent\n+    /// 5. Deduplication: Returns unique parent documents (multiple chunks may belong to same parent)\n+    /// \n+    /// The retriever expects chunks to have metadata:\n+    /// - \"parent_id\": Identifier of the parent document\n+    /// - \"chunk_index\": Position of this chunk within parent (optional)\n+    /// - \"chunk_start\": Character offset where chunk begins (optional)\n+    /// \n+    /// Parent documents are reconstructed by combining chunk content and filtering chunk-specific metadata.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Here's how the retrieval process works:\n+    /// \n+    /// Step 1: Find the best matching small chunks\n+    /// - Query: \"neural network backpropagation\"\n+    /// - Matches: [Chunk 5 from Doc A (score=0.9), Chunk 12 from Doc A (score=0.7), Chunk 3 from Doc B (score=0.8)]\n+    /// \n+    /// Step 2: Group chunks by parent document\n+    /// - Doc A: Chunk 5 (0.9), Chunk 12 (0.7)\n+    /// - Doc B: Chunk 3 (0.8)\n+    /// \n+    /// Step 3: Assign parent score = best child score\n+    /// - Doc A: 0.9 (from Chunk 5)\n+    /// - Doc B: 0.8 (from Chunk 3)\n+    /// \n+    /// Step 4: Return parent documents (full context!)\n+    /// - Doc A: Complete section on backpropagation including introduction, math, examples\n+    /// - Doc B: Complete chapter on neural network training\n+    /// \n+    /// This gives your LLM ALL the context it needs to provide a complete answer!\n+    /// </para>\n+    /// </remarks>\n+    protected override IEnumerable<Document<T>> RetrieveCore(\n+        string query,\n+        int topK,\n+        Dictionary<string, object> metadataFilters)\n+    {\n+        if (string.IsNullOrWhiteSpace(query))\n+            throw new ArgumentException(\"Query cannot be null or whitespace\", nameof(query));\n+\n+        if (topK <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(topK), \"topK must be positive\");\n+\n+        // Retrieve chunks at higher K to ensure we get enough parent documents\n+        var chunkK = topK * 3;\n+        \n+        // Use the document store to find similar chunks\n+        // The chunks should have metadata indicating their parent document\n+        var similarChunks = _documentStore.GetSimilarWithFilters(\n+            new Vector<T>(new T[0]), // Will be set by actual embedding\n+            chunkK,\n+            metadataFilters ?? new Dictionary<string, object>()\n+        ).ToList();\n+\n+        // Group by parent document ID\n+        var parentDocuments = new Dictionary<string, (Document<T> doc, T maxScore)>();\n+        \n+        foreach (var chunk in similarChunks)\n+        {\n+            // Extract parent document ID from metadata\n+            var parentId = chunk.Metadata.ContainsKey(\"parent_id\") \n+                ? chunk.Metadata[\"parent_id\"].ToString() \n+                : chunk.Id;\n+\n+            if (parentId == null) continue;\n+\n+            var score = chunk.RelevanceScore;\n+            \n+            if (!parentDocuments.ContainsKey(parentId))\n+            {\n+                // Create parent document by combining chunks\n+                var parentDoc = CreateParentDocument(chunk, parentId);\n+                parentDocuments[parentId] = (parentDoc, score);\n+            }\n+            else\n+            {\n+                // Update if this chunk has better score\n+                var existing = parentDocuments[parentId];\n+                if (NumOps.GreaterThan(score, existing.maxScore))\n+                {\n+                    parentDocuments[parentId] = (existing.doc, score);\n+                }\n+                \n+                // Append chunk content if including neighboring chunks\n+                if (_includeNeighboringChunks)\n+                {\n+                    parentDocuments[parentId].doc.Content += \"\\n\\n\" + chunk.Content;\n+                }",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/ParentDocumentRetriever.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Consider limiting content concatenation**\n\nWhen `includeNeighboringChunks` is true, content is concatenated without bounds (line 230). If many chunks belong to the same parent, the resulting document could grow very large and exceed LLM context windows.\n\n\n\nConsider adding a safeguard:\n\n```diff\n                 // Append chunk content if including neighboring chunks\n                 if (_includeNeighboringChunks)\n                 {\n+                    // Limit total parent size to avoid exceeding context windows\n+                    var currentLength = parentDocuments[parentId].doc.Content.Length;\n+                    if (currentLength < _parentSize)\n+                    {\n                         parentDocuments[parentId].doc.Content += \"\\n\\n\" + chunk.Content;\n+                    }\n                 }\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Retrievers/ParentDocumentRetriever.cs around\nlines 228 to 231, the code unboundedly appends neighboring chunk.Content to\nparentDocuments[parentId].doc.Content when _includeNeighboringChunks is true;\nadd a safeguard to prevent oversized documents by either limiting the number of\nneighbor chunks appended or enforcing a maximum concatenated content length\n(e.g., _maxNeighborChunks or _maxConcatenatedChars). Update the logic to track\nhow many neighbors have been appended (or current char count), only append while\nunder the configured limit, and if the limit is reached, stop adding further\nchunks and optionally trim the concatenated content to the max length\n(preserving whole sentences or adding an ellipsis). Ensure the new config has a\nsensible default and unit tests cover large-chunk scenarios.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:49:56Z",
    "updated_at": "2025-11-05T02:49:59Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669853",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669853"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669853"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669853/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 228,
    "original_start_line": 228,
    "start_side": "RIGHT",
    "line": 231,
    "original_line": 231,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 231,
    "position": 231,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669858",
    "pull_request_review_id": 3419457803,
    "id": 2492669858,
    "node_id": "PRRC_kwDOKSXUF86Ukx-i",
    "diff_hunk": "@@ -0,0 +1,1221 @@\n+## Pull Request Overview\n+\n+This PR adds comprehensive generic type support to the RAG (Retrieval-Augmented Generation) system, transforming the codebase from using fixed numeric types to supporting parameterized types (e.g., `float`, `double`) throughout the entire pipeline. The changes enable flexible numeric precision while maintaining consistent mathematical operations via `INumericOperations<T>`.\n+\n+Key changes:\n+- Generic type parameters added to core models (`Document<T>`, `GroundedAnswer<T>`, `VectorDocument<T>`)\n+- All interfaces and base classes updated to support type parameters (`IRetriever<T>`, `IReranker<T>`, `IGenerator<T>`, etc.)\n+- New retrieval strategies added (BM25, TF-IDF, Dense, Hybrid, MultiQuery)\n+- New reranking implementations (MMR, Diversity, CrossEncoder)\n+- RAG configuration system introduced\n+- Multiple document store implementations added (FAISS, Pinecone, Milvus, Weaviate, PostgresVector)\n+\n+### Reviewed Changes\n+\n+Copilot reviewed 68 out of 70 changed files in this pull request and generated 53 comments.\n+\n+<details>\n+<summary>Show a summary per file</summary>\n+\n+| File | Description |\n+| ---- | ----------- |\n+| Document.cs | Added generic type parameter with `INumericOperations<T>` field and `HasRelevanceScore` flag |\n+| GroundedAnswer.cs | Updated to support generic document types |\n+| VectorDocument.cs | Updated to use `Document<T>` |\n+| RetrieverBase.cs | Added generic type parameter and `NumOps` helper |\n+| RerankerBase.cs | Added generic type parameter and updated score normalization |\n+| RagPipeline.cs | Updated all interfaces to use generic types |\n+| InMemoryDocumentStore.cs | Updated to return `Document<T>` with proper score handling |\n+| Multiple new retrievers | BM25, TF-IDF, Dense, Hybrid, MultiQuery implementations |\n+| Multiple new rerankers | MMR, Diversity, CrossEncoder, Identity implementations |\n+| Configuration classes | New RAG configuration system with builder pattern |\n+| Embedding models | OpenAI, HuggingFace, ONNX, LocalTransformer implementations |\n+| Document stores | FAISS, Pinecone, Milvus, Weaviate, PostgresVector, Hybrid implementations |\n+</details>\n+\n+\n+\n+\n+\n+\n+---\n+\n+Γëí╞Æ├å├¡ <a href=\"/ooples/AiDotNet/new/master/.github/instructions?filename=*.instructions.md\" class=\"Link--inTextBlock\" target=\"_blank\" rel=\"noopener noreferrer\">Add Copilot custom instructions</a> for smarter, more guided reviews. <a href=\"https://docs.github.com/en/copilot/customizing-copilot/adding-repository-custom-instructions-for-github-copilot\" class=\"Link--inTextBlock\" target=\"_blank\" rel=\"noopener noreferrer\">Learn how to get started</a>.\n+**Actionable comments posted: 0**\n+\n+<details>\n+<summary>Γëí╞Æ┬║Γòú Nitpick comments (2)</summary><blockquote>\n+\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Evaluation/AnswerCorrectnessMetric.cs (1)</summary><blockquote>\n+\n+`16-49`: **Drop unused LLM configuration until it╬ô├ç├ûs wired up.**\n+\n+`_llmEndpoint` and `_llmApiKey` are required ctor params yet nothing in `EvaluateCore` consumes them, so callers must hand over secrets just to satisfy the signature. Either implement the fact-checking path or remove these fields/parameters for now to avoid misleading usage.\n+\n+</blockquote></details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Evaluation/ContextRelevanceMetric.cs (1)</summary><blockquote>\n+\n+`19-45`: **Cache query tokens once per evaluation.**\n+\n+We call `GetWords(answer.Query)` inside the loop, duplicating the split/hash work for every document. Hoisting it to a local before the loop keeps behavior identical and avoids repeated allocations.  \n+\n+```diff\n+-        foreach (var doc in answer.SourceDocuments)\n+-        {\n+-            var words1 = GetWords(answer.Query);\n++        var queryWords = GetWords(answer.Query);\n++        foreach (var doc in answer.SourceDocuments)\n++        {\n++            var words1 = queryWords;\n+             var words2 = GetWords(doc.Content);\n+```\n+\n+</blockquote></details>\n+\n+</blockquote></details>\n+\n+<details>\n+<summary>Γëí╞Æ├┤┬ú Review details</summary>\n+\n+**Configuration used**: CodeRabbit UI\n+\n+**Review profile**: CHILL\n+\n+**Plan**: Pro\n+\n+<details>\n+<summary>Γëí╞Æ├┤├æ Commits</summary>\n+\n+Reviewing files that changed from the base of the PR and between 4d459a5baaeed2f7174d07d29893c445365ef894 and 7fbd5910239204f8852637c980b53b76349cc626.\n+\n+</details>\n+\n+<details>\n+<summary>Γëí╞Æ├┤├å Files selected for processing (8)</summary>\n+\n+* `src/Interfaces/IRAGMetric.cs` (3 hunks)\n+* `src/RetrievalAugmentedGeneration/Evaluation/AnswerCorrectnessMetric.cs` (1 hunks)\n+* `src/RetrievalAugmentedGeneration/Evaluation/AnswerSimilarityMetric.cs` (2 hunks)\n+* `src/RetrievalAugmentedGeneration/Evaluation/ContextCoverageMetric.cs` (2 hunks)\n+* `src/RetrievalAugmentedGeneration/Evaluation/ContextRelevanceMetric.cs` (1 hunks)\n+* `src/RetrievalAugmentedGeneration/Evaluation/FaithfulnessMetric.cs` (2 hunks)\n+* `src/RetrievalAugmentedGeneration/Evaluation/NoiseRobustnessMetric.cs` (1 hunks)\n+* `src/RetrievalAugmentedGeneration/Evaluation/RAGMetricBase.cs` (4 hunks)\n+\n+</details>\n+\n+<details>\n+<summary>Γëí╞Æ├£┬║ Files skipped from review as they are similar to previous changes (1)</summary>\n+\n+* src/RetrievalAugmentedGeneration/Evaluation/NoiseRobustnessMetric.cs\n+\n+</details>\n+\n+<details>\n+<summary>Γëí╞Æ┬║Γûæ Additional context used</summary>\n+\n+<details>\n+<summary>Γëí╞Æ┬║┬╝ Code graph analysis (7)</summary>\n+\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Evaluation/FaithfulnessMetric.cs (4)</summary><blockquote>\n+\n+<details>\n+<summary>src/Interfaces/IRAGMetric.cs (1)</summary>\n+\n+* `T` (52-52)\n+\n+</details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Evaluation/AnswerCorrectnessMetric.cs (1)</summary>\n+\n+* `T` (34-49)\n+\n+</details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Evaluation/RAGMetricBase.cs (3)</summary>\n+\n+* `T` (51-60)\n+* `T` (73-73)\n+* `RAGMetricBase` (28-103)\n+\n+</details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Models/GroundedAnswer.cs (4)</summary>\n+\n+* `GroundedAnswer` (36-183)\n+* `GroundedAnswer` (151-153)\n+* `GroundedAnswer` (160-164)\n+* `GroundedAnswer` (174-182)\n+\n+</details>\n+\n+</blockquote></details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Evaluation/RAGMetricBase.cs (4)</summary><blockquote>\n+\n+<details>\n+<summary>src/Interfaces/IRAGMetric.cs (1)</summary>\n+\n+* `T` (52-52)\n+\n+</details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Evaluation/AnswerCorrectnessMetric.cs (1)</summary>\n+\n+* `T` (34-49)\n+\n+</details>\n+<details>\n+<summary>src/Helpers/MathHelper.cs (2)</summary>\n+\n+* `INumericOperations` (33-61)\n+* `MathHelper` (16-987)\n+\n+</details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Models/GroundedAnswer.cs (4)</summary>\n+\n+* `GroundedAnswer` (36-183)\n+* `GroundedAnswer` (151-153)\n+* `GroundedAnswer` (160-164)\n+* `GroundedAnswer` (174-182)\n+\n+</details>\n+\n+</blockquote></details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Evaluation/ContextCoverageMetric.cs (4)</summary><blockquote>\n+\n+<details>\n+<summary>src/Interfaces/IRAGMetric.cs (1)</summary>\n+\n+* `T` (52-52)\n+\n+</details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Evaluation/ContextRelevanceMetric.cs (1)</summary>\n+\n+* `T` (19-46)\n+\n+</details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Evaluation/RAGMetricBase.cs (4)</summary>\n+\n+* `T` (51-60)\n+* `T` (73-73)\n+* `RAGMetricBase` (28-103)\n+* `HashSet` (93-102)\n+\n+</details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Models/GroundedAnswer.cs (4)</summary>\n+\n+* `GroundedAnswer` (36-183)\n+* `GroundedAnswer` (151-153)\n+* `GroundedAnswer` (160-164)\n+* `GroundedAnswer` (174-182)\n+\n+</details>\n+\n+</blockquote></details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Evaluation/ContextRelevanceMetric.cs (2)</summary><blockquote>\n+\n+<details>\n+<summary>src/Interfaces/IRAGMetric.cs (1)</summary>\n+\n+* `T` (52-52)\n+\n+</details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Evaluation/RAGMetricBase.cs (3)</summary>\n+\n+* `T` (51-60)\n+* `T` (73-73)\n+* `RAGMetricBase` (28-103)\n+\n+</details>\n+\n+</blockquote></details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Evaluation/AnswerSimilarityMetric.cs (4)</summary><blockquote>\n+\n+<details>\n+<summary>src/Interfaces/IRAGMetric.cs (1)</summary>\n+\n+* `T` (52-52)\n+\n+</details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Evaluation/AnswerCorrectnessMetric.cs (1)</summary>\n+\n+* `T` (34-49)\n+\n+</details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Evaluation/RAGMetricBase.cs (3)</summary>\n+\n+* `T` (51-60)\n+* `T` (73-73)\n+* `RAGMetricBase` (28-103)\n+\n+</details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Models/GroundedAnswer.cs (4)</summary>\n+\n+* `GroundedAnswer` (36-183)\n+* `GroundedAnswer` (151-153)\n+* `GroundedAnswer` (160-164)\n+* `GroundedAnswer` (174-182)\n+\n+</details>\n+\n+</blockquote></details>\n+<details>\n+<summary>src/Interfaces/IRAGMetric.cs (7)</summary><blockquote>\n+\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Evaluation/AnswerCorrectnessMetric.cs (1)</summary>\n+\n+* `T` (34-49)\n+\n+</details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Evaluation/ContextCoverageMetric.cs (3)</summary>\n+\n+* `T` (75-86)\n+* `T` (91-103)\n+* `T` (108-146)\n+\n+</details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Evaluation/ContextRelevanceMetric.cs (1)</summary>\n+\n+* `T` (19-46)\n+\n+</details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Evaluation/FaithfulnessMetric.cs (1)</summary>\n+\n+* `T` (59-74)\n+\n+</details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Evaluation/NoiseRobustnessMetric.cs (1)</summary>\n+\n+* `T` (30-33)\n+\n+</details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Evaluation/RAGMetricBase.cs (2)</summary>\n+\n+* `T` (51-60)\n+* `T` (73-73)\n+\n+</details>\n+<details>\n+<summary>src/Interfaces/IGenerator.cs (1)</summary>\n+\n+* `GroundedAnswer` (99-99)\n+\n+</details>\n+\n+</blockquote></details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Evaluation/AnswerCorrectnessMetric.cs (3)</summary><blockquote>\n+\n+<details>\n+<summary>src/Interfaces/IRAGMetric.cs (1)</summary>\n+\n+* `T` (52-52)\n+\n+</details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Evaluation/AnswerSimilarityMetric.cs (1)</summary>\n+\n+* `T` (64-76)\n+\n+</details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Evaluation/RAGMetricBase.cs (3)</summary>\n+\n+* `T` (51-60)\n+* `T` (73-73)\n+* `RAGMetricBase` (28-103)\n+\n+</details>\n+\n+</blockquote></details>\n+\n+</details>\n+\n+</details>\n+\n+<details>\n+<summary>Γëí╞Æ├╢├º Additional comments (11)</summary><blockquote>\n+\n+<details>\n+<summary>src/Interfaces/IRAGMetric.cs (1)</summary><blockquote>\n+\n+`34-53`: **Generic interface update looks solid.**\n+\n+The shift to `IRAGMetric<T>` cleanly matches the generic metric base and preserves the expected contract.\n+\n+</blockquote></details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Evaluation/AnswerSimilarityMetric.cs (1)</summary><blockquote>\n+\n+`40-76`: **Nice generic port.**\n+\n+The Jaccard computation now flows through `NumOps` without regressions╬ô├ç├╢looks good.\n+\n+</blockquote></details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Evaluation/FaithfulnessMetric.cs (1)</summary><blockquote>\n+\n+`35-74`: **Faithfulness metric migration looks good.**\n+\n+Guards and the overlap ratio now cooperate with `NumOps`, so the generic conversion is clean.\n+\n+</blockquote></details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Evaluation/RAGMetricBase.cs (4)</summary><blockquote>\n+\n+`1-2`: **LGTM! Clean generic migration.**\n+\n+The addition of required imports and the generic class signature are correct. The `NumOps` field provides a consistent way for derived metrics to perform type-safe numeric operations across all supported numeric types.\n+\n+\n+\n+\n+Also applies to: 28-33\n+\n+---\n+\n+`51-60`: **LGTM! Type-safe evaluation flow.**\n+\n+The `Evaluate` method correctly returns `T` and uses `MathHelper.Clamp` with generic bounds (`NumOps.Zero`, `NumOps.One`) to ensure scores remain in the valid [0,1] range.\n+\n+---\n+\n+`73-86`: **LGTM! Abstract method signatures correctly updated.**\n+\n+The abstract method signatures for `EvaluateCore` and `ValidateAnswer` properly accept `GroundedAnswer<T>` and return `T`, enabling derived metrics to implement type-safe evaluation logic.\n+\n+---\n+\n+`93-102`: **LGTM! Helper method preserved for word-based metrics.**\n+\n+The `GetWords` method continues to support token extraction for metrics that perform word-level overlap calculations (e.g., Jaccard similarity). This is correctly retained in the generic base class.\n+\n+</blockquote></details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Evaluation/ContextCoverageMetric.cs (4)</summary><blockquote>\n+\n+`47-47`: **LGTM! Generic class signature correct.**\n+\n+The class correctly inherits from `RAGMetricBase<T>`, enabling type-safe numeric operations for all supported numeric types.\n+\n+---\n+\n+`75-86`: **LGTM! Evaluation dispatch logic correct.**\n+\n+The method correctly returns `NumOps.Zero` for empty document sets and properly dispatches to ground-truth or heuristic evaluation paths based on availability.\n+\n+---\n+\n+`91-103`: **LGTM! Ground-truth coverage calculation is correct.**\n+\n+The word-overlap coverage ratio is correctly computed using `NumOps.Divide` with proper integer-to-T conversion via `FromDouble`. Division by zero is prevented by the early-return guard on line 94-95.\n+\n+---\n+\n+`108-146`: **LGTM! Heuristic coverage calculation is well-implemented.**\n+\n+The heuristic approach correctly:\n+- Accumulates relevance scores using `NumOps` (lines 115-122)\n+- Computes average relevance with division-by-zero protection (line 124)\n+- Calculates diversity as unique-word ratio with zero-guard (lines 138-140)\n+- Applies weighted combination (70% relevance + 30% diversity) using type-safe operations (lines 142-145)\n+\n+All numeric operations properly use `NumOps`, and edge cases are handled.\n+\n+</blockquote></details>\n+\n+</blockquote></details>\n+\n+</details>\n+\n+<!-- This is an auto-generated comment by CodeRabbit for review status -->\n+**Actionable comments posted: 0**\n+\n+<details>\n+<summary>╬ô├ûΓòùΓê⌐Γòò├à Duplicate comments (1)</summary><blockquote>\n+\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/RerankingStrategies/ReciprocalRankFusion.cs (1)</summary><blockquote>\n+\n+`111-114`: **Still need to flag fused documents as scored.**\n+\n+`FuseRankings` writes `doc.RelevanceScore` but never flips `HasRelevanceScore`, so anything consuming the fused list still treats the entries as unscored╬ô├ç├╢exactly the issue we flagged earlier. Please set the flag when you assign the score.\n+\n+```diff\n+             foreach (var doc in reranked)\n+             {\n+                 doc.RelevanceScore = scores[doc.Id];\n++                doc.HasRelevanceScore = true;\n+             }\n+```\n+\n+</blockquote></details>\n+\n+</blockquote></details>\n+\n+<details>\n+<summary>Γëí╞Æ┬║Γòú Nitpick comments (2)</summary><blockquote>\n+\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/RerankingStrategies/LLMBasedReranker.cs (1)</summary><blockquote>\n+\n+`49-82`: **Reduce repeated tokenization to cut per-doc overhead.**\n+\n+`Tokenize(query)` (and even `Tokenize(document)`) runs multiple times per document through `AssessRelevance`, `ComputeSemanticSimilarity`, and `ComputeProximityScore`. That adds a lot of allocations and O(nΓö¼Γûô) lookups under load. Cache the query tokens once in `Rerank`, pass them into the helpers, and thread the already tokenized document through the scoring pipeline to avoid the repeated work.\n+\n+</blockquote></details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/RerankingStrategies/CrossEncoderReranker.cs (1)</summary><blockquote>\n+\n+`40-73`: **Cache the query tokens before the per-document loop.**\n+\n+Each invocation of `ComputeCrossEncoderScore` re-tokenizes the query, so ranking N documents costs O(NΓö¼Γûô) allocations. Tokenize once in `Rerank`, pass the token list into `ComputeCrossEncoderScore`, and avoid the repeated splits to keep this strategy competitive with the others.\n+\n+</blockquote></details>\n+\n+</blockquote></details>\n+\n+<details>\n+<summary>Γëí╞Æ├┤┬ú Review details</summary>\n+\n+**Configuration used**: CodeRabbit UI\n+\n+**Review profile**: CHILL\n+\n+**Plan**: Pro\n+\n+<details>\n+<summary>Γëí╞Æ├┤├æ Commits</summary>\n+\n+Reviewing files that changed from the base of the PR and between 0990756bfaebc406430e06b89f59a191b1f29d3a and a13deec89ea146c47a5cb614497587d052d2a301.\n+\n+</details>\n+\n+<details>\n+<summary>Γëí╞Æ├┤├å Files selected for processing (4)</summary>\n+\n+* `src/RetrievalAugmentedGeneration/RerankingStrategies/CrossEncoderReranker.cs` (1 hunks)\n+* `src/RetrievalAugmentedGeneration/RerankingStrategies/LLMBasedReranker.cs` (1 hunks)\n+* `src/RetrievalAugmentedGeneration/RerankingStrategies/ReciprocalRankFusion.cs` (1 hunks)\n+* `src/RetrievalAugmentedGeneration/RerankingStrategies/RerankingStrategyBase.cs` (1 hunks)\n+\n+</details>\n+\n+<details>\n+<summary>Γëí╞Æ┬║Γûæ Additional context used</summary>\n+\n+<details>\n+<summary>Γëí╞Æ┬║┬╝ Code graph analysis (4)</summary>\n+\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/RerankingStrategies/CrossEncoderReranker.cs (2)</summary><blockquote>\n+\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/RerankingStrategies/RerankingStrategyBase.cs (2)</summary>\n+\n+* `RerankingStrategyBase` (13-28)\n+* `List` (27-27)\n+\n+</details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Models/Document.cs (4)</summary>\n+\n+* `Document` (27-145)\n+* `Document` (119-122)\n+* `Document` (129-133)\n+* `Document` (141-144)\n+\n+</details>\n+\n+</blockquote></details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/RerankingStrategies/RerankingStrategyBase.cs (3)</summary><blockquote>\n+\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/RerankingStrategies/CrossEncoderReranker.cs (3)</summary>\n+\n+* `T` (60-74)\n+* `List` (32-58)\n+* `List` (76-83)\n+\n+</details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/RerankingStrategies/LLMBasedReranker.cs (3)</summary>\n+\n+* `T` (69-82)\n+* `List` (35-67)\n+* `List` (118-125)\n+\n+</details>\n+<details>\n+<summary>src/Helpers/MathHelper.cs (2)</summary>\n+\n+* `INumericOperations` (33-61)\n+* `MathHelper` (16-987)\n+\n+</details>\n+\n+</blockquote></details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/RerankingStrategies/LLMBasedReranker.cs (3)</summary><blockquote>\n+\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/RerankingStrategies/CrossEncoderReranker.cs (3)</summary>\n+\n+* `T` (60-74)\n+* `List` (32-58)\n+* `List` (76-83)\n+\n+</details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/RerankingStrategies/RerankingStrategyBase.cs (2)</summary>\n+\n+* `RerankingStrategyBase` (13-28)\n+* `List` (27-27)\n+\n+</details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Models/Document.cs (4)</summary>\n+\n+* `Document` (27-145)\n+* `Document` (119-122)\n+* `Document` (129-133)\n+* `Document` (141-144)\n+\n+</details>\n+\n+</blockquote></details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/RerankingStrategies/ReciprocalRankFusion.cs (2)</summary><blockquote>\n+\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/RerankingStrategies/RerankingStrategyBase.cs (2)</summary>\n+\n+* `RerankingStrategyBase` (13-28)\n+* `List` (27-27)\n+\n+</details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Models/Document.cs (4)</summary>\n+\n+* `Document` (27-145)\n+* `Document` (119-122)\n+* `Document` (129-133)\n+* `Document` (141-144)\n+\n+</details>\n+\n+</blockquote></details>\n+\n+</details>\n+\n+</details>\n+\n+<details>\n+<summary>╬ô├àΓûæ Context from checks skipped due to timeout of 90000ms. You can increase the timeout in your CodeRabbit configuration to a maximum of 15 minutes (900000ms). (1)</summary>\n+\n+* GitHub Check: Build All Frameworks\n+\n+</details>\n+\n+</details>\n+\n+<!-- This is an auto-generated comment by CodeRabbit for review status -->\n+All abstract methods from DocumentStoreBase have been implemented in AzureSearchDocumentStore. The class now properly overrides: DocumentCount, VectorDimension, Clear(), AddCore(), GetSimilarCore(), GetByIdCore(), and RemoveCore().\n+**Actionable comments posted: 0**\n+\n+<details>\n+<summary>╬ô├ûΓòùΓê⌐Γòò├à Duplicate comments (13)</summary><blockquote>\n+\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Retrievers/TFIDFRetriever.cs (2)</summary><blockquote>\n+\n+`35-38`: **Zero-vector workaround for corpus access.**\n+\n+This retriever uses the same fragile zero-vector pattern identified in BM25Retriever to retrieve all documents. As noted in previous reviews, both keyword-based retrievers need a dedicated `GetAllDocuments()` method in the `IDocumentStore<T>` interface instead of misusing the vector similarity API.\n+\n+---\n+\n+`129-143`: **Guard against empty term lists to prevent runtime exception.**\n+\n+When a document's content produces no tokens (e.g., empty content or only punctuation), `termCounts.Values.Max()` at line 133 throws `InvalidOperationException`, breaking retrieval. This issue was previously flagged but remains unresolved.\n+\n+\n+Apply this diff to handle empty term counts:\n+\n+```diff\n+             foreach (var doc in documents)\n+             {\n+                 var termTfidf = new Dictionary<string, T>();\n+                 var termCounts = docTermFreq[doc.Id];\n++                if (termCounts.Count == 0)\n++                {\n++                    _tfidf[doc.Id] = termTfidf;\n++                    continue;\n++                }\n+                 var maxFreq = termCounts.Values.Max();\n+ \n+                 foreach (var termCount in termCounts)\n+                 {\n+                     var tf = NumOps.FromDouble((double)termCount.Value / (double)maxFreq);\n+                     var tfidf = NumOps.Multiply(tf, _idf[termCount.Key]);\n+                     termTfidf[termCount.Key] = tfidf;\n+                 }\n+ \n+                 _tfidf[doc.Id] = termTfidf;\n+             }\n+```\n+\n+</blockquote></details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/EmbeddingModels/HuggingFaceEmbeddingModel.cs (2)</summary><blockquote>\n+\n+`51-65`: **Consolidate duplicated hash method to base class.**\n+\n+`GetDeterministicHash` is duplicated identically across `HuggingFaceEmbeddingModel.cs`, `OpenAIEmbeddingModel.cs`, `LocalTransformerEmbedding.cs`, and `ONNXSentenceTransformer.cs`. Move it to `EmbeddingModelBase<T>` as a protected method to eliminate duplication and simplify maintenance.\n+\n+---\n+\n+`22-35`: **Unused API parameters create misleading contract.**\n+\n+The constructor accepts `modelName` and `apiKey`, strongly implying this class calls the HuggingFace API. However, `EmbedCore` generates synthetic hash-based embeddings and never uses these fields. This will break production deployments expecting real semantic embeddings.\n+\n+\n+\n+\n+Either implement actual HuggingFace API calls or rename the class to reflect its synthetic nature (e.g., `SyntheticEmbeddingModel`) and remove the misleading parameters.\n+\n+</blockquote></details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/EmbeddingModels/OpenAIEmbeddingModel.cs (2)</summary><blockquote>\n+\n+`22-37`: **Placeholder implementation creates production risk.**\n+\n+The constructor requires an `apiKey` and accepts `modelName`, strongly suggesting OpenAI API integration. However, `EmbedCore` never calls OpenAI╬ô├ç├╢it returns synthetic cosine-based vectors. This will silently fail in production when developers expect real semantic embeddings.\n+\n+\n+\n+\n+Either implement actual OpenAI API calls using the stored credentials, or rename to `SyntheticEmbeddingModel` and remove the misleading API parameters.\n+\n+---\n+\n+`53-67`: **Extract duplicated method to base class.**\n+\n+This method is duplicated identically in `HuggingFaceEmbeddingModel.cs`, `LocalTransformerEmbedding.cs`, and `ONNXSentenceTransformer.cs`. Move `GetDeterministicHash` to `EmbeddingModelBase<T>` as a protected method.\n+\n+</blockquote></details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/EmbeddingModels/LocalTransformerEmbedding.cs (3)</summary><blockquote>\n+\n+`21-33`: **Unused modelPath creates misleading API.**\n+\n+The constructor validates and stores `modelPath`, implying this class loads a local transformer model. However, `EmbedCore` never uses `_modelPath`╬ô├ç├╢it generates synthetic hash-based embeddings. Developers will expect real model inference but receive placeholders.\n+\n+\n+\n+\n+Either implement actual local transformer loading from `_modelPath` or remove the parameter and rename to reflect synthetic behavior.\n+\n+---\n+\n+`49-63`: **Eliminate duplicated helper method.**\n+\n+`GetDeterministicHash` is duplicated across all four embedding model files. Move it to `EmbeddingModelBase<T>` as a protected method.\n+\n+---\n+\n+`35-47`: **Potential integer overflow in embedding calculation.**\n+\n+Line 42 computes `(double)hash * (i + 1) * 0.003`. For large dimensions, the multiplication `hash * (i + 1)` can overflow `int` before casting to `double`, producing incorrect embeddings.\n+\n+\n+\n+Cast to double before multiplication:\n+\n+```diff\n+ for (int i = 0; i < _dimension; i++)\n+ {\n+-    var val = NumOps.FromDouble(Math.Sin((double)hash * (i + 1) * 0.003));\n++    var val = NumOps.FromDouble(Math.Sin((double)hash * (double)(i + 1) * 0.003));\n+     values[i] = val;\n+ }\n+```\n+\n+</blockquote></details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/EmbeddingModels/ONNXSentenceTransformer.cs (3)</summary><blockquote>\n+\n+`21-33`: **Unused modelPath parameter misleads users.**\n+\n+The constructor validates and stores `modelPath`, implying ONNX model loading. However, `EmbedCore` ignores `_modelPath` and generates synthetic hash-based embeddings. This breaks the expected contract and will cause silent failures in production.\n+\n+\n+\n+\n+Either implement actual ONNX model loading from `_modelPath` or remove the parameter and document this as a synthetic placeholder.\n+\n+---\n+\n+`49-63`: **Move duplicated method to base class.**\n+\n+`GetDeterministicHash` is duplicated identically across `HuggingFaceEmbeddingModel.cs`, `OpenAIEmbeddingModel.cs`, `LocalTransformerEmbedding.cs`, and this file. Consolidate it in `EmbeddingModelBase<T>` as a protected method.\n+\n+---\n+\n+`35-47`: **Potential integer overflow in embedding calculation.**\n+\n+Line 42 computes `(double)hash * (i + 1) * 0.002`. When `dimension` is large, `hash * (i + 1)` can overflow `int` before the cast to `double`, corrupting higher-dimensional embeddings.\n+\n+\n+\n+Cast to double before multiplication:\n+\n+```diff\n+ for (int i = 0; i < _dimension; i++)\n+ {\n+-    var val = NumOps.FromDouble(Math.Cos((double)hash * (i + 1) * 0.002));\n++    var val = NumOps.FromDouble(Math.Cos((double)hash * (double)(i + 1) * 0.002));\n+     values[i] = val;\n+ }\n+```\n+\n+</blockquote></details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/RerankingStrategies/ReciprocalRankFusion.cs (1)</summary><blockquote>\n+\n+`108-111`: **Missing HasRelevanceScore flag assignment.**\n+\n+After assigning `RelevanceScore`, you must also set `doc.HasRelevanceScore = true`. This flag is used by downstream components to determine whether a document has been scored. Without it, these reranked documents may be incorrectly treated as unscored.\n+\n+\n+\n+Apply this diff to fix the issue:\n+\n+```diff\n+         foreach (var doc in reranked)\n+         {\n+             doc.RelevanceScore = scores[doc.Id];\n++            doc.HasRelevanceScore = true;\n+         }\n+```\n+\n+</blockquote></details>\n+\n+</blockquote></details>\n+\n+<details>\n+<summary>Γëí╞Æ┬║Γòú Nitpick comments (4)</summary><blockquote>\n+\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Retrievers/TFIDFRetriever.cs (2)</summary><blockquote>\n+\n+`77-85`: **Extract duplicate tokenization logic to a shared helper.**\n+\n+The `Tokenize` method is duplicated verbatim in `BM25Retriever.cs` (lines 169-177). Consider extracting this to `RetrieverBase<T>` or a shared `TextProcessingHelper` to maintain a single implementation.\n+\n+---\n+\n+`146-161`: **Extract duplicate filter matching logic to a shared helper.**\n+\n+The `MatchesFilters` method is duplicated verbatim in `BM25Retriever.cs` (lines 179-194). Consider extracting this to `RetrieverBase<T>` as a protected helper method to eliminate duplication.\n+\n+</blockquote></details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Retrievers/BM25Retriever.cs (1)</summary><blockquote>\n+\n+`152-155`: **Remove redundant `Distinct()` call.**\n+\n+Dictionary keys are already unique, so `.Distinct()` on line 152 is unnecessary and adds overhead. The document frequency increment logic is correct without it.\n+\n+\n+Apply this diff:\n+\n+```diff\n+-                foreach (var term in termCounts.Keys.Distinct())\n++                foreach (var term in termCounts.Keys)\n+                 {\n+                     _documentFrequencies[term]++;\n+                 }\n+```\n+\n+</blockquote></details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/RerankingStrategies/ReciprocalRankFusion.cs (1)</summary><blockquote>\n+\n+`56-64`: **Remove redundant dictionary checks for cleaner code.**\n+\n+Since the loop at lines 41-54 adds every `doc.Id` to the `scores` dictionary, all documents are guaranteed to have entries by line 56. The `TryGetValue` check at line 57 and the `Where` filter at line 60 are redundant and add unnecessary overhead.\n+\n+\n+\n+Apply this diff to simplify the code:\n+\n+```diff\n+         var reranked = documents\n+-            .OrderByDescending(d => scores.TryGetValue(d.Id, out var score) ? Convert.ToDouble(score) : 0.0)\n++            .OrderByDescending(d => Convert.ToDouble(scores[d.Id]))\n+             .ToList();\n+\n+-        foreach (var doc in reranked.Where(d => scores.ContainsKey(d.Id)))\n++        foreach (var doc in reranked)\n+         {\n+             doc.RelevanceScore = scores[doc.Id];\n+             doc.HasRelevanceScore = true;\n+         }\n+```\n+\n+</blockquote></details>\n+\n+</blockquote></details>\n+\n+<details>\n+<summary>Γëí╞Æ├┤┬ú Review details</summary>\n+\n+**Configuration used**: CodeRabbit UI\n+\n+**Review profile**: CHILL\n+\n+**Plan**: Pro\n+\n+<details>\n+<summary>Γëí╞Æ├┤├æ Commits</summary>\n+\n+Reviewing files that changed from the base of the PR and between 351c3dea4d2ca16823303d522d76f76de8960678 and c0be21878ba289b4ea925eacff9c9a9164ad807a.\n+\n+</details>\n+\n+<details>\n+<summary>Γëí╞Æ├┤├å Files selected for processing (7)</summary>\n+\n+* `src/RetrievalAugmentedGeneration/EmbeddingModels/HuggingFaceEmbeddingModel.cs` (1 hunks)\n+* `src/RetrievalAugmentedGeneration/EmbeddingModels/LocalTransformerEmbedding.cs` (1 hunks)\n+* `src/RetrievalAugmentedGeneration/EmbeddingModels/ONNXSentenceTransformer.cs` (1 hunks)\n+* `src/RetrievalAugmentedGeneration/EmbeddingModels/OpenAIEmbeddingModel.cs` (1 hunks)\n+* `src/RetrievalAugmentedGeneration/RerankingStrategies/ReciprocalRankFusion.cs` (1 hunks)\n+* `src/RetrievalAugmentedGeneration/Retrievers/BM25Retriever.cs` (1 hunks)\n+* `src/RetrievalAugmentedGeneration/Retrievers/TFIDFRetriever.cs` (1 hunks)\n+\n+</details>\n+\n+<details>\n+<summary>Γëí╞Æ┬║Γûæ Additional context used</summary>\n+\n+<details>\n+<summary>Γëí╞Æ┬║┬╝ Code graph analysis (7)</summary>\n+\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/EmbeddingModels/LocalTransformerEmbedding.cs (3)</summary><blockquote>\n+\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Embeddings/EmbeddingModelBase.cs (1)</summary>\n+\n+* `EmbeddingModelBase` (25-175)\n+\n+</details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/EmbeddingModels/HuggingFaceEmbeddingModel.cs (2)</summary>\n+\n+* `Vector` (37-49)\n+* `GetDeterministicHash` (51-65)\n+\n+</details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/EmbeddingModels/OpenAIEmbeddingModel.cs (2)</summary>\n+\n+* `Vector` (39-51)\n+* `GetDeterministicHash` (53-67)\n+\n+</details>\n+\n+</blockquote></details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/RerankingStrategies/ReciprocalRankFusion.cs (2)</summary><blockquote>\n+\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Rerankers/RerankerBase.cs (2)</summary>\n+\n+* `RerankerBase` (27-203)\n+* `IList` (154-202)\n+\n+</details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Models/Document.cs (4)</summary>\n+\n+* `Document` (28-158)\n+* `Document` (132-135)\n+* `Document` (142-146)\n+* `Document` (154-157)\n+\n+</details>\n+\n+</blockquote></details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/EmbeddingModels/ONNXSentenceTransformer.cs (2)</summary><blockquote>\n+\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Embeddings/EmbeddingModelBase.cs (1)</summary>\n+\n+* `EmbeddingModelBase` (25-175)\n+\n+</details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/EmbeddingModels/HuggingFaceEmbeddingModel.cs (2)</summary>\n+\n+* `Vector` (37-49)\n+* `GetDeterministicHash` (51-65)\n+\n+</details>\n+\n+</blockquote></details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/EmbeddingModels/OpenAIEmbeddingModel.cs (4)</summary><blockquote>\n+\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Embeddings/EmbeddingModelBase.cs (3)</summary>\n+\n+* `EmbeddingModelBase` (25-175)\n+* `Vector` (47-51)\n+* `Vector` (96-96)\n+\n+</details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/EmbeddingModels/HuggingFaceEmbeddingModel.cs (2)</summary>\n+\n+* `Vector` (37-49)\n+* `GetDeterministicHash` (51-65)\n+\n+</details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/EmbeddingModels/LocalTransformerEmbedding.cs (2)</summary>\n+\n+* `Vector` (35-47)\n+* `GetDeterministicHash` (49-63)\n+\n+</details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/EmbeddingModels/ONNXSentenceTransformer.cs (2)</summary>\n+\n+* `Vector` (35-47)\n+* `GetDeterministicHash` (49-63)\n+\n+</details>\n+\n+</blockquote></details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/EmbeddingModels/HuggingFaceEmbeddingModel.cs (4)</summary><blockquote>\n+\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Embeddings/EmbeddingModelBase.cs (1)</summary>\n+\n+* `EmbeddingModelBase` (25-175)\n+\n+</details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/EmbeddingModels/LocalTransformerEmbedding.cs (2)</summary>\n+\n+* `Vector` (35-47)\n+* `GetDeterministicHash` (49-63)\n+\n+</details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/EmbeddingModels/ONNXSentenceTransformer.cs (2)</summary>\n+\n+* `Vector` (35-47)\n+* `GetDeterministicHash` (49-63)\n+\n+</details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/EmbeddingModels/OpenAIEmbeddingModel.cs (2)</summary>\n+\n+* `Vector` (39-51)\n+* `GetDeterministicHash` (53-67)\n+\n+</details>\n+\n+</blockquote></details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Retrievers/BM25Retriever.cs (2)</summary><blockquote>\n+\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Retrievers/TFIDFRetriever.cs (3)</summary>\n+\n+* `IEnumerable` (30-75)\n+* `MatchesFilters` (146-161)\n+* `List` (77-85)\n+\n+</details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Models/Document.cs (4)</summary>\n+\n+* `Document` (28-158)\n+* `Document` (132-135)\n+* `Document` (142-146)\n+* `Document` (154-157)\n+\n+</details>\n+\n+</blockquote></details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Retrievers/TFIDFRetriever.cs (2)</summary><blockquote>\n+\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Retrievers/BM25Retriever.cs (5)</summary>\n+\n+* `T` (86-102)\n+* `T` (104-118)\n+* `IEnumerable` (41-84)\n+* `MatchesFilters` (179-194)\n+* `List` (169-177)\n+\n+</details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Models/Document.cs (4)</summary>\n+\n+* `Document` (28-158)\n+* `Document` (132-135)\n+* `Document` (142-146)\n+* `Document` (154-157)\n+\n+</details>\n+\n+</blockquote></details>\n+\n+</details>\n+\n+</details>\n+\n+<details>\n+<summary>Γëí╞Æ├╢├º Additional comments (10)</summary><blockquote>\n+\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Retrievers/TFIDFRetriever.cs (1)</summary><blockquote>\n+\n+`20-28`: **LGTM!**\n+\n+Constructor properly validates the document store and initializes all necessary data structures.\n+\n+</blockquote></details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/Retrievers/BM25Retriever.cs (6)</summary><blockquote>\n+\n+`25-39`: **LGTM!**\n+\n+Constructor properly validates the document store, converts BM25 parameters to the generic numeric type, and initializes all necessary data structures.\n+\n+---\n+\n+`41-84`: **Retrieval logic is correct; architectural concern noted elsewhere.**\n+\n+The scoring and filtering logic is correctly implemented. The zero-vector workaround at lines 46-49 is an architectural issue already flagged in past reviews and applies to both BM25 and TF-IDF retrievers.\n+\n+---\n+\n+`86-102`: **LGTM!**\n+\n+The BM25 term scoring formula is correctly implemented with proper length normalization and IDF weighting. The guards for missing data prevent runtime exceptions.\n+\n+---\n+\n+`104-118`: **LGTM!**\n+\n+The smoothed IDF calculation follows the BM25 standard formula with appropriate guards against edge cases.\n+\n+---\n+\n+`169-177`: **Duplicate tokenization logic.**\n+\n+This method is identical to the one in `TFIDFRetriever.cs` and should be extracted to a shared location, as noted in the TFIDFRetriever review.\n+\n+---\n+\n+`179-194`: **Duplicate filter matching logic.**\n+\n+This method is identical to the one in `TFIDFRetriever.cs` and should be extracted to a shared location, as noted in the TFIDFRetriever review.\n+\n+</blockquote></details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/EmbeddingModels/HuggingFaceEmbeddingModel.cs (1)</summary><blockquote>\n+\n+`37-49`: **Potential integer overflow in embedding calculation.**\n+\n+Line 44 computes `(double)hash * (i + 1) * 0.002`. When `dimension` is large (e.g., 10,000+), the multiplication `hash * (i + 1)` can overflow `int` before the cast to `double`, producing incorrect embeddings for higher-dimensional vectors.\n+\n+\n+\n+Apply this fix to cast before multiplication:\n+\n+```diff\n+ for (int i = 0; i < _dimension; i++)\n+ {\n+-    var val = NumOps.FromDouble(Math.Cos((double)hash * (i + 1) * 0.002));\n++    var val = NumOps.FromDouble(Math.Cos((double)hash * (double)(i + 1) * 0.002));\n+     values[i] = val;\n+ }\n+```\n+\n+\n+\n+> Likely an incorrect or invalid review comment.\n+\n+</blockquote></details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/EmbeddingModels/OpenAIEmbeddingModel.cs (1)</summary><blockquote>\n+\n+`39-51`: **Potential integer overflow in embedding calculation.**\n+\n+Line 46 computes `(double)hash * (i + 1) * 0.001`. For large dimensions, `hash * (i + 1)` can overflow `int` before the cast to `double`, corrupting embeddings in higher-dimensional vectors.\n+\n+\n+\n+Cast to double before multiplication:\n+\n+```diff\n+ for (int i = 0; i < _dimension; i++)\n+ {\n+-    var val = NumOps.FromDouble(Math.Cos((double)hash * (i + 1) * 0.001));\n++    var val = NumOps.FromDouble(Math.Cos((double)hash * (double)(i + 1) * 0.001));\n+     values[i] = val;\n+ }\n+```\n+\n+\n+\n+> Likely an incorrect or invalid review comment.\n+\n+</blockquote></details>\n+<details>\n+<summary>src/RetrievalAugmentedGeneration/RerankingStrategies/ReciprocalRankFusion.cs (1)</summary><blockquote>\n+\n+`15-29`: **LGTM!**\n+\n+The constructor properly validates the RRF constant `k` and the `ModifiesScores` property correctly indicates that this reranker modifies scores.\n+\n+</blockquote></details>\n+\n+</blockquote></details>\n+\n+</details>\n+",
    "path": "temp_reviews.txt",
    "commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n<details>\n<summary>≡ƒº⌐ Analysis chain</summary>\n\n**Remove this temporary review artifact file from the PR.**\n\nThis file is a snapshot of Copilot review output and should not be committed to the repository. Review comments belong in PR discussions or CI logs, not in the codebase. The presence of this file suggests it's either a leftover artifact or an incomplete review tracking attempt.\n\n\n\n\n**Issues to resolve:**\n\nBefore removing this file, verify that the following flagged issues have been addressed in the actual implementation:\n\n1. **Embedding model integer overflows** (lines 749ΓÇô764, 787ΓÇô802, 1161ΓÇô1176, 1186ΓÇô1200): Casting before multiplication in `LocalTransformerEmbedding.cs`, `ONNXSentenceTransformer.cs`, `HuggingFaceEmbeddingModel.cs`, and `OpenAIEmbeddingModel.cs`\n\n2. **Missing `HasRelevanceScore` flags** (lines 461ΓÇô472, 808ΓÇô823, 865ΓÇô887): `ReciprocalRankFusion.cs` and other rerankers must set `doc.HasRelevanceScore = true` when assigning scores\n\n3. **Empty term list guard in TFIDFRetriever** (lines 660ΓÇô688): Must handle documents with zero tokens before calling `.Max()`\n\n4. **Unused/misleading API parameters** (lines 700ΓÇô707, 713ΓÇô720, 732ΓÇô739, 770ΓÇô777): Embedding models accepting API keys/model paths but using synthetic hash-based embeddings\n\n5. **Duplicate `GetDeterministicHash` method** (lines 694ΓÇô696, 724ΓÇô727, 743ΓÇô745, 781ΓÇô783): Should be moved to `EmbeddingModelBase<T>`\n\n6. **Duplicate tokenization/filter logic** (lines 834ΓÇô842): `Tokenize` and `MatchesFilters` duplicated in `BM25Retriever.cs` and `TFIDFRetriever.cs`\n\n7. **Unused LLM configuration** (lines 52ΓÇô55): `AnswerCorrectnessMetric.cs` stores `_llmEndpoint` and `_llmApiKey` but never uses them\n\n8. **Query token caching** (lines 60ΓÇô73): `ContextRelevanceMetric.cs` recomputes `GetWords(answer.Query)` in a loop\n\nRun the following verification script to confirm these issues are resolved:\n\n\n\n**Action items:**\n\n1. Delete `temp_reviews.txt` from the PR after confirming issues are resolved\n2. Open tracking issues for any unresolved items if they were deferred intentionally\n3. Add proper test coverage for the issues identified (embedding overflow, score flag handling, edge cases)\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Verify critical issues have been fixed\n\necho \"=== Checking integer overflow fixes ===\"\nrg -A2 'Math\\.(Sin|Cos)\\(\\(double\\)hash' src/RetrievalAugmentedGeneration/EmbeddingModels/\n\necho \"=== Checking HasRelevanceScore flags ===\"\nrg -B2 'doc\\.RelevanceScore\\s*=' src/RetrievalAugmentedGeneration/ | grep -A2 'RelevanceScore'\n\necho \"=== Checking for GetDeterministicHash duplication ===\"\nrg 'GetDeterministicHash' src/RetrievalAugmentedGeneration/EmbeddingModels/ | wc -l\n\necho \"=== Checking TFIDFRetriever empty term guard ===\"\nrg -B3 'termCounts\\.Values\\.Max' src/RetrievalAugmentedGeneration/Retrievers/\n\necho \"=== Checking for unused LLM fields ===\"\nrg '_llmEndpoint|_llmApiKey' src/RetrievalAugmentedGeneration/Evaluation/AnswerCorrectnessMetric.cs\n```\n\nLength of output: 4826\n\n---\n\n\n\n</details>\n\nBased on the verification attempts, I now have enough information to generate a rewritten review comment. The key findings are:\n\n1. The directory structure shows the embedding models don't exist at the expected path (directory not found error)\n2. HasRelevanceScore flags are NOT being set alongside RelevanceScore assignments in multiple files\n3. The temp_reviews.txt file itself is a temporary artifact that shouldn't be committed\n4. Multiple critical issues remain unresolved from the review feedback\n\n**Remove this temporary artifact file; address critical unfixed issues in the RAG implementation.**\n\nThe file `temp_reviews.txt` is a CodeRabbit-generated review artifact and should not be committed to the repository. More importantly, multiple flagged issues remain **unresolved** and require attention before merge:\n\n**Critical issues still present:**\n\n1. **Missing `HasRelevanceScore` flag assignments** (Retrievers/Rerankers): Code assigns `doc.RelevanceScore` across multiple files (BM25Retriever, TFIDFRetriever, CrossEncoderReranker, etc.) but never sets `doc.HasRelevanceScore = true`. This breaks downstream consumers that rely on the flag to determine if a document has been scored. Must fix in all locations where `RelevanceScore` is assigned.\n\n2. **Unused LLM parameters in AnswerCorrectnessMetric** (`src/RetrievalAugmentedGeneration/Evaluation/AnswerCorrectnessMetric.cs`): Constructor requires `_llmEndpoint` and `_llmApiKey` but `EvaluateCore` never uses them. Either wire them up or remove to avoid misleading callers into providing credentials for unused functionality.\n\n3. **TFIDFRetriever empty term guard missing** (`src/RetrievalAugmentedGeneration/Retrievers/TFIDFRetriever.cs`): Calling `termCounts.Values.Max()` without checking if `termCounts` is empty will throw `InvalidOperationException`. Add guard before line with `.Max()` call.\n\n4. **Query token caching** (`src/RetrievalAugmentedGeneration/Evaluation/ContextRelevanceMetric.cs`): `GetWords(answer.Query)` is called inside the loop for every document, duplicating tokenization work. Move before loop to avoid repeated allocations.\n\n**Action items:**\n\n- Delete `temp_reviews.txt`\n- Set `HasRelevanceScore = true` in all locations where `RelevanceScore` is assigned\n- Remove unused LLM fields or implement fact-checking\n- Add empty-term guard in TFIDFRetriever\n- Hoist query tokenization before document loop in ContextRelevanceMetric\n\n<details>\n<summary>≡ƒº░ Tools</summary>\n\n<details>\n<summary>≡ƒ¬¢ LanguageTool</summary>\n\n[uncategorized] ~43-~43: The official name of this software platform is spelled with a capital ΓÇ£HΓÇ¥.\nContext: ...mentations | </details>       ---  Γëí╞Æ├å├¡ <a href=\"/ooples/AiDotNet/new/master/.github/instructions?filename=*.instructions.md...\n\n(GITHUB)\n\n---\n\n[style] ~485-~485: Consider using a synonym to be more concise.\nContext: ... and `ComputeProximityScore`. That adds a lot of allocations and O(nΓö¼Γûô) lookups under lo...\n\n(A_LOT_OF)\n\n---\n\n[style] ~814-~814: Consider using a different verb for a more formal wording.\nContext: ...ated as unscored.    Apply this diff to fix the issue:  ```diff          foreach (v...\n\n(FIX_RESOLVE)\n\n</details>\n\n</details>\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn temp_reviews.txt lines 1-1220: remove this temporary CodeRabbit-generated\nartifact from the commit and then address the unresolved critical issues called\nout ΓÇö for every location that assigns Document.RelevanceScore (e.g.,\nBM25Retriever, TFIDFRetriever, CrossEncoderReranker, ReciprocalRankFusion and\nany other retrievers/rerankers) also set doc.HasRelevanceScore = true\nimmediately after assigning the score; in\nsrc/RetrievalAugmentedGeneration/Evaluation/AnswerCorrectnessMetric.cs either\nremove the unused _llmEndpoint/_llmApiKey ctor parameters/fields or implement\nthe fact-checking path so the ctor no longer accepts unused secrets; in\nsrc/RetrievalAugmentedGeneration/Retrievers/TFIDFRetriever.cs add a guard before\ncalling termCounts.Values.Max() to handle empty termCounts (skip or set empty\nTF-IDF for that doc); and in\nsrc/RetrievalAugmentedGeneration/Evaluation/ContextRelevanceMetric.cs cache\nGetWords(answer.Query) to a local variable before the per-document loop and\nreuse it to avoid repeated tokenization.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:49:56Z",
    "updated_at": "2025-11-05T02:49:59Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669858",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669858"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492669858"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492669858/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 1,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 1220,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 1220,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492671905",
    "pull_request_review_id": 3419460740,
    "id": 2492671905,
    "node_id": "PRRC_kwDOKSXUF86Ukyeh",
    "diff_hunk": "@@ -0,0 +1,281 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Generators;\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using AiDotNet.RetrievalAugmentedGeneration.Retrievers;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Text.RegularExpressions;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.AdvancedPatterns;\n+\n+/// <summary>\n+/// Graph-based RAG (Retrieval Augmented Generation) that combines knowledge graph traversal with vector search for enhanced retrieval.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// GraphRAG enhances traditional RAG by maintaining a knowledge graph of entities and relationships alongside\n+/// vector embeddings. When a query mentions entities in the graph, GraphRAG retrieves both directly related\n+/// documents (via graph traversal) and semantically similar documents (via vector search), then boosts scores\n+/// for documents that appear in both results. This leverages structured knowledge for more accurate retrieval.\n+/// </para>\n+/// <para><b>For Beginners:</b> Think of a knowledge graph like a mind map or family tree of facts.\n+/// \n+/// Traditional RAG (vector-only):\n+/// - Question: \"What did Einstein discover?\"\n+/// - Search embeddings for documents about \"Einstein\" and \"discovery\"\n+/// - Problem: Might miss important connections or relationships\n+/// \n+/// GraphRAG (graph + vector):\n+/// - Question: \"What did Einstein discover?\"\n+/// - Step 1: Extract entities ΓåÆ \"Einstein\"\n+/// - Step 2: Check knowledge graph:\n+///   * Einstein ΓåÆ DISCOVERED ΓåÆ Theory of Relativity\n+///   * Einstein ΓåÆ WORKED_AT ΓåÆ Princeton University\n+///   * Theory of Relativity ΓåÆ INFLUENCED ΓåÆ Quantum Mechanics\n+/// - Step 3: Vector search for \"Einstein\" and \"discovery\"\n+/// - Step 4: Boost documents that mention graph-connected entities\n+/// - Result: Prioritizes documents about his actual discoveries over generic biographical info\n+/// \n+/// Real-world analogy:\n+/// - Regular search: Looking through books by reading every page\n+/// - GraphRAG: Using the index AND table of contents AND cross-references all together\n+/// </para>\n+/// <para><b>Example Usage:</b>\n+/// <code>\n+/// // Setup\n+/// var generator = new StubGenerator&lt;double&gt;(); // Or real LLM for entity extraction\n+/// var vectorRetriever = new DenseRetriever&lt;double&gt;(embeddingModel, documentStore);\n+/// \n+/// // Create GraphRAG instance\n+/// var graphRAG = new GraphRAG&lt;double&gt;(generator, vectorRetriever);\n+/// \n+/// // Build knowledge graph (can be done manually or from documents)\n+/// graphRAG.AddRelation(\"Albert Einstein\", \"DISCOVERED\", \"Theory of Relativity\");\n+/// graphRAG.AddRelation(\"Albert Einstein\", \"BORN_IN\", \"Germany\");\n+/// graphRAG.AddRelation(\"Theory of Relativity\", \"PUBLISHED\", \"1915\");\n+/// graphRAG.AddRelation(\"Theory of Relativity\", \"INFLUENCED\", \"GPS Technology\");\n+/// \n+/// // Retrieve with graph-enhanced search\n+/// var documents = graphRAG.Retrieve(\"What did Einstein discover?\", topK: 10);\n+/// \n+/// // GraphRAG will:\n+/// // 1. Extract \"Einstein\" from query\n+/// // 2. Find graph neighbors: \"Theory of Relativity\", \"Germany\", etc.\n+/// // 3. Perform vector search for the query\n+/// // 4. Boost docs mentioning \"Theory of Relativity\" (graph-connected)\n+/// // 5. Return top-10 with graph-boosted scores\n+/// </code>\n+/// </para>\n+/// <para><b>How It Works:</b>\n+/// The retrieval process:\n+/// 1. Entity Extraction - Use LLM to extract entities from the query\n+/// 2. Graph Traversal - Find all entities connected to query entities in the knowledge graph\n+/// 3. Vector Retrieval - Perform standard semantic search for the query\n+/// 4. Score Boosting - Multiply scores by 1.5x for documents mentioning graph-connected entities\n+/// 5. Ranking - Sort all documents by boosted scores and return top-K\n+/// \n+/// Current implementation uses:\n+/// - In-memory dictionary for knowledge graph (production should use Neo4j, GraphDB)\n+/// - Regex-based entity extraction (production should use NER models)\n+/// - Simple score boosting (production could use graph embeddings, PageRank, etc.)\n+/// </para>\n+/// <para><b>Benefits:</b>\n+/// - Structured reasoning - Leverages explicit relationships between entities\n+/// - Better precision - Prioritizes documents with known connections\n+/// - Explainable - Can trace why documents were selected via graph paths\n+/// - Handles multi-hop reasoning - Can traverse entity ΓåÆ relation ΓåÆ entity chains\n+/// - Complementary - Combines structured (graph) and unstructured (vector) knowledge\n+/// </para>\n+/// <para><b>Limitations:</b>\n+/// - Requires building/maintaining knowledge graph (initial overhead)\n+/// - Graph quality affects results (garbage in, garbage out)\n+/// - Entity extraction quality matters (missed entities = missed connections)\n+/// - Current implementation is in-memory only (not scalable for large graphs)\n+/// - Simple boosting strategy (more sophisticated approaches possible)\n+/// </para>\n+/// </remarks>\n+public class GraphRAG<T>\n+{\n+    private static readonly INumericOperations<T> NumOps = MathHelper.GetNumericOperations<T>();\n+    private readonly IGenerator<T> _generator;\n+    private readonly RetrieverBase<T> _vectorRetriever;\n+    private readonly Dictionary<string, List<(string relation, string target)>> _knowledgeGraph;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"GraphRAG{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"generator\">The LLM generator for entity extraction (use StubGenerator or real LLM).</param>\n+    /// <param name=\"vectorRetriever\">Vector retriever for unstructured data.</param>\n+    public GraphRAG(\n+        IGenerator<T> generator,\n+        RetrieverBase<T> vectorRetriever)\n+    {\n+        _generator = generator ?? throw new ArgumentNullException(nameof(generator));\n+        _vectorRetriever = vectorRetriever ?? throw new ArgumentNullException(nameof(vectorRetriever));\n+        _knowledgeGraph = new Dictionary<string, List<(string relation, string target)>>();\n+    }\n+\n+    /// <summary>\n+    /// Adds a relationship to the knowledge graph.\n+    /// </summary>\n+    /// <param name=\"entity\">The source entity.</param>\n+    /// <param name=\"relation\">The relationship type.</param>\n+    /// <param name=\"target\">The target entity.</param>\n+    public void AddRelation(string entity, string relation, string target)\n+    {\n+        if (!_knowledgeGraph.ContainsKey(entity))\n+        {\n+            _knowledgeGraph[entity] = new List<(string, string)>();\n+        }\n+        \n+        _knowledgeGraph[entity].Add((relation, target));\n+    }\n+\n+    /// <summary>\n+    /// Retrieves documents using combined knowledge graph traversal and vector similarity search.\n+    /// </summary>\n+    /// <param name=\"query\">The user's query to search for.</param>\n+    /// <param name=\"topK\">Maximum number of documents to return.</param>\n+    /// <returns>Documents ranked by graph-boosted relevance scores.</returns>\n+    /// <exception cref=\"ArgumentException\">Thrown when query is null or whitespace.</exception>\n+    /// <exception cref=\"ArgumentOutOfRangeException\">Thrown when topK is not positive.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// This method combines structured knowledge from the graph with unstructured vector search.\n+    /// Documents mentioning entities connected to the query in the knowledge graph receive\n+    /// higher relevance scores (1.5x boost).\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is the main retrieval method that uses both graph and vectors.\n+    /// \n+    /// Process:\n+    /// 1. Extract Entities: Use LLM to find entities in query (\"Einstein\", \"relativity\", etc.)\n+    /// 2. Graph Traversal: Find all entities connected to query entities in knowledge graph\n+    /// 3. Vector Search: Perform regular semantic search for the query\n+    /// 4. Score Boosting: Multiply score by 1.5x for docs that mention graph-connected entities\n+    /// 5. Ranking: Sort by boosted scores and return top-K\n+    /// \n+    /// Example:\n+    /// - Query: \"What did Einstein discover?\"\n+    /// - Extracted: [\"Einstein\"]\n+    /// - Graph neighbors: [\"Theory of Relativity\", \"E=mc┬▓\", \"Photoelectric Effect\"]\n+    /// - Vector search returns 20 documents\n+    /// - Document about \"Theory of Relativity\" gets 1.5x boost (in graph)\n+    /// - Document about \"Einstein's childhood\" stays normal (not in graph neighbors)\n+    /// - Result: Theory of Relativity doc ranks higher due to graph connection\n+    /// </para>\n+    /// </remarks>\n+    public IEnumerable<Document<T>> Retrieve(string query, int topK)\n+    {\n+        if (string.IsNullOrWhiteSpace(query))\n+            throw new ArgumentException(\"Query cannot be null or whitespace\", nameof(query));\n+\n+        if (topK <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(topK), \"topK must be positive\");\n+\n+        // Step 1: Extract entities from query using LLM\n+        var entities = ExtractEntities(query);\n+\n+        // Step 2: Traverse knowledge graph to find related entities\n+        var relatedEntities = new HashSet<string>(entities);\n+        var relationships = new List<string>();\n+\n+        foreach (var entity in entities)\n+        {\n+            if (_knowledgeGraph.ContainsKey(entity))\n+            {\n+                foreach (var (relation, target) in _knowledgeGraph[entity])\n+                {\n+                    relatedEntities.Add(target);\n+                    relationships.Add($\"{entity} {relation} {target}\");\n+                }\n+            }\n+        }\n+\n+        // Step 3: Use vector retriever for unstructured text\n+        var vectorResults = _vectorRetriever.Retrieve(query, topK).ToList();\n+\n+        // Step 4: Enrich vector results with graph information\n+        var enrichedResults = new List<Document<T>>();\n+\n+        foreach (var doc in vectorResults)\n+        {\n+            var enrichedContent = doc.Content;\n+            \n+            // Check if document mentions any of our graph entities\n+            var mentionedEntities = relatedEntities\n+                .Where(entity => doc.Content.ToLower().Contains(entity.ToLower()))\n+                .ToList();\n+\n+            if (mentionedEntities.Count > 0)\n+            {\n+                // Add graph context to the document\n+                var graphContext = $\"\\n\\nRelated knowledge: {string.Join(\"; \", relationships)}\";\n+                enrichedContent = doc.Content + graphContext;\n+\n+                // Boost relevance score for documents that match graph entities\n+                var boostFactor = 1.0 + (mentionedEntities.Count * 0.1);\n+                var originalScore = doc.HasRelevanceScore \n+                    ? Convert.ToDouble(doc.RelevanceScore) \n+                    : 0.5;\n+                var boostedScore = Math.Min(1.0, originalScore * boostFactor);\n+\n+                enrichedResults.Add(new Document<T>\n+                {\n+                    Id = doc.Id,\n+                    Content = enrichedContent,\n+                    Metadata = new Dictionary<string, object>(doc.Metadata)\n+                    {\n+                        [\"graph_entities\"] = mentionedEntities,\n+                        [\"graph_boosted\"] = true\n+                    },\n+                    RelevanceScore = NumOps.FromDouble(boostedScore),\n+                    HasRelevanceScore = true\n+                });\n+            }\n+            else\n+            {\n+                enrichedResults.Add(doc);\n+            }\n+        }\n+\n+        // Step 5: Return top-K enriched results sorted by boosted scores\n+        return enrichedResults\n+            .OrderByDescending(d => d.HasRelevanceScore ? d.RelevanceScore : default(T))\n+            .Take(topK);",
    "path": "src/RetrievalAugmentedGeneration/AdvancedPatterns/GraphRAG.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Avoid `Convert.ToDouble` / default comparer on generic scores**\n\nThis class is generic over `T`, but both the boost logic and final ordering assume `T` is implicitly convertible/comparable as `double`. That fails for supported types like `Complex<T>`, throwing at runtime and breaking ordering even for other numerics that donΓÇÖt implement `IComparable`. Use the numeric facade instead.  \n```diff\n-                var originalScore = doc.HasRelevanceScore \n-                    ? Convert.ToDouble(doc.RelevanceScore) \n+                var originalScore = doc.HasRelevanceScore \n+                    ? NumOps.ToDouble(doc.RelevanceScore) \n                     : 0.5;\n@@\n-        return enrichedResults\n-            .OrderByDescending(d => d.HasRelevanceScore ? d.RelevanceScore : default(T))\n+        return enrichedResults\n+            .OrderByDescending(d => NumOps.ToDouble(d.HasRelevanceScore ? d.RelevanceScore : NumOps.Zero))\n             .Take(topK);\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/AdvancedPatterns/GraphRAG.cs around lines\n216 to 248, the code uses Convert.ToDouble and the default generic comparer\nwhich assumes T is a double-compatible primitive; replace\nConvert.ToDouble(doc.RelevanceScore) with the numeric-facade conversion (e.g.,\nNumOps.ToDouble(doc.RelevanceScore)) when computing originalScore and ensure\nboostedScore is created via NumOps.FromDouble as before, and change the final\nordering projection to OrderByDescending(d => d.HasRelevanceScore ?\nNumOps.ToDouble(d.RelevanceScore) : 0.0) so sorting and boost math use the\nnumeric facade rather than type-specific conversions or default comparers.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:51:05Z",
    "updated_at": "2025-11-05T02:51:09Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492671905",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492671905"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492671905"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492671905/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 216,
    "original_start_line": 216,
    "start_side": "RIGHT",
    "line": 248,
    "original_line": 248,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 248,
    "position": 248,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492671911",
    "pull_request_review_id": 3419460740,
    "id": 2492671911,
    "node_id": "PRRC_kwDOKSXUF86Ukyen",
    "diff_hunk": "@@ -0,0 +1,223 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores\n+{\n+    /// <summary>\n+    /// Azure Cognitive Search-inspired document store with field-based indexing and search capabilities.\n+    /// Provides in-memory simulation of Azure Search features including field-level search and faceted filtering.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class AzureSearchDocumentStore<T> : DocumentStoreBase<T>\n+    {\n+        private readonly Dictionary<string, VectorDocument<T>> _documents;\n+        private readonly Dictionary<string, Dictionary<string, HashSet<string>>> _invertedIndex;\n+        private readonly string _serviceName;\n+        private readonly string _indexName;\n+        private int _vectorDimension;\n+\n+        public override int DocumentCount => _documents.Count;\n+        public override int VectorDimension => _vectorDimension;\n+\n+        public AzureSearchDocumentStore(string serviceName, string indexName, int initialCapacity = 1000)\n+        {\n+            if (string.IsNullOrWhiteSpace(serviceName))\n+                throw new ArgumentException(\"Service name cannot be empty\", nameof(serviceName));\n+            if (string.IsNullOrWhiteSpace(indexName))\n+                throw new ArgumentException(\"Index name cannot be empty\", nameof(indexName));\n+            if (initialCapacity <= 0)\n+                throw new ArgumentException(\"Initial capacity must be greater than zero\", nameof(initialCapacity));\n+\n+            _serviceName = serviceName;\n+            _indexName = indexName;\n+            _documents = new Dictionary<string, VectorDocument<T>>(initialCapacity);\n+            _invertedIndex = new Dictionary<string, Dictionary<string, HashSet<string>>>();\n+            _vectorDimension = 0;\n+        }\n+\n+        protected override void AddCore(VectorDocument<T> vectorDocument)\n+        {\n+            if (_documents.Count == 0)\n+            {\n+                _vectorDimension = vectorDocument.Embedding.Length;\n+            }\n+\n+            _documents[vectorDocument.Document.Id] = vectorDocument;\n+            IndexMetadata(vectorDocument.Document);\n+        }\n+\n+        protected override void AddBatchCore(IList<VectorDocument<T>> vectorDocuments)\n+        {\n+            if (vectorDocuments.Count == 0)\n+                return;\n+\n+            if (_documents.Count == 0)\n+            {\n+                _vectorDimension = vectorDocuments[0].Embedding.Length;\n+            }\n+\n+            foreach (var vectorDoc in vectorDocuments)\n+            {\n+                _documents[vectorDoc.Document.Id] = vectorDoc;\n+                IndexMetadata(vectorDoc.Document);\n+            }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/AzureSearchDocumentStore.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Validate batch embeddings against `_vectorDimension`.**\n\nDuring the first batch insert, `DocumentStoreBase` canΓÇÖt enforce the dimension check (DocumentCount is still 0), so mismatched embeddings enter the store and later break cosine similarity. Ensure each batch item matches `_vectorDimension` before indexing.\n\n\nApply this diff:\n\n```diff\n-            foreach (var vectorDoc in vectorDocuments)\n-            {\n-                _documents[vectorDoc.Document.Id] = vectorDoc;\n-                IndexMetadata(vectorDoc.Document);\n-            }\n+            foreach (var vectorDoc in vectorDocuments)\n+            {\n+                if (vectorDoc.Embedding.Length != _vectorDimension)\n+                    throw new ArgumentException(\n+                        $\"Vector dimension mismatch. Expected {_vectorDimension}, got {vectorDoc.Embedding.Length}\",\n+                        nameof(vectorDocuments));\n+\n+                _documents[vectorDoc.Document.Id] = vectorDoc;\n+                IndexMetadata(vectorDoc.Document);\n+            }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n            foreach (var vectorDoc in vectorDocuments)\n            {\n                if (vectorDoc.Embedding.Length != _vectorDimension)\n                    throw new ArgumentException(\n                        $\"Vector dimension mismatch. Expected {_vectorDimension}, got {vectorDoc.Embedding.Length}\",\n                        nameof(vectorDocuments));\n\n                _documents[vectorDoc.Document.Id] = vectorDoc;\n                IndexMetadata(vectorDoc.Document);\n            }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/AzureSearchDocumentStore.cs\naround lines 63 to 67, validate each vectorDocument's embedding length against\nthe store's _vectorDimension before adding to _documents or calling\nIndexMetadata: for each vectorDoc check that vectorDoc.Vector (or\nVector/Embedding property) is not null and its Length equals _vectorDimension,\nand if any item mismatches throw an ArgumentException (or\nArgumentOutOfRangeException) identifying the offending document(s) so the entire\nbatch is rejected; ensure no partial inserts occur by performing this validation\nup-front (or before assignment) and only proceed to assign to _documents and\ncall IndexMetadata when all embeddings pass.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:51:05Z",
    "updated_at": "2025-11-05T02:51:09Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492671911",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492671911"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492671911"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492671911/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 63,
    "original_start_line": 63,
    "start_side": "RIGHT",
    "line": 67,
    "original_line": 67,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 67,
    "position": 67,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492671920",
    "pull_request_review_id": 3419460740,
    "id": 2492671920,
    "node_id": "PRRC_kwDOKSXUF86Ukyew",
    "diff_hunk": "@@ -0,0 +1,203 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using Newtonsoft.Json.Linq;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Net.Http;\n+using System.Text;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// ChromaDB-based document store designed for simplicity and developer experience.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// ChromaDB is an open-source vector database that emphasizes ease of use while maintaining\n+/// high performance for similarity search operations.\n+/// </remarks>\n+public class ChromaDBDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly HttpClient _httpClient;\n+    private readonly string _collectionName;\n+    private int _vectorDimension;\n+    private int _documentCount;\n+    private readonly Dictionary<string, VectorDocument<T>> _cache;\n+\n+    public override int DocumentCount => _documentCount;\n+    public override int VectorDimension => _vectorDimension;\n+\n+    public ChromaDBDocumentStore(string endpoint, string collectionName, string apiKey)\n+    {\n+        if (string.IsNullOrWhiteSpace(endpoint))\n+            throw new ArgumentException(\"Endpoint cannot be empty\", nameof(endpoint));\n+        if (string.IsNullOrWhiteSpace(collectionName))\n+            throw new ArgumentException(\"Collection name cannot be empty\", nameof(collectionName));\n+\n+        _httpClient = new HttpClient { BaseAddress = new Uri(endpoint) };\n+        if (!string.IsNullOrWhiteSpace(apiKey))\n+            _httpClient.DefaultRequestHeaders.Add(\"X-Chroma-Token\", apiKey);\n+\n+        _collectionName = collectionName;\n+        _vectorDimension = 0;\n+        _documentCount = 0;\n+        _cache = new Dictionary<string, VectorDocument<T>>();\n+\n+        EnsureCollection();\n+    }\n+\n+    private void EnsureCollection()\n+    {\n+        var payload = new { name = _collectionName };\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(payload),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        _httpClient.PostAsync(\"/api/v1/collections\", content).Wait();\n+    }\n+\n+    protected override void AddCore(VectorDocument<T> vectorDocument)\n+    {\n+        if (_vectorDimension == 0)\n+            _vectorDimension = vectorDocument.Embedding.Length;\n+\n+        _cache[vectorDocument.Document.Id] = vectorDocument;\n+\n+        var embedding = vectorDocument.Embedding.ToArray().Select(v => Convert.ToDouble(v)).ToList();\n+        \n+        var payload = new\n+        {\n+            ids = new[] { vectorDocument.Document.Id },\n+            embeddings = new[] { embedding },\n+            documents = new[] { vectorDocument.Document.Content },\n+            metadatas = new[] { vectorDocument.Document.Metadata }\n+        };\n+\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(payload),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        var response = _httpClient.PostAsync($\"/api/v1/collections/{_collectionName}/add\", content).Result;\n+        if (response.IsSuccessStatusCode)\n+            _documentCount++;\n+    }\n+\n+    protected override void AddBatchCore(IList<VectorDocument<T>> vectorDocuments)\n+    {\n+        if (vectorDocuments.Count == 0) return;\n+        \n+        if (_vectorDimension == 0)\n+            _vectorDimension = vectorDocuments[0].Embedding.Length;\n+\n+        foreach (var vd in vectorDocuments)\n+            _cache[vd.Document.Id] = vd;\n+\n+        var ids = vectorDocuments.Select(vd => vd.Document.Id).ToList();\n+        var embeddings = vectorDocuments.Select(vd => \n+            vd.Embedding.ToArray().Select(v => Convert.ToDouble(v)).ToList()).ToList();\n+        var documents = vectorDocuments.Select(vd => vd.Document.Content).ToList();\n+        var metadatas = vectorDocuments.Select(vd => vd.Document.Metadata).ToList();\n+",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/ChromaDBDocumentStore.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Enforce consistent embedding dimensions in batch adds.**\n\nWhen the first batch seeds the store, `DocumentStoreBase` doesnΓÇÖt validate dimensions (DocumentCount is still 0), so mixed-length embeddings slip through, leading to runtime failures during cosine similarity calls. Validate every batch item against the established `_vectorDimension` before caching or sending it.\n\n\nApply this diff:\n\n```diff\n-        foreach (var vd in vectorDocuments)\n-            _cache[vd.Document.Id] = vd;\n+        foreach (var vd in vectorDocuments)\n+        {\n+            if (vd.Embedding.Length != _vectorDimension)\n+                throw new ArgumentException(\n+                    $\"Vector dimension mismatch. Expected {_vectorDimension}, got {vd.Embedding.Length}\",\n+                    nameof(vectorDocuments));\n+\n+            _cache[vd.Document.Id] = vd;\n+        }\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/ChromaDBDocumentStore.cs\naround lines 97 to 105, the code currently accepts batch vectorDocuments without\nenforcing consistent embedding dimensions, which allows mixed-length embeddings\nto be stored and later breaks cosine similarity; before caching or projecting to\nids/embeddings/documents/metadatas, validate each vd.Embedding.Length against\nthe existing _vectorDimension (if _vectorDimension is 0, set it from the first\nvalid embedding length), and if any item differs throw an ArgumentException (or\nsimilar) describing the expected and actual dimensions so the batch is rejected\nbefore mutating _cache or sending to Chroma. Ensure validation runs prior to any\nside effects (caching or list transformations).\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:51:05Z",
    "updated_at": "2025-11-05T02:51:09Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492671920",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492671920"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492671920"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492671920/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 97,
    "original_start_line": 97,
    "start_side": "RIGHT",
    "line": 105,
    "original_line": 105,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 105,
    "position": 105,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492671924",
    "pull_request_review_id": 3419460740,
    "id": 2492671924,
    "node_id": "PRRC_kwDOKSXUF86Ukye0",
    "diff_hunk": "@@ -0,0 +1,247 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using Newtonsoft.Json.Linq;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Net.Http;\n+using System.Text;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// Elasticsearch-based document store providing hybrid search capabilities (BM25 + dense vectors).\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Elasticsearch combines traditional full-text search (BM25) with vector similarity search,\n+/// making it ideal for hybrid retrieval scenarios where both keyword matching and semantic\n+/// similarity are important.\n+/// </remarks>\n+public class ElasticsearchDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly HttpClient _httpClient;\n+    private readonly string _indexName;\n+    private int _vectorDimension;\n+    private int _documentCount;\n+    private readonly Dictionary<string, VectorDocument<T>> _cache;\n+\n+    public override int DocumentCount => _documentCount;\n+    public override int VectorDimension => _vectorDimension;\n+\n+    public ElasticsearchDocumentStore(string endpoint, string indexName, string apiKey, string username, string password)\n+    {\n+        if (string.IsNullOrWhiteSpace(endpoint))\n+            throw new ArgumentException(\"Endpoint cannot be empty\", nameof(endpoint));\n+        if (string.IsNullOrWhiteSpace(indexName))\n+            throw new ArgumentException(\"Index name cannot be empty\", nameof(indexName));\n+\n+        _httpClient = new HttpClient { BaseAddress = new Uri(endpoint) };\n+        \n+        if (!string.IsNullOrWhiteSpace(apiKey))\n+            _httpClient.DefaultRequestHeaders.Add(\"Authorization\", $\"ApiKey {apiKey}\");\n+        else if (!string.IsNullOrWhiteSpace(username) && !string.IsNullOrWhiteSpace(password))\n+        {\n+            var auth = Convert.ToBase64String(Encoding.UTF8.GetBytes($\"{username}:{password}\"));\n+            _httpClient.DefaultRequestHeaders.Add(\"Authorization\", $\"Basic {auth}\");\n+        }\n+\n+        _indexName = indexName.ToLowerInvariant();\n+        _vectorDimension = 0;\n+        _documentCount = 0;\n+        _cache = new Dictionary<string, VectorDocument<T>>();\n+\n+        EnsureIndex();\n+    }\n+\n+    private void EnsureIndex()\n+    {\n+        var checkResponse = _httpClient.GetAsync($\"/{_indexName}\").Result;\n+        if (checkResponse.IsSuccessStatusCode) \n+        {\n+            UpdateDocumentCount();\n+            return;\n+        }\n+\n+        var mapping = new\n+        {\n+            mappings = new\n+            {\n+                properties = new\n+                {\n+                    id = new { type = \"keyword\" },\n+                    content = new { type = \"text\" },\n+                    embedding = new { type = \"dense_vector\", dims = 1536 },\n+                    metadata = new { type = \"object\", enabled = true }\n+                }\n+            }\n+        };\n+\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(mapping),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        _httpClient.PutAsync($\"/{_indexName}\", content).Wait();\n+    }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/ElasticsearchDocumentStore.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Parameterize the dense vector dimension instead of hard-coding 1536**\n\nElasticsearch requires the `dense_vector` fieldΓÇÖs `dims` to match the actual embedding length. Hard-coding 1536 means any model emitting a different size will cause `_doc` writes and `_search` calls to fail with 400 errors. Please accept the dimension as a constructor parameter (validate `> 0`), store it in `_vectorDimension`, and use that value when building the index mapping so the schema always matches the embeddings you ingest.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/ElasticsearchDocumentStore.cs\naround lines 68 to 88, the mapping currently hard-codes the dense_vector dims to\n1536; change the class to accept an int vectorDimension parameter in the\nconstructor, validate it is > 0, store it in a private field (e.g.\n_vectorDimension), and replace the hard-coded 1536 with that field when building\nthe mapping so the Elasticsearch schema matches the actual embedding size;\nensure any existing constructor overloads are updated or forwarded and add input\nvalidation that throws an ArgumentException for non-positive values.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commit 8e06c50",
    "created_at": "2025-11-05T02:51:06Z",
    "updated_at": "2025-11-05T15:41:36Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492671924",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492671924"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492671924"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492671924/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 70,
    "original_start_line": 68,
    "start_side": "RIGHT",
    "line": 90,
    "original_line": 88,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 88,
    "position": 90,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492671932",
    "pull_request_review_id": 3419460740,
    "id": 2492671932,
    "node_id": "PRRC_kwDOKSXUF86Ukye8",
    "diff_hunk": "@@ -0,0 +1,207 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores\n+{\n+    /// <summary>\n+    /// Qdrant-inspired document store with collection-based organization and payload filtering.\n+    /// Provides in-memory simulation of Qdrant features including collection management and efficient filtering.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class QdrantDocumentStore<T> : DocumentStoreBase<T>\n+    {\n+        private readonly Dictionary<string, VectorDocument<T>> _documents;\n+        private readonly string _collectionName;\n+        private int _vectorDimension;\n+        private readonly Dictionary<string, HashSet<string>> _payloadIndex;\n+\n+        public override int DocumentCount => _documents.Count;\n+        public override int VectorDimension => _vectorDimension;\n+\n+        public string CollectionName { get; private set; }\n+\n+        public QdrantDocumentStore(string collectionName, int initialCapacity = 1000)\n+        {\n+            if (string.IsNullOrWhiteSpace(collectionName))\n+                throw new ArgumentException(\"Collection name cannot be empty\", nameof(collectionName));\n+            if (initialCapacity <= 0)\n+                throw new ArgumentException(\"Initial capacity must be greater than zero\", nameof(initialCapacity));\n+\n+            _collectionName = collectionName;\n+            CollectionName = collectionName;\n+            _documents = new Dictionary<string, VectorDocument<T>>(initialCapacity);\n+            _payloadIndex = new Dictionary<string, HashSet<string>>();\n+            _vectorDimension = 0;\n+        }\n+\n+        protected override void AddCore(VectorDocument<T> vectorDocument)\n+        {\n+            if (_documents.Count == 0)\n+            {\n+                _vectorDimension = vectorDocument.Embedding.Length;\n+            }\n+\n+            _documents[vectorDocument.Document.Id] = vectorDocument;\n+            IndexPayload(vectorDocument.Document);\n+        }\n+\n+        protected override void AddBatchCore(IList<VectorDocument<T>> vectorDocuments)\n+        {\n+            if (vectorDocuments.Count == 0)\n+                return;\n+\n+            if (_documents.Count == 0)\n+            {\n+                _vectorDimension = vectorDocuments[0].Embedding.Length;\n+            }\n+\n+            foreach (var vectorDoc in vectorDocuments)\n+            {\n+                _documents[vectorDoc.Document.Id] = vectorDoc;\n+                IndexPayload(vectorDoc.Document);\n+            }\n+        }\n+\n+        protected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)\n+        {\n+            var scoredDocuments = new List<(Document<T> Document, T Score)>();\n+\n+            var candidateIds = GetFilteredCandidates(metadataFilters);\n+            IEnumerable<VectorDocument<T>> candidates;\n+\n+            if (candidateIds != null)\n+            {\n+                candidates = candidateIds\n+                    .Where(id => _documents.ContainsKey(id))\n+                    .Select(id => _documents[id]);\n+            }\n+            else\n+            {\n+                candidates = _documents.Values;\n+            }\n+\n+            var matchingDocuments = candidates\n+                .Where(vectorDoc => MatchesFilters(vectorDoc.Document, metadataFilters));\n+\n+            foreach (var vectorDoc in matchingDocuments)\n+            {\n+                var similarity = StatisticsHelper<T>.CosineSimilarity(queryVector, vectorDoc.Embedding);\n+                scoredDocuments.Add((vectorDoc.Document, similarity));\n+            }\n+\n+            var results = scoredDocuments\n+                .OrderByDescending(x => x.Score)\n+                .Take(topK)\n+                .Select(x =>\n+                {\n+                    x.Document.RelevanceScore = x.Score;\n+                    x.Document.HasRelevanceScore = true;\n+                    return x.Document;\n+                })\n+                .ToList();\n+\n+            return results;\n+        }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/QdrantDocumentStore.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Document or fix range query limitation with payload index optimization.**\n\nThe payload index optimization (lines 73-85) assumes equality-based filtering via `CreatePayloadKey`, but the base class `MatchesFilters` method (line 88) supports range queries for `IComparable` types (docValue >= filterValue semantics). This creates a mismatch:\n\n- **Payload index**: Returns documents where metadata exactly matches filter values (e.g., year == 2020)\n- **MatchesFilters**: Checks if metadata is >= filter values for numeric/comparable types (e.g., year >= 2020)\n\nWhen `candidateIds` is not null, only exact-match documents are checked by `MatchesFilters`, so documents that satisfy range conditions but not equality (e.g., year = 2021 when filter is year >= 2020) are excluded from results.\n\n**Recommendation**: For in-memory stores, equality filters are typical and this optimization is valuable. Either:\n1. Document that this store only supports equality filters for metadata\n2. Detect when filters require range semantics and return `null` from `GetFilteredCandidates` to force full scan with `MatchesFilters`\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/QdrantDocumentStore.cs around\nlines 69 to 108, the payload-index optimization (lines ~73-85) only applies\nequality semantics but the base MatchesFilters (line ~88) supports range\nsemantics for IComparable values, causing range-matching documents to be\nexcluded when candidateIds is used; fix by detecting when any metadata filter\nvalue requires range semantics (i.e., is IComparable) and in that case return\nnull from GetFilteredCandidates so the code falls back to a full scan that uses\nMatchesFilters, otherwise continue using the payload-equality optimization ΓÇö\nimplement a simple check over metadataFilters values for IComparable (excluding\nstrings) and if found set candidateIds to null (or have GetFilteredCandidates\nreturn null) before building candidates.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:51:06Z",
    "updated_at": "2025-11-05T02:51:09Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492671932",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492671932"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492671932"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492671932/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 69,
    "original_start_line": 69,
    "start_side": "RIGHT",
    "line": 108,
    "original_line": 108,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 108,
    "position": 108,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492671936",
    "pull_request_review_id": 3419460740,
    "id": 2492671936,
    "node_id": "PRRC_kwDOKSXUF86UkyfA",
    "diff_hunk": "@@ -0,0 +1,91 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// Redis-based vector document store for low-latency applications.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Redis with RedisSearch module provides fast vector similarity search with sub-millisecond latency.\n+/// Ideal for real-time applications requiring instant retrieval.\n+/// </remarks>\n+public class RedisVLDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly Dictionary<string, VectorDocument<T>> _store;\n+    private int _vectorDimension;\n+\n+    public override int DocumentCount => _store.Count;\n+    public override int VectorDimension => _vectorDimension;\n+\n+    public RedisVLDocumentStore(string connectionString, string indexName, int vectorDimension)\n+    {\n+        if (string.IsNullOrWhiteSpace(connectionString))\n+            throw new ArgumentException(\"Connection string cannot be empty\", nameof(connectionString));\n+        if (string.IsNullOrWhiteSpace(indexName))\n+            throw new ArgumentException(\"Index name cannot be empty\", nameof(indexName));\n+        if (vectorDimension <= 0)\n+            throw new ArgumentException(\"Vector dimension must be positive\", nameof(vectorDimension));\n+\n+        _store = new Dictionary<string, VectorDocument<T>>();\n+        _vectorDimension = vectorDimension;\n+    }\n+\n+    protected override void AddCore(VectorDocument<T> vectorDocument)\n+    {\n+        if (_vectorDimension == 0)\n+            _vectorDimension = vectorDocument.Embedding.Length;\n+\n+        _store[vectorDocument.Document.Id] = vectorDocument;\n+    }\n+\n+    protected override void AddBatchCore(IList<VectorDocument<T>> vectorDocuments)\n+    {\n+        if (vectorDocuments.Count == 0) return;\n+\n+        if (_vectorDimension == 0)\n+            _vectorDimension = vectorDocuments[0].Embedding.Length;\n+\n+        foreach (var vd in vectorDocuments)\n+            _store[vd.Document.Id] = vd;\n+    }\n+\n+    protected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)\n+    {\n+        var results = new List<(Document<T> doc, T score)>();\n+\n+        foreach (var vd in _store.Values)\n+        {\n+            var similarity = StatisticsHelper<T>.CosineSimilarity(queryVector, vd.Embedding);\n+            vd.Document.RelevanceScore = similarity;\n+            results.Add((vd.Document, similarity));\n+        }\n+\n+        return results\n+            .OrderByDescending(x => Convert.ToDouble(x.score))\n+            .Take(topK)\n+            .Select(x => x.doc);\n+    }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/RedisVLDocumentStore.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Honor metadata filters and avoid `Convert.ToDouble` in similarity ranking**\n\n`GetSimilarCore` is expected (per `DocumentStoreBase<T>.MatchesFilters`) to drop documents that fail the callerΓÇÖs metadata filters, but the current loop ignores them. Additionally, downstream code may rely on `HasRelevanceScore`, and the use of `Convert.ToDouble` will explode for non-`IConvertible` score types. Please gate on `MatchesFilters`, set the score flag, and order via `NumOps.ToDouble`.  \n```diff\n         foreach (var vd in _store.Values)\n         {\n-            var similarity = StatisticsHelper<T>.CosineSimilarity(queryVector, vd.Embedding);\n-            vd.Document.RelevanceScore = similarity;\n-            results.Add((vd.Document, similarity));\n+            if (!MatchesFilters(vd.Document, metadataFilters))\n+                continue;\n+\n+            var similarity = StatisticsHelper<T>.CosineSimilarity(queryVector, vd.Embedding);\n+            vd.Document.RelevanceScore = similarity;\n+            vd.Document.HasRelevanceScore = true;\n+            results.Add((vd.Document, similarity));\n         }\n \n         return results\n-            .OrderByDescending(x => Convert.ToDouble(x.score))\n+            .OrderByDescending(x => NumOps.ToDouble(x.score))\n             .Take(topK)\n             .Select(x => x.doc);\n``` \nAs per `DocumentStoreBase<T>` contract.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/RedisVLDocumentStore.cs\naround lines 59 to 74, the loop currently ignores metadata filters, doesn't set\nthe document relevance flag, and uses Convert.ToDouble which fails for\nnon-IConvertible score types; update the foreach to call\nMatchesFilters(metadataFilters, vd.Document.Metadata) and skip vd when it\nreturns false, assign vd.Document.RelevanceScore = similarity and\nvd.Document.HasRelevanceScore = true (or the equivalent flag in Document<T>),\nand when ordering use NumOps.ToDouble(x.score) instead of Convert.ToDouble to\nsafely convert the generic score type before OrderByDescending and Take(topK).\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:51:06Z",
    "updated_at": "2025-11-05T02:51:10Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492671936",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492671936"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492671936"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492671936/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 172,
    "original_start_line": 59,
    "start_side": "RIGHT",
    "line": 187,
    "original_line": 74,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 74,
    "position": 187,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492671941",
    "pull_request_review_id": 3419460740,
    "id": 2492671941,
    "node_id": "PRRC_kwDOKSXUF86UkyfF",
    "diff_hunk": "@@ -0,0 +1,156 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\n+using AiDotNet.Helpers;\n+using System.Linq;\n+using System.Text;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels;\n+\n+/// <summary>\n+/// Google PaLM embedding model integration.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Provides access to Google's PaLM (Pathways Language Model) embedding capabilities\n+/// through the Google Cloud Vertex AI platform.\n+/// </remarks>\n+public class GooglePalmEmbeddingModel<T> : EmbeddingModelBase<T>\n+{\n+    private readonly string _projectId;\n+    private readonly string _location;\n+    private readonly string _model;\n+    private readonly string _apiKey;\n+    private readonly int _dimension;\n+\n+    public override int EmbeddingDimension => _dimension;\n+    public override int MaxTokens => 2048;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"GooglePalmEmbeddingModel{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"projectId\">The Google Cloud project ID.</param>\n+    /// <param name=\"location\">The Google Cloud location.</param>\n+    /// <param name=\"model\">The PaLM model name.</param>\n+    /// <param name=\"apiKey\">The API key for authentication.</param>\n+    /// <param name=\"dimension\">The embedding dimension.</param>\n+    public GooglePalmEmbeddingModel(\n+        string projectId,\n+        string location,\n+        string model,\n+        string apiKey,\n+        int dimension = 768)\n+    {\n+        _projectId = projectId ?? throw new ArgumentNullException(nameof(projectId));\n+        _location = location ?? throw new ArgumentNullException(nameof(location));\n+        _model = model ?? throw new ArgumentNullException(nameof(model));\n+        _apiKey = apiKey ?? throw new ArgumentNullException(nameof(apiKey));\n+        _dimension = dimension;\n+    }\n+\n+    /// <summary>\n+    /// Generates embeddings using Google PaLM API.\n+    /// </summary>\n+    protected override Vector<T> EmbedCore(string text)\n+    {\n+        if (string.IsNullOrWhiteSpace(text))\n+            throw new ArgumentException(\"Text cannot be null or empty\", nameof(text));\n+\n+        // For production, this would call Google PaLM API\n+        // Fallback: Generate deterministic embedding based on text features\n+        return GenerateFallbackEmbedding(text, _dimension);\n+    }\n+\n+    private Vector<T> GenerateFallbackEmbedding(string text, int dimension)\n+    {\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        var embedding = new T[dimension];\n+        \n+        // Generate deterministic features from text\n+        var hash = text.GetHashCode();\n+        var random = new Random(hash);\n+\n+        // Character-based features\n+        var charFreqs = CalculateCharacterFrequencies(text);\n+        \n+        // Word-based features\n+        var words = text.ToLower().Split(new[] { ' ', '\\t', '\\n', '\\r' }, StringSplitOptions.RemoveEmptyEntries);\n+        var wordLength = words.Length > 0 ? words.Average(w => w.Length) : 0;\n+        \n+        // Generate embedding vector\n+        for (int i = 0; i < dimension; i++)\n+        {\n+            double value;\n+            \n+            if (i < charFreqs.Length)\n+            {\n+                value = charFreqs[i];\n+            }\n+            else if (i == charFreqs.Length)\n+            {\n+                value = wordLength / 10.0; // Normalized word length\n+            }\n+            else if (i == charFreqs.Length + 1)\n+            {\n+                value = words.Length / 100.0; // Normalized word count\n+            }\n+            else\n+            {\n+                // Random component based on text hash\n+                value = random.NextDouble() * 2.0 - 1.0;\n+            }\n+            \n+            embedding[i] = numOps.FromDouble(value);\n+        }\n+\n+        // Normalize to unit length\n+        var vector = new Vector<T>(embedding);\n+        var magnitude = CalculateMagnitude(vector, numOps);\n+        \n+        if (Convert.ToDouble(magnitude) > 0)\n+        {\n+            for (int i = 0; i < dimension; i++)\n+            {\n+                embedding[i] = numOps.Divide(embedding[i], magnitude);\n+            }",
    "path": "src/RetrievalAugmentedGeneration/Embeddings/GooglePalmEmbeddingModel.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Use `NumOps` for magnitude checks to keep generics working**\n\n`Convert.ToDouble(magnitude)` will throw for supported numeric types that are not `IConvertible` (e.g., `Complex<T>`). Stick with the numeric operations facade so the fallback normalization works for every `INumericOperations<T>` implementation.  \n```diff\n-        if (Convert.ToDouble(magnitude) > 0)\n+        if (numOps.ToDouble(magnitude) > 0)\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        if (numOps.ToDouble(magnitude) > 0)\n        {\n            for (int i = 0; i < dimension; i++)\n            {\n                embedding[i] = numOps.Divide(embedding[i], magnitude);\n            }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Embeddings/GooglePalmEmbeddingModel.cs\naround lines 110ΓÇô115, replace the Convert.ToDouble(magnitude) > 0 check with the\nnumeric-operations facade so generics work for all INumericOperations<T>\nimplementations; specifically, use numOps.Compare(magnitude, numOps.Zero) > 0\n(or the equivalent numOps.GreaterThan method if available) to test magnitude > 0\nbefore normalizing, ensuring all numeric comparisons use numOps rather than\nConvert.ToDouble.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:51:06Z",
    "updated_at": "2025-11-05T02:51:10Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492671941",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492671941"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492671941"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492671941/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 110,
    "original_start_line": 110,
    "start_side": "RIGHT",
    "line": 115,
    "original_line": 115,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 115,
    "position": 115,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492671945",
    "pull_request_review_id": 3419460740,
    "id": 2492671945,
    "node_id": "PRRC_kwDOKSXUF86UkyfJ",
    "diff_hunk": "@@ -0,0 +1,67 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\n+using System;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels\n+{\n+    /// <summary>\n+    /// HuggingFace-based embedding model for generating embeddings.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class HuggingFaceEmbeddingModel<T> : EmbeddingModelBase<T>\n+    {\n+        private readonly string _modelName;\n+        private readonly string _apiKey;\n+        private readonly int _dimension;\n+        private readonly int _maxTokens;\n+\n+        public override int EmbeddingDimension => _dimension;\n+        public override int MaxTokens => _maxTokens;\n+\n+        public HuggingFaceEmbeddingModel(string modelName, string apiKey = \"\", int dimension = 768, int maxTokens = 512)\n+        {\n+            if (string.IsNullOrWhiteSpace(modelName))\n+                throw new ArgumentException(\"Model name cannot be empty\", nameof(modelName));\n+            if (dimension <= 0)\n+                throw new ArgumentException(\"Dimension must be positive\", nameof(dimension));\n+            if (maxTokens <= 0)\n+                throw new ArgumentException(\"Max tokens must be positive\", nameof(maxTokens));\n+\n+            _modelName = modelName;\n+            _apiKey = apiKey ?? string.Empty;\n+            _dimension = dimension;\n+            _maxTokens = maxTokens;\n+        }\n+\n+        protected override Vector<T> EmbedCore(string text)\n+        {\n+            var values = new T[_dimension];\n+            var hash = GetDeterministicHash(text);\n+            \n+            for (int i = 0; i < _dimension; i++)\n+            {\n+                var val = NumOps.FromDouble(Math.Cos((double)hash * (i + 1) * 0.002));\n+                values[i] = val;\n+            }\n+\n+            return new Vector<T>(values).Normalize();",
    "path": "src/RetrievalAugmentedGeneration/Embeddings/HuggingFaceEmbeddingModel.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Hook up the real HuggingFace integration (or rename the stub).**\n\nLine 39 generates synthetic cosine vectors and completely ignores `_modelName` and `_apiKey`. The public contract promises a HuggingFace-backed model, so this silently produces meaningless embeddings and will sink any downstream retrieval quality. Either call the HuggingFace Inference API / local transformer using the configured model or make the class name & constructor explicit about being a placeholder with no external call.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Embeddings/HuggingFaceEmbeddingModel.cs\naround lines 37 to 48, the current EmbedCore implementation returns synthetic\ncosine-based vectors and ignores _modelName and _apiKey; replace this stub by\ncalling the HuggingFace embeddings endpoint or local transformer using the\nconfigured _modelName and _apiKey (or, if you intentionally want a placeholder,\nrename the class/constructor to indicate it's a synthetic stub). Specifically:\nimplement an HTTP client call (or use an existing SDK) to send the text to the\nHuggingFace Inference/Embeddings API, parse the numeric embedding response into\nthe Vector<T> of length _dimension, handle API errors/timeout and authentication\nvia _apiKey, and remove the deterministic cosine logic; alternatively, rename\nthe class and constructor to include \"Stub\" or \"Synthetic\" and document that it\ndoes not call external services.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:51:06Z",
    "updated_at": "2025-11-05T02:51:11Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492671945",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492671945"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492671945"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492671945/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 37,
    "original_start_line": 37,
    "start_side": "RIGHT",
    "line": 48,
    "original_line": 48,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 48,
    "position": 48,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492671951",
    "pull_request_review_id": 3419460740,
    "id": 2492671951,
    "node_id": "PRRC_kwDOKSXUF86UkyfP",
    "diff_hunk": "@@ -0,0 +1,291 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels;\n+\n+/// <summary>\n+/// Fine-tuner for sentence transformer embedding models on domain-specific training data using triplet loss.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// Fine-tuning adapts pre-trained embedding models to perform better on specific domains or tasks by\n+/// training on custom (anchor, positive, negative) triplets. This improves embedding quality for\n+/// specialized use cases like legal documents, medical terminology, or company-specific content.\n+/// </para>\n+/// <para><b>For Beginners:</b> Think of fine-tuning like teaching a translator specialized vocabulary.\n+/// \n+/// Pre-trained model (general knowledge):\n+/// - \"bank\" ΓåÆ embedding that works for both \"river bank\" and \"financial bank\"\n+/// - Problem: Not precise for your specific domain!\n+/// \n+/// Fine-tuned model (specialized):\n+/// - If you're building a financial app, train it with examples:\n+///   - Anchor: \"bank account\"\n+///   - Positive: \"savings account\" (similar in YOUR domain)\n+///   - Negative: \"river bank\" (different in YOUR domain)\n+/// - Result: Model learns \"bank\" means \"financial institution\" in your context\n+/// \n+/// Real-world example:\n+/// Medical domain:\n+/// - General model: \"cold\" could mean temperature or illness\n+/// - After fine-tuning with medical data:\n+///   - Anchor: \"patient has cold\"\n+///   - Positive: \"patient has flu\" (similar symptoms)\n+///   - Negative: \"cold weather\" (unrelated)\n+/// - Model now correctly groups medical conditions together\n+/// </para>\n+/// <para><b>Example Usage:</b>\n+/// <code>\n+/// // Initialize fine-tuner\n+/// var fineTuner = new SentenceTransformersFineTuner&lt;double&gt;(\n+///     baseModelPath: \"models/all-MiniLM-L6-v2.onnx\",\n+///     outputModelPath: \"models/my-domain-model.onnx\",\n+///     epochs: 10,\n+///     learningRate: 2e-5,\n+///     dimension: 384\n+/// );\n+/// \n+/// // Prepare training data (anchor, positive, negative)\n+/// var trainingData = new List&lt;(string, string, string)&gt;\n+/// {\n+///     (\"fraud detection\", \"fraudulent transaction\", \"legitimate payment\"),\n+///     (\"credit card\", \"debit card\", \"business card\"),\n+///     (\"interest rate\", \"APR\", \"laptop battery\"),\n+///     // ... more examples\n+/// };\n+/// \n+/// // Fine-tune the model\n+/// fineTuner.FineTune(trainingData);\n+/// \n+/// // Use fine-tuned model\n+/// var embedding = fineTuner.Embed(\"detect fraudulent activity\");\n+/// // Now produces embeddings optimized for your financial domain!\n+/// </code>\n+/// </para>\n+/// <para><b>How It Works:</b>\n+/// Training process:\n+/// \n+/// 1. Triplet Loss Function:\n+///    - Anchor embedding (A): Embed(\"fraud detection\")\n+///    - Positive embedding (P): Embed(\"fraudulent transaction\") - should be similar\n+///    - Negative embedding (N): Embed(\"legitimate payment\") - should be different\n+///    - Loss = max(0, distance(A,P) - distance(A,N) + margin)\n+///    - Goal: Make distance(A,P) small and distance(A,N) large\n+/// \n+/// 2. Training Loop:\n+///    - For each epoch (10 iterations):\n+///      * For each training triplet:\n+///        - Generate embeddings for anchor, positive, negative\n+///        - Calculate triplet loss\n+///        - Update model weights to minimize loss\n+///        - Cache updated embeddings\n+/// \n+/// 3. Result:\n+///    - Model learns to embed domain-specific texts closer together\n+///    - Generic texts pushed further apart\n+///    - Improved retrieval accuracy for your specific use case\n+/// \n+/// Current implementation simulates training with embedding caching.\n+/// Real training requires gradient descent and backpropagation through the neural network.\n+/// </para>\n+/// <para><b>Benefits:</b>\n+/// - Domain adaptation - Customize embeddings for specific industry/task\n+/// - Improved accuracy - Better retrieval performance on your data\n+/// - Less training data - Fine-tuning needs 100-10,000 examples vs millions for pre-training\n+/// - Transfer learning - Leverages existing knowledge from pre-trained model\n+/// - Cost-effective - Faster and cheaper than training from scratch\n+/// </para>\n+/// <para><b>Limitations:</b>\n+/// - Requires quality training data (good triplets are crucial)\n+/// - Can overfit with too few examples (aim for 1,000+ triplets minimum)\n+/// - Needs domain expertise to create meaningful triplets\n+/// - Current implementation simulates training (real training requires ML framework)\n+/// - Training time increases with model size and dataset size\n+/// </para>\n+/// </remarks>\n+public class SentenceTransformersFineTuner<T> : EmbeddingModelBase<T>\n+{\n+    private readonly string _baseModelPath;\n+    private readonly string _outputModelPath;\n+    private readonly int _epochs;\n+    private readonly double _learningRate;\n+    private readonly int _dimension;\n+\n+    private ONNXSentenceTransformer<T> _baseModel;\n+    private bool _isFineTuned;\n+    private Dictionary<string, Vector<T>> _fineTunedEmbeddingsCache;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"SentenceTransformersFineTuner{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"baseModelPath\">Path to the base model to fine-tune.</param>\n+    /// <param name=\"outputModelPath\">Path where fine-tuned model will be saved.</param>\n+    /// <param name=\"epochs\">Number of training epochs.</param>\n+    /// <param name=\"learningRate\">Learning rate for fine-tuning.</param>\n+    /// <param name=\"dimension\">The embedding dimension.</param>\n+    public SentenceTransformersFineTuner(\n+        string baseModelPath,\n+        string outputModelPath,\n+        int epochs,\n+        T learningRate,\n+        int dimension)\n+    {\n+        _baseModelPath = baseModelPath ?? throw new ArgumentNullException(nameof(baseModelPath));\n+        _outputModelPath = outputModelPath ?? throw new ArgumentNullException(nameof(outputModelPath));\n+        \n+        if (epochs <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(epochs), \"Epochs must be positive\");\n+            \n+        _epochs = epochs;\n+        _learningRate = Convert.ToDouble(learningRate);\n+        _dimension = dimension;\n+        \n+        _baseModel = new ONNXSentenceTransformer<T>(_baseModelPath, _dimension, MaxTokens);\n+        _isFineTuned = false;\n+        _fineTunedEmbeddingsCache = new Dictionary<string, Vector<T>>();\n+    }\n+\n+    /// <inheritdoc />\n+    public override int EmbeddingDimension => _dimension;\n+\n+    /// <inheritdoc />\n+    public override int MaxTokens => 512;\n+\n+    /// <summary>\n+    /// Fine-tunes the model on provided training data.\n+    /// </summary>\n+    /// <param name=\"trainingPairs\">Training pairs of (anchor, positive, negative) texts.</param>\n+    public void FineTune(IEnumerable<(string anchor, string positive, string negative)> trainingPairs)\n+    {\n+        if (trainingPairs == null)\n+            throw new ArgumentNullException(nameof(trainingPairs));\n+\n+        var pairs = trainingPairs.ToList();\n+        if (pairs.Count == 0)\n+        {\n+            throw new ArgumentException(\"Training pairs cannot be empty\", nameof(trainingPairs));\n+        }\n+\n+        // Simulate fine-tuning process\n+        // In production, this would use actual neural network training with:\n+        // 1. Triplet loss or contrastive loss\n+        // 2. Backpropagation through sentence transformer layers\n+        // 3. Optimizer (Adam/SGD) from src/Optimizers/\n+        // 4. Save updated model weights to ONNX format\n+\n+        Console.WriteLine($\"Fine-tuning model on {pairs.Count} training pairs for {_epochs} epochs...\");\n+\n+        // For this implementation, we'll create an adjustment layer\n+        // that modifies base embeddings based on training data patterns\n+        var adjustmentVectors = new Dictionary<string, Vector<T>>();\n+\n+        for (int epoch = 0; epoch < _epochs; epoch++)\n+        {\n+            foreach (var (anchor, positive, negative) in pairs)\n+            {\n+                // Get base embeddings\n+                var anchorEmb = _baseModel.Embed(anchor);\n+                var positiveEmb = _baseModel.Embed(positive);\n+                var negativeEmb = _baseModel.Embed(negative);\n+\n+                // Calculate triplet loss components\n+                var posDistance = CalculateDistance(anchorEmb, positiveEmb);\n+                var negDistance = CalculateDistance(anchorEmb, negativeEmb);\n+\n+                // If positive is not closer than negative, apply adjustment\n+                if (posDistance >= negDistance)\n+                {\n+                    // Create adjustment to move anchor closer to positive\n+                    var adjustment = CreateAdjustmentVector(anchorEmb, positiveEmb, _learningRate);\n+                    \n+                    // Store adjustment (simplified - in production would update model weights)\n+                    if (!adjustmentVectors.ContainsKey(anchor))\n+                    {\n+                        adjustmentVectors[anchor] = adjustment;\n+                    }\n+                    else\n+                    {\n+                        // Average with existing adjustment\n+                        adjustmentVectors[anchor] = AverageVectors(adjustmentVectors[anchor], adjustment);\n+                    }\n+                }\n+            }\n+        }\n+\n+        // Apply adjustments to cache\n+        foreach (var kvp in adjustmentVectors)\n+        {\n+            var baseEmb = _baseModel.Embed(kvp.Key);\n+            _fineTunedEmbeddingsCache[kvp.Key] = ApplyAdjustment(baseEmb, kvp.Value);\n+        }\n+\n+        _isFineTuned = true;\n+        Console.WriteLine($\"Fine-tuning complete. Adjusted {adjustmentVectors.Count} embeddings.\");\n+    }\n+\n+    /// <inheritdoc />\n+    protected override Vector<T> EmbedCore(string text)\n+    {\n+        // Check if we have a fine-tuned version\n+        if (_isFineTuned && _fineTunedEmbeddingsCache.ContainsKey(text))\n+        {\n+            return _fineTunedEmbeddingsCache[text];\n+        }",
    "path": "src/RetrievalAugmentedGeneration/Embeddings/SentenceTransformersFineTuner.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Normalize fine-tuning cache keys so the adjustments are actually used.**\n\nLine 208 stores the adjustment under the raw anchor string, and Line 236 looks it up with whatever text the caller passes. Because the dictionary key is case- and whitespace-sensitive, even trivial variationsΓÇöe.g., training on `\"fraud detection\"` but querying `\"Fraud Detection\"` or `\"fraud detection \"`ΓÇösilently bypass the cached vector. The underlying `ONNXSentenceTransformer` already normalizes input to lower-case trimmed tokens, so this mismatch happens easily and makes the simulated fine-tuning ineffective.\n\nPlease canonicalize the key both when persisting and when retrieving, so logically equivalent strings land on the same adjusted embedding.\n\n```diff\n@@\n-        Console.WriteLine($\"Fine-tuning model on {pairs.Count} training pairs for {_epochs} epochs...\");\n+        Console.WriteLine($\"Fine-tuning model on {pairs.Count} training pairs for {_epochs} epochs...\");\n@@\n-                if (posDistance >= negDistance)\n+                if (posDistance >= negDistance)\n                 {\n+                    var anchorKey = NormalizeKey(anchor);\n                     // Create adjustment to move anchor closer to positive\n                     var adjustment = CreateAdjustmentVector(anchorEmb, positiveEmb, _learningRate);\n                     \n                     // Store adjustment (simplified - in production would update model weights)\n-                    if (!adjustmentVectors.ContainsKey(anchor))\n+                    if (!adjustmentVectors.ContainsKey(anchorKey))\n                     {\n-                        adjustmentVectors[anchor] = adjustment;\n+                        adjustmentVectors[anchorKey] = adjustment;\n                     }\n                     else\n                     {\n                         // Average with existing adjustment\n-                        adjustmentVectors[anchor] = AverageVectors(adjustmentVectors[anchor], adjustment);\n+                        adjustmentVectors[anchorKey] = AverageVectors(adjustmentVectors[anchorKey], adjustment);\n                     }\n                 }\n             }\n         }\n \n         // Apply adjustments to cache\n         foreach (var kvp in adjustmentVectors)\n         {\n             var baseEmb = _baseModel.Embed(kvp.Key);\n             _fineTunedEmbeddingsCache[kvp.Key] = ApplyAdjustment(baseEmb, kvp.Value);\n         }\n@@\n     protected override Vector<T> EmbedCore(string text)\n     {\n-        // Check if we have a fine-tuned version\n-        if (_isFineTuned && _fineTunedEmbeddingsCache.ContainsKey(text))\n-        {\n-            return _fineTunedEmbeddingsCache[text];\n-        }\n+        var key = NormalizeKey(text);\n+        // Check if we have a fine-tuned version\n+        if (_isFineTuned && _fineTunedEmbeddingsCache.TryGetValue(key, out var adjusted))\n+        {\n+            return adjusted;\n+        }\n \n         // Fall back to base model\n         return _baseModel.Embed(text);\n     }\n+\n+    private static string NormalizeKey(string text) => text.Trim().ToLowerInvariant();\n```\n\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nsrc/RetrievalAugmentedGeneration/Embeddings/SentenceTransformersFineTuner.cs\naround lines 208 to 239: the adjustment cache keys are stored and looked up\nusing the raw anchor/text, so trivial casing/whitespace differences prevent\nhits; canonicalize the key both when saving and when reading (e.g., var\ncanonical = text.Trim().ToLowerInvariant()), use that canonical key when adding\nto adjustmentVectors and when setting _fineTunedEmbeddingsCache (and when\ncalling _baseModel.Embed for the base embedding), and use the same\ncanonicalization in EmbedCore to lookup _fineTunedEmbeddingsCache so logically\nequivalent strings map to the same adjusted embedding.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:51:06Z",
    "updated_at": "2025-11-05T02:51:11Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492671951",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492671951"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492671951"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492671951/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 208,
    "original_start_line": 208,
    "start_side": "RIGHT",
    "line": 239,
    "original_line": 239,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 239,
    "position": 239,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492671953",
    "pull_request_review_id": 3419460740,
    "id": 2492671953,
    "node_id": "PRRC_kwDOKSXUF86UkyfR",
    "diff_hunk": "@@ -0,0 +1,139 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Embeddings;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.EmbeddingModels;\n+\n+/// <summary>\n+/// Voyage AI-compatible embedding model using ONNX for high-performance local inference.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// This implementation provides Voyage AI-compatible embeddings using local ONNX inference instead of\n+/// external API calls. Voyage AI models are specifically optimized for retrieval tasks and achieve\n+/// state-of-the-art performance on benchmarks like MTEB (Massive Text Embedding Benchmark).\n+/// </para>\n+/// <para><b>For Beginners:</b> What are embeddings and why Voyage AI?\n+/// \n+/// Embeddings are numerical representations of text that capture meaning:\n+/// - \"cat\" ΓåÆ [0.2, 0.8, 0.1, ...] (768 numbers)\n+/// - \"kitten\" ΓåÆ [0.21, 0.79, 0.09, ...] (very similar numbers = similar meaning)\n+/// - \"car\" ΓåÆ [-0.5, 0.1, 0.9, ...] (different numbers = different meaning)\n+/// \n+/// Why Voyage AI?\n+/// - Specialized for search/retrieval (not general-purpose)\n+/// - Better at distinguishing relevant from irrelevant documents\n+/// - Supports longer documents (up to 16,000 tokens)\n+/// - Separate optimizations for \"query\" vs \"document\" embeddings\n+/// - Industry-leading benchmark scores\n+/// \n+/// Current Implementation:\n+/// Instead of calling Voyage AI's API, this uses ONNX (Open Neural Network Exchange) to run\n+/// Voyage-compatible models locally on your machine. Benefits:\n+/// - No API costs\n+/// - No network latency\n+/// - Works offline\n+/// - Data privacy (everything stays local)\n+/// </para>\n+/// <para><b>Example Usage:</b>\n+/// <code>\n+/// // Option 1: Local ONNX inference (current implementation)\n+/// var model = new VoyageAIEmbeddingModel&lt;double&gt;(\n+///     apiKey: \"not-used\",  // Not needed for ONNX\n+///     model: \"path/to/voyage-model.onnx\",  // Download from HuggingFace\n+///     inputType: \"document\",  // or \"query\"\n+///     dimension: 1024\n+/// );\n+/// \n+/// // Embed documents\n+/// var docEmbedding = model.Embed(\"Photosynthesis converts light into chemical energy\");\n+/// \n+/// // Embed queries (would use different model with inputType=\"query\")\n+/// var queryEmbedding = model.Embed(\"How does photosynthesis work?\");\n+/// \n+/// // Calculate similarity\n+/// var similarity = Vector.CosineSimilarity(queryEmbedding, docEmbedding);\n+/// // High similarity = relevant document!\n+/// </code>\n+/// </para>\n+/// <para><b>How It Works:</b>\n+/// Internal process:\n+/// 1. Text input ΓåÆ Tokenization (break into words/subwords)\n+/// 2. Tokens ΓåÆ ONNX model (neural network processing)\n+/// 3. Model output ΓåÆ Dense vector of numbers (embedding)\n+/// 4. Return embedding for similarity calculations\n+/// \n+/// InputType matters:\n+/// - \"document\": Optimized for indexing content (broader matching)\n+/// - \"query\": Optimized for search queries (precise matching)\n+/// - Use \"document\" type when embedding your document store\n+/// - Use \"query\" type when embedding user search queries\n+/// </para>\n+/// <para><b>Benefits:</b>\n+/// - State-of-the-art retrieval performance\n+/// - Long context support (16K tokens)\n+/// - Optimized for asymmetric search (queries vs documents)\n+/// - Local inference (no API dependency)\n+/// - Privacy-preserving (data never leaves your system)\n+/// - Cost-effective (no per-request charges)\n+/// </para>\n+/// <para><b>Limitations:</b>\n+/// - Requires ONNX model file (must download from HuggingFace)\n+/// - Model file can be large (300MB-2GB depending on variant)\n+/// - First inference slower due to model loading\n+/// - CPU inference slower than GPU\n+/// - No automatic model updates (must manually download new versions)\n+/// </para>\n+/// </remarks>\n+public class VoyageAIEmbeddingModel<T> : EmbeddingModelBase<T>\n+{\n+    private readonly string _apiKey;\n+    private readonly string _model;\n+    private readonly string _inputType;\n+\n+    private readonly int _dimension;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"VoyageAIEmbeddingModel{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"apiKey\">The Voyage AI API key.</param>\n+    /// <param name=\"model\">The model name (e.g., \"voyage-02\").</param>\n+    /// <param name=\"inputType\">The input type (\"document\" or \"query\").</param>\n+    /// <param name=\"dimension\">The embedding dimension.</param>\n+    public VoyageAIEmbeddingModel(\n+        string apiKey,\n+        string model,\n+        string inputType,\n+        int dimension)\n+    {\n+        _apiKey = apiKey ?? throw new ArgumentNullException(nameof(apiKey));\n+        _model = model ?? throw new ArgumentNullException(nameof(model));\n+        _inputType = inputType ?? throw new ArgumentNullException(nameof(inputType));\n+        _dimension = dimension;\n+    }\n+\n+    /// <inheritdoc />\n+    public override int EmbeddingDimension => _dimension;\n+\n+    /// <inheritdoc />\n+    public override int MaxTokens => 16000;\n+\n+    /// <inheritdoc />\n+    protected override Vector<T> EmbedCore(string text)\n+    {\n+        // Use ONNXSentenceTransformer as backend instead of external API\n+        // This provides local inference with ONNX models\n+        var onnxTransformer = new ONNXSentenceTransformer<T>(\n+            modelPath: _model,  // Model path instead of API model name\n+            dimension: _dimension,\n+            maxTokens: MaxTokens\n+        );\n+\n+        return onnxTransformer.Embed(text);",
    "path": "src/RetrievalAugmentedGeneration/Embeddings/VoyageAIEmbeddingModel.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Cache the ONNX backend instead of rebuilding it per call.**\n\nLine 127 constructs a new `ONNXSentenceTransformer<T>` for every `EmbedCore` invocation. When the ONNX loader is implemented, that will re-load a hundreds-of-MB model for every text, crippling throughput. Instantiate it once (e.g., in the constructor) and reuse it:\n\n```diff\n-    public VoyageAIEmbeddingModel(\n-        string apiKey,\n-        string model,\n-        string inputType,\n-        int dimension)\n-    {\n-        _apiKey = apiKey ?? throw new ArgumentNullException(nameof(apiKey));\n-        _model = model ?? throw new ArgumentNullException(nameof(model));\n-        _inputType = inputType ?? throw new ArgumentNullException(nameof(inputType));\n-        _dimension = dimension;\n-    }\n+    private readonly ONNXSentenceTransformer<T> _backend;\n+\n+    public VoyageAIEmbeddingModel(\n+        string apiKey,\n+        string model,\n+        string inputType,\n+        int dimension)\n+    {\n+        _apiKey = apiKey ?? throw new ArgumentNullException(nameof(apiKey));\n+        _model = model ?? throw new ArgumentNullException(nameof(model));\n+        _inputType = inputType ?? throw new ArgumentNullException(nameof(inputType));\n+        _dimension = dimension;\n+\n+        _backend = new ONNXSentenceTransformer<T>(\n+            modelPath: _model,\n+            dimension: _dimension,\n+            maxTokens: MaxTokens);\n+    }\n...\n-        var onnxTransformer = new ONNXSentenceTransformer<T>(\n-            modelPath: _model,\n-            dimension: _dimension,\n-            maxTokens: MaxTokens\n-        );\n-\n-        return onnxTransformer.Embed(text);\n+        return _backend.Embed(text);\n```\n\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Embeddings/VoyageAIEmbeddingModel.cs around\nlines 123 to 133, the ONNXSentenceTransformer is being constructed inside\nEmbedCore causing the model to be reloaded on every call; instead, add a private\nreadonly field for ONNXSentenceTransformer<T> and instantiate it once in the\nclass constructor using the same modelPath/_model, dimension/_dimension and\nMaxTokens values, then change EmbedCore to call the cached instance's Embed\nmethod; ensure the field is initialized before any EmbedCore calls (constructor)\nand keep the original parameters used when creating the transformer.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:51:06Z",
    "updated_at": "2025-11-05T02:51:11Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492671953",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492671953"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492671953"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492671953/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 123,
    "original_start_line": 123,
    "start_side": "RIGHT",
    "line": 133,
    "original_line": 133,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 133,
    "position": 133,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492671960",
    "pull_request_review_id": 3419460740,
    "id": 2492671960,
    "node_id": "PRRC_kwDOKSXUF86UkyfY",
    "diff_hunk": "@@ -0,0 +1,300 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Graph;\n+\n+/// <summary>\n+/// In-memory knowledge graph for storing and querying entity relationships.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type used for vector operations.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// A knowledge graph stores entities (nodes) and their relationships (edges) to enable structured information retrieval.\n+/// This implementation uses efficient in-memory data structures optimized for graph traversal and querying.\n+/// </para>\n+/// <para><b>For Beginners:</b> A knowledge graph is like a map of how information connects together.\n+/// \n+/// Imagine Wikipedia as a graph:\n+/// - Each article is a node (Albert Einstein, Physics, Germany, etc.)\n+/// - Links between articles are edges (Einstein STUDIED Physics, Einstein BORN_IN Germany)\n+/// - You can traverse the graph to find related information\n+/// \n+/// This class lets you:\n+/// 1. Add entities and relationships\n+/// 2. Find connections between entities\n+/// 3. Traverse the graph to discover related information\n+/// 4. Query based on entity types or relationships\n+/// \n+/// For example, to answer \"Who worked at Princeton?\":\n+/// 1. Find all edges with type \"WORKED_AT\"\n+/// 2. Filter for target = \"Princeton University\"\n+/// 3. Return the source entities (people who worked there)\n+/// </para>\n+/// </remarks>\n+public class KnowledgeGraph<T>\n+{\n+    private readonly Dictionary<string, GraphNode<T>> _nodes;\n+    private readonly Dictionary<string, GraphEdge<T>> _edges;\n+    private readonly Dictionary<string, HashSet<string>> _outgoingEdges; // nodeId -> edge IDs going out\n+    private readonly Dictionary<string, HashSet<string>> _incomingEdges; // nodeId -> edge IDs coming in\n+    private readonly Dictionary<string, HashSet<string>> _nodesByLabel; // label -> node IDs\n+    \n+    /// <summary>\n+    /// Gets the total number of nodes in the graph.\n+    /// </summary>\n+    public int NodeCount => _nodes.Count;\n+    \n+    /// <summary>\n+    /// Gets the total number of edges in the graph.\n+    /// </summary>\n+    public int EdgeCount => _edges.Count;\n+    \n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"KnowledgeGraph{T}\"/> class.\n+    /// </summary>\n+    public KnowledgeGraph()\n+    {\n+        _nodes = new Dictionary<string, GraphNode<T>>();\n+        _edges = new Dictionary<string, GraphEdge<T>>();\n+        _outgoingEdges = new Dictionary<string, HashSet<string>>();\n+        _incomingEdges = new Dictionary<string, HashSet<string>>();\n+        _nodesByLabel = new Dictionary<string, HashSet<string>>();\n+    }\n+    \n+    /// <summary>\n+    /// Adds a node to the graph or updates it if it already exists.\n+    /// </summary>\n+    /// <param name=\"node\">The node to add.</param>\n+    public void AddNode(GraphNode<T> node)\n+    {\n+        if (node == null)\n+            throw new ArgumentNullException(nameof(node));\n+            \n+        _nodes[node.Id] = node;\n+        \n+        if (!_nodesByLabel.ContainsKey(node.Label))\n+            _nodesByLabel[node.Label] = new HashSet<string>();\n+        _nodesByLabel[node.Label].Add(node.Id);\n+        \n+        if (!_outgoingEdges.ContainsKey(node.Id))\n+            _outgoingEdges[node.Id] = new HashSet<string>();\n+        if (!_incomingEdges.ContainsKey(node.Id))\n+            _incomingEdges[node.Id] = new HashSet<string>();",
    "path": "src/RetrievalAugmentedGeneration/Graph/KnowledgeGraph.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Restore label index when overwriting an existing node.**\n\nWhen `AddNode` replaces an existing node whose label changed, the ID stays behind in the old label bucket, so `GetNodesByLabel` returns stale members. Clean up the previous label entry before inserting the updated node.\n\n\nApply this diff:\n\n```diff\n-        _nodes[node.Id] = node;\n+        if (_nodes.TryGetValue(node.Id, out var existingNode) &&\n+            !string.Equals(existingNode.Label, node.Label, StringComparison.Ordinal))\n+        {\n+            if (_nodesByLabel.TryGetValue(existingNode.Label, out var oldLabelSet))\n+            {\n+                oldLabelSet.Remove(node.Id);\n+                if (oldLabelSet.Count == 0)\n+                {\n+                    _nodesByLabel.Remove(existingNode.Label);\n+                }\n+            }\n+        }\n+\n+        _nodes[node.Id] = node;\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Graph/KnowledgeGraph.cs around lines 74 to\n83, AddNode currently overwrites an existing node but does not remove its Id\nfrom the previous label bucket; detect if a node with the same Id already\nexists, and if so compare its old Label to the new node.Label; if the label\nchanged, remove the Id from _nodesByLabel[oldLabel] and if that HashSet becomes\nempty remove the key entirely before adding the Id to the new label bucket; then\nproceed to update _nodes and ensure _outgoingEdges/_incomingEdges initialization\nremains as-is.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:51:07Z",
    "updated_at": "2025-11-05T02:51:11Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492671960",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492671960"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492671960"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492671960/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 74,
    "original_start_line": 74,
    "start_side": "RIGHT",
    "line": 83,
    "original_line": 83,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 83,
    "position": 83,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492671965",
    "pull_request_review_id": 3419460740,
    "id": 2492671965,
    "node_id": "PRRC_kwDOKSXUF86Ukyfd",
    "diff_hunk": "@@ -0,0 +1,217 @@\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Text;\n+",
    "path": "src/RetrievalAugmentedGeneration/QueryExpansion/LearnedSparseEncoderExpansion.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Critical: Missing `using System;` directive causes compilation failure.**\n\nThe code uses `ArgumentNullException` (line 78), `ArgumentOutOfRangeException` (line 81), `ArgumentException` (line 91), `Math.Min` (line 163), `Math.Ceiling` (line 208), and `StringSplitOptions` (line 109), all of which require the `System` namespace.\n\n\n\nApply this diff:\n\n```diff\n using System.Collections.Generic;\n using System.Linq;\n using System.Text;\n+using System;\n\n namespace AiDotNet.RetrievalAugmentedGeneration.QueryExpansion;\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Text;\nusing System;\n\nnamespace AiDotNet.RetrievalAugmentedGeneration.QueryExpansion;\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn\nsrc/RetrievalAugmentedGeneration/QueryExpansion/LearnedSparseEncoderExpansion.cs\naround lines 1 to 4, the file is missing the System namespace import required\nfor ArgumentNullException, ArgumentOutOfRangeException, ArgumentException, Math\nmethods, and StringSplitOptions; add \"using System;\" among the existing using\ndirectives (e.g., directly after or before the current using lines) so the\nreferenced types and static methods resolve and the file compiles.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:51:07Z",
    "updated_at": "2025-11-05T02:51:12Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492671965",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492671965"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492671965"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492671965/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 1,
    "original_start_line": 1,
    "start_side": "RIGHT",
    "line": 4,
    "original_line": 4,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 4,
    "position": 4,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492671967",
    "pull_request_review_id": 3419460740,
    "id": 2492671967,
    "node_id": "PRRC_kwDOKSXUF86Ukyff",
    "diff_hunk": "@@ -0,0 +1,217 @@\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Text;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.QueryExpansion;\n+\n+/// <summary>\n+/// Expands queries using learned sparse representations (SPLADE-like) with term importance weighting for hybrid retrieval.\n+/// </summary>\n+/// <remarks>\n+/// <para>\n+/// LearnedSparseEncoderExpansion combines the benefits of sparse (keyword-based) and dense (semantic) retrieval by\n+/// using a learned model to expand queries with semantically related terms weighted by importance. Unlike traditional\n+/// query expansion that adds synonyms uniformly, this approach uses neural networks (e.g., SPLADE) to predict term\n+/// relevance scores, generating sparse representations where only important expansion terms are included. The model\n+/// learns which terms to add and their weights through training on retrieval tasks. This implementation provides a\n+/// heuristic-based fallback using term statistics (length, capitalization, occurrence patterns) and morphological\n+/// variations, but is designed to load actual SPLADE or similar models for production use. The weighted expansion\n+/// improves both recall (finds related documents) and precision (weights focus on relevant terms).\n+/// </para>\n+/// <para><b>For Beginners:</b> Think of this like a smart thesaurus that knows which related words actually matter:\n+/// \n+/// Regular thesaurus expansion: \"fast\" ΓåÆ \"quick rapid speedy swift hasty\" (all equally)\n+/// \n+/// Learned sparse expansion: \"fast\" ΓåÆ \"quick(0.8) rapid(0.7) speed(0.6)\" (weighted by importance)\n+/// \n+/// The weights tell the search how much each term matters!\n+/// \n+/// Example query: \"neural network training\"\n+/// \n+/// Expansion with weights:\n+/// - Original: neural(1.0) network(1.0) training(1.0)\n+/// - Expanded: neural(1.0) network(1.0) training(1.0) + networks(0.7) train(0.6) learning(0.8) optimization(0.7)\n+/// \n+/// ```csharp\n+/// var expander = new LearnedSparseEncoderExpansion(\n+///     modelPath: \"models/splade.onnx\",\n+///     maxExpansionTerms: 10,\n+///     minTermWeight: 0.5              // Only include terms weighted >= 0.5\n+/// );\n+/// \n+/// var queries = expander.ExpandQuery(\"climate change mitigation\");\n+/// // Returns: [\"climate change mitigation\", \n+/// //           \"climate climate change change mitigation global warming reduction carbon\"] \n+/// // (term repetition encodes weights)\n+/// ```\n+/// \n+/// Why use LearnedSparseEncoderExpansion:\n+/// - Best of both worlds: keyword precision + semantic expansion\n+/// - Learned weights focus on truly relevant terms (not all synonyms)\n+/// - Handles domain-specific terminology better than generic expansion\n+/// - Effective for technical and scientific queries\n+/// \n+/// When NOT to use it:\n+/// - Model not available (requires trained SPLADE/similar model)\n+/// - Simple keyword matching is sufficient\n+/// - Storage-constrained systems (expanded representations use more space)\n+/// - When pure semantic search works well enough (dense retrieval)\n+/// </para>\n+/// </remarks>\n+public class LearnedSparseEncoderExpansion : QueryExpansionBase\n+{\n+    private readonly string _modelPath;\n+    private readonly int _maxExpansionTerms;\n+    private readonly double _minTermWeight;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"LearnedSparseEncoderExpansion\"/> class.\n+    /// </summary>\n+    /// <param name=\"modelPath\">Path to the SPLADE or similar model.</param>\n+    /// <param name=\"maxExpansionTerms\">Maximum number of expansion terms to add.</param>\n+    /// <param name=\"minTermWeight\">Minimum weight threshold for including a term.</param>\n+    public LearnedSparseEncoderExpansion(\n+        string modelPath,\n+        int maxExpansionTerms,\n+        double minTermWeight)\n+    {\n+        _modelPath = modelPath ?? throw new ArgumentNullException(nameof(modelPath));\n+\n+        if (maxExpansionTerms <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(maxExpansionTerms), \"Max expansion terms must be positive\");\n+\n+        _maxExpansionTerms = maxExpansionTerms;\n+        _minTermWeight = minTermWeight;\n+    }\n+\n+    /// <inheritdoc />\n+    public override List<string> ExpandQuery(string query)\n+    {\n+        if (string.IsNullOrWhiteSpace(query))\n+            throw new ArgumentException(\"Query cannot be null or whitespace\", nameof(query));\n+\n+        var expandedQueries = new List<string> { query };\n+\n+        // Simulate learned sparse expansion using TF-IDF-like term importance\n+        var expansionTerms = GenerateExpansionTerms(query);\n+        \n+        if (expansionTerms.Count > 0)\n+        {\n+            var expandedQuery = BuildExpandedQuery(query, expansionTerms);\n+            expandedQueries.Add(expandedQuery);\n+        }\n+\n+        return expandedQueries;\n+    }\n+\n+    private List<(string term, double weight)> GenerateExpansionTerms(string query)\n+    {\n+        var terms = query.ToLower().Split(new[] { ' ', ',', '.', '!', '?' }, StringSplitOptions.RemoveEmptyEntries);\n+        var termWeights = new Dictionary<string, double>();\n+\n+        // Calculate term importance using heuristics\n+        foreach (var term in terms)\n+        {\n+            if (term.Length < 3) continue;\n+\n+            var weight = CalculateTermWeight(term, query);\n+            if (weight >= _minTermWeight)\n+            {\n+                termWeights[term] = weight;\n+            }\n+        }\n+\n+        // Generate related terms using morphological variations\n+        var expandedTerms = new Dictionary<string, double>(termWeights);\n+        foreach (var kvp in termWeights)\n+        {\n+            var term = kvp.Key;\n+            var weight = kvp.Value;\n+            var variants = GenerateMorphologicalVariants(term);\n+            foreach (var variant in variants)\n+            {\n+                var variantWeight = weight * 0.7; // Related terms get lower weight\n+                if (variantWeight >= _minTermWeight && !expandedTerms.ContainsKey(variant))\n+                {\n+                    expandedTerms[variant] = variantWeight;\n+                }\n+            }\n+        }\n+\n+        return expandedTerms\n+            .OrderByDescending(kv => kv.Value)\n+            .Take(_maxExpansionTerms)\n+            .Select(kv => (kv.Key, kv.Value))\n+            .ToList();\n+    }\n+\n+    private double CalculateTermWeight(string term, string query)\n+    {\n+        double weight = 0.5; // Base weight\n+\n+        // Longer terms are more specific\n+        if (term.Length > 6) weight += 0.2;\n+        if (term.Length > 10) weight += 0.2;\n+\n+        // Terms that appear once are more distinctive\n+        var occurrences = query.ToLower().Split(' ').Count(t => t == term);\n+        if (occurrences == 1) weight += 0.3;\n+\n+        // Capitalized terms (proper nouns) are important\n+        if (char.IsUpper(term[0])) weight += 0.3;\n+\n+        return Math.Min(1.0, weight);\n+    }",
    "path": "src/RetrievalAugmentedGeneration/QueryExpansion/LearnedSparseEncoderExpansion.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Dead code: capitalization check always false.**\n\nLine 161 checks `char.IsUpper(term[0])`, but this will always be false because on line 109, the query is converted to lowercase before splitting: `query.ToLower().Split(...)`. This check is unreachable and should be removed or the logic refactored to preserve case information.\n\n\n\nApply this diff to remove the dead code:\n\n```diff\n         var occurrences = query.ToLower().Split(' ').Count(t => t == term);\n         if (occurrences == 1) weight += 0.3;\n\n-        // Capitalized terms (proper nouns) are important\n-        if (char.IsUpper(term[0])) weight += 0.3;\n-\n         return Math.Min(1.0, weight);\n```\n\nIf capitalization checking is desired, you would need to track the original case of terms before lowercasing, or pass case information separately.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn\nsrc/RetrievalAugmentedGeneration/QueryExpansion/LearnedSparseEncoderExpansion.cs\naround lines 148 to 164, the capitalization check char.IsUpper(term[0]) is dead\nbecause terms are compared after lowercasing the query; remove that unreachable\ncapitalization branch or refactor to use the original-case input: either (A)\ndelete the capitalized-term weight adjustment and its comment, or (B) accept an\nadditional parameter/or capture the original query string, split it without\nToLower to obtain the original-cased term for the capitalization test and use\nthat when computing the +0.3 weight while keeping the occurrence check on the\nlowercase tokens. Ensure no null/empty term access before checking term[0] if\nyou choose option B.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:51:07Z",
    "updated_at": "2025-11-05T02:51:12Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492671967",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492671967"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492671967"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492671967/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 148,
    "original_start_line": 148,
    "start_side": "RIGHT",
    "line": 164,
    "original_line": 164,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 164,
    "position": 164,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492671970",
    "pull_request_review_id": 3419460740,
    "id": 2492671970,
    "node_id": "PRRC_kwDOKSXUF86Ukyfi",
    "diff_hunk": "@@ -0,0 +1,216 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers;\n+\n+/// <summary>\n+/// Retrieves documents using ColBERT's token-level late interaction mechanism.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for relevance scoring (typically float or double).</typeparam>\n+/// <remarks>\n+/// <para>\n+/// ColBERT (Contextualized Late Interaction over BERT) represents queries and documents as\n+/// multiple contextualized token embeddings rather than a single vector. This enables fine-grained\n+/// matching where each query token finds its best match among document tokens (MaxSim operation).\n+/// The approach provides significantly better retrieval quality than single-vector methods while\n+/// remaining more efficient than full cross-encoder reranking.\n+/// </para>\n+/// <para>\n+/// This implementation uses a fallback approach with token overlap scoring when the full ColBERT\n+/// model is not available, providing reasonable retrieval quality through lexical matching enhanced\n+/// with semantic information.\n+/// </para>\n+/// <para><b>For Beginners:</b> Think of ColBERT like a detailed word-by-word comparison.\n+/// \n+/// Traditional retrieval (like Dense Retrieval):\n+/// - Entire query ΓåÆ Single number list [0.2, 0.5, ...]\n+/// - Entire document ΓåÆ Single number list [0.3, 0.4, ...]\n+/// - Compare: Do these lists match?\n+/// \n+/// ColBERT retrieval:\n+/// - \"climate\" ΓåÆ [0.2, 0.5, ...]\n+/// - \"change\" ΓåÆ [0.1, 0.3, ...]\n+/// - \"solutions\" ΓåÆ [0.4, 0.2, ...]\n+/// - Each query word finds its best match in the document\n+/// - More precise matching!\n+/// \n+/// For example:\n+/// ```csharp\n+/// var retriever = new ColBERTRetriever<double>(\n+///     documentStore,\n+///     modelPath: \"colbert-v2.onnx\",\n+///     maxDocLength: 512,\n+///     maxQueryLength: 32\n+/// );\n+/// var results = retriever.Retrieve(\"climate change solutions\", topK: 10);\n+/// ```\n+/// \n+/// Why use ColBERT:\n+/// - More accurate than dense retrieval (considers individual terms)\n+/// - Faster than reranking entire documents\n+/// - Better at matching specific phrases\n+/// - Handles multi-aspect queries well\n+/// \n+/// When to use it:\n+/// - You need high precision\n+/// - Queries contain multiple distinct concepts\n+/// - You have computational resources for token-level matching\n+/// </para>\n+/// </remarks>\n+public class ColBERTRetriever<T> : RetrieverBase<T>\n+{\n+    private readonly string _modelPath;\n+    private readonly int _maxDocLength;\n+    private readonly int _maxQueryLength;\n+    private readonly IDocumentStore<T> _documentStore;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the ColBERTRetriever class.\n+    /// </summary>\n+    /// <param name=\"documentStore\">The document store containing indexed documents.</param>\n+    /// <param name=\"modelPath\">Path to the ColBERT model file (ONNX format).</param>\n+    /// <param name=\"maxDocLength\">Maximum document length in tokens (typically 180-512).</param>\n+    /// <param name=\"maxQueryLength\">Maximum query length in tokens (typically 32-64).</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when documentStore or modelPath is null.</exception>\n+    /// <exception cref=\"ArgumentOutOfRangeException\">Thrown when maxDocLength or maxQueryLength is not positive.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> These parameters control how much text ColBERT processes:\n+    /// \n+    /// - maxDocLength: How many words/tokens from each document (longer = more context but slower)\n+    /// - maxQueryLength: How many words/tokens from query (shorter queries are typical)\n+    /// \n+    /// Example: maxDocLength=512 means process up to about 512 words per document.\n+    /// </para>\n+    /// </remarks>\n+    public ColBERTRetriever(\n+        IDocumentStore<T> documentStore,\n+        string modelPath,\n+        int maxDocLength,\n+        int maxQueryLength)\n+    {\n+        _documentStore = documentStore ?? throw new ArgumentNullException(nameof(documentStore));\n+        _modelPath = modelPath ?? throw new ArgumentNullException(nameof(modelPath));\n+        \n+        if (maxDocLength <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(maxDocLength), \"Max document length must be positive\");\n+            \n+        if (maxQueryLength <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(maxQueryLength), \"Max query length must be positive\");\n+            \n+        _maxDocLength = maxDocLength;\n+        _maxQueryLength = maxQueryLength;\n+    }",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/ColBERTRetriever.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**The modelPath field is never used in the implementation.**\n\nThe constructor stores `_modelPath` (line 68, assigned at line 98), but the field is never referenced in the RetrieveCore method. The documentation claims this uses ColBERT models, but the implementation doesn't load or use any model.\n\n\n\nEither:\n1. Load and use the ONNX model at the specified path for token embeddings\n2. Remove the modelPath parameter if this is a fallback implementation\n3. Add a clear TODO explaining that full ColBERT model integration is pending\n\nBased on the PR objectives mentioning \"fallback approach,\" consider adding:\n\n```diff\n public ColBERTRetriever(\n     IDocumentStore<T> documentStore,\n     string modelPath,\n     int maxDocLength,\n     int maxQueryLength)\n {\n+    // TODO: Load ColBERT ONNX model from modelPath for production token-level embeddings\n+    // Current implementation uses token overlap fallback\n     _documentStore = documentStore ?? throw new ArgumentNullException(nameof(documentStore));\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    private readonly string _modelPath;\n    private readonly int _maxDocLength;\n    private readonly int _maxQueryLength;\n    private readonly IDocumentStore<T> _documentStore;\n\n    /// <summary>\n    /// Initializes a new instance of the ColBERTRetriever class.\n    /// </summary>\n    /// <param name=\"documentStore\">The document store containing indexed documents.</param>\n    /// <param name=\"modelPath\">Path to the ColBERT model file (ONNX format).</param>\n    /// <param name=\"maxDocLength\">Maximum document length in tokens (typically 180-512).</param>\n    /// <param name=\"maxQueryLength\">Maximum query length in tokens (typically 32-64).</param>\n    /// <exception cref=\"ArgumentNullException\">Thrown when documentStore or modelPath is null.</exception>\n    /// <exception cref=\"ArgumentOutOfRangeException\">Thrown when maxDocLength or maxQueryLength is not positive.</exception>\n    /// <remarks>\n    /// <para><b>For Beginners:</b> These parameters control how much text ColBERT processes:\n    /// \n    /// - maxDocLength: How many words/tokens from each document (longer = more context but slower)\n    /// - maxQueryLength: How many words/tokens from query (shorter queries are typical)\n    /// \n    /// Example: maxDocLength=512 means process up to about 512 words per document.\n    /// </para>\n    /// </remarks>\n    public ColBERTRetriever(\n        IDocumentStore<T> documentStore,\n        string modelPath,\n        int maxDocLength,\n        int maxQueryLength)\n    {\n        // TODO: Load ColBERT ONNX model from modelPath for production token-level embeddings\n        // Current implementation uses token overlap fallback\n        _documentStore = documentStore ?? throw new ArgumentNullException(nameof(documentStore));\n        _modelPath = modelPath ?? throw new ArgumentNullException(nameof(modelPath));\n        \n        if (maxDocLength <= 0)\n            throw new ArgumentOutOfRangeException(nameof(maxDocLength), \"Max document length must be positive\");\n            \n        if (maxQueryLength <= 0)\n            throw new ArgumentOutOfRangeException(nameof(maxQueryLength), \"Max query length must be positive\");\n            \n        _maxDocLength = maxDocLength;\n        _maxQueryLength = maxQueryLength;\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Retrievers/ColBERTRetriever.cs around lines\n68ΓÇô108 the _modelPath field is assigned but never used; update the class to\neither (A) fully wire the ONNX ColBERT model: load the model using an ONNX\nruntime inference session (or your project's model loader) in the constructor,\nexpose a private method to compute token/pooled embeddings for queries and\ndocuments, and call that in RetrieveCore to replace the current fallback\nembedding logic, ensuring proper disposal and error handling, or (B) remove the\nmodelPath parameter, _modelPath field, and its XML doc entries and adjust\ncallers and tests accordingly if this retriever is intentionally a non-model\nfallback, or (C) if integration is deferred, keep the parameter but add a clear\nTODO comment in the constructor and above RetrieveCore stating \"TODO: integrate\nColBERT ONNX model at _modelPath to produce embeddings; current implementation\nis a fallback\" and update the constructor XML to reflect that model loading is\npending.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:51:07Z",
    "updated_at": "2025-11-05T02:51:12Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492671970",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492671970"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492671970"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492671970/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 68,
    "original_start_line": 68,
    "start_side": "RIGHT",
    "line": 108,
    "original_line": 108,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 108,
    "position": 108,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492671979",
    "pull_request_review_id": 3419460740,
    "id": 2492671979,
    "node_id": "PRRC_kwDOKSXUF86Ukyfr",
    "diff_hunk": "@@ -0,0 +1,216 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers;\n+\n+/// <summary>\n+/// Retrieves documents using ColBERT's token-level late interaction mechanism.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for relevance scoring (typically float or double).</typeparam>\n+/// <remarks>\n+/// <para>\n+/// ColBERT (Contextualized Late Interaction over BERT) represents queries and documents as\n+/// multiple contextualized token embeddings rather than a single vector. This enables fine-grained\n+/// matching where each query token finds its best match among document tokens (MaxSim operation).\n+/// The approach provides significantly better retrieval quality than single-vector methods while\n+/// remaining more efficient than full cross-encoder reranking.\n+/// </para>\n+/// <para>\n+/// This implementation uses a fallback approach with token overlap scoring when the full ColBERT\n+/// model is not available, providing reasonable retrieval quality through lexical matching enhanced\n+/// with semantic information.\n+/// </para>\n+/// <para><b>For Beginners:</b> Think of ColBERT like a detailed word-by-word comparison.\n+/// \n+/// Traditional retrieval (like Dense Retrieval):\n+/// - Entire query ΓåÆ Single number list [0.2, 0.5, ...]\n+/// - Entire document ΓåÆ Single number list [0.3, 0.4, ...]\n+/// - Compare: Do these lists match?\n+/// \n+/// ColBERT retrieval:\n+/// - \"climate\" ΓåÆ [0.2, 0.5, ...]\n+/// - \"change\" ΓåÆ [0.1, 0.3, ...]\n+/// - \"solutions\" ΓåÆ [0.4, 0.2, ...]\n+/// - Each query word finds its best match in the document\n+/// - More precise matching!\n+/// \n+/// For example:\n+/// ```csharp\n+/// var retriever = new ColBERTRetriever<double>(\n+///     documentStore,\n+///     modelPath: \"colbert-v2.onnx\",\n+///     maxDocLength: 512,\n+///     maxQueryLength: 32\n+/// );\n+/// var results = retriever.Retrieve(\"climate change solutions\", topK: 10);\n+/// ```\n+/// \n+/// Why use ColBERT:\n+/// - More accurate than dense retrieval (considers individual terms)\n+/// - Faster than reranking entire documents\n+/// - Better at matching specific phrases\n+/// - Handles multi-aspect queries well\n+/// \n+/// When to use it:\n+/// - You need high precision\n+/// - Queries contain multiple distinct concepts\n+/// - You have computational resources for token-level matching\n+/// </para>\n+/// </remarks>\n+public class ColBERTRetriever<T> : RetrieverBase<T>\n+{\n+    private readonly string _modelPath;\n+    private readonly int _maxDocLength;\n+    private readonly int _maxQueryLength;\n+    private readonly IDocumentStore<T> _documentStore;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the ColBERTRetriever class.\n+    /// </summary>\n+    /// <param name=\"documentStore\">The document store containing indexed documents.</param>\n+    /// <param name=\"modelPath\">Path to the ColBERT model file (ONNX format).</param>\n+    /// <param name=\"maxDocLength\">Maximum document length in tokens (typically 180-512).</param>\n+    /// <param name=\"maxQueryLength\">Maximum query length in tokens (typically 32-64).</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when documentStore or modelPath is null.</exception>\n+    /// <exception cref=\"ArgumentOutOfRangeException\">Thrown when maxDocLength or maxQueryLength is not positive.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> These parameters control how much text ColBERT processes:\n+    /// \n+    /// - maxDocLength: How many words/tokens from each document (longer = more context but slower)\n+    /// - maxQueryLength: How many words/tokens from query (shorter queries are typical)\n+    /// \n+    /// Example: maxDocLength=512 means process up to about 512 words per document.\n+    /// </para>\n+    /// </remarks>\n+    public ColBERTRetriever(\n+        IDocumentStore<T> documentStore,\n+        string modelPath,\n+        int maxDocLength,\n+        int maxQueryLength)\n+    {\n+        _documentStore = documentStore ?? throw new ArgumentNullException(nameof(documentStore));\n+        _modelPath = modelPath ?? throw new ArgumentNullException(nameof(modelPath));\n+        \n+        if (maxDocLength <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(maxDocLength), \"Max document length must be positive\");\n+            \n+        if (maxQueryLength <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(maxQueryLength), \"Max query length must be positive\");\n+            \n+        _maxDocLength = maxDocLength;\n+        _maxQueryLength = maxQueryLength;\n+    }\n+\n+    /// <summary>\n+    /// Retrieves documents using token-level late interaction matching.\n+    /// </summary>\n+    /// <param name=\"query\">The validated query text.</param>\n+    /// <param name=\"topK\">The validated number of documents to retrieve.</param>\n+    /// <param name=\"metadataFilters\">The validated metadata filters.</param>\n+    /// <returns>Documents ordered by ColBERT relevance score (highest first).</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// In a full ColBERT implementation, this method would:\n+    /// 1. Tokenize and embed each query token\n+    /// 2. For each candidate document, tokenize and embed each document token\n+    /// 3. For each query token, find maximum similarity with any document token (MaxSim)\n+    /// 4. Sum MaxSim scores across all query tokens to get document score\n+    /// 5. Rank documents by total score\n+    /// \n+    /// This fallback implementation uses token overlap scoring as an approximation,\n+    /// analyzing which query terms appear in documents and how frequently.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> What happens when you search:\n+    /// \n+    /// 1. Your query is split into words: [\"climate\", \"change\", \"solutions\"]\n+    /// 2. For each word, find documents containing it\n+    /// 3. Documents with more matching words score higher\n+    /// 4. Return top-scoring documents\n+    /// \n+    /// This fallback approach approximates ColBERT's word-level matching without\n+    /// requiring the full neural model.\n+    /// </para>\n+    /// </remarks>\n+    protected override IEnumerable<Document<T>> RetrieveCore(\n+        string query,\n+        int topK,\n+        Dictionary<string, object> metadataFilters)\n+    {\n+        if (string.IsNullOrWhiteSpace(query))\n+            throw new ArgumentException(\"Query cannot be null or whitespace\", nameof(query));\n+\n+        if (topK <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(topK), \"topK must be positive\");\n+\n+        // Tokenize query\n+        var queryTokens = TokenizeAndTruncate(query, _maxQueryLength);\n+        \n+        // For production ColBERT, this would:\n+        // 1. Generate embeddings for each query token\n+        // 2. For each document, get token embeddings\n+        // 3. Compute MaxSim between query and document tokens\n+        // 4. Sum MaxSim scores across query tokens\n+        \n+        // Fallback: Use standard dense retrieval with enhanced scoring\n+        var documents = _documentStore.GetSimilarWithFilters(\n+            new Vector<T>(new T[0]), // Placeholder for query embedding\n+            topK * 2, // Oversample\n+            metadataFilters ?? new Dictionary<string, object>()\n+        ).ToList();",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/ColBERTRetriever.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Critical flaw: Placeholder empty vector defeats retrieval purpose.**\n\nPassing an empty vector `new Vector<T>(new T[0])` to the document store will not retrieve semantically relevant documents. This defeats the entire purpose of the retriever and will return arbitrary documents instead of those matching the query.\n\n\n\nThe implementation needs one of these solutions:\n\n1. **Embed the query** using an embedding model (requires adding an embedding model dependency)\n2. **Use a full-text retriever** instead of vector similarity (change the underlying document store method)\n3. **Document this as a stub** and add a clear TODO with explanation\n\nApply this diff for solution 1:\n\n```diff\n+        // TODO: Add IEmbeddingModel<T> dependency to constructor and embed query here\n+        // var queryVector = _embeddingModel.Embed(query);\n+        \n-        // Fallback: Use standard dense retrieval with enhanced scoring\n-        var documents = _documentStore.GetSimilarWithFilters(\n-            new Vector<T>(new T[0]), // Placeholder for query embedding\n-            topK * 2, // Oversample\n-            metadataFilters ?? new Dictionary<string, object>()\n-        ).ToList();\n+        throw new NotImplementedException(\n+            \"ColBERTRetriever requires query embedding. Add IEmbeddingModel<T> to constructor \" +\n+            \"and embed the query before calling GetSimilarWithFilters.\");\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Retrievers/ColBERTRetriever.cs around lines\n161 to 165, the code currently passes an empty vector as a placeholder which\nprevents semantic retrieval; replace this with a real embedding of the query by\ninjecting/using an IEmbeddingModel (or existing embedding service) to compute\nthe query embedding, validate the resulting vector is non-empty, convert it to\nthe Vector<T> expected by GetSimilarWithFilters, and then call\nGetSimilarWithFilters with that embedding (keep oversampling topK*2 and existing\nmetadataFilters); ensure to add the embedding model dependency/injection and\nhandle errors/nulls (or log and return empty result) so the retriever returns\nsemantically relevant documents.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T02:51:07Z",
    "updated_at": "2025-11-05T02:51:12Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492671979",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492671979"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492671979"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492671979/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 161,
    "original_start_line": 161,
    "start_side": "RIGHT",
    "line": 165,
    "original_line": 165,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 165,
    "position": 165,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492671982",
    "pull_request_review_id": 3419460740,
    "id": 2492671982,
    "node_id": "PRRC_kwDOKSXUF86Ukyfu",
    "diff_hunk": "@@ -0,0 +1,170 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\n+{\n+    /// <summary>\n+    /// TF-IDF (Term Frequency-Inverse Document Frequency) retrieval strategy.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class TFIDFRetriever<T> : RetrieverBase<T>\n+    {\n+        private readonly IDocumentStore<T> _documentStore;\n+        private readonly Dictionary<string, Dictionary<string, T>> _tfidf;\n+        private readonly Dictionary<string, T> _idf;\n+\n+        public TFIDFRetriever(IDocumentStore<T> documentStore, int defaultTopK = 5) : base(defaultTopK)\n+        {\n+            if (documentStore == null)\n+                throw new ArgumentNullException(nameof(documentStore));\n+                \n+            _documentStore = documentStore;\n+            _tfidf = new Dictionary<string, Dictionary<string, T>>();\n+            _idf = new Dictionary<string, T>();\n+        }\n+\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\n+        {\n+            var queryTerms = Tokenize(query);\n+            var scores = new Dictionary<string, T>();\n+\n+            var candidates = _documentStore.GetSimilar(\n+                new AiDotNet.LinearAlgebra.Vector<T>(new T[_documentStore.VectorDimension]), \n+                _documentStore.DocumentCount\n+            );\n+\n+            var candidatesList = candidates.ToList();\n+            BuildTFIDFStatistics(candidatesList);\n+\n+            foreach (var doc in candidatesList.Where(d => MatchesFilters(d, metadataFilters)))\n+            {\n+                var score = NumOps.Zero;\n+\n+                if (_tfidf.TryGetValue(doc.Id, out var docTfidf))\n+                {\n+                    foreach (var term in queryTerms.Where(t => docTfidf.ContainsKey(t)))\n+                    {\n+                        score = NumOps.Add(score, docTfidf[term]);\n+                    }\n+                }\n+\n+                scores[doc.Id] = score;\n+            }\n+\n+            var results = scores\n+                .OrderByDescending(kv => kv.Value)\n+                .Take(topK)\n+                .Select(kv =>\n+                {\n+                    var doc = candidates.FirstOrDefault(d => d.Id == kv.Key);\n+                    if (doc != null)\n+                    {\n+                        doc.RelevanceScore = kv.Value;\n+                        doc.HasRelevanceScore = true;\n+                    }\n+                    return doc;\n+                })\n+                .Where(d => d != null)\n+                .Cast<Document<T>>();\n+\n+            return results;\n+        }",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/TFIDFRetriever.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "17e2d69e963ef1312b2871957655d96a4a245236",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Make TF-IDF statistics per-request to avoid race conditions.**\n\n`BuildTFIDFStatistics` clears and repopulates the shared `_tfidf`/`_idf` dictionaries every call. If the retriever is invoked concurrentlyΓÇöas happens under typical web loadsΓÇöthose dictionaries are mutated from multiple threads, causing lost updates and sporadic `InvalidOperationException`s. Please keep the TF-IDF state local to the request (or guard it with synchronization) before shipping.  \n\n\n```diff\n-            BuildTFIDFStatistics(candidatesList);\n+            var tfidf = BuildTFIDFStatistics(candidatesList);\n@@\n-            foreach (var doc in candidatesList.Where(d => MatchesFilters(d, metadataFilters)))\n+            foreach (var doc in candidatesList.Where(d => MatchesFilters(d, metadataFilters)))\n             {\n                 var score = NumOps.Zero;\n \n-                if (_tfidf.TryGetValue(doc.Id, out var docTfidf))\n+                if (tfidf.TryGetValue(doc.Id, out var docTfidf))\n                 {\n                     foreach (var term in queryTerms.Where(t => docTfidf.ContainsKey(t)))\n                     {\n                         score = NumOps.Add(score, docTfidf[term]);\n                     }\n                 }\n@@\n-        private void BuildTFIDFStatistics(List<Document<T>> documents)\n+        private Dictionary<string, Dictionary<string, T>> BuildTFIDFStatistics(List<Document<T>> documents)\n         {\n             if (documents == null || documents.Count == 0)\n-                return;\n-\n-            _tfidf.Clear();\n-            _idf.Clear();\n+                return new Dictionary<string, Dictionary<string, T>>();\n \n             var termDocFreq = new Dictionary<string, int>();\n             var docTermFreq = new Dictionary<string, Dictionary<string, int>>();\n@@\n-            foreach (var term in termDocFreq.Keys)\n+            var idf = new Dictionary<string, T>();\n+            foreach (var term in termDocFreq.Keys)\n             {\n                 var df = termDocFreq[term];\n-                var idf = NumOps.FromDouble(Math.Log((double)documents.Count / (double)df));\n-                _idf[term] = idf;\n+                idf[term] = NumOps.FromDouble(Math.Log((double)documents.Count / (double)df));\n             }\n \n+            var tfidf = new Dictionary<string, Dictionary<string, T>>();\n             foreach (var doc in documents)\n             {\n                 var termTfidf = new Dictionary<string, T>();\n                 var termCounts = docTermFreq[doc.Id];\n \n                 if (termCounts.Count == 0)\n                 {\n-                    _tfidf[doc.Id] = termTfidf;\n-                    continue;\n+                    tfidf[doc.Id] = termTfidf;\n+                    continue;\n                 }\n \n                 var maxFreq = termCounts.Values.Max();\n \n                 foreach (var termCount in termCounts)\n                 {\n                     var tf = NumOps.FromDouble((double)termCount.Value / (double)maxFreq);\n-                    var tfidf = NumOps.Multiply(tf, _idf[termCount.Key]);\n-                    termTfidf[termCount.Key] = tfidf;\n+                    termTfidf[termCount.Key] = NumOps.Multiply(tf, idf[termCount.Key]);\n                 }\n \n-                _tfidf[doc.Id] = termTfidf;\n+                tfidf[doc.Id] = termTfidf;\n             }\n+\n+            return tfidf;\n         }\n```\n\n\nAlso applies to: 87-151\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Retrievers/TFIDFRetriever.cs around lines\n30-75 (and similarly 87-151), the current BuildTFIDFStatistics mutates shared\n_tfidf/_idf dictionaries causing race conditions under concurrent requests; make\nTF-IDF data request-local by changing BuildTFIDFStatistics to return per-call\ndictionaries (e.g., local tfidf and idf structures) or populate local variables\ninside RetrieveCore instead of touching shared fields, use those locals for\nscoring and avoid modifying any shared state; alternatively, if per-request\nstate is not possible, guard all accesses and mutations of _tfidf/_idf with a\ndedicated lock to serialize updates and reads, and ensure callers use the\nreturned/local dictionaries for the rest of the method.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 2deef99 to 8c3de98",
    "created_at": "2025-11-05T02:51:08Z",
    "updated_at": "2025-11-05T16:14:42Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492671982",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492671982"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2492671982"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2492671982/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 26,
    "original_start_line": 30,
    "start_side": "RIGHT",
    "line": 68,
    "original_line": 75,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 75,
    "position": 68,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494505736",
    "pull_request_review_id": 3422055133,
    "id": 2494505736,
    "node_id": "PRRC_kwDOKSXUF86UryMI",
    "diff_hunk": "@@ -34,10 +33,11 @@ namespace AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n /// - Citations [1], [2], [3] point to the source documents\n /// </para>\n /// </remarks>\n+/// <typeparam name=\"T\">The numeric data type used for relevance scoring.</typeparam>\n /// <typeparam name=\"TInput\">The input data type for the model.</typeparam>\n /// <typeparam name=\"TOutput\">The output data type for the model.</typeparam>\n /// <typeparam name=\"TMetadata\">The metadata type for model information.</typeparam>",
    "path": "src/Interfaces/IGenerator.cs",
    "commit_id": "e1a421dadc8f1b7ec0ea4d028e993e8d16d093e9",
    "original_commit_id": "a5001e00cd349253e278d6648bd7b3b7518aeea9",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Remove stale `<typeparam>` tags**\n\nLines 36-39 still document `TInput`, `TOutput`, and `TMetadata`, but the interface now only has the single generic parameter `T`. These tags will generate XML documentation warnings and mislead consumers. Please drop the extra `<typeparam>` entries (or reintroduce the matching type parameters if that was intended).\n\n\n\n```diff\n-/// <typeparam name=\"T\">The numeric data type used for relevance scoring.</typeparam>\n-/// <typeparam name=\"TInput\">The input data type for the model.</typeparam>\n-/// <typeparam name=\"TOutput\">The output data type for the model.</typeparam>\n-/// <typeparam name=\"TMetadata\">The metadata type for model information.</typeparam>\n+/// <typeparam name=\"T\">The numeric data type used for relevance scoring.</typeparam>\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n/// <typeparam name=\"T\">The numeric data type used for relevance scoring.</typeparam>\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/Interfaces/IGenerator.cs around lines 36 to 39, the XML doc includes\nstale <typeparam> entries for TInput, TOutput, and TMetadata while the interface\nonly has a single generic parameter T; remove the extraneous <typeparam> tags so\nthe XML doc matches the current type parameters (or alternatively restore the\ngenerics if intended), ensuring no orphaned documentation entries remain to\nprevent XML-doc warnings.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commit 3f01d96",
    "created_at": "2025-11-05T13:29:51Z",
    "updated_at": "2025-11-05T14:28:22Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494505736",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494505736"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494505736"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494505736/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 36,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 39,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 16,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494505770",
    "pull_request_review_id": 3422055133,
    "id": 2494505770,
    "node_id": "PRRC_kwDOKSXUF86UryMq",
    "diff_hunk": "@@ -0,0 +1,120 @@\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies;\n+\n+/// <summary>\n+/// Splits structured documents based on header tags (H1, H2, H3, etc.).\n+/// </summary>\n+/// <remarks>\n+/// Ideal for Markdown and HTML documents where headers provide natural semantic boundaries.\n+/// Preserves document structure and hierarchy.\n+/// </remarks>\n+public class HeaderBasedTextSplitter : ChunkingStrategyBase\n+{\n+    private readonly bool _combineSmallChunks;\n+    private readonly int _minChunkSize;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"HeaderBasedTextSplitter\"/> class.\n+    /// </summary>\n+    /// <param name=\"chunkSize\">Maximum size of each chunk in characters.</param>\n+    /// <param name=\"chunkOverlap\">The number of characters that should overlap between consecutive chunks.</param>\n+    /// <param name=\"minChunkSize\">Minimum size for chunk combination.</param>\n+    /// <param name=\"combineSmallChunks\">Whether to combine small chunks.</param>\n+    public HeaderBasedTextSplitter(\n+        int chunkSize,\n+        int chunkOverlap = 0,\n+        int minChunkSize = 100,\n+        bool combineSmallChunks = true)\n+        : base(chunkSize, chunkOverlap)\n+    {\n+        if (minChunkSize < 0)\n+            throw new ArgumentOutOfRangeException(nameof(minChunkSize), \"Min chunk size cannot be negative\");\n+        \n+        if (minChunkSize > chunkSize)\n+            throw new ArgumentOutOfRangeException(nameof(minChunkSize), \"Min chunk size cannot exceed max chunk size\");\n+            \n+        _minChunkSize = minChunkSize;\n+        _combineSmallChunks = combineSmallChunks;\n+    }\n+\n+    /// <summary>\n+    /// Core chunking logic that splits text based on header hierarchy.\n+    /// </summary>\n+    protected override IEnumerable<(string Chunk, int StartPosition, int EndPosition)> ChunkCore(string text)\n+    {\n+        var chunks = new List<(string, int, int)>();\n+        var lines = text.Split(new[] { \"\\r\\n\", \"\\r\", \"\\n\" }, StringSplitOptions.None);\n+        var currentChunk = new List<string>();\n+        var chunkStart = 0;\n+        var position = 0;\n+\n+        foreach (var line in lines)\n+        {\n+            var lineLength = line.Length + Environment.NewLine.Length;\n+\n+            // Check if line is a header (Markdown ## or HTML <h>)\n+            if (IsHeader(line))\n+            {\n+                // Save current chunk if not empty\n+                if (currentChunk.Count > 0)\n+                {\n+                    var content = string.Join(Environment.NewLine, currentChunk);\n+                    if (content.Length >= _minChunkSize || !_combineSmallChunks)\n+                    {\n+                        chunks.Add((content, chunkStart, position));\n+                    }\n+                    currentChunk.Clear();\n+                }",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/HeaderBasedTextSplitter.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "a5001e00cd349253e278d6648bd7b3b7518aeea9",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Critical data loss: Small chunks are discarded when `combineSmallChunks` is true.**\n\nWhen a header is encountered and the current chunk is smaller than `_minChunkSize` with `_combineSmallChunks = true`, the chunk is neither saved nor combinedΓÇöit is simply discarded. Line 67 clears `currentChunk` regardless of whether the chunk was saved (line 65), causing content loss.\n\nExample: Given `minChunkSize = 100` and `combineSmallChunks = true`:\n```markdown\n# Header A\nSmall content (50 chars)\n# Header B\nMore content\n```\nThe content under \"Header A\" will be completely lost because it's below the minimum size.\n\n\n\nThe intended behavior is unclear. Consider one of these fixes:\n\n**Option 1: Always preserve content (recommended for correctness):**\n\n```diff\n             // Save current chunk if not empty\n             if (currentChunk.Count > 0)\n             {\n                 var content = string.Join(Environment.NewLine, currentChunk);\n-                if (content.Length >= _minChunkSize || !_combineSmallChunks)\n-                {\n-                    chunks.Add((content, chunkStart, position));\n-                }\n+                // Always save to prevent data loss; mark small chunks in metadata if needed\n+                chunks.Add((content, chunkStart, position));\n                 currentChunk.Clear();\n             }\n```\n\n**Option 2: Combine small chunks with the following section:**\n\n```diff\n             // Save current chunk if not empty\n             if (currentChunk.Count > 0)\n             {\n                 var content = string.Join(Environment.NewLine, currentChunk);\n                 if (content.Length >= _minChunkSize || !_combineSmallChunks)\n                 {\n                     chunks.Add((content, chunkStart, position));\n+                    currentChunk.Clear();\n                 }\n-                currentChunk.Clear();\n+                // If not saved, keep content to combine with next section\n             }\n```\n\nNote: Option 2 would merge content across semantic boundaries (headers), which may not be desirable.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn\nsrc/RetrievalAugmentedGeneration/ChunkingStrategies/HeaderBasedTextSplitter.cs\naround lines 60-68, the current logic clears currentChunk whenever a header is\nencountered which causes small chunks to be discarded when _combineSmallChunks\nis true; change the branch so that small chunks are preserved: always add the\ncurrent chunk to chunks (i.e., call chunks.Add((content, chunkStart, position)))\nbefore clearing, regardless of whether content.Length >= _minChunkSize, or\nalternatively (if you prefer combining) avoid clearing the currentChunk when\n_combineSmallChunks is true so it will be merged with the next section;\nimplement the chosen behavior and ensure currentChunk.Clear() only runs after\nthe chunk has been preserved or explicitly merged.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commit 8e06c50",
    "created_at": "2025-11-05T13:29:52Z",
    "updated_at": "2025-11-05T15:41:38Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494505770",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494505770"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494505770"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494505770/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 60,
    "original_start_line": 60,
    "start_side": "RIGHT",
    "line": 65,
    "original_line": 68,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 68,
    "position": 65,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494505781",
    "pull_request_review_id": 3422055133,
    "id": 2494505781,
    "node_id": "PRRC_kwDOKSXUF86UryM1",
    "diff_hunk": "@@ -0,0 +1,120 @@\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies;\n+\n+/// <summary>\n+/// Splits structured documents based on header tags (H1, H2, H3, etc.).\n+/// </summary>\n+/// <remarks>\n+/// Ideal for Markdown and HTML documents where headers provide natural semantic boundaries.\n+/// Preserves document structure and hierarchy.\n+/// </remarks>\n+public class HeaderBasedTextSplitter : ChunkingStrategyBase\n+{\n+    private readonly bool _combineSmallChunks;\n+    private readonly int _minChunkSize;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"HeaderBasedTextSplitter\"/> class.\n+    /// </summary>\n+    /// <param name=\"chunkSize\">Maximum size of each chunk in characters.</param>\n+    /// <param name=\"chunkOverlap\">The number of characters that should overlap between consecutive chunks.</param>\n+    /// <param name=\"minChunkSize\">Minimum size for chunk combination.</param>\n+    /// <param name=\"combineSmallChunks\">Whether to combine small chunks.</param>\n+    public HeaderBasedTextSplitter(\n+        int chunkSize,\n+        int chunkOverlap = 0,\n+        int minChunkSize = 100,\n+        bool combineSmallChunks = true)\n+        : base(chunkSize, chunkOverlap)\n+    {\n+        if (minChunkSize < 0)\n+            throw new ArgumentOutOfRangeException(nameof(minChunkSize), \"Min chunk size cannot be negative\");\n+        \n+        if (minChunkSize > chunkSize)\n+            throw new ArgumentOutOfRangeException(nameof(minChunkSize), \"Min chunk size cannot exceed max chunk size\");\n+            \n+        _minChunkSize = minChunkSize;\n+        _combineSmallChunks = combineSmallChunks;\n+    }\n+\n+    /// <summary>\n+    /// Core chunking logic that splits text based on header hierarchy.\n+    /// </summary>\n+    protected override IEnumerable<(string Chunk, int StartPosition, int EndPosition)> ChunkCore(string text)\n+    {\n+        var chunks = new List<(string, int, int)>();\n+        var lines = text.Split(new[] { \"\\r\\n\", \"\\r\", \"\\n\" }, StringSplitOptions.None);\n+        var currentChunk = new List<string>();\n+        var chunkStart = 0;\n+        var position = 0;\n+\n+        foreach (var line in lines)\n+        {\n+            var lineLength = line.Length + Environment.NewLine.Length;\n+\n+            // Check if line is a header (Markdown ## or HTML <h>)\n+            if (IsHeader(line))\n+            {\n+                // Save current chunk if not empty\n+                if (currentChunk.Count > 0)\n+                {\n+                    var content = string.Join(Environment.NewLine, currentChunk);\n+                    if (content.Length >= _minChunkSize || !_combineSmallChunks)\n+                    {\n+                        chunks.Add((content, chunkStart, position));\n+                    }\n+                    currentChunk.Clear();\n+                }\n+\n+                chunkStart = position;\n+                currentChunk.Add(line);\n+            }\n+            else\n+            {\n+                currentChunk.Add(line);\n+\n+                // Split if chunk gets too large\n+                var currentSize = string.Join(Environment.NewLine, currentChunk).Length;\n+                if (currentSize >= ChunkSize)\n+                {\n+                    var content = string.Join(Environment.NewLine, currentChunk);\n+                    chunks.Add((content, chunkStart, position + lineLength));\n+                    currentChunk.Clear();\n+                    chunkStart = position + lineLength;\n+                }\n+            }\n+\n+            position += lineLength;\n+        }\n+\n+        // Add remaining content\n+        if (currentChunk.Count > 0)\n+        {\n+            var content = string.Join(Environment.NewLine, currentChunk);\n+            chunks.Add((content, chunkStart, position));\n+        }\n+\n+        return chunks;\n+    }\n+\n+    private bool IsHeader(string line)\n+    {\n+        if (string.IsNullOrWhiteSpace(line))\n+            return false;\n+\n+        var trimmed = line.TrimStart();\n+        \n+        // Markdown headers (# ## ### etc.)\n+        if (trimmed.StartsWith(\"#\"))\n+            return true;\n+\n+        // HTML headers (<h1> <h2> etc.)\n+        if (trimmed.StartsWith(\"<h\", StringComparison.OrdinalIgnoreCase) &&\n+            trimmed.Length > 2 &&\n+            char.IsDigit(trimmed[2]))\n+            return true;\n+\n+        return false;\n+    }",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/HeaderBasedTextSplitter.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "a5001e00cd349253e278d6648bd7b3b7518aeea9",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Minor: `IsHeader` accepts invalid HTML header tags like `<h7>`.**\n\nThe detection logic at lines 113-116 checks if a line starts with `<h` followed by any digit, but valid HTML headers are only `<h1>` through `<h6>`. Lines starting with `<h7>`, `<h8>`, etc. will be incorrectly treated as headers.\n\n\n\nApply this diff to restrict detection to valid HTML headers:\n\n```diff\n         // HTML headers (<h1> <h2> etc.)\n         if (trimmed.StartsWith(\"<h\", StringComparison.OrdinalIgnoreCase) &&\n             trimmed.Length > 2 &&\n-            char.IsDigit(trimmed[2]))\n+            char.IsDigit(trimmed[2]) &&\n+            trimmed[2] >= '1' && trimmed[2] <= '6')\n             return true;\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn\nsrc/RetrievalAugmentedGeneration/ChunkingStrategies/HeaderBasedTextSplitter.cs\naround lines 101 to 119, the IsHeader method currently treats any <hN> with N as\nany digit as a header; change the HTML-header check to only accept digits '1'\nthrough '6' (e.g., verify trimmed.Length > 2 && trimmed[2] >= '1' && trimmed[2]\n<= '6') and optionally ensure the character after the digit is not another digit\n(or is '>'/whitespace) so tags like <h7> or <h10> are rejected; update that\nconditional accordingly.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T13:29:52Z",
    "updated_at": "2025-11-05T13:29:54Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494505781",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494505781"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494505781"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494505781/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 104,
    "original_start_line": 101,
    "start_side": "RIGHT",
    "line": 122,
    "original_line": 119,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 119,
    "position": 122,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494505799",
    "pull_request_review_id": 3422055133,
    "id": 2494505799,
    "node_id": "PRRC_kwDOKSXUF86UryNH",
    "diff_hunk": "@@ -0,0 +1,138 @@\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies;\n+\n+/// <summary>\n+/// Multi-modal splitter for documents containing both text and images.\n+/// </summary>\n+/// <remarks>\n+/// Creates chunks that keep text and related images together, preserving the relationship\n+/// between visual and textual content for better context preservation.\n+/// </remarks>\n+public class MultiModalTextSplitter : ChunkingStrategyBase\n+{\n+    private readonly bool _preserveImageContext;\n+    private readonly int _contextWindowSize;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"MultiModalTextSplitter\"/> class.\n+    /// </summary>\n+    /// <param name=\"chunkSize\">Maximum size of text portion in each chunk.</param>\n+    /// <param name=\"chunkOverlap\">The number of characters that should overlap between consecutive chunks.</param>\n+    /// <param name=\"contextWindowSize\">Number of characters before/after image to include.</param>\n+    /// <param name=\"preserveImageContext\">Whether to keep surrounding text with images.</param>\n+    public MultiModalTextSplitter(\n+        int chunkSize,\n+        int chunkOverlap = 0,\n+        int contextWindowSize = 200,\n+        bool preserveImageContext = true)\n+        : base(chunkSize, chunkOverlap)\n+    {\n+        if (contextWindowSize < 0)\n+            throw new ArgumentOutOfRangeException(nameof(contextWindowSize), \"Context window size cannot be negative\");\n+        \n+        // Ensure minimum context window to prevent division issues\n+        if (contextWindowSize > 0 && contextWindowSize < 50)\n+            throw new ArgumentOutOfRangeException(nameof(contextWindowSize), \"Context window size must be at least 50 characters when enabled\");\n+            \n+        _contextWindowSize = contextWindowSize;\n+        _preserveImageContext = preserveImageContext;\n+    }\n+\n+    /// <summary>\n+    /// Core chunking logic that splits text while preserving text-image relationships.\n+    /// </summary>\n+    protected override IEnumerable<(string Chunk, int StartPosition, int EndPosition)> ChunkCore(string text)\n+    {\n+        var chunks = new List<(string, int, int)>();\n+        var lines = text.Split(new[] { \"\\r\\n\", \"\\r\", \"\\n\" }, StringSplitOptions.None);\n+        \n+        var currentChunk = new List<string>();\n+        var chunkStart = 0;\n+        var position = 0;\n+\n+        for (var i = 0; i < lines.Length; i++)\n+        {\n+            var line = lines[i];\n+            var lineLength = line.Length + Environment.NewLine.Length;\n+\n+            // Detect image references (Markdown ![alt](url) or HTML <img>)\n+            if (IsImageReference(line))\n+            {\n+                if (_preserveImageContext)\n+                {\n+                    // Include context before image\n+                    var contextStart = Math.Max(0, i - (_contextWindowSize / 50)); // Approximate lines\n+                    for (var j = contextStart; j < i; j++)\n+                    {\n+                        if (!currentChunk.Contains(lines[j]))\n+                        {\n+                            currentChunk.Add(lines[j]);\n+                        }\n+                    }\n+                }\n+\n+                currentChunk.Add(line);\n+\n+                if (_preserveImageContext && _contextWindowSize > 0)\n+                {\n+                    // Include context after image (minimum 1 line if context window is set)\n+                    var linesToInclude = Math.Max(1, _contextWindowSize / 50);\n+                    var contextEnd = Math.Min(lines.Length, i + linesToInclude);\n+                    for (var j = i + 1; j < contextEnd; j++)\n+                    {\n+                        currentChunk.Add(lines[j]);\n+                    }\n+                    i = contextEnd - 1; // Skip ahead\n+                }\n+\n+                // Create chunk with image\n+                var content = string.Join(Environment.NewLine, currentChunk);\n+                chunks.Add((content, chunkStart, position + lineLength));\n+\n+                currentChunk.Clear();\n+                chunkStart = position + lineLength;\n+            }\n+            else\n+            {\n+                currentChunk.Add(line);\n+\n+                // Split if chunk gets too large\n+                var currentSize = string.Join(Environment.NewLine, currentChunk).Length;\n+                if (currentSize >= ChunkSize)\n+                {\n+                    var content = string.Join(Environment.NewLine, currentChunk);\n+                    chunks.Add((content, chunkStart, position + lineLength));\n+                    currentChunk.Clear();\n+                    chunkStart = position + lineLength;\n+                }",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/MultiModalTextSplitter.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "a5001e00cd349253e278d6648bd7b3b7518aeea9",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n<details>\n<summary>≡ƒº⌐ Analysis chain</summary>\n\n**Verify chunk overlap implementation.**\n\nThe chunking logic creates non-overlapping chunks when splitting on size. While `chunkOverlap` is passed to the base class constructor (line 29), it's unclear whether overlap is applied by the base class or should be implemented here in `ChunkCore`.\n\n\n\nVerify that `ChunkingStrategyBase` applies the overlap, or if overlap logic should be added here. If overlap must be implemented in this method, consecutive chunks should share `chunkOverlap` characters (e.g., when flushing a chunk, set `chunkStart = position + lineLength - chunkOverlap` instead of `position + lineLength`).\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check if ChunkingStrategyBase implements overlap logic\nast-grep --pattern $'class ChunkingStrategyBase {\n  $$$\n}'\n\n# Also check if ChunkCore is expected to handle overlap\nrg -nP -A10 'class ChunkingStrategyBase' --type cs\n```\n\nLength of output: 1166\n\n---\n\n```\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Get the full ChunkingStrategyBase class to see ChunkOverlap property\ncat src/RetrievalAugmentedGeneration/ChunkingStrategies/ChunkingStrategyBase.cs\n```\n\nLength of output: 10565\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Get the ChunkCore method from MultiModalTextSplitter\nrg -n 'protected abstract.*ChunkCore|public override.*ChunkCore' -A 50 src/RetrievalAugmentedGeneration/ChunkingStrategies/MultiModalTextSplitter.cs --type cs\n```\n\nLength of output: 0\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Check if ChunkCore is actually implemented in base or if it's abstract\nrg -n 'ChunkCore' src/RetrievalAugmentedGeneration/ChunkingStrategies/ --type cs -C 2\n```\n\nLength of output: 10813\n\n---\n\n```\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Get the full ChunkCore method from MultiModalTextSplitter\ncat src/RetrievalAugmentedGeneration/ChunkingStrategies/MultiModalTextSplitter.cs | head -150\n```\n\nLength of output: 5331\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Alternative: use sed to extract lines 45-110\nsed -n '45,110p' src/RetrievalAugmentedGeneration/ChunkingStrategies/MultiModalTextSplitter.cs\n```\n\nLength of output: 2643\n\n---\n\n\n\n</details>\n\n**Chunk overlap is not implemented; chunks will never overlap even when specified.**\n\nThe `ChunkCore` method accepts and passes `chunkOverlap` to the base class but does not apply it when flushing chunks. At line 105, when flushing on size, and line 94, when flushing after image content, `chunkStart` is set to `position + lineLength` without subtracting overlap. This creates non-overlapping chunks regardless of the configured `chunkOverlap` value.\n\nFix by applying overlap to chunk boundaries (e.g., `chunkStart = position + lineLength - ChunkOverlap` at lines 94 and 105), or use the base class helper `CreateOverlappingChunks()` for the non-image portion if feasible.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/ChunkingStrategies/MultiModalTextSplitter.cs\naround lines 94 and 100-108, the flush logic sets chunkStart = position +\nlineLength which ignores the configured ChunkOverlap and prevents overlapping\nchunks; adjust the flush logic to apply the overlap (e.g., set chunkStart =\nMath.Max(0, position + lineLength - ChunkOverlap)) so the next chunk begins\nearlier by the overlap amount, or alternatively call the base class helper\nCreateOverlappingChunks() for the non-image flush path to produce properly\noverlapping chunks; ensure you guard against negative start indices when\nsubtracting the overlap.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 2deef99 to 8c3de98",
    "created_at": "2025-11-05T13:29:52Z",
    "updated_at": "2025-11-05T16:14:43Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494505799",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494505799"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494505799"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494505799/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 100,
    "original_start_line": 100,
    "start_side": "RIGHT",
    "line": 128,
    "original_line": 108,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 108,
    "position": 128,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494505821",
    "pull_request_review_id": 3422055133,
    "id": 2494505821,
    "node_id": "PRRC_kwDOKSXUF86UryNd",
    "diff_hunk": "@@ -0,0 +1,198 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ContextCompression\n+{\n+    /// <summary>\n+    /// Document summarizer for creating concise summaries of retrieved content.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class DocumentSummarizer<T> : ContextCompressorBase<T>\n+    {\n+        private readonly int _maxSummaryLength;\n+        private readonly string _llmEndpoint;\n+        private readonly string _apiKey;\n+\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"DocumentSummarizer{T}\"/> class.\n+        /// </summary>\n+        /// <param name=\"numericOperations\">The numeric operations for type T.</param>\n+        /// <param name=\"maxSummaryLength\">The maximum length of the summary in characters.</param>\n+        /// <param name=\"llmEndpoint\">The LLM API endpoint.</param>\n+        /// <param name=\"apiKey\">The API key for the LLM service.</param>\n+        public DocumentSummarizer(\n+            INumericOperations<T> numericOperations,\n+            int maxSummaryLength = 500,\n+            string llmEndpoint = \"\",\n+            string apiKey = \"\")\n+        {\n+            if (numericOperations == null)\n+                throw new ArgumentNullException(nameof(numericOperations));\n+                \n+            _maxSummaryLength = maxSummaryLength > 0\n+                ? maxSummaryLength\n+                : throw new ArgumentOutOfRangeException(nameof(maxSummaryLength));\n+            _llmEndpoint = llmEndpoint;\n+            _apiKey = apiKey;\n+        }\n+\n+        /// <summary>\n+        /// Compresses documents by summarizing their content.\n+        /// </summary>\n+        protected override List<Document<T>> CompressCore(\n+            List<Document<T>> documents,\n+            string query,\n+            Dictionary<string, object>? options = null)",
    "path": "src/RetrievalAugmentedGeneration/ContextCompression/DocumentSummarizer.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "a5001e00cd349253e278d6648bd7b3b7518aeea9",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Query parameter is unusedΓÇösummarization ignores query relevance.**\n\nThe `query` parameter is accepted but never used. Context compressors in RAG pipelines should prioritize content relevant to the query, yet this implementation produces generic extractive summaries that don't consider query terms or semantics. Consider incorporating query tokens into the importance scoring or filtering sentences by query relevance.\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 2deef99 to 8c3de98",
    "created_at": "2025-11-05T13:29:53Z",
    "updated_at": "2025-11-05T16:14:44Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494505821",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494505821"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494505821"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494505821/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 45,
    "original_start_line": 45,
    "start_side": "RIGHT",
    "line": 48,
    "original_line": 48,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 48,
    "position": 48,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494505841",
    "pull_request_review_id": 3422055133,
    "id": 2494505841,
    "node_id": "PRRC_kwDOKSXUF86UryNx",
    "diff_hunk": "@@ -0,0 +1,198 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ContextCompression\n+{\n+    /// <summary>\n+    /// Document summarizer for creating concise summaries of retrieved content.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class DocumentSummarizer<T> : ContextCompressorBase<T>\n+    {\n+        private readonly int _maxSummaryLength;\n+        private readonly string _llmEndpoint;\n+        private readonly string _apiKey;\n+\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"DocumentSummarizer{T}\"/> class.\n+        /// </summary>\n+        /// <param name=\"numericOperations\">The numeric operations for type T.</param>\n+        /// <param name=\"maxSummaryLength\">The maximum length of the summary in characters.</param>\n+        /// <param name=\"llmEndpoint\">The LLM API endpoint.</param>\n+        /// <param name=\"apiKey\">The API key for the LLM service.</param>\n+        public DocumentSummarizer(\n+            INumericOperations<T> numericOperations,\n+            int maxSummaryLength = 500,\n+            string llmEndpoint = \"\",\n+            string apiKey = \"\")\n+        {\n+            if (numericOperations == null)\n+                throw new ArgumentNullException(nameof(numericOperations));\n+                \n+            _maxSummaryLength = maxSummaryLength > 0\n+                ? maxSummaryLength\n+                : throw new ArgumentOutOfRangeException(nameof(maxSummaryLength));\n+            _llmEndpoint = llmEndpoint;\n+            _apiKey = apiKey;\n+        }\n+\n+        /// <summary>\n+        /// Compresses documents by summarizing their content.\n+        /// </summary>\n+        protected override List<Document<T>> CompressCore(\n+            List<Document<T>> documents,\n+            string query,\n+            Dictionary<string, object>? options = null)\n+        {\n+            var summarized = new List<Document<T>>();\n+\n+            foreach (var doc in documents)\n+            {\n+                var summary = SummarizeText(doc.Content);\n+                var summarizedDoc = new Document<T>(doc.Id, summary)\n+                {\n+                    Metadata = doc.Metadata,\n+                    RelevanceScore = doc.RelevanceScore,\n+                    HasRelevanceScore = doc.HasRelevanceScore\n+                };\n+                summarized.Add(summarizedDoc);\n+            }\n+\n+            return summarized;\n+        }\n+\n+        /// <summary>\n+        /// Summarizes a list of documents.\n+        /// </summary>\n+        /// <param name=\"documents\">The documents to summarize.</param>\n+        /// <returns>A list of summarized documents.</returns>\n+        public List<Document<T>> Summarize(List<Document<T>> documents)\n+        {\n+            if (documents == null) throw new ArgumentNullException(nameof(documents));\n+\n+            var summarized = new List<Document<T>>();\n+\n+            foreach (var doc in documents)\n+            {\n+                var summary = SummarizeText(doc.Content);\n+                var summarizedDoc = new Document<T>(doc.Id, summary)\n+                {\n+                    Metadata = doc.Metadata,\n+                    RelevanceScore = doc.RelevanceScore,\n+                    HasRelevanceScore = doc.HasRelevanceScore\n+                };\n+                summarized.Add(summarizedDoc);\n+            }\n+\n+            return summarized;\n+        }\n+\n+        /// <summary>\n+        /// Summarizes text to a maximum length.\n+        /// </summary>\n+        /// <param name=\"text\">The text to summarize.</param>\n+        /// <returns>The summarized text.</returns>\n+        public string SummarizeText(string text)\n+        {\n+            if (string.IsNullOrEmpty(text)) return text;\n+\n+            if (text.Length <= _maxSummaryLength)\n+            {\n+                return text;\n+            }\n+\n+            var sentences = SplitIntoSentences(text);\n+            var importantSentences = ExtractImportantSentences(sentences);\n+\n+            var summary = new System.Text.StringBuilder();\n+            foreach (var sentence in importantSentences)\n+            {\n+                if (summary.Length + sentence.Length > _maxSummaryLength)\n+                {\n+                    // If we haven't added anything yet and the first sentence is too long,\n+                    // truncate it to fit\n+                    if (summary.Length == 0 && sentence.Length > _maxSummaryLength)\n+                    {\n+                        return sentence.Substring(0, _maxSummaryLength).Trim() + \"...\";\n+                    }\n+                    break;\n+                }\n+                summary.Append(sentence).Append(\" \");\n+            }\n+\n+            var result = summary.ToString().Trim();\n+            // If result is empty (all sentences too long), return truncated first sentence\n+            return string.IsNullOrEmpty(result) && importantSentences.Any()\n+                ? importantSentences.First().Substring(0, Math.Min(importantSentences.First().Length, _maxSummaryLength)).Trim() + \"...\"\n+                : result;\n+        }\n+\n+        private List<string> ExtractImportantSentences(List<string> sentences)\n+        {\n+            var scored = new List<(string sentence, double score)>();\n+\n+            foreach (var sentence in sentences)\n+            {\n+                var importance = ComputeImportance(sentence, sentences);\n+                scored.Add((sentence, importance));\n+            }\n+\n+            return scored\n+                .OrderByDescending(x => x.score)\n+                .Select(x => x.sentence)\n+                .ToList();\n+        }\n+\n+        private double ComputeImportance(string sentence, List<string> allSentences)\n+        {\n+            var tokens = Tokenize(sentence);\n+            var uniqueTokens = tokens.Distinct().Count();\n+            var length = sentence.Length;\n+\n+            var positionScore = allSentences.IndexOf(sentence) == 0 ? 1.5 : 1.0;\n+\n+            var importance = (uniqueTokens * 0.5) + (Math.Min(length, 200) / 200.0 * 0.5);\n+            importance *= positionScore;\n+\n+            return importance;\n+        }\n+\n+        private List<string> SplitIntoSentences(string text)\n+        {\n+            var sentences = new List<string>();\n+            var sentenceEndings = new[] { \". \", \"! \", \"? \", \".\\n\", \"!\\n\", \"?\\n\" };\n+            var currentSentence = new System.Text.StringBuilder();\n+\n+            for (int i = 0; i < text.Length; i++)\n+            {\n+                currentSentence.Append(text[i]);\n+\n+                var matchedEnding = sentenceEndings.FirstOrDefault(ending => currentSentence.ToString().EndsWith(ending));\n+                if (matchedEnding != null)\n+                {\n+                    sentences.Add(currentSentence.ToString().Trim());\n+                    currentSentence.Clear();\n+                }\n+            }\n+\n+            if (currentSentence.Length > 0 && !string.IsNullOrWhiteSpace(currentSentence.ToString()))\n+            {\n+                sentences.Add(currentSentence.ToString().Trim());\n+            }\n+\n+            return sentences;\n+        }\n+\n+        private List<string> Tokenize(string text)\n+        {\n+            if (string.IsNullOrEmpty(text)) return new List<string>();\n+\n+            return text.ToLowerInvariant()\n+                .Split(new[] { ' ', '\\t', '\\n', '\\r', '.', ',', '!', '?' }, StringSplitOptions.RemoveEmptyEntries)\n+                .ToList();\n+        }",
    "path": "src/RetrievalAugmentedGeneration/ContextCompression/DocumentSummarizer.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "a5001e00cd349253e278d6648bd7b3b7518aeea9",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_≡ƒ¢á∩╕Å Refactor suggestion_ | _≡ƒƒá Major_\n\n**Extract duplicated text processing methods to shared utility class.**\n\nBoth `SplitIntoSentences` (lines 163-187) and `Tokenize` (lines 189-196) are duplicated verbatim in `SemanticChunkingStrategy.cs` and `LLMContextCompressor.cs`. This violates DRY and creates maintenance burden.\n\n\n\nRefactor by creating a shared utility class:\n\n```csharp\n// In AiDotNet.Helpers or similar namespace\npublic static class TextProcessingHelper\n{\n    public static List<string> SplitIntoSentences(string text)\n    {\n        var sentences = new List<string>();\n        var sentenceEndings = new[] { \". \", \"! \", \"? \", \".\\n\", \"!\\n\", \"?\\n\" };\n        var currentSentence = new System.Text.StringBuilder();\n\n        for (int i = 0; i < text.Length; i++)\n        {\n            currentSentence.Append(text[i]);\n\n            var matchedEnding = sentenceEndings.FirstOrDefault(ending => \n                currentSentence.ToString().EndsWith(ending));\n            if (matchedEnding != null)\n            {\n                sentences.Add(currentSentence.ToString().Trim());\n                currentSentence.Clear();\n            }\n        }\n\n        if (currentSentence.Length > 0 && !string.IsNullOrWhiteSpace(currentSentence.ToString()))\n        {\n            sentences.Add(currentSentence.ToString().Trim());\n        }\n\n        return sentences;\n    }\n\n    public static List<string> Tokenize(string text)\n    {\n        if (string.IsNullOrEmpty(text)) return new List<string>();\n\n        return text.ToLowerInvariant()\n            .Split(new[] { ' ', '\\t', '\\n', '\\r', '.', ',', '!', '?' }, \n                   StringSplitOptions.RemoveEmptyEntries)\n            .ToList();\n    }\n}\n```\n\nThen update this file and the other files to use the shared methods:\n\n```diff\n private List<string> SplitIntoSentences(string text)\n {\n-    var sentences = new List<string>();\n-    var sentenceEndings = new[] { \". \", \"! \", \"? \", \".\\n\", \"!\\n\", \"?\\n\" };\n-    var currentSentence = new System.Text.StringBuilder();\n-\n-    for (int i = 0; i < text.Length; i++)\n-    {\n-        currentSentence.Append(text[i]);\n-\n-        var matchedEnding = sentenceEndings.FirstOrDefault(ending => currentSentence.ToString().EndsWith(ending));\n-        if (matchedEnding != null)\n-        {\n-            sentences.Add(currentSentence.ToString().Trim());\n-            currentSentence.Clear();\n-        }\n-    }\n-\n-    if (currentSentence.Length > 0 && !string.IsNullOrWhiteSpace(currentSentence.ToString()))\n-    {\n-        sentences.Add(currentSentence.ToString().Trim());\n-    }\n-\n-    return sentences;\n+    return TextProcessingHelper.SplitIntoSentences(text);\n }\n\n private List<string> Tokenize(string text)\n {\n-    if (string.IsNullOrEmpty(text)) return new List<string>();\n-\n-    return text.ToLowerInvariant()\n-        .Split(new[] { ' ', '\\t', '\\n', '\\r', '.', ',', '!', '?' }, StringSplitOptions.RemoveEmptyEntries)\n-        .ToList();\n+    return TextProcessingHelper.Tokenize(text);\n }\n```\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T13:29:53Z",
    "updated_at": "2025-11-05T13:29:54Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494505841",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494505841"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494505841"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494505841/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 176,
    "original_start_line": 163,
    "start_side": "RIGHT",
    "line": 209,
    "original_line": 196,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 196,
    "position": 209,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494505865",
    "pull_request_review_id": 3422055133,
    "id": 2494505865,
    "node_id": "PRRC_kwDOKSXUF86UryOJ",
    "diff_hunk": "@@ -0,0 +1,165 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\n+{\n+    /// <summary>\n+    /// TF-IDF (Term Frequency-Inverse Document Frequency) retrieval strategy.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class TFIDFRetriever<T> : RetrieverBase<T>\n+    {\n+        private readonly IDocumentStore<T> _documentStore;\n+\n+        public TFIDFRetriever(IDocumentStore<T> documentStore, int defaultTopK = 5) : base(defaultTopK)\n+        {\n+            if (documentStore == null)\n+                throw new ArgumentNullException(nameof(documentStore));\n+                \n+            _documentStore = documentStore;\n+        }\n+\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\n+        {\n+            var queryTerms = Tokenize(query);\n+            var scores = new Dictionary<string, T>();\n+\n+            var candidates = _documentStore.GetSimilar(\n+                new AiDotNet.LinearAlgebra.Vector<T>(new T[_documentStore.VectorDimension]), \n+                _documentStore.DocumentCount\n+            );\n+\n+            var candidatesList = candidates.ToList();\n+            var tfidf = BuildTFIDFStatistics(candidatesList);\n+\n+            foreach (var doc in candidatesList.Where(d => MatchesFilters(d, metadataFilters)))\n+            {\n+                var score = NumOps.Zero;\n+\n+                if (tfidf.TryGetValue(doc.Id, out var docTfidf))\n+                {\n+                    foreach (var term in queryTerms.Where(t => docTfidf.ContainsKey(t)))\n+                    {\n+                        score = NumOps.Add(score, docTfidf[term]);\n+                    }\n+                }\n+\n+                scores[doc.Id] = score;\n+            }\n+\n+            var results = scores\n+                .OrderByDescending(kv => kv.Value)\n+                .Take(topK)\n+                .Select(kv =>\n+                {\n+                    var doc = candidates.FirstOrDefault(d => d.Id == kv.Key);\n+                    if (doc != null)\n+                    {\n+                        doc.RelevanceScore = kv.Value;\n+                        doc.HasRelevanceScore = true;\n+                    }\n+                    return doc;\n+                })\n+                .Where(d => d != null)\n+                .Cast<Document<T>>();\n+\n+            return results;",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/TFIDFRetriever.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "a5001e00cd349253e278d6648bd7b3b7518aeea9",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Inefficient document lookup after scoring.**\n\nLine 59 performs a linear search through `candidates` (potentially thousands of documents) for every result in the top K. Since you already iterate over the documents at line 39, store the `Document<T>` reference alongside the score to eliminate this O(topK ├ù N) overhead.\n\n\n\nApply this diff to cache document references:\n\n```diff\n protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\n {\n     var queryTerms = Tokenize(query);\n-    var scores = new Dictionary<string, T>();\n+    var scoredDocs = new List<(Document<T> Doc, T Score)>();\n\n     var candidates = _documentStore.GetSimilar(\n         new AiDotNet.LinearAlgebra.Vector<T>(new T[_documentStore.VectorDimension]), \n         _documentStore.DocumentCount\n     );\n\n     var candidatesList = candidates.ToList();\n     var tfidf = BuildTFIDFStatistics(candidatesList);\n\n     foreach (var doc in candidatesList.Where(d => MatchesFilters(d, metadataFilters)))\n     {\n         var score = NumOps.Zero;\n\n         if (tfidf.TryGetValue(doc.Id, out var docTfidf))\n         {\n             foreach (var term in queryTerms.Where(t => docTfidf.ContainsKey(t)))\n             {\n                 score = NumOps.Add(score, docTfidf[term]);\n             }\n         }\n\n-        scores[doc.Id] = score;\n+        scoredDocs.Add((doc, score));\n     }\n\n-    var results = scores\n-        .OrderByDescending(kv => kv.Value)\n+    var results = scoredDocs\n+        .OrderByDescending(sd => sd.Score)\n         .Take(topK)\n-        .Select(kv =>\n+        .Select(sd =>\n         {\n-            var doc = candidates.FirstOrDefault(d => d.Id == kv.Key);\n-            if (doc != null)\n-            {\n-                doc.RelevanceScore = kv.Value;\n-                doc.HasRelevanceScore = true;\n-            }\n-            return doc;\n+            sd.Doc.RelevanceScore = sd.Score;\n+            sd.Doc.HasRelevanceScore = true;\n+            return sd.Doc;\n-        })\n-        .Where(d => d != null)\n-        .Cast<Document<T>>();\n+        });\n\n     return results;\n }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\nprotected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\n{\n    var queryTerms = Tokenize(query);\n    var scoredDocs = new List<(Document<T> Doc, T Score)>();\n\n    var candidates = _documentStore.GetSimilar(\n        new AiDotNet.LinearAlgebra.Vector<T>(new T[_documentStore.VectorDimension]), \n        _documentStore.DocumentCount\n    );\n\n    var candidatesList = candidates.ToList();\n    var tfidf = BuildTFIDFStatistics(candidatesList);\n\n    foreach (var doc in candidatesList.Where(d => MatchesFilters(d, metadataFilters)))\n    {\n        var score = NumOps.Zero;\n\n        if (tfidf.TryGetValue(doc.Id, out var docTfidf))\n        {\n            foreach (var term in queryTerms.Where(t => docTfidf.ContainsKey(t)))\n            {\n                score = NumOps.Add(score, docTfidf[term]);\n            }\n        }\n\n        scoredDocs.Add((doc, score));\n    }\n\n    var results = scoredDocs\n        .OrderByDescending(sd => sd.Score)\n        .Take(topK)\n        .Select(sd =>\n        {\n            sd.Doc.RelevanceScore = sd.Score;\n            sd.Doc.HasRelevanceScore = true;\n            return sd.Doc;\n        });\n\n    return results;\n}\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Retrievers/TFIDFRetriever.cs around lines 54\nto 70, the code does a linear search against candidates for each topK score\nwhich creates O(topK * N) overhead; modify the scoring phase (earlier where you\niterate candidates) to store a direct reference to each Document<T> alongside\nits score (e.g., a Dictionary<string, Document<T>> or a list of (id, score, doc)\ntuples), then when ordering and taking topK, use that cached Document<T>\nreference instead of calling candidates.FirstOrDefault; set doc.RelevanceScore\nand doc.HasRelevanceScore on the cached instance and return the selected\nDocument<T> sequence ΓÇö this eliminates repeated linear lookups and reduces\ncomplexity to O(N + K log K).\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 2deef99 to 8c3de98",
    "created_at": "2025-11-05T13:29:53Z",
    "updated_at": "2025-11-05T16:14:53Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494505865",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494505865"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494505865"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494505865/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 51,
    "original_start_line": 54,
    "start_side": "RIGHT",
    "line": 67,
    "original_line": 70,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 70,
    "position": 67,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494655553",
    "pull_request_review_id": 3422265820,
    "id": 2494655553,
    "node_id": "PRRC_kwDOKSXUF86UsWxB",
    "diff_hunk": "@@ -0,0 +1,72 @@\n+using System.Text.RegularExpressions;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.QueryProcessors;\n+\n+/// <summary>\n+/// Extracts key terms and phrases from queries for focused retrieval.\n+/// </summary>\n+/// <remarks>\n+/// <para>\n+/// This processor identifies and extracts the most important keywords from a query,\n+/// removing filler words and focusing on content-bearing terms. This helps create\n+/// more focused and efficient searches.\n+/// </para>\n+/// <para><b>For Beginners:</b> Picks out the important words from your question.\n+/// \n+/// Examples:\n+/// - \"Explain the principles of quantum entanglement in simple terms\"\n+///   ΓåÆ \"quantum entanglement principles simple terms\"\n+/// - \"What are the main features of the new iPhone?\"\n+///   ΓåÆ \"main features new iPhone\"\n+/// - \"Can you tell me about machine learning algorithms?\"\n+///   ΓåÆ \"machine learning algorithms\"\n+/// \n+/// This focuses your search on what really matters!\n+/// </para>\n+/// </remarks>\n+public class KeywordExtractionQueryProcessor : QueryProcessorBase\n+{\n+    private readonly HashSet<string> _stopWords;\n+    private readonly int _minWordLength;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the KeywordExtractionQueryProcessor class.\n+    /// </summary>\n+    /// <param name=\"customStopWords\">Optional custom set of stop words to filter out.</param>\n+    /// <param name=\"minWordLength\">Minimum word length to keep (default: 2).</param>\n+    public KeywordExtractionQueryProcessor(\n+        HashSet<string>? customStopWords = null,\n+        int minWordLength = 2)\n+    {\n+        _stopWords = customStopWords ?? GetDefaultStopWords();\n+        _minWordLength = minWordLength;\n+    }\n+\n+    protected override string ProcessQueryCore(string query)\n+    {\n+        if (string.IsNullOrWhiteSpace(query))\n+            return query;\n+\n+        var words = Regex.Split(query.ToLowerInvariant(), @\"\\W+\")\n+            .Where(w => !string.IsNullOrWhiteSpace(w))\n+            .Where(w => w.Length >= _minWordLength)\n+            .Where(w => !_stopWords.Contains(w))\n+            .ToList();\n+\n+        return string.Join(\" \", words);\n+    }\n+\n+    private static HashSet<string> GetDefaultStopWords()\n+    {\n+        return new HashSet<string>\n+        {\n+            \"a\", \"an\", \"and\", \"are\", \"as\", \"at\", \"be\", \"by\", \"for\", \"from\",\n+            \"has\", \"he\", \"in\", \"is\", \"it\", \"its\", \"of\", \"on\", \"that\", \"the\",\n+            \"to\", \"was\", \"will\", \"with\", \"the\", \"this\", \"but\", \"they\", \"have\",\n+            \"had\", \"what\", \"when\", \"where\", \"who\", \"which\", \"why\", \"how\",\n+            \"can\", \"could\", \"should\", \"would\", \"may\", \"might\", \"must\",\n+            \"shall\", \"will\", \"do\", \"does\", \"did\", \"tell\", \"me\", \"you\",\n+            \"about\", \"explain\", \"describe\", \"please\"\n+        };\n+    }",
    "path": "src/RetrievalAugmentedGeneration/QueryProcessors/KeywordExtractionQueryProcessor.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "26162d2c7dd726f3ceef084ba4e7de4f637c64e8",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Fix duplicate \"the\" entry and extract shared stop word list.**\n\n**Issue 1**: Line 65 contains `\"the\"` twice, which is redundant in a HashSet:\n```csharp\n\"to\", \"was\", \"will\", \"with\", \"the\", \"this\", \"but\", \"they\", \"have\",\n```\n\n**Issue 2**: Multiple query processors define similar stop word lists (KeywordExtractionQueryProcessor, StopWordRemovalQueryProcessor). This should be extracted to a shared constant or utility class.\n\n\n\n\nApply this diff to fix the duplicate:\n\n```diff\n-            \"to\", \"was\", \"will\", \"with\", \"the\", \"this\", \"but\", \"they\", \"have\",\n+            \"to\", \"was\", \"will\", \"with\", \"this\", \"but\", \"they\", \"have\",\n```\n\nThen consider extracting a shared stop word set:\n\n```csharp\nnamespace AiDotNet.RetrievalAugmentedGeneration.QueryProcessors;\n\ninternal static class CommonStopWords\n{\n    internal static HashSet<string> GetEnglishStopWords() => new()\n    {\n        \"a\", \"an\", \"and\", \"are\", \"as\", \"at\", \"be\", \"by\", \"for\", \"from\",\n        // ... complete set\n    };\n}\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn\nsrc/RetrievalAugmentedGeneration/QueryProcessors/KeywordExtractionQueryProcessor.cs\naround lines 59-71, remove the duplicated \"the\" from the GetDefaultStopWords\nHashSet and replace the local stopword set with a call to a shared stopword\nprovider; create an internal static CommonStopWords (e.g., GetEnglishStopWords)\nin a shared namespace that returns the canonical HashSet<string> and update\nKeywordExtractionQueryProcessor and other processors\n(StopWordRemovalQueryProcessor) to use CommonStopWords.GetEnglishStopWords()\ninstead of duplicating literals so all processors share one source of truth.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T14:03:50Z",
    "updated_at": "2025-11-05T14:03:52Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494655553",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494655553"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494655553"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494655553/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 59,
    "original_start_line": 59,
    "start_side": "RIGHT",
    "line": 71,
    "original_line": 71,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 71,
    "position": 71,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494655563",
    "pull_request_review_id": 3422265820,
    "id": 2494655563,
    "node_id": "PRRC_kwDOKSXUF86UsWxL",
    "diff_hunk": "@@ -0,0 +1,138 @@\n+using System.Text.RegularExpressions;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.QueryProcessors;\n+\n+/// <summary>\n+/// Reduces words to their base or dictionary form (lemma) for better matching.\n+/// </summary>\n+/// <remarks>\n+/// <para>\n+/// Lemmatization transforms words to their base form considering the word's meaning.\n+/// Unlike stemming, it produces valid dictionary words.\n+/// </para>\n+/// <para><b>For Beginners:</b> Converts words to their basic form.\n+/// \n+/// Examples:\n+/// - \"running\" ΓåÆ \"run\"\n+/// - \"better\" ΓåÆ \"good\"\n+/// - \"was\" ΓåÆ \"be\"\n+/// - \"cars\" ΓåÆ \"car\"\n+/// \n+/// This helps match documents that use different forms of the same word!\n+/// </para>\n+/// </remarks>\n+public class LemmatizationQueryProcessor : QueryProcessorBase\n+{\n+    private readonly Dictionary<string, string> _lemmaMap;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the LemmatizationQueryProcessor class.\n+    /// </summary>\n+    /// <param name=\"customLemmaMap\">Optional custom lemmatization dictionary.</param>\n+    public LemmatizationQueryProcessor(Dictionary<string, string>? customLemmaMap = null)\n+    {\n+        _lemmaMap = customLemmaMap ?? GetDefaultLemmaMap();\n+    }\n+\n+    protected override string ProcessQueryCore(string query)\n+    {\n+        if (string.IsNullOrWhiteSpace(query))\n+            return query;\n+\n+        var words = Regex.Split(query, @\"(\\s+)\");\n+        var lemmatized = new List<string>();\n+\n+        foreach (var word in words)\n+        {\n+            if (string.IsNullOrWhiteSpace(word))\n+            {\n+                lemmatized.Add(word);\n+                continue;\n+            }\n+\n+            var lowerWord = word.ToLowerInvariant();\n+            \n+            if (_lemmaMap.TryGetValue(lowerWord, out var lemma))\n+            {\n+                lemmatized.Add(PreserveCase(word, lemma));\n+            }\n+            else if (lowerWord.EndsWith(\"ing\") && lowerWord.Length > 4)\n+            {\n+                var stem = lowerWord.Substring(0, lowerWord.Length - 3);\n+                if (_lemmaMap.TryGetValue(stem, out lemma))\n+                {\n+                    lemmatized.Add(PreserveCase(word, lemma));\n+                }\n+                else\n+                {\n+                    lemmatized.Add(PreserveCase(word, stem));\n+                }\n+            }\n+            else if (lowerWord.EndsWith(\"s\") && lowerWord.Length > 2 && !lowerWord.EndsWith(\"ss\"))\n+            {\n+                var singular = lowerWord.Substring(0, lowerWord.Length - 1);\n+                lemmatized.Add(PreserveCase(word, singular));\n+            }\n+            else\n+            {\n+                lemmatized.Add(word);\n+            }\n+        }\n+\n+        return string.Join(\"\", lemmatized);\n+    }\n+\n+    private static string PreserveCase(string original, string lemma)\n+    {\n+        if (string.IsNullOrEmpty(original) || string.IsNullOrEmpty(lemma))\n+            return lemma;\n+\n+        if (char.IsUpper(original[0]))\n+        {\n+            return char.ToUpper(lemma[0]) + lemma.Substring(1);\n+        }\n+\n+        return lemma;\n+    }",
    "path": "src/RetrievalAugmentedGeneration/QueryProcessors/LemmatizationQueryProcessor.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "26162d2c7dd726f3ceef084ba4e7de4f637c64e8",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_≡ƒ¢á∩╕Å Refactor suggestion_ | _≡ƒƒá Major_\n\n**Extract duplicated PreserveCase logic to a shared utility.**\n\nThis `PreserveCase` method is duplicated across multiple query processors (SpellCheckQueryProcessor at lines 73-84, and here). The logic is identical and should be extracted to a shared helper class or base class to follow DRY principles.\n\n\n\nConsider creating a shared utility class:\n\n```csharp\nnamespace AiDotNet.RetrievalAugmentedGeneration.QueryProcessors;\n\ninternal static class QueryProcessorHelpers\n{\n    internal static string PreserveCase(string original, string transformed)\n    {\n        if (string.IsNullOrEmpty(original) || string.IsNullOrEmpty(transformed))\n            return transformed;\n\n        if (char.IsUpper(original[0]))\n        {\n            return char.ToUpper(transformed[0]) + transformed.Substring(1);\n        }\n\n        return transformed;\n    }\n}\n```\n\nThen update LemmatizationQueryProcessor, SpellCheckQueryProcessor, and any other processors to use this shared helper.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn\nsrc/RetrievalAugmentedGeneration/QueryProcessors/LemmatizationQueryProcessor.cs\naround lines 85-96, the PreserveCase method is duplicated across processors;\nextract this logic into a shared internal static helper (e.g.,\nAiDotNet.RetrievalAugmentedGeneration.QueryProcessors.QueryProcessorHelpers.PreserveCase)\nthat accepts (string original, string transformed) and implements the same\nnull/empty and uppercase-first-char behavior, then remove the local PreserveCase\nmethod and replace calls with QueryProcessorHelpers.PreserveCase; repeat the\nsame replacement in SpellCheckQueryProcessor (around lines 73-84) and any other\nprocessors, adding the appropriate using or fully-qualified name and making the\nhelper internal static so all processors can access it.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T14:03:51Z",
    "updated_at": "2025-11-05T14:03:52Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494655563",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494655563"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494655563"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494655563/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 85,
    "original_start_line": 85,
    "start_side": "RIGHT",
    "line": 96,
    "original_line": 96,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 96,
    "position": 96,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494655568",
    "pull_request_review_id": 3422265820,
    "id": 2494655568,
    "node_id": "PRRC_kwDOKSXUF86UsWxQ",
    "diff_hunk": "@@ -0,0 +1,126 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.QueryProcessors;\n+\n+/// <summary>\n+/// Rewrites queries for clarity and completeness, especially in conversational contexts.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type for computations.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// This processor transforms conversational or context-dependent queries into standalone,\n+/// clear questions. It's particularly useful in multi-turn conversations where queries\n+/// reference previous context.\n+/// </para>\n+/// <para><b>For Beginners:</b> Makes incomplete questions complete by adding missing context.\n+/// \n+/// Conversational Examples:\n+/// - User: \"Tell me about transformers\"\n+/// - User: \"What about their applications?\" ΓåÆ \"What are the applications of transformers?\"\n+/// \n+/// Clarity Examples:\n+/// - \"how r cars made\" ΓåÆ \"how are cars manufactured\"\n+/// - \"wht is AI\" ΓåÆ \"what is artificial intelligence\"\n+/// \n+/// This makes your searches clearer and gets better results!\n+/// </para>\n+/// </remarks>\n+public class QueryRewritingProcessor<T> : QueryProcessorBase\n+{\n+    private readonly IGenerator<T>? _llmGenerator;\n+    private readonly List<string> _conversationHistory;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the QueryRewritingProcessor class.\n+    /// </summary>\n+    /// <param name=\"llmGenerator\">Optional LLM generator for sophisticated query rewriting.</param>\n+    public QueryRewritingProcessor(IGenerator<T>? llmGenerator = null)\n+    {\n+        _llmGenerator = llmGenerator;\n+        _conversationHistory = new List<string>();\n+    }\n+\n+    /// <summary>\n+    /// Adds a query to the conversation history for context-aware rewriting.\n+    /// </summary>\n+    /// <param name=\"query\">The query to add to history.</param>\n+    public void AddToHistory(string query)\n+    {\n+        if (!string.IsNullOrWhiteSpace(query))\n+        {\n+            _conversationHistory.Add(query);\n+            \n+            if (_conversationHistory.Count > 5)\n+            {\n+                _conversationHistory.RemoveAt(0);\n+            }\n+        }\n+    }\n+\n+    protected override string ProcessQueryCore(string query)\n+    {\n+        if (string.IsNullOrWhiteSpace(query))\n+            return query;\n+\n+        var processedQuery = ApplyBasicRewrites(query);\n+\n+        if (_llmGenerator != null && _conversationHistory.Count > 0)\n+        {\n+            processedQuery = RewriteWithContext(processedQuery);\n+        }\n+\n+        AddToHistory(processedQuery);\n+        return processedQuery;\n+    }",
    "path": "src/RetrievalAugmentedGeneration/QueryProcessors/QueryRewritingProcessor.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "26162d2c7dd726f3ceef084ba4e7de4f637c64e8",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**LLM generator parameter is ignored.**\n\nLine 69 gates contextual rewriting on `_llmGenerator`, yet neither `ProcessQueryCore` nor `RewriteWithContext` invoke the generator. Hooking up an `IGenerator<T>` provides zero effect, so the advertised advanced rewrite path is dead code. Please wire `_llmGenerator` into the rewriting flow (e.g., pass the history and current query to the generator and use its output) so that supplying a generator actually alters the processed query.\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T14:03:51Z",
    "updated_at": "2025-11-05T14:03:52Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494655568",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494655568"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494655568"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494655568/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 67,
    "original_start_line": 67,
    "start_side": "RIGHT",
    "line": 74,
    "original_line": 74,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 74,
    "position": 74,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494655582",
    "pull_request_review_id": 3422265820,
    "id": 2494655582,
    "node_id": "PRRC_kwDOKSXUF86UsWxe",
    "diff_hunk": "@@ -0,0 +1,126 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.QueryProcessors;\n+\n+/// <summary>\n+/// Rewrites queries for clarity and completeness, especially in conversational contexts.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type for computations.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// This processor transforms conversational or context-dependent queries into standalone,\n+/// clear questions. It's particularly useful in multi-turn conversations where queries\n+/// reference previous context.\n+/// </para>\n+/// <para><b>For Beginners:</b> Makes incomplete questions complete by adding missing context.\n+/// \n+/// Conversational Examples:\n+/// - User: \"Tell me about transformers\"\n+/// - User: \"What about their applications?\" ΓåÆ \"What are the applications of transformers?\"\n+/// \n+/// Clarity Examples:\n+/// - \"how r cars made\" ΓåÆ \"how are cars manufactured\"\n+/// - \"wht is AI\" ΓåÆ \"what is artificial intelligence\"\n+/// \n+/// This makes your searches clearer and gets better results!\n+/// </para>\n+/// </remarks>\n+public class QueryRewritingProcessor<T> : QueryProcessorBase\n+{\n+    private readonly IGenerator<T>? _llmGenerator;\n+    private readonly List<string> _conversationHistory;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the QueryRewritingProcessor class.\n+    /// </summary>\n+    /// <param name=\"llmGenerator\">Optional LLM generator for sophisticated query rewriting.</param>\n+    public QueryRewritingProcessor(IGenerator<T>? llmGenerator = null)\n+    {\n+        _llmGenerator = llmGenerator;\n+        _conversationHistory = new List<string>();\n+    }\n+\n+    /// <summary>\n+    /// Adds a query to the conversation history for context-aware rewriting.\n+    /// </summary>\n+    /// <param name=\"query\">The query to add to history.</param>\n+    public void AddToHistory(string query)\n+    {\n+        if (!string.IsNullOrWhiteSpace(query))\n+        {\n+            _conversationHistory.Add(query);\n+            \n+            if (_conversationHistory.Count > 5)\n+            {\n+                _conversationHistory.RemoveAt(0);\n+            }\n+        }\n+    }\n+\n+    protected override string ProcessQueryCore(string query)\n+    {\n+        if (string.IsNullOrWhiteSpace(query))\n+            return query;\n+\n+        var processedQuery = ApplyBasicRewrites(query);\n+\n+        if (_llmGenerator != null && _conversationHistory.Count > 0)\n+        {\n+            processedQuery = RewriteWithContext(processedQuery);\n+        }\n+\n+        AddToHistory(processedQuery);\n+        return processedQuery;\n+    }\n+\n+    private string ApplyBasicRewrites(string query)\n+    {\n+        var rewritten = query;\n+\n+        rewritten = rewritten.Replace(\" r \", \" are \");\n+        rewritten = rewritten.Replace(\" u \", \" you \");\n+        rewritten = rewritten.Replace(\"wht \", \"what \");\n+        rewritten = rewritten.Replace(\"hw \", \"how \");\n+        rewritten = rewritten.Replace(\" w/ \", \" with \");\n+        rewritten = rewritten.Replace(\" w/o \", \" without \");\n+\n+        return rewritten.Trim();\n+    }\n+\n+    private string RewriteWithContext(string query)\n+    {\n+        var contextualKeywords = new[] { \"it\", \"they\", \"them\", \"their\", \"that\", \"this\", \"these\", \"those\", \"what about\", \"how about\" };\n+        \n+        var lowerQuery = query.ToLowerInvariant();\n+        var needsContext = contextualKeywords.Any(keyword => lowerQuery.Contains(keyword));\n+\n+        if (!needsContext || _conversationHistory.Count == 0)\n+            return query;\n+\n+        var lastContext = _conversationHistory[_conversationHistory.Count - 1];\n+        \n+        if (lowerQuery.StartsWith(\"what about\") || lowerQuery.StartsWith(\"how about\"))\n+        {\n+            var topic = ExtractTopic(lastContext);\n+            if (!string.IsNullOrEmpty(topic))\n+            {\n+                return query.Replace(\"what about\", $\"what about {topic} and\")\n+                           .Replace(\"how about\", $\"how about {topic} and\");\n+            }",
    "path": "src/RetrievalAugmentedGeneration/QueryProcessors/QueryRewritingProcessor.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "26162d2c7dd726f3ceef084ba4e7de4f637c64e8",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Context rewrite fails for capitalized queries.**\n\nLines 107-108 use case-sensitive `Replace(\"what about\", ΓÇª)` / `Replace(\"how about\", ΓÇª)`, but typical inputs start with ΓÇ£What about ΓÇªΓÇ¥ or ΓÇ£How about ΓÇªΓÇ¥. Those replacements never trigger, so contextual augmentation silently fails. Consider normalizing with the lowercased check but rebuild the output using slices, e.g.:\n\n```diff\n-            if (!string.IsNullOrEmpty(topic))\n-            {\n-                return query.Replace(\"what about\", $\"what about {topic} and\")\n-                           .Replace(\"how about\", $\"how about {topic} and\");\n-            }\n+            if (!string.IsNullOrEmpty(topic))\n+            {\n+                const string whatAbout = \"what about\";\n+                const string howAbout = \"how about\";\n+\n+                if (lowerQuery.StartsWith(whatAbout, StringComparison.Ordinal))\n+                {\n+                    var suffix = query[whatAbout.Length..].TrimStart();\n+                    return $\"{query[..whatAbout.Length]} {topic} and {suffix}\";\n+                }\n+\n+                if (lowerQuery.StartsWith(howAbout, StringComparison.Ordinal))\n+                {\n+                    var suffix = query[howAbout.Length..].TrimStart();\n+                    return $\"{query[..howAbout.Length]} {topic} and {suffix}\";\n+                }\n+            }\n```\nThis keeps the original casing while making the rewrite reliable.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/QueryProcessors/QueryRewritingProcessor.cs\naround lines 102 to 109, the code checks lowerQuery.StartsWith(\"what about\") /\n\"how about\" but then calls case-sensitive Replace on the original query, so\ncapitalized inputs like \"What about...\" aren't modified; instead, compute the\nmatched prefix length from the lowercased check, take the original query's\nprefix slice (to preserve casing) and the remainder slice, and return\noriginalPrefix + $\" {topic} and\" + remainder (or otherwise insert the topic\nbetween the detected prefix and the rest). Do this for both \"what about\" and\n\"how about\" branches so the rewrite is reliable while preserving original\ncasing.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T14:03:51Z",
    "updated_at": "2025-11-05T14:03:52Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494655582",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494655582"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494655582"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494655582/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 102,
    "original_start_line": 102,
    "start_side": "RIGHT",
    "line": 109,
    "original_line": 109,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 109,
    "position": 109,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494655593",
    "pull_request_review_id": 3422265820,
    "id": 2494655593,
    "node_id": "PRRC_kwDOKSXUF86UsWxp",
    "diff_hunk": "@@ -0,0 +1,108 @@\n+using System.Text.RegularExpressions;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.QueryProcessors;\n+\n+/// <summary>\n+/// Processes queries by correcting common spelling errors using a dictionary-based approach.\n+/// </summary>\n+/// <remarks>\n+/// <para>\n+/// This processor improves retrieval accuracy by fixing typos and misspellings before\n+/// the query is sent to the retriever. Uses a simple edit distance algorithm combined\n+/// with a frequency-based dictionary to suggest corrections.\n+/// </para>\n+/// <para><b>For Beginners:</b> This fixes spelling mistakes in your search queries.\n+/// \n+/// Examples:\n+/// - \"photsynthesis\" ΓåÆ \"photosynthesis\"\n+/// - \"artifical intelligence\" ΓåÆ \"artificial intelligence\"\n+/// - \"machin learning\" ΓåÆ \"machine learning\"\n+/// \n+/// It helps you find documents even when you make typos!\n+/// </para>\n+/// </remarks>\n+public class SpellCheckQueryProcessor : QueryProcessorBase\n+{\n+    private readonly Dictionary<string, string> _corrections;\n+    private readonly int _maxEditDistance;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the SpellCheckQueryProcessor class.\n+    /// </summary>\n+    /// <param name=\"customDictionary\">Optional custom dictionary of corrections (misspelling ΓåÆ correct spelling).</param>\n+    /// <param name=\"maxEditDistance\">Maximum edit distance for fuzzy matching (default: 2).</param>\n+    public SpellCheckQueryProcessor(\n+        Dictionary<string, string>? customDictionary = null,\n+        int maxEditDistance = 2)\n+    {\n+        _maxEditDistance = maxEditDistance;\n+        _corrections = customDictionary ?? GetDefaultDictionary();\n+    }",
    "path": "src/RetrievalAugmentedGeneration/QueryProcessors/SpellCheckQueryProcessor.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "26162d2c7dd726f3ceef084ba4e7de4f637c64e8",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Remove unused `maxEditDistance` parameter or implement fuzzy matching.**\n\nThe `maxEditDistance` field is stored but never used in the `ProcessQueryCore` method. The implementation only performs exact dictionary lookups (line 60), not fuzzy edit distance matching. This creates a misleading API where users might expect fuzzy spell correction based on the parameter name and documentation.\n\n\n\n**Option 1**: Remove the unused parameter:\n\n```diff\n public SpellCheckQueryProcessor(\n-        Dictionary<string, string>? customDictionary = null,\n-        int maxEditDistance = 2)\n+        Dictionary<string, string>? customDictionary = null)\n {\n-        _maxEditDistance = maxEditDistance;\n         _corrections = customDictionary ?? GetDefaultDictionary();\n }\n```\n\nAnd remove the field declaration:\n\n```diff\n     private readonly Dictionary<string, string> _corrections;\n-    private readonly int _maxEditDistance;\n```\n\n**Option 2**: Implement edit distance-based fuzzy matching if that functionality is intended.\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    private readonly Dictionary<string, string> _corrections;\n\n    /// <summary>\n    /// Initializes a new instance of the SpellCheckQueryProcessor class.\n    /// </summary>\n    /// <param name=\"customDictionary\">Optional custom dictionary of corrections (misspelling ΓåÆ correct spelling).</param>\n    /// <param name=\"maxEditDistance\">Maximum edit distance for fuzzy matching (default: 2).</param>\n    public SpellCheckQueryProcessor(\n        Dictionary<string, string>? customDictionary = null)\n    {\n        _corrections = customDictionary ?? GetDefaultDictionary();\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/QueryProcessors/SpellCheckQueryProcessor.cs\naround lines 26 to 40, the constructor stores a _maxEditDistance that is never\nused by ProcessQueryCore (only exact dictionary lookups are done), so either\nremove the misleading parameter/field and related XML docs or implement fuzzy\nmatching: if you choose removal, delete the maxEditDistance parameter, the\n_maxEditDistance field and update the XML doc and any callers; if you choose\nimplementation, add an edit-distance (e.g., Levenshtein) comparison in\nProcessQueryCore to consider dictionary entries within _maxEditDistance, prefer\nexact matches first, break ties deterministically (e.g., shortest distance then\nlexicographic), and ensure the constructor retains _maxEditDistance and the\ndocumentation reflects fuzzy matching.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T14:03:51Z",
    "updated_at": "2025-11-05T14:03:52Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494655593",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494655593"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494655593"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494655593/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 26,
    "original_start_line": 26,
    "start_side": "RIGHT",
    "line": 40,
    "original_line": 40,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 40,
    "position": 40,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494655614",
    "pull_request_review_id": 3422265820,
    "id": 2494655614,
    "node_id": "PRRC_kwDOKSXUF86UsWx-",
    "diff_hunk": "@@ -0,0 +1,108 @@\n+using System.Text.RegularExpressions;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.QueryProcessors;\n+\n+/// <summary>\n+/// Processes queries by correcting common spelling errors using a dictionary-based approach.\n+/// </summary>\n+/// <remarks>\n+/// <para>\n+/// This processor improves retrieval accuracy by fixing typos and misspellings before\n+/// the query is sent to the retriever. Uses a simple edit distance algorithm combined\n+/// with a frequency-based dictionary to suggest corrections.\n+/// </para>\n+/// <para><b>For Beginners:</b> This fixes spelling mistakes in your search queries.\n+/// \n+/// Examples:\n+/// - \"photsynthesis\" ΓåÆ \"photosynthesis\"\n+/// - \"artifical intelligence\" ΓåÆ \"artificial intelligence\"\n+/// - \"machin learning\" ΓåÆ \"machine learning\"\n+/// \n+/// It helps you find documents even when you make typos!\n+/// </para>\n+/// </remarks>\n+public class SpellCheckQueryProcessor : QueryProcessorBase\n+{\n+    private readonly Dictionary<string, string> _corrections;\n+    private readonly int _maxEditDistance;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the SpellCheckQueryProcessor class.\n+    /// </summary>\n+    /// <param name=\"customDictionary\">Optional custom dictionary of corrections (misspelling ΓåÆ correct spelling).</param>\n+    /// <param name=\"maxEditDistance\">Maximum edit distance for fuzzy matching (default: 2).</param>\n+    public SpellCheckQueryProcessor(\n+        Dictionary<string, string>? customDictionary = null,\n+        int maxEditDistance = 2)\n+    {\n+        _maxEditDistance = maxEditDistance;\n+        _corrections = customDictionary ?? GetDefaultDictionary();\n+    }\n+\n+    protected override string ProcessQueryCore(string query)\n+    {\n+        if (string.IsNullOrWhiteSpace(query))\n+            return query;\n+\n+        var words = Regex.Split(query, @\"(\\s+)\");\n+        var correctedWords = new List<string>();\n+\n+        foreach (var word in words)\n+        {\n+            if (string.IsNullOrWhiteSpace(word))\n+            {\n+                correctedWords.Add(word);\n+                continue;\n+            }\n+\n+            var lowerWord = word.ToLowerInvariant();\n+            \n+            if (_corrections.TryGetValue(lowerWord, out var correction))\n+            {\n+                correctedWords.Add(PreserveCase(word, correction));\n+            }\n+            else\n+            {\n+                correctedWords.Add(word);\n+            }\n+        }\n+\n+        return string.Join(\"\", correctedWords);\n+    }\n+\n+    private static string PreserveCase(string original, string corrected)\n+    {\n+        if (string.IsNullOrEmpty(original) || string.IsNullOrEmpty(corrected))\n+            return corrected;\n+\n+        if (char.IsUpper(original[0]))\n+        {\n+            return char.ToUpper(corrected[0]) + corrected.Substring(1);\n+        }\n+\n+        return corrected;\n+    }\n+\n+    private static Dictionary<string, string> GetDefaultDictionary()\n+    {\n+        return new Dictionary<string, string>\n+        {\n+            { \"photsynthesis\", \"photosynthesis\" },\n+            { \"artifical\", \"artificial\" },\n+            { \"intelligance\", \"intelligence\" },\n+            { \"machin\", \"machine\" },\n+            { \"lerning\", \"learning\" },\n+            { \"nueral\", \"neural\" },\n+            { \"netowrk\", \"network\" },\n+            { \"algoritm\", \"algorithm\" },\n+            { \"optmization\", \"optimization\" },\n+            { \"retreival\", \"retrieval\" },\n+            { \"retreive\", \"retrieve\" },\n+            { \"genration\", \"generation\" },\n+            { \"embeddin\", \"embedding\" },\n+            { \"similrity\", \"similarity\" },\n+            { \"relevent\", \"relevant\" },\n+            { \"rerank\", \"rerank\" }\n+        };\n+    }",
    "path": "src/RetrievalAugmentedGeneration/QueryProcessors/SpellCheckQueryProcessor.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "26162d2c7dd726f3ceef084ba4e7de4f637c64e8",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Remove self-mapping entry in default dictionary.**\n\nLine 105 maps `\"rerank\"` to `\"rerank\"`, which serves no purpose in a spell correction dictionary. This entry should be removed.\n\n\n\nApply this diff:\n\n```diff\n             { \"similrity\", \"similarity\" },\n             { \"relevent\", \"relevant\" },\n-            { \"rerank\", \"rerank\" }\n         };\n     }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    private static Dictionary<string, string> GetDefaultDictionary()\n    {\n        return new Dictionary<string, string>\n        {\n            { \"photsynthesis\", \"photosynthesis\" },\n            { \"artifical\", \"artificial\" },\n            { \"intelligance\", \"intelligence\" },\n            { \"machin\", \"machine\" },\n            { \"lerning\", \"learning\" },\n            { \"nueral\", \"neural\" },\n            { \"netowrk\", \"network\" },\n            { \"algoritm\", \"algorithm\" },\n            { \"optmization\", \"optimization\" },\n            { \"retreival\", \"retrieval\" },\n            { \"retreive\", \"retrieve\" },\n            { \"genration\", \"generation\" },\n            { \"embeddin\", \"embedding\" },\n            { \"similrity\", \"similarity\" },\n            { \"relevent\", \"relevant\" }\n        };\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/QueryProcessors/SpellCheckQueryProcessor.cs\naround lines 86 to 107, remove the pointless self-mapping entry { \"rerank\",\n\"rerank\" } from the returned dictionary so the default spell-correction\ndictionary contains only genuine misspell->correct pairs; update the initializer\nto exclude that line and ensure trailing commas/formatting remain valid.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T14:03:51Z",
    "updated_at": "2025-11-05T14:03:52Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494655614",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494655614"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494655614"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494655614/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 86,
    "original_start_line": 86,
    "start_side": "RIGHT",
    "line": 107,
    "original_line": 107,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 107,
    "position": 107,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494728117",
    "pull_request_review_id": 3422366883,
    "id": 2494728117,
    "node_id": "PRRC_kwDOKSXUF86Usoe1",
    "diff_hunk": "@@ -351,4 +351,34 @@ public interface IPredictionModelBuilder<T, TInput, TOutput>\n     /// <param name=\"loraConfiguration\">The LoRA configuration implementation to use.</param>\n     /// <returns>The builder instance for method chaining.</returns>\n     IPredictionModelBuilder<T, TInput, TOutput> ConfigureLoRA(ILoRAConfiguration<T> loraConfiguration);\n+\n+    /// <summary>\n+    /// Configures the retrieval-augmented generation (RAG) components for use during model inference.\n+    /// </summary>\n+    /// <remarks>\n+    /// RAG enhances text generation by retrieving relevant documents from a knowledge base\n+    /// and using them as context for generating grounded, factual answers.\n+    ///\n+    /// <b>For Beginners:</b> RAG is like giving your AI access to a library before answering questions.\n+    /// Instead of relying only on what it learned during training, it can:\n+    /// 1. Search a document collection for relevant information\n+    /// 2. Read the relevant documents\n+    /// 3. Generate an answer based on those documents\n+    /// 4. Cite its sources\n+    ///\n+    /// This makes answers more accurate, up-to-date, and traceable to source materials.\n+    /// \n+    /// RAG operations (GenerateAnswer, RetrieveDocuments) are performed during inference via PredictionModelResult,\n+    /// not during model building.\n+    /// </remarks>\n+    /// <param name=\"retriever\">Optional retriever for finding relevant documents. If not provided, RAG won't be available.</param>\n+    /// <param name=\"reranker\">Optional reranker for improving document ranking quality. Default provided if retriever is set.</param>\n+    /// <param name=\"generator\">Optional generator for producing grounded answers. Default provided if retriever is set.</param>\n+    /// <param name=\"queryProcessors\">Optional query processors for improving search quality.</param>\n+    /// <returns>The builder instance for method chaining.</returns>\n+    IPredictionModelBuilder<T, TInput, TOutput> ConfigureRetrievalAugmentedGeneration(\n+        IRetriever<T>? retriever = null,\n+        IReranker<T>? reranker = null,\n+        IGenerator<T>? generator = null,\n+        IEnumerable<AiDotNet.RetrievalAugmentedGeneration.Interfaces.IQueryProcessor>? queryProcessors = null);",
    "path": "src/Interfaces/IPredictionModelBuilder.cs",
    "commit_id": "aabf17d25c83e742ce5af00518440c5da6742902",
    "original_commit_id": "aabf17d25c83e742ce5af00518440c5da6742902",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**RAG optional components currently break at runtime**\n\nAllowing `ConfigureRetrievalAugmentedGeneration` to accept `null` for `reranker`/`generator` (and the docs claiming defaults will appear) is misleadingΓÇöthe implementation just stores the values as-is, so `Build(...)` returns a `PredictionModelResult` with `RagReranker`/`RagGenerator == null`. The first call to `PredictionModelResult.GenerateAnswer(...)` then throws `InvalidOperationException` (ΓÇ£RAG pipeline not configuredΓÇ¥). In other words, a perfectly legal invocation like `ConfigureRetrievalAugmentedGeneration(retriever: myRetriever)` yields a runtime failure later. Please either require non-null reranker/generator up front or actually instantiate defaults so the API contract holds.\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T14:21:14Z",
    "updated_at": "2025-11-05T14:21:16Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494728117",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494728117"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494728117"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494728117/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 355,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 383,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 33,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494728134",
    "pull_request_review_id": 3422366883,
    "id": 2494728134,
    "node_id": "PRRC_kwDOKSXUF86UsofG",
    "diff_hunk": "@@ -382,4 +394,35 @@ public IPredictionModelBuilder<T, TInput, TOutput> ConfigureLoRA(ILoRAConfigurat\n         _loraConfiguration = loraConfiguration;\n         return this;\n     }\n-}\n\\ No newline at end of file\n+\n+    /// <summary>\n+    /// Configures the retrieval-augmented generation (RAG) components for use during model inference.\n+    /// </summary>\n+    /// <param name=\"retriever\">Optional retriever for finding relevant documents. If not provided, RAG functionality won't be available.</param>\n+    /// <param name=\"reranker\">Optional reranker for improving document ranking quality. If not provided, a default reranker will be used if RAG is configured.</param>\n+    /// <param name=\"generator\">Optional generator for producing grounded answers. If not provided, a default generator will be used if RAG is configured.</param>\n+    /// <param name=\"queryProcessors\">Optional query processors for improving search quality.</param>\n+    /// <returns>This builder instance for method chaining.</returns>\n+    /// <remarks>\n+    /// <b>For Beginners:</b> RAG combines retrieval and generation to create answers backed by real documents.\n+    /// Configure it with:\n+    /// - A retriever (finds relevant documents from your collection) - required for RAG\n+    /// - A reranker (improves the ordering of retrieved documents) - optional, defaults provided\n+    /// - A generator (creates answers based on the documents) - optional, defaults provided\n+    /// - Optional query processors (improve search queries before retrieval)\n+    /// \n+    /// RAG operations are performed during inference (after model training) via the PredictionModelResult.\n+    /// </remarks>\n+    public IPredictionModelBuilder<T, TInput, TOutput> ConfigureRetrievalAugmentedGeneration(\n+        IRetriever<T>? retriever = null,\n+        IReranker<T>? reranker = null,\n+        IGenerator<T>? generator = null,\n+        IEnumerable<AiDotNet.RetrievalAugmentedGeneration.Interfaces.IQueryProcessor>? queryProcessors = null)\n+    {\n+        _ragRetriever = retriever;\n+        _ragReranker = reranker;\n+        _ragGenerator = generator;\n+        _queryProcessors = queryProcessors;\n+        return this;\n+    }",
    "path": "src/PredictionModelBuilder.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "aabf17d25c83e742ce5af00518440c5da6742902",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Guard against partially configured RAG pipelines**\n\nRight now weΓÇÖll happily persist `_ragRetriever` while leaving `_ragReranker`/`_ragGenerator` null, which guarantees `PredictionModelResult.GenerateAnswer(...)` will explode when someone actually uses the pipeline. Please fail fast here: if a retriever is supplied, require the reranker and generator (or initialize known defaults). For example:\n\n```diff\n     public IPredictionModelBuilder<T, TInput, TOutput> ConfigureRetrievalAugmentedGeneration(\n         IRetriever<T>? retriever = null,\n         IReranker<T>? reranker = null,\n         IGenerator<T>? generator = null,\n         IEnumerable<IQueryProcessor>? queryProcessors = null)\n     {\n+        if (retriever != null && (reranker == null || generator == null))\n+            throw new ArgumentException(\"RAG retriever requires both reranker and generator to be provided.\", nameof(retriever));\n+        if (retriever == null && (reranker != null || generator != null))\n+            throw new ArgumentException(\"RAG reranker/generator cannot be set without a retriever.\");\n+\n         _ragRetriever = retriever;\n         _ragReranker = reranker;\n         _ragGenerator = generator;\n         _queryProcessors = queryProcessors?.ToList();\n         return this;\n     }\n```\n\n(Adjust exception text or default wiring as needed.) This keeps the builder from handing out unusable models.\n\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T14:21:14Z",
    "updated_at": "2025-11-05T14:21:16Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494728134",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494728134"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494728134"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494728134/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 417,
    "original_start_line": 417,
    "start_side": "RIGHT",
    "line": 427,
    "original_line": 427,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 64,
    "position": 64,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494762483",
    "pull_request_review_id": 3422411998,
    "id": 2494762483,
    "node_id": "PRRC_kwDOKSXUF86Usw3z",
    "diff_hunk": "@@ -809,4 +854,124 @@ private static ModelMetadata<T> ExtractMetadataFromSerializedData(byte[] data)\n         var deserializedObject = JsonConvert.DeserializeObject<PredictionModelResult<T, TInput, TOutput>>(jsonString, settings);\n         return deserializedObject?.ModelMetaData ?? new();\n     }\n+\n+    /// <summary>\n+    /// Generates a grounded answer using the configured RAG pipeline during inference.\n+    /// </summary>\n+    /// <param name=\"query\">The question to answer.</param>\n+    /// <param name=\"topK\">Number of documents to retrieve (optional).</param>\n+    /// <param name=\"topKAfterRerank\">Number of documents after reranking (optional).</param>\n+    /// <param name=\"metadataFilters\">Optional filters for document selection.</param>\n+    /// <returns>A grounded answer with source citations.</returns>\n+    /// <exception cref=\"InvalidOperationException\">Thrown when RAG components are not configured.</exception>\n+    /// <remarks>\n+    /// <b>For Beginners:</b> Use this during inference to get AI-generated answers backed by your documents.\n+    /// The system will search your document collection, find the most relevant sources,\n+    /// and generate an answer with citations.\n+    /// \n+    /// RAG must be configured via PredictionModelBuilder.ConfigureRetrievalAugmentedGeneration() before building the model.\n+    /// </remarks>\n+    public AiDotNet.RetrievalAugmentedGeneration.Models.GroundedAnswer<T> GenerateAnswer(\n+        string query,\n+        int? topK = null,\n+        int? topKAfterRerank = null,\n+        Dictionary<string, object>? metadataFilters = null)\n+    {\n+        if (RagRetriever == null || RagReranker == null || RagGenerator == null)\n+        {\n+            throw new InvalidOperationException(\n+                \"RAG pipeline not configured. Configure RAG components using PredictionModelBuilder.ConfigureRetrievalAugmentedGeneration() before building the model.\");\n+        }\n+\n+        if (string.IsNullOrWhiteSpace(query))\n+            throw new ArgumentException(\"Query cannot be null or empty\", nameof(query));\n+\n+        var processedQuery = query;\n+        if (QueryProcessors != null)\n+        {\n+            foreach (var processor in QueryProcessors)\n+            {\n+                processedQuery = processor.ProcessQuery(processedQuery);\n+            }\n+        }\n+\n+        var filters = metadataFilters ?? new Dictionary<string, object>();\n+        var effectiveTopK = topK ?? RagRetriever.DefaultTopK;\n+        var retrievedDocs = RagRetriever.Retrieve(processedQuery, effectiveTopK, filters);\n+\n+        var retrievedList = retrievedDocs.ToList();\n+\n+        if (retrievedList.Count == 0)\n+        {\n+            return new AiDotNet.RetrievalAugmentedGeneration.Models.GroundedAnswer<T>\n+            {\n+                Query = query,\n+                Answer = \"I couldn't find any relevant information to answer this question.\",\n+                SourceDocuments = new List<AiDotNet.RetrievalAugmentedGeneration.Models.Document<T>>(),\n+                Citations = new List<string>(),\n+                ConfidenceScore = 0.0\n+            };\n+        }\n+\n+        var rerankedDocs = RagReranker.Rerank(processedQuery, retrievedList);\n+        \n+        if (topKAfterRerank.HasValue)\n+        {\n+            rerankedDocs = rerankedDocs.Take(topKAfterRerank.Value);\n+        }\n+\n+        var contextDocs = rerankedDocs.ToList();\n+        return RagGenerator.GenerateGrounded(processedQuery, contextDocs);\n+    }",
    "path": "src/Models/Results/PredictionModelResult.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "e1a421dadc8f1b7ec0ea4d028e993e8d16d093e9",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_≡ƒ¢á∩╕Å Refactor suggestion_ | _≡ƒƒá Major_\n\n**Query processing logic is duplicated.**\n\nThe query processing loop (lines 889-896) is identical to the one in `RetrieveDocuments` (lines 957-964). Consider extracting this into a private helper method to eliminate duplication:\n\n\n\n```csharp\nprivate string ProcessQueryWithProcessors(string query)\n{\n    if (QueryProcessors == null)\n        return query;\n    \n    var processedQuery = query;\n    foreach (var processor in QueryProcessors)\n    {\n        processedQuery = processor.ProcessQuery(processedQuery);\n    }\n    return processedQuery;\n}\n```\n\nThen use it in both methods:\n```csharp\nvar processedQuery = ProcessQueryWithProcessors(query);\n```\n\nThis also makes the query processing pipeline easier to test and maintain.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/Models/Results/PredictionModelResult.cs around lines 874 to 925, the\nquery processing loop is duplicated; extract it into a private helper (e.g.,\nProcessQueryWithProcessors) that returns the original query if QueryProcessors\nis null and otherwise runs each processor in order, then replace the inline\nloops in GenerateAnswer and RetrieveDocuments to call this helper so query\nprocessing is centralized and testable.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T14:28:05Z",
    "updated_at": "2025-11-05T14:28:06Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494762483",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494762483"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494762483"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494762483/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 874,
    "original_start_line": 874,
    "start_side": "RIGHT",
    "line": 925,
    "original_line": 925,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 181,
    "position": 181,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494886964",
    "pull_request_review_id": 3422589426,
    "id": 2494886964,
    "node_id": "PRRC_kwDOKSXUF86UtPQ0",
    "diff_hunk": "@@ -0,0 +1,221 @@\n+using AiDotNet.NumericOperations;\n+using AiDotNet.RetrievalAugmentedGeneration.Generators;\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using AiDotNet.RetrievalAugmentedGeneration.Retrievers;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.AdvancedPatterns;\n+\n+/// <summary>\n+/// Chain-of-Thought retriever that generates reasoning steps before retrieving documents.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// This advanced retrieval pattern uses large language models to break down complex queries\n+/// into intermediate reasoning steps before retrieving documents. By generating a chain of\n+/// thought, the retriever can identify key concepts, sub-questions, and the logical order\n+/// in which information should be gathered, leading to more comprehensive and relevant results.\n+/// </para>\n+/// <para><b>For Beginners:</b> Think of this like asking a research assistant to explain their thought process.\n+/// \n+/// Normal retriever:\n+/// - Question: \"How does photosynthesis impact climate change?\"\n+/// - Action: Search for documents about \"photosynthesis\" and \"climate change\"\n+/// \n+/// Chain-of-Thought retriever:\n+/// - Question: \"How does photosynthesis impact climate change?\"\n+/// - Reasoning: \"First, I need to understand what photosynthesis is. Then, I need to know how it\n+///   relates to carbon dioxide. Finally, I need to connect CO2 to climate change.\"\n+/// - Actions: \n+///   1. Search for \"what is photosynthesis\"\n+///   2. Search for \"photosynthesis carbon dioxide absorption\"\n+///   3. Search for \"CO2 levels and climate change\"\n+/// - Result: More complete answer because we gathered all prerequisite knowledge\n+/// \n+/// This is especially useful for complex questions that require understanding multiple concepts\n+/// in a specific order.\n+/// </para>\n+/// <para><b>Example Usage:</b>\n+/// <code>\n+/// // Create generator (StubGenerator for testing, or real LLM for production)\n+/// var generator = new StubGenerator&lt;double&gt;();\n+/// \n+/// // Create base retriever\n+/// var baseRetriever = new DenseRetriever&lt;double&gt;(embeddingModel, documentStore);\n+/// \n+/// // Create chain-of-thought retriever\n+/// var cotRetriever = new ChainOfThoughtRetriever&lt;double&gt;(generator, baseRetriever);\n+/// \n+/// // Retrieve with reasoning\n+/// var documents = cotRetriever.Retrieve(\n+///     \"What are the economic impacts of renewable energy adoption?\",\n+///     topK: 10\n+/// );\n+/// \n+/// // The retriever will:\n+/// // 1. Generate reasoning steps (costs, benefits, job creation, etc.)\n+/// // 2. Retrieve documents for each reasoning step\n+/// // 3. Deduplicate and return top-10 most relevant documents\n+/// </code>\n+/// </para>\n+/// <para><b>Production Readiness:</b>\n+/// Current implementation uses IGenerator interface which can accept:\n+/// - StubGenerator for development/testing\n+/// - Real LLM (GPT-4, Claude, Gemini) for production\n+/// \n+/// To make production-ready:\n+/// 1. Replace StubGenerator with real LLM generator\n+/// 2. Optionally tune the reasoning prompt for your domain\n+/// 3. Adjust max sub-queries limit based on LLM costs\n+/// 4. Consider caching reasoning for common queries\n+/// </para>\n+/// <para><b>Benefits:</b>\n+/// - More comprehensive results for complex queries\n+/// - Better coverage of prerequisite knowledge\n+/// - Improved relevance through structured reasoning\n+/// - Transparent reasoning process for debugging\n+/// </para>\n+/// <para><b>Limitations:</b>\n+/// - Requires LLM access (costs/latency)\n+/// - Quality depends on LLM reasoning ability\n+/// - May retrieve redundant documents if reasoning overlaps\n+/// - Slower than direct retrieval\n+/// </para>\n+/// </remarks>\n+public class ChainOfThoughtRetriever<T>\n+{\n+    private readonly IGenerator<T> _generator;\n+    private readonly RetrieverBase<T> _baseRetriever;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"ChainOfThoughtRetriever{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"generator\">The LLM generator for reasoning (use StubGenerator or real LLM).</param>\n+    /// <param name=\"baseRetriever\">The underlying retriever to use.</param>\n+    public ChainOfThoughtRetriever(\n+        IGenerator<T> generator,\n+        RetrieverBase<T> baseRetriever)\n+    {\n+        _generator = generator ?? throw new ArgumentNullException(nameof(generator));\n+        _baseRetriever = baseRetriever ?? throw new ArgumentNullException(nameof(baseRetriever));\n+    }\n+\n+    /// <summary>\n+    /// Retrieves documents using chain-of-thought reasoning.\n+    /// </summary>\n+    /// <param name=\"query\">The user's query that requires reasoning.</param>\n+    /// <param name=\"topK\">Maximum number of documents to return.</param>\n+    /// <returns>Collection of documents ranked by relevance, gathered through reasoning steps.</returns>\n+    /// <exception cref=\"ArgumentException\">Thrown when query is null or whitespace.</exception>\n+    /// <exception cref=\"ArgumentOutOfRangeException\">Thrown when topK is not positive.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// This method generates intermediate reasoning steps using the LLM, extracts sub-queries\n+    /// from those steps, retrieves documents for each sub-query, deduplicates results, and\n+    /// returns the top-K most relevant documents.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is the main method you call to get documents.\n+    /// \n+    /// The process:\n+    /// 1. Send your question to an LLM to break it down into reasoning steps\n+    /// 2. Extract sub-questions from those steps\n+    /// 3. Retrieve documents for each sub-question (limit: 5 docs per sub-question)\n+    /// 4. Remove duplicates\n+    /// 5. Sort by relevance and return top-K documents\n+    /// \n+    /// Example:\n+    /// - Query: \"How do vaccines work and why are they important?\"\n+    /// - Reasoning: \"1. Understand immune system basics, 2. Explain vaccine mechanism, \n+    ///   3. Discuss disease prevention benefits\"\n+    /// - Sub-queries: [\"immune system\", \"how vaccines work\", \"vaccine benefits\"]\n+    /// - Retrieves: 5 docs about immune system + 5 about vaccines + 5 about benefits\n+    /// - Returns: Top-10 unique documents (topK=10)\n+    /// </para>\n+    /// </remarks>\n+    public IEnumerable<Document<T>> Retrieve(string query, int topK)\n+    {\n+        if (string.IsNullOrWhiteSpace(query))\n+            throw new ArgumentException(\"Query cannot be null or whitespace\", nameof(query));\n+\n+        if (topK <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(topK), \"topK must be positive\");\n+\n+        // Step 1: Generate reasoning steps using LLM\n+        var reasoningPrompt = $@\"Given the question: '{query}'\n+\n+Please break this question into a chain of thought reasoning steps:\n+1. What are the key concepts to understand?\n+2. What sub-questions need to be answered?\n+3. In what order should information be gathered?\n+\n+Provide numbered reasoning steps.\";\n+\n+        var reasoningResponse = _generator.Generate(reasoningPrompt);\n+\n+        // Step 2: Extract key concepts and sub-questions from reasoning\n+        var subQueries = ExtractSubQueries(reasoningResponse, query);\n+\n+        // Step 3: Retrieve documents for each sub-query\n+        var allDocuments = new List<Document<T>>();\n+        var seenIds = new HashSet<string>();\n+\n+        foreach (var subQuery in subQueries.Take(3)) // Limit to top 3 sub-queries\n+        {\n+            var docs = _baseRetriever.Retrieve(subQuery, topK: 5); // Get 5 per sub-query\n+            \n+            foreach (var doc in docs)\n+            {\n+                if (!seenIds.Contains(doc.Id))\n+                {\n+                    allDocuments.Add(doc);\n+                    seenIds.Add(doc.Id);\n+                }\n+            }\n+        }\n+\n+        // Step 4: Return top-K documents sorted by relevance\n+        return allDocuments\n+            .OrderByDescending(d => d.HasRelevanceScore ? d.RelevanceScore : default(T))\n+            .Take(topK);\n+    }",
    "path": "src/RetrievalAugmentedGeneration/AdvancedPatterns/ChainOfThoughtRetriever.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "aaf7a4a8027b8bb88ec10c6b3b8f1d9ba7d9e40e",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Respect the callerΓÇÖs metadata filters**\n\nThis overload drops metadata filters entirely. In multi-tenant deployments the base retriever enforces scoping via those filters; skipping them lets this wrapper fetch documents belonging to other tenants. Add a `Dictionary<string, object>? metadataFilters` parameter (defaulting to an empty dictionary) and forward it to every `_baseRetriever.Retrieve` call inside the loop.\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T14:50:56Z",
    "updated_at": "2025-11-05T14:51:03Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494886964",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494886964"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494886964"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494886964/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 139,
    "original_start_line": 139,
    "start_side": "RIGHT",
    "line": 184,
    "original_line": 184,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 184,
    "position": 184,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494886991",
    "pull_request_review_id": 3422589426,
    "id": 2494886991,
    "node_id": "PRRC_kwDOKSXUF86UtPRP",
    "diff_hunk": "@@ -0,0 +1,289 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Generators;\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using AiDotNet.RetrievalAugmentedGeneration.Retrievers;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Text;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.AdvancedPatterns;\n+\n+/// <summary>\n+/// FLARE (Forward-Looking Active REtrieval) pattern that actively decides when and what to retrieve during generation.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// FLARE (Forward-Looking Active REtrieval augmented generation) is an advanced RAG pattern that monitors\n+/// the language model's confidence during generation. When uncertainty is detected (low confidence on\n+/// next tokens), FLARE automatically retrieves additional relevant information to improve answer quality.\n+/// This creates a dynamic retrieval loop where retrieval happens only when needed, rather than all upfront.\n+/// </para>\n+/// <para><b>For Beginners:</b> Think of FLARE like asking follow-up questions when you're unsure.\n+/// \n+/// Normal RAG:\n+/// - Question: \"What is quantum computing?\"\n+/// - Step 1: Retrieve all documents about quantum computing\n+/// - Step 2: Generate complete answer from those documents\n+/// - Problem: Might miss specific details or retrieve too much irrelevant info\n+/// \n+/// FLARE:\n+/// - Question: \"What is quantum computing?\"\n+/// - Step 1: Start generating answer...\n+/// - Step 2: \"Quantum computing uses quantum bits or...\" (confident, keep going)\n+/// - Step 3: \"...which leverage principles like...\" (uncertain - what principles exactly?)\n+/// - Step 4: RETRIEVE more docs about \"quantum principles superposition entanglement\"\n+/// - Step 5: Continue with new information: \"...superposition and entanglement...\"\n+/// - Result: More focused retrieval, better coverage of uncertainty areas\n+/// \n+/// It's like having a conversation where you ask for clarification only when you need it,\n+/// rather than reading an entire encyclopedia upfront.\n+/// </para>\n+/// <para><b>Example Usage:</b>\n+/// <code>\n+/// // Setup\n+/// var generator = new StubGenerator&lt;double&gt;(); // Or real LLM\n+/// var retriever = new DenseRetriever&lt;double&gt;(embeddingModel, documentStore);\n+/// \n+/// // Create FLARE retriever with uncertainty threshold\n+/// var flare = new FLARERetriever&lt;double&gt;(\n+///     generator,\n+///     retriever,\n+///     uncertaintyThreshold: 0.5  // Retrieve when confidence drops below 50%\n+/// );\n+/// \n+/// // Generate answer with active retrieval\n+/// var answer = flare.GenerateWithActiveRetrieval(\n+///     \"Explain how CRISPR gene editing works and its applications\"\n+/// );\n+/// \n+/// // FLARE will:\n+/// // 1. Start generating about CRISPR\n+/// // 2. Detect uncertainty about specific mechanisms\n+/// // 3. Retrieve more docs about \"CRISPR Cas9 mechanism\"\n+/// // 4. Continue generating with new info\n+/// // 5. Detect uncertainty about applications\n+/// // 6. Retrieve docs about \"CRISPR medical applications\"\n+/// // 7. Complete answer with all retrieved knowledge\n+/// </code>\n+/// </para>\n+/// <para><b>How It Works:</b>\n+/// The retrieval process is:\n+/// 1. Initial retrieval - Get top-3 relevant documents\n+/// 2. Start generating answer with initial context\n+/// 3. Monitor confidence - Check for uncertainty signals (keywords like \"I'm not sure\", \"unclear\")\n+/// 4. Active retrieval - When uncertain, extract missing topics and retrieve more docs\n+/// 5. Integrate new information - Continue generating with expanded context\n+/// 6. Repeat - Maximum 5 iterations to prevent infinite loops\n+/// 7. Return complete answer assembled from all iterations\n+/// \n+/// Current implementation uses keyword detection for uncertainty. Production version would use:\n+/// - Token-level confidence scores (logprobs from LLM)\n+/// - Attention weights to identify knowledge gaps\n+/// - Explicit uncertainty statements from the model\n+/// </para>\n+/// <para><b>Benefits:</b>\n+/// - More efficient retrieval - Only fetches what's needed\n+/// - Better coverage - Addresses uncertainty areas specifically\n+/// - Reduced noise - Avoids retrieving irrelevant documents upfront\n+/// - Adaptive - Responds to complexity of the question dynamically\n+/// - Cost-effective - Fewer total documents retrieved vs exhaustive upfront retrieval\n+/// </para>\n+/// <para><b>Limitations:</b>\n+/// - Requires LLM with confidence scores (logprobs) for best results\n+/// - Multiple LLM calls increase latency\n+/// - May miss information if uncertainty detection fails\n+/// - Current implementation uses simple keyword matching (needs improvement with real LLM logprobs)\n+/// </para>\n+/// </remarks>\n+public class FLARERetriever<T>\n+{\n+    private static readonly INumericOperations<T> NumOps = MathHelper.GetNumericOperations<T>();\n+    private readonly IGenerator<T> _generator;\n+    private readonly RetrieverBase<T> _baseRetriever;\n+    private readonly double _uncertaintyThreshold;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"FLARERetriever{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"generator\">The LLM generator (use StubGenerator or real LLM).</param>\n+    /// <param name=\"baseRetriever\">The underlying retriever to use.</param>\n+    /// <param name=\"uncertaintyThreshold\">Threshold for triggering retrieval (0.0-1.0, default 0.5).</param>\n+    public FLARERetriever(\n+        IGenerator<T> generator,\n+        RetrieverBase<T> baseRetriever,\n+        double uncertaintyThreshold = 0.5)\n+    {\n+        _generator = generator ?? throw new ArgumentNullException(nameof(generator));\n+        _baseRetriever = baseRetriever ?? throw new ArgumentNullException(nameof(baseRetriever));\n+        \n+        if (uncertaintyThreshold < 0.0 || uncertaintyThreshold > 1.0)\n+            throw new ArgumentOutOfRangeException(nameof(uncertaintyThreshold), \"Threshold must be between 0 and 1\");\n+            \n+        _uncertaintyThreshold = uncertaintyThreshold;\n+    }\n+\n+    /// <summary>\n+    /// Generates an answer with active retrieval triggered by detected uncertainty.\n+    /// </summary>\n+    /// <param name=\"query\">The user's question to answer.</param>\n+    /// <returns>Complete answer generated iteratively with active retrieval when needed.</returns>\n+    /// <exception cref=\"ArgumentException\">Thrown when query is null or whitespace.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// This method implements the FLARE algorithm: it generates an answer incrementally,\n+    /// monitoring for signs of uncertainty at each step. When uncertainty exceeds the threshold,\n+    /// it retrieves additional relevant documents and continues generation with the new context.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is the main method that implements \"generate with retrieval on demand\".\n+    /// \n+    /// Process:\n+    /// 1. Initial Setup: Retrieve top-3 documents for the query\n+    /// 2. Generate Partial Answer: Create answer segment with current context\n+    /// 3. Check Confidence: Look for uncertainty signals (\"I'm not sure\", \"unclear\", etc.)\n+    /// 4. If Uncertain:\n+    ///    - Extract what information is missing\n+    ///    - Retrieve documents about that specific topic\n+    ///    - Add to context\n+    /// 5. Repeat: Up to 5 times or until confident\n+    /// 6. Return: Complete answer assembled from all iterations\n+    /// \n+    /// Example Flow:\n+    /// - Query: \"How does photosynthesis work?\"\n+    /// - Iteration 1: \"Photosynthesis is...\" (confident)\n+    /// - Iteration 2: \"...but the exact chemical process of...\" (UNCERTAIN!)\n+    /// - Retrieval: Get docs about \"photosynthesis chemical reactions\"\n+    /// - Iteration 3: \"...the light-dependent reactions convert...\" (confident with new info)\n+    /// - Done: Return complete answer\n+    /// </para>\n+    /// </remarks>\n+    public string GenerateWithActiveRetrieval(string query)\n+    {\n+        if (string.IsNullOrWhiteSpace(query))\n+            throw new ArgumentException(\"Query cannot be null or whitespace\", nameof(query));\n+\n+        var answer = new StringBuilder();\n+        var currentQuery = query;\n+        var maxIterations = 5; // Prevent infinite loops\n+        var iteration = 0;\n+\n+        // Initial retrieval\n+        var initialDocs = _baseRetriever.Retrieve(query, topK: 3).ToList();\n+        var allRetrievedDocs = new List<Document<T>>(initialDocs);\n+\n+        while (iteration < maxIterations)\n+        {\n+            iteration++;\n+\n+            // Generate partial answer\n+            var partialPrompt = $@\"Query: {currentQuery}\n+\n+Context: {string.Join(\"\\n\\n\", allRetrievedDocs.Select(d => d.Content.Substring(0, Math.Min(200, d.Content.Length))))}\n+\n+Please provide a partial answer. If you need more information, indicate what is missing.\";\n+\n+            var partialAnswer = _generator.Generate(partialPrompt);\n+            answer.AppendLine(partialAnswer);\n+\n+            // Detect if more information is needed (simplified confidence check)\n+            var confidence = CalculateConfidence(partialAnswer, allRetrievedDocs);\n+            \n+            if (confidence >= _uncertaintyThreshold)\n+            {\n+                // Confident enough, stop\n+                break;\n+            }\n+\n+            // Extract what information is needed\n+            var missingInfo = ExtractMissingInformation(partialAnswer);\n+            if (string.IsNullOrEmpty(missingInfo))\n+            {\n+                // No clear indication of missing info, stop\n+                break;\n+            }\n+\n+            // Retrieve additional documents\n+            var additionalDocs = _baseRetriever.Retrieve(missingInfo, topK: 2).ToList();\n+            \n+            if (additionalDocs.Count == 0)\n+            {\n+                // No more documents found, stop\n+                break;\n+            }\n+\n+            // Add to retrieved documents\n+            foreach (var doc in additionalDocs)\n+            {\n+                if (!allRetrievedDocs.Any(d => d.Id == doc.Id))\n+                {\n+                    allRetrievedDocs.Add(doc);\n+                }\n+            }\n+\n+            // Update current query for next iteration\n+            currentQuery = missingInfo;\n+        }\n+\n+        return answer.ToString().Trim();\n+    }\n+\n+    private double CalculateConfidence(string generatedText, List<Document<T>> retrievedDocs)\n+    {\n+        // Simplified confidence: based on text length and document coverage\n+        // In production, this would use token-level confidence from the LLM\n+        \n+        if (string.IsNullOrWhiteSpace(generatedText))\n+            return 0.0;\n+\n+        // Check for uncertainty indicators\n+        var uncertaintyPhrases = new[] { \"not sure\", \"don't know\", \"need more\", \"unclear\", \"missing\", \"uncertain\" };\n+        var hasUncertainty = uncertaintyPhrases.Any(phrase => \n+            generatedText.ToLower().Contains(phrase));\n+\n+        if (hasUncertainty)\n+            return 0.3; // Low confidence\n+\n+        // Check length (longer answers are usually more complete)\n+        var lengthScore = Math.Min(1.0, generatedText.Length / 500.0);\n+\n+        // Check document relevance\n+        var avgRelevance = retrievedDocs\n+            .Where(d => d.HasRelevanceScore)\n+            .Select(d => Convert.ToDouble(d.RelevanceScore))\n+            .DefaultIfEmpty(0.5)\n+            .Average();\n+\n+        return (lengthScore + avgRelevance) / 2.0;",
    "path": "src/RetrievalAugmentedGeneration/AdvancedPatterns/FLARERetriever.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "aaf7a4a8027b8bb88ec10c6b3b8f1d9ba7d9e40e",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Keep confidence calculations generic**\n\n`Convert.ToDouble(d.RelevanceScore)` violates the generic contractΓÇömany `INumericOperations<T>` implementations do not implement `IConvertible`, so this throws at runtime. Use the numeric-ops abstraction throughout, e.g.:\n\n```diff\n-        var avgRelevance = retrievedDocs\n-            .Where(d => d.HasRelevanceScore)\n-            .Select(d => Convert.ToDouble(d.RelevanceScore))\n-            .DefaultIfEmpty(0.5)\n-            .Average();\n+        var scored = retrievedDocs\n+            .Where(d => d.HasRelevanceScore)\n+            .Select(d => NumOps.ToDouble(d.RelevanceScore));\n+        var avgRelevance = scored.Any() ? scored.Average() : 0.5;\n```\n\n(or keep everything in `T` and only convert once via `NumOps.ToDouble`). This way FLARE works for every numeric type you just added.\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        var scored = retrievedDocs\n            .Where(d => d.HasRelevanceScore)\n            .Select(d => NumOps.ToDouble(d.RelevanceScore));\n        var avgRelevance = scored.Any() ? scored.Average() : 0.5;\n\n        return (lengthScore + avgRelevance) / 2.0;\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/AdvancedPatterns/FLARERetriever.cs around\nlines 253-259, the code uses Convert.ToDouble(d.RelevanceScore) which breaks the\ngeneric numeric contract; replace that call with the numeric-ops abstraction\n(e.g. use the existing INumericOperations<T> instance,\nNumOps.ToDouble(d.RelevanceScore)) or keep calculations in T and do a single\nconversion via NumOps.ToDouble when computing the final average. Ensure you\ncompute the average using NumOps (sum with NumOps.Add / divide via\nNumOps.ToDouble or convert each RelevanceScore with NumOps.ToDouble before\naveraging), and update method scope/signature to accept or use the NumOps\ninstance if it isnΓÇÖt already available.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T14:50:57Z",
    "updated_at": "2025-11-05T14:51:03Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494886991",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494886991"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494886991"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494886991/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 253,
    "original_start_line": 253,
    "start_side": "RIGHT",
    "line": 259,
    "original_line": 259,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 259,
    "position": 259,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887011",
    "pull_request_review_id": 3422589426,
    "id": 2494887011,
    "node_id": "PRRC_kwDOKSXUF86UtPRj",
    "diff_hunk": "@@ -0,0 +1,281 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Generators;\n+using AiDotNet.Interfaces;",
    "path": "src/RetrievalAugmentedGeneration/AdvancedPatterns/GraphRAG.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "aaf7a4a8027b8bb88ec10c6b3b8f1d9ba7d9e40e",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Remove duplicate using directive.**\n\nThe `using AiDotNet.Interfaces;` directive appears on both lines 2 and 4.\n\n\nApply this diff:\n\n```diff\n using AiDotNet.Helpers;\n using AiDotNet.Interfaces;\n using AiDotNet.RetrievalAugmentedGeneration.Generators;\n-using AiDotNet.Interfaces;\n using AiDotNet.RetrievalAugmentedGeneration.Models;\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\nusing AiDotNet.Interfaces;\nusing AiDotNet.RetrievalAugmentedGeneration.Generators;\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/AdvancedPatterns/GraphRAG.cs around lines 2\nto 4, there is a duplicate using directive for AiDotNet.Interfaces; remove the\nredundant duplicate (keep a single using AiDotNet.Interfaces;), ensure the\nremaining using directives are unique and ordered as desired, then save the\nfile.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T14:50:57Z",
    "updated_at": "2025-11-05T14:51:03Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494887011",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887011"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494887011"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887011/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 2,
    "original_start_line": 2,
    "start_side": "RIGHT",
    "line": 4,
    "original_line": 4,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 4,
    "position": 4,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887028",
    "pull_request_review_id": 3422589426,
    "id": 2494887028,
    "node_id": "PRRC_kwDOKSXUF86UtPR0",
    "diff_hunk": "@@ -0,0 +1,277 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Generators;\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using AiDotNet.RetrievalAugmentedGeneration.Retrievers;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Text;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.AdvancedPatterns;\n+\n+/// <summary>\n+/// Self-correcting retriever that iteratively refines answers through critique, error detection, and targeted re-retrieval.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// This advanced RAG pattern implements a self-correction loop: retrieve documents, generate answer,\n+/// critique the answer for errors or gaps, retrieve additional targeted documents, and repeat until\n+/// the answer is satisfactory. This mirrors how humans refine their understanding through iteration.\n+/// </para>\n+/// <para><b>For Beginners:</b> Think of this like writing an essay with self-editing.\n+/// \n+/// Normal approach:\n+/// - Research topic once ΓåÆ Write essay ΓåÆ Submit (might have errors!)\n+/// \n+/// Self-correcting approach:\n+/// - Research ΓåÆ Write draft ΓåÆ Read and critique ΓåÆ \"Wait, I'm missing data about X\"\n+/// - Research X specifically ΓåÆ Add to essay ΓåÆ Critique again ΓåÆ \"This part contradicts that part\"\n+/// - Research to resolve ΓåÆ Fix contradiction ΓåÆ Final review ΓåÆ Submit when satisfied\n+/// \n+/// Example:\n+/// Question: \"What caused the fall of the Roman Empire?\"\n+/// \n+/// Iteration 1:\n+/// - Retrieved: General docs about Roman Empire\n+/// - Answer: \"Economic problems and barbarian invasions caused the fall\"\n+/// - Critique: \"Too vague - which economic problems? When did invasions happen?\"\n+/// - Satisfied: NO\n+/// \n+/// Iteration 2:\n+/// - Re-retrieve: \"Roman Empire economic problems inflation\"\n+/// - Answer: \"Currency debasement and inflation in the 3rd century, plus Germanic invasions in 410 AD\"\n+/// - Critique: \"Better, but missing Eastern vs Western Empire distinction\"\n+/// - Satisfied: NO\n+/// \n+/// Iteration 3:\n+/// - Re-retrieve: \"Western Roman Empire Eastern Byzantine\"\n+/// - Answer: \"Western Empire fell in 476 AD due to economics + invasions; Eastern continued as Byzantine\"\n+/// - Critique: \"Complete and accurate!\"\n+/// - Satisfied: YES ΓåÆ Return answer\n+/// </para>\n+/// <para><b>How It Works:</b>\n+/// The self-correction process:\n+/// 1. Initial Retrieval - Get top-K relevant documents for query\n+/// 2. Generate Answer - Create initial answer from retrieved documents\n+/// 3. Generate Critique - LLM critiques its own answer for errors/gaps\n+/// 4. Check Satisfaction - Parse critique for approval keywords\n+/// 5. If Not Satisfied:\n+///    a. Extract gaps - Identify what information is missing\n+///    b. Re-retrieve - Get documents about missing topics\n+///    c. Generate improved answer with all documents\n+///    d. Repeat critique (max 3 iterations)\n+/// 6. Return final answer\n+/// \n+/// Current implementation uses keyword detection for satisfaction.\n+/// Production should use structured critique (JSON) with explicit quality scores.\n+/// </para>\n+/// <para><b>Benefits:</b>\n+/// - Higher accuracy through iterative refinement\n+/// - Catches and corrects initial mistakes\n+/// - Identifies and fills knowledge gaps automatically\n+/// - More comprehensive answers\n+/// - Transparent - shows reasoning through critique\n+/// </para>\n+/// <para><b>Limitations:</b>\n+/// - Multiple LLM calls (higher cost/latency)\n+/// - May not converge if critique is inconsistent\n+/// - Depends heavily on LLM's self-critique ability\n+/// - Limited to max iterations (prevents infinite loops)\n+/// </para>\n+/// </remarks>\n+public class SelfCorrectingRetriever<T>\n+{\n+    private static readonly INumericOperations<T> NumOps = MathHelper.GetNumericOperations<T>();\n+    private readonly IGenerator<T> _generator;\n+    private readonly RetrieverBase<T> _baseRetriever;\n+    private readonly int _maxIterations;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"SelfCorrectingRetriever{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"generator\">The LLM generator (use StubGenerator or real LLM).</param>\n+    /// <param name=\"baseRetriever\">The underlying retriever to use.</param>\n+    /// <param name=\"maxIterations\">Maximum number of correction iterations (default: 3).</param>\n+    public SelfCorrectingRetriever(\n+        IGenerator<T> generator,\n+        RetrieverBase<T> baseRetriever,\n+        int maxIterations = 3)\n+    {\n+        _generator = generator ?? throw new ArgumentNullException(nameof(generator));\n+        _baseRetriever = baseRetriever ?? throw new ArgumentNullException(nameof(baseRetriever));\n+        \n+        if (maxIterations <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(maxIterations), \"Max iterations must be positive\");\n+            \n+        _maxIterations = maxIterations;\n+    }\n+\n+    /// <summary>\n+    /// Retrieves documents and generates a self-corrected, refined answer through iterative critique.\n+    /// </summary>\n+    /// <param name=\"query\">The user's question requiring a high-quality answer.</param>\n+    /// <param name=\"topK\">Number of documents to retrieve in each iteration.</param>\n+    /// <returns>Final refined answer after self-correction iterations.</returns>\n+    /// <exception cref=\"ArgumentException\">Thrown when query is null or whitespace.</exception>\n+    /// <exception cref=\"ArgumentOutOfRangeException\">Thrown when topK is not positive.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// This method implements the self-correction loop: retrieve ΓåÆ generate ΓåÆ critique ΓåÆ refine.\n+    /// It continues iterating until the answer passes self-critique or max iterations is reached.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is like having a built-in editor that keeps improving your answer.\n+    /// \n+    /// The process:\n+    /// 1. Retrieve: Get topK documents about the query\n+    /// 2. Generate: Create answer from those documents\n+    /// 3. Critique: Ask LLM \"Is this answer complete and accurate?\"\n+    /// 4. Evaluate Critique:\n+    ///    - If critique contains \"satisfactory\", \"complete\", \"accurate\" ΓåÆ Done!\n+    ///    - If critique mentions gaps/errors ΓåÆ Continue improving\n+    /// 5. Extract Gaps: What information is missing from critique?\n+    /// 6. Re-Retrieve: Get more documents about the gaps\n+    /// 7. Re-Generate: Create improved answer with ALL documents (old + new)\n+    /// 8. Repeat: Back to step 3 (max 3 times total)\n+    /// 9. Return: Best answer achieved\n+    /// \n+    /// Safety: Maximum 3 iterations prevent infinite loops if model keeps finding issues.\n+    /// </para>\n+    /// </remarks>\n+    public string RetrieveAndAnswer(string query, int topK)\n+    {\n+        if (string.IsNullOrWhiteSpace(query))\n+            throw new ArgumentException(\"Query cannot be null or whitespace\", nameof(query));\n+\n+        if (topK <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(topK), \"topK must be positive\");\n+\n+        // Step 1: Initial retrieval\n+        var documents = _baseRetriever.Retrieve(query, topK).ToList();\n+        \n+        if (documents.Count == 0)\n+        {\n+            return \"I don't have enough information to answer this question.\";\n+        }\n+\n+        var currentAnswer = string.Empty;\n+        var iteration = 0;\n+\n+        while (iteration < _maxIterations)\n+        {\n+            iteration++;\n+\n+            // Step 2: Generate answer from current documents\n+            var groundedAnswer = _generator.GenerateGrounded(query, documents);\n+            currentAnswer = groundedAnswer.Answer;\n+\n+            // Step 3: Critique the answer\n+            var critiquePrompt = $@\"Query: {query}\n+\n+Answer: {currentAnswer}\n+\n+Please critique this answer:\n+1. Are there any factual errors?\n+2. Are there gaps in coverage?\n+3. What additional information would improve it?\n+4. Is it complete and accurate?\n+\n+Provide a brief critique.\";\n+\n+            var critique = _generator.Generate(critiquePrompt);\n+\n+            // Step 4: Check if answer is satisfactory\n+            if (IsAnswerSatisfactory(critique))\n+            {\n+                // Answer is good, stop iterating\n+                break;\n+            }\n+\n+            // Step 5: Identify missing information\n+            var missingInfo = ExtractMissingInformation(critique);\n+            \n+            if (string.IsNullOrEmpty(missingInfo))\n+            {\n+                // No clear improvement path, stop\n+                break;\n+            }\n+\n+            // Step 6: Retrieve additional documents\n+            var additionalDocs = _baseRetriever.Retrieve(missingInfo, topK: 2).ToList();\n+            \n+            if (additionalDocs.Count == 0)\n+            {\n+                // No new documents found, stop\n+                break;\n+            }\n+\n+            // Step 7: Add new documents to the set\n+            foreach (var doc in additionalDocs)\n+            {\n+                if (!documents.Any(d => d.Id == doc.Id))\n+                {\n+                    documents.Add(doc);\n+                }\n+            }\n+\n+            // Continue to next iteration with expanded document set\n+        }\n+\n+        return currentAnswer;",
    "path": "src/RetrievalAugmentedGeneration/AdvancedPatterns/SelfCorrectingRetriever.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "aaf7a4a8027b8bb88ec10c6b3b8f1d9ba7d9e40e",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Allow metadata filters to flow through the self-correction loop**\n\nThis routine always calls `_baseRetriever.Retrieve` with the overload that creates a fresh, unfiltered dictionary. In a multi-tenant store that leaks documents belonging to other tenants every time the self-corrector re-queries. Accept an optional `metadataFilters` parameter (default empty), reuse it for the initial retrieval, and pass it through each subsequent `_baseRetriever.Retrieve` call.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/AdvancedPatterns/SelfCorrectingRetriever.cs\naround lines 143 to 222, the method RetrieveAndAnswer always calls\n_baseRetriever.Retrieve without passing metadata filters, which causes\nunfiltered re-queries; add an optional parameter (e.g.,\nIDictionary<string,string> metadataFilters = null or\nIReadOnlyDictionary<string,string> metadataFilters = null) to RetrieveAndAnswer,\nuse that parameter for the initial _baseRetriever.Retrieve call instead of the\noverload that builds a fresh dictionary, and pass the same metadataFilters into\nevery subsequent _baseRetriever.Retrieve invocation (including the\nadditionalDocs retrieval) so the same tenant/metadata constraints persist\nthrough the entire self-correction loop.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T14:50:57Z",
    "updated_at": "2025-11-05T14:51:03Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494887028",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887028"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494887028"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887028/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 143,
    "original_start_line": 143,
    "start_side": "RIGHT",
    "line": 222,
    "original_line": 222,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 222,
    "position": 222,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887039",
    "pull_request_review_id": 3422589426,
    "id": 2494887039,
    "node_id": "PRRC_kwDOKSXUF86UtPR_",
    "diff_hunk": "@@ -0,0 +1,218 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Text.RegularExpressions;\n+using AiDotNet.Interfaces;\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies;\n+\n+/// <summary>\n+/// Production-ready intelligent chunker that decides where to split text based on semantic boundaries.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// This chunker analyzes text structure to identify optimal split points based on:\n+/// - Paragraph boundaries\n+/// - Topic transitions (detected via sentence similarity)\n+/// - Natural breaks in content flow (headers, lists, code blocks)\n+/// - Semantic coherence within chunks\n+/// </para>\n+/// <para><b>For Beginners:</b> This is like a smart text splitter that understands content structure.\n+/// \n+/// Think of it like organizing a book:\n+/// - Don't split in the middle of a sentence or paragraph\n+/// - Keep related ideas together in the same chunk\n+/// - Start new chunks at natural topic boundaries\n+/// - Maintain context with overlapping content\n+/// \n+/// How it works:\n+/// 1. Identifies structural elements (paragraphs, sections, lists)\n+/// 2. Calculates semantic coherence scores for potential splits\n+/// 3. Creates chunks at natural boundaries\n+/// 4. Adds overlap for context preservation\n+/// \n+/// Example:\n+/// - Input: Long article about climate change\n+/// - Output: Chunks at section boundaries, keeping introduction separate from data analysis,\n+///   solutions separate from problems, etc.\n+/// \n+/// Unlike simple fixed-size chunking:\n+/// - Γ£ô Respects paragraph boundaries\n+/// - Γ£ô Keeps related sentences together  \n+/// - Γ£ô Detects topic changes\n+/// - Γ£ô Preserves document structure\n+/// \n+/// Production features:\n+/// - No external API dependencies\n+/// - Fast heuristic-based topic detection\n+/// - Configurable chunk sizes and overlap\n+/// - Handles multiple document formats\n+/// - Maintains semantic coherence\n+/// </para>\n+/// </remarks>\n+public class AgenticChunker : ChunkingStrategyBase\n+{\n+    private readonly int _maxChunkSize;\n+    private readonly double _coherenceThreshold;",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/AgenticChunker.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "aaf7a4a8027b8bb88ec10c6b3b8f1d9ba7d9e40e",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Unused field: _coherenceThreshold is never referenced.**\n\nThe `_coherenceThreshold` field is initialized but never used in `DetectBoundaries`, `CreateSemanticChunks`, or any other method. Either implement coherence-based logic or remove the field and parameter.\n\n\n\n\nAlso applies to: 70-70, 83-83\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/ChunkingStrategies/AgenticChunker.cs around\nlines 59, 70 and 83, the private field _coherenceThreshold is declared and a\ncorresponding constructor parameter exists but the value is never used; remove\nthe unused field and constructor parameter (and its assignment) and update any\ncall sites that pass that parameter to the AgenticChunker constructor;\nalternatively, if coherence-based behavior is required, implement usage by\napplying the threshold in DetectBoundaries/CreateSemanticChunks where coherence\nis computedΓÇöpick one approach, remove the dead field/parameter and associated\ntests/usages if removing, or wire the threshold into the boundary/semantic chunk\nlogic if keeping.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T14:50:57Z",
    "updated_at": "2025-11-05T14:51:03Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494887039",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887039"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494887039"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887039/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": 59,
    "original_line": 59,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 59,
    "position": 59,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887054",
    "pull_request_review_id": 3422589426,
    "id": 2494887054,
    "node_id": "PRRC_kwDOKSXUF86UtPSO",
    "diff_hunk": "@@ -0,0 +1,218 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Text.RegularExpressions;\n+using AiDotNet.Interfaces;\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies;\n+\n+/// <summary>\n+/// Production-ready intelligent chunker that decides where to split text based on semantic boundaries.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// This chunker analyzes text structure to identify optimal split points based on:\n+/// - Paragraph boundaries\n+/// - Topic transitions (detected via sentence similarity)\n+/// - Natural breaks in content flow (headers, lists, code blocks)\n+/// - Semantic coherence within chunks\n+/// </para>\n+/// <para><b>For Beginners:</b> This is like a smart text splitter that understands content structure.\n+/// \n+/// Think of it like organizing a book:\n+/// - Don't split in the middle of a sentence or paragraph\n+/// - Keep related ideas together in the same chunk\n+/// - Start new chunks at natural topic boundaries\n+/// - Maintain context with overlapping content\n+/// \n+/// How it works:\n+/// 1. Identifies structural elements (paragraphs, sections, lists)\n+/// 2. Calculates semantic coherence scores for potential splits\n+/// 3. Creates chunks at natural boundaries\n+/// 4. Adds overlap for context preservation\n+/// \n+/// Example:\n+/// - Input: Long article about climate change\n+/// - Output: Chunks at section boundaries, keeping introduction separate from data analysis,\n+///   solutions separate from problems, etc.\n+/// \n+/// Unlike simple fixed-size chunking:\n+/// - Γ£ô Respects paragraph boundaries\n+/// - Γ£ô Keeps related sentences together  \n+/// - Γ£ô Detects topic changes\n+/// - Γ£ô Preserves document structure\n+/// \n+/// Production features:\n+/// - No external API dependencies\n+/// - Fast heuristic-based topic detection\n+/// - Configurable chunk sizes and overlap\n+/// - Handles multiple document formats\n+/// - Maintains semantic coherence\n+/// </para>\n+/// </remarks>\n+public class AgenticChunker : ChunkingStrategyBase\n+{\n+    private readonly int _maxChunkSize;\n+    private readonly double _coherenceThreshold;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"AgenticChunker\"/> class.\n+    /// </summary>\n+    /// <param name=\"maxChunkSize\">Maximum size of each chunk in characters (default: 1000).</param>\n+    /// <param name=\"overlap\">Number of overlapping characters between chunks (default: 200).</param>\n+    /// <param name=\"coherenceThreshold\">Minimum coherence score to keep sentences together (0-1, default: 0.3).</param>\n+    public AgenticChunker(\n+        int maxChunkSize = 1000,\n+        int overlap = 200,\n+        double coherenceThreshold = 0.3)\n+        : base(maxChunkSize, overlap)\n+    {\n+        if (maxChunkSize <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(maxChunkSize), \"Max chunk size must be positive\");\n+        if (overlap < 0)\n+            throw new ArgumentOutOfRangeException(nameof(overlap), \"Overlap cannot be negative\");\n+        if (overlap >= maxChunkSize)\n+            throw new ArgumentOutOfRangeException(nameof(overlap), \"Overlap must be less than max chunk size\");\n+        if (coherenceThreshold < 0 || coherenceThreshold > 1)\n+            throw new ArgumentOutOfRangeException(nameof(coherenceThreshold), \"Coherence threshold must be between 0 and 1\");\n+            \n+        _maxChunkSize = maxChunkSize;\n+        _coherenceThreshold = coherenceThreshold;\n+    }\n+\n+    /// <summary>\n+    /// Splits text into chunks using intelligent boundary detection.\n+    /// </summary>\n+    protected override IEnumerable<(string Chunk, int StartPosition, int EndPosition)> ChunkCore(string text)\n+    {\n+        if (string.IsNullOrWhiteSpace(text))\n+            yield break;\n+\n+        // Detect structural boundaries (paragraphs, sections)\n+        var boundaries = DetectBoundaries(text);\n+        \n+        // Create chunks at natural boundaries\n+        var chunks = CreateSemanticChunks(text, boundaries);\n+        \n+        foreach (var chunk in chunks)\n+        {\n+            yield return chunk;\n+        }\n+    }\n+\n+    private List<int> DetectBoundaries(string text)\n+    {\n+        var boundaries = new List<int> { 0 };\n+\n+        // Detect paragraph breaks (double newlines)\n+        var paragraphPattern = @\"\\n\\s*\\n\";\n+        var paragraphMatches = Regex.Matches(text, paragraphPattern);\n+        foreach (Match match in paragraphMatches)\n+        {\n+            boundaries.Add(match.Index);\n+        }\n+\n+        // Detect section headers (lines starting with #, or all caps)\n+        var lines = text.Split('\\n');",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/AgenticChunker.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "aaf7a4a8027b8bb88ec10c6b3b8f1d9ba7d9e40e",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Fix position tracking to handle all line ending types.**\n\nThe code splits by `'\\n'` (line 119) and adds `line.Length + 1` (lines 132, 144), assuming a single-character newline. This breaks with Windows-style `\\r\\n` line endings, causing all subsequent boundary positions to be incorrect.\n\n\n\nApply this diff to handle all line ending types consistently:\n\n```diff\n-        var lines = text.Split('\\n');\n+        var lines = text.Split(new[] { \"\\r\\n\", \"\\r\", \"\\n\" }, StringSplitOptions.None);\n         var position = 0;\n         foreach (var line in lines)\n         {\n             var trimmed = line.Trim();\n             \n             // Markdown headers or all-caps headers\n             if (trimmed.StartsWith(\"#\") || \n                 (trimmed.Length > 3 && trimmed.Length < 100 && trimmed == trimmed.ToUpperInvariant() && !trimmed.All(char.IsDigit)))\n             {\n                 boundaries.Add(position);\n             }\n             \n-            position += line.Length + 1; // +1 for newline\n+            // Calculate actual newline length at this position\n+            var newlineLength = 0;\n+            if (position + line.Length < text.Length)\n+            {\n+                if (position + line.Length + 1 < text.Length && \n+                    text[position + line.Length] == '\\r' && \n+                    text[position + line.Length + 1] == '\\n')\n+                    newlineLength = 2;\n+                else if (text[position + line.Length] == '\\r' || text[position + line.Length] == '\\n')\n+                    newlineLength = 1;\n+            }\n+            position += line.Length + newlineLength;\n         }\n \n         // Detect list boundaries\n         var listPattern = @\"^\\s*[\\d\\-\\*]\\s+\";\n         position = 0;\n         foreach (var line in lines)\n         {\n             if (Regex.IsMatch(line, listPattern))\n             {\n                 boundaries.Add(position);\n             }\n-            position += line.Length + 1;\n+            var newlineLength = 0;\n+            if (position + line.Length < text.Length)\n+            {\n+                if (position + line.Length + 1 < text.Length && \n+                    text[position + line.Length] == '\\r' && \n+                    text[position + line.Length + 1] == '\\n')\n+                    newlineLength = 2;\n+                else if (text[position + line.Length] == '\\r' || text[position + line.Length] == '\\n')\n+                    newlineLength = 1;\n+            }\n+            position += line.Length + newlineLength;\n         }\n```\n\nAlternatively, track line ending positions during the split for better performance.\n\n\nAlso applies to: 132-132, 144-144\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T14:50:58Z",
    "updated_at": "2025-11-05T14:51:03Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494887054",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887054"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494887054"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887054/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": 119,
    "original_line": 119,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 119,
    "position": 119,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887080",
    "pull_request_review_id": 3422589426,
    "id": 2494887080,
    "node_id": "PRRC_kwDOKSXUF86UtPSo",
    "diff_hunk": "@@ -0,0 +1,186 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies;\n+\n+/// <summary>\n+/// Splits text into chunks based on sentence boundaries to preserve semantic coherence.\n+/// </summary>\n+/// <remarks>\n+/// <para>\n+/// This chunking strategy splits text at sentence boundaries (periods, question marks,\n+/// exclamation points) and combines sentences until reaching the target chunk size.\n+/// This approach preserves complete thoughts and improves retrieval quality compared\n+/// to arbitrary character-based splitting.\n+/// </para>\n+/// <para><b>For Beginners:</b> This keeps complete sentences together in each chunk.\n+/// \n+/// Think of it like organizing a book:\n+/// - Bad way: Cut every 500 characters, even mid-sentence\n+///   \"The cat sat on the m|at. The dog ran ar|ound the yard.\"\n+/// - Good way: Keep sentences whole\n+///   Chunk 1: \"The cat sat on the mat. The dog ran around the yard.\"\n+///   Chunk 2: \"The bird flew over the fence. The fish swam in the pond.\"\n+/// \n+/// Why this matters:\n+/// - Retrieval works better when searching complete thoughts\n+/// - Generators get more coherent context\n+/// - No weird sentence fragments that confuse the model\n+/// \n+/// Parameters:\n+/// - targetChunkSize: Aim for this many characters per chunk\n+/// - maxChunkSize: Never exceed this size (may break sentences if needed)\n+/// - overlapSentences: Number of sentences to repeat between chunks for context\n+/// \n+/// Example with targetChunkSize=100, overlapSentences=1:\n+/// \"First sentence. Second sentence. Third sentence. Fourth sentence.\"\n+/// \n+/// Chunk 1: \"First sentence. Second sentence. Third sentence.\"\n+/// Chunk 2: \"Third sentence. Fourth sentence.\" (overlap: \"Third sentence\")\n+/// </para>\n+/// </remarks>\n+public class SentenceChunkingStrategy : ChunkingStrategyBase\n+{\n+    private readonly int _targetChunkSize;\n+    private readonly int _maxChunkSize;\n+    private readonly int _overlapSentences;\n+    private static readonly char[] SentenceEnders = { '.', '!', '?' };\n+\n+    /// <summary>\n+    /// Initializes a new instance of the SentenceChunkingStrategy class.\n+    /// </summary>\n+    /// <param name=\"targetChunkSize\">Target size for each chunk in characters (default: 500).</param>\n+    /// <param name=\"maxChunkSize\">Maximum allowed chunk size in characters (default: 1000).</param>\n+    /// <param name=\"overlapSentences\">Number of sentences to overlap between chunks (default: 1).</param>\n+    public SentenceChunkingStrategy(int targetChunkSize = 500, int maxChunkSize = 1000, int overlapSentences = 1)\n+        : base(maxChunkSize, 0)\n+    {\n+        if (maxChunkSize < targetChunkSize)\n+        {\n+            throw new ArgumentException(\"Maximum chunk size must be greater than or equal to target chunk size.\", nameof(maxChunkSize));\n+        }\n+\n+        if (overlapSentences < 0)\n+        {\n+            throw new ArgumentException(\"Overlap sentences cannot be negative.\", nameof(overlapSentences));\n+        }\n+\n+        _targetChunkSize = targetChunkSize;\n+        _maxChunkSize = maxChunkSize;\n+        _overlapSentences = overlapSentences;\n+    }\n+\n+    /// <summary>\n+    /// Splits text into chunks at sentence boundaries.\n+    /// </summary>\n+    /// <param name=\"text\">The validated text to split.</param>\n+    /// <returns>Chunks with position information.</returns>\n+    protected override IEnumerable<(string Chunk, int StartPosition, int EndPosition)> ChunkCore(string text)\n+    {\n+        var sentences = SplitIntoSentences(text);\n+        var chunks = new List<string>();\n+        var currentChunk = new List<string>();\n+        var currentLength = 0;\n+\n+        for (int i = 0; i < sentences.Count; i++)\n+        {\n+            var sentence = sentences[i];\n+            var sentenceLength = sentence.Length;\n+\n+            // If adding this sentence would exceed maxChunkSize and we have content, create a chunk\n+            if (currentLength + sentenceLength > _maxChunkSize && currentChunk.Count > 0)\n+            {\n+                chunks.Add(string.Join(\" \", currentChunk));\n+                \n+                // Keep the last N sentences for overlap\n+                var overlapStart = Math.Max(0, currentChunk.Count - _overlapSentences);\n+                currentChunk = currentChunk.GetRange(overlapStart, currentChunk.Count - overlapStart);\n+                currentLength = currentChunk.Sum(s => s.Length + 1) - 1; // +1 for space, -1 to remove last space\n+            }\n+\n+            currentChunk.Add(sentence);\n+            currentLength += sentenceLength + (currentChunk.Count > 1 ? 1 : 0); // Add space if not first sentence",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/SentenceChunkingStrategy.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "aaf7a4a8027b8bb88ec10c6b3b8f1d9ba7d9e40e",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Single sentences exceeding `maxChunkSize` bypass size enforcement.**\n\nWhen `currentChunk` is empty and a sentence is longer than `maxChunkSize`, the condition on line 90 evaluates to false (because `currentChunk.Count > 0` is false), and the sentence is added at line 100 without any size check. This creates chunks that violate the configured `maxChunkSize` limit, potentially causing issues with downstream components that rely on size constraints (e.g., embedding model token limits).\n\n**Example:**\n```csharp\n// If a sentence is 5000 chars but maxChunkSize is 1000:\n// - currentChunk.Count == 0 initially\n// - Condition on line 90 evaluates to false\n// - Sentence is added at line 100\n// - Resulting chunk is 5000 chars (5├ù the limit)\n```\n\nAs noted in the previous review, consider one of these approaches:\n1. Split oversized sentences on word boundaries (preferred), falling back to character splits if necessary\n2. At minimum, log a warning when a sentence exceeds `maxChunkSize` and document this behavior clearly in XML comments\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T14:50:58Z",
    "updated_at": "2025-11-05T14:51:03Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494887080",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887080"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494887080"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887080/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 90,
    "original_start_line": 90,
    "start_side": "RIGHT",
    "line": 101,
    "original_line": 101,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 101,
    "position": 101,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887096",
    "pull_request_review_id": 3422589426,
    "id": 2494887096,
    "node_id": "PRRC_kwDOKSXUF86UtPS4",
    "diff_hunk": "@@ -0,0 +1,186 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies;\n+\n+/// <summary>\n+/// Splits text into chunks based on sentence boundaries to preserve semantic coherence.\n+/// </summary>\n+/// <remarks>\n+/// <para>\n+/// This chunking strategy splits text at sentence boundaries (periods, question marks,\n+/// exclamation points) and combines sentences until reaching the target chunk size.\n+/// This approach preserves complete thoughts and improves retrieval quality compared\n+/// to arbitrary character-based splitting.\n+/// </para>\n+/// <para><b>For Beginners:</b> This keeps complete sentences together in each chunk.\n+/// \n+/// Think of it like organizing a book:\n+/// - Bad way: Cut every 500 characters, even mid-sentence\n+///   \"The cat sat on the m|at. The dog ran ar|ound the yard.\"\n+/// - Good way: Keep sentences whole\n+///   Chunk 1: \"The cat sat on the mat. The dog ran around the yard.\"\n+///   Chunk 2: \"The bird flew over the fence. The fish swam in the pond.\"\n+/// \n+/// Why this matters:\n+/// - Retrieval works better when searching complete thoughts\n+/// - Generators get more coherent context\n+/// - No weird sentence fragments that confuse the model\n+/// \n+/// Parameters:\n+/// - targetChunkSize: Aim for this many characters per chunk\n+/// - maxChunkSize: Never exceed this size (may break sentences if needed)\n+/// - overlapSentences: Number of sentences to repeat between chunks for context\n+/// \n+/// Example with targetChunkSize=100, overlapSentences=1:\n+/// \"First sentence. Second sentence. Third sentence. Fourth sentence.\"\n+/// \n+/// Chunk 1: \"First sentence. Second sentence. Third sentence.\"\n+/// Chunk 2: \"Third sentence. Fourth sentence.\" (overlap: \"Third sentence\")\n+/// </para>\n+/// </remarks>\n+public class SentenceChunkingStrategy : ChunkingStrategyBase\n+{\n+    private readonly int _targetChunkSize;\n+    private readonly int _maxChunkSize;\n+    private readonly int _overlapSentences;\n+    private static readonly char[] SentenceEnders = { '.', '!', '?' };\n+\n+    /// <summary>\n+    /// Initializes a new instance of the SentenceChunkingStrategy class.\n+    /// </summary>\n+    /// <param name=\"targetChunkSize\">Target size for each chunk in characters (default: 500).</param>\n+    /// <param name=\"maxChunkSize\">Maximum allowed chunk size in characters (default: 1000).</param>\n+    /// <param name=\"overlapSentences\">Number of sentences to overlap between chunks (default: 1).</param>\n+    public SentenceChunkingStrategy(int targetChunkSize = 500, int maxChunkSize = 1000, int overlapSentences = 1)\n+        : base(maxChunkSize, 0)\n+    {\n+        if (maxChunkSize < targetChunkSize)\n+        {\n+            throw new ArgumentException(\"Maximum chunk size must be greater than or equal to target chunk size.\", nameof(maxChunkSize));\n+        }\n+\n+        if (overlapSentences < 0)\n+        {\n+            throw new ArgumentException(\"Overlap sentences cannot be negative.\", nameof(overlapSentences));\n+        }\n+\n+        _targetChunkSize = targetChunkSize;\n+        _maxChunkSize = maxChunkSize;\n+        _overlapSentences = overlapSentences;\n+    }\n+\n+    /// <summary>\n+    /// Splits text into chunks at sentence boundaries.\n+    /// </summary>\n+    /// <param name=\"text\">The validated text to split.</param>\n+    /// <returns>Chunks with position information.</returns>\n+    protected override IEnumerable<(string Chunk, int StartPosition, int EndPosition)> ChunkCore(string text)\n+    {\n+        var sentences = SplitIntoSentences(text);\n+        var chunks = new List<string>();\n+        var currentChunk = new List<string>();\n+        var currentLength = 0;\n+\n+        for (int i = 0; i < sentences.Count; i++)\n+        {\n+            var sentence = sentences[i];\n+            var sentenceLength = sentence.Length;\n+\n+            // If adding this sentence would exceed maxChunkSize and we have content, create a chunk\n+            if (currentLength + sentenceLength > _maxChunkSize && currentChunk.Count > 0)\n+            {\n+                chunks.Add(string.Join(\" \", currentChunk));\n+                \n+                // Keep the last N sentences for overlap\n+                var overlapStart = Math.Max(0, currentChunk.Count - _overlapSentences);\n+                currentChunk = currentChunk.GetRange(overlapStart, currentChunk.Count - overlapStart);\n+                currentLength = currentChunk.Sum(s => s.Length + 1) - 1; // +1 for space, -1 to remove last space\n+            }\n+\n+            currentChunk.Add(sentence);\n+            currentLength += sentenceLength + (currentChunk.Count > 1 ? 1 : 0); // Add space if not first sentence\n+\n+            // If we've reached target size, create a chunk\n+            if (currentLength >= _targetChunkSize)\n+            {\n+                chunks.Add(string.Join(\" \", currentChunk));\n+                \n+                // Keep the last N sentences for overlap\n+                var overlapStart = Math.Max(0, currentChunk.Count - _overlapSentences);\n+                currentChunk = currentChunk.GetRange(overlapStart, currentChunk.Count - overlapStart);\n+                currentLength = currentChunk.Sum(s => s.Length + 1) - 1;\n+            }\n+        }\n+\n+        // Add remaining sentences as final chunk\n+        if (currentChunk.Count > 0)\n+        {\n+            chunks.Add(string.Join(\" \", currentChunk));\n+        }\n+\n+        // Convert to tuples with positions\n+        var results = new List<(string, int, int)>();\n+        var position = 0;\n+        foreach (var chunk in chunks)\n+        {\n+            var endPos = position + chunk.Length;\n+            results.Add((chunk, position, endPos));\n+            position = endPos;\n+        }",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/SentenceChunkingStrategy.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "aaf7a4a8027b8bb88ec10c6b3b8f1d9ba7d9e40e",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Position calculation incorrectly assumes chunks are contiguous.**\n\nThe position tracking treats chunks as sequential (non-overlapping), but the chunking logic creates overlapping chunks by retaining the last N sentences (lines 95-97, 109-111). This produces incorrect `StartPosition` and `EndPosition` values that don't reflect where chunks actually appear in the original text.\n\n**Example of the issue:**\n```\nOriginal text: \"Sentence one. Sentence two. Sentence three.\"\nWith overlapSentences=1:\n\nChunk 1: \"Sentence one. Sentence two.\"\nChunk 2: \"Sentence two. Sentence three.\" (overlap: \"Sentence two.\")\n\nCurrent code produces:\n  Chunk 1: (0, 28) Γ£ô correct\n  Chunk 2: (28, 58) Γ£ù wrong\n\nExpected positions:\n  Chunk 1: (0, 28)\n  Chunk 2: (14, 44) ΓÇö \"Sentence two.\" starts at position 14\n```\n\nThis was flagged in the previous review. To fix, track positions during chunk construction by locating each chunk in the original text, or maintain a running offset that accounts for the overlap. Any downstream code relying on accurate positions (highlighting, citations, etc.) will fail with the current implementation.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn\nsrc/RetrievalAugmentedGeneration/ChunkingStrategies/SentenceChunkingStrategy.cs\naround lines 121-129, the code assumes chunks are contiguous when computing\nStartPosition/EndPosition, but earlier logic creates overlapping chunks so\npositions are wrong; fix by computing positions against the original source text\nas chunks are constructed (or by maintaining an offset that accounts for\noverlap) ΓÇö for each chunk find its first occurrence in the original text\nstarting from the last matched index (use IndexOf with a start index or\nequivalent) to get the actual StartPosition, set EndPosition = StartPosition +\nchunk.Length, and add those accurate positions to the results list so\noverlapping chunks reflect their true offsets in the original text.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T14:50:58Z",
    "updated_at": "2025-11-05T14:51:03Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494887096",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887096"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494887096"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887096/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 121,
    "original_start_line": 121,
    "start_side": "RIGHT",
    "line": 129,
    "original_line": 129,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 129,
    "position": 129,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887112",
    "pull_request_review_id": 3422589426,
    "id": 2494887112,
    "node_id": "PRRC_kwDOKSXUF86UtPTI",
    "diff_hunk": "@@ -1,7 +1,8 @@\n using System.Linq;\n using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n using AiDotNet.LinearAlgebra;\n-using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.Interfaces;",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs",
    "commit_id": "3503d418e63f8ea60886e3219a79e56205bc9100",
    "original_commit_id": "aaf7a4a8027b8bb88ec10c6b3b8f1d9ba7d9e40e",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Remove duplicate using directive.**\n\nThe `using AiDotNet.Interfaces;` directive appears on both lines 3 and 5.\n\n\nApply this diff:\n\n```diff\n using AiDotNet.Helpers;\n using AiDotNet.Interfaces;\n using AiDotNet.LinearAlgebra;\n-using AiDotNet.Interfaces;\n using AiDotNet.RetrievalAugmentedGeneration.Models;\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\nusing AiDotNet.Interfaces;\nusing AiDotNet.LinearAlgebra;\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/DocumentStoreBase.cs around\nlines 3 to 5, there is a duplicate using directive for AiDotNet.Interfaces;\nremove the redundant line so each namespace is imported only once ΓÇö keep one\nusing AiDotNet.Interfaces; and retain using AiDotNet.LinearAlgebra; then save\nthe file.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T14:50:58Z",
    "updated_at": "2025-11-05T14:51:03Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494887112",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887112"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494887112"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887112/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 3,
    "original_start_line": 3,
    "start_side": "RIGHT",
    "line": 5,
    "original_line": 5,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 6,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887127",
    "pull_request_review_id": 3422589426,
    "id": 2494887127,
    "node_id": "PRRC_kwDOKSXUF86UtPTX",
    "diff_hunk": "@@ -0,0 +1,114 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores\n+{\n+    /// <summary>\n+    /// Hybrid document store combining vector and keyword search strategies.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class HybridDocumentStore<T> : DocumentStoreBase<T>",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/HybridDocumentStore.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "aaf7a4a8027b8bb88ec10c6b3b8f1d9ba7d9e40e",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Implement missing GetAllCore method.**\n\nThe class does not implement the required abstract method `GetAllCore()` from `DocumentStoreBase<T>`, causing the build to fail.\n\n\nAdd this implementation:\n\n```diff\n+    protected override IEnumerable<Document<T>> GetAllCore()\n+    {\n+        return _vectorStore.GetAll();\n+    }\n+\n     public override void Clear()\n```\n\n<details>\n<summary>≡ƒº░ Tools</summary>\n\n<details>\n<summary>≡ƒ¬¢ GitHub Check: Build All Frameworks</summary>\n\n[failure] 15-15: \n'HybridDocumentStore<T>' does not implement inherited abstract member 'DocumentStoreBase<T>.GetAllCore()'\n\n</details>\n<details>\n<summary>≡ƒ¬¢ GitHub Check: Publish Size Analysis</summary>\n\n[failure] 15-15: \n'HybridDocumentStore<T>' does not implement inherited abstract member 'DocumentStoreBase<T>.GetAllCore()'\n\n</details>\n\n</details>\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/HybridDocumentStore.cs around\nline 15, the class is missing the required abstract override of GetAllCore()\nfrom DocumentStoreBase<T>, causing build failure; add a protected override\nIEnumerable<T> GetAllCore() implementation that returns the store's underlying\ncollection of documents (e.g., the internal list/collection field used to hold\ndocuments ΓÇö _documents/_items/etc.), or, if the store delegates to another\ncomponent, iterate that source and yield/return all items; ensure the method\nsignature matches the base class (protected override IEnumerable<T>\nGetAllCore()) and does not throw NotImplementedException.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commit 3503d41",
    "created_at": "2025-11-05T14:50:59Z",
    "updated_at": "2025-11-05T14:51:30Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494887127",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887127"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494887127"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887127/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": null,
    "start_side": null,
    "line": 40,
    "original_line": 15,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 15,
    "position": 40,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887150",
    "pull_request_review_id": 3422589426,
    "id": 2494887150,
    "node_id": "PRRC_kwDOKSXUF86UtPTu",
    "diff_hunk": "@@ -1,4 +1,6 @@\n-using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.Interfaces;",
    "path": "src/RetrievalAugmentedGeneration/Evaluation/RAGMetricBase.cs",
    "commit_id": "3503d418e63f8ea60886e3219a79e56205bc9100",
    "original_commit_id": "aaf7a4a8027b8bb88ec10c6b3b8f1d9ba7d9e40e",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Remove duplicate using statement.**\n\nLine 3 duplicates the `using AiDotNet.Interfaces;` statement from line 2.\n\n\n\nApply this diff:\n\n```diff\n using AiDotNet.Helpers;\n using AiDotNet.Interfaces;\n-using AiDotNet.Interfaces;\n using AiDotNet.RetrievalAugmentedGeneration.Models;\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\nusing AiDotNet.Helpers;\nusing AiDotNet.Interfaces;\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Evaluation/RAGMetricBase.cs around lines 1\nto 3, there is a duplicate using directive for AiDotNet.Interfaces on line 3;\nremove the redundant second using statement so the file only contains a single\nusing AiDotNet.Interfaces; entry.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T14:50:59Z",
    "updated_at": "2025-11-05T14:51:03Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494887150",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887150"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494887150"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887150/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 1,
    "original_start_line": 1,
    "start_side": "RIGHT",
    "line": 3,
    "original_line": 3,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 4,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887179",
    "pull_request_review_id": 3422589426,
    "id": 2494887179,
    "node_id": "PRRC_kwDOKSXUF86UtPUL",
    "diff_hunk": "@@ -0,0 +1,261 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Text;\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.NeuralNetworks;\n+using AiDotNet.Interfaces;",
    "path": "src/RetrievalAugmentedGeneration/Generators/NeuralGenerator.cs",
    "commit_id": "3503d418e63f8ea60886e3219a79e56205bc9100",
    "original_commit_id": "aaf7a4a8027b8bb88ec10c6b3b8f1d9ba7d9e40e",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Remove duplicate import.**\n\nLine 8 duplicates the `using AiDotNet.Interfaces;` statement from line 6.\n\n\nApply this diff:\n\n```diff\n using AiDotNet.Helpers;\n using AiDotNet.Interfaces;\n using AiDotNet.NeuralNetworks;\n-using AiDotNet.Interfaces;\n using AiDotNet.RetrievalAugmentedGeneration.Models;\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Generators/NeuralGenerator.cs around lines 6\nto 8, there is a duplicate using directive: remove the repeated \"using\nAiDotNet.Interfaces;\" on line 8 so only one import of AiDotNet.Interfaces\nremains; keep the other using statements intact and save the file.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T14:50:59Z",
    "updated_at": "2025-11-05T14:51:03Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494887179",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887179"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494887179"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887179/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 6,
    "original_start_line": 6,
    "start_side": "RIGHT",
    "line": 8,
    "original_line": 8,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 8,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887194",
    "pull_request_review_id": 3422589426,
    "id": 2494887194,
    "node_id": "PRRC_kwDOKSXUF86UtPUa",
    "diff_hunk": "@@ -0,0 +1,261 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Text;\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.NeuralNetworks;\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Generators;\n+\n+/// <summary>\n+/// Production-ready neural network-based text generator for RAG systems.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations and relevance scoring.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// This generator uses an LSTM-based neural network architecture for text generation, providing\n+/// production-ready language modeling capabilities for retrieval-augmented generation tasks.\n+/// It processes retrieved context documents and generates coherent, grounded answers with proper\n+/// citations to source material.\n+/// </para>\n+/// <para><b>For Beginners:</b> This generator creates intelligent answers using neural networks.\n+///\n+/// Think of it like a smart writer with training:\n+/// - Takes your question and retrieved documents\n+/// - Uses an LSTM neural network to understand the context\n+/// - Generates a well-written answer\n+/// - Includes proper citations to sources\n+///\n+/// How it works:\n+/// 1. Encodes the question and context into numerical representations\n+/// 2. Processes through LSTM layers to understand relationships\n+/// 3. Generates text token-by-token using learned patterns\n+/// 4. Formats output with citations\n+/// 5. Calculates confidence based on context relevance\n+///\n+/// Unlike StubGenerator (which just formats text), this:\n+/// - Actually understands language patterns\n+/// - Can paraphrase and synthesize information\n+/// - Generates fluent, natural responses\n+/// - Adapts tone and style based on training\n+///\n+/// Production features:\n+/// - Configurable context and generation limits\n+/// - Proper error handling for edge cases\n+/// - Citation extraction and source attribution\n+/// - Confidence scoring based on retrieval quality\n+/// - Memory-efficient processing for large contexts\n+/// </para>\n+/// </remarks>\n+public class NeuralGenerator<T> : IGenerator<T>\n+{\n+    private static readonly INumericOperations<T> NumOps = MathHelper.GetNumericOperations<T>();\n+    private readonly LSTMNeuralNetwork<T> _network;\n+    private readonly int _maxContextTokens;\n+    private readonly int _maxGenerationTokens;\n+    private readonly double _temperature;\n+    private readonly int _vocabularySize;\n+\n+    /// <summary>\n+    /// Gets the maximum number of tokens this generator can process in a single request.\n+    /// </summary>\n+    public int MaxContextTokens => _maxContextTokens;\n+\n+    /// <summary>\n+    /// Gets the maximum number of tokens this generator can generate in a response.\n+    /// </summary>\n+    public int MaxGenerationTokens => _maxGenerationTokens;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the NeuralGenerator class.\n+    /// </summary>\n+    /// <param name=\"network\">The LSTM neural network for text generation.</param>\n+    /// <param name=\"vocabularySize\">Size of the token vocabulary (default: 50000).</param>\n+    /// <param name=\"maxContextTokens\">The maximum context tokens (default: 4096).</param>\n+    /// <param name=\"maxGenerationTokens\">The maximum generation tokens (default: 1024).</param>\n+    /// <param name=\"temperature\">Sampling temperature for generation (default: 0.7). Higher = more creative.</param>\n+    public NeuralGenerator(\n+        LSTMNeuralNetwork<T> network,\n+        int vocabularySize = 50000,\n+        int maxContextTokens = 4096,\n+        int maxGenerationTokens = 1024,\n+        double temperature = 0.7)\n+    {\n+        _network = network ?? throw new ArgumentNullException(nameof(network));\n+\n+        if (vocabularySize <= 0)\n+            throw new ArgumentException(\"Vocabulary size must be positive\", nameof(vocabularySize));\n+        if (maxContextTokens <= 0)\n+            throw new ArgumentException(\"MaxContextTokens must be greater than zero\", nameof(maxContextTokens));\n+        if (maxGenerationTokens <= 0)\n+            throw new ArgumentException(\"MaxGenerationTokens must be greater than zero\", nameof(maxGenerationTokens));\n+        if (temperature <= 0)\n+            throw new ArgumentException(\"Temperature must be positive\", nameof(temperature));\n+\n+        _vocabularySize = vocabularySize;\n+        _maxContextTokens = maxContextTokens;\n+        _maxGenerationTokens = maxGenerationTokens;\n+        _temperature = temperature;\n+    }",
    "path": "src/RetrievalAugmentedGeneration/Generators/NeuralGenerator.cs",
    "commit_id": "3503d418e63f8ea60886e3219a79e56205bc9100",
    "original_commit_id": "aaf7a4a8027b8bb88ec10c6b3b8f1d9ba7d9e40e",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n<details>\n<summary>≡ƒº⌐ Analysis chain</summary>\n\n**Critical: Constructor parameters are never used.**\n\nThe constructor accepts `network` and `temperature` parameters but neither is used in the actual text generation logic:\n\n1. **`_network` (LSTMNeuralNetwork)**: Stored but never invoked. The generation logic in `GenerateTokens` (lines 220-239) and `PredictNextToken` (lines 241-252) uses a simplistic random-based approach instead of the LSTM network.\n\n2. **`_temperature`**: Stored but never applied to control sampling randomness during generation.\n\nThis contradicts the class documentation claiming \"production-ready neural network-based text generator\" and misleads users who pass in a trained LSTM network expecting it to be used.\n\n\n\nEither:\n- Integrate the LSTM network into the generation pipeline and apply temperature sampling, or\n- Update documentation to clarify this is a simplified/stub implementation and remove the \"production-ready\" claims\n\nRun this script to confirm the fields are never referenced in method bodies:\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Verify _network and _temperature are not used in generation logic\nrg -n '_network\\.' src/RetrievalAugmentedGeneration/Generators/NeuralGenerator.cs\nrg -n '_temperature' src/RetrievalAugmentedGeneration/Generators/NeuralGenerator.cs | grep -v 'private readonly' | grep -v '_temperature ='\n```\n\nLength of output: 0\n\n---\n\n</parameter>\n</invoke>\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Step 1: Verify file exists and show the entire file\necho \"=== Checking if file exists ===\"\nif [ -f \"src/RetrievalAugmentedGeneration/Generators/NeuralGenerator.cs\" ]; then\n    echo \"File found\"\nelse\n    echo \"File NOT found at expected path\"\n    find . -name \"NeuralGenerator.cs\" -type f 2>/dev/null\nfi\n\necho \"\"\necho \"=== Searching for field declarations ===\"\nrg -n 'private readonly.*_network' src/RetrievalAugmentedGeneration/Generators/NeuralGenerator.cs\nrg -n 'private readonly.*_temperature' src/RetrievalAugmentedGeneration/Generators/NeuralGenerator.cs\n\necho \"\"\necho \"=== Comprehensive search for _network usage (any context) ===\"\nrg -n '_network' src/RetrievalAugmentedGeneration/Generators/NeuralGenerator.cs\n\necho \"\"\necho \"=== Comprehensive search for _temperature usage (any context) ===\"\nrg -n '_temperature' src/RetrievalAugmentedGeneration/Generators/NeuralGenerator.cs\n\necho \"\"\necho \"=== Show GenerateTokens and PredictNextToken methods ===\"\nrg -A 20 'private.*GenerateTokens\\|private.*PredictNextToken' src/RetrievalAugmentedGeneration/Generators/NeuralGenerator.cs\n```\n\nLength of output: 595\n\n---\n\n</parameter>\n</invoke>\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Get all method definitions in the class\necho \"=== All method signatures in NeuralGenerator.cs ===\"\nrg -n '^\\s*(public|private|protected).*\\(' src/RetrievalAugmentedGeneration/Generators/NeuralGenerator.cs | head -30\n\necho \"\"\necho \"=== Check for Generate method ===\"\nrg -n 'Generate' src/RetrievalAugmentedGeneration/Generators/NeuralGenerator.cs\n\necho \"\"\necho \"=== Show full method bodies to understand implementation ===\"\n# Get a larger context window to see actual method implementations\nrg -A 30 'public.*Generate' src/RetrievalAugmentedGeneration/Generators/NeuralGenerator.cs | head -100\n```\n\nLength of output: 3692\n\n---\n\n</parameter>\n</invoke>\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Get the full GenerateTokens method implementation\necho \"=== GenerateTokens method (line 220+) ===\"\nsed -n '220,240p' src/RetrievalAugmentedGeneration/Generators/NeuralGenerator.cs\n\necho \"\"\necho \"=== PredictNextToken method (line 241+) ===\"\nsed -n '241,253p' src/RetrievalAugmentedGeneration/Generators/NeuralGenerator.cs\n\necho \"\"\necho \"=== Full file line count to find end of methods ===\"\nwc -l src/RetrievalAugmentedGeneration/Generators/NeuralGenerator.cs\n\necho \"\"\necho \"=== Extended view of PredictNextToken ===\"\nsed -n '241,260p' src/RetrievalAugmentedGeneration/Generators/NeuralGenerator.cs\n```\n\nLength of output: 2078\n\n---\n\n\n\n</details>\n\n**Critical: Constructor parameters `network` and `temperature` are never used in generation logic.**\n\nThe `_network` field (assigned line 87) and `_temperature` field (assigned line 101) are stored but completely unused:\n\n- **`GenerateTokens` (lines 220-238)**: Uses a hardcoded `Random` seed and simplistic arithmetic (`(lastToken + random.Next(1, 100)) % _vocabularySize`) instead of the LSTM network. Even includes a comment admitting this: \"Simplified next-token prediction (production would use full neural network forward pass)\".\n- **`PredictNextToken` (lines 241-252)**: No reference to `_network` or `_temperature`ΓÇöjust random token generation.\n\nThis contradicts the class documentation claiming \"production-ready neural network-based text generator\" and misleads users passing a trained LSTM network.\n\n**Fix**: Either integrate `_network` and apply `_temperature` sampling, or update documentation to clarify this is a stub/simplified implementation.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Generators/NeuralGenerator.cs around lines\n80 to 102, the constructor stores _network and _temperature but the generation\nmethods ignore them; update the generator so it either uses the provided\nLSTMNeuralNetwork<T> and temperature for sampling or mark the class explicitly\nas a stub. To fix, replace the current random-based next-token logic with a call\ninto _network to run a forward pass on the current context to obtain\nlogits/probabilities, apply temperature scaling to the logits (divide logits by\n_temperature), convert to a probability distribution (softmax), and sample the\nnext token from that distribution; remove the ad-hoc Random arithmetic, ensure\nproper null checks and exception messages remain, and add unit tests or comments\nshowing that the network and temperature are now used. If you choose the\nalternative, update the class documentation/summary to clearly state this is a\nsimplified stub and not production neural sampling, and keep constructor\nassignment but document why network/temperature are unused.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T14:50:59Z",
    "updated_at": "2025-11-05T14:51:03Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494887194",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887194"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494887194"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887194/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 80,
    "original_start_line": 80,
    "start_side": "RIGHT",
    "line": 102,
    "original_line": 102,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 102,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887211",
    "pull_request_review_id": 3422589426,
    "id": 2494887211,
    "node_id": "PRRC_kwDOKSXUF86UtPUr",
    "diff_hunk": "@@ -0,0 +1,261 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Text;\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.NeuralNetworks;\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Generators;\n+\n+/// <summary>\n+/// Production-ready neural network-based text generator for RAG systems.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for calculations and relevance scoring.</typeparam>\n+/// <remarks>\n+/// <para>\n+/// This generator uses an LSTM-based neural network architecture for text generation, providing\n+/// production-ready language modeling capabilities for retrieval-augmented generation tasks.\n+/// It processes retrieved context documents and generates coherent, grounded answers with proper\n+/// citations to source material.\n+/// </para>\n+/// <para><b>For Beginners:</b> This generator creates intelligent answers using neural networks.\n+///\n+/// Think of it like a smart writer with training:\n+/// - Takes your question and retrieved documents\n+/// - Uses an LSTM neural network to understand the context\n+/// - Generates a well-written answer\n+/// - Includes proper citations to sources\n+///\n+/// How it works:\n+/// 1. Encodes the question and context into numerical representations\n+/// 2. Processes through LSTM layers to understand relationships\n+/// 3. Generates text token-by-token using learned patterns\n+/// 4. Formats output with citations\n+/// 5. Calculates confidence based on context relevance\n+///\n+/// Unlike StubGenerator (which just formats text), this:\n+/// - Actually understands language patterns\n+/// - Can paraphrase and synthesize information\n+/// - Generates fluent, natural responses\n+/// - Adapts tone and style based on training\n+///\n+/// Production features:\n+/// - Configurable context and generation limits\n+/// - Proper error handling for edge cases\n+/// - Citation extraction and source attribution\n+/// - Confidence scoring based on retrieval quality\n+/// - Memory-efficient processing for large contexts\n+/// </para>\n+/// </remarks>\n+public class NeuralGenerator<T> : IGenerator<T>\n+{\n+    private static readonly INumericOperations<T> NumOps = MathHelper.GetNumericOperations<T>();\n+    private readonly LSTMNeuralNetwork<T> _network;\n+    private readonly int _maxContextTokens;\n+    private readonly int _maxGenerationTokens;\n+    private readonly double _temperature;\n+    private readonly int _vocabularySize;\n+\n+    /// <summary>\n+    /// Gets the maximum number of tokens this generator can process in a single request.\n+    /// </summary>\n+    public int MaxContextTokens => _maxContextTokens;\n+\n+    /// <summary>\n+    /// Gets the maximum number of tokens this generator can generate in a response.\n+    /// </summary>\n+    public int MaxGenerationTokens => _maxGenerationTokens;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the NeuralGenerator class.\n+    /// </summary>\n+    /// <param name=\"network\">The LSTM neural network for text generation.</param>\n+    /// <param name=\"vocabularySize\">Size of the token vocabulary (default: 50000).</param>\n+    /// <param name=\"maxContextTokens\">The maximum context tokens (default: 4096).</param>\n+    /// <param name=\"maxGenerationTokens\">The maximum generation tokens (default: 1024).</param>\n+    /// <param name=\"temperature\">Sampling temperature for generation (default: 0.7). Higher = more creative.</param>\n+    public NeuralGenerator(\n+        LSTMNeuralNetwork<T> network,\n+        int vocabularySize = 50000,\n+        int maxContextTokens = 4096,\n+        int maxGenerationTokens = 1024,\n+        double temperature = 0.7)\n+    {\n+        _network = network ?? throw new ArgumentNullException(nameof(network));\n+\n+        if (vocabularySize <= 0)\n+            throw new ArgumentException(\"Vocabulary size must be positive\", nameof(vocabularySize));\n+        if (maxContextTokens <= 0)\n+            throw new ArgumentException(\"MaxContextTokens must be greater than zero\", nameof(maxContextTokens));\n+        if (maxGenerationTokens <= 0)\n+            throw new ArgumentException(\"MaxGenerationTokens must be greater than zero\", nameof(maxGenerationTokens));\n+        if (temperature <= 0)\n+            throw new ArgumentException(\"Temperature must be positive\", nameof(temperature));\n+\n+        _vocabularySize = vocabularySize;\n+        _maxContextTokens = maxContextTokens;\n+        _maxGenerationTokens = maxGenerationTokens;\n+        _temperature = temperature;\n+    }\n+\n+    /// <summary>\n+    /// Generates a text response based on a prompt.\n+    /// </summary>\n+    /// <param name=\"prompt\">The input prompt or question.</param>\n+    /// <returns>The generated text response.</returns>\n+    public string Generate(string prompt)\n+    {\n+        if (string.IsNullOrWhiteSpace(prompt))\n+            throw new ArgumentException(\"Prompt cannot be null or empty\", nameof(prompt));\n+\n+        var tokens = TokenizeText(prompt);\n+        if (tokens.Count == 0)\n+            return \"Unable to process empty input.\";\n+\n+        // Limit to context window\n+        if (tokens.Count > _maxContextTokens)\n+            tokens = tokens.Take(_maxContextTokens).ToList();\n+\n+        // Generate response using neural network\n+        var generated = GenerateTokens(tokens, _maxGenerationTokens);\n+        return DetokenizeText(generated);\n+    }\n+\n+    /// <summary>\n+    /// Generates a grounded answer using provided context documents.\n+    /// </summary>\n+    /// <param name=\"query\">The user's original query or question.</param>\n+    /// <param name=\"context\">The retrieved documents providing context for the answer.</param>\n+    /// <returns>A grounded answer with the generated text, source documents, and extracted citations.</returns>\n+    public GroundedAnswer<T> GenerateGrounded(string query, IEnumerable<Document<T>> context)\n+    {\n+        if (string.IsNullOrWhiteSpace(query))\n+            throw new ArgumentException(\"Query cannot be null or empty\", nameof(query));\n+\n+        var contextList = context?.ToList() ?? new List<Document<T>>();\n+\n+        if (contextList.Count == 0)\n+        {\n+            return new GroundedAnswer<T>\n+            {\n+                Query = query,\n+                Answer = \"I don't have enough information to answer this question based on the provided context.\",\n+                SourceDocuments = new List<Document<T>>(),\n+                Citations = new List<string>(),\n+                ConfidenceScore = 0.0\n+            };\n+        }\n+\n+        // Build prompt with context and query\n+        var promptBuilder = new StringBuilder();\n+        promptBuilder.AppendLine(\"Context documents:\");\n+        promptBuilder.AppendLine();\n+\n+        var citations = new List<string>();\n+        for (int i = 0; i < contextList.Count; i++)\n+        {\n+            var doc = contextList[i];\n+            var citationNum = i + 1;\n+\n+            promptBuilder.AppendLine($\"[{citationNum}] {TruncateText(doc.Content, 500)}\");\n+            promptBuilder.AppendLine();\n+\n+            citations.Add($\"[{citationNum}] Document ID: {doc.Id}\");\n+        }\n+\n+        promptBuilder.AppendLine($\"Question: {query}\");\n+        promptBuilder.AppendLine();\n+        promptBuilder.AppendLine(\"Answer based on the context:\");\n+\n+        // Generate answer using neural network\n+        var fullPrompt = promptBuilder.ToString();\n+        var generatedAnswer = Generate(fullPrompt);\n+\n+        // Calculate confidence based on retrieval scores\n+        var avgScore = contextList\n+            .Where(d => d.HasRelevanceScore)\n+            .Select(d => Convert.ToDouble(d.RelevanceScore))\n+            .DefaultIfEmpty(0.6)\n+            .Average();\n+\n+        var confidenceScore = Math.Min(1.0, Math.Max(0.0, avgScore));\n+\n+        return new GroundedAnswer<T>\n+        {\n+            Query = query,\n+            Answer = generatedAnswer,\n+            SourceDocuments = contextList,\n+            Citations = citations,\n+            ConfidenceScore = confidenceScore\n+        };\n+    }\n+\n+    private List<int> TokenizeText(string text)\n+    {\n+        // Simple word-based tokenization (production would use BPE or WordPiece)\n+        var words = text.Split(new[] { ' ', '\\t', '\\n', '\\r', '.', ',', '!', '?', ';', ':' },\n+            StringSplitOptions.RemoveEmptyEntries);\n+\n+        var tokens = new List<int>();\n+        foreach (var word in words)\n+        {\n+            // Hash word to token ID (deterministic mapping)\n+            var tokenId = Math.Abs(word.ToLowerInvariant().GetHashCode()) % _vocabularySize;\n+            tokens.Add(tokenId);\n+        }\n+\n+        return tokens;\n+    }\n+\n+    private string DetokenizeText(List<int> tokens)\n+    {\n+        // Simplified detokenization\n+        var words = tokens.Select(t => $\"token_{t}\");\n+        return string.Join(\" \", words);\n+    }",
    "path": "src/RetrievalAugmentedGeneration/Generators/NeuralGenerator.cs",
    "commit_id": "3503d418e63f8ea60886e3219a79e56205bc9100",
    "original_commit_id": "aaf7a4a8027b8bb88ec10c6b3b8f1d9ba7d9e40e",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Critical: Detokenization produces placeholder text, not actual language.**\n\nThe detokenization logic returns placeholder strings like `\"token_123 token_456 token_789\"` rather than actual text. This means:\n\n1. `Generate()` will return unusable output (e.g., \"token_42 token_1337 token_891\")\n2. `GenerateGrounded()` will include this gibberish in the answer field\n3. End users will receive meaningless responses\n\nThis contradicts the \"production-ready\" claim in the class documentation (line 14) and makes the generator unsuitable for actual use.\n\n\nConsider either:\n- Implementing proper detokenization with a real vocabulary mapping, or  \n- Updating documentation to clarify this is a stub/prototype implementation and not production-ready\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T14:50:59Z",
    "updated_at": "2025-11-05T14:51:03Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494887211",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887211"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494887211"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887211/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 213,
    "original_start_line": 213,
    "start_side": "RIGHT",
    "line": 218,
    "original_line": 218,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 218,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887233",
    "pull_request_review_id": 3422589426,
    "id": 2494887233,
    "node_id": "PRRC_kwDOKSXUF86UtPVB",
    "diff_hunk": "@@ -1,11 +1,15 @@\n-using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.Interfaces;",
    "path": "src/RetrievalAugmentedGeneration/Rerankers/RerankerBase.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "aaf7a4a8027b8bb88ec10c6b3b8f1d9ba7d9e40e",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Remove duplicate using statement.**\n\nThe `using AiDotNet.Interfaces;` directive appears twice (lines 2 and 3).\n\n\nApply this diff to remove the duplicate:\n\n```diff\n using AiDotNet.Helpers;\n using AiDotNet.Interfaces;\n-using AiDotNet.Interfaces;\n \n using AiDotNet.RetrievalAugmentedGeneration.Models;\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\nusing AiDotNet.Helpers;\nusing AiDotNet.Interfaces;\n\nusing AiDotNet.RetrievalAugmentedGeneration.Models;\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Rerankers/RerankerBase.cs around lines 2 to\n3, there is a duplicated using directive for AiDotNet.Interfaces; remove the\nredundant second occurrence (keep a single using AiDotNet.Interfaces;) and\nensure no other identical duplicate using statements remain in the file.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T14:51:00Z",
    "updated_at": "2025-11-05T14:51:04Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494887233",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887233"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494887233"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887233/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 2,
    "original_start_line": 2,
    "start_side": "RIGHT",
    "line": 3,
    "original_line": 3,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 4,
    "position": 4,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887253",
    "pull_request_review_id": 3422589426,
    "id": 2494887253,
    "node_id": "PRRC_kwDOKSXUF86UtPVV",
    "diff_hunk": "@@ -142,31 +151,50 @@ protected virtual void ValidateTopK(int topK)\n     /// - Makes scores consistent with other retrievers/rerankers\n     /// </para>\n     /// </remarks>\n-    protected IList<Document> NormalizeScores(IList<Document> documents)\n+    protected IList<Document<T>> NormalizeScores(IList<Document<T>> documents)\n     {\n-        var docsWithScores = documents.Where(d => d.RelevanceScore.HasValue).ToList();\n+        var docsWithScores = documents.Where(d => d.HasRelevanceScore).ToList();\n         if (docsWithScores.Count == 0)\n             return documents;\n \n-        var scores = docsWithScores.Select(d => d.RelevanceScore!.Value).ToList();\n-        var minScore = scores.Min();\n-        var maxScore = scores.Max();\n-        var range = maxScore - minScore;\n+        var scores = docsWithScores.Select(d => d.RelevanceScore).ToList();\n+        if (scores.Count == 0)\n+            return documents;",
    "path": "src/RetrievalAugmentedGeneration/Rerankers/RerankerBase.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "aaf7a4a8027b8bb88ec10c6b3b8f1d9ba7d9e40e",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Remove redundant count check.**\n\nThe check `if (scores.Count == 0)` is redundant. Since `scores` is derived from `docsWithScores` via `Select`, if `docsWithScores.Count > 0` (checked on line 157), then `scores.Count` must also be greater than 0.\n\n\nApply this diff to remove the redundant check:\n\n```diff\n var scores = docsWithScores.Select(d => d.RelevanceScore).ToList();\n-if (scores.Count == 0)\n-    return documents;\n-\n var minScore = scores[0];\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        var scores = docsWithScores.Select(d => d.RelevanceScore).ToList();\n        var minScore = scores[0];\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Rerankers/RerankerBase.cs around lines\n161-162, remove the redundant check \"if (scores.Count == 0) return documents;\"\nbecause scores is derived from docsWithScores and the method already verifies\ndocsWithScores.Count > 0 earlier (line 157); delete these two lines so behavior\nremains unchanged and rely on the existing docsWithScores emptiness check to\nguard against empty sequences.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T14:51:00Z",
    "updated_at": "2025-11-05T14:51:04Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494887253",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887253"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494887253"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887253/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 161,
    "original_start_line": 161,
    "start_side": "RIGHT",
    "line": 162,
    "original_line": 162,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 86,
    "position": 86,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887279",
    "pull_request_review_id": 3422589426,
    "id": 2494887279,
    "node_id": "PRRC_kwDOKSXUF86UtPVv",
    "diff_hunk": "@@ -0,0 +1,194 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\n+{\n+    /// <summary>\n+    /// BM25 (Best Matching 25) retrieval algorithm for sparse keyword-based search.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class BM25Retriever<T> : RetrieverBase<T>\n+    {\n+        private readonly IDocumentStore<T> _documentStore;\n+        private readonly T _k1;\n+        private readonly T _b;\n+\n+        public BM25Retriever(IDocumentStore<T> documentStore, int defaultTopK = 5, double k1 = 1.5, double b = 0.75) \n+            : base(defaultTopK)\n+        {\n+            if (documentStore == null)\n+                throw new ArgumentNullException(nameof(documentStore));\n+\n+            _documentStore = documentStore;\n+            _k1 = NumOps.FromDouble(k1);\n+            _b = NumOps.FromDouble(b);\n+        }\n+\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\n+        {\n+            var queryTerms = Tokenize(query);\n+            var scores = new Dictionary<string, T>();\n+\n+            var candidates = _documentStore.GetAll().ToList();\n+            var corpusStats = BuildCorpusStatistics(candidates);\n+\n+            foreach (var doc in candidates.Where(d => MatchesFilters(d, metadataFilters)))\n+            {\n+                var score = NumOps.Zero;\n+                \n+                foreach (var term in queryTerms)\n+                {\n+                    var termScore = CalculateBM25Term(doc.Id, term, corpusStats);\n+                    score = NumOps.Add(score, termScore);\n+                }\n+\n+                scores[doc.Id] = score;\n+            }\n+\n+            var results = scores\n+                .OrderByDescending(kv => kv.Value)\n+                .Take(topK)\n+                .Select(kv =>\n+                {\n+                    var doc = candidates.FirstOrDefault(d => d.Id == kv.Key);\n+                    if (doc != null)\n+                    {\n+                        doc.RelevanceScore = kv.Value;\n+                        doc.HasRelevanceScore = true;\n+                    }\n+                    return doc;\n+                })\n+                .Where(d => d != null)\n+                .Cast<Document<T>>();",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/BM25Retriever.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "aaf7a4a8027b8bb88ec10c6b3b8f1d9ba7d9e40e",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Inefficient document lookup after scoring.**\n\nLine 57 performs a linear search through `candidates` for every result in the top K, creating O(topK ├ù N) overhead. This is the same issue present in `TFIDFRetriever`. Store the `Document<T>` reference alongside the score during the scoring phase.\n\n\n\nApply this diff:\n\n```diff\n-            var scores = new Dictionary<string, T>();\n+            var scoredDocs = new List<(Document<T> Doc, T Score)>();\n\n             var candidates = _documentStore.GetAll().ToList();\n             var corpusStats = BuildCorpusStatistics(candidates);\n\n             foreach (var doc in candidates.Where(d => MatchesFilters(d, metadataFilters)))\n             {\n                 var score = NumOps.Zero;\n                 \n                 foreach (var term in queryTerms)\n                 {\n                     var termScore = CalculateBM25Term(doc.Id, term, corpusStats);\n                     score = NumOps.Add(score, termScore);\n                 }\n\n-                scores[doc.Id] = score;\n+                scoredDocs.Add((doc, score));\n             }\n\n-            var results = scores\n-                .OrderByDescending(kv => kv.Value)\n+            var results = scoredDocs\n+                .OrderByDescending(sd => sd.Score)\n                 .Take(topK)\n-                .Select(kv =>\n+                .Select(sd =>\n                 {\n-                    var doc = candidates.FirstOrDefault(d => d.Id == kv.Key);\n-                    if (doc != null)\n-                    {\n-                        doc.RelevanceScore = kv.Value;\n-                        doc.HasRelevanceScore = true;\n-                    }\n-                    return doc;\n-                })\n-                .Where(d => d != null)\n-                .Cast<Document<T>>();\n+                    sd.Doc.RelevanceScore = sd.Score;\n+                    sd.Doc.HasRelevanceScore = true;\n+                    return sd.Doc;\n+                });\n\n             return results;\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T14:51:00Z",
    "updated_at": "2025-11-05T14:51:04Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494887279",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887279"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494887279"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887279/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 52,
    "original_start_line": 52,
    "start_side": "RIGHT",
    "line": 66,
    "original_line": 66,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 66,
    "position": 66,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887302",
    "pull_request_review_id": 3422589426,
    "id": 2494887302,
    "node_id": "PRRC_kwDOKSXUF86UtPWG",
    "diff_hunk": "@@ -0,0 +1,257 @@\n+global using AiDotNet.Interfaces;\n+global using System.Text.RegularExpressions;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers;\n+\n+/// <summary>\n+/// Retrieves documents by identifying entities and their relationships in knowledge graph structures.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for relevance scoring (typically float or double).</typeparam>\n+/// <remarks>\n+/// <para>\n+/// GraphRetriever enhances traditional vector search by leveraging entity recognition and relationship scoring.\n+/// It extracts entities from the query (proper nouns, quoted terms, numbers), then boosts documents that contain\n+/// these entities and demonstrate relationships between them (co-occurrence within proximity). This approach is\n+/// particularly effective for queries requiring structured information or multi-hop reasoning across connected facts.\n+/// The retriever uses a hybrid scoring strategy combining base vector similarity with entity match scores (30% weight)\n+/// and relationship scores (20% weight), making it superior to plain vector search for graph-structured data.\n+/// </para>\n+/// <para><b>For Beginners:</b> Think of this like a detective searching a case file:\n+/// \n+/// A regular search finds documents by overall similarity. GraphRetriever is smarterΓÇöit:\n+/// 1. Identifies key people, places, and things in your question (entities)\n+/// 2. Looks for documents mentioning those entities\n+/// 3. Gives extra points when entities appear close together (showing relationships)\n+/// \n+/// For example, asking \"How does John know Mary?\" works better than \"John Mary relationship\" because:\n+/// - It finds documents mentioning both John AND Mary\n+/// - It prioritizes documents where John and Mary appear near each other\n+/// - It understands you're looking for connections, not just mentions\n+/// \n+/// ```csharp\n+/// var graphRetriever = new GraphRetriever<double>(\n+///     documentStore,\n+///     graphEndpoint: \"http://localhost:7200\",\n+///     graphQueryLanguage: \"SPARQL\",\n+///     maxHops: 3\n+/// );\n+/// \n+/// var results = graphRetriever.Retrieve(\"How does Einstein relate to quantum physics?\", topK: 5);\n+/// // Finds documents with both \"Einstein\" and \"quantum physics\" appearing together\n+/// ```\n+/// \n+/// Why use GraphRetriever:\n+/// - Better at finding connected facts (multi-hop questions)\n+/// - Understands relationships between entities\n+/// - Ideal for knowledge-intensive queries (science, history, technical domains)\n+/// - Works well with structured or semi-structured data\n+/// \n+/// When NOT to use it:\n+/// - Simple factual lookups (basic vector search is faster)\n+/// - Queries without clear entities (abstract concepts, opinions)\n+/// - Documents lacking entity mentions or relationships\n+/// </para>\n+/// </remarks>\n+public class GraphRetriever<T> : RetrieverBase<T>\n+{\n+    private readonly string _graphEndpoint;\n+    private readonly string _graphQueryLanguage;\n+    private readonly int _maxHops;\n+\n+    private readonly IDocumentStore<T> _documentStore;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"GraphRetriever{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"documentStore\">The document store containing indexed documents with entity metadata.</param>\n+    /// <param name=\"graphEndpoint\">The knowledge graph database endpoint URL (e.g., \"http://localhost:7200\" for GraphDB).</param>\n+    /// <param name=\"graphQueryLanguage\">The graph query language used by the endpoint (e.g., \"SPARQL\" for RDF stores, \"Cypher\" for Neo4j).</param>\n+    /// <param name=\"maxHops\">Maximum number of relationship hops to traverse when exploring the graph (1-3 recommended for performance).</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when documentStore, graphEndpoint, or graphQueryLanguage is null.</exception>\n+    /// <exception cref=\"ArgumentOutOfRangeException\">Thrown when maxHops is less than or equal to zero.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This constructor sets up the retriever with your document storage and graph database connection.\n+    /// \n+    /// The maxHops parameter controls how far the retriever looks for connectionsΓÇöthink of it like \"degrees of separation\":\n+    /// - maxHops = 1: Only direct connections (e.g., Einstein ΓåÆ relativity)\n+    /// - maxHops = 2: Friends of friends (e.g., Einstein ΓåÆ relativity ΓåÆ quantum mechanics)\n+    /// - maxHops = 3: Extended network (rarely needed, slower)\n+    /// \n+    /// Most queries work well with maxHops = 2.\n+    /// </para>\n+    /// </remarks>\n+    public GraphRetriever(\n+        IDocumentStore<T> documentStore,\n+        string graphEndpoint,\n+        string graphQueryLanguage,\n+        int maxHops)\n+    {\n+        _documentStore = documentStore ?? throw new ArgumentNullException(nameof(documentStore));\n+        _graphEndpoint = graphEndpoint ?? throw new ArgumentNullException(nameof(graphEndpoint));\n+        _graphQueryLanguage = graphQueryLanguage ?? throw new ArgumentNullException(nameof(graphQueryLanguage));\n+        \n+        if (maxHops <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(maxHops), \"Max hops must be positive\");\n+            \n+        _maxHops = maxHops;\n+    }\n+\n+    /// <summary>\n+    /// Retrieves relevant documents by extracting entities from the query and scoring based on entity matches and relationships.\n+    /// </summary>\n+    /// <param name=\"query\">The validated search query (non-empty).</param>\n+    /// <param name=\"topK\">The validated number of documents to return (positive integer).</param>\n+    /// <param name=\"metadataFilters\">The validated metadata filters for document selection.</param>\n+    /// <returns>Documents ordered by enhanced relevance score (combining vector similarity, entity matches, and relationship proximity).</returns>\n+    /// <exception cref=\"ArgumentException\">Thrown when query is null or whitespace.</exception>\n+    /// <exception cref=\"ArgumentOutOfRangeException\">Thrown when topK is less than or equal to zero.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// This method implements a three-stage retrieval pipeline:\n+    /// 1. Entity Extraction: Uses regex patterns to identify proper nouns, quoted terms, and numbers from the query\n+    /// 2. Enhanced Retrieval: Oversamples documents (topK * 2) with entity filters for comprehensive coverage\n+    /// 3. Relationship Scoring: Calculates entity co-occurrence scores within 200-character windows, then combines:\n+    ///    - Base vector similarity (100% weight)\n+    ///    - Entity match score (30% boost)\n+    ///    - Relationship score (20% boost)\n+    /// \n+    /// The algorithm prioritizes documents where query entities appear together, indicating factual relationships.\n+    /// For production systems, this would integrate with graph databases (e.g., Neo4j, GraphDB) via SPARQL/Cypher queries.\n+    /// The current implementation uses metadata-enhanced vector retrieval as a fallback.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This method does the actual searching. Here's what happens:\n+    /// \n+    /// Step 1: Extract entities from your query\n+    /// - Query: \"How did Marie Curie discover radium?\"\n+    /// - Entities found: [\"Marie Curie\", \"radium\"]\n+    /// \n+    /// Step 2: Find documents mentioning these entities\n+    /// - Gets more documents than needed (topK * 2) to ensure good coverage\n+    /// \n+    /// Step 3: Score documents based on:\n+    /// - Do they mention the entities? (+30% boost)\n+    /// - Do the entities appear near each other? (+20% boost if within ~200 characters)\n+    /// \n+    /// Step 4: Return the best topK documents\n+    /// \n+    /// The result is documents that don't just mention your keywords, but actually connect them together.\n+    /// </para>\n+    /// </remarks>\n+    protected override IEnumerable<Document<T>> RetrieveCore(\n+        string query,\n+        int topK,\n+        Dictionary<string, object> metadataFilters)\n+    {\n+        if (string.IsNullOrWhiteSpace(query))\n+            throw new ArgumentException(\"Query cannot be null or whitespace\", nameof(query));\n+\n+        if (topK <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(topK), \"topK must be positive\");\n+\n+        // Extract entities from query (simple NER)\n+        var entities = ExtractEntities(query);\n+\n+        // For production, this would query a graph database\n+        // Fallback: Use metadata-enhanced vector retrieval\n+        var enhancedFilters = new Dictionary<string, object>(metadataFilters ?? new Dictionary<string, object>());\n+        \n+        // Add entity filters if entities were found\n+        if (entities.Count > 0)\n+        {\n+            enhancedFilters[\"entities\"] = entities;\n+        }",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/GraphRetriever.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "aaf7a4a8027b8bb88ec10c6b3b8f1d9ba7d9e40e",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**DonΓÇÖt add filters that the stores canΓÇÖt satisfy**\n\nInjecting an `\"entities\"` entry into `metadataFilters` causes every document store based on `DocumentStoreBase` to reject all candidates, because their metadata does not contain such a key. Remove this filter (or only add it when indexing stores actually persist entity metadata) and keep entity scoring in the post-processing phase.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Retrievers/GraphRetriever.cs around lines\n158 to 162, the code injects an \"entities\" entry into metadata filters which\ncauses DocumentStoreBase-derived stores to reject all candidates; remove the\nenhancement that adds enhancedFilters[\"entities\"] (or guard it with a check that\nthe target store actually persists entity metadata) so that entity filtering is\nnot applied at indexing/store retrieval time; keep entity-based scoring and\nfiltering in the post-processing phase after documents are retrieved.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T14:51:00Z",
    "updated_at": "2025-11-05T14:51:04Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494887302",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887302"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494887302"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887302/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 158,
    "original_start_line": 158,
    "start_side": "RIGHT",
    "line": 162,
    "original_line": 162,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 162,
    "position": 162,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887331",
    "pull_request_review_id": 3422589426,
    "id": 2494887331,
    "node_id": "PRRC_kwDOKSXUF86UtPWj",
    "diff_hunk": "@@ -0,0 +1,257 @@\n+global using AiDotNet.Interfaces;\n+global using System.Text.RegularExpressions;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers;\n+\n+/// <summary>\n+/// Retrieves documents by identifying entities and their relationships in knowledge graph structures.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for relevance scoring (typically float or double).</typeparam>\n+/// <remarks>\n+/// <para>\n+/// GraphRetriever enhances traditional vector search by leveraging entity recognition and relationship scoring.\n+/// It extracts entities from the query (proper nouns, quoted terms, numbers), then boosts documents that contain\n+/// these entities and demonstrate relationships between them (co-occurrence within proximity). This approach is\n+/// particularly effective for queries requiring structured information or multi-hop reasoning across connected facts.\n+/// The retriever uses a hybrid scoring strategy combining base vector similarity with entity match scores (30% weight)\n+/// and relationship scores (20% weight), making it superior to plain vector search for graph-structured data.\n+/// </para>\n+/// <para><b>For Beginners:</b> Think of this like a detective searching a case file:\n+/// \n+/// A regular search finds documents by overall similarity. GraphRetriever is smarterΓÇöit:\n+/// 1. Identifies key people, places, and things in your question (entities)\n+/// 2. Looks for documents mentioning those entities\n+/// 3. Gives extra points when entities appear close together (showing relationships)\n+/// \n+/// For example, asking \"How does John know Mary?\" works better than \"John Mary relationship\" because:\n+/// - It finds documents mentioning both John AND Mary\n+/// - It prioritizes documents where John and Mary appear near each other\n+/// - It understands you're looking for connections, not just mentions\n+/// \n+/// ```csharp\n+/// var graphRetriever = new GraphRetriever<double>(\n+///     documentStore,\n+///     graphEndpoint: \"http://localhost:7200\",\n+///     graphQueryLanguage: \"SPARQL\",\n+///     maxHops: 3\n+/// );\n+/// \n+/// var results = graphRetriever.Retrieve(\"How does Einstein relate to quantum physics?\", topK: 5);\n+/// // Finds documents with both \"Einstein\" and \"quantum physics\" appearing together\n+/// ```\n+/// \n+/// Why use GraphRetriever:\n+/// - Better at finding connected facts (multi-hop questions)\n+/// - Understands relationships between entities\n+/// - Ideal for knowledge-intensive queries (science, history, technical domains)\n+/// - Works well with structured or semi-structured data\n+/// \n+/// When NOT to use it:\n+/// - Simple factual lookups (basic vector search is faster)\n+/// - Queries without clear entities (abstract concepts, opinions)\n+/// - Documents lacking entity mentions or relationships\n+/// </para>\n+/// </remarks>\n+public class GraphRetriever<T> : RetrieverBase<T>\n+{\n+    private readonly string _graphEndpoint;\n+    private readonly string _graphQueryLanguage;\n+    private readonly int _maxHops;\n+\n+    private readonly IDocumentStore<T> _documentStore;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"GraphRetriever{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"documentStore\">The document store containing indexed documents with entity metadata.</param>\n+    /// <param name=\"graphEndpoint\">The knowledge graph database endpoint URL (e.g., \"http://localhost:7200\" for GraphDB).</param>\n+    /// <param name=\"graphQueryLanguage\">The graph query language used by the endpoint (e.g., \"SPARQL\" for RDF stores, \"Cypher\" for Neo4j).</param>\n+    /// <param name=\"maxHops\">Maximum number of relationship hops to traverse when exploring the graph (1-3 recommended for performance).</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when documentStore, graphEndpoint, or graphQueryLanguage is null.</exception>\n+    /// <exception cref=\"ArgumentOutOfRangeException\">Thrown when maxHops is less than or equal to zero.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This constructor sets up the retriever with your document storage and graph database connection.\n+    /// \n+    /// The maxHops parameter controls how far the retriever looks for connectionsΓÇöthink of it like \"degrees of separation\":\n+    /// - maxHops = 1: Only direct connections (e.g., Einstein ΓåÆ relativity)\n+    /// - maxHops = 2: Friends of friends (e.g., Einstein ΓåÆ relativity ΓåÆ quantum mechanics)\n+    /// - maxHops = 3: Extended network (rarely needed, slower)\n+    /// \n+    /// Most queries work well with maxHops = 2.\n+    /// </para>\n+    /// </remarks>\n+    public GraphRetriever(\n+        IDocumentStore<T> documentStore,\n+        string graphEndpoint,\n+        string graphQueryLanguage,\n+        int maxHops)\n+    {\n+        _documentStore = documentStore ?? throw new ArgumentNullException(nameof(documentStore));\n+        _graphEndpoint = graphEndpoint ?? throw new ArgumentNullException(nameof(graphEndpoint));\n+        _graphQueryLanguage = graphQueryLanguage ?? throw new ArgumentNullException(nameof(graphQueryLanguage));\n+        \n+        if (maxHops <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(maxHops), \"Max hops must be positive\");\n+            \n+        _maxHops = maxHops;\n+    }\n+\n+    /// <summary>\n+    /// Retrieves relevant documents by extracting entities from the query and scoring based on entity matches and relationships.\n+    /// </summary>\n+    /// <param name=\"query\">The validated search query (non-empty).</param>\n+    /// <param name=\"topK\">The validated number of documents to return (positive integer).</param>\n+    /// <param name=\"metadataFilters\">The validated metadata filters for document selection.</param>\n+    /// <returns>Documents ordered by enhanced relevance score (combining vector similarity, entity matches, and relationship proximity).</returns>\n+    /// <exception cref=\"ArgumentException\">Thrown when query is null or whitespace.</exception>\n+    /// <exception cref=\"ArgumentOutOfRangeException\">Thrown when topK is less than or equal to zero.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// This method implements a three-stage retrieval pipeline:\n+    /// 1. Entity Extraction: Uses regex patterns to identify proper nouns, quoted terms, and numbers from the query\n+    /// 2. Enhanced Retrieval: Oversamples documents (topK * 2) with entity filters for comprehensive coverage\n+    /// 3. Relationship Scoring: Calculates entity co-occurrence scores within 200-character windows, then combines:\n+    ///    - Base vector similarity (100% weight)\n+    ///    - Entity match score (30% boost)\n+    ///    - Relationship score (20% boost)\n+    /// \n+    /// The algorithm prioritizes documents where query entities appear together, indicating factual relationships.\n+    /// For production systems, this would integrate with graph databases (e.g., Neo4j, GraphDB) via SPARQL/Cypher queries.\n+    /// The current implementation uses metadata-enhanced vector retrieval as a fallback.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This method does the actual searching. Here's what happens:\n+    /// \n+    /// Step 1: Extract entities from your query\n+    /// - Query: \"How did Marie Curie discover radium?\"\n+    /// - Entities found: [\"Marie Curie\", \"radium\"]\n+    /// \n+    /// Step 2: Find documents mentioning these entities\n+    /// - Gets more documents than needed (topK * 2) to ensure good coverage\n+    /// \n+    /// Step 3: Score documents based on:\n+    /// - Do they mention the entities? (+30% boost)\n+    /// - Do the entities appear near each other? (+20% boost if within ~200 characters)\n+    /// \n+    /// Step 4: Return the best topK documents\n+    /// \n+    /// The result is documents that don't just mention your keywords, but actually connect them together.\n+    /// </para>\n+    /// </remarks>\n+    protected override IEnumerable<Document<T>> RetrieveCore(\n+        string query,\n+        int topK,\n+        Dictionary<string, object> metadataFilters)\n+    {\n+        if (string.IsNullOrWhiteSpace(query))\n+            throw new ArgumentException(\"Query cannot be null or whitespace\", nameof(query));\n+\n+        if (topK <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(topK), \"topK must be positive\");\n+\n+        // Extract entities from query (simple NER)\n+        var entities = ExtractEntities(query);\n+\n+        // For production, this would query a graph database\n+        // Fallback: Use metadata-enhanced vector retrieval\n+        var enhancedFilters = new Dictionary<string, object>(metadataFilters ?? new Dictionary<string, object>());\n+        \n+        // Add entity filters if entities were found\n+        if (entities.Count > 0)\n+        {\n+            enhancedFilters[\"entities\"] = entities;\n+        }\n+\n+        // Retrieve documents with entity context\n+        var documents = _documentStore.GetSimilarWithFilters(\n+            new Vector<T>(new T[0]), // Placeholder\n+            topK * 2, // Oversample for filtering\n+            enhancedFilters\n+        ).ToList();",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/GraphRetriever.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "aaf7a4a8027b8bb88ec10c6b3b8f1d9ba7d9e40e",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Embed the query before calling the document store**\n\nJust like the other retrievers, passing an empty vector disables semantic lookup and risks runtime failures. Add an embedding model dependency and call it here so you feed a real vector into `_documentStore.GetSimilarWithFilters`.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Retrievers/GraphRetriever.cs around lines\n164 to 169, the code passes an empty Vector<T> which disables semantic lookup;\ninject or use the existing embedding model dependency here to compute the query\nembedding (await if async) and pass that real float[]/Vector<T> into\n_documentStore.GetSimilarWithFilters instead of the placeholder, keeping the\noversample topK*2 and applying enhancedFilters; ensure null/empty checks on the\nembedding result and propagate async signatures if needed.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T14:51:01Z",
    "updated_at": "2025-11-05T14:51:04Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494887331",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887331"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494887331"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887331/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 164,
    "original_start_line": 164,
    "start_side": "RIGHT",
    "line": 169,
    "original_line": 169,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 169,
    "position": 169,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887357",
    "pull_request_review_id": 3422589426,
    "id": 2494887357,
    "node_id": "PRRC_kwDOKSXUF86UtPW9",
    "diff_hunk": "@@ -0,0 +1,257 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers;\n+\n+/// <summary>\n+/// Retrieves documents by matching against multiple vector representations per document for improved precision.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for relevance scoring (typically float or double).</typeparam>\n+/// <remarks>\n+/// <para>\n+/// MultiVectorRetriever represents each document with multiple embedding vectors instead of a single one,\n+/// enabling capture of different semantic aspects (summaries, key sentences, topics) within the same document.\n+/// This approach is particularly effective for long documents with diverse content or when queries might match\n+/// different parts of a document. The retriever generates scores from each vector representation and aggregates\n+/// them using configurable methods (max, mean, weighted) to produce a final relevance score. This technique\n+/// improves both precision and recall compared to single-vector retrieval, especially for complex documents.\n+/// </para>\n+/// <para><b>For Beginners:</b> Think of this like having multiple \"angles\" to describe the same book:\n+/// \n+/// Regular retrieval:\n+/// - Each book has ONE summary\n+/// - Search compares your question to that ONE summary\n+/// \n+/// Multi-vector retrieval:\n+/// - Each book has MULTIPLE summaries (intro, chapters, conclusion, key topics)\n+/// - Search compares your question to ALL summaries\n+/// - Returns books that match ANY of their summaries\n+/// \n+/// For example, searching \"neural networks\" in a machine learning textbook:\n+/// - Chapter 1 summary: \"Introduction to AI and data\"\n+/// - Chapter 5 summary: \"Deep learning and neural networks\" ΓåÉ MATCH!\n+/// - Chapter 10 summary: \"Practical applications\"\n+/// \n+/// ```csharp\n+/// var retriever = new MultiVectorRetriever<double>(\n+///     documentStore,\n+///     vectorsPerDocument: 5,        // 5 vectors per document\n+///     aggregationMethod: \"max\"       // Use best match\n+/// );\n+/// \n+/// var results = retriever.Retrieve(\"quantum computing applications\", topK: 5);\n+/// // Finds documents where ANY of the 5 vectors match your query well\n+/// ```\n+/// \n+/// Why use MultiVectorRetriever:\n+/// - Better for long documents with multiple topics\n+/// - Captures diverse aspects of complex content\n+/// - Improves recall (finds more relevant matches)\n+/// - Ideal for technical papers, books, reports\n+/// \n+/// When NOT to use it:\n+/// - Short documents (single vector is sufficient and faster)\n+/// - Storage-constrained systems (requires 3-5x more storage)\n+/// - Real-time systems (slower than single-vector retrieval)\n+/// - Documents with uniform content (no benefit from multiple vectors)\n+/// </para>\n+/// </remarks>\n+public class MultiVectorRetriever<T> : RetrieverBase<T>\n+{\n+    private readonly int _vectorsPerDocument;\n+    private readonly string _aggregationMethod;\n+\n+    private readonly IDocumentStore<T> _documentStore;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"MultiVectorRetriever{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"documentStore\">The document store containing documents with multiple vector representations.</param>\n+    /// <param name=\"vectorsPerDocument\">Number of embedding vectors per document (typically 3-5 for optimal balance).</param>\n+    /// <param name=\"aggregationMethod\">Method for combining vector scores: \"max\" (best match), \"mean\" (average), or \"weighted\" (position-based weights).</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when documentStore or aggregationMethod is null.</exception>\n+    /// <exception cref=\"ArgumentOutOfRangeException\">Thrown when vectorsPerDocument is less than or equal to zero.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This sets up the retriever with your multi-vector document store.\n+    /// \n+    /// Aggregation methods explained:\n+    /// - \"max\": Use the BEST matching vector (recommended for precise queries)\n+    /// - \"mean\": AVERAGE all vector scores (recommended for general queries)\n+    /// - \"weighted\": Give MORE importance to earlier vectors (e.g., summaries > details)\n+    /// \n+    /// Common configurations:\n+    /// - Technical papers: 5 vectors, \"max\" aggregation\n+    /// - General documents: 3 vectors, \"mean\" aggregation\n+    /// - Hierarchical content: 5 vectors, \"weighted\" aggregation\n+    /// </para>\n+    /// </remarks>\n+    public MultiVectorRetriever(\n+        IDocumentStore<T> documentStore,\n+        int vectorsPerDocument,\n+        string aggregationMethod)\n+    {\n+        _documentStore = documentStore ?? throw new ArgumentNullException(nameof(documentStore));\n+        \n+        if (vectorsPerDocument <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(vectorsPerDocument), \"Vectors per document must be positive\");\n+            \n+        _vectorsPerDocument = vectorsPerDocument;\n+        _aggregationMethod = aggregationMethod ?? throw new ArgumentNullException(nameof(aggregationMethod));\n+    }\n+\n+    /// <summary>\n+    /// Retrieves documents by comparing the query against all vector representations and aggregating scores.\n+    /// </summary>\n+    /// <param name=\"query\">The validated search query (non-empty).</param>\n+    /// <param name=\"topK\">The validated number of documents to return (positive integer).</param>\n+    /// <param name=\"metadataFilters\">The validated metadata filters for document selection.</param>\n+    /// <returns>Documents ordered by aggregated relevance score (highest first).</returns>\n+    /// <exception cref=\"ArgumentException\">Thrown when query is null or whitespace.</exception>\n+    /// <exception cref=\"ArgumentOutOfRangeException\">Thrown when topK is less than or equal to zero.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// This method implements multi-vector retrieval through:\n+    /// 1. Oversampling: Retrieves topK * vectorsPerDocument * 2 candidate vectors for comprehensive coverage\n+    /// 2. Grouping: Groups vectors by base document ID (format: \"docId_vector_N\")\n+    /// 3. Aggregation: Combines vector scores using the specified method:\n+    ///    - Max: Takes highest score among all vectors\n+    ///    - Mean: Computes arithmetic mean of all scores\n+    ///    - Weighted: Applies decreasing weights (1/1, 1/2, 1/3, ...) to vectors by position\n+    /// 4. Ranking: Sorts documents by aggregated scores and returns top-K\n+    /// \n+    /// The aggregation method significantly impacts behavior:\n+    /// - Max favors documents with at least one strong match (high precision)\n+    /// - Mean favors documents with consistent matches across vectors (balanced)\n+    /// - Weighted favors documents with strong primary vector matches (hierarchical content)\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This is where the actual searching happens:\n+    /// \n+    /// Step 1: Compare your query to ALL vectors of ALL documents\n+    /// - Document A: [vector1=0.9, vector2=0.3, vector3=0.7]\n+    /// - Document B: [vector1=0.6, vector2=0.8, vector3=0.5]\n+    /// \n+    /// Step 2: Combine scores based on aggregation method\n+    /// - Max: Document A = 0.9, Document B = 0.8\n+    /// - Mean: Document A = 0.63, Document B = 0.63\n+    /// - Weighted: Document A = 0.76, Document B = 0.67\n+    /// \n+    /// Step 3: Return best documents\n+    /// \n+    /// This ensures you find documents even if only PART of them matches your query!\n+    /// </para>\n+    /// </remarks>\n+    protected override IEnumerable<Document<T>> RetrieveCore(\n+        string query,\n+        int topK,\n+        Dictionary<string, object> metadataFilters)\n+    {\n+        if (string.IsNullOrWhiteSpace(query))\n+            throw new ArgumentException(\"Query cannot be null or whitespace\", nameof(query));\n+\n+        if (topK <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(topK), \"topK must be positive\");\n+\n+        // Generate query embedding (would use actual embedding model in production)\n+        var queryVector = new Vector<T>(new T[0]); // Placeholder\n+\n+        // Retrieve documents and their multiple vectors\n+        var allDocuments = _documentStore.GetSimilarWithFilters(\n+            queryVector,\n+            topK * _vectorsPerDocument * 2, // Oversample\n+            metadataFilters ?? new Dictionary<string, object>()\n+        ).ToList();",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/MultiVectorRetriever.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "aaf7a4a8027b8bb88ec10c6b3b8f1d9ba7d9e40e",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Real query embeddings are required**\n\n`new Vector<T>(new T[0])` is just an empty placeholder, so the multi-vector search never executes a meaningful similarity lookup and may throw dimension errors. Inject whichever embedding model produced the stored vectors, embed the query here, and pass that real vector to `_documentStore.GetSimilarWithFilters`.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Retrievers/MultiVectorRetriever.cs around\nlines 160 to 168, the code creates an empty placeholder query vector which\nprevents a real similarity search and can cause dimension errors; replace the\nplaceholder with a real embedding by calling the same embedding model (or\nservice) used when indexing documents, ensure the returned vector matches the\nstored vectors' dimensionality, and pass that vector to\n_documentStore.GetSimilarWithFilters; also add null/length checks and surface a\nhelpful error or fallback if embedding generation fails or dimensions mismatch\nbefore invoking the document store.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T14:51:01Z",
    "updated_at": "2025-11-05T14:51:04Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494887357",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887357"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494887357"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887357/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 160,
    "original_start_line": 160,
    "start_side": "RIGHT",
    "line": 168,
    "original_line": 168,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 168,
    "position": 168,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887384",
    "pull_request_review_id": 3422589426,
    "id": 2494887384,
    "node_id": "PRRC_kwDOKSXUF86UtPXY",
    "diff_hunk": "@@ -0,0 +1,263 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers;\n+\n+/// <summary>\n+/// Retrieves precise small chunks for matching but returns complete parent documents for comprehensive context.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for relevance scoring (typically float or double).</typeparam>\n+/// <remarks>\n+/// <para>\n+/// ParentDocumentRetriever solves the \"chunk size dilemma\" in RAG systems: small chunks enable precise matching\n+/// but lack context, while large chunks provide context but reduce precision. This retriever uses a two-tier\n+/// approachΓÇösearch against small chunks (e.g., paragraphs) for accuracy, then return their larger parent documents\n+/// (e.g., full sections or pages) for complete context. This ensures the LLM receives sufficient information to\n+/// generate accurate answers while maintaining high retrieval precision. The retriever can optionally include\n+/// neighboring chunks to expand context boundaries. This pattern is particularly effective for structured content\n+/// (technical docs, research papers, legal documents) where individual paragraphs are meaningful but answers require\n+/// broader context.\n+/// </para>\n+/// <para><b>For Beginners:</b> Think of this like a two-step library search:\n+/// \n+/// The Problem:\n+/// - Small chunks (paragraphs): Easy to match precisely, BUT not enough context\n+/// - Large chunks (whole pages): Plenty of context, BUT hard to match precisely\n+/// \n+/// The Solution - Parent Document Retrieval:\n+/// 1. Search using SMALL chunks for precision\n+/// 2. Return the LARGE parent document for context\n+/// \n+/// Real-world example:\n+/// Query: \"How does photosynthesis work?\"\n+/// \n+/// Small chunk matches: \"...chlorophyll absorbs light energy...\" (paragraph 3)\n+/// Returns: Full page containing introduction + detailed process + diagram\n+/// \n+/// ```csharp\n+/// var retriever = new ParentDocumentRetriever<double>(\n+///     documentStore,\n+///     chunkSize: 256,                    // Small chunks for precision\n+///     parentSize: 2048,                  // Large parents for context\n+///     includeNeighboringChunks: true     // Add nearby chunks too\n+/// );\n+/// \n+/// var results = retriever.Retrieve(\"explain quantum entanglement\", topK: 3);\n+/// // Finds precise paragraphs but returns full sections with complete explanation\n+/// ```\n+/// \n+/// Why use ParentDocumentRetriever:\n+/// - Best of both worlds: precise matching + complete context\n+/// - Ideal for technical documentation and research papers\n+/// - Reduces LLM hallucinations (more context = better answers)\n+/// - Works great with structured content (headings, sections, chapters)\n+/// \n+/// When NOT to use it:\n+/// - Very short documents (chunks = parents already)\n+/// - Documents with redundant content (wastes context window)\n+/// - When you need ONLY the matching excerpt (use regular retrieval)\n+/// - Memory-constrained systems (returns more content per match)\n+/// </para>\n+/// </remarks>\n+public class ParentDocumentRetriever<T> : RetrieverBase<T>\n+{\n+    private readonly IDocumentStore<T> _documentStore;\n+    private readonly int _chunkSize;\n+    private readonly int _parentSize;\n+    private readonly bool _includeNeighboringChunks;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"ParentDocumentRetriever{T}\"/> class.\n+    /// </summary>\n+    /// <param name=\"documentStore\">The document store containing chunked documents with parent metadata.</param>\n+    /// <param name=\"chunkSize\">Character length of child chunks used for matching (typically 128-512 characters).</param>\n+    /// <param name=\"parentSize\">Character length of parent documents returned (typically 1024-4096 characters).</param>\n+    /// <param name=\"includeNeighboringChunks\">Whether to include adjacent chunks around the matched chunk (expands context boundaries).</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown when documentStore is null.</exception>\n+    /// <exception cref=\"ArgumentOutOfRangeException\">Thrown when chunkSize or parentSize is less than or equal to zero.</exception>\n+    /// <exception cref=\"ArgumentException\">Thrown when parentSize is less than chunkSize.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> This configures how the retriever balances precision vs. context.\n+    /// \n+    /// Recommended configurations:\n+    /// \n+    /// Technical Documentation:\n+    /// - chunkSize: 256 (1-2 paragraphs)\n+    /// - parentSize: 2048 (full section)\n+    /// - includeNeighboringChunks: true (add context before/after)\n+    /// \n+    /// Research Papers:\n+    /// - chunkSize: 512 (paragraph or two)\n+    /// - parentSize: 4096 (entire subsection)\n+    /// - includeNeighboringChunks: false (rely on section boundaries)\n+    /// \n+    /// General Content:\n+    /// - chunkSize: 128 (few sentences)\n+    /// - parentSize: 1024 (multiple paragraphs)\n+    /// - includeNeighboringChunks: true (smooth transitions)\n+    /// \n+    /// The includeNeighboringChunks parameter is helpful when chunk boundaries might split important context.\n+    /// </para>\n+    /// </remarks>\n+    public ParentDocumentRetriever(\n+        IDocumentStore<T> documentStore,\n+        int chunkSize,\n+        int parentSize,\n+        bool includeNeighboringChunks)\n+    {\n+        _documentStore = documentStore ?? throw new ArgumentNullException(nameof(documentStore));\n+        \n+        if (chunkSize <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(chunkSize), \"Chunk size must be positive\");\n+            \n+        if (parentSize <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(parentSize), \"Parent size must be positive\");\n+            \n+        if (parentSize < chunkSize)\n+            throw new ArgumentException(\"Parent size must be greater than or equal to chunk size\");\n+            \n+        _chunkSize = chunkSize;\n+        _parentSize = parentSize;\n+        _includeNeighboringChunks = includeNeighboringChunks;\n+    }\n+\n+    /// <summary>\n+    /// Retrieves parent documents by matching against child chunks and reconstructing full parent context.\n+    /// </summary>\n+    /// <param name=\"query\">The validated search query (non-empty).</param>\n+    /// <param name=\"topK\">The validated number of parent documents to return (positive integer).</param>\n+    /// <param name=\"metadataFilters\">The validated metadata filters for document selection.</param>\n+    /// <returns>Parent documents ordered by their best child chunk's relevance score (highest first).</returns>\n+    /// <exception cref=\"ArgumentException\">Thrown when query is null or whitespace.</exception>\n+    /// <exception cref=\"ArgumentOutOfRangeException\">Thrown when topK is less than or equal to zero.</exception>\n+    /// <remarks>\n+    /// <para>\n+    /// This method implements a hierarchical retrieval pipeline:\n+    /// 1. Oversampling: Retrieves topK * 3 child chunks to ensure sufficient parent document coverage\n+    /// 2. Parent Grouping: Groups chunks by parent_id metadata field\n+    /// 3. Score Aggregation: Assigns each parent the MAXIMUM score of its child chunks (best match wins)\n+    /// 4. Optional Expansion: If includeNeighboringChunks=true, concatenates all matching chunks for each parent\n+    /// 5. Deduplication: Returns unique parent documents (multiple chunks may belong to same parent)\n+    /// \n+    /// The retriever expects chunks to have metadata:\n+    /// - \"parent_id\": Identifier of the parent document\n+    /// - \"chunk_index\": Position of this chunk within parent (optional)\n+    /// - \"chunk_start\": Character offset where chunk begins (optional)\n+    /// \n+    /// Parent documents are reconstructed by combining chunk content and filtering chunk-specific metadata.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Here's how the retrieval process works:\n+    /// \n+    /// Step 1: Find the best matching small chunks\n+    /// - Query: \"neural network backpropagation\"\n+    /// - Matches: [Chunk 5 from Doc A (score=0.9), Chunk 12 from Doc A (score=0.7), Chunk 3 from Doc B (score=0.8)]\n+    /// \n+    /// Step 2: Group chunks by parent document\n+    /// - Doc A: Chunk 5 (0.9), Chunk 12 (0.7)\n+    /// - Doc B: Chunk 3 (0.8)\n+    /// \n+    /// Step 3: Assign parent score = best child score\n+    /// - Doc A: 0.9 (from Chunk 5)\n+    /// - Doc B: 0.8 (from Chunk 3)\n+    /// \n+    /// Step 4: Return parent documents (full context!)\n+    /// - Doc A: Complete section on backpropagation including introduction, math, examples\n+    /// - Doc B: Complete chapter on neural network training\n+    /// \n+    /// This gives your LLM ALL the context it needs to provide a complete answer!\n+    /// </para>\n+    /// </remarks>\n+    protected override IEnumerable<Document<T>> RetrieveCore(\n+        string query,\n+        int topK,\n+        Dictionary<string, object> metadataFilters)\n+    {\n+        if (string.IsNullOrWhiteSpace(query))\n+            throw new ArgumentException(\"Query cannot be null or whitespace\", nameof(query));\n+\n+        if (topK <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(topK), \"topK must be positive\");\n+\n+        // Retrieve chunks at higher K to ensure we get enough parent documents\n+        var chunkK = topK * 3;\n+        \n+        // Use the document store to find similar chunks\n+        // The chunks should have metadata indicating their parent document\n+        var similarChunks = _documentStore.GetSimilarWithFilters(\n+            new Vector<T>(new T[0]), // Will be set by actual embedding\n+            chunkK,\n+            metadataFilters ?? new Dictionary<string, object>()\n+        ).ToList();",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/ParentDocumentRetriever.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "aaf7a4a8027b8bb88ec10c6b3b8f1d9ba7d9e40e",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Parent retrieval needs a real query embedding**\n\nCalling `GetSimilarWithFilters` with `new Vector<T>(new T[0])` defeats the whole point of dense chunk retrieval and may crash stores expecting a concrete dimension. Add an embedding model dependency to this retriever, embed `query`, and pass that vector instead of the empty placeholder.\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T14:51:01Z",
    "updated_at": "2025-11-05T14:51:04Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494887384",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887384"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494887384"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887384/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 192,
    "original_start_line": 192,
    "start_side": "RIGHT",
    "line": 196,
    "original_line": 196,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 196,
    "position": 196,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887403",
    "pull_request_review_id": 3422589426,
    "id": 2494887403,
    "node_id": "PRRC_kwDOKSXUF86UtPXr",
    "diff_hunk": "@@ -1,11 +1,14 @@\n-using AiDotNet.RetrievalAugmentedGeneration.Interfaces;\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.Interfaces;",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/RetrieverBase.cs",
    "commit_id": "3503d418e63f8ea60886e3219a79e56205bc9100",
    "original_commit_id": "aaf7a4a8027b8bb88ec10c6b3b8f1d9ba7d9e40e",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Remove duplicate using directive.**\n\nThe `using AiDotNet.Interfaces;` directive appears on both lines 2 and 3.\n\n\nApply this diff:\n\n```diff\n using AiDotNet.Helpers;\n using AiDotNet.Interfaces;\n-using AiDotNet.Interfaces;\n using AiDotNet.RetrievalAugmentedGeneration.Models;\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\nusing AiDotNet.Interfaces;\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Retrievers/RetrieverBase.cs around lines 2\nto 3, there's a duplicate using directive \"using AiDotNet.Interfaces;\" ΓÇö remove\nthe redundant second occurrence so the file contains the directive only once;\nensure no other using statements are affected and save the file.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T14:51:01Z",
    "updated_at": "2025-11-05T14:51:04Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494887403",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887403"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494887403"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494887403/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 2,
    "original_start_line": 2,
    "start_side": "RIGHT",
    "line": 3,
    "original_line": 3,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 4,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494980055",
    "pull_request_review_id": 3422732638,
    "id": 2494980055,
    "node_id": "PRRC_kwDOKSXUF86Utl_X",
    "diff_hunk": "@@ -0,0 +1,287 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using Newtonsoft.Json.Linq;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Net.Http;\n+using System.Text;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// ChromaDB-based document store designed for simplicity and developer experience.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// ChromaDB is an open-source vector database that emphasizes ease of use while maintaining\n+/// high performance for similarity search operations.\n+/// </remarks>\n+public class ChromaDBDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly HttpClient _httpClient;\n+    private readonly string _collectionName;\n+    private int _vectorDimension;\n+    private int _documentCount;\n+    private readonly Dictionary<string, VectorDocument<T>> _cache;\n+\n+    public override int DocumentCount => _documentCount;\n+    public override int VectorDimension => _vectorDimension;\n+\n+    public ChromaDBDocumentStore(string endpoint, string collectionName, string apiKey)\n+    {\n+        if (string.IsNullOrWhiteSpace(endpoint))\n+            throw new ArgumentException(\"Endpoint cannot be empty\", nameof(endpoint));\n+        if (string.IsNullOrWhiteSpace(collectionName))\n+            throw new ArgumentException(\"Collection name cannot be empty\", nameof(collectionName));\n+\n+        _httpClient = new HttpClient { BaseAddress = new Uri(endpoint) };\n+        if (!string.IsNullOrWhiteSpace(apiKey))\n+            _httpClient.DefaultRequestHeaders.Add(\"X-Chroma-Token\", apiKey);\n+\n+        _collectionName = collectionName;\n+        _vectorDimension = 0;\n+        _documentCount = 0;\n+        _cache = new Dictionary<string, VectorDocument<T>>();\n+\n+        EnsureCollection();\n+    }\n+\n+    private void EnsureCollection()\n+    {\n+        var payload = new { name = _collectionName };\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(payload),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        _httpClient.PostAsync(\"/api/v1/collections\", content).Wait();\n+    }\n+\n+    protected override void AddCore(VectorDocument<T> vectorDocument)\n+    {\n+        if (_vectorDimension == 0)\n+            _vectorDimension = vectorDocument.Embedding.Length;\n+\n+        _cache[vectorDocument.Document.Id] = vectorDocument;\n+\n+        var embedding = vectorDocument.Embedding.ToArray().Select(v => Convert.ToDouble(v)).ToList();\n+        \n+        var payload = new\n+        {\n+            ids = new[] { vectorDocument.Document.Id },\n+            embeddings = new[] { embedding },\n+            documents = new[] { vectorDocument.Document.Content },\n+            metadatas = new[] { vectorDocument.Document.Metadata }\n+        };\n+\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(payload),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        var response = _httpClient.PostAsync($\"/api/v1/collections/{_collectionName}/add\", content).Result;\n+        if (response.IsSuccessStatusCode)\n+            _documentCount++;\n+    }\n+\n+    protected override void AddBatchCore(IList<VectorDocument<T>> vectorDocuments)\n+    {\n+        if (vectorDocuments.Count == 0) return;\n+        \n+        if (_vectorDimension == 0)\n+            _vectorDimension = vectorDocuments[0].Embedding.Length;\n+\n+        foreach (var vd in vectorDocuments)\n+            _cache[vd.Document.Id] = vd;\n+\n+        var ids = vectorDocuments.Select(vd => vd.Document.Id).ToList();\n+        var embeddings = vectorDocuments.Select(vd => \n+            vd.Embedding.ToArray().Select(v => Convert.ToDouble(v)).ToList()).ToList();\n+        var documents = vectorDocuments.Select(vd => vd.Document.Content).ToList();\n+        var metadatas = vectorDocuments.Select(vd => vd.Document.Metadata).ToList();\n+\n+        var payload = new { ids, embeddings, documents, metadatas };\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(payload),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        var response = _httpClient.PostAsync($\"/api/v1/collections/{_collectionName}/add\", content).Result;\n+        if (response.IsSuccessStatusCode)\n+            _documentCount += vectorDocuments.Count;\n+    }\n+\n+    protected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)\n+    {\n+        var embedding = queryVector.ToArray().Select(v => Convert.ToDouble(v)).ToList();\n+        \n+        var payload = new\n+        {\n+            query_embeddings = new[] { embedding },\n+            n_results = topK\n+        };\n+\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(payload),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        var response = _httpClient.PostAsync($\"/api/v1/collections/{_collectionName}/query\", content).Result;\n+        var responseContent = response.Content.ReadAsStringAsync().Result;\n+        var result = JObject.Parse(responseContent);\n+\n+        var results = new List<Document<T>>();\n+        var ids = result[\"ids\"]?[0];\n+        var documents = result[\"documents\"]?[0];\n+        var metadatas = result[\"metadatas\"]?[0];\n+        var distances = result[\"distances\"]?[0];\n+\n+        if (ids == null || documents == null) return results;\n+\n+        for (int i = 0; i < ids.Count(); i++)\n+        {\n+            var idToken = ids[i];\n+            var docToken = documents[i];\n+            if (idToken == null || docToken == null) continue;\n+\n+            var id = idToken.ToString();\n+            var doc = docToken.ToString();\n+            var metadataObj = metadatas?[i]?.ToObject<Dictionary<string, string>>() ?? new Dictionary<string, string>();\n+            var metadata = new Dictionary<string, object>();\n+            foreach (var kvp in metadataObj)\n+                metadata[kvp.Key] = kvp.Value;\n+\n+            var distance = distances != null ? Convert.ToDouble(distances[i]) : 0.0;\n+\n+            var document = new Document<T>(id, doc, metadata);\n+            document.RelevanceScore = NumOps.FromDouble(1.0 / (1.0 + distance));",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/ChromaDBDocumentStore.cs",
    "commit_id": "e972697ed5a758045091402fb3f80cadcdfa8829",
    "original_commit_id": "e972697ed5a758045091402fb3f80cadcdfa8829",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Fix metadata deserialization and set relevance flag.**\n\nTwo issues:\n\n1. **Metadata type mismatch** (line 152-155): Deserializing to `Dictionary<string, string>` strips numeric and boolean types, breaking filter comparisons in `MatchesFilters`. Use `Dictionary<string, object>` instead.\n2. **Missing flag** (line 160): `HasRelevanceScore` is never set to `true`, which downstream code may rely on.\n\n\n\nApply this diff:\n\n```diff\n             var id = idToken.ToString();\n             var doc = docToken.ToString();\n-            var metadataObj = metadatas?[i]?.ToObject<Dictionary<string, string>>() ?? new Dictionary<string, string>();\n-            var metadata = new Dictionary<string, object>();\n-            foreach (var kvp in metadataObj)\n-                metadata[kvp.Key] = kvp.Value;\n+            var metadata = metadatas?[i]?.ToObject<Dictionary<string, object>>() ?? new Dictionary<string, object>();\n\n             var distance = distances != null ? Convert.ToDouble(distances[i]) : 0.0;\n\n             var document = new Document<T>(id, doc, metadata);\n             document.RelevanceScore = NumOps.FromDouble(1.0 / (1.0 + distance));\n+            document.HasRelevanceScore = true;\n\n             results.Add(document);\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/ChromaDBDocumentStore.cs\naround lines 152 to 160, the metadata is deserialized into\nDictionary<string,string> which strips numeric/boolean types and breaks filter\ncomparisons, and the document's HasRelevanceScore flag is never set; change the\ndeserialization target to Dictionary<string, object> (preserve original value\ntypes) and copy those values into the metadata Dictionary<string,object> used to\nconstruct the Document<T>, then after setting document.RelevanceScore set\ndocument.HasRelevanceScore = true so downstream code knows the score is present.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commit 8e06c50",
    "created_at": "2025-11-05T15:09:18Z",
    "updated_at": "2025-11-05T15:41:45Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494980055",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494980055"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494980055"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494980055/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 152,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 160,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 160,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494980071",
    "pull_request_review_id": 3422732638,
    "id": 2494980071,
    "node_id": "PRRC_kwDOKSXUF86Utl_n",
    "diff_hunk": "@@ -0,0 +1,345 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using Newtonsoft.Json.Linq;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Net.Http;\n+using System.Text;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// Elasticsearch-based document store providing hybrid search capabilities (BM25 + dense vectors).\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Elasticsearch combines traditional full-text search (BM25) with vector similarity search,\n+/// making it ideal for hybrid retrieval scenarios where both keyword matching and semantic\n+/// similarity are important.\n+/// </remarks>\n+public class ElasticsearchDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly HttpClient _httpClient;\n+    private readonly string _indexName;\n+    private int _vectorDimension;\n+    private int _documentCount;\n+    private readonly Dictionary<string, VectorDocument<T>> _cache;\n+\n+    public override int DocumentCount => _documentCount;\n+    public override int VectorDimension => _vectorDimension;\n+\n+    public ElasticsearchDocumentStore(string endpoint, string indexName, string apiKey, string username, string password)\n+    {\n+        if (string.IsNullOrWhiteSpace(endpoint))\n+            throw new ArgumentException(\"Endpoint cannot be empty\", nameof(endpoint));\n+        if (string.IsNullOrWhiteSpace(indexName))\n+            throw new ArgumentException(\"Index name cannot be empty\", nameof(indexName));\n+\n+        _httpClient = new HttpClient { BaseAddress = new Uri(endpoint) };\n+        \n+        if (!string.IsNullOrWhiteSpace(apiKey))\n+            _httpClient.DefaultRequestHeaders.Add(\"Authorization\", $\"ApiKey {apiKey}\");\n+        else if (!string.IsNullOrWhiteSpace(username) && !string.IsNullOrWhiteSpace(password))\n+        {\n+            var auth = Convert.ToBase64String(Encoding.UTF8.GetBytes($\"{username}:{password}\"));\n+            _httpClient.DefaultRequestHeaders.Add(\"Authorization\", $\"Basic {auth}\");\n+        }\n+\n+        _indexName = indexName.ToLowerInvariant();\n+        _vectorDimension = 0;\n+        _documentCount = 0;\n+        _cache = new Dictionary<string, VectorDocument<T>>();\n+\n+        EnsureIndex();\n+    }\n+\n+    private void EnsureIndex()\n+    {\n+        var checkResponse = _httpClient.GetAsync($\"/{_indexName}\").Result;\n+        if (checkResponse.IsSuccessStatusCode) \n+        {\n+            UpdateDocumentCount();\n+            return;\n+        }\n+\n+        var mapping = new\n+        {\n+            mappings = new\n+            {\n+                properties = new\n+                {\n+                    id = new { type = \"keyword\" },\n+                    content = new { type = \"text\" },\n+                    embedding = new { type = \"dense_vector\", dims = 1536 },\n+                    metadata = new { type = \"object\", enabled = true }\n+                }\n+            }\n+        };\n+\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(mapping),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        _httpClient.PutAsync($\"/{_indexName}\", content).Wait();",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/ElasticsearchDocumentStore.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "e972697ed5a758045091402fb3f80cadcdfa8829",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Make the dense vector dimension configurable (1536 breaks other models).**\n\nThe mapping hardcodes `dims = 1536`, but `AddCore` later sets `_vectorDimension` from the first embedding. If that embedding isnΓÇÖt length 1536, Elasticsearch rejects the insert/search with a 400. Please supply the actual dimensionΓÇöeither by accepting it in the constructor or by creating/updating the mapping lazily once `_vectorDimension` is known.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/ElasticsearchDocumentStore.cs\naround lines 68 to 87, the mapping currently hardcodes \"dims = 1536\" which can\nbreak for other embedding sizes; change the implementation to use the actual\nvector dimension instead: accept an int vectorDimension in the class constructor\n(or a nullable field), or defer creating/updating the index mapping until\n_vectorDimension is known (after AddCore reads the first embedding), then\nserialize the mapping with dims = _vectorDimension and PUT/PUT-mapping only\nonce; ensure you check whether the index already exists and handle concurrent\ncalls (create-if-not-exists or update mapping safely) so Elasticsearch receives\nthe correct dense_vector dims.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commit 8e06c50",
    "created_at": "2025-11-05T15:09:18Z",
    "updated_at": "2025-11-05T15:41:43Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494980071",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494980071"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494980071"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494980071/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 70,
    "original_start_line": 68,
    "start_side": "RIGHT",
    "line": 89,
    "original_line": 87,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 87,
    "position": 89,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494980089",
    "pull_request_review_id": 3422732638,
    "id": 2494980089,
    "node_id": "PRRC_kwDOKSXUF86Utl_5",
    "diff_hunk": "@@ -0,0 +1,345 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using Newtonsoft.Json.Linq;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Net.Http;\n+using System.Text;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// Elasticsearch-based document store providing hybrid search capabilities (BM25 + dense vectors).\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Elasticsearch combines traditional full-text search (BM25) with vector similarity search,\n+/// making it ideal for hybrid retrieval scenarios where both keyword matching and semantic\n+/// similarity are important.\n+/// </remarks>\n+public class ElasticsearchDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly HttpClient _httpClient;\n+    private readonly string _indexName;\n+    private int _vectorDimension;\n+    private int _documentCount;\n+    private readonly Dictionary<string, VectorDocument<T>> _cache;\n+\n+    public override int DocumentCount => _documentCount;\n+    public override int VectorDimension => _vectorDimension;\n+\n+    public ElasticsearchDocumentStore(string endpoint, string indexName, string apiKey, string username, string password)\n+    {\n+        if (string.IsNullOrWhiteSpace(endpoint))\n+            throw new ArgumentException(\"Endpoint cannot be empty\", nameof(endpoint));\n+        if (string.IsNullOrWhiteSpace(indexName))\n+            throw new ArgumentException(\"Index name cannot be empty\", nameof(indexName));\n+\n+        _httpClient = new HttpClient { BaseAddress = new Uri(endpoint) };\n+        \n+        if (!string.IsNullOrWhiteSpace(apiKey))\n+            _httpClient.DefaultRequestHeaders.Add(\"Authorization\", $\"ApiKey {apiKey}\");\n+        else if (!string.IsNullOrWhiteSpace(username) && !string.IsNullOrWhiteSpace(password))\n+        {\n+            var auth = Convert.ToBase64String(Encoding.UTF8.GetBytes($\"{username}:{password}\"));\n+            _httpClient.DefaultRequestHeaders.Add(\"Authorization\", $\"Basic {auth}\");\n+        }\n+\n+        _indexName = indexName.ToLowerInvariant();\n+        _vectorDimension = 0;\n+        _documentCount = 0;\n+        _cache = new Dictionary<string, VectorDocument<T>>();\n+\n+        EnsureIndex();\n+    }\n+\n+    private void EnsureIndex()\n+    {\n+        var checkResponse = _httpClient.GetAsync($\"/{_indexName}\").Result;\n+        if (checkResponse.IsSuccessStatusCode) \n+        {\n+            UpdateDocumentCount();\n+            return;\n+        }\n+\n+        var mapping = new\n+        {\n+            mappings = new\n+            {\n+                properties = new\n+                {\n+                    id = new { type = \"keyword\" },\n+                    content = new { type = \"text\" },\n+                    embedding = new { type = \"dense_vector\", dims = 1536 },\n+                    metadata = new { type = \"object\", enabled = true }\n+                }\n+            }\n+        };\n+\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(mapping),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        _httpClient.PutAsync($\"/{_indexName}\", content).Wait();\n+    }\n+\n+    private void UpdateDocumentCount()\n+    {\n+        var response = _httpClient.GetAsync($\"/{_indexName}/_count\").Result;\n+        if (response.IsSuccessStatusCode)\n+        {\n+            var responseContent = response.Content.ReadAsStringAsync().Result;\n+            var result = JObject.Parse(responseContent);\n+            _documentCount = result[\"count\"]?.Value<int>() ?? 0;\n+        }\n+    }\n+\n+    protected override void AddCore(VectorDocument<T> vectorDocument)\n+    {\n+        if (_vectorDimension == 0)\n+            _vectorDimension = vectorDocument.Embedding.Length;\n+\n+        _cache[vectorDocument.Document.Id] = vectorDocument;\n+\n+        var embedding = vectorDocument.Embedding.ToArray().Select(v => Convert.ToDouble(v)).ToArray();\n+        \n+        var doc = new\n+        {\n+            id = vectorDocument.Document.Id,\n+            content = vectorDocument.Document.Content,\n+            embedding,\n+            metadata = vectorDocument.Document.Metadata\n+        };\n+\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(doc),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        var response = _httpClient.PutAsync($\"/{_indexName}/_doc/{vectorDocument.Document.Id}\", content).Result;\n+        if (response.IsSuccessStatusCode)\n+            _documentCount++;\n+    }\n+\n+    protected override void AddBatchCore(IList<VectorDocument<T>> vectorDocuments)\n+    {\n+        if (vectorDocuments.Count == 0) return;\n+        \n+        if (_vectorDimension == 0)\n+            _vectorDimension = vectorDocuments[0].Embedding.Length;\n+\n+        foreach (var vd in vectorDocuments)\n+            _cache[vd.Document.Id] = vd;\n+\n+        var bulkBody = new StringBuilder();\n+        foreach (var vd in vectorDocuments)\n+        {\n+            var indexAction = new { index = new { _index = _indexName, _id = vd.Document.Id } };\n+            bulkBody.AppendLine(Newtonsoft.Json.JsonConvert.SerializeObject(indexAction));\n+\n+            var embedding = vd.Embedding.ToArray().Select(v => Convert.ToDouble(v)).ToArray();\n+            var doc = new\n+            {\n+                id = vd.Document.Id,\n+                content = vd.Document.Content,\n+                embedding,\n+                metadata = vd.Document.Metadata\n+            };\n+            bulkBody.AppendLine(Newtonsoft.Json.JsonConvert.SerializeObject(doc));\n+        }\n+\n+        var content = new StringContent(bulkBody.ToString(), Encoding.UTF8, \"application/x-ndjson\");\n+        var response = _httpClient.PostAsync($\"/{_indexName}/_bulk\", content).Result;\n+        if (response.IsSuccessStatusCode)\n+            _documentCount += vectorDocuments.Count;\n+    }\n+\n+    protected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)\n+    {\n+        var embedding = queryVector.ToArray().Select(v => Convert.ToDouble(v)).ToArray();\n+        \n+        var query = new\n+        {\n+            size = topK,\n+            query = new\n+            {\n+                script_score = new\n+                {\n+                    query = new { match_all = new { } },\n+                    script = new\n+                    {\n+                        source = \"cosineSimilarity(params.query_vector, 'embedding') + 1.0\",\n+                        @params = new { query_vector = embedding }\n+                    }\n+                }\n+            }\n+        };\n+\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(query),\n+            Encoding.UTF8,\n+            \"application/json\");\n+",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/ElasticsearchDocumentStore.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "e972697ed5a758045091402fb3f80cadcdfa8829",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Apply metadata filters in the ES query.**\n\n`GetSimilarCore` always runs `match_all`, so requests that include `metadataFilters` ignore the callerΓÇÖs constraints. Please translate `metadataFilters` into an appropriate `bool`/`filter` clause (e.g., `term`/`range`) before calculating scores.\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commit 8e06c50",
    "created_at": "2025-11-05T15:09:18Z",
    "updated_at": "2025-11-05T15:41:46Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494980089",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494980089"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494980089"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494980089/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 198,
    "original_start_line": 167,
    "start_side": "RIGHT",
    "line": 217,
    "original_line": 186,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 186,
    "position": 217,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494980098",
    "pull_request_review_id": 3422732638,
    "id": 2494980098,
    "node_id": "PRRC_kwDOKSXUF86UtmAC",
    "diff_hunk": "@@ -0,0 +1,345 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using Newtonsoft.Json.Linq;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Net.Http;\n+using System.Text;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// Elasticsearch-based document store providing hybrid search capabilities (BM25 + dense vectors).\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Elasticsearch combines traditional full-text search (BM25) with vector similarity search,\n+/// making it ideal for hybrid retrieval scenarios where both keyword matching and semantic\n+/// similarity are important.\n+/// </remarks>\n+public class ElasticsearchDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly HttpClient _httpClient;\n+    private readonly string _indexName;\n+    private int _vectorDimension;\n+    private int _documentCount;\n+    private readonly Dictionary<string, VectorDocument<T>> _cache;\n+\n+    public override int DocumentCount => _documentCount;\n+    public override int VectorDimension => _vectorDimension;\n+\n+    public ElasticsearchDocumentStore(string endpoint, string indexName, string apiKey, string username, string password)\n+    {\n+        if (string.IsNullOrWhiteSpace(endpoint))\n+            throw new ArgumentException(\"Endpoint cannot be empty\", nameof(endpoint));\n+        if (string.IsNullOrWhiteSpace(indexName))\n+            throw new ArgumentException(\"Index name cannot be empty\", nameof(indexName));\n+\n+        _httpClient = new HttpClient { BaseAddress = new Uri(endpoint) };\n+        \n+        if (!string.IsNullOrWhiteSpace(apiKey))\n+            _httpClient.DefaultRequestHeaders.Add(\"Authorization\", $\"ApiKey {apiKey}\");\n+        else if (!string.IsNullOrWhiteSpace(username) && !string.IsNullOrWhiteSpace(password))\n+        {\n+            var auth = Convert.ToBase64String(Encoding.UTF8.GetBytes($\"{username}:{password}\"));\n+            _httpClient.DefaultRequestHeaders.Add(\"Authorization\", $\"Basic {auth}\");\n+        }\n+\n+        _indexName = indexName.ToLowerInvariant();\n+        _vectorDimension = 0;\n+        _documentCount = 0;\n+        _cache = new Dictionary<string, VectorDocument<T>>();\n+\n+        EnsureIndex();\n+    }\n+\n+    private void EnsureIndex()\n+    {\n+        var checkResponse = _httpClient.GetAsync($\"/{_indexName}\").Result;\n+        if (checkResponse.IsSuccessStatusCode) \n+        {\n+            UpdateDocumentCount();\n+            return;\n+        }\n+\n+        var mapping = new\n+        {\n+            mappings = new\n+            {\n+                properties = new\n+                {\n+                    id = new { type = \"keyword\" },\n+                    content = new { type = \"text\" },\n+                    embedding = new { type = \"dense_vector\", dims = 1536 },\n+                    metadata = new { type = \"object\", enabled = true }\n+                }\n+            }\n+        };\n+\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(mapping),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        _httpClient.PutAsync($\"/{_indexName}\", content).Wait();\n+    }\n+\n+    private void UpdateDocumentCount()\n+    {\n+        var response = _httpClient.GetAsync($\"/{_indexName}/_count\").Result;\n+        if (response.IsSuccessStatusCode)\n+        {\n+            var responseContent = response.Content.ReadAsStringAsync().Result;\n+            var result = JObject.Parse(responseContent);\n+            _documentCount = result[\"count\"]?.Value<int>() ?? 0;\n+        }\n+    }\n+\n+    protected override void AddCore(VectorDocument<T> vectorDocument)\n+    {\n+        if (_vectorDimension == 0)\n+            _vectorDimension = vectorDocument.Embedding.Length;\n+\n+        _cache[vectorDocument.Document.Id] = vectorDocument;\n+\n+        var embedding = vectorDocument.Embedding.ToArray().Select(v => Convert.ToDouble(v)).ToArray();\n+        \n+        var doc = new\n+        {\n+            id = vectorDocument.Document.Id,\n+            content = vectorDocument.Document.Content,\n+            embedding,\n+            metadata = vectorDocument.Document.Metadata\n+        };\n+\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(doc),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        var response = _httpClient.PutAsync($\"/{_indexName}/_doc/{vectorDocument.Document.Id}\", content).Result;\n+        if (response.IsSuccessStatusCode)\n+            _documentCount++;\n+    }\n+\n+    protected override void AddBatchCore(IList<VectorDocument<T>> vectorDocuments)\n+    {\n+        if (vectorDocuments.Count == 0) return;\n+        \n+        if (_vectorDimension == 0)\n+            _vectorDimension = vectorDocuments[0].Embedding.Length;\n+\n+        foreach (var vd in vectorDocuments)\n+            _cache[vd.Document.Id] = vd;\n+\n+        var bulkBody = new StringBuilder();\n+        foreach (var vd in vectorDocuments)\n+        {\n+            var indexAction = new { index = new { _index = _indexName, _id = vd.Document.Id } };\n+            bulkBody.AppendLine(Newtonsoft.Json.JsonConvert.SerializeObject(indexAction));\n+\n+            var embedding = vd.Embedding.ToArray().Select(v => Convert.ToDouble(v)).ToArray();\n+            var doc = new\n+            {\n+                id = vd.Document.Id,\n+                content = vd.Document.Content,\n+                embedding,\n+                metadata = vd.Document.Metadata\n+            };\n+            bulkBody.AppendLine(Newtonsoft.Json.JsonConvert.SerializeObject(doc));\n+        }\n+\n+        var content = new StringContent(bulkBody.ToString(), Encoding.UTF8, \"application/x-ndjson\");\n+        var response = _httpClient.PostAsync($\"/{_indexName}/_bulk\", content).Result;\n+        if (response.IsSuccessStatusCode)\n+            _documentCount += vectorDocuments.Count;\n+    }\n+\n+    protected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)\n+    {\n+        var embedding = queryVector.ToArray().Select(v => Convert.ToDouble(v)).ToArray();\n+        \n+        var query = new\n+        {\n+            size = topK,\n+            query = new\n+            {\n+                script_score = new\n+                {\n+                    query = new { match_all = new { } },\n+                    script = new\n+                    {\n+                        source = \"cosineSimilarity(params.query_vector, 'embedding') + 1.0\",\n+                        @params = new { query_vector = embedding }\n+                    }\n+                }\n+            }\n+        };\n+\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(query),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        var response = _httpClient.PostAsync($\"/{_indexName}/_search\", content).Result;\n+        var responseContent = response.Content.ReadAsStringAsync().Result;\n+        var result = JObject.Parse(responseContent);\n+\n+        var results = new List<Document<T>>();\n+        var hits = result[\"hits\"]?[\"hits\"];\n+        if (hits == null) return results;\n+\n+        foreach (var hit in hits)\n+        {\n+            var source = hit[\"_source\"];\n+            if (source == null) continue;\n+\n+            var id = source[\"id\"]?.ToString() ?? string.Empty;\n+            var docContent = source[\"content\"]?.ToString() ?? string.Empty;\n+            var metadataObj = source[\"metadata\"]?.ToObject<Dictionary<string, string>>() ?? new Dictionary<string, string>();\n+            var metadata = new Dictionary<string, object>();\n+            foreach (var kvp in metadataObj)\n+                metadata[kvp.Key] = kvp.Value;\n+\n+            var score = Convert.ToDouble(hit[\"_score\"]);\n+\n+            var document = new Document<T>(id, docContent, metadata);\n+            document.RelevanceScore = NumOps.FromDouble(score);\n+\n+            results.Add(document);",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/ElasticsearchDocumentStore.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "e972697ed5a758045091402fb3f80cadcdfa8829",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Preserve metadata types and mark relevance on the returned documents.**\n\nCasting `_source[\"metadata\"]` to `Dictionary<string,string>` drops numeric/bool values and breaks range equality checks later. Deserialize into `Dictionary<string,object>` (or `JsonElement`) and rehydrate accordingly. Also, set `HasRelevanceScore = true` when assigning `RelevanceScore` so downstream code sees that the document was scored.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/ElasticsearchDocumentStore.cs\naround lines 191 to 212, the code currently deserializes _source[\"metadata\"] to\nDictionary<string,string> which strips numeric and boolean types and then\nrecreates a Dictionary<string,object>, losing original types; change the\ndeserialization to Dictionary<string,object> (or use JsonElement and convert\nvalues to appropriate CLR types) so metadata retains numeric/bool types when\nadding to the document metadata dictionary, and after setting\ndocument.RelevanceScore also set document.HasRelevanceScore = true so downstream\ncode recognizes the score.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commit 8e06c50",
    "created_at": "2025-11-05T15:09:18Z",
    "updated_at": "2025-11-05T15:41:48Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494980098",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494980098"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494980098"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494980098/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 222,
    "original_start_line": 191,
    "start_side": "RIGHT",
    "line": 243,
    "original_line": 212,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 212,
    "position": 243,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494980108",
    "pull_request_review_id": 3422732638,
    "id": 2494980108,
    "node_id": "PRRC_kwDOKSXUF86UtmAM",
    "diff_hunk": "@@ -0,0 +1,337 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores\n+{\n+    /// <summary>\n+    /// FAISS-inspired document store with indexed vectors for efficient similarity search.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This implementation provides an in-memory simulation of Facebook AI Similarity Search (FAISS),\n+    /// using integer-based indexing for fast vector lookup. It maintains both document storage and\n+    /// a separate index mapping for optimized retrieval operations.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> FAISS is Facebook's ultra-fast vector search library.\n+    /// \n+    /// Think of it like a specialized phone book for vectors:\n+    /// - Each document gets a unique number (index)\n+    /// - Vectors are stored separately for faster search\n+    /// - Can handle millions of vectors efficiently\n+    /// \n+    /// This in-memory version is good for:\n+    /// - Testing FAISS-style indexing patterns\n+    /// - Medium-sized collections (< 100K documents)\n+    /// - Prototyping before deploying real FAISS\n+    /// \n+    /// Real FAISS provides:\n+    /// - GPU acceleration for billion-scale search\n+    /// - Advanced indexing (IVF, HNSW, Product Quantization)\n+    /// - Sub-millisecond search on huge datasets\n+    /// </para>\n+    /// </remarks>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class FAISSDocumentStore<T> : DocumentStoreBase<T>\n+    {\n+        private readonly Dictionary<string, VectorDocument<T>> _documents;\n+        private readonly Dictionary<int, Vector<T>> _indexedVectors;\n+        private int _vectorDimension;\n+        private int _currentIndex;\n+\n+        /// <summary>\n+        /// Gets the number of documents currently stored.\n+        /// </summary>\n+        public override int DocumentCount => _documents.Count;\n+\n+        /// <summary>\n+        /// Gets the dimensionality of vectors stored in this index.\n+        /// </summary>\n+        public override int VectorDimension => _vectorDimension;\n+\n+        /// <summary>\n+        /// Initializes a new instance of the FAISSDocumentStore class.\n+        /// </summary>\n+        /// <param name=\"initialCapacity\">The initial capacity for internal dictionaries (default: 1000).</param>\n+        /// <exception cref=\"ArgumentException\">Thrown when initial capacity is not positive.</exception>\n+        /// <remarks>\n+        /// <para><b>For Beginners:</b> Creates a new FAISS-style document store.\n+        /// \n+        /// Example:\n+        /// <code>\n+        /// // Create a store for embeddings\n+        /// var store = new FAISSDocumentStore&lt;float&gt;();\n+        /// \n+        /// // Create a larger store\n+        /// var bigStore = new FAISSDocumentStore&lt;double&gt;(initialCapacity: 50000);\n+        /// </code>\n+        /// \n+        /// The initial capacity is just a hint - the store can grow beyond it.\n+        /// </para>\n+        /// </remarks>\n+        public FAISSDocumentStore(int initialCapacity = 1000)\n+        {\n+            if (initialCapacity <= 0)\n+                throw new ArgumentException(\"Initial capacity must be greater than zero\", nameof(initialCapacity));\n+\n+            _documents = new Dictionary<string, VectorDocument<T>>(initialCapacity);\n+            _indexedVectors = new Dictionary<int, Vector<T>>(initialCapacity);\n+            _vectorDimension = 0;\n+            _currentIndex = 0;\n+        }\n+\n+        /// <summary>\n+        /// Core logic for adding a single vector document with automatic indexing.\n+        /// </summary>\n+        /// <param name=\"vectorDocument\">The validated vector document to add.</param>\n+        /// <remarks>\n+        /// <para>\n+        /// Assigns a unique integer index to each document for fast lookup in the vector index.\n+        /// The first document added determines the vector dimension for all subsequent additions.\n+        /// </para>\n+        /// </remarks>\n+        protected override void AddCore(VectorDocument<T> vectorDocument)\n+        {\n+            if (_documents.Count == 0)\n+            {\n+                _vectorDimension = vectorDocument.Embedding.Length;\n+            }\n+\n+            var index = _currentIndex++;\n+            _documents[vectorDocument.Document.Id] = vectorDocument;\n+            _indexedVectors[index] = vectorDocument.Embedding;\n+        }\n+\n+        /// <summary>\n+        /// Core logic for adding multiple vector documents in a batch with automatic indexing.\n+        /// </summary>\n+        /// <param name=\"vectorDocuments\">The validated list of vector documents to add.</param>\n+        /// <exception cref=\"ArgumentException\">Thrown when a document's embedding has inconsistent dimensions.</exception>\n+        /// <remarks>\n+        /// <para>\n+        /// Batch operations are more efficient than adding documents individually. Each document\n+        /// receives a sequential integer index for fast vector lookup.\n+        /// </para>\n+        /// <para><b>For Beginners:</b> Adding many documents at once is faster.\n+        /// \n+        /// Instead of:\n+        /// <code>\n+        /// // Slow - one at a time\n+        /// foreach (var doc in docs)\n+        ///     store.Add(doc);\n+        /// </code>\n+        /// \n+        /// Do this:\n+        /// <code>\n+        /// // Fast - all at once\n+        /// store.AddBatch(docs);\n+        /// </code>\n+        /// </para>\n+        /// </remarks>\n+        protected override void AddBatchCore(IList<VectorDocument<T>> vectorDocuments)\n+        {\n+            if (_vectorDimension == 0 && vectorDocuments.Count > 0)\n+            {\n+                _vectorDimension = vectorDocuments[0].Embedding.Length;\n+            }\n+\n+            foreach (var vectorDocument in vectorDocuments)\n+            {\n+                if (vectorDocument.Embedding.Length != _vectorDimension)\n+                    throw new ArgumentException(\n+                        $\"Vector dimension mismatch in batch. Expected {_vectorDimension}, got {vectorDocument.Embedding.Length}\",\n+                        nameof(vectorDocuments));\n+\n+                var index = _currentIndex++;\n+                _documents[vectorDocument.Document.Id] = vectorDocument;\n+                _indexedVectors[index] = vectorDocument.Embedding;\n+            }\n+        }\n+\n+        /// <summary>\n+        /// Core logic for similarity search using cosine similarity with optional metadata filtering.\n+        /// </summary>\n+        /// <param name=\"queryVector\">The validated query vector.</param>\n+        /// <param name=\"topK\">The validated number of documents to return.</param>\n+        /// <param name=\"metadataFilters\">The validated metadata filters.</param>\n+        /// <returns>Top-k similar documents ordered by cosine similarity score.</returns>\n+        /// <remarks>\n+        /// <para>\n+        /// Searches through all documents, filters by metadata, computes cosine similarity scores,\n+        /// and returns the top-k most similar documents ordered by descending similarity.\n+        /// </para>\n+        /// <para><b>For Beginners:</b> Finds documents most similar to your query.\n+        /// \n+        /// Process:\n+        /// 1. Filter documents by metadata (if filters provided)\n+        /// 2. Calculate similarity between query and each document\n+        /// 3. Sort by similarity score (highest first)\n+        /// 4. Return top-k matches\n+        /// \n+        /// Example:\n+        /// <code>\n+        /// // Find 5 most similar documents\n+        /// var results = store.GetSimilar(queryVector, topK: 5);\n+        /// \n+        /// // Find 10 similar documents from 2024\n+        /// var filters = new Dictionary&lt;string, object&gt; { [\"year\"] = \"2024\" };\n+        /// var filtered = store.GetSimilarWithFilters(queryVector, 10, filters);\n+        /// </code>\n+        /// </para>\n+        /// </remarks>\n+        protected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)\n+        {\n+            var scoredDocuments = new List<(Document<T> Document, T Score)>();\n+\n+            var matchingDocuments = _documents.Values\n+                .Where(vectorDoc => MatchesFilters(vectorDoc.Document, metadataFilters));\n+\n+            foreach (var vectorDoc in matchingDocuments)\n+            {\n+                var similarity = StatisticsHelper<T>.CosineSimilarity(queryVector, vectorDoc.Embedding);\n+                scoredDocuments.Add((vectorDoc.Document, similarity));\n+            }\n+\n+            var results = scoredDocuments\n+                .OrderByDescending(x => x.Score)\n+                .Take(topK)\n+                .Select(x =>\n+                {\n+                    x.Document.RelevanceScore = x.Score;\n+                    x.Document.HasRelevanceScore = true;\n+                    return x.Document;\n+                })\n+                .ToList();\n+\n+            return results;\n+        }\n+\n+        /// <summary>\n+        /// Core logic for retrieving a document by its unique identifier.\n+        /// </summary>\n+        /// <param name=\"documentId\">The validated document ID.</param>\n+        /// <returns>The document if found; otherwise, null.</returns>\n+        /// <remarks>\n+        /// <para><b>For Beginners:</b> Gets a specific document by its ID.\n+        /// \n+        /// Example:\n+        /// <code>\n+        /// var doc = store.GetById(\"doc123\");\n+        /// if (doc != null)\n+        ///     Console.WriteLine($\"Found: {doc.Content}\");\n+        /// </code>\n+        /// </para>\n+        /// </remarks>\n+        protected override Document<T>? GetByIdCore(string documentId)\n+        {\n+            return _documents.TryGetValue(documentId, out var vectorDoc) ? vectorDoc.Document : null;\n+        }\n+\n+        /// <summary>\n+        /// Core logic for removing a document from the store.\n+        /// </summary>\n+        /// <param name=\"documentId\">The validated document ID.</param>\n+        /// <returns>True if the document was found and removed; otherwise, false.</returns>\n+        /// <remarks>\n+        /// <para>\n+        /// Removes the document from storage. When the last document is removed, the vector dimension\n+        /// is reset and indices are cleared, allowing a new dimension on next add.\n+        /// </para>\n+        /// <para><b>For Beginners:</b> Removes a document from the store.\n+        /// \n+        /// Example:\n+        /// <code>\n+        /// if (store.Remove(\"doc123\"))\n+        ///     Console.WriteLine(\"Document removed\");\n+        /// else\n+        ///     Console.WriteLine(\"Document not found\");\n+        /// </code>\n+        /// </para>\n+        /// </remarks>\n+        protected override bool RemoveCore(string documentId)\n+        {\n+            var removed = _documents.Remove(documentId);\n+            if (removed && _documents.Count == 0)\n+            {\n+                _vectorDimension = 0;\n+                _currentIndex = 0;\n+                _indexedVectors.Clear();\n+            }\n+            return removed;\n+        }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/FAISSDocumentStore.cs",
    "commit_id": "e972697ed5a758045091402fb3f80cadcdfa8829",
    "original_commit_id": "e972697ed5a758045091402fb3f80cadcdfa8829",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Keep the FAISS index in sync when removing documents.**\n\n`RemoveCore` deletes the entry from `_documents` but leaves the corresponding embedding in `_indexedVectors`. Over time that leaks memory and makes `_indexedVectors` diverge from the live corpus. Please track the per-document index (e.g., via a reverse map) and delete the entry from `_indexedVectors` when a document is removed.\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commit 8e06c50",
    "created_at": "2025-11-05T15:09:18Z",
    "updated_at": "2025-11-05T15:41:50Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494980108",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494980108"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494980108"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494980108/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 254,
    "original_start_line": 254,
    "start_side": "RIGHT",
    "line": 278,
    "original_line": 264,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 264,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494980122",
    "pull_request_review_id": 3422732638,
    "id": 2494980122,
    "node_id": "PRRC_kwDOKSXUF86UtmAa",
    "diff_hunk": "@@ -0,0 +1,326 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores\n+{\n+    /// <summary>\n+    /// Pinecone-inspired document store with index-based vector organization.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This implementation provides an in-memory simulation of Pinecone, a fully managed vector database service.\n+    /// It organizes documents into named indices and uses cosine similarity for retrieval.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Pinecone is a popular cloud-based vector database.\n+    /// \n+    /// Think of indices like separate databases:\n+    /// - Each index has a unique name (like \"ProductSearchIndex\")\n+    /// - Documents are grouped by use case\n+    /// - Makes it easy to manage multiple vector search applications\n+    /// \n+    /// This in-memory version is good for:\n+    /// - Prototyping before using real Pinecone\n+    /// - Testing Pinecone-style index organization\n+    /// - Small to medium document collections (< 100K documents)\n+    /// \n+    /// Real Pinecone provides:\n+    /// - Fully managed cloud service (no infrastructure to manage)\n+    /// - Auto-scaling to handle any load\n+    /// - Advanced filtering and hybrid search\n+    /// - Sub-50ms query latency at scale\n+    /// </para>\n+    /// </remarks>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class PineconeDocumentStore<T> : DocumentStoreBase<T>\n+    {\n+        private readonly Dictionary<string, VectorDocument<T>> _documents;\n+        private readonly string _indexName;\n+        private int _vectorDimension;\n+\n+        /// <summary>\n+        /// Gets the number of documents currently stored in the index.\n+        /// </summary>\n+        public override int DocumentCount => _documents.Count;\n+\n+        /// <summary>\n+        /// Gets the dimensionality of vectors stored in this index.\n+        /// </summary>\n+        public override int VectorDimension => _vectorDimension;\n+\n+        /// <summary>\n+        /// Initializes a new instance of the PineconeDocumentStore class.\n+        /// </summary>\n+        /// <param name=\"indexName\">The name of the index to organize documents.</param>\n+        /// <param name=\"initialCapacity\">The initial capacity for the internal dictionary (default: 1000).</param>\n+        /// <exception cref=\"ArgumentException\">Thrown when index name is empty or initial capacity is not positive.</exception>\n+        /// <remarks>\n+        /// <para><b>For Beginners:</b> Creates a new Pinecone-style index.\n+        /// \n+        /// Example:\n+        /// <code>\n+        /// // Create an index for product vectors\n+        /// var store = new PineconeDocumentStore&lt;float&gt;(\"ProductIndex\");\n+        /// \n+        /// // Create a larger index for articles\n+        /// var bigStore = new PineconeDocumentStore&lt;double&gt;(\"ArticleIndex\", 10000);\n+        /// </code>\n+        /// \n+        /// The index name helps organize different vector collections.\n+        /// </para>\n+        /// </remarks>\n+        public PineconeDocumentStore(string indexName, int initialCapacity = 1000)\n+        {\n+            if (string.IsNullOrWhiteSpace(indexName))\n+                throw new ArgumentException(\"Index name cannot be empty\", nameof(indexName));\n+            if (initialCapacity <= 0)\n+                throw new ArgumentException(\"Initial capacity must be greater than zero\", nameof(initialCapacity));\n+\n+            _indexName = indexName;\n+            _documents = new Dictionary<string, VectorDocument<T>>(initialCapacity);\n+            _vectorDimension = 0;\n+        }\n+\n+        /// <summary>\n+        /// Core logic for adding a single vector document to the index.\n+        /// </summary>\n+        /// <param name=\"vectorDocument\">The validated vector document to add.</param>\n+        /// <remarks>\n+        /// <para>\n+        /// The first document added determines the vector dimension for the entire index.\n+        /// All subsequent documents must have embeddings of the same dimension.\n+        /// </para>\n+        /// </remarks>\n+        protected override void AddCore(VectorDocument<T> vectorDocument)\n+        {\n+            if (_documents.Count == 0)\n+            {\n+                _vectorDimension = vectorDocument.Embedding.Length;\n+            }\n+\n+            _documents[vectorDocument.Document.Id] = vectorDocument;\n+        }\n+\n+        /// <summary>\n+        /// Core logic for adding multiple vector documents in a batch operation.\n+        /// </summary>\n+        /// <param name=\"vectorDocuments\">The validated list of vector documents to add.</param>\n+        /// <exception cref=\"ArgumentException\">Thrown when a document's embedding has inconsistent dimensions.</exception>\n+        /// <remarks>\n+        /// <para>\n+        /// Batch addition is significantly more efficient than adding documents one at a time.\n+        /// All documents in the batch must have embeddings with the same dimension.\n+        /// </para>\n+        /// <para><b>For Beginners:</b> Adding documents in batches is much faster.\n+        /// \n+        /// Bad (slow):\n+        /// <code>\n+        /// foreach (var doc in documents)\n+        ///     store.Add(doc);\n+        /// </code>\n+        /// \n+        /// Good (fast):\n+        /// <code>\n+        /// store.AddBatch(documents);\n+        /// </code>\n+        /// </para>\n+        /// </remarks>\n+        protected override void AddBatchCore(IList<VectorDocument<T>> vectorDocuments)\n+        {\n+            if (_vectorDimension == 0 && vectorDocuments.Count > 0)\n+            {\n+                _vectorDimension = vectorDocuments[0].Embedding.Length;\n+            }\n+\n+            foreach (var vectorDocument in vectorDocuments)\n+            {\n+                if (vectorDocument.Embedding.Length != _vectorDimension)\n+                    throw new ArgumentException(\n+                        $\"Vector dimension mismatch in batch. Expected {_vectorDimension}, got {vectorDocument.Embedding.Length} for document {vectorDocument.Document.Id}\",\n+                        nameof(vectorDocuments));\n+\n+                _documents[vectorDocument.Document.Id] = vectorDocument;\n+            }\n+        }\n+\n+        /// <summary>\n+        /// Core logic for similarity search using cosine similarity with optional metadata filtering.\n+        /// </summary>\n+        /// <param name=\"queryVector\">The validated query vector.</param>\n+        /// <param name=\"topK\">The validated number of documents to return.</param>\n+        /// <param name=\"metadataFilters\">The validated metadata filters.</param>\n+        /// <returns>Top-k similar documents ordered by cosine similarity score.</returns>\n+        /// <remarks>\n+        /// <para>\n+        /// Performs a similarity search across all documents in the index, optionally filtering\n+        /// by metadata. Results are ordered by decreasing cosine similarity and limited to top-k matches.\n+        /// </para>\n+        /// <para><b>For Beginners:</b> Finds the most similar documents to your query.\n+        /// \n+        /// How it works:\n+        /// 1. Filter documents by metadata (if provided)\n+        /// 2. Calculate similarity between query and each document's embedding\n+        /// 3. Sort by similarity (highest first)\n+        /// 4. Return the top-k best matches\n+        /// \n+        /// Example:\n+        /// <code>\n+        /// // Find 10 most similar products\n+        /// var results = store.GetSimilar(queryVector, topK: 10);\n+        /// \n+        /// // Find similar products in \"Electronics\" category\n+        /// var filters = new Dictionary&lt;string, object&gt; { [\"category\"] = \"Electronics\" };\n+        /// var filtered = store.GetSimilarWithFilters(queryVector, 5, filters);\n+        /// </code>\n+        /// </para>\n+        /// </remarks>\n+        protected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)\n+        {\n+            var scoredDocuments = new List<(Document<T> Document, T Score)>();\n+\n+            var matchingDocuments = _documents.Values\n+                .Where(vectorDoc => MatchesFilters(vectorDoc.Document, metadataFilters));\n+\n+            foreach (var vectorDoc in matchingDocuments)\n+            {\n+                var similarity = StatisticsHelper<T>.CosineSimilarity(queryVector, vectorDoc.Embedding);\n+                scoredDocuments.Add((vectorDoc.Document, similarity));\n+            }\n+\n+            var results = scoredDocuments\n+                .OrderByDescending(x => x.Score)\n+                .Take(topK)\n+                .Select(x =>\n+                {\n+                    x.Document.RelevanceScore = x.Score;\n+                    x.Document.HasRelevanceScore = true;\n+                    return x.Document;\n+                })\n+                .ToList();",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/PineconeDocumentStore.cs",
    "commit_id": "e972697ed5a758045091402fb3f80cadcdfa8829",
    "original_commit_id": "e972697ed5a758045091402fb3f80cadcdfa8829",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Return scored copies instead of mutating cached documents.**\n\nSetting `RelevanceScore` and `HasRelevanceScore` directly on `_documents` leaves stale scores hanging around for future calls. Please clone the document (or otherwise produce a new instance) before attaching the relevance metadata.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/PineconeDocumentStore.cs\naround lines 193 to 202, the code mutates cached document instances by setting\nRelevanceScore and HasRelevanceScore; instead create and return new document\ninstances so the cache isnΓÇÖt modified. Fix by cloning each x.Document (shallow\ncopy or create a new Document populated from x.Document fields) inside the\nSelect, set RelevanceScore and HasRelevanceScore on the clone, and return the\nclone; ensure the original _documents collection is never mutated.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commit 8e06c50",
    "created_at": "2025-11-05T15:09:18Z",
    "updated_at": "2025-11-05T15:41:52Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494980122",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494980122"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494980122"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494980122/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 193,
    "original_start_line": 193,
    "start_side": "RIGHT",
    "line": 206,
    "original_line": 202,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 202,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494980138",
    "pull_request_review_id": 3422732638,
    "id": 2494980138,
    "node_id": "PRRC_kwDOKSXUF86UtmAq",
    "diff_hunk": "@@ -0,0 +1,327 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores\n+{\n+    /// <summary>\n+    /// PostgreSQL with pgvector-inspired document store for relational database vector storage.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This implementation provides an in-memory simulation of PostgreSQL with the pgvector extension,\n+    /// which adds vector similarity search capabilities to the popular relational database.\n+    /// It organizes documents in table-like structures with cosine similarity for retrieval.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> PostgreSQL is a powerful open-source relational database, and pgvector adds AI capabilities.\n+    /// \n+    /// Think of it like a smart database table:\n+    /// - Each table stores documents with vector embeddings\n+    /// - Combines traditional SQL queries with vector search\n+    /// - Leverage existing PostgreSQL infrastructure\n+    /// \n+    /// This in-memory version is good for:\n+    /// - Prototyping pgvector applications\n+    /// - Testing table-based organization\n+    /// - Small to medium collections (< 100K documents)\n+    /// \n+    /// Real PostgreSQL + pgvector provides:\n+    /// - ACID transactions for data integrity\n+    /// - Complex SQL joins with vector search\n+    /// - Proven reliability and scalability\n+    /// - Integration with existing database infrastructure\n+    /// </para>\n+    /// </remarks>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class PostgresVectorDocumentStore<T> : DocumentStoreBase<T>\n+    {\n+        private readonly Dictionary<string, VectorDocument<T>> _documents;\n+        private readonly string _tableName;\n+        private int _vectorDimension;\n+\n+        /// <summary>\n+        /// Gets the number of documents currently stored in the table.\n+        /// </summary>\n+        public override int DocumentCount => _documents.Count;\n+\n+        /// <summary>\n+        /// Gets the dimensionality of vectors stored in this table.\n+        /// </summary>\n+        public override int VectorDimension => _vectorDimension;\n+\n+        /// <summary>\n+        /// Initializes a new instance of the PostgresVectorDocumentStore class.\n+        /// </summary>\n+        /// <param name=\"tableName\">The table name to organize documents.</param>\n+        /// <param name=\"initialCapacity\">The initial capacity for the internal dictionary (default: 1000).</param>\n+        /// <exception cref=\"ArgumentException\">Thrown when table name is empty or initial capacity is not positive.</exception>\n+        /// <remarks>\n+        /// <para><b>For Beginners:</b> Creates a new pgvector-style document table.\n+        /// \n+        /// Example:\n+        /// <code>\n+        /// // Create a table for documents\n+        /// var store = new PostgresVectorDocumentStore&lt;float&gt;(\"documents\");\n+        /// \n+        /// // Create a table for embeddings\n+        /// var embStore = new PostgresVectorDocumentStore&lt;double&gt;(\"embeddings\", 10000);\n+        /// </code>\n+        /// \n+        /// The table name helps organize different document collections.\n+        /// </para>\n+        /// </remarks>\n+        public PostgresVectorDocumentStore(string tableName, int initialCapacity = 1000)\n+        {\n+            if (string.IsNullOrWhiteSpace(tableName))\n+                throw new ArgumentException(\"Table name cannot be empty\", nameof(tableName));\n+            if (initialCapacity <= 0)\n+                throw new ArgumentException(\"Initial capacity must be greater than zero\", nameof(initialCapacity));\n+\n+            _tableName = tableName;\n+            _documents = new Dictionary<string, VectorDocument<T>>(initialCapacity);\n+            _vectorDimension = 0;\n+        }\n+\n+        /// <summary>\n+        /// Core logic for adding a single vector document to the table.\n+        /// </summary>\n+        /// <param name=\"vectorDocument\">The validated vector document to add.</param>\n+        /// <remarks>\n+        /// <para>\n+        /// The first document added determines the vector dimension for all documents in this table.\n+        /// All subsequent documents must have embeddings of the same dimension.\n+        /// </para>\n+        /// </remarks>\n+        protected override void AddCore(VectorDocument<T> vectorDocument)\n+        {\n+            if (_documents.Count == 0)\n+            {\n+                _vectorDimension = vectorDocument.Embedding.Length;\n+            }\n+\n+            _documents[vectorDocument.Document.Id] = vectorDocument;\n+        }\n+\n+        /// <summary>\n+        /// Core logic for adding multiple vector documents in a batch operation.\n+        /// </summary>\n+        /// <param name=\"vectorDocuments\">The validated list of vector documents to add.</param>\n+        /// <exception cref=\"ArgumentException\">Thrown when a document's embedding has inconsistent dimensions.</exception>\n+        /// <remarks>\n+        /// <para>\n+        /// Batch operations are more efficient than adding documents individually, similar to PostgreSQL's\n+        /// bulk insert capabilities. All documents must have embeddings with the same dimension.\n+        /// </para>\n+        /// <para><b>For Beginners:</b> Adding many documents at once is much faster.\n+        /// \n+        /// Slow way:\n+        /// <code>\n+        /// foreach (var doc in documents)\n+        ///     store.Add(doc); // Many individual inserts\n+        /// </code>\n+        /// \n+        /// Fast way:\n+        /// <code>\n+        /// store.AddBatch(documents); // Single bulk insert\n+        /// </code>\n+        /// </para>\n+        /// </remarks>\n+        protected override void AddBatchCore(IList<VectorDocument<T>> vectorDocuments)\n+        {\n+            if (_vectorDimension == 0 && vectorDocuments.Count > 0)\n+            {\n+                _vectorDimension = vectorDocuments[0].Embedding.Length;\n+            }\n+\n+            foreach (var vectorDocument in vectorDocuments)\n+            {\n+                if (vectorDocument.Embedding.Length != _vectorDimension)\n+                    throw new ArgumentException(\n+                        $\"Vector dimension mismatch in batch. Expected {_vectorDimension}, got {vectorDocument.Embedding.Length} for document {vectorDocument.Document.Id}\",\n+                        nameof(vectorDocuments));\n+\n+                _documents[vectorDocument.Document.Id] = vectorDocument;\n+            }\n+        }\n+\n+        /// <summary>\n+        /// Core logic for similarity search using cosine similarity with optional metadata filtering.\n+        /// </summary>\n+        /// <param name=\"queryVector\">The validated query vector.</param>\n+        /// <param name=\"topK\">The validated number of documents to return.</param>\n+        /// <param name=\"metadataFilters\">The validated metadata filters.</param>\n+        /// <returns>Top-k similar documents ordered by cosine similarity score.</returns>\n+        /// <remarks>\n+        /// <para>\n+        /// Performs vector similarity search across all documents in the table, optionally filtering by metadata.\n+        /// In real pgvector, this would use the vector similarity operators (<->, <=>, <#>) in SQL.\n+        /// </para>\n+        /// <para><b>For Beginners:</b> Finds the most similar documents to your query.\n+        /// \n+        /// How it works:\n+        /// 1. Filter documents by metadata (like SQL WHERE clause)\n+        /// 2. Calculate similarity between query and each document\n+        /// 3. Sort by similarity (highest first, like SQL ORDER BY)\n+        /// 4. Return top-k matches (like SQL LIMIT)\n+        /// \n+        /// Example:\n+        /// <code>\n+        /// // Find 10 most similar documents\n+        /// var results = store.GetSimilar(queryVector, topK: 10);\n+        /// \n+        /// // Find similar documents from 2024\n+        /// var filters = new Dictionary&lt;string, object&gt; { [\"year\"] = \"2024\" };\n+        /// var filtered = store.GetSimilarWithFilters(queryVector, 5, filters);\n+        /// </code>\n+        /// </para>\n+        /// </remarks>\n+        protected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)\n+        {\n+            var scoredDocuments = new List<(Document<T> Document, T Score)>();\n+\n+            var matchingDocuments = _documents.Values\n+                .Where(vectorDoc => MatchesFilters(vectorDoc.Document, metadataFilters));\n+\n+            foreach (var vectorDoc in matchingDocuments)\n+            {\n+                var similarity = StatisticsHelper<T>.CosineSimilarity(queryVector, vectorDoc.Embedding);\n+                scoredDocuments.Add((vectorDoc.Document, similarity));\n+            }\n+\n+            var results = scoredDocuments\n+                .OrderByDescending(x => x.Score)\n+                .Take(topK)\n+                .Select(x =>\n+                {\n+                    x.Document.RelevanceScore = x.Score;\n+                    x.Document.HasRelevanceScore = true;\n+                    return x.Document;\n+                })\n+                .ToList();",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/PostgresVectorDocumentStore.cs",
    "commit_id": "e972697ed5a758045091402fb3f80cadcdfa8829",
    "original_commit_id": "e972697ed5a758045091402fb3f80cadcdfa8829",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Avoid leaking query-specific state into stored documents.**\n\nThe selector mutates the cached `Document<T>` by setting `RelevanceScore`/`HasRelevanceScore`. That score persists after the query and shows up in unrelated contexts. Please return a copy (or clone) with the relevance metadata instead of altering the stored instance.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn\nsrc/RetrievalAugmentedGeneration/DocumentStores/PostgresVectorDocumentStore.cs\naround lines 194 to 203, the LINQ selector is mutating the cached Document by\nsetting RelevanceScore/HasRelevanceScore which leaks query-specific state;\ninstead create and return a copy of the Document with the relevance metadata\nset. Fix by instantiating a new Document (or calling an existing Clone/Copy\nmethod) populated from x.Document, set RelevanceScore and HasRelevanceScore on\nthat new instance, and return the new instance so the stored cached document is\nnot modified.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commit 8e06c50",
    "created_at": "2025-11-05T15:09:19Z",
    "updated_at": "2025-11-05T15:41:56Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494980138",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494980138"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494980138"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494980138/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 194,
    "original_start_line": 194,
    "start_side": "RIGHT",
    "line": 207,
    "original_line": 203,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 203,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494980154",
    "pull_request_review_id": 3422732638,
    "id": 2494980154,
    "node_id": "PRRC_kwDOKSXUF86UtmA6",
    "diff_hunk": "@@ -0,0 +1,312 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// Lightweight SQLite-based vector store using the SQLite-VSS extension.\n+/// </summary>\n+/// <remarks>\n+/// <para>\n+/// This implementation provides an in-memory simulation of SQLite with the VSS (Vector Similarity Search) extension,\n+/// which adds vector search capabilities to the serverless, file-based SQLite database. Perfect for\n+/// edge deployments, mobile apps, and development environments.\n+/// </para>\n+/// <para><b>For Beginners:</b> SQLite is a tiny, serverless database that stores data in a single file, and VSS adds AI search.\n+/// \n+/// Think of it like an Excel file with AI superpowers:\n+/// - No server needed - just a file on disk\n+/// - Perfect for apps, IoT devices, mobile\n+/// - Combine SQL queries with vector search\n+/// \n+/// This in-memory version is good for:\n+/// - Prototyping SQLite-VSS applications\n+/// - Testing local vector search\n+/// - Small collections (< 10K documents)\n+/// \n+/// Real SQLite-VSS provides:\n+/// - Zero-configuration deployment\n+/// - Single-file database (easy backup/transfer)\n+/// - ACID transactions for data integrity\n+/// - Perfect for edge AI and mobile apps\n+/// </para>\n+/// </remarks>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+public class SQLiteVSSDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly Dictionary<string, VectorDocument<T>> _store;\n+    private int _vectorDimension;\n+\n+    /// <summary>\n+    /// Gets the number of documents currently stored in the database.\n+    /// </summary>\n+    public override int DocumentCount => _store.Count;\n+\n+    /// <summary>\n+    /// Gets the dimensionality of vectors stored in this database.\n+    /// </summary>\n+    public override int VectorDimension => _vectorDimension;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the SQLiteVSSDocumentStore class.\n+    /// </summary>\n+    /// <param name=\"databasePath\">The path to the SQLite database file.</param>\n+    /// <param name=\"tableName\">The table name to organize documents.</param>\n+    /// <param name=\"vectorDimension\">The dimension of vector embeddings.</param>\n+    /// <exception cref=\"ArgumentException\">Thrown when parameters are invalid.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> Creates a new SQLite vector store.\n+    /// \n+    /// Example:\n+    /// <code>\n+    /// // Create a store with 384-dimensional embeddings\n+    /// var store = new SQLiteVSSDocumentStore&lt;float&gt;(\n+    ///     \"vectors.db\",\n+    ///     \"documents\",\n+    ///     vectorDimension: 384\n+    /// );\n+    /// </code>\n+    /// \n+    /// The database file will be created if it doesn't exist.\n+    /// Vector dimension must be known upfront to create the table properly.\n+    /// </para>\n+    /// </remarks>\n+    public SQLiteVSSDocumentStore(string databasePath, string tableName, int vectorDimension)\n+    {\n+        if (string.IsNullOrWhiteSpace(databasePath))\n+            throw new ArgumentException(\"Database path cannot be empty\", nameof(databasePath));\n+        if (string.IsNullOrWhiteSpace(tableName))\n+            throw new ArgumentException(\"Table name cannot be empty\", nameof(tableName));\n+        if (vectorDimension <= 0)\n+            throw new ArgumentException(\"Vector dimension must be positive\", nameof(vectorDimension));\n+\n+        _store = new Dictionary<string, VectorDocument<T>>();\n+        _vectorDimension = vectorDimension;\n+    }\n+\n+    /// <summary>\n+    /// Core logic for adding a single vector document to the database.\n+    /// </summary>\n+    /// <param name=\"vectorDocument\">The validated vector document to add.</param>\n+    /// <remarks>\n+    /// <para>\n+    /// Stores the document in the SQLite table with its vector embedding.\n+    /// In real SQLite-VSS, this would use INSERT statements with vss0 virtual table functions.\n+    /// </para>\n+    /// </remarks>\n+    protected override void AddCore(VectorDocument<T> vectorDocument)\n+    {\n+        if (_vectorDimension == 0)\n+            _vectorDimension = vectorDocument.Embedding.Length;\n+\n+        _store[vectorDocument.Document.Id] = vectorDocument;\n+    }\n+\n+    /// <summary>\n+    /// Core logic for adding multiple vector documents in a batch operation.\n+    /// </summary>\n+    /// <param name=\"vectorDocuments\">The validated list of vector documents to add.</param>\n+    /// <remarks>\n+    /// <para>\n+    /// Batch operations use SQLite transactions for better performance, inserting all documents\n+    /// in a single ACID transaction.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Batch operations are much faster in SQLite.\n+    /// \n+    /// Slow (many transactions):\n+    /// <code>\n+    /// foreach (var doc in documents)\n+    ///     store.Add(doc); // Each insert is a separate transaction\n+    /// </code>\n+    /// \n+    /// Fast (single transaction):\n+    /// <code>\n+    /// store.AddBatch(documents); // All inserts in one transaction\n+    /// </code>\n+    /// \n+    /// This can be 100x faster for large batches!\n+    /// </para>\n+    /// </remarks>\n+    protected override void AddBatchCore(IList<VectorDocument<T>> vectorDocuments)\n+    {\n+        if (vectorDocuments.Count == 0) return;\n+\n+        if (_vectorDimension == 0)\n+            _vectorDimension = vectorDocuments[0].Embedding.Length;\n+\n+        foreach (var vd in vectorDocuments)\n+            _store[vd.Document.Id] = vd;\n+    }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/SQLiteVSSDocumentStore.cs",
    "commit_id": "e972697ed5a758045091402fb3f80cadcdfa8829",
    "original_commit_id": "e972697ed5a758045091402fb3f80cadcdfa8829",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Validate batch dimensions before accepting documents.**\n\nWhen `_vectorDimension` was provided in the constructor, `_vectorDimension == 0` is false during the first batch insert, so this loop adds embeddings without ever checking their length. A single mismatched vector will then poison the store and cause later `Add`/`GetSimilar` calls to throw dimension errors. Please assert `vectorDocument.Embedding.Length == _vectorDimension` for every document (including the first batch) or derive the dimension from the first document and validate subsequent entries accordingly.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/SQLiteVSSDocumentStore.cs\naround lines 136 to 144, the batch insert does not validate embedding lengths\nwhen _vectorDimension is already set, which can allow mismatched-dimension\nvectors to be stored; update the method to (a) if _vectorDimension == 0, set it\nfrom the first vector's Embedding.Length, (b) for every vector in\nvectorDocuments assert/throw if vd.Embedding.Length != _vectorDimension, and (c)\nonly after validation add the documents to _store so no invalid-dimension\nvectors are persisted.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commit 8e06c50",
    "created_at": "2025-11-05T15:09:19Z",
    "updated_at": "2025-11-05T15:41:58Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494980154",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494980154"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494980154"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494980154/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 136,
    "original_start_line": 136,
    "start_side": "RIGHT",
    "line": 153,
    "original_line": 144,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 144,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494980172",
    "pull_request_review_id": 3422732638,
    "id": 2494980172,
    "node_id": "PRRC_kwDOKSXUF86UtmBM",
    "diff_hunk": "@@ -0,0 +1,312 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// Lightweight SQLite-based vector store using the SQLite-VSS extension.\n+/// </summary>\n+/// <remarks>\n+/// <para>\n+/// This implementation provides an in-memory simulation of SQLite with the VSS (Vector Similarity Search) extension,\n+/// which adds vector search capabilities to the serverless, file-based SQLite database. Perfect for\n+/// edge deployments, mobile apps, and development environments.\n+/// </para>\n+/// <para><b>For Beginners:</b> SQLite is a tiny, serverless database that stores data in a single file, and VSS adds AI search.\n+/// \n+/// Think of it like an Excel file with AI superpowers:\n+/// - No server needed - just a file on disk\n+/// - Perfect for apps, IoT devices, mobile\n+/// - Combine SQL queries with vector search\n+/// \n+/// This in-memory version is good for:\n+/// - Prototyping SQLite-VSS applications\n+/// - Testing local vector search\n+/// - Small collections (< 10K documents)\n+/// \n+/// Real SQLite-VSS provides:\n+/// - Zero-configuration deployment\n+/// - Single-file database (easy backup/transfer)\n+/// - ACID transactions for data integrity\n+/// - Perfect for edge AI and mobile apps\n+/// </para>\n+/// </remarks>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+public class SQLiteVSSDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly Dictionary<string, VectorDocument<T>> _store;\n+    private int _vectorDimension;\n+\n+    /// <summary>\n+    /// Gets the number of documents currently stored in the database.\n+    /// </summary>\n+    public override int DocumentCount => _store.Count;\n+\n+    /// <summary>\n+    /// Gets the dimensionality of vectors stored in this database.\n+    /// </summary>\n+    public override int VectorDimension => _vectorDimension;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the SQLiteVSSDocumentStore class.\n+    /// </summary>\n+    /// <param name=\"databasePath\">The path to the SQLite database file.</param>\n+    /// <param name=\"tableName\">The table name to organize documents.</param>\n+    /// <param name=\"vectorDimension\">The dimension of vector embeddings.</param>\n+    /// <exception cref=\"ArgumentException\">Thrown when parameters are invalid.</exception>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b> Creates a new SQLite vector store.\n+    /// \n+    /// Example:\n+    /// <code>\n+    /// // Create a store with 384-dimensional embeddings\n+    /// var store = new SQLiteVSSDocumentStore&lt;float&gt;(\n+    ///     \"vectors.db\",\n+    ///     \"documents\",\n+    ///     vectorDimension: 384\n+    /// );\n+    /// </code>\n+    /// \n+    /// The database file will be created if it doesn't exist.\n+    /// Vector dimension must be known upfront to create the table properly.\n+    /// </para>\n+    /// </remarks>\n+    public SQLiteVSSDocumentStore(string databasePath, string tableName, int vectorDimension)\n+    {\n+        if (string.IsNullOrWhiteSpace(databasePath))\n+            throw new ArgumentException(\"Database path cannot be empty\", nameof(databasePath));\n+        if (string.IsNullOrWhiteSpace(tableName))\n+            throw new ArgumentException(\"Table name cannot be empty\", nameof(tableName));\n+        if (vectorDimension <= 0)\n+            throw new ArgumentException(\"Vector dimension must be positive\", nameof(vectorDimension));\n+\n+        _store = new Dictionary<string, VectorDocument<T>>();\n+        _vectorDimension = vectorDimension;\n+    }\n+\n+    /// <summary>\n+    /// Core logic for adding a single vector document to the database.\n+    /// </summary>\n+    /// <param name=\"vectorDocument\">The validated vector document to add.</param>\n+    /// <remarks>\n+    /// <para>\n+    /// Stores the document in the SQLite table with its vector embedding.\n+    /// In real SQLite-VSS, this would use INSERT statements with vss0 virtual table functions.\n+    /// </para>\n+    /// </remarks>\n+    protected override void AddCore(VectorDocument<T> vectorDocument)\n+    {\n+        if (_vectorDimension == 0)\n+            _vectorDimension = vectorDocument.Embedding.Length;\n+\n+        _store[vectorDocument.Document.Id] = vectorDocument;\n+    }\n+\n+    /// <summary>\n+    /// Core logic for adding multiple vector documents in a batch operation.\n+    /// </summary>\n+    /// <param name=\"vectorDocuments\">The validated list of vector documents to add.</param>\n+    /// <remarks>\n+    /// <para>\n+    /// Batch operations use SQLite transactions for better performance, inserting all documents\n+    /// in a single ACID transaction.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Batch operations are much faster in SQLite.\n+    /// \n+    /// Slow (many transactions):\n+    /// <code>\n+    /// foreach (var doc in documents)\n+    ///     store.Add(doc); // Each insert is a separate transaction\n+    /// </code>\n+    /// \n+    /// Fast (single transaction):\n+    /// <code>\n+    /// store.AddBatch(documents); // All inserts in one transaction\n+    /// </code>\n+    /// \n+    /// This can be 100x faster for large batches!\n+    /// </para>\n+    /// </remarks>\n+    protected override void AddBatchCore(IList<VectorDocument<T>> vectorDocuments)\n+    {\n+        if (vectorDocuments.Count == 0) return;\n+\n+        if (_vectorDimension == 0)\n+            _vectorDimension = vectorDocuments[0].Embedding.Length;\n+\n+        foreach (var vd in vectorDocuments)\n+            _store[vd.Document.Id] = vd;\n+    }\n+\n+    /// <summary>\n+    /// Core logic for similarity search using cosine similarity with optional metadata filtering.\n+    /// </summary>\n+    /// <param name=\"queryVector\">The validated query vector.</param>\n+    /// <param name=\"topK\">The validated number of documents to return.</param>\n+    /// <param name=\"metadataFilters\">The validated metadata filters.</param>\n+    /// <returns>Top-k similar documents ordered by cosine similarity score.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Performs vector similarity search using in-memory calculations. In real SQLite-VSS, this would use\n+    /// the vss_search() SQL function to efficiently find nearest neighbors.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Finds the most similar documents in the SQLite database.\n+    /// \n+    /// How it works:\n+    /// 1. Filter documents by metadata (like SQL WHERE clause)\n+    /// 2. Calculate similarity for each document\n+    /// 3. Sort by similarity (highest first, like SQL ORDER BY)\n+    /// 4. Return top-k matches (like SQL LIMIT)\n+    /// \n+    /// In real SQLite-VSS, this uses efficient indexing structures like HNSW\n+    /// for fast approximate nearest neighbor search.\n+    /// \n+    /// Example:\n+    /// <code>\n+    /// // Find 10 most similar documents\n+    /// var results = store.GetSimilar(queryVector, topK: 10);\n+    /// </code>\n+    /// </para>\n+    /// </remarks>\n+    protected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)\n+    {\n+        var results = new List<(Document<T> doc, T score)>();\n+\n+        foreach (var vd in _store.Values)\n+        {\n+            var similarity = StatisticsHelper<T>.CosineSimilarity(queryVector, vd.Embedding);\n+            vd.Document.RelevanceScore = similarity;\n+            results.Add((vd.Document, similarity));\n+        }\n+\n+        return results\n+            .OrderByDescending(x => Convert.ToDouble(x.score))\n+            .Take(topK)\n+            .Select(x => x.doc);\n+    }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/SQLiteVSSDocumentStore.cs",
    "commit_id": "e972697ed5a758045091402fb3f80cadcdfa8829",
    "original_commit_id": "e972697ed5a758045091402fb3f80cadcdfa8829",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Respect metadata filters and set relevance metadata on the results.**\n\n`metadataFilters` is ignored here, so filtered searches return unfiltered documents. Also, the returned documents never get `HasRelevanceScore = true`, leaving downstream consumers thinking no score is available. Please use `MatchesFilters` to gate the candidate set and hydrate both `RelevanceScore` and `HasRelevanceScore` on the objects you return.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/SQLiteVSSDocumentStore.cs\naround lines 176 to 191, the method ignores metadataFilters and never sets\nHasRelevanceScore; update the loop to only consider entries where\nMatchesFilters(metadataFilters, vd.Document.Metadata) returns true (handle\nnull/empty filters), compute and assign vd.Document.RelevanceScore as before,\nand also set vd.Document.HasRelevanceScore = true for any document you add to\nresults; finally order, take topK and return the documents as before.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commit 8e06c50",
    "created_at": "2025-11-05T15:09:19Z",
    "updated_at": "2025-11-05T15:42:00Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494980172",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494980172"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2494980172"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2494980172/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 185,
    "original_start_line": 176,
    "start_side": "RIGHT",
    "line": 213,
    "original_line": 191,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 191,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150086",
    "pull_request_review_id": 3422976623,
    "id": 2495150086,
    "node_id": "PRRC_kwDOKSXUF86UuPgG",
    "diff_hunk": "@@ -0,0 +1,117 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies;\n+\n+/// <summary>\n+/// Splits structured documents based on header tags (H1, H2, H3, etc.).\n+/// </summary>\n+/// <remarks>\n+/// Ideal for Markdown and HTML documents where headers provide natural semantic boundaries.\n+/// Preserves document structure and hierarchy.\n+/// </remarks>\n+public class HeaderBasedTextSplitter : ChunkingStrategyBase\n+{\n+    private readonly bool _combineSmallChunks;\n+    private readonly int _minChunkSize;",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/HeaderBasedTextSplitter.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "79d215b091efb01a9397bf46a29a74fdee4d11bc",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Critical: Constructor parameters `minChunkSize` and `combineSmallChunks` are stored but never used.**\n\nThe constructor accepts and validates `minChunkSize` and `combineSmallChunks` parameters, storing them in `_minChunkSize` and `_combineSmallChunks` fields. However, the `ChunkCore` method never references these fields, making the parameters non-functional. Users expecting these parameters to control chunking behavior will encounter unexpected results.\n\n\n\nApply this diff to remove the unused parameters and fields:\n\n```diff\n public class HeaderBasedTextSplitter : ChunkingStrategyBase\n {\n-    private readonly bool _combineSmallChunks;\n-    private readonly int _minChunkSize;\n-\n     /// <summary>\n     /// Initializes a new instance of the <see cref=\"HeaderBasedTextSplitter\"/> class.\n     /// </summary>\n     /// <param name=\"chunkSize\">Maximum size of each chunk in characters.</param>\n     /// <param name=\"chunkOverlap\">The number of characters that should overlap between consecutive chunks.</param>\n-    /// <param name=\"minChunkSize\">Minimum size for chunk combination.</param>\n-    /// <param name=\"combineSmallChunks\">Whether to combine small chunks.</param>\n     public HeaderBasedTextSplitter(\n         int chunkSize,\n-        int chunkOverlap = 0,\n-        int minChunkSize = 100,\n-        bool combineSmallChunks = true)\n+        int chunkOverlap = 0)\n         : base(chunkSize, chunkOverlap)\n     {\n-        if (minChunkSize < 0)\n-            throw new ArgumentOutOfRangeException(nameof(minChunkSize), \"Min chunk size cannot be negative\");\n-        \n-        if (minChunkSize > chunkSize)\n-            throw new ArgumentOutOfRangeException(nameof(minChunkSize), \"Min chunk size cannot exceed max chunk size\");\n-            \n-        _minChunkSize = minChunkSize;\n-        _combineSmallChunks = combineSmallChunks;\n     }\n```\n\nAlternatively, if the intention is to implement chunk combination logic, please implement the missing functionality in `ChunkCore` to honor these parameters.\n\n\nAlso applies to: 27-28, 37-38\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn\nsrc/RetrievalAugmentedGeneration/ChunkingStrategies/HeaderBasedTextSplitter.cs\naround lines 14-15 (and also the related occurrences at 27-28 and 37-38), the\nconstructor parameters and private fields for minChunkSize and\ncombineSmallChunks are stored but never used; either remove these\nparameters/fields from the class and constructor (and clean up\nvalidation/assignments) or implement their intended behavior in ChunkCore: when\n_combineSmallChunks is true, after initial chunking iterate adjacent chunks and\nmerge them until each resultant chunk length >= _minChunkSize (or no further\nmerges possible), preserving original order and header boundaries where\napplicable, and ensure the constructor validation remains consistent with the\nchosen approach.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 69faff4 to ef3c452",
    "created_at": "2025-11-05T15:51:43Z",
    "updated_at": "2025-11-05T17:10:51Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2495150086",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150086"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2495150086"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150086/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 14,
    "original_start_line": 14,
    "start_side": "RIGHT",
    "line": 15,
    "original_line": 15,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 15,
    "position": 15,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150094",
    "pull_request_review_id": 3422976623,
    "id": 2495150094,
    "node_id": "PRRC_kwDOKSXUF86UuPgO",
    "diff_hunk": "@@ -0,0 +1,117 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies;\n+\n+/// <summary>\n+/// Splits structured documents based on header tags (H1, H2, H3, etc.).\n+/// </summary>\n+/// <remarks>\n+/// Ideal for Markdown and HTML documents where headers provide natural semantic boundaries.\n+/// Preserves document structure and hierarchy.\n+/// </remarks>\n+public class HeaderBasedTextSplitter : ChunkingStrategyBase\n+{\n+    private readonly bool _combineSmallChunks;\n+    private readonly int _minChunkSize;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"HeaderBasedTextSplitter\"/> class.\n+    /// </summary>\n+    /// <param name=\"chunkSize\">Maximum size of each chunk in characters.</param>\n+    /// <param name=\"chunkOverlap\">The number of characters that should overlap between consecutive chunks.</param>\n+    /// <param name=\"minChunkSize\">Minimum size for chunk combination.</param>\n+    /// <param name=\"combineSmallChunks\">Whether to combine small chunks.</param>\n+    public HeaderBasedTextSplitter(\n+        int chunkSize,\n+        int chunkOverlap = 0,\n+        int minChunkSize = 100,\n+        bool combineSmallChunks = true)\n+        : base(chunkSize, chunkOverlap)\n+    {\n+        if (minChunkSize < 0)\n+            throw new ArgumentOutOfRangeException(nameof(minChunkSize), \"Min chunk size cannot be negative\");\n+        \n+        if (minChunkSize > chunkSize)\n+            throw new ArgumentOutOfRangeException(nameof(minChunkSize), \"Min chunk size cannot exceed max chunk size\");\n+            \n+        _minChunkSize = minChunkSize;\n+        _combineSmallChunks = combineSmallChunks;\n+    }\n+\n+    /// <summary>\n+    /// Core chunking logic that splits text based on header hierarchy.\n+    /// </summary>\n+    protected override IEnumerable<(string Chunk, int StartPosition, int EndPosition)> ChunkCore(string text)\n+    {\n+        var chunks = new List<(string, int, int)>();\n+        var lines = text.Split(new[] { \"\\r\\n\", \"\\r\", \"\\n\" }, StringSplitOptions.None);\n+        var currentChunk = new List<string>();\n+        var chunkStart = 0;\n+        var position = 0;\n+\n+        foreach (var line in lines)\n+        {\n+            var lineLength = line.Length + Environment.NewLine.Length;\n+\n+            // Check if line is a header (Markdown ## or HTML <h>)\n+            if (IsHeader(line))\n+            {\n+                // Save current chunk if not empty (always preserve to prevent data loss)\n+                if (currentChunk.Count > 0)\n+                {\n+                    var content = string.Join(Environment.NewLine, currentChunk);\n+                    chunks.Add((content, chunkStart, position));\n+                    currentChunk.Clear();\n+                }\n+\n+                chunkStart = position;\n+                currentChunk.Add(line);\n+            }\n+            else\n+            {\n+                currentChunk.Add(line);\n+\n+                // Split if chunk gets too large\n+                var currentSize = string.Join(Environment.NewLine, currentChunk).Length;\n+                if (currentSize >= ChunkSize)\n+                {\n+                    var content = string.Join(Environment.NewLine, currentChunk);\n+                    chunks.Add((content, chunkStart, position + lineLength));\n+                    currentChunk.Clear();\n+                    chunkStart = position + lineLength;\n+                }\n+            }\n+\n+            position += lineLength;\n+        }\n+\n+        // Add remaining content\n+        if (currentChunk.Count > 0)\n+        {\n+            var content = string.Join(Environment.NewLine, currentChunk);\n+            chunks.Add((content, chunkStart, position));\n+        }\n+\n+        return chunks;\n+    }",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/HeaderBasedTextSplitter.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "79d215b091efb01a9397bf46a29a74fdee4d11bc",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Major: `ChunkOverlap` parameter is accepted but not implemented.**\n\nThe constructor accepts a `chunkOverlap` parameter and passes it to the base class, but `ChunkCore` never applies overlap between chunks. When splitting at header boundaries (lines 60-65) or size boundaries (lines 74-82), the code sets `chunkStart` to the position immediately after the previous chunk with no overlap (line 81). This violates the expected behavior documented in the base class.\n\n\n\nConsider one of these approaches:\n\n**Option 1: Implement overlap (recommended if overlap makes sense for header-based splitting):**\n\n```diff\n                 if (currentSize >= ChunkSize)\n                 {\n                     var content = string.Join(Environment.NewLine, currentChunk);\n                     chunks.Add((content, chunkStart, position + lineLength));\n-                    currentChunk.Clear();\n-                    chunkStart = position + lineLength;\n+                    \n+                    // Keep last ChunkOverlap characters for overlap\n+                    var overlapContent = content.Length > ChunkOverlap \n+                        ? content.Substring(content.Length - ChunkOverlap) \n+                        : content;\n+                    currentChunk.Clear();\n+                    currentChunk.Add(overlapContent);\n+                    chunkStart = position + lineLength - overlapContent.Length;\n                 }\n```\n\n**Option 2: Remove the parameter if overlap isn't applicable:**\n\n```diff\n     public HeaderBasedTextSplitter(\n         int chunkSize,\n-        int chunkOverlap = 0,\n         int minChunkSize = 100,\n         bool combineSmallChunks = true)\n-        : base(chunkSize, chunkOverlap)\n+        : base(chunkSize, 0)\n```\n\nUpdate documentation to clarify that header-based splitting doesn't support overlap since it maintains semantic boundaries.\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T15:51:43Z",
    "updated_at": "2025-11-05T15:51:47Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2495150094",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150094"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2495150094"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150094/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 44,
    "original_start_line": 44,
    "start_side": "RIGHT",
    "line": 102,
    "original_line": 96,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 96,
    "position": 102,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150099",
    "pull_request_review_id": 3422976623,
    "id": 2495150099,
    "node_id": "PRRC_kwDOKSXUF86UuPgT",
    "diff_hunk": "@@ -0,0 +1,117 @@\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ChunkingStrategies;\n+\n+/// <summary>\n+/// Splits structured documents based on header tags (H1, H2, H3, etc.).\n+/// </summary>\n+/// <remarks>\n+/// Ideal for Markdown and HTML documents where headers provide natural semantic boundaries.\n+/// Preserves document structure and hierarchy.\n+/// </remarks>\n+public class HeaderBasedTextSplitter : ChunkingStrategyBase\n+{\n+    private readonly bool _combineSmallChunks;\n+    private readonly int _minChunkSize;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the <see cref=\"HeaderBasedTextSplitter\"/> class.\n+    /// </summary>\n+    /// <param name=\"chunkSize\">Maximum size of each chunk in characters.</param>\n+    /// <param name=\"chunkOverlap\">The number of characters that should overlap between consecutive chunks.</param>\n+    /// <param name=\"minChunkSize\">Minimum size for chunk combination.</param>\n+    /// <param name=\"combineSmallChunks\">Whether to combine small chunks.</param>\n+    public HeaderBasedTextSplitter(\n+        int chunkSize,\n+        int chunkOverlap = 0,\n+        int minChunkSize = 100,\n+        bool combineSmallChunks = true)\n+        : base(chunkSize, chunkOverlap)\n+    {\n+        if (minChunkSize < 0)\n+            throw new ArgumentOutOfRangeException(nameof(minChunkSize), \"Min chunk size cannot be negative\");\n+        \n+        if (minChunkSize > chunkSize)\n+            throw new ArgumentOutOfRangeException(nameof(minChunkSize), \"Min chunk size cannot exceed max chunk size\");\n+            \n+        _minChunkSize = minChunkSize;\n+        _combineSmallChunks = combineSmallChunks;\n+    }\n+\n+    /// <summary>\n+    /// Core chunking logic that splits text based on header hierarchy.\n+    /// </summary>\n+    protected override IEnumerable<(string Chunk, int StartPosition, int EndPosition)> ChunkCore(string text)\n+    {\n+        var chunks = new List<(string, int, int)>();\n+        var lines = text.Split(new[] { \"\\r\\n\", \"\\r\", \"\\n\" }, StringSplitOptions.None);\n+        var currentChunk = new List<string>();\n+        var chunkStart = 0;\n+        var position = 0;\n+\n+        foreach (var line in lines)\n+        {\n+            var lineLength = line.Length + Environment.NewLine.Length;",
    "path": "src/RetrievalAugmentedGeneration/ChunkingStrategies/HeaderBasedTextSplitter.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "79d215b091efb01a9397bf46a29a74fdee4d11bc",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Major: Position tracking is incorrect for non-native line endings.**\n\nLine 54 increments `position` by `line.Length + Environment.NewLine.Length`, but line 47 splits on the actual separators in the input text (`\\r\\n`, `\\r`, or `\\n`). When the input uses different line endings than the current platform's `Environment.NewLine` (e.g., Unix files on Windows), the position calculations drift and `StartPosition`/`EndPosition` values become inaccurate.\n\nFor example, if processing a Unix file (LF line endings) on Windows:\n- Actual separator length: 1 byte (`\\n`)\n- `Environment.NewLine.Length` on Windows: 2 bytes (`\\r\\n`)\n- Position drift: 1 byte per line\n\n\n\nApply this diff to track positions based on actual input separators:\n\n```diff\n         var chunks = new List<(string, int, int)>();\n         var lines = text.Split(new[] { \"\\r\\n\", \"\\r\", \"\\n\" }, StringSplitOptions.None);\n         var currentChunk = new List<string>();\n         var chunkStart = 0;\n-        var position = 0;\n+        var position = 0;\n+        \n+        // Detect actual line separator used in input\n+        var separatorLength = 1; // Default to LF\n+        if (text.Contains(\"\\r\\n\"))\n+            separatorLength = 2;\n\n         foreach (var line in lines)\n         {\n-            var lineLength = line.Length + Environment.NewLine.Length;\n+            var lineLength = line.Length + separatorLength;\n```\n\n\nAlso applies to: 85-85\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn\nsrc/RetrievalAugmentedGeneration/ChunkingStrategies/HeaderBasedTextSplitter.cs\naround lines 47-54 (and also at line 85), the code increments position using\nEnvironment.NewLine.Length which is wrong when the input uses different line\nendings; instead compute the actual separator length from the original text at\nthe current position: after a line of length L, look at the original input at\nindex position+L to see whether the next character(s) form \"\\r\\n\"\n(separatorLen=2), \"\\r\" or \"\\n\" (separatorLen=1), or nothing (separatorLen=0),\nthen increment position by L + separatorLen; replace uses of\nEnvironment.NewLine.Length with this computed separatorLen logic and apply the\nsame fix at the other occurrence around line 85 so StartPosition/EndPosition are\nbased on real input separators.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T15:51:43Z",
    "updated_at": "2025-11-05T15:51:47Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2495150099",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150099"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2495150099"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150099/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 47,
    "original_start_line": 47,
    "start_side": "RIGHT",
    "line": 54,
    "original_line": 54,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 54,
    "position": 54,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150107",
    "pull_request_review_id": 3422976623,
    "id": 2495150107,
    "node_id": "PRRC_kwDOKSXUF86UuPgb",
    "diff_hunk": "@@ -0,0 +1,288 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores\n+{\n+    /// <summary>\n+    /// Azure Cognitive Search-inspired document store with field-based indexing and search capabilities.\n+    /// Provides in-memory simulation of Azure Search features including field-level search and faceted filtering.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class AzureSearchDocumentStore<T> : DocumentStoreBase<T>\n+    {\n+        private readonly Dictionary<string, VectorDocument<T>> _documents;\n+        private readonly Dictionary<string, Dictionary<string, HashSet<string>>> _invertedIndex;\n+        private readonly string _serviceName;\n+        private readonly string _indexName;\n+        private int _vectorDimension;\n+\n+        public override int DocumentCount => _documents.Count;\n+        public override int VectorDimension => _vectorDimension;\n+\n+        public AzureSearchDocumentStore(string serviceName, string indexName, int initialCapacity = 1000)\n+        {\n+            if (string.IsNullOrWhiteSpace(serviceName))\n+                throw new ArgumentException(\"Service name cannot be empty\", nameof(serviceName));\n+            if (string.IsNullOrWhiteSpace(indexName))\n+                throw new ArgumentException(\"Index name cannot be empty\", nameof(indexName));\n+            if (initialCapacity <= 0)\n+                throw new ArgumentException(\"Initial capacity must be greater than zero\", nameof(initialCapacity));\n+\n+            _serviceName = serviceName;\n+            _indexName = indexName;\n+            _documents = new Dictionary<string, VectorDocument<T>>(initialCapacity);\n+            _invertedIndex = new Dictionary<string, Dictionary<string, HashSet<string>>>();\n+            _vectorDimension = 0;\n+        }\n+\n+        protected override void AddCore(VectorDocument<T> vectorDocument)\n+        {\n+            if (_documents.Count == 0)\n+            {\n+                _vectorDimension = vectorDocument.Embedding.Length;\n+            }\n+\n+            _documents[vectorDocument.Document.Id] = vectorDocument;\n+            IndexMetadata(vectorDocument.Document);\n+        }\n+\n+        protected override void AddBatchCore(IList<VectorDocument<T>> vectorDocuments)\n+        {\n+            if (vectorDocuments.Count == 0)\n+                return;\n+\n+            if (_documents.Count == 0)\n+            {\n+                _vectorDimension = vectorDocuments[0].Embedding.Length;\n+            }\n+\n+            foreach (var vectorDoc in vectorDocuments)\n+            {\n+                _documents[vectorDoc.Document.Id] = vectorDoc;\n+                IndexMetadata(vectorDoc.Document);\n+            }\n+        }\n+\n+        protected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)\n+        {\n+            var candidateIds = GetCandidateIds(metadataFilters);\n+            var scoredDocuments = new List<(Document<T> Document, T Score)>();\n+\n+            IEnumerable<VectorDocument<T>> candidates;\n+            if (candidateIds != null)\n+            {\n+                candidates = candidateIds\n+                    .Where(id => _documents.ContainsKey(id))\n+                    .Select(id => _documents[id]);\n+            }\n+            else\n+            {\n+                candidates = _documents.Values;\n+            }\n+\n+            var matchingDocuments = candidates\n+                .Where(vectorDoc => MatchesFilters(vectorDoc.Document, metadataFilters));\n+\n+            foreach (var vectorDoc in matchingDocuments)\n+            {\n+                var similarity = StatisticsHelper<T>.CosineSimilarity(queryVector, vectorDoc.Embedding);\n+                scoredDocuments.Add((vectorDoc.Document, similarity));\n+            }\n+\n+            var results = scoredDocuments\n+                .OrderByDescending(x => x.Score)\n+                .Take(topK)\n+                .Select(x =>\n+                {\n+                    x.Document.RelevanceScore = x.Score;\n+                    x.Document.HasRelevanceScore = true;\n+                    return x.Document;\n+                })\n+                .ToList();\n+\n+            return results;\n+        }\n+\n+        protected override Document<T>? GetByIdCore(string documentId)\n+        {\n+            return _documents.TryGetValue(documentId, out var vectorDoc) ? vectorDoc.Document : null;\n+        }\n+\n+        protected override bool RemoveCore(string documentId)\n+        {\n+            if (!_documents.TryGetValue(documentId, out var vectorDoc))\n+                return false;\n+\n+            RemoveFromIndex(vectorDoc.Document);\n+            _documents.Remove(documentId);\n+\n+            if (_documents.Count == 0)\n+            {\n+                _vectorDimension = 0;\n+            }\n+\n+            return true;\n+        }\n+\n+        /// <summary>\n+        /// Core logic for retrieving all documents in the index.\n+        /// </summary>\n+        /// <returns>An enumerable of all documents without their vector embeddings.</returns>\n+        /// <remarks>\n+        /// <para>\n+        /// Returns all documents from the Azure Search index in no particular order.\n+        /// Vector embeddings are not included, only document content and metadata.\n+        /// </para>\n+        /// <para><b>For Beginners:</b> Gets every document in the index.\n+        /// \n+        /// Use cases:\n+        /// - Export all documents for backup\n+        /// - Migrate to a different index or service\n+        /// - Bulk reindexing or analysis\n+        /// - Debugging facet indices\n+        /// \n+        /// Warning: For large indices (> 10K documents), this can use significant memory.\n+        /// In real Azure Search, use continuation tokens for pagination.\n+        /// \n+        /// Example:\n+        /// <code>\n+        /// // Get all documents\n+        /// var allDocs = store.GetAll().ToList();\n+        /// Console.WriteLine($\"Total documents in {_indexName}: {allDocs.Count}\");\n+        /// \n+        /// // Export to JSON\n+        /// var json = JsonConvert.SerializeObject(allDocs);\n+        /// File.WriteAllText($\"{_serviceName}_{_indexName}_export.json\", json);\n+        /// </code>\n+        /// </para>\n+        /// </remarks>\n+        protected override IEnumerable<Document<T>> GetAllCore()\n+        {\n+            return _documents.Values.Select(vd => vd.Document).ToList();\n+        }\n+\n+        /// <summary>\n+        /// Removes all documents from the index and clears all inverted indices.\n+        /// </summary>\n+        /// <remarks>\n+        /// <para>\n+        /// Clears all documents, field-level inverted indices, and resets the vector dimension to 0.\n+        /// The service and index names remain unchanged and the index is ready to accept new documents.\n+        /// </para>\n+        /// <para><b>For Beginners:</b> Completely empties the Azure Search index and all its facet indices.\n+        /// \n+        /// After calling Clear():\n+        /// - All documents are removed\n+        /// - Inverted index is cleared (all facets)\n+        /// - Vector dimension resets to 0\n+        /// - Index is ready for new documents\n+        /// \n+        /// Use with caution - this cannot be undone!\n+        /// \n+        /// Example:\n+        /// <code>\n+        /// store.Clear();\n+        /// Console.WriteLine($\"Documents in index: {store.DocumentCount}\"); // 0\n+        /// </code>\n+        /// </para>\n+        /// </remarks>\n+        public override void Clear()\n+        {\n+            _documents.Clear();\n+            _invertedIndex.Clear();\n+            _vectorDimension = 0;\n+        }\n+\n+        private void IndexMetadata(Document<T> document)\n+        {\n+            foreach (var kvp in document.Metadata)\n+            {\n+                var fieldName = kvp.Key;\n+                var fieldValue = kvp.Value;\n+\n+                if (!_invertedIndex.ContainsKey(fieldName))\n+                {\n+                    _invertedIndex[fieldName] = new Dictionary<string, HashSet<string>>();\n+                }\n+\n+                // Preserve original value for comparisons, convert to string only for indexing\n+                var indexKey = fieldValue?.ToString() ?? string.Empty;\n+\n+                if (!_invertedIndex[fieldName].ContainsKey(indexKey))\n+                {\n+                    _invertedIndex[fieldName][indexKey] = new HashSet<string>();\n+                }\n+\n+                _invertedIndex[fieldName][indexKey].Add(document.Id);\n+            }\n+        }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/AzureSearchDocumentStore.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "79d215b091efb01a9397bf46a29a74fdee4d11bc",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Inverted index string conversion causes false negatives for typed metadata filters.**\n\nLines 213, 229, and 259 convert all metadata values and filter values to strings for indexing. This breaks filtering on non-string types:\n\n**Scenario:**\n- Document metadata: `{ \"age\": 25 }` (int)\n- Indexed as: `_invertedIndex[\"age\"][\"25\"] = {docId}`\n- Query filter: `{ \"age\": 30 }`\n- GetCandidateIds looks for `_invertedIndex[\"age\"][\"30\"]`\n- Returns empty set (line 276) because \"30\" was never indexed\n- Zero candidates ΓåÆ MatchesFilters never runs ΓåÆ document incorrectly excluded\n\nEven though `MatchesFilters` in the base class can handle `IComparable` range semantics, `GetCandidateIds` performs exact string matching and returns empty before `MatchesFilters` is reached.\n\nFor exact equality on the same value (e.g., filter `{ \"age\": 25 }`), string conversion works. But for any other value or type-aware comparison, the index causes false negatives.\n\n\n\n**Solution:** Preserve types in the inverted index or skip the index for non-string filters.\n\n<details>\n<summary>Option 1: Skip inverted index for non-string filters (simpler, safer)</summary>\n\n```diff\n private HashSet<string>? GetCandidateIds(Dictionary<string, object> metadataFilters)\n {\n     if (metadataFilters.Count == 0)\n         return null;\n+    \n+    // Skip inverted index optimization for non-string filters to avoid false negatives\n+    if (metadataFilters.Values.Any(v => v != null && v.GetType() != typeof(string)))\n+        return null; // Full scan\n \n     HashSet<string>? candidateIds = null;\n \n     foreach (var filter in metadataFilters)\n     {\n         var fieldName = filter.Key;\n         var indexKey = filter.Value?.ToString() ?? string.Empty;\n \n         if (_invertedIndex.TryGetValue(fieldName, out var fieldIndex))\n         {\n```\n\nThis approach returns `null` (triggering a full scan in `GetSimilarCore`) when any filter value is non-string, ensuring `MatchesFilters` evaluates all documents with their original typed values.\n</details>\n\n<details>\n<summary>Option 2: Preserve types in inverted index (more complex)</summary>\n\nChange the index type to handle typed keys and update all three methods accordingly. This requires careful equality comparison for boxed value types.\n</details>\n\nBased on learnings (past review comment flagged this same issue).\n\n\nAlso applies to: 249-286\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/AzureSearchDocumentStore.cs\naround lines 200-222 (and related logic at 249-286), the inverted index\ncurrently converts all metadata values to strings which causes false negatives\nfor non-string typed filters; update the logic so the index is only used for\nstring-valued metadata/filters: (1) in IndexMetadata only add entries for\nmetadata values that are already strings (skip indexing non-string values), and\n(2) in GetCandidateIds detect when any incoming filter value is non-string and\nreturn null (force a full-scan so MatchesFilters can evaluate typed\ncomparisons). Ensure these changes avoid changing stored Document.Metadata and\nonly affect whether the inverted index is consulted.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T15:51:43Z",
    "updated_at": "2025-11-05T15:51:47Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2495150107",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150107"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2495150107"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150107/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 200,
    "original_start_line": 200,
    "start_side": "RIGHT",
    "line": 222,
    "original_line": 222,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 222,
    "position": 222,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150114",
    "pull_request_review_id": 3422976623,
    "id": 2495150114,
    "node_id": "PRRC_kwDOKSXUF86UuPgi",
    "diff_hunk": "@@ -0,0 +1,287 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using Newtonsoft.Json.Linq;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Net.Http;\n+using System.Text;\n+",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/ChromaDBDocumentStore.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "79d215b091efb01a9397bf46a29a74fdee4d11bc",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Fix import ordering per pipeline failure.**\n\nThe pipeline reports a formatting issue with import ordering at line 1.\n\n\nEnsure using directives follow the project's style guidelines (typically: System namespaces first, then third-party, then project namespaces, all alphabetically sorted).\n\n<details>\n<summary>≡ƒº░ Tools</summary>\n\n<details>\n<summary>≡ƒ¬¢ GitHub Actions: Build</summary>\n\n[error] 1-1: Import/order formatting issue at line 1.\n\n</details>\n\n</details>\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/ChromaDBDocumentStore.cs\naround lines 1 to 11, the using directives are out of the project's expected\norder; reorder them so System namespaces come first (alphabetically), then\nthirdΓÇæparty namespaces (e.g., Newtonsoft.Json.Linq), then project namespaces\n(AiDotNet.*) alphabetically, and remove any extra blank lines so the block is\nconsistently sorted and formatted per the pipeline style rules.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T15:51:44Z",
    "updated_at": "2025-11-05T15:51:47Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2495150114",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150114"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2495150114"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150114/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 1,
    "original_start_line": 1,
    "start_side": "RIGHT",
    "line": 11,
    "original_line": 11,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 11,
    "position": 11,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150120",
    "pull_request_review_id": 3422976623,
    "id": 2495150120,
    "node_id": "PRRC_kwDOKSXUF86UuPgo",
    "diff_hunk": "@@ -0,0 +1,287 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using Newtonsoft.Json.Linq;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Net.Http;\n+using System.Text;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// ChromaDB-based document store designed for simplicity and developer experience.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// ChromaDB is an open-source vector database that emphasizes ease of use while maintaining\n+/// high performance for similarity search operations.\n+/// </remarks>\n+public class ChromaDBDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly HttpClient _httpClient;\n+    private readonly string _collectionName;\n+    private int _vectorDimension;\n+    private int _documentCount;\n+    private readonly Dictionary<string, VectorDocument<T>> _cache;\n+\n+    public override int DocumentCount => _documentCount;\n+    public override int VectorDimension => _vectorDimension;\n+\n+    public ChromaDBDocumentStore(string endpoint, string collectionName, string apiKey)\n+    {\n+        if (string.IsNullOrWhiteSpace(endpoint))\n+            throw new ArgumentException(\"Endpoint cannot be empty\", nameof(endpoint));\n+        if (string.IsNullOrWhiteSpace(collectionName))\n+            throw new ArgumentException(\"Collection name cannot be empty\", nameof(collectionName));\n+\n+        _httpClient = new HttpClient { BaseAddress = new Uri(endpoint) };\n+        if (!string.IsNullOrWhiteSpace(apiKey))\n+            _httpClient.DefaultRequestHeaders.Add(\"X-Chroma-Token\", apiKey);\n+\n+        _collectionName = collectionName;\n+        _vectorDimension = 0;\n+        _documentCount = 0;\n+        _cache = new Dictionary<string, VectorDocument<T>>();\n+\n+        EnsureCollection();\n+    }\n+\n+    private void EnsureCollection()\n+    {\n+        var payload = new { name = _collectionName };\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(payload),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        _httpClient.PostAsync(\"/api/v1/collections\", content).Wait();\n+    }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/ChromaDBDocumentStore.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "79d215b091efb01a9397bf46a29a74fdee4d11bc",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Address HttpClient lifetime and synchronous blocking issues.**\n\nMultiple issues with HTTP client management and async handling:\n\n1. **HttpClient socket exhaustion** (lines 40-42): Creating `HttpClient` instances per store can exhaust sockets. Use `IHttpClientFactory` (preferred) or a static `HttpClient`.\n2. **Synchronous blocking** (line 60): `.Wait()` on async operations can cause deadlocks in ASP.NET or UI contexts. Make constructor accept a factory/client or initialize async.\n3. **No disposal** (line 24): `HttpClient` is never disposed. Implement `IDisposable` or use `IHttpClientFactory`.\n4. **No error handling** (lines 52-61): If collection creation fails or already exists, no exception is caught or logged.\n\n\nConsider this pattern:\n\n```diff\n-    private readonly HttpClient _httpClient;\n+    private readonly IHttpClientFactory _httpClientFactory;\n+    private HttpClient HttpClient => _httpClientFactory.CreateClient();\n\n-    public ChromaDBDocumentStore(string endpoint, string collectionName, string apiKey)\n+    public ChromaDBDocumentStore(\n+        string endpoint, \n+        string collectionName, \n+        string apiKey,\n+        IHttpClientFactory httpClientFactory)\n     {\n         // ... validation ...\n-        _httpClient = new HttpClient { BaseAddress = new Uri(endpoint) };\n-        if (!string.IsNullOrWhiteSpace(apiKey))\n-            _httpClient.DefaultRequestHeaders.Add(\"X-Chroma-Token\", apiKey);\n+        _httpClientFactory = httpClientFactory ?? throw new ArgumentNullException(nameof(httpClientFactory));\n+        _endpoint = endpoint;\n+        _apiKey = apiKey;\n         \n-        EnsureCollection();\n+        // Note: Initialize collection asynchronously via separate InitializeAsync method\n+        // or accept an already-initialized client\n     }\n```\n\nAlternatively, if `IHttpClientFactory` is not available, use a static `HttpClient` and configure per-request headers.\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T15:51:44Z",
    "updated_at": "2025-11-05T15:51:47Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2495150120",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150120"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2495150120"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150120/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 33,
    "original_start_line": 33,
    "start_side": "RIGHT",
    "line": 61,
    "original_line": 61,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 61,
    "position": 61,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150126",
    "pull_request_review_id": 3422976623,
    "id": 2495150126,
    "node_id": "PRRC_kwDOKSXUF86UuPgu",
    "diff_hunk": "@@ -0,0 +1,287 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using Newtonsoft.Json.Linq;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Net.Http;\n+using System.Text;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// ChromaDB-based document store designed for simplicity and developer experience.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// ChromaDB is an open-source vector database that emphasizes ease of use while maintaining\n+/// high performance for similarity search operations.\n+/// </remarks>\n+public class ChromaDBDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly HttpClient _httpClient;\n+    private readonly string _collectionName;\n+    private int _vectorDimension;\n+    private int _documentCount;\n+    private readonly Dictionary<string, VectorDocument<T>> _cache;\n+\n+    public override int DocumentCount => _documentCount;\n+    public override int VectorDimension => _vectorDimension;\n+\n+    public ChromaDBDocumentStore(string endpoint, string collectionName, string apiKey)\n+    {\n+        if (string.IsNullOrWhiteSpace(endpoint))\n+            throw new ArgumentException(\"Endpoint cannot be empty\", nameof(endpoint));\n+        if (string.IsNullOrWhiteSpace(collectionName))\n+            throw new ArgumentException(\"Collection name cannot be empty\", nameof(collectionName));\n+\n+        _httpClient = new HttpClient { BaseAddress = new Uri(endpoint) };\n+        if (!string.IsNullOrWhiteSpace(apiKey))\n+            _httpClient.DefaultRequestHeaders.Add(\"X-Chroma-Token\", apiKey);\n+\n+        _collectionName = collectionName;\n+        _vectorDimension = 0;\n+        _documentCount = 0;\n+        _cache = new Dictionary<string, VectorDocument<T>>();\n+\n+        EnsureCollection();\n+    }\n+\n+    private void EnsureCollection()\n+    {\n+        var payload = new { name = _collectionName };\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(payload),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        _httpClient.PostAsync(\"/api/v1/collections\", content).Wait();\n+    }\n+\n+    protected override void AddCore(VectorDocument<T> vectorDocument)\n+    {\n+        if (_vectorDimension == 0)\n+            _vectorDimension = vectorDocument.Embedding.Length;\n+\n+        _cache[vectorDocument.Document.Id] = vectorDocument;\n+\n+        var embedding = vectorDocument.Embedding.ToArray().Select(v => Convert.ToDouble(v)).ToList();\n+        \n+        var payload = new\n+        {\n+            ids = new[] { vectorDocument.Document.Id },\n+            embeddings = new[] { embedding },\n+            documents = new[] { vectorDocument.Document.Content },\n+            metadatas = new[] { vectorDocument.Document.Metadata }\n+        };\n+\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(payload),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        var response = _httpClient.PostAsync($\"/api/v1/collections/{_collectionName}/add\", content).Result;\n+        if (response.IsSuccessStatusCode)\n+            _documentCount++;\n+    }\n+\n+    protected override void AddBatchCore(IList<VectorDocument<T>> vectorDocuments)\n+    {\n+        if (vectorDocuments.Count == 0) return;\n+        \n+        if (_vectorDimension == 0)\n+            _vectorDimension = vectorDocuments[0].Embedding.Length;\n+\n+        foreach (var vd in vectorDocuments)\n+            _cache[vd.Document.Id] = vd;\n+\n+        var ids = vectorDocuments.Select(vd => vd.Document.Id).ToList();\n+        var embeddings = vectorDocuments.Select(vd => \n+            vd.Embedding.ToArray().Select(v => Convert.ToDouble(v)).ToList()).ToList();\n+        var documents = vectorDocuments.Select(vd => vd.Document.Content).ToList();\n+        var metadatas = vectorDocuments.Select(vd => vd.Document.Metadata).ToList();\n+\n+        var payload = new { ids, embeddings, documents, metadatas };\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(payload),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        var response = _httpClient.PostAsync($\"/api/v1/collections/{_collectionName}/add\", content).Result;\n+        if (response.IsSuccessStatusCode)\n+            _documentCount += vectorDocuments.Count;\n+    }\n+\n+    protected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)\n+    {\n+        var embedding = queryVector.ToArray().Select(v => Convert.ToDouble(v)).ToList();\n+        \n+        var payload = new\n+        {\n+            query_embeddings = new[] { embedding },\n+            n_results = topK\n+        };\n+\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(payload),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        var response = _httpClient.PostAsync($\"/api/v1/collections/{_collectionName}/query\", content).Result;\n+        var responseContent = response.Content.ReadAsStringAsync().Result;\n+        var result = JObject.Parse(responseContent);\n+\n+        var results = new List<Document<T>>();\n+        var ids = result[\"ids\"]?[0];\n+        var documents = result[\"documents\"]?[0];\n+        var metadatas = result[\"metadatas\"]?[0];\n+        var distances = result[\"distances\"]?[0];\n+\n+        if (ids == null || documents == null) return results;\n+\n+        for (int i = 0; i < ids.Count(); i++)\n+        {\n+            var idToken = ids[i];\n+            var docToken = documents[i];\n+            if (idToken == null || docToken == null) continue;\n+\n+            var id = idToken.ToString();\n+            var doc = docToken.ToString();\n+            var metadataObj = metadatas?[i]?.ToObject<Dictionary<string, object>>() ?? new Dictionary<string, object>();\n+\n+            var distance = distances != null ? Convert.ToDouble(distances[i]) : 0.0;\n+\n+            var document = new Document<T>(id, doc, metadataObj)\n+            {\n+                RelevanceScore = NumOps.FromDouble(1.0 / (1.0 + distance)),\n+                HasRelevanceScore = true\n+            };\n+\n+            results.Add(document);\n+        }\n+\n+        return results;\n+    }\n+\n+    protected override Document<T>? GetByIdCore(string documentId)\n+    {\n+        if (_cache.TryGetValue(documentId, out var vectorDoc))\n+            return vectorDoc.Document;\n+\n+        return null;\n+    }\n+\n+    /// <summary>\n+    /// Core logic for removing a document from the ChromaDB collection.\n+    /// </summary>\n+    /// <param name=\"documentId\">The validated document ID.</param>\n+    /// <returns>True if the document was found and removed; otherwise, false.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Removes the document from both the cache and the ChromaDB collection via API call.\n+    /// If successful, decrements the document count.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Deletes a document from ChromaDB.\n+    /// \n+    /// Example:\n+    /// <code>\n+    /// if (store.Remove(\"doc-123\"))\n+    ///     Console.WriteLine(\"Document removed from ChromaDB\");\n+    /// </code>\n+    /// </para>\n+    /// </remarks>\n+    protected override bool RemoveCore(string documentId)\n+    {\n+        _cache.Remove(documentId);\n+\n+        var payload = new { ids = new[] { documentId } };\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(payload),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        var response = _httpClient.PostAsync($\"/api/v1/collections/{_collectionName}/delete\", content).Result;\n+        if (response.IsSuccessStatusCode && _documentCount > 0)\n+        {\n+            _documentCount--;\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+    /// <summary>\n+    /// Core logic for retrieving all documents from the ChromaDB collection.\n+    /// </summary>\n+    /// <returns>An enumerable of all documents without their vector embeddings.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Returns all documents from the cache (in-memory representation) of the ChromaDB collection.\n+    /// In a real ChromaDB deployment, this would query the collection's entire dataset.\n+    /// Vector embeddings are not included in the results.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Gets every document from the ChromaDB collection.\n+    /// \n+    /// Use cases:\n+    /// - Export collection contents for backup\n+    /// - Migrate to a different ChromaDB collection or database\n+    /// - Bulk processing or reindexing\n+    /// - Debugging to see all stored documents\n+    /// \n+    /// Warning: For large collections (> 10K documents), this can use significant memory.\n+    /// In production ChromaDB, consider using pagination with limit/offset parameters.\n+    /// \n+    /// Example:\n+    /// <code>\n+    /// // Get all documents\n+    /// var allDocs = store.GetAll().ToList();\n+    /// Console.WriteLine($\"Total documents in {_collectionName}: {allDocs.Count}\");\n+    /// \n+    /// // Export to JSON\n+    /// var json = JsonConvert.SerializeObject(allDocs);\n+    /// File.WriteAllText($\"chroma_{_collectionName}_export.json\", json);\n+    /// </code>\n+    /// </para>\n+    /// </remarks>\n+    protected override IEnumerable<Document<T>> GetAllCore()\n+    {\n+        return _cache.Values.Select(vd => vd.Document).ToList();\n+    }\n+\n+    /// <summary>\n+    /// Removes all documents from the ChromaDB collection and recreates it.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Clears the cache, deletes the ChromaDB collection, resets counters, and recreates an empty collection.\n+    /// The collection name remains unchanged and is ready to accept new documents.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Completely empties the ChromaDB collection.\n+    /// \n+    /// After calling Clear():\n+    /// - All documents are removed from ChromaDB\n+    /// - Cache is cleared\n+    /// - Document count resets to 0\n+    /// - Vector dimension resets to 0\n+    /// - Collection is recreated (empty)\n+    /// - Ready for new documents\n+    /// \n+    /// Use with caution - this cannot be undone!\n+    /// \n+    /// Example:\n+    /// <code>\n+    /// store.Clear();\n+    /// Console.WriteLine($\"Documents in collection: {store.DocumentCount}\"); // 0\n+    /// </code>\n+    /// </para>\n+    /// </remarks>\n+    public override void Clear()\n+    {\n+        _cache.Clear();\n+        _httpClient.DeleteAsync($\"/api/v1/collections/{_collectionName}\").Wait();\n+        _documentCount = 0;\n+        _vectorDimension = 0;\n+        EnsureCollection();\n+    }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/ChromaDBDocumentStore.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "79d215b091efb01a9397bf46a29a74fdee4d11bc",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Dispose HttpResponseMessage to prevent resource leaks.**\n\nThroughout the class (lines 60, 85, 112, 132, 205, 282), `HttpResponseMessage` instances returned by `PostAsync`/`DeleteAsync` are never disposed. This can exhaust connections in the HTTP connection pool.\n\n\nApply a `using` pattern for all responses:\n\n```diff\n-        var response = _httpClient.PostAsync($\"/api/v1/collections/{_collectionName}/add\", content).Result;\n-        if (response.IsSuccessStatusCode)\n-            _documentCount++;\n+        using var response = _httpClient.PostAsync($\"/api/v1/collections/{_collectionName}/add\", content).Result;\n+        response.EnsureSuccessStatusCode();\n+        _documentCount++;\n```\n\nApply this pattern to all HTTP calls in the class. Alternatively, if using async/await, wrap in `using`:\n\n```csharp\nusing var response = await _httpClient.PostAsync(...);\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/ChromaDBDocumentStore.cs\naround lines 52ΓÇô286, every call that calls _httpClient.PostAsync(...) or\nDeleteAsync(...) currently uses .Wait()/.Result and never disposes the returned\nHttpResponseMessage, leaking connections; update each HTTP call to use a using\npattern to dispose the response (e.g., use using var response = await\n_httpClient.PostAsync(...); or if you must stay synchronous use using var\nresponse = _httpClient.PostAsync(...).GetAwaiter().GetResult();), remove the\nseparate .Wait()/.Result usages, and ensure success checks remain inside the\nusing scope so the response is properly disposed in all code paths.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T15:51:44Z",
    "updated_at": "2025-11-05T15:51:47Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2495150126",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150126"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2495150126"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150126/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 52,
    "original_start_line": 52,
    "start_side": "RIGHT",
    "line": 286,
    "original_line": 286,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 286,
    "position": 286,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150129",
    "pull_request_review_id": 3422976623,
    "id": 2495150129,
    "node_id": "PRRC_kwDOKSXUF86UuPgx",
    "diff_hunk": "@@ -0,0 +1,287 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using Newtonsoft.Json.Linq;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Net.Http;\n+using System.Text;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// ChromaDB-based document store designed for simplicity and developer experience.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// ChromaDB is an open-source vector database that emphasizes ease of use while maintaining\n+/// high performance for similarity search operations.\n+/// </remarks>\n+public class ChromaDBDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly HttpClient _httpClient;\n+    private readonly string _collectionName;\n+    private int _vectorDimension;\n+    private int _documentCount;\n+    private readonly Dictionary<string, VectorDocument<T>> _cache;\n+\n+    public override int DocumentCount => _documentCount;\n+    public override int VectorDimension => _vectorDimension;\n+\n+    public ChromaDBDocumentStore(string endpoint, string collectionName, string apiKey)\n+    {\n+        if (string.IsNullOrWhiteSpace(endpoint))\n+            throw new ArgumentException(\"Endpoint cannot be empty\", nameof(endpoint));\n+        if (string.IsNullOrWhiteSpace(collectionName))\n+            throw new ArgumentException(\"Collection name cannot be empty\", nameof(collectionName));\n+\n+        _httpClient = new HttpClient { BaseAddress = new Uri(endpoint) };\n+        if (!string.IsNullOrWhiteSpace(apiKey))\n+            _httpClient.DefaultRequestHeaders.Add(\"X-Chroma-Token\", apiKey);\n+\n+        _collectionName = collectionName;\n+        _vectorDimension = 0;\n+        _documentCount = 0;\n+        _cache = new Dictionary<string, VectorDocument<T>>();\n+\n+        EnsureCollection();\n+    }\n+\n+    private void EnsureCollection()\n+    {\n+        var payload = new { name = _collectionName };\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(payload),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        _httpClient.PostAsync(\"/api/v1/collections\", content).Wait();\n+    }\n+\n+    protected override void AddCore(VectorDocument<T> vectorDocument)\n+    {\n+        if (_vectorDimension == 0)\n+            _vectorDimension = vectorDocument.Embedding.Length;\n+\n+        _cache[vectorDocument.Document.Id] = vectorDocument;\n+\n+        var embedding = vectorDocument.Embedding.ToArray().Select(v => Convert.ToDouble(v)).ToList();\n+        \n+        var payload = new\n+        {\n+            ids = new[] { vectorDocument.Document.Id },\n+            embeddings = new[] { embedding },\n+            documents = new[] { vectorDocument.Document.Content },\n+            metadatas = new[] { vectorDocument.Document.Metadata }\n+        };\n+\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(payload),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        var response = _httpClient.PostAsync($\"/api/v1/collections/{_collectionName}/add\", content).Result;\n+        if (response.IsSuccessStatusCode)\n+            _documentCount++;\n+    }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/ChromaDBDocumentStore.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "79d215b091efb01a9397bf46a29a74fdee4d11bc",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Validate embedding dimension and avoid synchronous blocking.**\n\nTwo issues:\n\n1. **Missing dimension validation** (lines 65-66): After the first document sets `_vectorDimension`, subsequent adds don't validate that `vectorDocument.Embedding.Length == _vectorDimension`. This allows mixed dimensions to slip through.\n2. **Synchronous blocking** (line 85): `.Result` can deadlock in sync contexts.\n\n\nApply this diff:\n\n```diff\n     protected override void AddCore(VectorDocument<T> vectorDocument)\n     {\n         if (_vectorDimension == 0)\n             _vectorDimension = vectorDocument.Embedding.Length;\n+        else if (vectorDocument.Embedding.Length != _vectorDimension)\n+            throw new ArgumentException(\n+                $\"Vector dimension mismatch. Expected {_vectorDimension}, got {vectorDocument.Embedding.Length}\",\n+                nameof(vectorDocument));\n\n         _cache[vectorDocument.Document.Id] = vectorDocument;\n         // ... rest of method\n```\n\nFor the async issue, consider converting all core methods to async variants if the base class supports it, or document that this implementation is blocking and unsuitable for high-concurrency scenarios.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T15:51:44Z",
    "updated_at": "2025-11-05T15:51:47Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2495150129",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150129"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2495150129"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150129/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 63,
    "original_start_line": 63,
    "start_side": "RIGHT",
    "line": 88,
    "original_line": 88,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 88,
    "position": 88,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150134",
    "pull_request_review_id": 3422976623,
    "id": 2495150134,
    "node_id": "PRRC_kwDOKSXUF86UuPg2",
    "diff_hunk": "@@ -0,0 +1,287 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using Newtonsoft.Json.Linq;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Net.Http;\n+using System.Text;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// ChromaDB-based document store designed for simplicity and developer experience.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// ChromaDB is an open-source vector database that emphasizes ease of use while maintaining\n+/// high performance for similarity search operations.\n+/// </remarks>\n+public class ChromaDBDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly HttpClient _httpClient;\n+    private readonly string _collectionName;\n+    private int _vectorDimension;\n+    private int _documentCount;\n+    private readonly Dictionary<string, VectorDocument<T>> _cache;\n+\n+    public override int DocumentCount => _documentCount;\n+    public override int VectorDimension => _vectorDimension;\n+\n+    public ChromaDBDocumentStore(string endpoint, string collectionName, string apiKey)\n+    {\n+        if (string.IsNullOrWhiteSpace(endpoint))\n+            throw new ArgumentException(\"Endpoint cannot be empty\", nameof(endpoint));\n+        if (string.IsNullOrWhiteSpace(collectionName))\n+            throw new ArgumentException(\"Collection name cannot be empty\", nameof(collectionName));\n+\n+        _httpClient = new HttpClient { BaseAddress = new Uri(endpoint) };\n+        if (!string.IsNullOrWhiteSpace(apiKey))\n+            _httpClient.DefaultRequestHeaders.Add(\"X-Chroma-Token\", apiKey);\n+\n+        _collectionName = collectionName;\n+        _vectorDimension = 0;\n+        _documentCount = 0;\n+        _cache = new Dictionary<string, VectorDocument<T>>();\n+\n+        EnsureCollection();\n+    }\n+\n+    private void EnsureCollection()\n+    {\n+        var payload = new { name = _collectionName };\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(payload),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        _httpClient.PostAsync(\"/api/v1/collections\", content).Wait();\n+    }\n+\n+    protected override void AddCore(VectorDocument<T> vectorDocument)\n+    {\n+        if (_vectorDimension == 0)\n+            _vectorDimension = vectorDocument.Embedding.Length;\n+\n+        _cache[vectorDocument.Document.Id] = vectorDocument;\n+\n+        var embedding = vectorDocument.Embedding.ToArray().Select(v => Convert.ToDouble(v)).ToList();\n+        \n+        var payload = new\n+        {\n+            ids = new[] { vectorDocument.Document.Id },\n+            embeddings = new[] { embedding },\n+            documents = new[] { vectorDocument.Document.Content },\n+            metadatas = new[] { vectorDocument.Document.Metadata }\n+        };\n+\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(payload),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        var response = _httpClient.PostAsync($\"/api/v1/collections/{_collectionName}/add\", content).Result;\n+        if (response.IsSuccessStatusCode)\n+            _documentCount++;\n+    }\n+\n+    protected override void AddBatchCore(IList<VectorDocument<T>> vectorDocuments)\n+    {\n+        if (vectorDocuments.Count == 0) return;\n+        \n+        if (_vectorDimension == 0)\n+            _vectorDimension = vectorDocuments[0].Embedding.Length;\n+\n+        foreach (var vd in vectorDocuments)\n+            _cache[vd.Document.Id] = vd;\n+\n+        var ids = vectorDocuments.Select(vd => vd.Document.Id).ToList();\n+        var embeddings = vectorDocuments.Select(vd => \n+            vd.Embedding.ToArray().Select(v => Convert.ToDouble(v)).ToList()).ToList();\n+        var documents = vectorDocuments.Select(vd => vd.Document.Content).ToList();\n+        var metadatas = vectorDocuments.Select(vd => vd.Document.Metadata).ToList();\n+\n+        var payload = new { ids, embeddings, documents, metadatas };\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(payload),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        var response = _httpClient.PostAsync($\"/api/v1/collections/{_collectionName}/add\", content).Result;\n+        if (response.IsSuccessStatusCode)\n+            _documentCount += vectorDocuments.Count;\n+    }\n+\n+    protected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)\n+    {\n+        var embedding = queryVector.ToArray().Select(v => Convert.ToDouble(v)).ToList();\n+        \n+        var payload = new\n+        {\n+            query_embeddings = new[] { embedding },\n+            n_results = topK\n+        };\n+\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(payload),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        var response = _httpClient.PostAsync($\"/api/v1/collections/{_collectionName}/query\", content).Result;\n+        var responseContent = response.Content.ReadAsStringAsync().Result;\n+        var result = JObject.Parse(responseContent);\n+\n+        var results = new List<Document<T>>();\n+        var ids = result[\"ids\"]?[0];\n+        var documents = result[\"documents\"]?[0];\n+        var metadatas = result[\"metadatas\"]?[0];\n+        var distances = result[\"distances\"]?[0];\n+\n+        if (ids == null || documents == null) return results;\n+\n+        for (int i = 0; i < ids.Count(); i++)\n+        {\n+            var idToken = ids[i];\n+            var docToken = documents[i];\n+            if (idToken == null || docToken == null) continue;\n+\n+            var id = idToken.ToString();\n+            var doc = docToken.ToString();\n+            var metadataObj = metadatas?[i]?.ToObject<Dictionary<string, object>>() ?? new Dictionary<string, object>();\n+\n+            var distance = distances != null ? Convert.ToDouble(distances[i]) : 0.0;\n+\n+            var document = new Document<T>(id, doc, metadataObj)\n+            {\n+                RelevanceScore = NumOps.FromDouble(1.0 / (1.0 + distance)),\n+                HasRelevanceScore = true\n+            };\n+\n+            results.Add(document);\n+        }\n+\n+        return results;\n+    }\n+\n+    protected override Document<T>? GetByIdCore(string documentId)\n+    {\n+        if (_cache.TryGetValue(documentId, out var vectorDoc))\n+            return vectorDoc.Document;\n+\n+        return null;\n+    }\n+\n+    /// <summary>\n+    /// Core logic for removing a document from the ChromaDB collection.\n+    /// </summary>\n+    /// <param name=\"documentId\">The validated document ID.</param>\n+    /// <returns>True if the document was found and removed; otherwise, false.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Removes the document from both the cache and the ChromaDB collection via API call.\n+    /// If successful, decrements the document count.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Deletes a document from ChromaDB.\n+    /// \n+    /// Example:\n+    /// <code>\n+    /// if (store.Remove(\"doc-123\"))\n+    ///     Console.WriteLine(\"Document removed from ChromaDB\");\n+    /// </code>\n+    /// </para>\n+    /// </remarks>\n+    protected override bool RemoveCore(string documentId)\n+    {\n+        _cache.Remove(documentId);\n+\n+        var payload = new { ids = new[] { documentId } };\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(payload),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        var response = _httpClient.PostAsync($\"/api/v1/collections/{_collectionName}/delete\", content).Result;\n+        if (response.IsSuccessStatusCode && _documentCount > 0)\n+        {\n+            _documentCount--;\n+            return true;\n+        }\n+        return false;\n+    }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/ChromaDBDocumentStore.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "79d215b091efb01a9397bf46a29a74fdee4d11bc",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Fix cache inconsistency on removal failure.**\n\nLine 197 removes the document from the cache before the ChromaDB API call. If the API request fails or returns an error, the cache is inconsistentΓÇödocument is gone from cache but still exists in ChromaDB.\n\n\nApply this diff:\n\n```diff\n     protected override bool RemoveCore(string documentId)\n     {\n-        _cache.Remove(documentId);\n\n         var payload = new { ids = new[] { documentId } };\n         // ... serialize and post ...\n\n         var response = _httpClient.PostAsync($\"/api/v1/collections/{_collectionName}/delete\", content).Result;\n         if (response.IsSuccessStatusCode && _documentCount > 0)\n         {\n+            _cache.Remove(documentId);\n             _documentCount--;\n             return true;\n         }\n         return false;\n     }\n```\n\nThis ensures cache is only updated after a successful ChromaDB deletion.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/ChromaDBDocumentStore.cs\naround lines 195 to 212, the code removes the document from _cache before\ncalling the ChromaDB delete API which causes cache inconsistency if the API call\nfails; move the _cache.Remove(documentId) (and the _documentCount-- decrement)\nso they execute only after verifying response.IsSuccessStatusCode, i.e. call the\nAPI first, check success, then remove from cache and decrement _documentCount\nand return true; if the API fails return false and leave the cache untouched.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T15:51:44Z",
    "updated_at": "2025-11-05T15:51:47Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2495150134",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150134"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2495150134"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150134/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 195,
    "original_start_line": 195,
    "start_side": "RIGHT",
    "line": 212,
    "original_line": 212,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 212,
    "position": 212,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150140",
    "pull_request_review_id": 3422976623,
    "id": 2495150140,
    "node_id": "PRRC_kwDOKSXUF86UuPg8",
    "diff_hunk": "@@ -0,0 +1,287 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using Newtonsoft.Json.Linq;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Net.Http;\n+using System.Text;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// ChromaDB-based document store designed for simplicity and developer experience.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// ChromaDB is an open-source vector database that emphasizes ease of use while maintaining\n+/// high performance for similarity search operations.\n+/// </remarks>\n+public class ChromaDBDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly HttpClient _httpClient;\n+    private readonly string _collectionName;\n+    private int _vectorDimension;\n+    private int _documentCount;\n+    private readonly Dictionary<string, VectorDocument<T>> _cache;\n+\n+    public override int DocumentCount => _documentCount;\n+    public override int VectorDimension => _vectorDimension;\n+\n+    public ChromaDBDocumentStore(string endpoint, string collectionName, string apiKey)\n+    {\n+        if (string.IsNullOrWhiteSpace(endpoint))\n+            throw new ArgumentException(\"Endpoint cannot be empty\", nameof(endpoint));\n+        if (string.IsNullOrWhiteSpace(collectionName))\n+            throw new ArgumentException(\"Collection name cannot be empty\", nameof(collectionName));\n+\n+        _httpClient = new HttpClient { BaseAddress = new Uri(endpoint) };\n+        if (!string.IsNullOrWhiteSpace(apiKey))\n+            _httpClient.DefaultRequestHeaders.Add(\"X-Chroma-Token\", apiKey);\n+\n+        _collectionName = collectionName;\n+        _vectorDimension = 0;\n+        _documentCount = 0;\n+        _cache = new Dictionary<string, VectorDocument<T>>();\n+\n+        EnsureCollection();\n+    }\n+\n+    private void EnsureCollection()\n+    {\n+        var payload = new { name = _collectionName };\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(payload),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        _httpClient.PostAsync(\"/api/v1/collections\", content).Wait();\n+    }\n+\n+    protected override void AddCore(VectorDocument<T> vectorDocument)\n+    {\n+        if (_vectorDimension == 0)\n+            _vectorDimension = vectorDocument.Embedding.Length;\n+\n+        _cache[vectorDocument.Document.Id] = vectorDocument;\n+\n+        var embedding = vectorDocument.Embedding.ToArray().Select(v => Convert.ToDouble(v)).ToList();\n+        \n+        var payload = new\n+        {\n+            ids = new[] { vectorDocument.Document.Id },\n+            embeddings = new[] { embedding },\n+            documents = new[] { vectorDocument.Document.Content },\n+            metadatas = new[] { vectorDocument.Document.Metadata }\n+        };\n+\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(payload),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        var response = _httpClient.PostAsync($\"/api/v1/collections/{_collectionName}/add\", content).Result;\n+        if (response.IsSuccessStatusCode)\n+            _documentCount++;\n+    }\n+\n+    protected override void AddBatchCore(IList<VectorDocument<T>> vectorDocuments)\n+    {\n+        if (vectorDocuments.Count == 0) return;\n+        \n+        if (_vectorDimension == 0)\n+            _vectorDimension = vectorDocuments[0].Embedding.Length;\n+\n+        foreach (var vd in vectorDocuments)\n+            _cache[vd.Document.Id] = vd;\n+\n+        var ids = vectorDocuments.Select(vd => vd.Document.Id).ToList();\n+        var embeddings = vectorDocuments.Select(vd => \n+            vd.Embedding.ToArray().Select(v => Convert.ToDouble(v)).ToList()).ToList();\n+        var documents = vectorDocuments.Select(vd => vd.Document.Content).ToList();\n+        var metadatas = vectorDocuments.Select(vd => vd.Document.Metadata).ToList();\n+\n+        var payload = new { ids, embeddings, documents, metadatas };\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(payload),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        var response = _httpClient.PostAsync($\"/api/v1/collections/{_collectionName}/add\", content).Result;\n+        if (response.IsSuccessStatusCode)\n+            _documentCount += vectorDocuments.Count;\n+    }\n+\n+    protected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)\n+    {\n+        var embedding = queryVector.ToArray().Select(v => Convert.ToDouble(v)).ToList();\n+        \n+        var payload = new\n+        {\n+            query_embeddings = new[] { embedding },\n+            n_results = topK\n+        };\n+\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(payload),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        var response = _httpClient.PostAsync($\"/api/v1/collections/{_collectionName}/query\", content).Result;\n+        var responseContent = response.Content.ReadAsStringAsync().Result;\n+        var result = JObject.Parse(responseContent);\n+\n+        var results = new List<Document<T>>();\n+        var ids = result[\"ids\"]?[0];\n+        var documents = result[\"documents\"]?[0];\n+        var metadatas = result[\"metadatas\"]?[0];\n+        var distances = result[\"distances\"]?[0];\n+\n+        if (ids == null || documents == null) return results;\n+\n+        for (int i = 0; i < ids.Count(); i++)\n+        {\n+            var idToken = ids[i];\n+            var docToken = documents[i];\n+            if (idToken == null || docToken == null) continue;\n+\n+            var id = idToken.ToString();\n+            var doc = docToken.ToString();\n+            var metadataObj = metadatas?[i]?.ToObject<Dictionary<string, object>>() ?? new Dictionary<string, object>();\n+\n+            var distance = distances != null ? Convert.ToDouble(distances[i]) : 0.0;\n+\n+            var document = new Document<T>(id, doc, metadataObj)\n+            {\n+                RelevanceScore = NumOps.FromDouble(1.0 / (1.0 + distance)),\n+                HasRelevanceScore = true\n+            };\n+\n+            results.Add(document);\n+        }\n+\n+        return results;\n+    }\n+\n+    protected override Document<T>? GetByIdCore(string documentId)\n+    {\n+        if (_cache.TryGetValue(documentId, out var vectorDoc))\n+            return vectorDoc.Document;\n+\n+        return null;\n+    }\n+\n+    /// <summary>\n+    /// Core logic for removing a document from the ChromaDB collection.\n+    /// </summary>\n+    /// <param name=\"documentId\">The validated document ID.</param>\n+    /// <returns>True if the document was found and removed; otherwise, false.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Removes the document from both the cache and the ChromaDB collection via API call.\n+    /// If successful, decrements the document count.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Deletes a document from ChromaDB.\n+    /// \n+    /// Example:\n+    /// <code>\n+    /// if (store.Remove(\"doc-123\"))\n+    ///     Console.WriteLine(\"Document removed from ChromaDB\");\n+    /// </code>\n+    /// </para>\n+    /// </remarks>\n+    protected override bool RemoveCore(string documentId)\n+    {\n+        _cache.Remove(documentId);\n+\n+        var payload = new { ids = new[] { documentId } };\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(payload),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        var response = _httpClient.PostAsync($\"/api/v1/collections/{_collectionName}/delete\", content).Result;\n+        if (response.IsSuccessStatusCode && _documentCount > 0)\n+        {\n+            _documentCount--;\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+    /// <summary>\n+    /// Core logic for retrieving all documents from the ChromaDB collection.\n+    /// </summary>\n+    /// <returns>An enumerable of all documents without their vector embeddings.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Returns all documents from the cache (in-memory representation) of the ChromaDB collection.\n+    /// In a real ChromaDB deployment, this would query the collection's entire dataset.\n+    /// Vector embeddings are not included in the results.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Gets every document from the ChromaDB collection.\n+    /// \n+    /// Use cases:\n+    /// - Export collection contents for backup\n+    /// - Migrate to a different ChromaDB collection or database\n+    /// - Bulk processing or reindexing\n+    /// - Debugging to see all stored documents\n+    /// \n+    /// Warning: For large collections (> 10K documents), this can use significant memory.\n+    /// In production ChromaDB, consider using pagination with limit/offset parameters.\n+    /// \n+    /// Example:\n+    /// <code>\n+    /// // Get all documents\n+    /// var allDocs = store.GetAll().ToList();\n+    /// Console.WriteLine($\"Total documents in {_collectionName}: {allDocs.Count}\");\n+    /// \n+    /// // Export to JSON\n+    /// var json = JsonConvert.SerializeObject(allDocs);\n+    /// File.WriteAllText($\"chroma_{_collectionName}_export.json\", json);\n+    /// </code>\n+    /// </para>\n+    /// </remarks>\n+    protected override IEnumerable<Document<T>> GetAllCore()\n+    {\n+        return _cache.Values.Select(vd => vd.Document).ToList();\n+    }\n+\n+    /// <summary>\n+    /// Removes all documents from the ChromaDB collection and recreates it.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Clears the cache, deletes the ChromaDB collection, resets counters, and recreates an empty collection.\n+    /// The collection name remains unchanged and is ready to accept new documents.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Completely empties the ChromaDB collection.\n+    /// \n+    /// After calling Clear():\n+    /// - All documents are removed from ChromaDB\n+    /// - Cache is cleared\n+    /// - Document count resets to 0\n+    /// - Vector dimension resets to 0\n+    /// - Collection is recreated (empty)\n+    /// - Ready for new documents\n+    /// \n+    /// Use with caution - this cannot be undone!\n+    /// \n+    /// Example:\n+    /// <code>\n+    /// store.Clear();\n+    /// Console.WriteLine($\"Documents in collection: {store.DocumentCount}\"); // 0\n+    /// </code>\n+    /// </para>\n+    /// </remarks>\n+    public override void Clear()\n+    {\n+        _cache.Clear();\n+        _httpClient.DeleteAsync($\"/api/v1/collections/{_collectionName}\").Wait();\n+        _documentCount = 0;\n+        _vectorDimension = 0;\n+        EnsureCollection();\n+    }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/ChromaDBDocumentStore.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "79d215b091efb01a9397bf46a29a74fdee4d11bc",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Add error handling and avoid synchronous blocking.**\n\nTwo issues:\n\n1. **No error handling** (line 282): If `DeleteAsync` fails (collection doesn't exist, network error), the exception propagates but counters are already reset, leaving the store in an inconsistent state.\n2. **Synchronous blocking** (lines 282, 285): `.Wait()` can cause deadlocks.\n\n\nApply this diff to handle errors gracefully:\n\n```diff\n     public override void Clear()\n     {\n-        _cache.Clear();\n-        _httpClient.DeleteAsync($\"/api/v1/collections/{_collectionName}\").Wait();\n-        _documentCount = 0;\n-        _vectorDimension = 0;\n-        EnsureCollection();\n+        try\n+        {\n+            _httpClient.DeleteAsync($\"/api/v1/collections/{_collectionName}\").Wait();\n+        }\n+        catch (Exception ex)\n+        {\n+            // Log error or handle collection not existing\n+            // Optionally: ignore 404 if collection already deleted\n+        }\n+        \n+        _cache.Clear();\n+        _documentCount = 0;\n+        _vectorDimension = 0;\n+        EnsureCollection();\n     }\n```\n\nConsider making this async or document that blocking behavior is expected.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/ChromaDBDocumentStore.cs\naround lines 279-286, the Clear method synchronously blocks on\n_httpClient.DeleteAsync and resets internal counters before the delete\ncompletes; change Clear to an async method (e.g., Task ClearAsync), await\n_httpClient.DeleteAsync(...).ConfigureAwait(false), and wrap the HTTP call in a\ntry/catch to handle HttpRequestException/TaskCanceledException/Exception: on\nsuccess reset _documentCount and _vectorDimension and call EnsureCollection, on\nfailure log the error and avoid leaving counters in an inconsistent state\n(either keep previous values or set a safe fallback), and rethrow or surface the\nerror as appropriate; if you cannot change the API to async, at minimum use\nGetAwaiter().GetResult() and catch exceptions rather than using .Wait(), and\ndocument the blocking behavior.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T15:51:44Z",
    "updated_at": "2025-11-05T15:51:47Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2495150140",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150140"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2495150140"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150140/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 279,
    "original_start_line": 279,
    "start_side": "RIGHT",
    "line": 286,
    "original_line": 286,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 286,
    "position": 286,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150149",
    "pull_request_review_id": 3422976623,
    "id": 2495150149,
    "node_id": "PRRC_kwDOKSXUF86UuPhF",
    "diff_hunk": "@@ -0,0 +1,376 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using Newtonsoft.Json.Linq;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Net.Http;\n+using System.Text;\n+",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/ElasticsearchDocumentStore.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "79d215b091efb01a9397bf46a29a74fdee4d11bc",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Fix the import formatting issue.**\n\nThe pipeline reports an import/order formatting issue. Please run the project's code formatter to resolve this.\n\n<details>\n<summary>≡ƒº░ Tools</summary>\n\n<details>\n<summary>≡ƒ¬¢ GitHub Actions: Build</summary>\n\n[error] 1-1: Import/order formatting issue at line 1.\n\n</details>\n\n</details>\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/ElasticsearchDocumentStore.cs\naround lines 1 to 11, the using/import statements are not formatted in the\nproject's expected order; run the repository's C# formatter (e.g., dotnet format\nor your configured IDE formatter) to sort and normalize the using directives\n(remove duplicates, sort alphabetically/group system first, then thirdΓÇæparty,\nthen project namespaces) and fix spacing so the import/order linter passes.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T15:51:44Z",
    "updated_at": "2025-11-05T15:51:47Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2495150149",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150149"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2495150149"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150149/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 1,
    "original_start_line": 1,
    "start_side": "RIGHT",
    "line": 11,
    "original_line": 11,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 11,
    "position": 11,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150158",
    "pull_request_review_id": 3422976623,
    "id": 2495150158,
    "node_id": "PRRC_kwDOKSXUF86UuPhO",
    "diff_hunk": "@@ -0,0 +1,376 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using Newtonsoft.Json.Linq;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Net.Http;\n+using System.Text;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// Elasticsearch-based document store providing hybrid search capabilities (BM25 + dense vectors).\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Elasticsearch combines traditional full-text search (BM25) with vector similarity search,\n+/// making it ideal for hybrid retrieval scenarios where both keyword matching and semantic\n+/// similarity are important.\n+/// </remarks>\n+public class ElasticsearchDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly HttpClient _httpClient;\n+    private readonly string _indexName;\n+    private int _vectorDimension;\n+    private int _documentCount;\n+    private readonly Dictionary<string, VectorDocument<T>> _cache;\n+\n+    public override int DocumentCount => _documentCount;\n+    public override int VectorDimension => _vectorDimension;\n+\n+    public ElasticsearchDocumentStore(string endpoint, string indexName, string apiKey, string username, string password, int vectorDimension = 1536)\n+    {\n+        if (string.IsNullOrWhiteSpace(endpoint))\n+            throw new ArgumentException(\"Endpoint cannot be empty\", nameof(endpoint));\n+        if (string.IsNullOrWhiteSpace(indexName))\n+            throw new ArgumentException(\"Index name cannot be empty\", nameof(indexName));\n+        if (vectorDimension <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(vectorDimension), \"Vector dimension must be positive\");\n+\n+        _httpClient = new HttpClient { BaseAddress = new Uri(endpoint) };\n+        \n+        if (!string.IsNullOrWhiteSpace(apiKey))\n+            _httpClient.DefaultRequestHeaders.Add(\"Authorization\", $\"ApiKey {apiKey}\");\n+        else if (!string.IsNullOrWhiteSpace(username) && !string.IsNullOrWhiteSpace(password))\n+        {\n+            var auth = Convert.ToBase64String(Encoding.UTF8.GetBytes($\"{username}:{password}\"));\n+            _httpClient.DefaultRequestHeaders.Add(\"Authorization\", $\"Basic {auth}\");\n+        }\n+\n+        _indexName = indexName.ToLowerInvariant();\n+        _vectorDimension = vectorDimension;\n+        _documentCount = 0;\n+        _cache = new Dictionary<string, VectorDocument<T>>();\n+\n+        EnsureIndex();\n+    }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/ElasticsearchDocumentStore.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "79d215b091efb01a9397bf46a29a74fdee4d11bc",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Validate that authentication credentials are provided.**\n\nThe constructor accepts both `apiKey` and `username`/`password` but doesn't ensure at least one authentication method is supplied. If all auth parameters are null or empty, the HTTP client will make unauthenticated requests that will likely fail.\n\n\n\nApply this diff to validate authentication:\n\n```diff\n     if (vectorDimension <= 0)\n         throw new ArgumentOutOfRangeException(nameof(vectorDimension), \"Vector dimension must be positive\");\n+    \n+    if (string.IsNullOrWhiteSpace(apiKey) && \n+        (string.IsNullOrWhiteSpace(username) || string.IsNullOrWhiteSpace(password)))\n+        throw new ArgumentException(\"Either apiKey or both username and password must be provided for authentication\");\n\n     _httpClient = new HttpClient { BaseAddress = new Uri(endpoint) };\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    public ElasticsearchDocumentStore(string endpoint, string indexName, string apiKey, string username, string password, int vectorDimension = 1536)\n    {\n        if (string.IsNullOrWhiteSpace(endpoint))\n            throw new ArgumentException(\"Endpoint cannot be empty\", nameof(endpoint));\n        if (string.IsNullOrWhiteSpace(indexName))\n            throw new ArgumentException(\"Index name cannot be empty\", nameof(indexName));\n        if (vectorDimension <= 0)\n            throw new ArgumentOutOfRangeException(nameof(vectorDimension), \"Vector dimension must be positive\");\n        \n        if (string.IsNullOrWhiteSpace(apiKey) && \n            (string.IsNullOrWhiteSpace(username) || string.IsNullOrWhiteSpace(password)))\n            throw new ArgumentException(\"Either apiKey or both username and password must be provided for authentication\");\n\n        _httpClient = new HttpClient { BaseAddress = new Uri(endpoint) };\n        \n        if (!string.IsNullOrWhiteSpace(apiKey))\n            _httpClient.DefaultRequestHeaders.Add(\"Authorization\", $\"ApiKey {apiKey}\");\n        else if (!string.IsNullOrWhiteSpace(username) && !string.IsNullOrWhiteSpace(password))\n        {\n            var auth = Convert.ToBase64String(Encoding.UTF8.GetBytes($\"{username}:{password}\"));\n            _httpClient.DefaultRequestHeaders.Add(\"Authorization\", $\"Basic {auth}\");\n        }\n\n        _indexName = indexName.ToLowerInvariant();\n        _vectorDimension = vectorDimension;\n        _documentCount = 0;\n        _cache = new Dictionary<string, VectorDocument<T>>();\n\n        EnsureIndex();\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/ElasticsearchDocumentStore.cs\naround lines 34 to 59, the constructor currently allows creating an\nunauthenticated HttpClient if both apiKey and username/password are missing; add\na precondition that at least one authentication method is supplied and throw an\nArgumentException (or similar) if neither apiKey nor both username and password\nare provided. Implement the check before creating the HttpClient (or before\nsetting Authorization headers), validating string.IsNullOrWhiteSpace(apiKey) AND\n(string.IsNullOrWhiteSpace(username) OR string.IsNullOrWhiteSpace(password)) and\nthrow with a clear message like \"Authentication required: provide apiKey or\nusername and password\".\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T15:51:45Z",
    "updated_at": "2025-11-05T15:51:47Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2495150158",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150158"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2495150158"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150158/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 34,
    "original_start_line": 34,
    "start_side": "RIGHT",
    "line": 59,
    "original_line": 59,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 59,
    "position": 59,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150164",
    "pull_request_review_id": 3422976623,
    "id": 2495150164,
    "node_id": "PRRC_kwDOKSXUF86UuPhU",
    "diff_hunk": "@@ -0,0 +1,376 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using Newtonsoft.Json.Linq;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Net.Http;\n+using System.Text;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// Elasticsearch-based document store providing hybrid search capabilities (BM25 + dense vectors).\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Elasticsearch combines traditional full-text search (BM25) with vector similarity search,\n+/// making it ideal for hybrid retrieval scenarios where both keyword matching and semantic\n+/// similarity are important.\n+/// </remarks>\n+public class ElasticsearchDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly HttpClient _httpClient;\n+    private readonly string _indexName;\n+    private int _vectorDimension;\n+    private int _documentCount;\n+    private readonly Dictionary<string, VectorDocument<T>> _cache;\n+\n+    public override int DocumentCount => _documentCount;\n+    public override int VectorDimension => _vectorDimension;\n+\n+    public ElasticsearchDocumentStore(string endpoint, string indexName, string apiKey, string username, string password, int vectorDimension = 1536)\n+    {\n+        if (string.IsNullOrWhiteSpace(endpoint))\n+            throw new ArgumentException(\"Endpoint cannot be empty\", nameof(endpoint));\n+        if (string.IsNullOrWhiteSpace(indexName))\n+            throw new ArgumentException(\"Index name cannot be empty\", nameof(indexName));\n+        if (vectorDimension <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(vectorDimension), \"Vector dimension must be positive\");\n+\n+        _httpClient = new HttpClient { BaseAddress = new Uri(endpoint) };\n+        \n+        if (!string.IsNullOrWhiteSpace(apiKey))\n+            _httpClient.DefaultRequestHeaders.Add(\"Authorization\", $\"ApiKey {apiKey}\");\n+        else if (!string.IsNullOrWhiteSpace(username) && !string.IsNullOrWhiteSpace(password))\n+        {\n+            var auth = Convert.ToBase64String(Encoding.UTF8.GetBytes($\"{username}:{password}\"));\n+            _httpClient.DefaultRequestHeaders.Add(\"Authorization\", $\"Basic {auth}\");\n+        }\n+\n+        _indexName = indexName.ToLowerInvariant();\n+        _vectorDimension = vectorDimension;\n+        _documentCount = 0;\n+        _cache = new Dictionary<string, VectorDocument<T>>();\n+\n+        EnsureIndex();\n+    }\n+\n+    private void EnsureIndex()\n+    {\n+        var checkResponse = _httpClient.GetAsync($\"/{_indexName}\").Result;\n+        if (checkResponse.IsSuccessStatusCode) \n+        {\n+            UpdateDocumentCount();\n+            return;\n+        }\n+\n+        var mapping = new\n+        {\n+            mappings = new\n+            {\n+                properties = new\n+                {\n+                    id = new { type = \"keyword\" },\n+                    content = new { type = \"text\" },\n+                    embedding = new { type = \"dense_vector\", dims = _vectorDimension },\n+                    metadata = new { type = \"object\", enabled = true }\n+                }\n+            }\n+        };\n+\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(mapping),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        _httpClient.PutAsync($\"/{_indexName}\", content).Wait();\n+    }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/ElasticsearchDocumentStore.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "79d215b091efb01a9397bf46a29a74fdee4d11bc",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Add error handling and consider async alternatives.**\n\nThe method uses `.Result` to block on async HTTP operations, which can cause deadlocks in UI or ASP.NET contexts and is generally considered an anti-pattern. Additionally, there's no error handling if HTTP requests fail (network issues, auth failures, Elasticsearch errors).\n\n\n\nConsider these improvements:\n\n1. Add try-catch blocks around HTTP calls to handle failures gracefully\n2. Check response status codes and throw meaningful exceptions\n3. If keeping synchronous API, document the blocking behavior\n4. Consider providing an async variant (`EnsureIndexAsync()`) for better performance\n\nExample error handling:\n\n```csharp\nprivate void EnsureIndex()\n{\n    try\n    {\n        var checkResponse = _httpClient.GetAsync($\"/{_indexName}\").Result;\n        if (checkResponse.IsSuccessStatusCode) \n        {\n            UpdateDocumentCount();\n            return;\n        }\n        \n        if (!checkResponse.StatusCode == System.Net.HttpStatusCode.NotFound)\n        {\n            throw new InvalidOperationException(\n                $\"Failed to check index existence: {checkResponse.StatusCode}\");\n        }\n\n        // Create index...\n        var createResponse = _httpClient.PutAsync($\"/{_indexName}\", content).Result;\n        if (!createResponse.IsSuccessStatusCode)\n        {\n            var errorContent = createResponse.Content.ReadAsStringAsync().Result;\n            throw new InvalidOperationException(\n                $\"Failed to create index: {createResponse.StatusCode}, {errorContent}\");\n        }\n    }\n    catch (Exception ex) when (!(ex is InvalidOperationException))\n    {\n        throw new InvalidOperationException(\n            $\"Error ensuring Elasticsearch index '{_indexName}': {ex.Message}\", ex);\n    }\n}\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nsrc/RetrievalAugmentedGeneration/DocumentStores/ElasticsearchDocumentStore.cs\nlines 61-90: the method blocks on async HTTP calls using .Result and lacks error\nhandling; replace blocking calls with proper async alternatives by implementing\nEnsureIndexAsync() that awaits GetAsync/PutAsync, add try-catch around the HTTP\ncalls to catch and rethrow meaningful InvalidOperationException (including\nresponse.StatusCode and response.Content when non-success), validate NotFound vs\nother statuses before attempting index creation, and if you must keep a\nsynchronous wrapper either document the blocking behavior or call the async\nmethod via Task.Run(...).GetAwaiter().GetResult() with explicit comment to\nreduce deadlock risk.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T15:51:45Z",
    "updated_at": "2025-11-05T15:51:47Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2495150164",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150164"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2495150164"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150164/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 61,
    "original_start_line": 61,
    "start_side": "RIGHT",
    "line": 90,
    "original_line": 90,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 90,
    "position": 90,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150170",
    "pull_request_review_id": 3422976623,
    "id": 2495150170,
    "node_id": "PRRC_kwDOKSXUF86UuPha",
    "diff_hunk": "@@ -0,0 +1,376 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using Newtonsoft.Json.Linq;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Net.Http;\n+using System.Text;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// Elasticsearch-based document store providing hybrid search capabilities (BM25 + dense vectors).\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Elasticsearch combines traditional full-text search (BM25) with vector similarity search,\n+/// making it ideal for hybrid retrieval scenarios where both keyword matching and semantic\n+/// similarity are important.\n+/// </remarks>\n+public class ElasticsearchDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly HttpClient _httpClient;\n+    private readonly string _indexName;\n+    private int _vectorDimension;\n+    private int _documentCount;\n+    private readonly Dictionary<string, VectorDocument<T>> _cache;\n+\n+    public override int DocumentCount => _documentCount;\n+    public override int VectorDimension => _vectorDimension;\n+\n+    public ElasticsearchDocumentStore(string endpoint, string indexName, string apiKey, string username, string password, int vectorDimension = 1536)\n+    {\n+        if (string.IsNullOrWhiteSpace(endpoint))\n+            throw new ArgumentException(\"Endpoint cannot be empty\", nameof(endpoint));\n+        if (string.IsNullOrWhiteSpace(indexName))\n+            throw new ArgumentException(\"Index name cannot be empty\", nameof(indexName));\n+        if (vectorDimension <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(vectorDimension), \"Vector dimension must be positive\");\n+\n+        _httpClient = new HttpClient { BaseAddress = new Uri(endpoint) };\n+        \n+        if (!string.IsNullOrWhiteSpace(apiKey))\n+            _httpClient.DefaultRequestHeaders.Add(\"Authorization\", $\"ApiKey {apiKey}\");\n+        else if (!string.IsNullOrWhiteSpace(username) && !string.IsNullOrWhiteSpace(password))\n+        {\n+            var auth = Convert.ToBase64String(Encoding.UTF8.GetBytes($\"{username}:{password}\"));\n+            _httpClient.DefaultRequestHeaders.Add(\"Authorization\", $\"Basic {auth}\");\n+        }\n+\n+        _indexName = indexName.ToLowerInvariant();\n+        _vectorDimension = vectorDimension;\n+        _documentCount = 0;\n+        _cache = new Dictionary<string, VectorDocument<T>>();\n+\n+        EnsureIndex();\n+    }\n+\n+    private void EnsureIndex()\n+    {\n+        var checkResponse = _httpClient.GetAsync($\"/{_indexName}\").Result;\n+        if (checkResponse.IsSuccessStatusCode) \n+        {\n+            UpdateDocumentCount();\n+            return;\n+        }\n+\n+        var mapping = new\n+        {\n+            mappings = new\n+            {\n+                properties = new\n+                {\n+                    id = new { type = \"keyword\" },\n+                    content = new { type = \"text\" },\n+                    embedding = new { type = \"dense_vector\", dims = _vectorDimension },\n+                    metadata = new { type = \"object\", enabled = true }\n+                }\n+            }\n+        };\n+\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(mapping),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        _httpClient.PutAsync($\"/{_indexName}\", content).Wait();\n+    }\n+\n+    private void UpdateDocumentCount()\n+    {\n+        var response = _httpClient.GetAsync($\"/{_indexName}/_count\").Result;\n+        if (response.IsSuccessStatusCode)\n+        {\n+            var responseContent = response.Content.ReadAsStringAsync().Result;\n+            var result = JObject.Parse(responseContent);\n+            _documentCount = result[\"count\"]?.Value<int>() ?? 0;\n+        }\n+    }\n+\n+    protected override void AddCore(VectorDocument<T> vectorDocument)\n+    {\n+        if (vectorDocument.Embedding.Length != _vectorDimension)\n+            throw new ArgumentException($\"Document embedding dimension ({vectorDocument.Embedding.Length}) does not match the store's configured dimension ({_vectorDimension}).\");\n+\n+        _cache[vectorDocument.Document.Id] = vectorDocument;\n+\n+        var embedding = vectorDocument.Embedding.ToArray().Select(v => Convert.ToDouble(v)).ToArray();\n+        \n+        var doc = new\n+        {\n+            id = vectorDocument.Document.Id,\n+            content = vectorDocument.Document.Content,\n+            embedding,\n+            metadata = vectorDocument.Document.Metadata\n+        };\n+\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(doc),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        var response = _httpClient.PutAsync($\"/{_indexName}/_doc/{vectorDocument.Document.Id}\", content).Result;\n+        if (response.IsSuccessStatusCode)\n+            _documentCount++;\n+    }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/ElasticsearchDocumentStore.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "79d215b091efb01a9397bf46a29a74fdee4d11bc",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Fix cache inconsistency and Clear() interaction bug.**\n\nTwo critical issues:\n\n1. **Clear() breaks subsequent adds**: The `Clear()` method sets `_vectorDimension = 0`, which causes this validation check to reject all documents after a clear operation. Since no document can have embedding length 0, the store becomes unusable after calling `Clear()`.\n\n2. **Cache/Elasticsearch inconsistency**: The document is added to `_cache` (line 108) before the HTTP call. If the PUT request fails, the document remains in the cache but isn't in Elasticsearch, causing `GetByIdCore` to return documents that aren't actually stored.\n\n\n\nApply this diff to fix both issues:\n\n```diff\n protected override void AddCore(VectorDocument<T> vectorDocument)\n {\n+    // Allow _vectorDimension to be set from first document if cleared\n+    if (_vectorDimension == 0)\n+        _vectorDimension = vectorDocument.Embedding.Length;\n+    \n     if (vectorDocument.Embedding.Length != _vectorDimension)\n         throw new ArgumentException($\"Document embedding dimension ({vectorDocument.Embedding.Length}) does not match the store's configured dimension ({_vectorDimension}).\");\n\n-    _cache[vectorDocument.Document.Id] = vectorDocument;\n-\n     var embedding = vectorDocument.Embedding.ToArray().Select(v => Convert.ToDouble(v)).ToArray();\n     \n     var doc = new\n     {\n         id = vectorDocument.Document.Id,\n         content = vectorDocument.Document.Content,\n         embedding,\n         metadata = vectorDocument.Document.Metadata\n     };\n\n     var content = new StringContent(\n         Newtonsoft.Json.JsonConvert.SerializeObject(doc),\n         Encoding.UTF8,\n         \"application/json\");\n\n     var response = _httpClient.PutAsync($\"/{_indexName}/_doc/{vectorDocument.Document.Id}\", content).Result;\n     if (response.IsSuccessStatusCode)\n+    {\n+        _cache[vectorDocument.Document.Id] = vectorDocument;\n         _documentCount++;\n+    }\n+    else\n+    {\n+        var errorContent = response.Content.ReadAsStringAsync().Result;\n+        throw new InvalidOperationException(\n+            $\"Failed to add document '{vectorDocument.Document.Id}': {response.StatusCode}, {errorContent}\");\n+    }\n }\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/ElasticsearchDocumentStore.cs\naround lines 103 to 128, the code currently rejects all adds after Clear()\nbecause Clear() sets _vectorDimension = 0 and the validation unconditionally\ncompares embedding length to _vectorDimension, and it also writes to _cache\nbefore the HTTP PUT so failed requests leave stale cache entries. Change the\nvalidation to only enforce the dimension check when _vectorDimension is nonΓÇæzero\n(e.g., if (_vectorDimension != 0 && vectorDocument.Embedding.Length !=\n_vectorDimension) throw ...), and move the _cache update and _documentCount\nincrement to occur only after a successful HTTP response (i.e., perform the PUT,\ncheck response.IsSuccessStatusCode, then update _cache and _documentCount).\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T15:51:45Z",
    "updated_at": "2025-11-05T15:51:47Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2495150170",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150170"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2495150170"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150170/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 103,
    "original_start_line": 103,
    "start_side": "RIGHT",
    "line": 128,
    "original_line": 128,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 128,
    "position": 128,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150175",
    "pull_request_review_id": 3422976623,
    "id": 2495150175,
    "node_id": "PRRC_kwDOKSXUF86UuPhf",
    "diff_hunk": "@@ -0,0 +1,376 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using Newtonsoft.Json.Linq;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Net.Http;\n+using System.Text;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// Elasticsearch-based document store providing hybrid search capabilities (BM25 + dense vectors).\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Elasticsearch combines traditional full-text search (BM25) with vector similarity search,\n+/// making it ideal for hybrid retrieval scenarios where both keyword matching and semantic\n+/// similarity are important.\n+/// </remarks>\n+public class ElasticsearchDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly HttpClient _httpClient;\n+    private readonly string _indexName;\n+    private int _vectorDimension;\n+    private int _documentCount;\n+    private readonly Dictionary<string, VectorDocument<T>> _cache;\n+\n+    public override int DocumentCount => _documentCount;\n+    public override int VectorDimension => _vectorDimension;\n+\n+    public ElasticsearchDocumentStore(string endpoint, string indexName, string apiKey, string username, string password, int vectorDimension = 1536)\n+    {\n+        if (string.IsNullOrWhiteSpace(endpoint))\n+            throw new ArgumentException(\"Endpoint cannot be empty\", nameof(endpoint));\n+        if (string.IsNullOrWhiteSpace(indexName))\n+            throw new ArgumentException(\"Index name cannot be empty\", nameof(indexName));\n+        if (vectorDimension <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(vectorDimension), \"Vector dimension must be positive\");\n+\n+        _httpClient = new HttpClient { BaseAddress = new Uri(endpoint) };\n+        \n+        if (!string.IsNullOrWhiteSpace(apiKey))\n+            _httpClient.DefaultRequestHeaders.Add(\"Authorization\", $\"ApiKey {apiKey}\");\n+        else if (!string.IsNullOrWhiteSpace(username) && !string.IsNullOrWhiteSpace(password))\n+        {\n+            var auth = Convert.ToBase64String(Encoding.UTF8.GetBytes($\"{username}:{password}\"));\n+            _httpClient.DefaultRequestHeaders.Add(\"Authorization\", $\"Basic {auth}\");\n+        }\n+\n+        _indexName = indexName.ToLowerInvariant();\n+        _vectorDimension = vectorDimension;\n+        _documentCount = 0;\n+        _cache = new Dictionary<string, VectorDocument<T>>();\n+\n+        EnsureIndex();\n+    }\n+\n+    private void EnsureIndex()\n+    {\n+        var checkResponse = _httpClient.GetAsync($\"/{_indexName}\").Result;\n+        if (checkResponse.IsSuccessStatusCode) \n+        {\n+            UpdateDocumentCount();\n+            return;\n+        }\n+\n+        var mapping = new\n+        {\n+            mappings = new\n+            {\n+                properties = new\n+                {\n+                    id = new { type = \"keyword\" },\n+                    content = new { type = \"text\" },\n+                    embedding = new { type = \"dense_vector\", dims = _vectorDimension },\n+                    metadata = new { type = \"object\", enabled = true }\n+                }\n+            }\n+        };\n+\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(mapping),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        _httpClient.PutAsync($\"/{_indexName}\", content).Wait();\n+    }\n+\n+    private void UpdateDocumentCount()\n+    {\n+        var response = _httpClient.GetAsync($\"/{_indexName}/_count\").Result;\n+        if (response.IsSuccessStatusCode)\n+        {\n+            var responseContent = response.Content.ReadAsStringAsync().Result;\n+            var result = JObject.Parse(responseContent);\n+            _documentCount = result[\"count\"]?.Value<int>() ?? 0;\n+        }\n+    }\n+\n+    protected override void AddCore(VectorDocument<T> vectorDocument)\n+    {\n+        if (vectorDocument.Embedding.Length != _vectorDimension)\n+            throw new ArgumentException($\"Document embedding dimension ({vectorDocument.Embedding.Length}) does not match the store's configured dimension ({_vectorDimension}).\");\n+\n+        _cache[vectorDocument.Document.Id] = vectorDocument;\n+\n+        var embedding = vectorDocument.Embedding.ToArray().Select(v => Convert.ToDouble(v)).ToArray();\n+        \n+        var doc = new\n+        {\n+            id = vectorDocument.Document.Id,\n+            content = vectorDocument.Document.Content,\n+            embedding,\n+            metadata = vectorDocument.Document.Metadata\n+        };\n+\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(doc),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        var response = _httpClient.PutAsync($\"/{_indexName}/_doc/{vectorDocument.Document.Id}\", content).Result;\n+        if (response.IsSuccessStatusCode)\n+            _documentCount++;\n+    }\n+\n+    protected override void AddBatchCore(IList<VectorDocument<T>> vectorDocuments)\n+    {\n+        if (vectorDocuments.Count == 0) return;\n+\n+        foreach (var vd in vectorDocuments)\n+        {\n+            if (vd.Embedding.Length != _vectorDimension)\n+                throw new ArgumentException($\"Document embedding dimension ({vd.Embedding.Length}) does not match the store's configured dimension ({_vectorDimension}).\");\n+            _cache[vd.Document.Id] = vd;\n+        }\n+\n+        var bulkBody = new StringBuilder();\n+        foreach (var vd in vectorDocuments)\n+        {\n+            var indexAction = new { index = new { _index = _indexName, _id = vd.Document.Id } };\n+            bulkBody.AppendLine(Newtonsoft.Json.JsonConvert.SerializeObject(indexAction));\n+\n+            var embedding = vd.Embedding.ToArray().Select(v => Convert.ToDouble(v)).ToArray();\n+            var doc = new\n+            {\n+                id = vd.Document.Id,\n+                content = vd.Document.Content,\n+                embedding,\n+                metadata = vd.Document.Metadata\n+            };\n+            bulkBody.AppendLine(Newtonsoft.Json.JsonConvert.SerializeObject(doc));\n+        }\n+\n+        var content = new StringContent(bulkBody.ToString(), Encoding.UTF8, \"application/x-ndjson\");\n+        var response = _httpClient.PostAsync($\"/{_indexName}/_bulk\", content).Result;\n+        if (response.IsSuccessStatusCode)\n+            _documentCount += vectorDocuments.Count;\n+    }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/ElasticsearchDocumentStore.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "79d215b091efb01a9397bf46a29a74fdee4d11bc",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Fix cache inconsistency, Clear() interaction, and bulk error handling.**\n\nMultiple critical issues:\n\n1. **Clear() breaks subsequent adds**: Same issue as `AddCore` - if `_vectorDimension` is 0 after `Clear()`, validation fails for all documents.\n\n2. **Cache/Elasticsearch inconsistency**: Documents are added to cache (line 138) before the bulk operation. If the bulk request fails or has partial errors, cached documents won't match Elasticsearch.\n\n3. **Bulk response doesn't check for errors**: Elasticsearch bulk API returns 200 even with partial failures. The response contains an `errors` boolean and per-item status that must be checked.\n\n\n\nApply this diff:\n\n```diff\n protected override void AddBatchCore(IList<VectorDocument<T>> vectorDocuments)\n {\n     if (vectorDocuments.Count == 0) return;\n\n+    // Allow _vectorDimension to be set from first document if cleared\n+    if (_vectorDimension == 0)\n+        _vectorDimension = vectorDocuments[0].Embedding.Length;\n+\n     foreach (var vd in vectorDocuments)\n     {\n         if (vd.Embedding.Length != _vectorDimension)\n             throw new ArgumentException($\"Document embedding dimension ({vd.Embedding.Length}) does not match the store's configured dimension ({_vectorDimension}).\");\n-        _cache[vd.Document.Id] = vd;\n     }\n\n     var bulkBody = new StringBuilder();\n     foreach (var vd in vectorDocuments)\n     {\n         var indexAction = new { index = new { _index = _indexName, _id = vd.Document.Id } };\n         bulkBody.AppendLine(Newtonsoft.Json.JsonConvert.SerializeObject(indexAction));\n\n         var embedding = vd.Embedding.ToArray().Select(v => Convert.ToDouble(v)).ToArray();\n         var doc = new\n         {\n             id = vd.Document.Id,\n             content = vd.Document.Content,\n             embedding,\n             metadata = vd.Document.Metadata\n         };\n         bulkBody.AppendLine(Newtonsoft.Json.JsonConvert.SerializeObject(doc));\n     }\n\n     var content = new StringContent(bulkBody.ToString(), Encoding.UTF8, \"application/x-ndjson\");\n     var response = _httpClient.PostAsync($\"/{_indexName}/_bulk\", content).Result;\n     if (response.IsSuccessStatusCode)\n-        _documentCount += vectorDocuments.Count;\n+    {\n+        var responseContent = response.Content.ReadAsStringAsync().Result;\n+        var result = JObject.Parse(responseContent);\n+        \n+        if (result[\"errors\"]?.Value<bool>() == true)\n+        {\n+            // Partial failure - check which items succeeded\n+            var items = result[\"items\"];\n+            int successCount = 0;\n+            for (int i = 0; i < vectorDocuments.Count && i < items.Count(); i++)\n+            {\n+                var item = items[i][\"index\"];\n+                var status = item[\"status\"]?.Value<int>() ?? 500;\n+                if (status >= 200 && status < 300)\n+                {\n+                    _cache[vectorDocuments[i].Document.Id] = vectorDocuments[i];\n+                    successCount++;\n+                }\n+            }\n+            _documentCount += successCount;\n+            throw new InvalidOperationException(\n+                $\"Bulk operation had errors: {successCount}/{vectorDocuments.Count} documents added successfully\");\n+        }\n+        else\n+        {\n+            // All succeeded\n+            foreach (var vd in vectorDocuments)\n+                _cache[vd.Document.Id] = vd;\n+            _documentCount += vectorDocuments.Count;\n+        }\n+    }\n+    else\n+    {\n+        var errorContent = response.Content.ReadAsStringAsync().Result;\n+        throw new InvalidOperationException(\n+            $\"Bulk operation failed: {response.StatusCode}, {errorContent}\");\n+    }\n }\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T15:51:45Z",
    "updated_at": "2025-11-05T15:51:47Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2495150175",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150175"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2495150175"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150175/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 130,
    "original_start_line": 130,
    "start_side": "RIGHT",
    "line": 162,
    "original_line": 162,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 162,
    "position": 162,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150178",
    "pull_request_review_id": 3422976623,
    "id": 2495150178,
    "node_id": "PRRC_kwDOKSXUF86UuPhi",
    "diff_hunk": "@@ -0,0 +1,376 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using Newtonsoft.Json.Linq;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Net.Http;\n+using System.Text;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// Elasticsearch-based document store providing hybrid search capabilities (BM25 + dense vectors).\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Elasticsearch combines traditional full-text search (BM25) with vector similarity search,\n+/// making it ideal for hybrid retrieval scenarios where both keyword matching and semantic\n+/// similarity are important.\n+/// </remarks>\n+public class ElasticsearchDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly HttpClient _httpClient;\n+    private readonly string _indexName;\n+    private int _vectorDimension;\n+    private int _documentCount;\n+    private readonly Dictionary<string, VectorDocument<T>> _cache;\n+\n+    public override int DocumentCount => _documentCount;\n+    public override int VectorDimension => _vectorDimension;\n+\n+    public ElasticsearchDocumentStore(string endpoint, string indexName, string apiKey, string username, string password, int vectorDimension = 1536)\n+    {\n+        if (string.IsNullOrWhiteSpace(endpoint))\n+            throw new ArgumentException(\"Endpoint cannot be empty\", nameof(endpoint));\n+        if (string.IsNullOrWhiteSpace(indexName))\n+            throw new ArgumentException(\"Index name cannot be empty\", nameof(indexName));\n+        if (vectorDimension <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(vectorDimension), \"Vector dimension must be positive\");\n+\n+        _httpClient = new HttpClient { BaseAddress = new Uri(endpoint) };\n+        \n+        if (!string.IsNullOrWhiteSpace(apiKey))\n+            _httpClient.DefaultRequestHeaders.Add(\"Authorization\", $\"ApiKey {apiKey}\");\n+        else if (!string.IsNullOrWhiteSpace(username) && !string.IsNullOrWhiteSpace(password))\n+        {\n+            var auth = Convert.ToBase64String(Encoding.UTF8.GetBytes($\"{username}:{password}\"));\n+            _httpClient.DefaultRequestHeaders.Add(\"Authorization\", $\"Basic {auth}\");\n+        }\n+\n+        _indexName = indexName.ToLowerInvariant();\n+        _vectorDimension = vectorDimension;\n+        _documentCount = 0;\n+        _cache = new Dictionary<string, VectorDocument<T>>();\n+\n+        EnsureIndex();\n+    }\n+\n+    private void EnsureIndex()\n+    {\n+        var checkResponse = _httpClient.GetAsync($\"/{_indexName}\").Result;\n+        if (checkResponse.IsSuccessStatusCode) \n+        {\n+            UpdateDocumentCount();\n+            return;\n+        }\n+\n+        var mapping = new\n+        {\n+            mappings = new\n+            {\n+                properties = new\n+                {\n+                    id = new { type = \"keyword\" },\n+                    content = new { type = \"text\" },\n+                    embedding = new { type = \"dense_vector\", dims = _vectorDimension },\n+                    metadata = new { type = \"object\", enabled = true }\n+                }\n+            }\n+        };\n+\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(mapping),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        _httpClient.PutAsync($\"/{_indexName}\", content).Wait();\n+    }\n+\n+    private void UpdateDocumentCount()\n+    {\n+        var response = _httpClient.GetAsync($\"/{_indexName}/_count\").Result;\n+        if (response.IsSuccessStatusCode)\n+        {\n+            var responseContent = response.Content.ReadAsStringAsync().Result;\n+            var result = JObject.Parse(responseContent);\n+            _documentCount = result[\"count\"]?.Value<int>() ?? 0;\n+        }\n+    }\n+\n+    protected override void AddCore(VectorDocument<T> vectorDocument)\n+    {\n+        if (vectorDocument.Embedding.Length != _vectorDimension)\n+            throw new ArgumentException($\"Document embedding dimension ({vectorDocument.Embedding.Length}) does not match the store's configured dimension ({_vectorDimension}).\");\n+\n+        _cache[vectorDocument.Document.Id] = vectorDocument;\n+\n+        var embedding = vectorDocument.Embedding.ToArray().Select(v => Convert.ToDouble(v)).ToArray();\n+        \n+        var doc = new\n+        {\n+            id = vectorDocument.Document.Id,\n+            content = vectorDocument.Document.Content,\n+            embedding,\n+            metadata = vectorDocument.Document.Metadata\n+        };\n+\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(doc),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        var response = _httpClient.PutAsync($\"/{_indexName}/_doc/{vectorDocument.Document.Id}\", content).Result;\n+        if (response.IsSuccessStatusCode)\n+            _documentCount++;\n+    }\n+\n+    protected override void AddBatchCore(IList<VectorDocument<T>> vectorDocuments)\n+    {\n+        if (vectorDocuments.Count == 0) return;\n+\n+        foreach (var vd in vectorDocuments)\n+        {\n+            if (vd.Embedding.Length != _vectorDimension)\n+                throw new ArgumentException($\"Document embedding dimension ({vd.Embedding.Length}) does not match the store's configured dimension ({_vectorDimension}).\");\n+            _cache[vd.Document.Id] = vd;\n+        }\n+\n+        var bulkBody = new StringBuilder();\n+        foreach (var vd in vectorDocuments)\n+        {\n+            var indexAction = new { index = new { _index = _indexName, _id = vd.Document.Id } };\n+            bulkBody.AppendLine(Newtonsoft.Json.JsonConvert.SerializeObject(indexAction));\n+\n+            var embedding = vd.Embedding.ToArray().Select(v => Convert.ToDouble(v)).ToArray();\n+            var doc = new\n+            {\n+                id = vd.Document.Id,\n+                content = vd.Document.Content,\n+                embedding,\n+                metadata = vd.Document.Metadata\n+            };\n+            bulkBody.AppendLine(Newtonsoft.Json.JsonConvert.SerializeObject(doc));\n+        }\n+\n+        var content = new StringContent(bulkBody.ToString(), Encoding.UTF8, \"application/x-ndjson\");\n+        var response = _httpClient.PostAsync($\"/{_indexName}/_bulk\", content).Result;\n+        if (response.IsSuccessStatusCode)\n+            _documentCount += vectorDocuments.Count;\n+    }\n+\n+    protected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)\n+    {\n+        var embedding = queryVector.ToArray().Select(v => Convert.ToDouble(v)).ToArray();\n+        \n+        // Build the query with metadata filters\n+        object queryClause;\n+        if (metadataFilters != null && metadataFilters.Any())\n+        {\n+            var mustClauses = new List<object>();\n+            foreach (var filter in metadataFilters)\n+            {\n+                mustClauses.Add(new\n+                {\n+                    term = new Dictionary<string, object>\n+                    {\n+                        [$\"metadata.{filter.Key}\"] = filter.Value\n+                    }\n+                });\n+            }\n+            queryClause = new\n+            {\n+                @bool = new\n+                {\n+                    must = mustClauses\n+                }\n+            };\n+        }\n+        else\n+        {\n+            queryClause = new { match_all = new { } };\n+        }\n+\n+        var query = new\n+        {\n+            size = topK,\n+            query = new\n+            {\n+                script_score = new\n+                {\n+                    query = queryClause,\n+                    script = new\n+                    {\n+                        source = \"cosineSimilarity(params.query_vector, 'embedding') + 1.0\",\n+                        @params = new { query_vector = embedding }\n+                    }\n+                }\n+            }\n+        };\n+\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(query),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        var response = _httpClient.PostAsync($\"/{_indexName}/_search\", content).Result;\n+        var responseContent = response.Content.ReadAsStringAsync().Result;\n+        var result = JObject.Parse(responseContent);\n+\n+        var results = new List<Document<T>>();\n+        var hits = result[\"hits\"]?[\"hits\"];\n+        if (hits == null) return results;\n+\n+        foreach (var hit in hits)\n+        {\n+            var source = hit[\"_source\"];\n+            if (source == null) continue;\n+\n+            var id = source[\"id\"]?.ToString() ?? string.Empty;\n+            var docContent = source[\"content\"]?.ToString() ?? string.Empty;\n+            var metadataObj = source[\"metadata\"]?.ToObject<Dictionary<string, object>>() ?? new Dictionary<string, object>();\n+\n+            var score = Convert.ToDouble(hit[\"_score\"]);\n+\n+            var document = new Document<T>(id, docContent, metadataObj)\n+            {\n+                RelevanceScore = NumOps.FromDouble(score),\n+                HasRelevanceScore = true\n+            };\n+\n+            results.Add(document);\n+        }\n+\n+        return results;\n+    }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/ElasticsearchDocumentStore.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "79d215b091efb01a9397bf46a29a74fdee4d11bc",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Document metadata filter limitations and add error handling.**\n\nThe metadata filter implementation uses `term` queries which have limitations:\n\n1. **Exact matches only**: Term queries won't work for text fields with analyzers - they require exact values. For text search, use `match` queries instead.\n2. **Numeric comparisons**: The current implementation only supports equality. Range queries (gt, gte, lt, lte) are not supported.\n3. **Error handling**: No handling for HTTP failures or malformed responses.\n\n\n\n\nConsider documenting these limitations in XML comments and adding error handling:\n\n```diff\n protected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)\n {\n+    try\n+    {\n         var embedding = queryVector.ToArray().Select(v => Convert.ToDouble(v)).ToArray();\n         \n         // Build the query with metadata filters\n+        // Note: Filters use term queries for exact matches on keyword/numeric fields\n         object queryClause;\n         // ... rest of query building ...\n\n         var response = _httpClient.PostAsync($\"/{_indexName}/_search\", content).Result;\n+        \n+        if (!response.IsSuccessStatusCode)\n+        {\n+            var errorContent = response.Content.ReadAsStringAsync().Result;\n+            throw new InvalidOperationException(\n+                $\"Search failed: {response.StatusCode}, {errorContent}\");\n+        }\n+        \n         var responseContent = response.Content.ReadAsStringAsync().Result;\n         var result = JObject.Parse(responseContent);\n\n         // ... parse results ...\n         \n         return results;\n+    }\n+    catch (Exception ex) when (!(ex is InvalidOperationException || ex is ArgumentException))\n+    {\n+        throw new InvalidOperationException(\n+            $\"Error searching Elasticsearch: {ex.Message}\", ex);\n+    }\n }\n```\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T15:51:45Z",
    "updated_at": "2025-11-05T15:51:47Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2495150178",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150178"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2495150178"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150178/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 164,
    "original_start_line": 164,
    "start_side": "RIGHT",
    "line": 247,
    "original_line": 247,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 247,
    "position": 247,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150183",
    "pull_request_review_id": 3422976623,
    "id": 2495150183,
    "node_id": "PRRC_kwDOKSXUF86UuPhn",
    "diff_hunk": "@@ -0,0 +1,376 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using Newtonsoft.Json.Linq;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Net.Http;\n+using System.Text;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// Elasticsearch-based document store providing hybrid search capabilities (BM25 + dense vectors).\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Elasticsearch combines traditional full-text search (BM25) with vector similarity search,\n+/// making it ideal for hybrid retrieval scenarios where both keyword matching and semantic\n+/// similarity are important.\n+/// </remarks>\n+public class ElasticsearchDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly HttpClient _httpClient;\n+    private readonly string _indexName;\n+    private int _vectorDimension;\n+    private int _documentCount;\n+    private readonly Dictionary<string, VectorDocument<T>> _cache;\n+\n+    public override int DocumentCount => _documentCount;\n+    public override int VectorDimension => _vectorDimension;\n+\n+    public ElasticsearchDocumentStore(string endpoint, string indexName, string apiKey, string username, string password, int vectorDimension = 1536)\n+    {\n+        if (string.IsNullOrWhiteSpace(endpoint))\n+            throw new ArgumentException(\"Endpoint cannot be empty\", nameof(endpoint));\n+        if (string.IsNullOrWhiteSpace(indexName))\n+            throw new ArgumentException(\"Index name cannot be empty\", nameof(indexName));\n+        if (vectorDimension <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(vectorDimension), \"Vector dimension must be positive\");\n+\n+        _httpClient = new HttpClient { BaseAddress = new Uri(endpoint) };\n+        \n+        if (!string.IsNullOrWhiteSpace(apiKey))\n+            _httpClient.DefaultRequestHeaders.Add(\"Authorization\", $\"ApiKey {apiKey}\");\n+        else if (!string.IsNullOrWhiteSpace(username) && !string.IsNullOrWhiteSpace(password))\n+        {\n+            var auth = Convert.ToBase64String(Encoding.UTF8.GetBytes($\"{username}:{password}\"));\n+            _httpClient.DefaultRequestHeaders.Add(\"Authorization\", $\"Basic {auth}\");\n+        }\n+\n+        _indexName = indexName.ToLowerInvariant();\n+        _vectorDimension = vectorDimension;\n+        _documentCount = 0;\n+        _cache = new Dictionary<string, VectorDocument<T>>();\n+\n+        EnsureIndex();\n+    }\n+\n+    private void EnsureIndex()\n+    {\n+        var checkResponse = _httpClient.GetAsync($\"/{_indexName}\").Result;\n+        if (checkResponse.IsSuccessStatusCode) \n+        {\n+            UpdateDocumentCount();\n+            return;\n+        }\n+\n+        var mapping = new\n+        {\n+            mappings = new\n+            {\n+                properties = new\n+                {\n+                    id = new { type = \"keyword\" },\n+                    content = new { type = \"text\" },\n+                    embedding = new { type = \"dense_vector\", dims = _vectorDimension },\n+                    metadata = new { type = \"object\", enabled = true }\n+                }\n+            }\n+        };\n+\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(mapping),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        _httpClient.PutAsync($\"/{_indexName}\", content).Wait();\n+    }\n+\n+    private void UpdateDocumentCount()\n+    {\n+        var response = _httpClient.GetAsync($\"/{_indexName}/_count\").Result;\n+        if (response.IsSuccessStatusCode)\n+        {\n+            var responseContent = response.Content.ReadAsStringAsync().Result;\n+            var result = JObject.Parse(responseContent);\n+            _documentCount = result[\"count\"]?.Value<int>() ?? 0;\n+        }\n+    }\n+\n+    protected override void AddCore(VectorDocument<T> vectorDocument)\n+    {\n+        if (vectorDocument.Embedding.Length != _vectorDimension)\n+            throw new ArgumentException($\"Document embedding dimension ({vectorDocument.Embedding.Length}) does not match the store's configured dimension ({_vectorDimension}).\");\n+\n+        _cache[vectorDocument.Document.Id] = vectorDocument;\n+\n+        var embedding = vectorDocument.Embedding.ToArray().Select(v => Convert.ToDouble(v)).ToArray();\n+        \n+        var doc = new\n+        {\n+            id = vectorDocument.Document.Id,\n+            content = vectorDocument.Document.Content,\n+            embedding,\n+            metadata = vectorDocument.Document.Metadata\n+        };\n+\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(doc),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        var response = _httpClient.PutAsync($\"/{_indexName}/_doc/{vectorDocument.Document.Id}\", content).Result;\n+        if (response.IsSuccessStatusCode)\n+            _documentCount++;\n+    }\n+\n+    protected override void AddBatchCore(IList<VectorDocument<T>> vectorDocuments)\n+    {\n+        if (vectorDocuments.Count == 0) return;\n+\n+        foreach (var vd in vectorDocuments)\n+        {\n+            if (vd.Embedding.Length != _vectorDimension)\n+                throw new ArgumentException($\"Document embedding dimension ({vd.Embedding.Length}) does not match the store's configured dimension ({_vectorDimension}).\");\n+            _cache[vd.Document.Id] = vd;\n+        }\n+\n+        var bulkBody = new StringBuilder();\n+        foreach (var vd in vectorDocuments)\n+        {\n+            var indexAction = new { index = new { _index = _indexName, _id = vd.Document.Id } };\n+            bulkBody.AppendLine(Newtonsoft.Json.JsonConvert.SerializeObject(indexAction));\n+\n+            var embedding = vd.Embedding.ToArray().Select(v => Convert.ToDouble(v)).ToArray();\n+            var doc = new\n+            {\n+                id = vd.Document.Id,\n+                content = vd.Document.Content,\n+                embedding,\n+                metadata = vd.Document.Metadata\n+            };\n+            bulkBody.AppendLine(Newtonsoft.Json.JsonConvert.SerializeObject(doc));\n+        }\n+\n+        var content = new StringContent(bulkBody.ToString(), Encoding.UTF8, \"application/x-ndjson\");\n+        var response = _httpClient.PostAsync($\"/{_indexName}/_bulk\", content).Result;\n+        if (response.IsSuccessStatusCode)\n+            _documentCount += vectorDocuments.Count;\n+    }\n+\n+    protected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)\n+    {\n+        var embedding = queryVector.ToArray().Select(v => Convert.ToDouble(v)).ToArray();\n+        \n+        // Build the query with metadata filters\n+        object queryClause;\n+        if (metadataFilters != null && metadataFilters.Any())\n+        {\n+            var mustClauses = new List<object>();\n+            foreach (var filter in metadataFilters)\n+            {\n+                mustClauses.Add(new\n+                {\n+                    term = new Dictionary<string, object>\n+                    {\n+                        [$\"metadata.{filter.Key}\"] = filter.Value\n+                    }\n+                });\n+            }\n+            queryClause = new\n+            {\n+                @bool = new\n+                {\n+                    must = mustClauses\n+                }\n+            };\n+        }\n+        else\n+        {\n+            queryClause = new { match_all = new { } };\n+        }\n+\n+        var query = new\n+        {\n+            size = topK,\n+            query = new\n+            {\n+                script_score = new\n+                {\n+                    query = queryClause,\n+                    script = new\n+                    {\n+                        source = \"cosineSimilarity(params.query_vector, 'embedding') + 1.0\",\n+                        @params = new { query_vector = embedding }\n+                    }\n+                }\n+            }\n+        };\n+\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(query),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        var response = _httpClient.PostAsync($\"/{_indexName}/_search\", content).Result;\n+        var responseContent = response.Content.ReadAsStringAsync().Result;\n+        var result = JObject.Parse(responseContent);\n+\n+        var results = new List<Document<T>>();\n+        var hits = result[\"hits\"]?[\"hits\"];\n+        if (hits == null) return results;\n+\n+        foreach (var hit in hits)\n+        {\n+            var source = hit[\"_source\"];\n+            if (source == null) continue;\n+\n+            var id = source[\"id\"]?.ToString() ?? string.Empty;\n+            var docContent = source[\"content\"]?.ToString() ?? string.Empty;\n+            var metadataObj = source[\"metadata\"]?.ToObject<Dictionary<string, object>>() ?? new Dictionary<string, object>();\n+\n+            var score = Convert.ToDouble(hit[\"_score\"]);\n+\n+            var document = new Document<T>(id, docContent, metadataObj)\n+            {\n+                RelevanceScore = NumOps.FromDouble(score),\n+                HasRelevanceScore = true\n+            };\n+\n+            results.Add(document);\n+        }\n+\n+        return results;\n+    }\n+\n+    protected override Document<T>? GetByIdCore(string documentId)\n+    {\n+        if (_cache.TryGetValue(documentId, out var vectorDoc))\n+            return vectorDoc.Document;\n+\n+        return null;\n+    }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/ElasticsearchDocumentStore.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "79d215b091efb01a9397bf46a29a74fdee4d11bc",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Query Elasticsearch instead of relying only on cache.**\n\nThe current implementation only checks the in-memory cache, which means:\n- Documents indexed directly in Elasticsearch won't be found\n- After application restart, no documents can be retrieved by ID until they're re-added\n- The cache can become stale if documents are modified externally\n\n\n\nConsider querying Elasticsearch directly:\n\n```diff\n protected override Document<T>? GetByIdCore(string documentId)\n {\n+    try\n+    {\n-        if (_cache.TryGetValue(documentId, out var vectorDoc))\n-            return vectorDoc.Document;\n+        var response = _httpClient.GetAsync($\"/{_indexName}/_doc/{documentId}\").Result;\n+        \n+        if (response.StatusCode == System.Net.HttpStatusCode.NotFound)\n+            return null;\n+            \n+        if (!response.IsSuccessStatusCode)\n+        {\n+            var errorContent = response.Content.ReadAsStringAsync().Result;\n+            throw new InvalidOperationException(\n+                $\"Failed to get document '{documentId}': {response.StatusCode}, {errorContent}\");\n+        }\n+        \n+        var responseContent = response.Content.ReadAsStringAsync().Result;\n+        var result = JObject.Parse(responseContent);\n+        var source = result[\"_source\"];\n+        \n+        if (source == null)\n+            return null;\n+        \n+        var id = source[\"id\"]?.ToString() ?? documentId;\n+        var content = source[\"content\"]?.ToString() ?? string.Empty;\n+        var metadata = source[\"metadata\"]?.ToObject<Dictionary<string, object>>() \n+            ?? new Dictionary<string, object>();\n+        \n+        return new Document<T>(id, content, metadata);\n+    }\n+    catch (Exception ex) when (!(ex is InvalidOperationException))\n+    {\n+        throw new InvalidOperationException(\n+            $\"Error retrieving document '{documentId}': {ex.Message}\", ex);\n+    }\n-    return null;\n }\n```\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T15:51:45Z",
    "updated_at": "2025-11-05T15:51:48Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2495150183",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150183"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2495150183"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150183/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 249,
    "original_start_line": 249,
    "start_side": "RIGHT",
    "line": 255,
    "original_line": 255,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 255,
    "position": 255,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150193",
    "pull_request_review_id": 3422976623,
    "id": 2495150193,
    "node_id": "PRRC_kwDOKSXUF86UuPhx",
    "diff_hunk": "@@ -0,0 +1,376 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using Newtonsoft.Json.Linq;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Net.Http;\n+using System.Text;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// Elasticsearch-based document store providing hybrid search capabilities (BM25 + dense vectors).\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Elasticsearch combines traditional full-text search (BM25) with vector similarity search,\n+/// making it ideal for hybrid retrieval scenarios where both keyword matching and semantic\n+/// similarity are important.\n+/// </remarks>\n+public class ElasticsearchDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly HttpClient _httpClient;\n+    private readonly string _indexName;\n+    private int _vectorDimension;\n+    private int _documentCount;\n+    private readonly Dictionary<string, VectorDocument<T>> _cache;\n+\n+    public override int DocumentCount => _documentCount;\n+    public override int VectorDimension => _vectorDimension;\n+\n+    public ElasticsearchDocumentStore(string endpoint, string indexName, string apiKey, string username, string password, int vectorDimension = 1536)\n+    {\n+        if (string.IsNullOrWhiteSpace(endpoint))\n+            throw new ArgumentException(\"Endpoint cannot be empty\", nameof(endpoint));\n+        if (string.IsNullOrWhiteSpace(indexName))\n+            throw new ArgumentException(\"Index name cannot be empty\", nameof(indexName));\n+        if (vectorDimension <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(vectorDimension), \"Vector dimension must be positive\");\n+\n+        _httpClient = new HttpClient { BaseAddress = new Uri(endpoint) };\n+        \n+        if (!string.IsNullOrWhiteSpace(apiKey))\n+            _httpClient.DefaultRequestHeaders.Add(\"Authorization\", $\"ApiKey {apiKey}\");\n+        else if (!string.IsNullOrWhiteSpace(username) && !string.IsNullOrWhiteSpace(password))\n+        {\n+            var auth = Convert.ToBase64String(Encoding.UTF8.GetBytes($\"{username}:{password}\"));\n+            _httpClient.DefaultRequestHeaders.Add(\"Authorization\", $\"Basic {auth}\");\n+        }\n+\n+        _indexName = indexName.ToLowerInvariant();\n+        _vectorDimension = vectorDimension;\n+        _documentCount = 0;\n+        _cache = new Dictionary<string, VectorDocument<T>>();\n+\n+        EnsureIndex();\n+    }\n+\n+    private void EnsureIndex()\n+    {\n+        var checkResponse = _httpClient.GetAsync($\"/{_indexName}\").Result;\n+        if (checkResponse.IsSuccessStatusCode) \n+        {\n+            UpdateDocumentCount();\n+            return;\n+        }\n+\n+        var mapping = new\n+        {\n+            mappings = new\n+            {\n+                properties = new\n+                {\n+                    id = new { type = \"keyword\" },\n+                    content = new { type = \"text\" },\n+                    embedding = new { type = \"dense_vector\", dims = _vectorDimension },\n+                    metadata = new { type = \"object\", enabled = true }\n+                }\n+            }\n+        };\n+\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(mapping),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        _httpClient.PutAsync($\"/{_indexName}\", content).Wait();\n+    }\n+\n+    private void UpdateDocumentCount()\n+    {\n+        var response = _httpClient.GetAsync($\"/{_indexName}/_count\").Result;\n+        if (response.IsSuccessStatusCode)\n+        {\n+            var responseContent = response.Content.ReadAsStringAsync().Result;\n+            var result = JObject.Parse(responseContent);\n+            _documentCount = result[\"count\"]?.Value<int>() ?? 0;\n+        }\n+    }\n+\n+    protected override void AddCore(VectorDocument<T> vectorDocument)\n+    {\n+        if (vectorDocument.Embedding.Length != _vectorDimension)\n+            throw new ArgumentException($\"Document embedding dimension ({vectorDocument.Embedding.Length}) does not match the store's configured dimension ({_vectorDimension}).\");\n+\n+        _cache[vectorDocument.Document.Id] = vectorDocument;\n+\n+        var embedding = vectorDocument.Embedding.ToArray().Select(v => Convert.ToDouble(v)).ToArray();\n+        \n+        var doc = new\n+        {\n+            id = vectorDocument.Document.Id,\n+            content = vectorDocument.Document.Content,\n+            embedding,\n+            metadata = vectorDocument.Document.Metadata\n+        };\n+\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(doc),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        var response = _httpClient.PutAsync($\"/{_indexName}/_doc/{vectorDocument.Document.Id}\", content).Result;\n+        if (response.IsSuccessStatusCode)\n+            _documentCount++;\n+    }\n+\n+    protected override void AddBatchCore(IList<VectorDocument<T>> vectorDocuments)\n+    {\n+        if (vectorDocuments.Count == 0) return;\n+\n+        foreach (var vd in vectorDocuments)\n+        {\n+            if (vd.Embedding.Length != _vectorDimension)\n+                throw new ArgumentException($\"Document embedding dimension ({vd.Embedding.Length}) does not match the store's configured dimension ({_vectorDimension}).\");\n+            _cache[vd.Document.Id] = vd;\n+        }\n+\n+        var bulkBody = new StringBuilder();\n+        foreach (var vd in vectorDocuments)\n+        {\n+            var indexAction = new { index = new { _index = _indexName, _id = vd.Document.Id } };\n+            bulkBody.AppendLine(Newtonsoft.Json.JsonConvert.SerializeObject(indexAction));\n+\n+            var embedding = vd.Embedding.ToArray().Select(v => Convert.ToDouble(v)).ToArray();\n+            var doc = new\n+            {\n+                id = vd.Document.Id,\n+                content = vd.Document.Content,\n+                embedding,\n+                metadata = vd.Document.Metadata\n+            };\n+            bulkBody.AppendLine(Newtonsoft.Json.JsonConvert.SerializeObject(doc));\n+        }\n+\n+        var content = new StringContent(bulkBody.ToString(), Encoding.UTF8, \"application/x-ndjson\");\n+        var response = _httpClient.PostAsync($\"/{_indexName}/_bulk\", content).Result;\n+        if (response.IsSuccessStatusCode)\n+            _documentCount += vectorDocuments.Count;\n+    }\n+\n+    protected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)\n+    {\n+        var embedding = queryVector.ToArray().Select(v => Convert.ToDouble(v)).ToArray();\n+        \n+        // Build the query with metadata filters\n+        object queryClause;\n+        if (metadataFilters != null && metadataFilters.Any())\n+        {\n+            var mustClauses = new List<object>();\n+            foreach (var filter in metadataFilters)\n+            {\n+                mustClauses.Add(new\n+                {\n+                    term = new Dictionary<string, object>\n+                    {\n+                        [$\"metadata.{filter.Key}\"] = filter.Value\n+                    }\n+                });\n+            }\n+            queryClause = new\n+            {\n+                @bool = new\n+                {\n+                    must = mustClauses\n+                }\n+            };\n+        }\n+        else\n+        {\n+            queryClause = new { match_all = new { } };\n+        }\n+\n+        var query = new\n+        {\n+            size = topK,\n+            query = new\n+            {\n+                script_score = new\n+                {\n+                    query = queryClause,\n+                    script = new\n+                    {\n+                        source = \"cosineSimilarity(params.query_vector, 'embedding') + 1.0\",\n+                        @params = new { query_vector = embedding }\n+                    }\n+                }\n+            }\n+        };\n+\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(query),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        var response = _httpClient.PostAsync($\"/{_indexName}/_search\", content).Result;\n+        var responseContent = response.Content.ReadAsStringAsync().Result;\n+        var result = JObject.Parse(responseContent);\n+\n+        var results = new List<Document<T>>();\n+        var hits = result[\"hits\"]?[\"hits\"];\n+        if (hits == null) return results;\n+\n+        foreach (var hit in hits)\n+        {\n+            var source = hit[\"_source\"];\n+            if (source == null) continue;\n+\n+            var id = source[\"id\"]?.ToString() ?? string.Empty;\n+            var docContent = source[\"content\"]?.ToString() ?? string.Empty;\n+            var metadataObj = source[\"metadata\"]?.ToObject<Dictionary<string, object>>() ?? new Dictionary<string, object>();\n+\n+            var score = Convert.ToDouble(hit[\"_score\"]);\n+\n+            var document = new Document<T>(id, docContent, metadataObj)\n+            {\n+                RelevanceScore = NumOps.FromDouble(score),\n+                HasRelevanceScore = true\n+            };\n+\n+            results.Add(document);\n+        }\n+\n+        return results;\n+    }\n+\n+    protected override Document<T>? GetByIdCore(string documentId)\n+    {\n+        if (_cache.TryGetValue(documentId, out var vectorDoc))\n+            return vectorDoc.Document;\n+\n+        return null;\n+    }\n+\n+    /// <summary>\n+    /// Core logic for removing a document from the Elasticsearch index.\n+    /// </summary>\n+    /// <param name=\"documentId\">The validated document ID.</param>\n+    /// <returns>True if the document was found and removed; otherwise, false.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Removes the document from both the cache and the Elasticsearch index via DELETE API.\n+    /// If successful, decrements the document count.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Deletes a document from Elasticsearch.\n+    /// \n+    /// In Elasticsearch terms, this is like:\n+    /// <code>\n+    /// DELETE /index/_doc/document_id\n+    /// </code>\n+    /// \n+    /// Example:\n+    /// <code>\n+    /// if (store.Remove(\"doc-123\"))\n+    ///     Console.WriteLine(\"Document removed from Elasticsearch\");\n+    /// </code>\n+    /// </para>\n+    /// </remarks>\n+    protected override bool RemoveCore(string documentId)\n+    {\n+        _cache.Remove(documentId);\n+\n+        var response = _httpClient.DeleteAsync($\"/{_indexName}/_doc/{documentId}\").Result;\n+        if (response.IsSuccessStatusCode && _documentCount > 0)\n+        {\n+            _documentCount--;\n+            return true;\n+        }\n+        return false;\n+    }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/ElasticsearchDocumentStore.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "79d215b091efb01a9397bf46a29a74fdee4d11bc",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Fix cache inconsistency in RemoveCore.**\n\nThe document is removed from cache (line 283) before the DELETE request. If the HTTP call fails, the document is removed from the cache but remains in Elasticsearch, causing `GetByIdCore` to not find documents that still exist.\n\n\n\nApply this diff to ensure cache consistency:\n\n```diff\n protected override bool RemoveCore(string documentId)\n {\n-    _cache.Remove(documentId);\n-\n     var response = _httpClient.DeleteAsync($\"/{_indexName}/_doc/{documentId}\").Result;\n-    if (response.IsSuccessStatusCode && _documentCount > 0)\n+    \n+    if (response.StatusCode == System.Net.HttpStatusCode.NotFound)\n+    {\n+        _cache.Remove(documentId);\n+        return false;\n+    }\n+    \n+    if (response.IsSuccessStatusCode)\n     {\n+        _cache.Remove(documentId);\n-        _documentCount--;\n+        if (_documentCount > 0)\n+            _documentCount--;\n         return true;\n     }\n+    \n+    // HTTP error - don't remove from cache\n+    var errorContent = response.Content.ReadAsStringAsync().Result;\n+    throw new InvalidOperationException(\n+        $\"Failed to remove document '{documentId}': {response.StatusCode}, {errorContent}\");\n-    return false;\n }\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/ElasticsearchDocumentStore.cs\naround lines 281 to 292, the code removes the document from the in-memory cache\nbefore issuing the HTTP DELETE, which can leave the cache inconsistent if the\nDELETE fails; change the flow so the DELETE is executed first and only on a\nsuccessful response decrement _documentCount and then remove the document from\n_cache and return true; on failure do not modify the cache or _documentCount and\nreturn false (also ensure any exceptions from the HTTP call are allowed to\npropagate or handled consistently with existing error handling).\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T15:51:46Z",
    "updated_at": "2025-11-05T15:51:48Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2495150193",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150193"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2495150193"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150193/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 281,
    "original_start_line": 281,
    "start_side": "RIGHT",
    "line": 292,
    "original_line": 292,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 292,
    "position": 292,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150200",
    "pull_request_review_id": 3422976623,
    "id": 2495150200,
    "node_id": "PRRC_kwDOKSXUF86UuPh4",
    "diff_hunk": "@@ -0,0 +1,376 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using Newtonsoft.Json.Linq;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Net.Http;\n+using System.Text;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// Elasticsearch-based document store providing hybrid search capabilities (BM25 + dense vectors).\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Elasticsearch combines traditional full-text search (BM25) with vector similarity search,\n+/// making it ideal for hybrid retrieval scenarios where both keyword matching and semantic\n+/// similarity are important.\n+/// </remarks>\n+public class ElasticsearchDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly HttpClient _httpClient;\n+    private readonly string _indexName;\n+    private int _vectorDimension;\n+    private int _documentCount;\n+    private readonly Dictionary<string, VectorDocument<T>> _cache;\n+\n+    public override int DocumentCount => _documentCount;\n+    public override int VectorDimension => _vectorDimension;\n+\n+    public ElasticsearchDocumentStore(string endpoint, string indexName, string apiKey, string username, string password, int vectorDimension = 1536)\n+    {\n+        if (string.IsNullOrWhiteSpace(endpoint))\n+            throw new ArgumentException(\"Endpoint cannot be empty\", nameof(endpoint));\n+        if (string.IsNullOrWhiteSpace(indexName))\n+            throw new ArgumentException(\"Index name cannot be empty\", nameof(indexName));\n+        if (vectorDimension <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(vectorDimension), \"Vector dimension must be positive\");\n+\n+        _httpClient = new HttpClient { BaseAddress = new Uri(endpoint) };\n+        \n+        if (!string.IsNullOrWhiteSpace(apiKey))\n+            _httpClient.DefaultRequestHeaders.Add(\"Authorization\", $\"ApiKey {apiKey}\");\n+        else if (!string.IsNullOrWhiteSpace(username) && !string.IsNullOrWhiteSpace(password))\n+        {\n+            var auth = Convert.ToBase64String(Encoding.UTF8.GetBytes($\"{username}:{password}\"));\n+            _httpClient.DefaultRequestHeaders.Add(\"Authorization\", $\"Basic {auth}\");\n+        }\n+\n+        _indexName = indexName.ToLowerInvariant();\n+        _vectorDimension = vectorDimension;\n+        _documentCount = 0;\n+        _cache = new Dictionary<string, VectorDocument<T>>();\n+\n+        EnsureIndex();\n+    }\n+\n+    private void EnsureIndex()\n+    {\n+        var checkResponse = _httpClient.GetAsync($\"/{_indexName}\").Result;\n+        if (checkResponse.IsSuccessStatusCode) \n+        {\n+            UpdateDocumentCount();\n+            return;\n+        }\n+\n+        var mapping = new\n+        {\n+            mappings = new\n+            {\n+                properties = new\n+                {\n+                    id = new { type = \"keyword\" },\n+                    content = new { type = \"text\" },\n+                    embedding = new { type = \"dense_vector\", dims = _vectorDimension },\n+                    metadata = new { type = \"object\", enabled = true }\n+                }\n+            }\n+        };\n+\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(mapping),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        _httpClient.PutAsync($\"/{_indexName}\", content).Wait();\n+    }\n+\n+    private void UpdateDocumentCount()\n+    {\n+        var response = _httpClient.GetAsync($\"/{_indexName}/_count\").Result;\n+        if (response.IsSuccessStatusCode)\n+        {\n+            var responseContent = response.Content.ReadAsStringAsync().Result;\n+            var result = JObject.Parse(responseContent);\n+            _documentCount = result[\"count\"]?.Value<int>() ?? 0;\n+        }\n+    }\n+\n+    protected override void AddCore(VectorDocument<T> vectorDocument)\n+    {\n+        if (vectorDocument.Embedding.Length != _vectorDimension)\n+            throw new ArgumentException($\"Document embedding dimension ({vectorDocument.Embedding.Length}) does not match the store's configured dimension ({_vectorDimension}).\");\n+\n+        _cache[vectorDocument.Document.Id] = vectorDocument;\n+\n+        var embedding = vectorDocument.Embedding.ToArray().Select(v => Convert.ToDouble(v)).ToArray();\n+        \n+        var doc = new\n+        {\n+            id = vectorDocument.Document.Id,\n+            content = vectorDocument.Document.Content,\n+            embedding,\n+            metadata = vectorDocument.Document.Metadata\n+        };\n+\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(doc),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        var response = _httpClient.PutAsync($\"/{_indexName}/_doc/{vectorDocument.Document.Id}\", content).Result;\n+        if (response.IsSuccessStatusCode)\n+            _documentCount++;\n+    }\n+\n+    protected override void AddBatchCore(IList<VectorDocument<T>> vectorDocuments)\n+    {\n+        if (vectorDocuments.Count == 0) return;\n+\n+        foreach (var vd in vectorDocuments)\n+        {\n+            if (vd.Embedding.Length != _vectorDimension)\n+                throw new ArgumentException($\"Document embedding dimension ({vd.Embedding.Length}) does not match the store's configured dimension ({_vectorDimension}).\");\n+            _cache[vd.Document.Id] = vd;\n+        }\n+\n+        var bulkBody = new StringBuilder();\n+        foreach (var vd in vectorDocuments)\n+        {\n+            var indexAction = new { index = new { _index = _indexName, _id = vd.Document.Id } };\n+            bulkBody.AppendLine(Newtonsoft.Json.JsonConvert.SerializeObject(indexAction));\n+\n+            var embedding = vd.Embedding.ToArray().Select(v => Convert.ToDouble(v)).ToArray();\n+            var doc = new\n+            {\n+                id = vd.Document.Id,\n+                content = vd.Document.Content,\n+                embedding,\n+                metadata = vd.Document.Metadata\n+            };\n+            bulkBody.AppendLine(Newtonsoft.Json.JsonConvert.SerializeObject(doc));\n+        }\n+\n+        var content = new StringContent(bulkBody.ToString(), Encoding.UTF8, \"application/x-ndjson\");\n+        var response = _httpClient.PostAsync($\"/{_indexName}/_bulk\", content).Result;\n+        if (response.IsSuccessStatusCode)\n+            _documentCount += vectorDocuments.Count;\n+    }\n+\n+    protected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)\n+    {\n+        var embedding = queryVector.ToArray().Select(v => Convert.ToDouble(v)).ToArray();\n+        \n+        // Build the query with metadata filters\n+        object queryClause;\n+        if (metadataFilters != null && metadataFilters.Any())\n+        {\n+            var mustClauses = new List<object>();\n+            foreach (var filter in metadataFilters)\n+            {\n+                mustClauses.Add(new\n+                {\n+                    term = new Dictionary<string, object>\n+                    {\n+                        [$\"metadata.{filter.Key}\"] = filter.Value\n+                    }\n+                });\n+            }\n+            queryClause = new\n+            {\n+                @bool = new\n+                {\n+                    must = mustClauses\n+                }\n+            };\n+        }\n+        else\n+        {\n+            queryClause = new { match_all = new { } };\n+        }\n+\n+        var query = new\n+        {\n+            size = topK,\n+            query = new\n+            {\n+                script_score = new\n+                {\n+                    query = queryClause,\n+                    script = new\n+                    {\n+                        source = \"cosineSimilarity(params.query_vector, 'embedding') + 1.0\",\n+                        @params = new { query_vector = embedding }\n+                    }\n+                }\n+            }\n+        };\n+\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(query),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        var response = _httpClient.PostAsync($\"/{_indexName}/_search\", content).Result;\n+        var responseContent = response.Content.ReadAsStringAsync().Result;\n+        var result = JObject.Parse(responseContent);\n+\n+        var results = new List<Document<T>>();\n+        var hits = result[\"hits\"]?[\"hits\"];\n+        if (hits == null) return results;\n+\n+        foreach (var hit in hits)\n+        {\n+            var source = hit[\"_source\"];\n+            if (source == null) continue;\n+\n+            var id = source[\"id\"]?.ToString() ?? string.Empty;\n+            var docContent = source[\"content\"]?.ToString() ?? string.Empty;\n+            var metadataObj = source[\"metadata\"]?.ToObject<Dictionary<string, object>>() ?? new Dictionary<string, object>();\n+\n+            var score = Convert.ToDouble(hit[\"_score\"]);\n+\n+            var document = new Document<T>(id, docContent, metadataObj)\n+            {\n+                RelevanceScore = NumOps.FromDouble(score),\n+                HasRelevanceScore = true\n+            };\n+\n+            results.Add(document);\n+        }\n+\n+        return results;\n+    }\n+\n+    protected override Document<T>? GetByIdCore(string documentId)\n+    {\n+        if (_cache.TryGetValue(documentId, out var vectorDoc))\n+            return vectorDoc.Document;\n+\n+        return null;\n+    }\n+\n+    /// <summary>\n+    /// Core logic for removing a document from the Elasticsearch index.\n+    /// </summary>\n+    /// <param name=\"documentId\">The validated document ID.</param>\n+    /// <returns>True if the document was found and removed; otherwise, false.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Removes the document from both the cache and the Elasticsearch index via DELETE API.\n+    /// If successful, decrements the document count.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Deletes a document from Elasticsearch.\n+    /// \n+    /// In Elasticsearch terms, this is like:\n+    /// <code>\n+    /// DELETE /index/_doc/document_id\n+    /// </code>\n+    /// \n+    /// Example:\n+    /// <code>\n+    /// if (store.Remove(\"doc-123\"))\n+    ///     Console.WriteLine(\"Document removed from Elasticsearch\");\n+    /// </code>\n+    /// </para>\n+    /// </remarks>\n+    protected override bool RemoveCore(string documentId)\n+    {\n+        _cache.Remove(documentId);\n+\n+        var response = _httpClient.DeleteAsync($\"/{_indexName}/_doc/{documentId}\").Result;\n+        if (response.IsSuccessStatusCode && _documentCount > 0)\n+        {\n+            _documentCount--;\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+    /// <summary>\n+    /// Core logic for retrieving all documents from the Elasticsearch index.\n+    /// </summary>\n+    /// <returns>An enumerable of all documents without their vector embeddings.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Returns all documents from the cache (in-memory representation) of the Elasticsearch index.\n+    /// In a real Elasticsearch deployment with large indices, use the scroll API for efficient\n+    /// retrieval of all documents without loading everything into memory at once.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Gets every document from the Elasticsearch index.\n+    /// \n+    /// Use cases:\n+    /// - Export index contents for backup\n+    /// - Migrate to a different index or cluster\n+    /// - Bulk reindexing operations\n+    /// - Debugging to see all indexed documents\n+    /// \n+    /// Warning: For large indices (> 10K documents), this can use significant memory.\n+    /// In production Elasticsearch, use the scroll API with pagination:\n+    /// - POST /index/_search?scroll=1m with size parameter\n+    /// - Iterate through scroll_id responses\n+    /// - Clear scroll when done\n+    /// \n+    /// Example:\n+    /// <code>\n+    /// // Get all documents\n+    /// var allDocs = store.GetAll().ToList();\n+    /// Console.WriteLine($\"Total documents in {_indexName}: {allDocs.Count}\");\n+    /// \n+    /// // Export to JSON\n+    /// var json = JsonConvert.SerializeObject(allDocs);\n+    /// File.WriteAllText($\"elasticsearch_{_indexName}_export.json\", json);\n+    /// </code>\n+    /// </para>\n+    /// </remarks>\n+    protected override IEnumerable<Document<T>> GetAllCore()\n+    {\n+        return _cache.Values.Select(vd => vd.Document).ToList();\n+    }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/ElasticsearchDocumentStore.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "79d215b091efb01a9397bf46a29a74fdee4d11bc",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Implement Elasticsearch scroll API as documented.**\n\nThe XML documentation mentions using the scroll API for efficient retrieval, but the implementation only returns cached documents. This means:\n- After app restart, GetAll() returns an empty collection even if Elasticsearch has documents\n- Documents indexed directly in Elasticsearch won't appear\n- The cache can become stale\n\n\n\nConsider implementing the scroll API as documented:\n\n```csharp\nprotected override IEnumerable<Document<T>> GetAllCore()\n{\n    var results = new List<Document<T>>();\n    \n    try\n    {\n        // Initial search with scroll\n        var query = new { size = 1000, query = new { match_all = new { } } };\n        var content = new StringContent(\n            Newtonsoft.Json.JsonConvert.SerializeObject(query),\n            Encoding.UTF8,\n            \"application/json\");\n        \n        var response = _httpClient.PostAsync($\"/{_indexName}/_search?scroll=1m\", content).Result;\n        if (!response.IsSuccessStatusCode)\n            throw new InvalidOperationException($\"Initial scroll failed: {response.StatusCode}\");\n        \n        var responseContent = response.Content.ReadAsStringAsync().Result;\n        var result = JObject.Parse(responseContent);\n        var scrollId = result[\"_scroll_id\"]?.ToString();\n        \n        while (true)\n        {\n            var hits = result[\"hits\"]?[\"hits\"];\n            if (hits == null || !hits.Any())\n                break;\n            \n            foreach (var hit in hits)\n            {\n                var source = hit[\"_source\"];\n                if (source != null)\n                {\n                    var id = source[\"id\"]?.ToString() ?? string.Empty;\n                    var docContent = source[\"content\"]?.ToString() ?? string.Empty;\n                    var metadata = source[\"metadata\"]?.ToObject<Dictionary<string, object>>() \n                        ?? new Dictionary<string, object>();\n                    results.Add(new Document<T>(id, docContent, metadata));\n                }\n            }\n            \n            // Continue scroll\n            var scrollQuery = new { scroll = \"1m\", scroll_id = scrollId };\n            var scrollContent = new StringContent(\n                Newtonsoft.Json.JsonConvert.SerializeObject(scrollQuery),\n                Encoding.UTF8,\n                \"application/json\");\n            response = _httpClient.PostAsync(\"/_search/scroll\", scrollContent).Result;\n            responseContent = response.Content.ReadAsStringAsync().Result;\n            result = JObject.Parse(responseContent);\n        }\n        \n        // Clear scroll\n        if (scrollId != null)\n            _httpClient.DeleteAsync($\"/_search/scroll/{scrollId}\").Wait();\n    }\n    catch (Exception ex)\n    {\n        throw new InvalidOperationException($\"Error retrieving all documents: {ex.Message}\", ex);\n    }\n    \n    return results;\n}\n```\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T15:51:46Z",
    "updated_at": "2025-11-05T15:51:48Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2495150200",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150200"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2495150200"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150200/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 330,
    "original_start_line": 330,
    "start_side": "RIGHT",
    "line": 333,
    "original_line": 333,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 333,
    "position": 333,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150208",
    "pull_request_review_id": 3422976623,
    "id": 2495150208,
    "node_id": "PRRC_kwDOKSXUF86UuPiA",
    "diff_hunk": "@@ -0,0 +1,376 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using Newtonsoft.Json.Linq;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using System.Net.Http;\n+using System.Text;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// Elasticsearch-based document store providing hybrid search capabilities (BM25 + dense vectors).\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+/// <remarks>\n+/// Elasticsearch combines traditional full-text search (BM25) with vector similarity search,\n+/// making it ideal for hybrid retrieval scenarios where both keyword matching and semantic\n+/// similarity are important.\n+/// </remarks>\n+public class ElasticsearchDocumentStore<T> : DocumentStoreBase<T>\n+{\n+    private readonly HttpClient _httpClient;\n+    private readonly string _indexName;\n+    private int _vectorDimension;\n+    private int _documentCount;\n+    private readonly Dictionary<string, VectorDocument<T>> _cache;\n+\n+    public override int DocumentCount => _documentCount;\n+    public override int VectorDimension => _vectorDimension;\n+\n+    public ElasticsearchDocumentStore(string endpoint, string indexName, string apiKey, string username, string password, int vectorDimension = 1536)\n+    {\n+        if (string.IsNullOrWhiteSpace(endpoint))\n+            throw new ArgumentException(\"Endpoint cannot be empty\", nameof(endpoint));\n+        if (string.IsNullOrWhiteSpace(indexName))\n+            throw new ArgumentException(\"Index name cannot be empty\", nameof(indexName));\n+        if (vectorDimension <= 0)\n+            throw new ArgumentOutOfRangeException(nameof(vectorDimension), \"Vector dimension must be positive\");\n+\n+        _httpClient = new HttpClient { BaseAddress = new Uri(endpoint) };\n+        \n+        if (!string.IsNullOrWhiteSpace(apiKey))\n+            _httpClient.DefaultRequestHeaders.Add(\"Authorization\", $\"ApiKey {apiKey}\");\n+        else if (!string.IsNullOrWhiteSpace(username) && !string.IsNullOrWhiteSpace(password))\n+        {\n+            var auth = Convert.ToBase64String(Encoding.UTF8.GetBytes($\"{username}:{password}\"));\n+            _httpClient.DefaultRequestHeaders.Add(\"Authorization\", $\"Basic {auth}\");\n+        }\n+\n+        _indexName = indexName.ToLowerInvariant();\n+        _vectorDimension = vectorDimension;\n+        _documentCount = 0;\n+        _cache = new Dictionary<string, VectorDocument<T>>();\n+\n+        EnsureIndex();\n+    }\n+\n+    private void EnsureIndex()\n+    {\n+        var checkResponse = _httpClient.GetAsync($\"/{_indexName}\").Result;\n+        if (checkResponse.IsSuccessStatusCode) \n+        {\n+            UpdateDocumentCount();\n+            return;\n+        }\n+\n+        var mapping = new\n+        {\n+            mappings = new\n+            {\n+                properties = new\n+                {\n+                    id = new { type = \"keyword\" },\n+                    content = new { type = \"text\" },\n+                    embedding = new { type = \"dense_vector\", dims = _vectorDimension },\n+                    metadata = new { type = \"object\", enabled = true }\n+                }\n+            }\n+        };\n+\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(mapping),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        _httpClient.PutAsync($\"/{_indexName}\", content).Wait();\n+    }\n+\n+    private void UpdateDocumentCount()\n+    {\n+        var response = _httpClient.GetAsync($\"/{_indexName}/_count\").Result;\n+        if (response.IsSuccessStatusCode)\n+        {\n+            var responseContent = response.Content.ReadAsStringAsync().Result;\n+            var result = JObject.Parse(responseContent);\n+            _documentCount = result[\"count\"]?.Value<int>() ?? 0;\n+        }\n+    }\n+\n+    protected override void AddCore(VectorDocument<T> vectorDocument)\n+    {\n+        if (vectorDocument.Embedding.Length != _vectorDimension)\n+            throw new ArgumentException($\"Document embedding dimension ({vectorDocument.Embedding.Length}) does not match the store's configured dimension ({_vectorDimension}).\");\n+\n+        _cache[vectorDocument.Document.Id] = vectorDocument;\n+\n+        var embedding = vectorDocument.Embedding.ToArray().Select(v => Convert.ToDouble(v)).ToArray();\n+        \n+        var doc = new\n+        {\n+            id = vectorDocument.Document.Id,\n+            content = vectorDocument.Document.Content,\n+            embedding,\n+            metadata = vectorDocument.Document.Metadata\n+        };\n+\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(doc),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        var response = _httpClient.PutAsync($\"/{_indexName}/_doc/{vectorDocument.Document.Id}\", content).Result;\n+        if (response.IsSuccessStatusCode)\n+            _documentCount++;\n+    }\n+\n+    protected override void AddBatchCore(IList<VectorDocument<T>> vectorDocuments)\n+    {\n+        if (vectorDocuments.Count == 0) return;\n+\n+        foreach (var vd in vectorDocuments)\n+        {\n+            if (vd.Embedding.Length != _vectorDimension)\n+                throw new ArgumentException($\"Document embedding dimension ({vd.Embedding.Length}) does not match the store's configured dimension ({_vectorDimension}).\");\n+            _cache[vd.Document.Id] = vd;\n+        }\n+\n+        var bulkBody = new StringBuilder();\n+        foreach (var vd in vectorDocuments)\n+        {\n+            var indexAction = new { index = new { _index = _indexName, _id = vd.Document.Id } };\n+            bulkBody.AppendLine(Newtonsoft.Json.JsonConvert.SerializeObject(indexAction));\n+\n+            var embedding = vd.Embedding.ToArray().Select(v => Convert.ToDouble(v)).ToArray();\n+            var doc = new\n+            {\n+                id = vd.Document.Id,\n+                content = vd.Document.Content,\n+                embedding,\n+                metadata = vd.Document.Metadata\n+            };\n+            bulkBody.AppendLine(Newtonsoft.Json.JsonConvert.SerializeObject(doc));\n+        }\n+\n+        var content = new StringContent(bulkBody.ToString(), Encoding.UTF8, \"application/x-ndjson\");\n+        var response = _httpClient.PostAsync($\"/{_indexName}/_bulk\", content).Result;\n+        if (response.IsSuccessStatusCode)\n+            _documentCount += vectorDocuments.Count;\n+    }\n+\n+    protected override IEnumerable<Document<T>> GetSimilarCore(Vector<T> queryVector, int topK, Dictionary<string, object> metadataFilters)\n+    {\n+        var embedding = queryVector.ToArray().Select(v => Convert.ToDouble(v)).ToArray();\n+        \n+        // Build the query with metadata filters\n+        object queryClause;\n+        if (metadataFilters != null && metadataFilters.Any())\n+        {\n+            var mustClauses = new List<object>();\n+            foreach (var filter in metadataFilters)\n+            {\n+                mustClauses.Add(new\n+                {\n+                    term = new Dictionary<string, object>\n+                    {\n+                        [$\"metadata.{filter.Key}\"] = filter.Value\n+                    }\n+                });\n+            }\n+            queryClause = new\n+            {\n+                @bool = new\n+                {\n+                    must = mustClauses\n+                }\n+            };\n+        }\n+        else\n+        {\n+            queryClause = new { match_all = new { } };\n+        }\n+\n+        var query = new\n+        {\n+            size = topK,\n+            query = new\n+            {\n+                script_score = new\n+                {\n+                    query = queryClause,\n+                    script = new\n+                    {\n+                        source = \"cosineSimilarity(params.query_vector, 'embedding') + 1.0\",\n+                        @params = new { query_vector = embedding }\n+                    }\n+                }\n+            }\n+        };\n+\n+        var content = new StringContent(\n+            Newtonsoft.Json.JsonConvert.SerializeObject(query),\n+            Encoding.UTF8,\n+            \"application/json\");\n+\n+        var response = _httpClient.PostAsync($\"/{_indexName}/_search\", content).Result;\n+        var responseContent = response.Content.ReadAsStringAsync().Result;\n+        var result = JObject.Parse(responseContent);\n+\n+        var results = new List<Document<T>>();\n+        var hits = result[\"hits\"]?[\"hits\"];\n+        if (hits == null) return results;\n+\n+        foreach (var hit in hits)\n+        {\n+            var source = hit[\"_source\"];\n+            if (source == null) continue;\n+\n+            var id = source[\"id\"]?.ToString() ?? string.Empty;\n+            var docContent = source[\"content\"]?.ToString() ?? string.Empty;\n+            var metadataObj = source[\"metadata\"]?.ToObject<Dictionary<string, object>>() ?? new Dictionary<string, object>();\n+\n+            var score = Convert.ToDouble(hit[\"_score\"]);\n+\n+            var document = new Document<T>(id, docContent, metadataObj)\n+            {\n+                RelevanceScore = NumOps.FromDouble(score),\n+                HasRelevanceScore = true\n+            };\n+\n+            results.Add(document);\n+        }\n+\n+        return results;\n+    }\n+\n+    protected override Document<T>? GetByIdCore(string documentId)\n+    {\n+        if (_cache.TryGetValue(documentId, out var vectorDoc))\n+            return vectorDoc.Document;\n+\n+        return null;\n+    }\n+\n+    /// <summary>\n+    /// Core logic for removing a document from the Elasticsearch index.\n+    /// </summary>\n+    /// <param name=\"documentId\">The validated document ID.</param>\n+    /// <returns>True if the document was found and removed; otherwise, false.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Removes the document from both the cache and the Elasticsearch index via DELETE API.\n+    /// If successful, decrements the document count.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Deletes a document from Elasticsearch.\n+    /// \n+    /// In Elasticsearch terms, this is like:\n+    /// <code>\n+    /// DELETE /index/_doc/document_id\n+    /// </code>\n+    /// \n+    /// Example:\n+    /// <code>\n+    /// if (store.Remove(\"doc-123\"))\n+    ///     Console.WriteLine(\"Document removed from Elasticsearch\");\n+    /// </code>\n+    /// </para>\n+    /// </remarks>\n+    protected override bool RemoveCore(string documentId)\n+    {\n+        _cache.Remove(documentId);\n+\n+        var response = _httpClient.DeleteAsync($\"/{_indexName}/_doc/{documentId}\").Result;\n+        if (response.IsSuccessStatusCode && _documentCount > 0)\n+        {\n+            _documentCount--;\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+    /// <summary>\n+    /// Core logic for retrieving all documents from the Elasticsearch index.\n+    /// </summary>\n+    /// <returns>An enumerable of all documents without their vector embeddings.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// Returns all documents from the cache (in-memory representation) of the Elasticsearch index.\n+    /// In a real Elasticsearch deployment with large indices, use the scroll API for efficient\n+    /// retrieval of all documents without loading everything into memory at once.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Gets every document from the Elasticsearch index.\n+    /// \n+    /// Use cases:\n+    /// - Export index contents for backup\n+    /// - Migrate to a different index or cluster\n+    /// - Bulk reindexing operations\n+    /// - Debugging to see all indexed documents\n+    /// \n+    /// Warning: For large indices (> 10K documents), this can use significant memory.\n+    /// In production Elasticsearch, use the scroll API with pagination:\n+    /// - POST /index/_search?scroll=1m with size parameter\n+    /// - Iterate through scroll_id responses\n+    /// - Clear scroll when done\n+    /// \n+    /// Example:\n+    /// <code>\n+    /// // Get all documents\n+    /// var allDocs = store.GetAll().ToList();\n+    /// Console.WriteLine($\"Total documents in {_indexName}: {allDocs.Count}\");\n+    /// \n+    /// // Export to JSON\n+    /// var json = JsonConvert.SerializeObject(allDocs);\n+    /// File.WriteAllText($\"elasticsearch_{_indexName}_export.json\", json);\n+    /// </code>\n+    /// </para>\n+    /// </remarks>\n+    protected override IEnumerable<Document<T>> GetAllCore()\n+    {\n+        return _cache.Values.Select(vd => vd.Document).ToList();\n+    }\n+\n+    /// <summary>\n+    /// Removes all documents from the Elasticsearch index and recreates it.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// Clears the cache, deletes the Elasticsearch index, resets counters, and recreates the index\n+    /// with the same mapping. The index name remains unchanged and is ready to accept new documents.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> Completely empties the Elasticsearch index and recreates it.\n+    /// \n+    /// After calling Clear():\n+    /// - Index is deleted from Elasticsearch\n+    /// - Cache is cleared\n+    /// - Document count resets to 0\n+    /// - Vector dimension resets to 0\n+    /// - Index is recreated with fresh mapping\n+    /// - Ready for new documents\n+    /// \n+    /// Use with caution - this cannot be undone!\n+    /// \n+    /// In Elasticsearch terms, this is like:\n+    /// <code>\n+    /// DELETE /index_name\n+    /// PUT /index_name with mappings\n+    /// </code>\n+    /// \n+    /// Example:\n+    /// <code>\n+    /// store.Clear();\n+    /// Console.WriteLine($\"Documents in index: {store.DocumentCount}\"); // 0\n+    /// </code>\n+    /// </para>\n+    /// </remarks>\n+    public override void Clear()\n+    {\n+        _cache.Clear();\n+        _httpClient.DeleteAsync($\"/{_indexName}\").Wait();\n+        _documentCount = 0;\n+        _vectorDimension = 0;\n+        EnsureIndex();\n+    }",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/ElasticsearchDocumentStore.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "79d215b091efb01a9397bf46a29a74fdee4d11bc",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Don't reset vector dimension in Clear().**\n\nResetting `_vectorDimension = 0` makes the store unusable after calling `Clear()` because subsequent `AddCore` and `AddBatchCore` calls will fail validation (no document can have embedding dimension 0).\n\n\n\nApply this diff to preserve the configured dimension:\n\n```diff\n public override void Clear()\n {\n     _cache.Clear();\n     _httpClient.DeleteAsync($\"/{_indexName}\").Wait();\n     _documentCount = 0;\n-    _vectorDimension = 0;\n     EnsureIndex();\n }\n```\n\nThis preserves the dimension configured at construction time, allowing the store to continue accepting documents after a clear operation.\n\n<!-- suggestion_start -->\n\n<details>\n<summary>≡ƒô¥ Committable suggestion</summary>\n\n> ΓÇ╝∩╕Å **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    public override void Clear()\n    {\n        _cache.Clear();\n        _httpClient.DeleteAsync($\"/{_indexName}\").Wait();\n        _documentCount = 0;\n        EnsureIndex();\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/ElasticsearchDocumentStore.cs\naround lines 368 to 375, the Clear() method resets _vectorDimension to 0 which\nbreaks subsequent AddCore/AddBatchCore validation; remove the line that sets\n_vectorDimension = 0 so the configured embedding dimension is preserved, keep\nclearing the cache, deleting the index, resetting _documentCount, and calling\nEnsureIndex() so the store can continue accepting documents after Clear().\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T15:51:46Z",
    "updated_at": "2025-11-05T15:51:48Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2495150208",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150208"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2495150208"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150208/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 368,
    "original_start_line": 368,
    "start_side": "RIGHT",
    "line": 375,
    "original_line": 375,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 375,
    "position": 375,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150215",
    "pull_request_review_id": 3422976623,
    "id": 2495150215,
    "node_id": "PRRC_kwDOKSXUF86UuPiH",
    "diff_hunk": "@@ -0,0 +1,334 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.DocumentStores;\n+\n+/// <summary>\n+/// Lightweight SQLite-based vector store using the SQLite-VSS extension.\n+/// </summary>\n+/// <remarks>\n+/// <para>\n+/// This implementation provides an in-memory simulation of SQLite with the VSS (Vector Similarity Search) extension,\n+/// which adds vector search capabilities to the serverless, file-based SQLite database. Perfect for\n+/// edge deployments, mobile apps, and development environments.\n+/// </para>\n+/// <para><b>For Beginners:</b> SQLite is a tiny, serverless database that stores data in a single file, and VSS adds AI search.\n+/// \n+/// Think of it like an Excel file with AI superpowers:\n+/// - No server needed - just a file on disk\n+/// - Perfect for apps, IoT devices, mobile\n+/// - Combine SQL queries with vector search\n+/// \n+/// This in-memory version is good for:\n+/// - Prototyping SQLite-VSS applications\n+/// - Testing local vector search\n+/// - Small collections (< 10K documents)\n+/// \n+/// Real SQLite-VSS provides:\n+/// - Zero-configuration deployment\n+/// - Single-file database (easy backup/transfer)\n+/// - ACID transactions for data integrity\n+/// - Perfect for edge AI and mobile apps\n+/// </para>\n+/// </remarks>\n+/// <typeparam name=\"T\">The numeric data type used for vector operations.</typeparam>\n+public class SQLiteVSSDocumentStore<T> : DocumentStoreBase<T>",
    "path": "src/RetrievalAugmentedGeneration/DocumentStores/SQLiteVSSDocumentStore.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "79d215b091efb01a9397bf46a29a74fdee4d11bc",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n**Class name and documentation are fundamentally misleadingΓÇöimplementation is in-memory, not SQLite-based.**\n\nThe class is named `SQLiteVSSDocumentStore` and all XML documentation describes it as a \"SQLite-based vector store using the SQLite-VSS extension\" with \"file-based database\" storage, \"ACID transactions,\" and suitability for \"edge deployments, mobile apps.\" However, the actual implementation uses a transient in-memory `Dictionary<string, VectorDocument<T>>` (line 42) with no SQLite integration whatsoever.\n\n**Impact:**\n- Consumers expecting persistent, file-based storage will experience silent data loss on process restart\n- The class violates the Liskov Substitution PrincipleΓÇöit cannot substitute for a real document store in production\n- Documentation promises features (ACID, single-file backup, zero-config deployment) that are not delivered\n\n**Required actions:**\n\n1. **Rename the class** to reflect its actual behavior: `InMemoryDocumentStore<T>` or `DictionaryDocumentStore<T>`\n2. **Rewrite all XML documentation** to describe the in-memory, non-persistent nature\n3. **Remove references** to SQLite, file-based storage, edge deployment, database files, transactions, and ACID guarantees\n4. **Update constructor** to remove file-path parameters that suggest persistence (see separate comment)\n\nAlternatively, if SQLite-VSS integration is planned but deferred, mark this class `internal` or `abstract` until the real implementation is complete, and document it as a temporary in-memory stub.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/DocumentStores/SQLiteVSSDocumentStore.cs\naround lines 11 to 40, the class is misnamed and documented as a SQLite-VSS\nfile-based store while the implementation is an in-memory Dictionary; rename the\nclass to InMemoryDocumentStore<T> (or DictionaryDocumentStore<T>), rewrite the\nXML summary/remarks to clearly state it is an in-memory, non-persistent store\n(remove any mention of SQLite, VSS, file-based storage, ACID, transactions,\nbackups, edge/mobile persistence), remove or change constructor parameters that\nimply a file path/persistence, and update visibility to internal or abstract if\nyou intend to mark it as a temporary stub; ensure all references/usages are\nupdated to the new name and documentation reflects its transient behavior.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T15:51:46Z",
    "updated_at": "2025-11-05T15:51:48Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2495150215",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150215"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2495150215"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495150215/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 11,
    "original_start_line": 11,
    "start_side": "RIGHT",
    "line": 40,
    "original_line": 40,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 40,
    "position": 40,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495242270",
    "pull_request_review_id": 3423115081,
    "id": 2495242270,
    "node_id": "PRRC_kwDOKSXUF86UumAe",
    "diff_hunk": "@@ -0,0 +1,211 @@\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+using AiDotNet.Helpers;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.ContextCompression\n+{\n+    /// <summary>\n+    /// Document summarizer for creating concise summaries of retrieved content.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class DocumentSummarizer<T> : ContextCompressorBase<T>\n+    {\n+        private readonly int _maxSummaryLength;\n+        private readonly string _llmEndpoint;\n+        private readonly string _apiKey;\n+\n+        /// <summary>\n+        /// Initializes a new instance of the <see cref=\"DocumentSummarizer{T}\"/> class.\n+        /// </summary>\n+        /// <param name=\"numericOperations\">The numeric operations for type T.</param>\n+        /// <param name=\"maxSummaryLength\">The maximum length of the summary in characters.</param>\n+        /// <param name=\"llmEndpoint\">The LLM API endpoint.</param>\n+        /// <param name=\"apiKey\">The API key for the LLM service.</param>\n+        public DocumentSummarizer(\n+            INumericOperations<T> numericOperations,\n+            int maxSummaryLength = 500,\n+            string llmEndpoint = \"\",\n+            string apiKey = \"\")\n+        {\n+            if (numericOperations == null)\n+                throw new ArgumentNullException(nameof(numericOperations));\n+                \n+            _maxSummaryLength = maxSummaryLength > 0\n+                ? maxSummaryLength\n+                : throw new ArgumentOutOfRangeException(nameof(maxSummaryLength));\n+            _llmEndpoint = llmEndpoint;\n+            _apiKey = apiKey;\n+        }\n+\n+        /// <summary>\n+        /// Compresses documents by summarizing their content with query-aware sentence selection.\n+        /// </summary>\n+        protected override List<Document<T>> CompressCore(\n+            List<Document<T>> documents,\n+            string query,\n+            Dictionary<string, object>? options = null)\n+        {\n+            var summarized = new List<Document<T>>();\n+            var queryTerms = Tokenize(query.ToLowerInvariant());\n+\n+            foreach (var doc in documents)\n+            {\n+                var summary = SummarizeText(doc.Content, queryTerms);\n+                var summarizedDoc = new Document<T>(doc.Id, summary)\n+                {\n+                    Metadata = doc.Metadata,\n+                    RelevanceScore = doc.RelevanceScore,\n+                    HasRelevanceScore = doc.HasRelevanceScore\n+                };\n+                summarized.Add(summarizedDoc);\n+            }\n+\n+            return summarized;\n+        }\n+\n+        /// <summary>\n+        /// Summarizes a list of documents.\n+        /// </summary>\n+        /// <param name=\"documents\">The documents to summarize.</param>\n+        /// <returns>A list of summarized documents.</returns>\n+        public List<Document<T>> Summarize(List<Document<T>> documents)\n+        {\n+            if (documents == null) throw new ArgumentNullException(nameof(documents));\n+\n+            var summarized = new List<Document<T>>();\n+\n+            foreach (var doc in documents)\n+            {\n+                var summary = SummarizeText(doc.Content);\n+                var summarizedDoc = new Document<T>(doc.Id, summary)\n+                {\n+                    Metadata = doc.Metadata,\n+                    RelevanceScore = doc.RelevanceScore,\n+                    HasRelevanceScore = doc.HasRelevanceScore\n+                };\n+                summarized.Add(summarizedDoc);\n+            }\n+\n+            return summarized;\n+        }\n+\n+        /// <summary>\n+        /// Summarizes text to a maximum length with query-aware sentence selection.\n+        /// </summary>\n+        /// <param name=\"text\">The text to summarize.</param>\n+        /// <param name=\"queryTerms\">Optional query terms to prioritize relevant content.</param>\n+        /// <returns>The summarized text.</returns>\n+        public string SummarizeText(string text, List<string>? queryTerms = null)\n+        {\n+            if (string.IsNullOrEmpty(text)) return text;\n+\n+            if (text.Length <= _maxSummaryLength)\n+            {\n+                return text;\n+            }\n+\n+            var sentences = SplitIntoSentences(text);\n+            var importantSentences = ExtractImportantSentences(sentences, queryTerms);\n+\n+            var summary = new System.Text.StringBuilder();\n+            foreach (var sentence in importantSentences)\n+            {\n+                if (summary.Length + sentence.Length > _maxSummaryLength)\n+                {\n+                    // If we haven't added anything yet and the first sentence is too long,\n+                    // truncate it to fit\n+                    if (summary.Length == 0 && sentence.Length > _maxSummaryLength)\n+                    {\n+                        return sentence.Substring(0, _maxSummaryLength).Trim() + \"...\";\n+                    }\n+                    break;\n+                }\n+                summary.Append(sentence).Append(\" \");\n+            }\n+\n+            var result = summary.ToString().Trim();\n+            // If result is empty (all sentences too long), return truncated first sentence\n+            return string.IsNullOrEmpty(result) && importantSentences.Any()\n+                ? importantSentences.First().Substring(0, Math.Min(importantSentences.First().Length, _maxSummaryLength)).Trim() + \"...\"\n+                : result;\n+        }\n+\n+        private List<string> ExtractImportantSentences(List<string> sentences, List<string>? queryTerms = null)\n+        {\n+            var scored = new List<(string sentence, double score)>();\n+\n+            foreach (var sentence in sentences)\n+            {\n+                var importance = ComputeImportance(sentence, sentences, queryTerms);\n+                scored.Add((sentence, importance));\n+            }\n+\n+            return scored\n+                .OrderByDescending(x => x.score)\n+                .Select(x => x.sentence)\n+                .ToList();\n+        }\n+\n+        private double ComputeImportance(string sentence, List<string> allSentences, List<string>? queryTerms = null)\n+        {\n+            var tokens = Tokenize(sentence);\n+            var uniqueTokens = tokens.Distinct().Count();\n+            var length = sentence.Length;\n+\n+            var positionScore = allSentences.IndexOf(sentence) == 0 ? 1.5 : 1.0;\n+\n+            var importance = (uniqueTokens * 0.5) + (Math.Min(length, 200) / 200.0 * 0.5);\n+            importance *= positionScore;\n+            \n+            // Boost sentences containing query terms\n+            if (queryTerms != null && queryTerms.Count > 0)\n+            {\n+                var sentenceLower = sentence.ToLowerInvariant();\n+                var matchCount = queryTerms.Count(qt => sentenceLower.Contains(qt));\n+                if (matchCount > 0)\n+                {\n+                    importance *= (1.0 + (matchCount * 0.5)); // Boost by 50% per matching query term\n+                }\n+            }",
    "path": "src/RetrievalAugmentedGeneration/ContextCompression/DocumentSummarizer.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "8c3de9886866aa7d5486cb21b0905e50e9eac774",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒí Minor_\n\n**Query term matching can produce false positives.**\n\nThe substring matching at line 166 will incorrectly boost sentences containing partial word matches. For example, query term \"car\" would match \"scar\" or \"cartoon\".\n\n\n\nConsider using word boundary matching:\n\n```diff\n             if (queryTerms != null && queryTerms.Count > 0)\n             {\n                 var sentenceLower = sentence.ToLowerInvariant();\n-                var matchCount = queryTerms.Count(qt => sentenceLower.Contains(qt));\n+                var sentenceTokens = Tokenize(sentenceLower);\n+                var matchCount = queryTerms.Count(qt => sentenceTokens.Contains(qt));\n                 if (matchCount > 0)\n                 {\n                     importance *= (1.0 + (matchCount * 0.5)); // Boost by 50% per matching query term\n                 }\n             }\n```\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/ContextCompression/DocumentSummarizer.cs\naround lines 163 to 171, the current substring-based query term matching causes\nfalse positives (e.g., \"car\" matching \"scar\"); replace the simple .Contains\nchecks with word-boundary matching: build safe, case-insensitive matches by\nescaping each query term (Regex.Escape) and either compile a single alternation\nregex like @\"\\b(term1|term2|...)\\b\" with RegexOptions.CultureInvariant |\nRegexOptions.IgnoreCase | RegexOptions.Compiled or test each term with a\nper-term regex using \\b boundaries; then count matches based on regex.Matches\n(or Match collection per term) instead of substring Contains, ensuring\npunctuation and boundaries are respected and performance is considered by\nreusing the compiled regex.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T16:21:02Z",
    "updated_at": "2025-11-05T16:21:03Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2495242270",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495242270"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2495242270"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495242270/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 163,
    "original_start_line": 163,
    "start_side": "RIGHT",
    "line": 171,
    "original_line": 171,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 171,
    "position": 171,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495242285",
    "pull_request_review_id": 3423115081,
    "id": 2495242285,
    "node_id": "PRRC_kwDOKSXUF86UumAt",
    "diff_hunk": "@@ -0,0 +1,166 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\n+{\n+    /// <summary>\n+    /// TF-IDF (Term Frequency-Inverse Document Frequency) retrieval strategy.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class TFIDFRetriever<T> : RetrieverBase<T>\n+    {\n+        private readonly IDocumentStore<T> _documentStore;\n+\n+        public TFIDFRetriever(IDocumentStore<T> documentStore, int defaultTopK = 5) : base(defaultTopK)\n+        {\n+            if (documentStore == null)\n+                throw new ArgumentNullException(nameof(documentStore));\n+                \n+            _documentStore = documentStore;\n+        }\n+\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\n+        {\n+            var queryTerms = Tokenize(query);\n+            var scores = new Dictionary<string, T>();\n+\n+            var candidates = _documentStore.GetSimilar(\n+                new AiDotNet.LinearAlgebra.Vector<T>(new T[_documentStore.VectorDimension]), \n+                _documentStore.DocumentCount\n+            );",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/TFIDFRetriever.cs",
    "commit_id": "8c3de9886866aa7d5486cb21b0905e50e9eac774",
    "original_commit_id": "8c3de9886866aa7d5486cb21b0905e50e9eac774",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒö┤ Critical_\n\n<details>\n<summary>≡ƒº⌐ Analysis chain</summary>\n\n**Verify if IDocumentStore.GetAll() is available.**\n\nThe code still uses the `GetSimilar()` workaround with a zero-filled vector to retrieve all documents. Past review comments indicated this should be addressed by adding a `GetAll()` method to the document store interface. If this method is now available, the retriever should use it instead of the semantic mismatch with vector similarity search.\n\n---\n\n≡ƒÅü Script executed:\n\n```shell\n#!/bin/bash\n# Description: Check if IDocumentStore interface now has GetAll or GetAllDocuments method\n\n# Search for GetAll methods in IDocumentStore interface\nrg -n \"GetAll\" src/Interfaces/IDocumentStore.cs -C 3\n\n# If not found, search for alternative bulk retrieval methods\nrg -n \"interface IDocumentStore\" src/Interfaces/IDocumentStore.cs -A 30 | grep -i \"get.*document\"\n```\n\nLength of output: 289\n\n---\n\n\n\n</details>\n\nNow let me verify the exact implementation in the TFIDFRetriever class:\n\n**Replace GetSimilar() workaround with GetAll() method call.**\n\nThe code still uses `GetSimilar()` with a zero-filled vector to retrieve all documents (lines 31-34). The `IDocumentStore.GetAll()` method is now available and should be used instead, as it correctly represents the intent and avoids the semantic mismatch of querying vector similarity with an empty vector.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Retrievers/TFIDFRetriever.cs around lines 31\nto 34, the code currently calls _documentStore.GetSimilar(...) with a\nzero-filled vector to fetch every document; replace that workaround by calling\n_documentStore.GetAll() instead. Remove the zero-vector creation and the\nDocumentCount argument, assign the result of GetAll() to the same candidates\nvariable (or adjust its type if necessary), and ensure any downstream logic\nstill iterates over the returned collection.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\nΓ£à Addressed in commits 69faff4 to ef3c452",
    "created_at": "2025-11-05T16:21:02Z",
    "updated_at": "2025-11-05T17:10:52Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2495242285",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495242285"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2495242285"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495242285/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": null,
    "original_start_line": 31,
    "start_side": "RIGHT",
    "line": null,
    "original_line": 34,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 34,
    "position": 1,
    "subject_type": "line"
  },
  {
    "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495242296",
    "pull_request_review_id": 3423115081,
    "id": 2495242296,
    "node_id": "PRRC_kwDOKSXUF86UumA4",
    "diff_hunk": "@@ -0,0 +1,166 @@\n+using AiDotNet.Helpers;\n+using AiDotNet.Interfaces;\n+using AiDotNet.RetrievalAugmentedGeneration.Models;\n+using System;\n+using System.Collections.Generic;\n+using System.Linq;\n+\n+namespace AiDotNet.RetrievalAugmentedGeneration.Retrievers\n+{\n+    /// <summary>\n+    /// TF-IDF (Term Frequency-Inverse Document Frequency) retrieval strategy.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type for vector operations.</typeparam>\n+    public class TFIDFRetriever<T> : RetrieverBase<T>\n+    {\n+        private readonly IDocumentStore<T> _documentStore;\n+\n+        public TFIDFRetriever(IDocumentStore<T> documentStore, int defaultTopK = 5) : base(defaultTopK)\n+        {\n+            if (documentStore == null)\n+                throw new ArgumentNullException(nameof(documentStore));\n+                \n+            _documentStore = documentStore;\n+        }\n+\n+        protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\n+        {\n+            var queryTerms = Tokenize(query);\n+            var scores = new Dictionary<string, T>();\n+\n+            var candidates = _documentStore.GetSimilar(\n+                new AiDotNet.LinearAlgebra.Vector<T>(new T[_documentStore.VectorDimension]), \n+                _documentStore.DocumentCount\n+            );\n+\n+            var candidatesList = candidates.ToList();\n+            var candidatesById = candidatesList.ToDictionary(d => d.Id);\n+            var tfidf = BuildTFIDFStatistics(candidatesList);",
    "path": "src/RetrievalAugmentedGeneration/Retrievers/TFIDFRetriever.cs",
    "commit_id": "ef3c452c6ecfc08c9ae3eaf6861a81cfacd5cc15",
    "original_commit_id": "8c3de9886866aa7d5486cb21b0905e50e9eac774",
    "user": {
      "login": "coderabbitai[bot]",
      "id": 136622811,
      "node_id": "BOT_kgDOCCSy2w",
      "avatar_url": "https://avatars.githubusercontent.com/in/347564?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/coderabbitai%5Bbot%5D",
      "html_url": "https://github.com/apps/coderabbitai",
      "followers_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/coderabbitai%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "_ΓÜá∩╕Å Potential issue_ | _≡ƒƒá Major_\n\n**Performance bottleneck: TF-IDF statistics recomputed on every query.**\n\n`BuildTFIDFStatistics` is called on every retrieval, scanning the entire document collection and recomputing term frequencies, document frequencies, and TF-IDF vectors from scratch. For a corpus of N documents with average length M, this is O(N*M) on every query.\n\nFor production use with large document collections (10k+ documents), this will cause severe performance degradation. TF-IDF statistics should be computed once when documents are indexed and cached, only rebuilding when the document store changes.\n\n\n\nConsider one of these approaches:\n1. **Pre-compute and cache**: Build statistics in constructor or first query, cache in instance fields, and provide a method to refresh when documents change.\n2. **Event-based invalidation**: Subscribe to document store changes and invalidate cache when documents are added/removed.\n3. **Separate indexing phase**: Require explicit `BuildIndex()` call after documents are loaded, making the cost explicit to users.\n\nExample for approach 1:\n\n```diff\n public class TFIDFRetriever<T> : RetrieverBase<T>\n {\n     private readonly IDocumentStore<T> _documentStore;\n+    private Dictionary<string, Dictionary<string, T>>? _cachedTfidf;\n+    private int _lastDocumentCount;\n\n     // ... constructor ...\n\n     protected override IEnumerable<Document<T>> RetrieveCore(string query, int topK, Dictionary<string, object> metadataFilters)\n     {\n         var queryTerms = Tokenize(query);\n         var scores = new Dictionary<string, T>();\n\n         var candidates = _documentStore.GetSimilar(\n             new AiDotNet.LinearAlgebra.Vector<T>(new T[_documentStore.VectorDimension]), \n             _documentStore.DocumentCount\n         );\n\n         var candidatesList = candidates.ToList();\n         var candidatesById = candidatesList.ToDictionary(d => d.Id);\n-        var tfidf = BuildTFIDFStatistics(candidatesList);\n+        \n+        // Rebuild statistics only if document count changed\n+        if (_cachedTfidf == null || _lastDocumentCount != _documentStore.DocumentCount)\n+        {\n+            _cachedTfidf = BuildTFIDFStatistics(candidatesList);\n+            _lastDocumentCount = _documentStore.DocumentCount;\n+        }\n+        var tfidf = _cachedTfidf;\n\n         // ... rest of method ...\n     }\n }\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>≡ƒñû Prompt for AI Agents</summary>\n\n```\nIn src/RetrievalAugmentedGeneration/Retrievers/TFIDFRetriever.cs around lines\n36-38, BuildTFIDFStatistics is being called on every retrieval causing O(N*M)\nwork per query; change to compute and cache TF-IDF statistics once (e.g., in the\nconstructor or lazily on first query) storing term frequencies, document\nfrequencies and TF-IDF vectors in instance fields, and add a public RefreshIndex\nor InvalidateCache method to recompute when documents change; alternatively hook\ninto document-store change events to invalidate the cache so subsequent queries\nreuse the cached statistics instead of rebuilding them each time.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->",
    "created_at": "2025-11-05T16:21:02Z",
    "updated_at": "2025-11-05T16:21:03Z",
    "html_url": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2495242296",
    "pull_request_url": "https://api.github.com/repos/ooples/AiDotNet/pulls/304",
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495242296"
      },
      "html": {
        "href": "https://github.com/ooples/AiDotNet/pull/304#discussion_r2495242296"
      },
      "pull_request": {
        "href": "https://api.github.com/repos/ooples/AiDotNet/pulls/304"
      }
    },
    "reactions": {
      "url": "https://api.github.com/repos/ooples/AiDotNet/pulls/comments/2495242296/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "start_line": 32,
    "original_start_line": 36,
    "start_side": "RIGHT",
    "line": 34,
    "original_line": 38,
    "side": "RIGHT",
    "author_association": "CONTRIBUTOR",
    "original_position": 38,
    "position": 34,
    "subject_type": "line"
  }
]
