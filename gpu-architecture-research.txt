Loaded cached credentials.
File C:\Users\yolan\.cache/vscode-ripgrep/ripgrep-v13.0.0-10-x86_64-pc-windows-msvc.zip has been cached
Of course. This is an excellent and classic architecture problem in computational frameworks. Here is a detailed analysis based on how major frameworks solve this, with a concrete recommendation for AiDotNet.

### Executive Summary & Recommendation

The most robust and widely adopted pattern is a **unified public API with backend-specific kernel dispatch**. Users interact with a single `Tensor` object and a single `DenseLayer` class, regardless of the underlying hardware. The framework, guided by a device context, dispatches the actual computation to the correct, highly-optimized kernel (CPU or GPU).

For AiDotNet, I recommend a hybrid of your proposed options, leaning heavily towards a **Strategy Pattern** combined with an **Abstract Factory**.

1.  **Unify your data structure:** Evolve `Vector<T>` and `Tensor<T>` into a single, powerful `Tensor<T>` class. This class should be capable of holding data on any device (CPU or GPU). It acts as a descriptor for the data, not just the data itself.
2.  **Use the Strategy Pattern for Operations:** Your layers (`DenseLayer<T>`) should be thin, public-facing classes that hold parameters (weights, biases) and delegate the actual computation to a strategy object (e.g., `IDenseStrategy<T>`).
3.  **Implement device-specific strategies:** You will have `CpuDenseStrategy<T>` and `GpuDenseStrategy<T>`. The `unmanaged` constraint is applied only to the GPU strategy, preventing it from polluting your public API.
4.  **Use a Factory for object creation:** A `LayerFactory` or `ModelBuilder` will be responsible for creating layers and injecting the correct strategy based on the current device context (e.g., `AiDotNet.Device = "GPU"`).

This approach avoids public API fragmentation (unlike Option B) and minimizes code duplication in the main classes (unlike Option A), while cleanly isolating the `unmanaged` constraint.

---

### Detailed Analysis of Research Questions

#### 1. How do major ML frameworks handle CPU/GPU variants?

**PyTorch & TensorFlow:**
Both frameworks present a **unified API**. A developer writes code against a single `Tensor` object and a single set of operations (e.g., `torch.matmul`, `tf.linalg.matmul`).

The core concept is the **Device**.
- **PyTorch:** `torch.device('cpu')` or `torch.device('cuda:0')`. Tensors are explicitly moved between devices using `my_tensor.to(device)`. When an operation is called (e.g., `a + b`), PyTorch inspects the device of the input tensors and dispatches the call to the appropriate pre-compiled C++/CUDA kernel.
- **TensorFlow:** `with tf.device('/CPU:0'):` or `with tf.device('/GPU:0'):`. TensorFlow's runtime builds a computation graph and places operations on the specified devices. Like PyTorch, it dispatches to device-specific kernels.

**ONNX Runtime:**
ONNX uses the concept of **Execution Providers (EPs)**. When you create an inference session, you provide a list of preferred EPs (e.g., `['CUDAExecutionProvider', 'CPUExecutionProvider']`). ONNX Runtime walks the graph and assigns each operation to the first EP in the list that supports it. This is a clear example of the **Backend/Provider Pattern**. The user doesn't interact with device-specific operations at all, only with the high-level `InferenceSession`.

**ML.NET:**
ML.NET's approach is higher-level. GPU support is typically enabled for specific, coarse-grained components (like an entire model's inference via `ApplyOnnxModel` or specific trainers) rather than for fine-grained tensor operations. You might specify `gpuDeviceId: 0` in a pipeline component's options. It abstracts the device choice away into configuration.

#### 2. What design patterns do they use for conditional GPU acceleration?

The dominant patterns are:

*   **Backend/Provider Pattern:** A central dispatcher or registry holds multiple implementations (backends) for each operation (e.g., `MatMul`). Implementations are registered for `(operation_name, device_type, data_type)`. When `MatMul` is called on a GPU tensor, the dispatcher looks up the `('MatMul', 'GPU', 'float32')` kernel and executes it. This is the core of TF and PyTorch's C++ dispatch mechanism.

*   **Strategy Pattern:** This is a higher-level application of the same idea, suitable for your library. A context class (e.g., `DenseLayer`) delegates its computational logic to a separate strategy object. The layer doesn't know *how* the math is done, only that it has a strategy that can do it.

    ```csharp
    // Public facing layer
    public class DenseLayer<T>
    {
        private readonly IDenseStrategy<T> _strategy;
        public Tensor<T> Weights { get; }

        // The strategy is injected by a factory
        public DenseLayer(IDenseStrategy<T> strategy, Tensor<T> weights) {
            _strategy = strategy;
            Weights = weights;
        }

        public Tensor<T> Forward(Tensor<T> input) {
            return _strategy.Execute(input, Weights);
        }
    }
    ```

*   **Abstract Factory Pattern:** A factory is used to construct objects (layers, optimizers) for the correct device, hiding the concrete implementations from the user.

    ```csharp
    public class LayerFactory
    {
        private readonly Device _device;

        public LayerFactory(Device device) { _device = device; }

        public DenseLayer<T> CreateDenseLayer<T>(...) where T : unmanaged
        {
            if (_device == Device.GPU) {
                // GpuDenseStrategy requires 'unmanaged'
                var strategy = new GpuDenseStrategy<T>();
                return new DenseLayer<T>(strategy, ...);
            }
            else {
                // This path won't work if T can be non-unmanaged.
                // This highlights the need for careful interface design.
                // See the final recommendation below.
            }
        }
    }
    ```

#### 3. Do they use different interfaces/signatures for GPU operations?

**No.** This is a critical point. At the user-facing level (Python, C#), the interface is **identical**. The power of these frameworks comes from the fact that the following code works without changes whether the tensor is on a CPU or GPU:

```python
# PyTorch example
output = my_layer(input_tensor)
loss = loss_function(output, target)
```

Different signatures (your Option B) would break this abstraction, forcing users to write device-specific code paths, which is what a framework should prevent.

#### 4. How do they minimize code duplication?

At the lowest level (C++), they use templates and preprocessor macros extensively. A single source file for an operation can be used to generate kernels for dozens of `(device, data_type)` combinations.

For your C# library, the **Strategy Pattern** is the best way to achieve this.
- The core logic and parameter management reside in `DenseLayer<T>`. This code is written once.
- The device-specific logic is isolated in the strategy classes (`CpuDenseStrategy`, `GpuDenseStrategy`).
- If there's any common setup or validation logic, it can be placed in a shared abstract base class for the strategies.

#### 5. Industry best practices for factory/runtime selection of GPU vs CPU?

1.  **Explicit Device Placement:** The user explicitly places tensors and models on a device (e.g., `model.to('cuda')`). This is the most common pattern in training and research code.
2.  **Global Context:** A static variable or context manager sets the default device for all subsequent operations (e.g., `tf.device(...)`).
3.  **Automatic Selection:** The framework automatically selects the best device. This is common in inference scenarios where the framework can query available hardware and pick the fastest option (e.g., ONNX Runtime's EP priority list).

For AiDotNet, a simple static property is a great start:
`AiDotNet.Configuration.DefaultDevice = Device.Gpu;`

Your factory would then read this static property to decide which strategy to instantiate.

#### 6. Performance patterns: Is it acceptable to have different input/output types for GPU variants?

It is **highly discouraged** for the public API, primarily because it kills performance and usability. Every time you switch between a `Vector<T>` and a `Tensor<T>`, you would likely need to perform a memory copy and conversion.

The best practice is a **single, unified `Tensor` class**. This class would internally manage the data's location.

```csharp
public class Tensor<T>
{
    public Device Device { get; private set; }
    private IMemory<T> _data; // IMemory could be an interface implemented by a CPU array and an ILGPU MemoryBuffer

    // This is the key: ToDevice handles the copy
    public Tensor<T> ToDevice(Device newDevice)
    {
        if (newDevice == this.Device) return this;

        // Logic to copy data from current device's _data
        // to a new buffer on the newDevice.
        // e.g., copy from CPU array to ILGPU MemoryBuffer
    }
}
```

#### 7. Examples of real-world libraries that solved similar constraint propagation issues?

This `unmanaged` constraint is a uniquely C#/.NET challenge not present in Python/C++. The closest parallel is how C++ libraries use templates.

The solution lies in **isolating the constraint**. The `unmanaged` constraint should not appear in your public, user-facing classes (`DenseLayer<T>`). It should only appear on the concrete, internal implementations that require it (`GpuDenseStrategy<T>`).

The **Strategy Pattern** is the textbook solution for this. The `DenseLayer<T>` depends on an `IDenseStrategy<T>` interface. This interface has no constraints. The constraint is applied to the `GpuDenseStrategy<T>` class that implements the interface, which is perfectly valid in C#.

### Concrete Recommendation for AiDotNet

Here is a sketch of the recommended architecture:

```csharp
// 1. Unified Tensor and Device Enum
public enum DeviceType { Cpu, Gpu }

public class Tensor<T>
{
    public DeviceType Device { get; }
    public IMemoryBuffer<T> Buffer { get; } // Using ILGPU's buffer concept
    // ... methods for shape, size, ToDevice(), etc.
}

// 2. Strategy Interface (NO constraints here)
public interface IActivationStrategy<T>
{
    Tensor<T> Forward(Tensor<T> input);
    Tensor<T> Backward(Tensor<T> gradient);
}

// 3. Concrete CPU Strategy (using flexible INumericOperations)
public class CpuReluStrategy<T> : IActivationStrategy<T> where T : INumericOperations<T>
{
    public Tensor<T> Forward(Tensor<T> input) { /* CPU implementation */ }
    public Tensor<T> Backward(Tensor<T> gradient) { /* CPU implementation */ }
}

// 4. Concrete GPU Strategy (WITH the 'unmanaged' constraint)
public class GpuReluStrategy<T> : IActivationStrategy<T> where T : unmanaged
{
    public Tensor<T> Forward(Tensor<T> input)
    {
        // Ensure input is on GPU, then launch ILGPU kernel
    }
    public Tensor<T> Backward(Tensor<T> gradient) { /* ILGPU kernel */ }
}

// 5. Public-facing Layer (NO constraints here)
public class ActivationLayer<T>
{
    private readonly IActivationStrategy<T> _strategy;

    // Injected by the factory
    internal ActivationLayer(IActivationStrategy<T> strategy)
    {
        _strategy = strategy;
    }

    public Tensor<T> Forward(Tensor<T> input) => _strategy.Forward(input);
}

// 6. Factory to tie it all together
public static class LayerFactory
{
    public static ActivationLayer<T> CreateActivationLayer<T>(DeviceType device)
    {
        IActivationStrategy<T> strategy = (device, typeof(T)) switch
        {
            (DeviceType.Gpu, Type t) when IsUnmanaged(t) =>
                // Activator.CreateInstance can be used to overcome generic constraint issues
                (IActivationStrategy<T>)Activator.CreateInstance(typeof(GpuReluStrategy<>).MakeGenericType(typeof(T))),

            (DeviceType.Cpu, _) =>
                (IActivationStrategy<T>)Activator.CreateInstance(typeof(CpuReluStrategy<>).MakeGenericType(typeof(T))),

            _ => throw new NotSupportedException($"Device {device} not supported for type {typeof(T).Name}")
        };

        return new ActivationLayer<T>(strategy);
    }

    private static bool IsUnmanaged(Type t) =>
        t.IsPrimitive || t.IsPointer || t.IsEnum || (t.IsValueType && !t.IsGenericType && t.GetFields().All(f => IsUnmanaged(f.FieldType)));
}
```
This structure provides the best of all worlds: a clean public API, isolation of hardware-specific code and constraints, and minimal duplication.
