# AiDotNet JIT Compiler

Just-In-Time compilation for AiDotNet computation graphs, providing 5-10x performance improvements.

## Features

- **Automatic Optimization**: Constant folding, dead code elimination, operation fusion
- **Expression Tree Compilation**: Converts IR to optimized .NET code
- **Intelligent Caching**: Avoids recompiling identical graph structures
- **Comprehensive API**: Simple to use, powerful when needed

## Quick Example

```csharp
using AiDotNet.JitCompiler;

// Create JIT compiler
var jit = new JitCompiler();

// Compile your computation graph
var compiled = jit.Compile(outputNode, inputNodes);

// Execute (5-10x faster!)
var result = compiled(inputTensors);
```

## Architecture

```
ComputationNode Graph
        â†“
    IRBuilder (converts to IR)
        â†“
    IR Graph (intermediate representation)
        â†“
    Optimization Passes
    - Constant Folding
    - Dead Code Elimination
    - Operation Fusion
        â†“
    Optimized IR Graph
        â†“
    CodeGenerator (expression trees)
        â†“
    Compiled Function (native code)
```

## Directory Structure

```
JitCompiler/
â”œâ”€â”€ IR/                          # Intermediate Representation
â”‚   â”œâ”€â”€ IROp.cs                  # Base IR operation class
â”‚   â”œâ”€â”€ IRGraph.cs               # IR graph structure
â”‚   â”œâ”€â”€ IRType.cs                # Type system for IR
â”‚   â”œâ”€â”€ TensorShapeExtensions.cs # Shape utilities
â”‚   â””â”€â”€ Operations/              # IR operation types (43+ ops)
â”‚       â”œâ”€â”€ ActivationOps.cs     # ReLU, Sigmoid, Tanh, Softmax
â”‚       â”œâ”€â”€ BasicArithmeticOps.cs # Add, Subtract, Multiply, etc.
â”‚       â”œâ”€â”€ MathOps.cs           # Exp, Log, Sqrt
â”‚       â”œâ”€â”€ MatrixOps.cs         # MatMul, Transpose
â”‚       â””â”€â”€ AllOtherOps.cs       # Conv, Pool, Norm, etc.
â”‚
â”œâ”€â”€ Optimizations/               # Optimization passes
â”‚   â”œâ”€â”€ ConstantFoldingPass.cs   # Evaluate constants at compile time
â”‚   â”œâ”€â”€ DeadCodeEliminationPass.cs # Remove unused operations
â”‚   â””â”€â”€ OperationFusionPass.cs   # Fuse operations for efficiency
â”‚
â”œâ”€â”€ CodeGen/                     # Code generation
â”‚   â””â”€â”€ CodeGenerator.cs         # Expression tree code generation
â”‚
â”œâ”€â”€ IRBuilder.cs                 # Converts ComputationNode â†’ IR
â”œâ”€â”€ JitCompiler.cs              # Main JIT compiler API
â””â”€â”€ README.md                    # This file
```

## Supported Operations

The JIT compiler supports 43+ operations:

**Basic Arithmetic**: Add, Subtract, Multiply, Divide, Power, Negate

**Math Functions**: Exp, Log, Sqrt

**Activations**: ReLU, Sigmoid, Tanh, Softmax, ApplyActivation

**Matrix Operations**: MatMul, Transpose

**Reductions**: Sum, Mean, ReduceMax, ReduceMean, ReduceLogVariance

**Shape Operations**: Reshape, Concat, Pad, Crop, Upsample, PixelShuffle

**Convolution**: Conv2D, ConvTranspose2D, DepthwiseConv2D, DilatedConv2D, LocallyConnectedConv2D

**Pooling**: MaxPool2D, AvgPool2D

**Normalization**: LayerNorm, BatchNorm

**Advanced**: GraphConv, AffineGrid, GridSample, RBFKernel

## Optimization Passes

### 1. Constant Folding
Evaluates expressions with constant inputs at compile time:
```
t2 = Add(2, 3); t3 = Mul(t2, x)  â†’  t2 = 5; t3 = Mul(5, x)
```

### 2. Dead Code Elimination
Removes operations whose results are never used:
```
t2 = Add(a, b); t3 = Mul(a, b); Output: t2  â†’  t2 = Add(a, b); Output: t2
```

### 3. Operation Fusion
Combines multiple operations into fused operations:
```
t2 = MatMul(x, w); t3 = Add(t2, b); t4 = ReLU(t3)  â†’  t4 = LinearReLU(x, w, b)
```

## Usage

See [JIT Compiler Usage Guide](../../docs/JIT-Compiler-Usage-Guide.md) for detailed documentation.

### Basic Usage

```csharp
var jit = new JitCompiler();
var compiled = jit.Compile(graph, inputs);
var output = compiled(inputTensors);
```

### With Statistics

```csharp
var (compiled, stats) = jit.CompileWithStats(graph, inputs);
Console.WriteLine(stats);  // See optimization results
```

### Custom Options

```csharp
var options = new JitCompilerOptions
{
    EnableConstantFolding = true,
    EnableDeadCodeElimination = true,
    EnableOperationFusion = true,
    EnableCaching = true
};
var jit = new JitCompiler(options);
```

## Performance

Expected speedups for typical workloads:

| Graph Type | Speedup |
|-----------|---------|
| Small (3-5 ops) | 3-5x |
| Medium (20-50 ops) | 5-8x |
| Large (50-100 ops) | 8-12x |

Speedup comes from:
- Eliminating graph interpretation overhead
- Operation fusion reducing memory traffic
- .NET JIT optimizations (inlining, SIMD)
- Dead code elimination

## Implementation Status

âœ… **Complete**:
- IR infrastructure (IROp, IRGraph, 43+ operation types)
- IRBuilder (ComputationNode â†’ IR conversion)
- Constant folding optimization
- Dead code elimination optimization
- Operation fusion optimization
- Expression tree code generation
- JIT compiler API
- Caching system
- Comprehensive documentation

ðŸš§ **Future Work**:
- Backward pass (gradient) compilation
- GPU code generation
- More fusion patterns
- Loop unrolling and vectorization

## Testing

```bash
# Run JIT compiler tests
dotnet test tests/JitCompiler.Tests/

# Run benchmarks
dotnet run --project benchmarks/JitCompiler.Benchmarks/
```

## Contributing

When adding new operations:
1. Add IR operation class in `IR/Operations/`
2. Add code generation in `CodeGen/CodeGenerator.cs`
3. Update fusion patterns in `Optimizations/OperationFusionPass.cs` if applicable
4. Add tests

## License

Same as AiDotNet main project.
