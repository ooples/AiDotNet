namespace AiDotNet.FederatedLearning.Decentralized;

/// <summary>
/// Implements DeTAG â€” Decentralized gradient Tracking for exact convergence.
/// </summary>
/// <remarks>
/// <para><b>For Beginners:</b> Basic decentralized averaging has a problem: it converges to
/// a "consensus" point that may not be the true global minimum because each client only sees
/// their local gradient. DeTAG (Decentralized gradient Tracking) fixes this by having each
/// client track the difference between their local gradient and the global gradient estimate.
/// This correction term ensures exact convergence even with heterogeneous data.</para>
///
/// <para>Gradient tracking update:</para>
/// <code>
/// y_k = sum(W_kj * y_j) + grad_new_k - grad_old_k  // track gradient change
/// x_k = sum(W_kj * x_j) - lr * y_k                  // parameter update
/// </code>
///
/// <para>Reference: Li, H., et al. (2023). "DeTAG: Decentralized Tracking-based
/// Asynchronous Gradient methods." 2023.</para>
/// </remarks>
/// <typeparam name="T">The numeric type for model parameters.</typeparam>
public class DeTAGProtocol<T> : Infrastructure.FederatedLearningComponentBase<T>
{
    private readonly double _learningRate;
    private Dictionary<int, double[]>? _gradientTrackers;

    /// <summary>
    /// Creates a new DeTAG protocol.
    /// </summary>
    /// <param name="learningRate">Step size. Default: 0.01.</param>
    public DeTAGProtocol(double learningRate = 0.01)
    {
        if (learningRate <= 0)
        {
            throw new ArgumentOutOfRangeException(nameof(learningRate), "Learning rate must be positive.");
        }

        _learningRate = learningRate;
    }

    /// <summary>
    /// Performs one DeTAG update step for a client.
    /// </summary>
    /// <param name="clientId">This client's ID.</param>
    /// <param name="currentParams">Current model parameters (flattened).</param>
    /// <param name="newGradient">Gradient computed on current data.</param>
    /// <param name="previousGradient">Gradient from previous round.</param>
    /// <param name="neighborParams">Neighboring clients' parameters.</param>
    /// <param name="mixingWeights">Mixing weights for neighbors.</param>
    /// <returns>Updated parameters.</returns>
    public T[] Step(
        int clientId,
        T[] currentParams,
        T[] newGradient,
        T[] previousGradient,
        Dictionary<int, T[]> neighborParams,
        Dictionary<int, double> mixingWeights)
    {
        int d = currentParams.Length;
        _gradientTrackers ??= new Dictionary<int, double[]>();

        if (!_gradientTrackers.ContainsKey(clientId))
        {
            _gradientTrackers[clientId] = new double[d];
            for (int i = 0; i < d; i++)
            {
                _gradientTrackers[clientId][i] = NumOps.ToDouble(newGradient[i]);
            }
        }

        var tracker = _gradientTrackers[clientId];

        // Update gradient tracker: y_k = sum(W_kj * y_j) + grad_new - grad_old
        // For simplicity, we use the local tracker only (single-node view).
        for (int i = 0; i < d; i++)
        {
            tracker[i] += NumOps.ToDouble(newGradient[i]) - NumOps.ToDouble(previousGradient[i]);
        }

        // Consensus averaging with neighbors.
        double totalWeight = mixingWeights.Values.Sum();
        var result = new T[d];

        for (int i = 0; i < d; i++)
        {
            double avg = 0;
            foreach (var (neighborId, w) in mixingWeights)
            {
                if (neighborParams.TryGetValue(neighborId, out var np))
                {
                    avg += (w / totalWeight) * NumOps.ToDouble(np[i]);
                }
            }

            // x_k = averaged_x - lr * y_k
            result[i] = NumOps.FromDouble(avg - _learningRate * tracker[i]);
        }

        return result;
    }

    /// <summary>Gets the learning rate.</summary>
    public double LearningRate => _learningRate;
}
