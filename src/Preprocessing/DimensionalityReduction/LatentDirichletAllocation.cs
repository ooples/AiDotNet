using AiDotNet.Helpers;
using AiDotNet.Tensors.LinearAlgebra;

namespace AiDotNet.Preprocessing.DimensionalityReduction;

/// <summary>
/// Latent Dirichlet Allocation for topic modeling.
/// </summary>
/// <remarks>
/// <para>
/// LDA is a generative probabilistic model that discovers latent topics in a collection
/// of documents (or more generally, in count data). Each document is modeled as a mixture
/// of topics, and each topic is a distribution over words (features).
/// </para>
/// <para>
/// The model assumes:
/// - Each document has a distribution over topics (theta)
/// - Each topic has a distribution over words (beta)
/// - Words in documents are generated by first sampling a topic, then sampling a word
/// </para>
/// <para><b>For Beginners:</b> LDA finds hidden themes (topics) in your data:
/// - Input: Document-term matrix (rows=documents, columns=word counts)
/// - Output: Document-topic distribution (what topics each document is about)
/// - Also learns: Topic-word distribution (what words define each topic)
/// - Use for: Topic modeling, document clustering, feature extraction
/// </para>
/// </remarks>
/// <typeparam name="T">The numeric type for calculations (e.g., float, double).</typeparam>
public class LatentDirichletAllocation<T> : TransformerBase<T, Matrix<T>, Matrix<T>>
{
    private readonly int _nComponents;
    private readonly double _docTopicPrior; // Alpha
    private readonly double _topicWordPrior; // Beta (eta)
    private readonly LdaLearningMethod _learningMethod;
    private readonly int _maxIter;
    private readonly double _tol;
    private readonly int _batchSize;
    private readonly int? _randomState;

    // Fitted parameters
    private double[,]? _components; // Topic-word distribution (k x vocab)
    private double[,]? _expDigammaComponents;
    private int _nVocab;

    /// <summary>
    /// Gets the number of topics.
    /// </summary>
    public int NComponents => _nComponents;

    /// <summary>
    /// Gets the document-topic prior (alpha).
    /// </summary>
    public double DocTopicPrior => _docTopicPrior;

    /// <summary>
    /// Gets the topic-word prior (beta/eta).
    /// </summary>
    public double TopicWordPrior => _topicWordPrior;

    /// <summary>
    /// Gets the learning method.
    /// </summary>
    public LdaLearningMethod LearningMethod => _learningMethod;

    /// <summary>
    /// Gets the topic-word distribution matrix (topics x vocabulary).
    /// Each row is a topic, each column is a word probability.
    /// </summary>
    public double[,]? Components => _components;

    /// <summary>
    /// Gets the vocabulary size.
    /// </summary>
    public int NVocab => _nVocab;

    /// <summary>
    /// Gets whether this transformer supports inverse transformation.
    /// </summary>
    public override bool SupportsInverseTransform => false;

    /// <summary>
    /// Creates a new instance of <see cref="LatentDirichletAllocation{T}"/>.
    /// </summary>
    /// <param name="nComponents">Number of topics. Defaults to 10.</param>
    /// <param name="docTopicPrior">Prior for document-topic distribution (alpha). If null, uses 1/n_topics.</param>
    /// <param name="topicWordPrior">Prior for topic-word distribution (beta/eta). If null, uses 1/n_topics.</param>
    /// <param name="learningMethod">Learning method to use. Defaults to Online.</param>
    /// <param name="maxIter">Maximum number of iterations. Defaults to 10.</param>
    /// <param name="tol">Convergence tolerance. Defaults to 1e-4.</param>
    /// <param name="batchSize">Batch size for online learning. Defaults to 128.</param>
    /// <param name="randomState">Random seed for reproducibility.</param>
    /// <param name="columnIndices">The column indices to use, or null for all columns.</param>
    public LatentDirichletAllocation(
        int nComponents = 10,
        double? docTopicPrior = null,
        double? topicWordPrior = null,
        LdaLearningMethod learningMethod = LdaLearningMethod.Online,
        int maxIter = 10,
        double tol = 1e-4,
        int batchSize = 128,
        int? randomState = null,
        int[]? columnIndices = null)
        : base(columnIndices)
    {
        if (nComponents < 1)
        {
            throw new ArgumentException("Number of components must be at least 1.", nameof(nComponents));
        }

        _nComponents = nComponents;
        _docTopicPrior = docTopicPrior ?? 1.0 / nComponents;
        _topicWordPrior = topicWordPrior ?? 1.0 / nComponents;
        _learningMethod = learningMethod;
        _maxIter = maxIter;
        _tol = tol;
        _batchSize = batchSize;
        _randomState = randomState;
    }

    /// <summary>
    /// Fits LDA using variational inference.
    /// </summary>
    protected override void FitCore(Matrix<T> data)
    {
        int n = data.Rows; // Number of documents
        _nVocab = data.Columns; // Vocabulary size
        int k = _nComponents;

        var random = _randomState.HasValue
            ? RandomHelper.CreateSeededRandom(_randomState.Value)
            : RandomHelper.CreateSeededRandom(42);

        // Convert data to double array (document-term counts)
        var X = new double[n, _nVocab];
        for (int i = 0; i < n; i++)
        {
            for (int j = 0; j < _nVocab; j++)
            {
                X[i, j] = NumOps.ToDouble(data[i, j]);
            }
        }

        // Initialize topic-word distribution (lambda)
        _components = new double[k, _nVocab];
        for (int t = 0; t < k; t++)
        {
            double sum = 0;
            for (int w = 0; w < _nVocab; w++)
            {
                _components[t, w] = random.NextDouble() + _topicWordPrior;
                sum += _components[t, w];
            }
            // Normalize
            for (int w = 0; w < _nVocab; w++)
            {
                _components[t, w] /= sum;
            }
        }

        if (_learningMethod == LdaLearningMethod.Batch)
        {
            FitBatch(X, n, k);
        }
        else
        {
            FitOnline(X, n, k, random);
        }

        // Precompute exp(digamma) for components for transform
        _expDigammaComponents = new double[k, _nVocab];
        for (int t = 0; t < k; t++)
        {
            for (int w = 0; w < _nVocab; w++)
            {
                _expDigammaComponents[t, w] = Math.Exp(Digamma(_components[t, w]));
            }
        }
    }

    private void FitBatch(double[,] X, int n, int k)
    {
        // Batch variational inference
        var lambda = new double[k, _nVocab];

        // Initialize lambda
        for (int t = 0; t < k; t++)
        {
            for (int w = 0; w < _nVocab; w++)
            {
                lambda[t, w] = _topicWordPrior + (double)n / k;
            }
        }

        for (int iter = 0; iter < _maxIter; iter++)
        {
            var oldLambda = (double[,])lambda.Clone();

            // Reset lambda to prior
            for (int t = 0; t < k; t++)
            {
                for (int w = 0; w < _nVocab; w++)
                {
                    lambda[t, w] = _topicWordPrior;
                }
            }

            // E-step: Update document-topic distributions and accumulate
            for (int d = 0; d < n; d++)
            {
                var (gamma, phi) = UpdateDocumentParameters(X, d, lambda, k);

                // Accumulate sufficient statistics
                for (int w = 0; w < _nVocab; w++)
                {
                    if (X[d, w] > 0)
                    {
                        for (int t = 0; t < k; t++)
                        {
                            lambda[t, w] += X[d, w] * phi[w, t];
                        }
                    }
                }
            }

            // Check convergence
            double maxChange = 0;
            for (int t = 0; t < k; t++)
            {
                for (int w = 0; w < _nVocab; w++)
                {
                    double change = Math.Abs(lambda[t, w] - oldLambda[t, w]);
                    maxChange = Math.Max(maxChange, change);
                }
            }

            if (maxChange < _tol)
            {
                break;
            }
        }

        // Copy final lambda to components and normalize
        for (int t = 0; t < k; t++)
        {
            double sum = 0;
            for (int w = 0; w < _nVocab; w++)
            {
                sum += lambda[t, w];
            }
            for (int w = 0; w < _nVocab; w++)
            {
                _components![t, w] = lambda[t, w] / sum;
            }
        }
    }

    private void FitOnline(double[,] X, int n, int k, Random random)
    {
        // Online variational inference (stochastic)
        var lambda = new double[k, _nVocab];

        // Initialize lambda
        for (int t = 0; t < k; t++)
        {
            for (int w = 0; w < _nVocab; w++)
            {
                lambda[t, w] = random.NextDouble() + _topicWordPrior;
            }
        }

        int updateCount = 0;
        double tau0 = 64.0;
        double kappa = 0.7;

        for (int iter = 0; iter < _maxIter; iter++)
        {
            // Shuffle document indices
            var indices = Enumerable.Range(0, n).OrderBy(x => random.Next()).ToArray();

            for (int batchStart = 0; batchStart < n; batchStart += _batchSize)
            {
                int batchEnd = Math.Min(batchStart + _batchSize, n);
                int batchN = batchEnd - batchStart;

                // Compute sufficient statistics from batch
                var batchSStats = new double[k, _nVocab];

                for (int b = 0; b < batchN; b++)
                {
                    int d = indices[batchStart + b];
                    var (gamma, phi) = UpdateDocumentParameters(X, d, lambda, k);

                    for (int w = 0; w < _nVocab; w++)
                    {
                        if (X[d, w] > 0)
                        {
                            for (int t = 0; t < k; t++)
                            {
                                batchSStats[t, w] += X[d, w] * phi[w, t];
                            }
                        }
                    }
                }

                // Update lambda using stochastic gradient
                updateCount++;
                double rho = Math.Pow(tau0 + updateCount, -kappa);

                for (int t = 0; t < k; t++)
                {
                    for (int w = 0; w < _nVocab; w++)
                    {
                        double expectedStat = _topicWordPrior + (double)n / batchN * batchSStats[t, w];
                        lambda[t, w] = (1 - rho) * lambda[t, w] + rho * expectedStat;
                    }
                }
            }
        }

        // Copy final lambda to components and normalize
        for (int t = 0; t < k; t++)
        {
            double sum = 0;
            for (int w = 0; w < _nVocab; w++)
            {
                sum += lambda[t, w];
            }
            for (int w = 0; w < _nVocab; w++)
            {
                _components![t, w] = lambda[t, w] / sum;
            }
        }
    }

    private (double[] Gamma, double[,] Phi) UpdateDocumentParameters(
        double[,] X, int d, double[,] lambda, int k)
    {
        // Initialize gamma (document-topic distribution)
        var gamma = new double[k];
        for (int t = 0; t < k; t++)
        {
            gamma[t] = _docTopicPrior + (double)_nVocab / k;
        }

        // Initialize phi (word-topic assignments)
        var phi = new double[_nVocab, k];

        // Compute digamma(lambda) sum for each topic
        var digammaLambdaSum = new double[k];
        for (int t = 0; t < k; t++)
        {
            double sum = 0;
            for (int w = 0; w < _nVocab; w++)
            {
                sum += lambda[t, w];
            }
            digammaLambdaSum[t] = Digamma(sum);
        }

        // Iterative update
        for (int innerIter = 0; innerIter < 20; innerIter++)
        {
            var oldGamma = (double[])gamma.Clone();

            // Compute digamma sum before reset (using previous iteration's gamma values)
            double digammaGammaSum = Digamma(oldGamma.Sum());

            // Reset gamma
            for (int t = 0; t < k; t++)
            {
                gamma[t] = _docTopicPrior;
            }

            // Update phi and gamma

            for (int w = 0; w < _nVocab; w++)
            {
                if (X[d, w] <= 0) continue;

                // Update phi[w, :]
                double maxLogPhi = double.NegativeInfinity;
                for (int t = 0; t < k; t++)
                {
                    double logPhi = Digamma(gamma[t]) - digammaGammaSum
                                  + Digamma(lambda[t, w]) - digammaLambdaSum[t];
                    phi[w, t] = logPhi;
                    maxLogPhi = Math.Max(maxLogPhi, logPhi);
                }

                // Normalize phi[w, :] (log-sum-exp)
                double sum = 0;
                for (int t = 0; t < k; t++)
                {
                    phi[w, t] = Math.Exp(phi[w, t] - maxLogPhi);
                    sum += phi[w, t];
                }
                for (int t = 0; t < k; t++)
                {
                    phi[w, t] /= sum;
                    gamma[t] += X[d, w] * phi[w, t];
                }
            }

            // Check convergence
            double maxChange = 0;
            for (int t = 0; t < k; t++)
            {
                maxChange = Math.Max(maxChange, Math.Abs(gamma[t] - oldGamma[t]));
            }
            if (maxChange < 1e-4)
            {
                break;
            }
        }

        return (gamma, phi);
    }

    private static double Digamma(double x)
    {
        // Approximation of the digamma function
        if (x <= 0)
        {
            return double.NegativeInfinity;
        }

        // For small x, use recurrence relation
        double result = 0;
        while (x < 6)
        {
            result -= 1.0 / x;
            x += 1;
        }

        // Asymptotic expansion for large x
        double x2 = 1.0 / (x * x);
        return result + Math.Log(x) - 0.5 / x
               - x2 * (1.0 / 12.0 - x2 * (1.0 / 120.0 - x2 / 252.0));
    }

    /// <summary>
    /// Transforms documents to topic distributions.
    /// </summary>
    protected override Matrix<T> TransformCore(Matrix<T> data)
    {
        if (_components is null)
        {
            throw new InvalidOperationException("LatentDirichletAllocation has not been fitted.");
        }

        int n = data.Rows;
        int k = _nComponents;

        // Create lambda from components for inference
        var lambda = new double[k, _nVocab];
        for (int t = 0; t < k; t++)
        {
            double sum = 0;
            for (int w = 0; w < _nVocab; w++)
            {
                // Convert back to unnormalized form
                lambda[t, w] = _components[t, w] * _nVocab + _topicWordPrior;
                sum += lambda[t, w];
            }
        }

        // Convert input to double
        var X = new double[n, _nVocab];
        for (int i = 0; i < n; i++)
        {
            for (int j = 0; j < _nVocab; j++)
            {
                X[i, j] = NumOps.ToDouble(data[i, j]);
            }
        }

        var result = new T[n, k];

        for (int d = 0; d < n; d++)
        {
            var (gamma, _) = UpdateDocumentParameters(X, d, lambda, k);

            // Normalize gamma to get topic distribution
            double gammaSum = gamma.Sum();
            for (int t = 0; t < k; t++)
            {
                result[d, t] = NumOps.FromDouble(gamma[t] / gammaSum);
            }
        }

        return new Matrix<T>(result);
    }

    /// <summary>
    /// Inverse transformation is not supported.
    /// </summary>
    protected override Matrix<T> InverseTransformCore(Matrix<T> data)
    {
        throw new NotSupportedException("LatentDirichletAllocation does not support inverse transformation.");
    }

    /// <summary>
    /// Gets the output feature names after transformation.
    /// </summary>
    public override string[] GetFeatureNamesOut(string[]? inputFeatureNames = null)
    {
        var names = new string[_nComponents];
        for (int i = 0; i < _nComponents; i++)
        {
            names[i] = $"Topic{i + 1}";
        }
        return names;
    }

    /// <summary>
    /// Gets the top words for each topic.
    /// </summary>
    /// <param name="nTopWords">Number of top words to return per topic.</param>
    /// <param name="featureNames">Optional vocabulary names.</param>
    /// <returns>Array of arrays containing top word indices (or names if provided) for each topic.</returns>
    public string[][] GetTopWordsPerTopic(int nTopWords = 10, string[]? featureNames = null)
    {
        if (_components is null)
        {
            throw new InvalidOperationException("LatentDirichletAllocation has not been fitted.");
        }

        int k = _nComponents;
        var result = new string[k][];

        for (int t = 0; t < k; t++)
        {
            var topIndices = Enumerable.Range(0, _nVocab)
                .OrderByDescending(w => _components[t, w])
                .Take(nTopWords)
                .ToArray();

            if (featureNames is not null)
            {
                result[t] = topIndices.Select(i => i < featureNames.Length ? featureNames[i] : $"word{i}").ToArray();
            }
            else
            {
                result[t] = topIndices.Select(i => $"word{i}").ToArray();
            }
        }

        return result;
    }
}

/// <summary>
/// Specifies the learning method for LDA.
/// </summary>
public enum LdaLearningMethod
{
    /// <summary>
    /// Batch variational Bayes. Uses all data at each iteration.
    /// More accurate but slower for large datasets.
    /// </summary>
    Batch,

    /// <summary>
    /// Online variational Bayes. Uses mini-batches.
    /// Faster and more scalable for large datasets.
    /// </summary>
    Online
}
