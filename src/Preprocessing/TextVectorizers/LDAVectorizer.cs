using AiDotNet.Interfaces;
using AiDotNet.LinearAlgebra;
using AiDotNet.Tokenization.Interfaces;

namespace AiDotNet.Preprocessing.TextVectorizers;

/// <summary>
/// Converts text documents to topic distribution vectors using Latent Dirichlet Allocation (LDA).
/// </summary>
/// <remarks>
/// <para>
/// LDA is a generative probabilistic model that discovers latent topics in a corpus.
/// Each document is represented as a mixture of topics, and each topic is characterized
/// by a distribution over words.
/// </para>
/// <para>
/// The algorithm assumes:
/// - Each document is generated by first choosing a topic distribution
/// - Then for each word, choosing a topic from that distribution
/// - Finally choosing a word from that topic's word distribution
/// </para>
/// <para><b>For Beginners:</b> LDA discovers the "themes" in your documents:
/// - A news corpus might have topics: "sports", "politics", "technology"
/// - Each document gets a probability for each topic (e.g., 70% sports, 20% politics, 10% tech)
/// - Great for topic modeling, document clustering, and content recommendation
/// - Unlike LSA, LDA topics are interpretable probability distributions
/// </para>
/// </remarks>
/// <typeparam name="T">The numeric type for calculations (e.g., float, double).</typeparam>
public class LDAVectorizer<T> : TextVectorizerBase<T>
{
    private readonly int _nTopics;
    private readonly int _maxIterations;
    private readonly double _alpha;
    private readonly double _beta;
    private readonly double _tolerance;
    private readonly int? _randomState;

    private CountVectorizer<T>? _countVectorizer;
    private double[,]? _topicWordDistribution; // phi: (n_topics, n_words)
    private double[,]? _documentTopicDistribution; // theta: (n_docs, n_topics)

    /// <summary>
    /// Gets the number of topics.
    /// </summary>
    public int NTopics => _nTopics;

    /// <summary>
    /// Gets the topic-word distribution matrix (phi).
    /// Each row is a topic, each column is a word's probability in that topic.
    /// </summary>
    public double[,]? TopicWordDistribution => _topicWordDistribution;

    /// <summary>
    /// Creates a new instance of <see cref="LDAVectorizer{T}"/>.
    /// </summary>
    /// <param name="nTopics">Number of topics to discover. Defaults to 10.</param>
    /// <param name="maxIterations">Maximum number of Gibbs sampling iterations. Defaults to 100.</param>
    /// <param name="alpha">Document-topic prior (higher = documents have more topics). Defaults to 0.1.</param>
    /// <param name="beta">Topic-word prior (higher = topics have more words). Defaults to 0.01.</param>
    /// <param name="tolerance">Convergence tolerance. Defaults to 1e-4.</param>
    /// <param name="minDf">Minimum document frequency (absolute count). Defaults to 2.</param>
    /// <param name="maxDf">Maximum document frequency (proportion 0-1). Defaults to 0.95.</param>
    /// <param name="maxFeatures">Maximum vocabulary size. Null for unlimited.</param>
    /// <param name="nGramRange">N-gram range (min, max). Defaults to (1, 1) for unigrams.</param>
    /// <param name="lowercase">Convert all text to lowercase. Defaults to true.</param>
    /// <param name="randomState">Random seed for reproducibility. Null for random.</param>
    /// <param name="tokenizer">Custom tokenizer function. Null for default.</param>
    /// <param name="stopWords">Words to exclude. Defaults to English stop words.</param>
    /// <param name="advancedTokenizer">Optional ITokenizer for subword tokenization.</param>
    public LDAVectorizer(
        int nTopics = 10,
        int maxIterations = 100,
        double alpha = 0.1,
        double beta = 0.01,
        double tolerance = 1e-4,
        int minDf = 2,
        double maxDf = 0.95,
        int? maxFeatures = null,
        (int Min, int Max)? nGramRange = null,
        bool lowercase = true,
        int? randomState = null,
        Func<string, IEnumerable<string>>? tokenizer = null,
        HashSet<string>? stopWords = null,
        ITokenizer? advancedTokenizer = null)
        : base(minDf, maxDf, maxFeatures, nGramRange, lowercase, tokenizer, stopWords ?? EnglishStopWords, advancedTokenizer)
    {
        if (nTopics < 1)
            throw new ArgumentException("Number of topics must be at least 1.", nameof(nTopics));
        if (maxIterations < 1)
            throw new ArgumentOutOfRangeException(nameof(maxIterations), "Max iterations must be at least 1.");
        if (alpha <= 0)
            throw new ArgumentOutOfRangeException(nameof(alpha), "Alpha must be > 0.");
        if (beta <= 0)
            throw new ArgumentOutOfRangeException(nameof(beta), "Beta must be > 0.");
        if (tolerance < 0)
            throw new ArgumentOutOfRangeException(nameof(tolerance), "Tolerance must be >= 0.");

        _nTopics = nTopics;
        _maxIterations = maxIterations;
        _alpha = alpha;
        _beta = beta;
        _tolerance = tolerance;
        _randomState = randomState;
    }

    /// <inheritdoc/>
    public override bool IsFitted => _topicWordDistribution is not null && _countVectorizer is not null;

    /// <inheritdoc/>
    public override int FeatureCount => _nTopics;

    /// <summary>
    /// Fits the LDA model to the corpus using collapsed Gibbs sampling.
    /// </summary>
    /// <param name="documents">The text documents to learn topics from.</param>
    public override void Fit(IEnumerable<string> documents)
    {
        var docList = documents.ToList();
        _nDocs = docList.Count;

        // Create count vectorizer
        _countVectorizer = new CountVectorizer<T>(
            minDf: _minDf,
            maxDf: _maxDf,
            maxFeatures: _maxFeatures,
            nGramRange: _nGramRange,
            lowercase: _lowercase,
            tokenizer: _tokenizer,
            stopWords: _stopWords,
            advancedTokenizer: _advancedTokenizer);

        var countMatrix = _countVectorizer.FitTransform(docList);
        _vocabulary = _countVectorizer.Vocabulary;
        _featureNames = Enumerable.Range(0, _nTopics).Select(i => $"topic_{i}").ToArray();

        int nDocs = countMatrix.Rows;
        int nWords = countMatrix.Columns;

        // Initialize random
        var random = _randomState.HasValue ? RandomHelper.CreateSeededRandom(_randomState.Value) : RandomHelper.CreateSecureRandom();

        // Initialize topic assignments for each word occurrence
        var wordTopics = new List<List<int>>(); // wordTopics[doc][word_position] = topic
        var docTopicCounts = new int[nDocs, _nTopics]; // n_dk
        var topicWordCounts = new int[_nTopics, nWords]; // n_kw
        var topicCounts = new int[_nTopics]; // n_k

        // Initialize randomly
        for (int d = 0; d < nDocs; d++)
        {
            var docWordTopics = new List<int>();
            for (int w = 0; w < nWords; w++)
            {
                int count = (int)NumOps.ToDouble(countMatrix[d, w]);
                for (int c = 0; c < count; c++)
                {
                    int topic = random.Next(_nTopics);
                    docWordTopics.Add(topic);
                    docWordTopics.Add(w); // Store word index too

                    docTopicCounts[d, topic]++;
                    topicWordCounts[topic, w]++;
                    topicCounts[topic]++;
                }
            }
            wordTopics.Add(docWordTopics);
        }

        // Gibbs sampling
        double prevLogLikelihood = double.NegativeInfinity;

        for (int iter = 0; iter < _maxIterations; iter++)
        {
            for (int d = 0; d < nDocs; d++)
            {
                var docWordTopics = wordTopics[d];
                for (int i = 0; i < docWordTopics.Count; i += 2)
                {
                    int oldTopic = docWordTopics[i];
                    int w = docWordTopics[i + 1];

                    // Remove current assignment
                    docTopicCounts[d, oldTopic]--;
                    topicWordCounts[oldTopic, w]--;
                    topicCounts[oldTopic]--;

                    // Compute conditional probabilities
                    var probs = new double[_nTopics];
                    double sumProb = 0;

                    for (int k = 0; k < _nTopics; k++)
                    {
                        double pTopic = (docTopicCounts[d, k] + _alpha) *
                                       (topicWordCounts[k, w] + _beta) /
                                       (topicCounts[k] + nWords * _beta);
                        probs[k] = pTopic;
                        sumProb += pTopic;
                    }

                    // Sample new topic
                    double u = random.NextDouble() * sumProb;
                    double cumulative = 0;
                    int newTopic = _nTopics - 1;

                    for (int k = 0; k < _nTopics; k++)
                    {
                        cumulative += probs[k];
                        if (u <= cumulative)
                        {
                            newTopic = k;
                            break;
                        }
                    }

                    // Update assignment
                    docWordTopics[i] = newTopic;
                    docTopicCounts[d, newTopic]++;
                    topicWordCounts[newTopic, w]++;
                    topicCounts[newTopic]++;
                }
            }

            // Check convergence every 10 iterations
            if (iter % 10 == 0)
            {
                double logLikelihood = ComputeLogLikelihood(docTopicCounts, topicWordCounts, topicCounts, nWords);
                if (Math.Abs(logLikelihood - prevLogLikelihood) < _tolerance)
                    break;
                prevLogLikelihood = logLikelihood;
            }
        }

        // Compute final distributions
        _topicWordDistribution = new double[_nTopics, nWords];
        for (int k = 0; k < _nTopics; k++)
        {
            double sum = topicCounts[k] + nWords * _beta;
            for (int w = 0; w < nWords; w++)
            {
                _topicWordDistribution[k, w] = (topicWordCounts[k, w] + _beta) / sum;
            }
        }

        _documentTopicDistribution = new double[nDocs, _nTopics];
        for (int d = 0; d < nDocs; d++)
        {
            double sum = 0;
            for (int k = 0; k < _nTopics; k++)
            {
                sum += docTopicCounts[d, k] + _alpha;
            }
            for (int k = 0; k < _nTopics; k++)
            {
                _documentTopicDistribution[d, k] = (docTopicCounts[d, k] + _alpha) / sum;
            }
        }
    }

    private double ComputeLogLikelihood(int[,] docTopicCounts, int[,] topicWordCounts, int[] topicCounts, int nWords)
    {
        double ll = 0;
        int nDocs = docTopicCounts.GetLength(0);

        for (int d = 0; d < nDocs; d++)
        {
            for (int k = 0; k < _nTopics; k++)
            {
                if (docTopicCounts[d, k] > 0)
                {
                    ll += docTopicCounts[d, k] * Math.Log((docTopicCounts[d, k] + _alpha) / (_nTopics * _alpha));
                }
            }
        }

        return ll;
    }

    /// <summary>
    /// Transforms documents to topic distribution vectors.
    /// </summary>
    /// <param name="documents">The documents to transform.</param>
    /// <returns>Matrix where each row is a document's topic probability distribution.</returns>
    public override Matrix<T> Transform(IEnumerable<string> documents)
    {
        if (_topicWordDistribution is null || _countVectorizer is null)
        {
            throw new InvalidOperationException("LDAVectorizer has not been fitted. Call Fit() or FitTransform() first.");
        }

        var docList = documents.ToList();
        var countMatrix = _countVectorizer.Transform(docList);

        int nDocs = countMatrix.Rows;
        int nWords = countMatrix.Columns;

        // Use variational inference for new documents
        var result = new double[nDocs, _nTopics];
        var random = _randomState.HasValue ? RandomHelper.CreateSeededRandom(_randomState.Value) : RandomHelper.CreateSecureRandom();

        for (int d = 0; d < nDocs; d++)
        {
            // Initialize topic proportions uniformly
            var gamma = new double[_nTopics];
            for (int k = 0; k < _nTopics; k++)
            {
                gamma[k] = _alpha + (double)nWords / _nTopics;
            }

            // Iterate to convergence
            for (int iter = 0; iter < 20; iter++)
            {
                var newGamma = new double[_nTopics];
                for (int k = 0; k < _nTopics; k++)
                {
                    newGamma[k] = _alpha;
                }

                for (int w = 0; w < nWords; w++)
                {
                    double count = NumOps.ToDouble(countMatrix[d, w]);
                    if (count == 0) continue;

                    // Compute phi (topic assignment probabilities for this word)
                    var phi = new double[_nTopics];
                    double sumPhi = 0;

                    for (int k = 0; k < _nTopics; k++)
                    {
                        phi[k] = _topicWordDistribution[k, w] * Math.Exp(Digamma(gamma[k]));
                        sumPhi += phi[k];
                    }

                    // Normalize and update gamma
                    for (int k = 0; k < _nTopics; k++)
                    {
                        phi[k] /= sumPhi;
                        newGamma[k] += count * phi[k];
                    }
                }

                gamma = newGamma;
            }

            // Normalize gamma to get topic proportions
            double sumGamma = gamma.Sum();
            for (int k = 0; k < _nTopics; k++)
            {
                result[d, k] = gamma[k] / sumGamma;
            }
        }

        // Convert to output type
        var output = new T[nDocs, _nTopics];
        for (int i = 0; i < nDocs; i++)
        {
            for (int j = 0; j < _nTopics; j++)
            {
                output[i, j] = NumOps.FromDouble(result[i, j]);
            }
        }

        return new Matrix<T>(output);
    }

    /// <summary>
    /// Digamma function approximation.
    /// </summary>
    private static double Digamma(double x)
    {
        if (x <= 0) return double.NegativeInfinity;

        double result = 0;
        while (x < 6)
        {
            result -= 1 / x;
            x += 1;
        }

        result += Math.Log(x) - 1 / (2 * x);
        double x2 = 1 / (x * x);
        result -= x2 * (1.0 / 12 - x2 * (1.0 / 120 - x2 / 252));

        return result;
    }

    /// <summary>
    /// Gets the top words for each topic.
    /// </summary>
    /// <param name="nWords">Number of top words to return per topic.</param>
    /// <returns>Array of arrays, where each inner array contains the top words for a topic.</returns>
    public string[][] GetTopWordsPerTopic(int nWords = 10)
    {
        if (_topicWordDistribution is null || _countVectorizer?.FeatureNames is null)
        {
            throw new InvalidOperationException("LDAVectorizer has not been fitted.");
        }

        var vocabArray = _countVectorizer.FeatureNames;
        var result = new string[_nTopics][];

        for (int k = 0; k < _nTopics; k++)
        {
            var wordProbs = new List<(int Index, double Prob)>();
            for (int w = 0; w < vocabArray.Length; w++)
            {
                wordProbs.Add((w, _topicWordDistribution[k, w]));
            }

            result[k] = wordProbs
                .OrderByDescending(wp => wp.Prob)
                .Take(nWords)
                .Select(wp => vocabArray[wp.Index])
                .ToArray();
        }

        return result;
    }
}
