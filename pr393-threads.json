{"data":{"repository":{"pullRequest":{"reviewThreads":{"nodes":[{"id":"PRRT_kwDOKSXUF85g9VqR","isResolved":true,"isOutdated":true,"path":"scripts/launch-distributed-training.ps1","line":null,"originalLine":106,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VG0XT","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**Stop splitting user-supplied ProgramArgs on raw spaces**\n\nCurrently we treat `-ProgramArgs \"--epochs 100 --config \\\"My Path.json\\\"\"` as a single string and call `.Split(\" \")`, which strips quotes and mis-tokenizes any value containing spaces. The launched training process then receives mangled arguments. Please capture the remaining command-line tokens as an array instead of manual splitting, and append them directly to the mpiexec argument list.\n\n```diff\n-    [Parameter(Mandatory=$false, HelpMessage=\"Additional arguments to pass to your program\")]\n-    [string]$ProgramArgs = \"\",\n+    [Parameter(\n+        Mandatory = $false,\n+        HelpMessage = \"Additional arguments to pass to your program\",\n+        ValueFromRemainingArguments = $true)]\n+    [string[]]$ProgramArgs = @(),\n ...\n-if ($ProgramArgs) {\n-    # Split program args and add them\n-    $mpiArgsList += $ProgramArgs.Split(\" \")\n-}\n+if ($ProgramArgs.Count -gt 0) {\n+    $mpiArgsList += $ProgramArgs\n+}\n```\nRemember to align the usage examples in the header comments with the new calling pattern (e.g., pass extra args after the named parameters or as a string array).\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn scripts/launch-distributed-training.ps1 around lines 32-106, ProgramArgs is\nbeing treated as a single string and then .Split(\" \") is used which breaks\nquoted values and paths with spaces; change the Parameter definition to accept\nan array of strings (e.g. [string[]] $ProgramArgs or use\nValueFromRemainingArguments=$true) so PowerShell preserves tokenization, then\nappend the ProgramArgs array directly to $mpiArgsList (remove the .Split call).\nAlso update the header usage examples to show passing extra args as separate\ntokens or using a string array form.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits 380cb58 to e206a9b","createdAt":"2025-11-07T03:53:13Z","path":"scripts/launch-distributed-training.ps1","line":null,"originalLine":106,"diffHunk":"@@ -0,0 +1,147 @@\n+################################################################################\n+# AiDotNet Distributed Training Launcher (PowerShell)\n+#\n+# This script launches distributed training using MPI across multiple processes.\n+#\n+# For Beginners:\n+# MPI (Message Passing Interface) is a standard for running programs across\n+# multiple computers or processors. Think of it like a coordinator that starts\n+# your program on multiple machines at once and helps them communicate.\n+#\n+# Usage:\n+#   .\\launch-distributed-training.ps1 -NumProcesses <num> -Program <path> [-ProgramArgs <args>]\n+#\n+# Examples:\n+#   # Run on 4 GPUs locally\n+#   .\\launch-distributed-training.ps1 -NumProcesses 4 -Program \".\\MyTrainingApp.exe\"\n+#\n+#   # Run on 8 GPUs with additional arguments\n+#   .\\launch-distributed-training.ps1 -NumProcesses 8 -Program \".\\MyTrainingApp.exe\" -ProgramArgs \"--epochs 100 --lr 0.001\"\n+#\n+#   # Run across 2 machines with 4 GPUs each\n+#   .\\launch-distributed-training.ps1 -NumProcesses 8 -Program \".\\MyTrainingApp.exe\" -Hosts \"machine1,machine2\"\n+################################################################################\n+\n+param(\n+    [Parameter(Mandatory=$true, HelpMessage=\"Number of processes to spawn (typically equals number of GPUs)\")]\n+    [int]$NumProcesses,\n+\n+    [Parameter(Mandatory=$true, HelpMessage=\"Path to your training program executable\")]\n+    [string]$Program,\n+\n+    [Parameter(Mandatory=$false, HelpMessage=\"Additional arguments to pass to your program\")]\n+    [string]$ProgramArgs = \"\",\n+\n+    [Parameter(Mandatory=$false, HelpMessage=\"Comma-separated list of host machines\")]\n+    [string]$Hosts = \"\"\n+)\n+\n+# Display header\n+Write-Host \"======================================\" -ForegroundColor Cyan\n+Write-Host \"AiDotNet Distributed Training Launcher\" -ForegroundColor Cyan\n+Write-Host \"======================================\" -ForegroundColor Cyan\n+Write-Host \"\"\n+\n+# Display configuration\n+Write-Host \"Configuration:\" -ForegroundColor Yellow\n+Write-Host \"  Number of processes: $NumProcesses\"\n+Write-Host \"  Program: $Program\"\n+if ($ProgramArgs) {\n+    Write-Host \"  Program arguments: $ProgramArgs\"\n+}\n+if ($Hosts) {\n+    Write-Host \"  Hosts: $Hosts\"\n+}\n+Write-Host \"\"\n+\n+# Check if mpiexec is available\n+$mpiexec = Get-Command mpiexec -ErrorAction SilentlyContinue\n+\n+if (-not $mpiexec) {\n+    Write-Host \"Error: mpiexec not found in PATH\" -ForegroundColor Red\n+    Write-Host \"\"\n+    Write-Host \"For Beginners:\" -ForegroundColor Yellow\n+    Write-Host \"  You need to install Microsoft MPI to run distributed training on Windows.\"\n+    Write-Host \"  Download from: https://docs.microsoft.com/en-us/message-passing-interface/microsoft-mpi\"\n+    Write-Host \"\"\n+    Write-Host \"  Installation steps:\"\n+    Write-Host \"    1. Download MS-MPI installer\"\n+    Write-Host \"    2. Install both the runtime (msmpisetup.exe) and SDK (msmpisdk.msi)\"\n+    Write-Host \"    3. Restart your terminal/PowerShell\"\n+    exit 1\n+}\n+\n+Write-Host \"Using MPI command: $($mpiexec.Source)\" -ForegroundColor Green\n+Write-Host \"\"\n+\n+# Check if program exists\n+if (-not (Test-Path $Program)) {\n+    Write-Host \"Error: Program '$Program' not found\" -ForegroundColor Red\n+    Write-Host \"\"\n+    Write-Host \"For Beginners:\" -ForegroundColor Yellow\n+    Write-Host \"  Make sure you've built your training program and the path is correct.\"\n+    Write-Host \"  Example: dotnet publish -c Release -o .\\publish\"\n+    Write-Host \"  Then use: -Program '.\\publish\\MyTrainingApp.exe'\"\n+    exit 1\n+}\n+\n+# Build mpiexec command\n+$mpiCommand = \"mpiexec\"\n+$mpiArgsList = @(\n+    \"-n\", $NumProcesses.ToString()\n+)\n+\n+# Add hosts if specified\n+if ($Hosts) {\n+    $mpiArgsList += @(\"-hosts\", $Hosts)\n+}\n+\n+# Add the program\n+$mpiArgsList += $Program\n+\n+# Add program arguments if specified\n+if ($ProgramArgs) {\n+    # Split program args and add them\n+    $mpiArgsList += $ProgramArgs.Split(\" \")\n+}"}]}},{"id":"PRRT_kwDOKSXUF85g9VqU","isResolved":true,"isOutdated":true,"path":"scripts/launch-distributed-training.sh","line":136,"originalLine":107,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VG0Xd","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**Preserve user argument quoting when invoking mpiexec**\n\nStoring `\"$@\"` in a scalar and expanding `$PROGRAM_ARGS`/`$PROGRAM` unquoted means any argument with spaces or shell metacharacters is re-tokenized incorrectly when we invoke mpiexec. That breaks otherwise valid launch commands (e.g., config files under paths with spaces). Please keep the extra args in an array and quote each component when executing. Same fix also hardens the program path handling.\n\n```diff\n-PROGRAM_ARGS=\"$@\"\n+PROGRAM_ARGS=(\"$@\")\n ...\n-echo \"  Program arguments: $PROGRAM_ARGS\"\n+if [ \"${#PROGRAM_ARGS[@]}\" -gt 0 ]; then\n+    echo \"  Program arguments: ${PROGRAM_ARGS[*]}\"\n+else\n+    echo \"  Program arguments: (none)\"\n+fi\n ...\n-echo \"Command: $MPI_CMD -n $NUM_PROCESSES $PROGRAM $PROGRAM_ARGS\"\n+echo \"Command: $MPI_CMD -n $NUM_PROCESSES $PROGRAM ${PROGRAM_ARGS[*]}\"\n ...\n-$MPI_CMD -n $NUM_PROCESSES $PROGRAM $PROGRAM_ARGS\n+\"$MPI_CMD\" -n \"$NUM_PROCESSES\" \"$PROGRAM\" \"${PROGRAM_ARGS[@]}\"\n```\n\n<details>\n<summary>üß∞ Tools</summary>\n\n<details>\n<summary>ü™õ Shellcheck (0.11.0)</summary>\n\n[warning] 48-48: Assigning an array to a string! Assign as array, or use * instead of @ to concatenate.\n\n(SC2124)\n\n</details>\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits 380cb58 to e206a9b","createdAt":"2025-11-07T03:53:13Z","path":"scripts/launch-distributed-training.sh","line":136,"originalLine":107,"diffHunk":"@@ -0,0 +1,126 @@\n+#!/bin/bash\n+\n+################################################################################\n+# AiDotNet Distributed Training Launcher (Bash)\n+#\n+# This script launches distributed training using MPI across multiple processes.\n+#\n+# For Beginners:\n+# MPI (Message Passing Interface) is a standard for running programs across\n+# multiple computers or processors. Think of it like a coordinator that starts\n+# your program on multiple machines at once and helps them communicate.\n+#\n+# Usage:\n+#   ./launch-distributed-training.sh <num_processes> <program> [args...]\n+#\n+# Examples:\n+#   # Run on 4 GPUs locally\n+#   ./launch-distributed-training.sh 4 ./MyTrainingApp\n+#\n+#   # Run on 8 GPUs with additional arguments\n+#   ./launch-distributed-training.sh 8 ./MyTrainingApp --epochs 100 --lr 0.001\n+#\n+#   # Run across 2 machines with 4 GPUs each\n+#   ./launch-distributed-training.sh 8 ./MyTrainingApp --hosts machine1,machine2\n+################################################################################\n+\n+# Check if enough arguments provided\n+if [ \"$#\" -lt 2 ]; then\n+    echo \"Error: Insufficient arguments\"\n+    echo \"\"\n+    echo \"Usage: $0 <num_processes> <program> [args...]\"\n+    echo \"\"\n+    echo \"Arguments:\"\n+    echo \"  num_processes  - Number of processes to spawn (typically equals number of GPUs)\"\n+    echo \"  program        - Path to your training program executable\"\n+    echo \"  args           - Any additional arguments to pass to your program\"\n+    echo \"\"\n+    echo \"Examples:\"\n+    echo \"  $0 4 ./MyTrainingApp\"\n+    echo \"  $0 8 ./MyTrainingApp --epochs 100\"\n+    exit 1\n+fi\n+\n+# Parse arguments\n+NUM_PROCESSES=$1\n+PROGRAM=$2\n+shift 2\n+PROGRAM_ARGS=\"$@\"\n+\n+echo \"======================================\"\n+echo \"AiDotNet Distributed Training Launcher\"\n+echo \"======================================\"\n+echo \"\"\n+echo \"Configuration:\"\n+echo \"  Number of processes: $NUM_PROCESSES\"\n+echo \"  Program: $PROGRAM\"\n+echo \"  Program arguments: $PROGRAM_ARGS\"\n+echo \"\"\n+\n+# Check if mpiexec/mpirun is available\n+if command -v mpiexec &> /dev/null; then\n+    MPI_CMD=\"mpiexec\"\n+elif command -v mpirun &> /dev/null; then\n+    MPI_CMD=\"mpirun\"\n+else\n+    echo \"Error: Neither mpiexec nor mpirun found in PATH\"\n+    echo \"\"\n+    echo \"For Beginners:\"\n+    echo \"  You need to install MPI to run distributed training.\"\n+    echo \"  On Ubuntu/Debian: sudo apt-get install mpich\"\n+    echo \"  On macOS: brew install mpich\"\n+    echo \"  On Windows: Install Microsoft MPI from https://docs.microsoft.com/en-us/message-passing-interface/microsoft-mpi\"\n+    exit 1\n+fi\n+\n+echo \"Using MPI command: $MPI_CMD\"\n+echo \"\"\n+\n+# Check if program exists\n+if [ ! -f \"$PROGRAM\" ]; then\n+    echo \"Error: Program '$PROGRAM' not found\"\n+    echo \"\"\n+    echo \"For Beginners:\"\n+    echo \"  Make sure you've built your training program and the path is correct.\"\n+    echo \"  Example: dotnet publish -c Release -o ./publish\"\n+    echo \"  Then use: $0 4 ./publish/MyTrainingApp\"\n+    exit 1\n+fi\n+\n+# Make program executable if it isn't\n+if [ ! -x \"$PROGRAM\" ]; then\n+    echo \"Warning: Program is not executable. Making it executable...\"\n+    chmod +x \"$PROGRAM\"\n+fi\n+\n+# Launch distributed training\n+echo \"Launching distributed training...\"\n+echo \"Command: $MPI_CMD -n $NUM_PROCESSES $PROGRAM $PROGRAM_ARGS\"\n+echo \"\"\n+echo \"======================================\"\n+echo \"\"\n+\n+# Execute MPI command\n+# -n: Number of processes\n+# The program and its arguments follow\n+$MPI_CMD -n $NUM_PROCESSES $PROGRAM $PROGRAM_ARGS\n+"}]}},{"id":"PRRT_kwDOKSXUF85g9VqX","isResolved":true,"isOutdated":true,"path":"src/DistributedTraining/InMemoryCommunicationBackend.cs","line":null,"originalLine":140,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VG0Xm","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Barrier logic deadlocks**  \nEach rank builds `barrierId` from `DateTime.UtcNow.Ticks`, so every caller waits on a different key. With `worldSize > 1`, `_barrierCounters[barrierId]` never reaches `WorldSize`, threads spin on `Monitor.Wait`, and the program hangs. Use a shared identifier (for example, store the current barrier generation in a static dictionary and let the first entrant create it, while everyone else reuses it) so all ranks synchronize on the same entry before cleaning it up.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/DistributedTraining/InMemoryCommunicationBackend.cs around lines 112-140,\nthe Barrier method builds a unique barrierId from DateTime.UtcNow.Ticks so each\ncaller uses a different key and never reaches worldSize, causing deadlock;\nchange the logic to use a shared barrier identifier (e.g., a class-level integer\n_barrierGeneration combined with a fixed prefix) that is created/seen by all\nranks under _globalLock, have the first entrant create the counter entry and\nothers reuse it, increment the counter for that shared key, wait in the standard\nMonitor.Wait loop until the counter == _worldSize, PulseAll when ready, and have\nrank 0 perform cleanup (remove the key and increment _barrierGeneration) so\nsubsequent barriers reuse a new shared id; ensure all accesses and\ncreation/removal are done while holding _globalLock to avoid races.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits 380cb58 to e206a9b","createdAt":"2025-11-07T03:53:13Z","path":"src/DistributedTraining/InMemoryCommunicationBackend.cs","line":null,"originalLine":140,"diffHunk":"@@ -0,0 +1,484 @@\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.NumericOperations;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// A simple in-memory implementation of distributed communication for testing and single-machine scenarios.\n+/// This backend simulates multiple processes by using shared memory and locks.\n+///\n+/// For Beginners:\n+/// This is a \"fake\" distributed system that runs on a single machine.\n+/// It's perfect for testing your distributed code without needing multiple GPUs or machines.\n+/// Think of it as a practice mode - it simulates distributed behavior but everything\n+/// runs in one process.\n+///\n+/// Example:\n+/// <code>\n+/// // Create a simulated distributed environment with 4 \"processes\"\n+/// var backend = new InMemoryCommunicationBackend&lt;double&gt;(rank: 0, worldSize: 4);\n+/// CommunicationManager.Initialize(backend);\n+/// </code>\n+///\n+/// Note: For production distributed training, you would use an MPI-based backend instead.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type for operations</typeparam>\n+public class InMemoryCommunicationBackend<T> : ICommunicationBackend<T> where T : struct\n+{\n+    private readonly int _rank;\n+    private readonly int _worldSize;\n+    private readonly INumericOperations<T> _numOps;\n+    private bool _isInitialized;\n+\n+    // Shared state for simulating collective operations\n+    // In a real implementation, this would be handled by the MPI backend\n+    private static readonly object _globalLock = new object();\n+    private static readonly Dictionary<string, List<Vector<T>>> _sharedBuffers = new();\n+    private static readonly Dictionary<string, int> _barrierCounters = new();\n+\n+    /// <inheritdoc/>\n+    public int Rank => _rank;\n+\n+    /// <inheritdoc/>\n+    public int WorldSize => _worldSize;\n+\n+    /// <inheritdoc/>\n+    public bool IsInitialized => _isInitialized;\n+\n+    /// <summary>\n+    /// Creates a new in-memory communication backend.\n+    ///\n+    /// For Beginners:\n+    /// You create one of these for each simulated \"process\". If you want to simulate\n+    /// 4 GPUs, you create 4 instances with ranks 0, 1, 2, 3, all with worldSize=4.\n+    /// </summary>\n+    /// <param name=\"rank\">The rank (ID) of this simulated process (0-based)</param>\n+    /// <param name=\"worldSize\">The total number of simulated processes</param>\n+    /// <exception cref=\"ArgumentException\">Thrown if rank or worldSize are invalid</exception>\n+    public InMemoryCommunicationBackend(int rank, int worldSize)\n+    {\n+        if (rank < 0 || rank >= worldSize)\n+        {\n+            throw new ArgumentException(\n+                $\"Invalid rank {rank}. Must be between 0 and {worldSize - 1}.\");\n+        }\n+\n+        if (worldSize <= 0)\n+        {\n+            throw new ArgumentException(\n+                $\"Invalid worldSize {worldSize}. Must be positive.\");\n+        }\n+\n+        _rank = rank;\n+        _worldSize = worldSize;\n+        _numOps = MathHelper.GetNumericOperations<T>();\n+        _isInitialized = false;\n+    }\n+\n+    /// <inheritdoc/>\n+    public void Initialize()\n+    {\n+        lock (_globalLock)\n+        {\n+            if (_isInitialized)\n+            {\n+                return;\n+            }\n+\n+            _isInitialized = true;\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public void Shutdown()\n+    {\n+        lock (_globalLock)\n+        {\n+            if (!_isInitialized)\n+            {\n+                return;\n+            }\n+\n+            // Clear any remaining shared state\n+            _sharedBuffers.Clear();\n+            _barrierCounters.Clear();\n+\n+            _isInitialized = false;\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public void Barrier()\n+    {\n+        EnsureInitialized();\n+\n+        lock (_globalLock)\n+        {\n+            string barrierId = $\"barrier_{DateTime.UtcNow.Ticks}\";\n+\n+            if (!_barrierCounters.ContainsKey(barrierId))\n+            {\n+                _barrierCounters[barrierId] = 0;\n+            }\n+\n+            _barrierCounters[barrierId]++;\n+\n+            // Wait until all processes have reached the barrier\n+            while (_barrierCounters[barrierId] < _worldSize)\n+            {\n+                Monitor.Wait(_globalLock, 10);\n+            }\n+\n+            Monitor.PulseAll(_globalLock);\n+\n+            // Cleanup\n+            if (_rank == 0)\n+            {\n+                _barrierCounters.Remove(barrierId);\n+            }\n+        }"}]}},{"id":"PRRT_kwDOKSXUF85g9VqZ","isResolved":true,"isOutdated":true,"path":"src/DistributedTraining/InMemoryCommunicationBackend.cs","line":null,"originalLine":383,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VG0Xp","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Collective buffers never line up across ranks**  \nAllReduce/AllGather/Broadcast/Scatter each create a fresh `Guid.NewGuid()` buffer ID per call. Because every rank generates its own GUID, they push data into different dictionary slots, the participant counts never reach `WorldSize`, and every collective call blocks forever. Please generate a shared operation ID (e.g., track an operation counter in shared state and let the first entrant publish the ID for others to reuse) and reuse it for the duration of the collective, cleaning up once rank 0 finishes. Apply the same fix to every collective to avoid deadlocks.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/DistributedTraining/InMemoryCommunicationBackend.cs around lines 163 to\n383, each collective currently creates a new bufferId with Guid.NewGuid()\ncausing each rank to use a different key and deadlock; replace per-call GUIDs\nwith a shared operation id mechanism in the instance shared state (e.g., a\nprivate integer operationCounter and a Dictionary<int,string> or\nDictionary<string,List<Vector<T>>> where the first entrant increments\noperationCounter, publishes that id (or derived bufferId) under the global lock,\nand all other ranks reuse that same id for the duration of the collective),\nensure wait/notify logic uses that shared id to coordinate contributions, and\nhave rank 0 remove the shared buffer when cleaning up; apply the same change to\nAllReduce, AllGather, Broadcast, and Scatter to ensure all ranks target the same\nbuffer key.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits 380cb58 to e206a9b","createdAt":"2025-11-07T03:53:13Z","path":"src/DistributedTraining/InMemoryCommunicationBackend.cs","line":null,"originalLine":383,"diffHunk":"@@ -0,0 +1,484 @@\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.NumericOperations;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// A simple in-memory implementation of distributed communication for testing and single-machine scenarios.\n+/// This backend simulates multiple processes by using shared memory and locks.\n+///\n+/// For Beginners:\n+/// This is a \"fake\" distributed system that runs on a single machine.\n+/// It's perfect for testing your distributed code without needing multiple GPUs or machines.\n+/// Think of it as a practice mode - it simulates distributed behavior but everything\n+/// runs in one process.\n+///\n+/// Example:\n+/// <code>\n+/// // Create a simulated distributed environment with 4 \"processes\"\n+/// var backend = new InMemoryCommunicationBackend&lt;double&gt;(rank: 0, worldSize: 4);\n+/// CommunicationManager.Initialize(backend);\n+/// </code>\n+///\n+/// Note: For production distributed training, you would use an MPI-based backend instead.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type for operations</typeparam>\n+public class InMemoryCommunicationBackend<T> : ICommunicationBackend<T> where T : struct\n+{\n+    private readonly int _rank;\n+    private readonly int _worldSize;\n+    private readonly INumericOperations<T> _numOps;\n+    private bool _isInitialized;\n+\n+    // Shared state for simulating collective operations\n+    // In a real implementation, this would be handled by the MPI backend\n+    private static readonly object _globalLock = new object();\n+    private static readonly Dictionary<string, List<Vector<T>>> _sharedBuffers = new();\n+    private static readonly Dictionary<string, int> _barrierCounters = new();\n+\n+    /// <inheritdoc/>\n+    public int Rank => _rank;\n+\n+    /// <inheritdoc/>\n+    public int WorldSize => _worldSize;\n+\n+    /// <inheritdoc/>\n+    public bool IsInitialized => _isInitialized;\n+\n+    /// <summary>\n+    /// Creates a new in-memory communication backend.\n+    ///\n+    /// For Beginners:\n+    /// You create one of these for each simulated \"process\". If you want to simulate\n+    /// 4 GPUs, you create 4 instances with ranks 0, 1, 2, 3, all with worldSize=4.\n+    /// </summary>\n+    /// <param name=\"rank\">The rank (ID) of this simulated process (0-based)</param>\n+    /// <param name=\"worldSize\">The total number of simulated processes</param>\n+    /// <exception cref=\"ArgumentException\">Thrown if rank or worldSize are invalid</exception>\n+    public InMemoryCommunicationBackend(int rank, int worldSize)\n+    {\n+        if (rank < 0 || rank >= worldSize)\n+        {\n+            throw new ArgumentException(\n+                $\"Invalid rank {rank}. Must be between 0 and {worldSize - 1}.\");\n+        }\n+\n+        if (worldSize <= 0)\n+        {\n+            throw new ArgumentException(\n+                $\"Invalid worldSize {worldSize}. Must be positive.\");\n+        }\n+\n+        _rank = rank;\n+        _worldSize = worldSize;\n+        _numOps = MathHelper.GetNumericOperations<T>();\n+        _isInitialized = false;\n+    }\n+\n+    /// <inheritdoc/>\n+    public void Initialize()\n+    {\n+        lock (_globalLock)\n+        {\n+            if (_isInitialized)\n+            {\n+                return;\n+            }\n+\n+            _isInitialized = true;\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public void Shutdown()\n+    {\n+        lock (_globalLock)\n+        {\n+            if (!_isInitialized)\n+            {\n+                return;\n+            }\n+\n+            // Clear any remaining shared state\n+            _sharedBuffers.Clear();\n+            _barrierCounters.Clear();\n+\n+            _isInitialized = false;\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public void Barrier()\n+    {\n+        EnsureInitialized();\n+\n+        lock (_globalLock)\n+        {\n+            string barrierId = $\"barrier_{DateTime.UtcNow.Ticks}\";\n+\n+            if (!_barrierCounters.ContainsKey(barrierId))\n+            {\n+                _barrierCounters[barrierId] = 0;\n+            }\n+\n+            _barrierCounters[barrierId]++;\n+\n+            // Wait until all processes have reached the barrier\n+            while (_barrierCounters[barrierId] < _worldSize)\n+            {\n+                Monitor.Wait(_globalLock, 10);\n+            }\n+\n+            Monitor.PulseAll(_globalLock);\n+\n+            // Cleanup\n+            if (_rank == 0)\n+            {\n+                _barrierCounters.Remove(barrierId);\n+            }\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public void AllReduce(Vector<T> data, ReductionOperation operation)\n+    {\n+        EnsureInitialized();\n+\n+        if (data == null)\n+        {\n+            throw new ArgumentNullException(nameof(data));\n+        }\n+\n+        // For single process, no communication needed\n+        if (_worldSize == 1)\n+        {\n+            if (operation == ReductionOperation.Average)\n+            {\n+                // Already averaged (only one value)\n+            }\n+            return;\n+        }\n+\n+        string bufferId = $\"allreduce_{Guid.NewGuid()}\";\n+\n+        lock (_globalLock)\n+        {\n+            // Initialize shared buffer\n+            if (!_sharedBuffers.ContainsKey(bufferId))\n+            {\n+                _sharedBuffers[bufferId] = new List<Vector<T>>();\n+            }\n+\n+            // Contribute local data\n+            _sharedBuffers[bufferId].Add(data.Clone());\n+\n+            // Wait until all processes have contributed\n+            while (_sharedBuffers[bufferId].Count < _worldSize)\n+            {\n+                Monitor.Wait(_globalLock, 10);\n+            }\n+\n+            Monitor.PulseAll(_globalLock);\n+\n+            // Perform reduction\n+            var allData = _sharedBuffers[bufferId];\n+            var result = PerformReduction(allData, operation);\n+\n+            // Copy result back to input data\n+            for (int i = 0; i < data.Length; i++)\n+            {\n+                data[i] = result[i];\n+            }\n+\n+            // Cleanup (rank 0 cleans up)\n+            if (_rank == 0)\n+            {\n+                _sharedBuffers.Remove(bufferId);\n+            }\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public Vector<T> AllGather(Vector<T> sendData)\n+    {\n+        EnsureInitialized();\n+\n+        if (sendData == null)\n+        {\n+            throw new ArgumentNullException(nameof(sendData));\n+        }\n+\n+        // For single process, just return a copy\n+        if (_worldSize == 1)\n+        {\n+            return sendData.Clone();\n+        }\n+\n+        string bufferId = $\"allgather_{Guid.NewGuid()}\";\n+\n+        lock (_globalLock)\n+        {\n+            // Initialize shared buffer\n+            if (!_sharedBuffers.ContainsKey(bufferId))\n+            {\n+                _sharedBuffers[bufferId] = new List<Vector<T>>(new Vector<T>[_worldSize]);\n+            }\n+\n+            // Contribute local data\n+            _sharedBuffers[bufferId][_rank] = sendData.Clone();\n+\n+            // Wait until all processes have contributed\n+            int contributedCount = 0;\n+            while (contributedCount < _worldSize)\n+            {\n+                contributedCount = _sharedBuffers[bufferId].Count(v => v != null);\n+                if (contributedCount < _worldSize)\n+                {\n+                    Monitor.Wait(_globalLock, 10);\n+                }\n+            }\n+\n+            Monitor.PulseAll(_globalLock);\n+\n+            // Concatenate all data\n+            var allData = _sharedBuffers[bufferId];\n+            int totalLength = allData.Sum(v => v.Length);\n+            var result = new T[totalLength];\n+            int offset = 0;\n+\n+            for (int i = 0; i < _worldSize; i++)\n+            {\n+                var data = allData[i];\n+                Array.Copy(data.ToArray(), 0, result, offset, data.Length);\n+                offset += data.Length;\n+            }\n+\n+            // Cleanup\n+            if (_rank == 0)\n+            {\n+                _sharedBuffers.Remove(bufferId);\n+            }\n+\n+            return new Vector<T>(result);\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public Vector<T> Broadcast(Vector<T> data, int root = 0)\n+    {\n+        EnsureInitialized();\n+\n+        if (root < 0 || root >= _worldSize)\n+        {\n+            throw new ArgumentException($\"Invalid root {root}. Must be between 0 and {_worldSize - 1}.\");\n+        }\n+\n+        // For single process, just return a copy\n+        if (_worldSize == 1)\n+        {\n+            return data?.Clone() ?? throw new ArgumentNullException(nameof(data));\n+        }\n+\n+        string bufferId = $\"broadcast_{Guid.NewGuid()}\";\n+\n+        lock (_globalLock)\n+        {\n+            Vector<T> result;\n+\n+            // Root process stores the data\n+            if (_rank == root)\n+            {\n+                if (data == null)\n+                {\n+                    throw new ArgumentNullException(nameof(data), \"Data cannot be null on root process.\");\n+                }\n+                _sharedBuffers[bufferId] = new List<Vector<T>> { data.Clone() };\n+            }\n+\n+            // Wait for root to store data\n+            while (!_sharedBuffers.ContainsKey(bufferId))\n+            {\n+                Monitor.Wait(_globalLock, 10);\n+            }\n+\n+            Monitor.PulseAll(_globalLock);\n+\n+            // All processes retrieve the data\n+            result = _sharedBuffers[bufferId][0].Clone();\n+\n+            // Cleanup\n+            if (_rank == 0)\n+            {\n+                _sharedBuffers.Remove(bufferId);\n+            }\n+\n+            return result;\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public Vector<T> Scatter(Vector<T> sendData, int root = 0)\n+    {\n+        EnsureInitialized();\n+\n+        if (root < 0 || root >= _worldSize)\n+        {\n+            throw new ArgumentException($\"Invalid root {root}. Must be between 0 and {_worldSize - 1}.\");\n+        }\n+\n+        // For single process, just return a copy\n+        if (_worldSize == 1)\n+        {\n+            return sendData?.Clone() ?? throw new ArgumentNullException(nameof(sendData));\n+        }\n+\n+        string bufferId = $\"scatter_{Guid.NewGuid()}\";\n+\n+        lock (_globalLock)\n+        {\n+            // Root process splits and stores the data\n+            if (_rank == root)\n+            {\n+                if (sendData == null)\n+                {\n+                    throw new ArgumentNullException(nameof(sendData), \"Data cannot be null on root process.\");\n+                }\n+\n+                if (sendData.Length % _worldSize != 0)\n+                {\n+                    throw new ArgumentException(\n+                        $\"Data length {sendData.Length} must be divisible by world size {_worldSize}.\");\n+                }\n+\n+                int chunkSize = sendData.Length / _worldSize;\n+                _sharedBuffers[bufferId] = new List<Vector<T>>();\n+\n+                for (int i = 0; i < _worldSize; i++)\n+                {\n+                    var chunk = new T[chunkSize];\n+                    Array.Copy(sendData.ToArray(), i * chunkSize, chunk, 0, chunkSize);\n+                    _sharedBuffers[bufferId].Add(new Vector<T>(chunk));\n+                }\n+            }\n+\n+            // Wait for root to split data\n+            while (!_sharedBuffers.ContainsKey(bufferId))\n+            {\n+                Monitor.Wait(_globalLock, 10);\n+            }\n+\n+            Monitor.PulseAll(_globalLock);\n+\n+            // Each process retrieves its chunk\n+            var result = _sharedBuffers[bufferId][_rank].Clone();\n+\n+            // Cleanup\n+            if (_rank == 0)\n+            {\n+                _sharedBuffers.Remove(bufferId);\n+            }\n+\n+            return result;\n+        }"}]}},{"id":"PRRT_kwDOKSXUF85g9Vqd","isResolved":true,"isOutdated":true,"path":"src/DistributedTraining/ShardedModel.cs","line":null,"originalLine":41,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VG0X0","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Implement the required interface members**  \n`ShardedModel<T, TInput, TOutput>` still misses `DeepCopy()`, `GetActiveFeatureIndices()`, `SetActiveFeatureIndices(...)`, and `IsFeatureUsed(...)` from the inherited interfaces, so `dotnet build` fails with CS0535. Please delegate these members to the wrapped model (and wrap the deep copy in a new `ShardedModel`) before merging. A minimal fix looks like:\n\n```diff\n+    public IFullModel<T, TInput, TOutput> DeepCopy()\n+    {\n+        var cloned = _wrappedModel.DeepCopy();\n+        return new ShardedModel<T, TInput, TOutput>(cloned, _config);\n+    }\n+\n+    public IEnumerable<int> GetActiveFeatureIndices()\n+    {\n+        return _wrappedModel.GetActiveFeatureIndices();\n+    }\n+\n+    public void SetActiveFeatureIndices(IEnumerable<int> indices)\n+    {\n+        _wrappedModel.SetActiveFeatureIndices(indices);\n+        _cachedFullParameters = null;\n+    }\n+\n+    public bool IsFeatureUsed(int featureIndex)\n+    {\n+        return _wrappedModel.IsFeatureUsed(featureIndex);\n+    }\n```\n\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>üß∞ Tools</summary>\n\n<details>\n<summary>ü™õ GitHub Actions: Build</summary>\n\n[error] 40-40: dotnet build failed: CS0535: 'ShardedModel<T, TInput, TOutput>' does not implement interface member 'IFeatureAware.GetActiveFeatureIndices()'.\n\n</details>\n<details>\n<summary>ü™õ GitHub Actions: Quality Gates (.NET)</summary>\n\n[error] 40-40: CS0535: 'ShardedModel<T, TInput, TOutput>' does not implement interface member 'IFeatureAware.GetActiveFeatureIndices()'.\n\n</details>\n<details>\n<summary>ü™õ GitHub Check: Build All Frameworks</summary>\n\n[failure] 40-40: \n'ShardedModel<T, TInput, TOutput>' does not implement interface member 'IFeatureAware.SetActiveFeatureIndices(IEnumerable<int>)'\n\n---\n\n[failure] 40-40: \n'ShardedModel<T, TInput, TOutput>' does not implement interface member 'IFeatureAware.GetActiveFeatureIndices()'\n\n---\n\n[failure] 40-40: \n'ShardedModel<T, TInput, TOutput>' does not implement interface member 'ICloneable<IFullModel<T, TInput, TOutput>>.DeepCopy()'\n\n---\n\n[failure] 40-40: \n'ShardedModel<T, TInput, TOutput>' does not implement interface member 'IFeatureAware.IsFeatureUsed(int)'\n\n---\n\n[failure] 40-40: \n'ShardedModel<T, TInput, TOutput>' does not implement interface member 'IFeatureAware.SetActiveFeatureIndices(IEnumerable<int>)'\n\n---\n\n[failure] 40-40: \n'ShardedModel<T, TInput, TOutput>' does not implement interface member 'IFeatureAware.GetActiveFeatureIndices()'\n\n---\n\n[failure] 40-40: \n'ShardedModel<T, TInput, TOutput>' does not implement interface member 'ICloneable<IFullModel<T, TInput, TOutput>>.DeepCopy()'\n\n---\n\n[failure] 40-40: \n'ShardedModel<T, TInput, TOutput>' does not implement interface member 'IFeatureAware.IsFeatureUsed(int)'\n\n---\n\n[failure] 40-40: \n'ShardedModel<T, TInput, TOutput>' does not implement interface member 'IFeatureAware.SetActiveFeatureIndices(IEnumerable<int>)'\n\n---\n\n[failure] 40-40: \n'ShardedModel<T, TInput, TOutput>' does not implement interface member 'IFeatureAware.GetActiveFeatureIndices()'\n\n</details>\n<details>\n<summary>ü™õ GitHub Check: Publish Size Analysis</summary>\n\n[failure] 40-40: \n'ShardedModel<T, TInput, TOutput>' does not implement interface member 'ICloneable<IFullModel<T, TInput, TOutput>>.DeepCopy()'\n\n---\n\n[failure] 40-40: \n'ShardedModel<T, TInput, TOutput>' does not implement interface member 'IFeatureAware.IsFeatureUsed(int)'\n\n---\n\n[failure] 40-40: \n'ShardedModel<T, TInput, TOutput>' does not implement interface member 'IFeatureAware.SetActiveFeatureIndices(IEnumerable<int>)'\n\n---\n\n[failure] 40-40: \n'ShardedModel<T, TInput, TOutput>' does not implement interface member 'IFeatureAware.GetActiveFeatureIndices()'\n\n</details>\n\n</details>\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/DistributedTraining/ShardedModel.cs around lines 40-41, the class\nShardedModel<T, TInput, TOutput> is missing required interface members\nDeepCopy(), GetActiveFeatureIndices(), SetActiveFeatureIndices(...), and\nIsFeatureUsed(...), causing CS0535; implement each by delegating to the\ninternal/wrapped model instance (call wrappedModel.GetActiveFeatureIndices(),\nwrappedModel.SetActiveFeatureIndices(...), wrappedModel.IsFeatureUsed(...)) and\nimplement DeepCopy() by calling wrappedModel.DeepCopy() and returning a new\nShardedModel constructed around that deep copy (preserving shard metadata),\nensuring generics and access modifiers match the interface signatures.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits 380cb58 to 6e232b4","createdAt":"2025-11-07T03:53:13Z","path":"src/DistributedTraining/ShardedModel.cs","line":null,"originalLine":41,"diffHunk":"@@ -0,0 +1,345 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.Models;\n+using AiDotNet.NumericOperations;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Implements a distributed model wrapper that shards parameters across multiple processes.\n+///\n+/// For Beginners:\n+/// This class wraps any existing model and makes it work across multiple GPUs or machines.\n+/// It automatically handles:\n+/// - Splitting parameters across processes (sharding)\n+/// - Gathering parameters when needed for forward pass\n+/// - Averaging gradients across all processes during training\n+///\n+/// Think of it like a team project where each person holds part of the solution.\n+/// When you need the full solution, everyone shares their part (AllGather).\n+/// When everyone learns something new, they share and average their learnings (AllReduce).\n+///\n+/// Example:\n+/// <code>\n+/// // Original model\n+/// var model = new NeuralNetworkModel&lt;double&gt;(...);\n+///\n+/// // Wrap it for distributed training\n+/// var backend = new InMemoryCommunicationBackend&lt;double&gt;(rank: 0, worldSize: 4);\n+/// var config = new ShardingConfiguration&lt;double&gt;(backend);\n+/// var distributedModel = new ShardedModel&lt;double, Tensor&lt;double&gt;, Tensor&lt;double&gt;&gt;(model, config);\n+///\n+/// // Now train as usual - distributed magic happens automatically!\n+/// distributedModel.Train(inputs, outputs);\n+/// </code>\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type</typeparam>\n+/// <typeparam name=\"TInput\">The input type for the model</typeparam>\n+/// <typeparam name=\"TOutput\">The output type for the model</typeparam>\n+public class ShardedModel<T, TInput, TOutput> : IShardedModel<T, TInput, TOutput> where T : struct\n+{"}]}},{"id":"PRRT_kwDOKSXUF85g9Vqh","isResolved":true,"isOutdated":true,"path":"src/DistributedTraining/ShardedModel.cs","line":null,"originalLine":159,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VG0X7","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Fix gradient synchronization to avoid crashes**  \n`SynchronizeGradients()` calls `AllReduce` on `_localParameterShard`, but shard sizes differ whenever `ParameterCount % WorldSize != 0`. In that case the first rank entering the lock sets the reduction length, so another rank with a longer shard triggers an `IndexOutOfRangeException`, or a shorter shard leaves its tail elements never averaged. Even when lengths match, you are averaging parameters instead of gradients, so every training step corrupts the optimizer state. Please switch to reducing the actual gradient buffers (with consistent lengths across ranks) before sharding, or disable auto-sync until gradient tensors are plumbed correctly.\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits 380cb58 to 6e232b4","createdAt":"2025-11-07T03:53:13Z","path":"src/DistributedTraining/ShardedModel.cs","line":null,"originalLine":159,"diffHunk":"@@ -0,0 +1,345 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.Models;\n+using AiDotNet.NumericOperations;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Implements a distributed model wrapper that shards parameters across multiple processes.\n+///\n+/// For Beginners:\n+/// This class wraps any existing model and makes it work across multiple GPUs or machines.\n+/// It automatically handles:\n+/// - Splitting parameters across processes (sharding)\n+/// - Gathering parameters when needed for forward pass\n+/// - Averaging gradients across all processes during training\n+///\n+/// Think of it like a team project where each person holds part of the solution.\n+/// When you need the full solution, everyone shares their part (AllGather).\n+/// When everyone learns something new, they share and average their learnings (AllReduce).\n+///\n+/// Example:\n+/// <code>\n+/// // Original model\n+/// var model = new NeuralNetworkModel&lt;double&gt;(...);\n+///\n+/// // Wrap it for distributed training\n+/// var backend = new InMemoryCommunicationBackend&lt;double&gt;(rank: 0, worldSize: 4);\n+/// var config = new ShardingConfiguration&lt;double&gt;(backend);\n+/// var distributedModel = new ShardedModel&lt;double, Tensor&lt;double&gt;, Tensor&lt;double&gt;&gt;(model, config);\n+///\n+/// // Now train as usual - distributed magic happens automatically!\n+/// distributedModel.Train(inputs, outputs);\n+/// </code>\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type</typeparam>\n+/// <typeparam name=\"TInput\">The input type for the model</typeparam>\n+/// <typeparam name=\"TOutput\">The output type for the model</typeparam>\n+public class ShardedModel<T, TInput, TOutput> : IShardedModel<T, TInput, TOutput> where T : struct\n+{\n+    private readonly IFullModel<T, TInput, TOutput> _wrappedModel;\n+    private readonly IShardingConfiguration<T> _config;\n+    private readonly INumericOperations<T> _numOps;\n+    private Vector<T> _localParameterShard;\n+    private Vector<T>? _cachedFullParameters;\n+    private int _shardStartIndex;\n+    private int _shardSize;\n+\n+    /// <inheritdoc/>\n+    public IFullModel<T, TInput, TOutput> WrappedModel => _wrappedModel;\n+\n+    /// <inheritdoc/>\n+    public int Rank => _config.CommunicationBackend.Rank;\n+\n+    /// <inheritdoc/>\n+    public int WorldSize => _config.CommunicationBackend.WorldSize;\n+\n+    /// <inheritdoc/>\n+    public Vector<T> LocalParameterShard => _localParameterShard;\n+\n+    /// <inheritdoc/>\n+    public IShardingConfiguration<T> ShardingConfiguration => _config;\n+\n+    /// <inheritdoc/>\n+    public int ParameterCount => _wrappedModel.ParameterCount;\n+\n+    /// <summary>\n+    /// Creates a new sharded model wrapping an existing model.\n+    ///\n+    /// For Beginners:\n+    /// This constructor takes your existing model and makes it distributed.\n+    /// You provide:\n+    /// 1. The model you want to make distributed\n+    /// 2. A configuration that tells us how to do the distribution\n+    ///\n+    /// The constructor automatically:\n+    /// - Splits the model's parameters across all processes\n+    /// - Sets up communication channels\n+    /// - Prepares everything for distributed training\n+    /// </summary>\n+    /// <param name=\"wrappedModel\">The model to wrap with distributed capabilities</param>\n+    /// <param name=\"config\">Configuration for sharding and communication</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown if model or config is null</exception>\n+    public ShardedModel(IFullModel<T, TInput, TOutput> wrappedModel, IShardingConfiguration<T> config)\n+    {\n+        _wrappedModel = wrappedModel ?? throw new ArgumentNullException(nameof(wrappedModel));\n+        _config = config ?? throw new ArgumentNullException(nameof(config));\n+        _numOps = MathHelper.GetNumericOperations<T>();\n+\n+        // Initialize the communication backend if not already done\n+        if (!_config.CommunicationBackend.IsInitialized)\n+        {\n+            _config.CommunicationBackend.Initialize();\n+        }\n+\n+        // Shard the model parameters across processes\n+        InitializeSharding();\n+    }\n+\n+    /// <summary>\n+    /// Initializes parameter sharding by dividing parameters across processes.\n+    ///\n+    /// For Beginners:\n+    /// This method splits the model's parameters into chunks and gives each\n+    /// process its own chunk to manage. It's like dividing a deck of cards\n+    /// evenly among players.\n+    /// </summary>\n+    private void InitializeSharding()\n+    {\n+        var fullParameters = _wrappedModel.GetParameters();\n+        int totalParams = fullParameters.Length;\n+\n+        // Calculate shard size for this process\n+        int baseShardSize = totalParams / WorldSize;\n+        int remainder = totalParams % WorldSize;\n+\n+        // Distribute remainder among first 'remainder' processes\n+        _shardSize = baseShardSize + (Rank < remainder ? 1 : 0);\n+        _shardStartIndex = Rank * baseShardSize + Math.Min(Rank, remainder);\n+\n+        // Extract local shard\n+        var shardData = new T[_shardSize];\n+        Array.Copy(fullParameters.ToArray(), _shardStartIndex, shardData, 0, _shardSize);\n+        _localParameterShard = new Vector<T>(shardData);\n+\n+        // Cache invalidated\n+        _cachedFullParameters = null;\n+    }\n+\n+    /// <inheritdoc/>\n+    public Vector<T> GatherFullParameters()\n+    {\n+        // Use cached version if available\n+        if (_cachedFullParameters != null)\n+        {\n+            return _cachedFullParameters;\n+        }\n+\n+        // Gather parameters from all processes\n+        var gathered = _config.CommunicationBackend.AllGather(_localParameterShard);\n+        _cachedFullParameters = gathered;\n+        return gathered;\n+    }\n+\n+    /// <inheritdoc/>\n+    public void SynchronizeGradients()\n+    {\n+        // Get gradients from wrapped model\n+        // Note: This assumes the model exposes gradients somehow.\n+        // For now, we'll just synchronize the local parameter shard\n+        // In a real implementation, gradients would be tracked separately\n+\n+        // Perform AllReduce with average operation\n+        _config.CommunicationBackend.AllReduce(_localParameterShard, ReductionOperation.Average);\n+\n+        // Invalidate cached full parameters\n+        _cachedFullParameters = null;\n+    }"}]}},{"id":"PRRT_kwDOKSXUF85g9Vql","isResolved":true,"isOutdated":false,"path":"tests/UnitTests/DistributedTraining/DistributedTrainingTests.cs","line":94,"originalLine":92,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VG0YD","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**Fix collective tests to avoid deadlock.** The `InMemoryCommunicationBackend` waits for all ranks to enter each collective under the same `Monitor`. Here we call `AllReduce` sequentially on the same thread, so the very first invocation blocks forever waiting for the other ranks that never get a chance to contribute. This locks up the test run (and any other test reusing the same pattern, e.g., the Average/AllGather variants). Drive the four simulated ranks in parallel (e.g., `Parallel.For`, tasks, or threads) so their `AllReduce` calls execute concurrently.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn tests/UnitTests/DistributedTraining/DistributedTrainingTests.cs around lines\n54 to 92, the test calls backends[i].AllReduce sequentially which deadlocks\nbecause the InMemoryCommunicationBackend expects all ranks to enter the\ncollective concurrently; change the Act section to drive the four simulated\nranks in parallel (e.g., use Parallel.For, start four Tasks with Task.Run and\nthen Task.WaitAll, or spawn four threads) so each backend invokes AllReduce\nconcurrently and the collective can complete.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits 380cb58 to 6e232b4","createdAt":"2025-11-07T03:53:14Z","path":"tests/UnitTests/DistributedTraining/DistributedTrainingTests.cs","line":94,"originalLine":92,"diffHunk":"@@ -0,0 +1,409 @@\n+using Xunit;\n+using AiDotNet.DistributedTraining;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.Models;\n+using AiDotNet.NumericOperations;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNetTests.DistributedTraining;\n+\n+/// <summary>\n+/// Integration tests for distributed training functionality.\n+/// These tests verify that distributed training produces numerically equivalent results\n+/// to single-process training.\n+///\n+/// For Beginners:\n+/// These tests ensure that when we train a model across multiple processes (distributed),\n+/// we get the same result as training on a single process. This is crucial because it\n+/// proves that our distributed implementation is correct and not introducing errors.\n+/// </summary>\n+public class DistributedTrainingTests\n+{\n+    private readonly INumericOperations<double> _numOps = MathHelper.GetNumericOperations<double>();\n+\n+    /// <summary>\n+    /// Tests that the InMemoryCommunicationBackend correctly initializes.\n+    /// </summary>\n+    [Fact]\n+    public void InMemoryBackend_Initialize_Succeeds()\n+    {\n+        // Arrange\n+        var backend = new InMemoryCommunicationBackend<double>(rank: 0, worldSize: 4);\n+\n+        // Act\n+        backend.Initialize();\n+\n+        // Assert\n+        Assert.True(backend.IsInitialized);\n+        Assert.Equal(0, backend.Rank);\n+        Assert.Equal(4, backend.WorldSize);\n+\n+        // Cleanup\n+        backend.Shutdown();\n+        Assert.False(backend.IsInitialized);\n+    }\n+\n+    /// <summary>\n+    /// Tests that AllReduce with Sum operation correctly sums values across processes.\n+    ///\n+    /// For Beginners:\n+    /// This simulates having 4 processes, each with a vector [1, 2, 3].\n+    /// After AllReduce with Sum, each should have [4, 8, 12] (sum of all 4 vectors).\n+    /// </summary>\n+    [Fact]\n+    public void InMemoryBackend_AllReduceSum_CorrectlyCombinesValues()\n+    {\n+        // Arrange - Simulate 4 processes\n+        var backends = new[]\n+        {\n+            new InMemoryCommunicationBackend<double>(0, 4),\n+            new InMemoryCommunicationBackend<double>(1, 4),\n+            new InMemoryCommunicationBackend<double>(2, 4),\n+            new InMemoryCommunicationBackend<double>(3, 4)\n+        };\n+\n+        foreach (var backend in backends)\n+        {\n+            backend.Initialize();\n+        }\n+\n+        var data = new[]\n+        {\n+            new Vector<double>(new[] { 1.0, 2.0, 3.0 }),\n+            new Vector<double>(new[] { 1.0, 2.0, 3.0 }),\n+            new Vector<double>(new[] { 1.0, 2.0, 3.0 }),\n+            new Vector<double>(new[] { 1.0, 2.0, 3.0 })\n+        };\n+\n+        // Act - Perform AllReduce on each \"process\"\n+        for (int i = 0; i < 4; i++)\n+        {\n+            backends[i].AllReduce(data[i], ReductionOperation.Sum);\n+        }\n+\n+        // Assert - All processes should have the same summed result\n+        var expected = new[] { 4.0, 8.0, 12.0 };\n+        for (int i = 0; i < 4; i++)\n+        {\n+            Assert.Equal(expected[0], data[i][0], precision: 10);\n+            Assert.Equal(expected[1], data[i][1], precision: 10);\n+            Assert.Equal(expected[2], data[i][2], precision: 10);\n+        }\n+"}]}},{"id":"PRRT_kwDOKSXUF85g9V8E","isResolved":true,"isOutdated":true,"path":"src/DistributedTraining/CommunicationManager.cs","line":null,"originalLine":98,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VG0zK","author":{"login":"copilot-pull-request-reviewer"},"body":"The CommunicationManager only supports float and double types, which is documented, but the error messages could be more helpful. When an unsupported type is used, consider suggesting alternatives or explaining why only float/double are supported (likely due to MPI type mapping constraints).\n```suggestion\n                    \"Only float and double are supported because of MPI type mapping constraints. \" +\n                    \"Please use float or double for distributed operations.\");\n```","createdAt":"2025-11-07T03:54:05Z","path":"src/DistributedTraining/CommunicationManager.cs","line":null,"originalLine":98,"diffHunk":"@@ -0,0 +1,320 @@\n+using AiDotNet.LinearAlgebra;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Central manager for distributed communication operations.\n+/// Provides a static API for collective communication in distributed training scenarios.\n+///\n+/// For Beginners:\n+/// This is your main entry point for distributed training communication.\n+/// It's a \"wrapper\" that makes it easy to communicate between different processes/GPUs\n+/// without worrying about the underlying implementation details.\n+///\n+/// Example usage:\n+/// <code>\n+/// // Initialize communication (do this once at startup)\n+/// CommunicationManager.Initialize(new InMemoryCommunicationBackend&lt;double&gt;());\n+///\n+/// // Get your process ID and total number of processes\n+/// int myRank = CommunicationManager.GetRank();\n+/// int totalProcesses = CommunicationManager.GetWorldSize();\n+///\n+/// // Average gradients across all processes\n+/// Vector&lt;double&gt; gradients = ...; // Your local gradients\n+/// CommunicationManager.AllReduce(gradients, ReductionOperation.Sum);\n+/// // Now 'gradients' contains the sum from all processes\n+///\n+/// // Clean up when done\n+/// CommunicationManager.Shutdown();\n+/// </code>\n+/// </summary>\n+public static class CommunicationManager\n+{\n+    private static readonly object _lock = new object();\n+    private static ICommunicationBackend<float>? _floatBackend;\n+    private static ICommunicationBackend<double>? _doubleBackend;\n+    private static bool _isInitialized = false;\n+\n+    /// <summary>\n+    /// Gets whether the communication manager has been initialized.\n+    /// </summary>\n+    public static bool IsInitialized\n+    {\n+        get\n+        {\n+            lock (_lock)\n+            {\n+                return _isInitialized;\n+            }\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Initializes the communication manager with the specified backend.\n+    /// This must be called before any other operations.\n+    ///\n+    /// For Beginners:\n+    /// This sets up the communication system. You need to provide a \"backend\"\n+    /// which is the actual implementation that does the communication.\n+    /// For testing, use InMemoryCommunicationBackend. For real distributed training,\n+    /// you would use an MPI backend.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type (float or double)</typeparam>\n+    /// <param name=\"backend\">The communication backend to use</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown if backend is null</exception>\n+    /// <exception cref=\"InvalidOperationException\">Thrown if already initialized</exception>\n+    public static void Initialize<T>(ICommunicationBackend<T> backend) where T : struct\n+    {\n+        if (backend == null)\n+        {\n+            throw new ArgumentNullException(nameof(backend));\n+        }\n+\n+        lock (_lock)\n+        {\n+            if (_isInitialized)\n+            {\n+                throw new InvalidOperationException(\n+                    \"CommunicationManager is already initialized. Call Shutdown() first.\");\n+            }\n+\n+            // Initialize the backend\n+            backend.Initialize();\n+\n+            // Store the backend (type-specific)\n+            if (typeof(T) == typeof(float))\n+            {\n+                _floatBackend = backend as ICommunicationBackend<float>;\n+            }\n+            else if (typeof(T) == typeof(double))\n+            {\n+                _doubleBackend = backend as ICommunicationBackend<double>;\n+            }\n+            else\n+            {\n+                throw new NotSupportedException(\n+                    $\"Type {typeof(T).Name} is not supported for distributed communication. \" +\n+                    \"Use float or double.\");"}]}},{"id":"PRRT_kwDOKSXUF85g9V8K","isResolved":true,"isOutdated":true,"path":"scripts/launch-distributed-training.sh","line":null,"originalLine":106,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VG0zS","author":{"login":"copilot-pull-request-reviewer"},"body":"Security risk: The script executes `$PROGRAM` without validation or sanitization. While the script checks if the file exists, it doesn't verify the file is a legitimate executable or prevent path traversal. Consider adding validation that the program is in an expected location or has expected permissions.","createdAt":"2025-11-07T03:54:05Z","path":"scripts/launch-distributed-training.sh","line":null,"originalLine":106,"diffHunk":"@@ -0,0 +1,126 @@\n+#!/bin/bash\n+\n+################################################################################\n+# AiDotNet Distributed Training Launcher (Bash)\n+#\n+# This script launches distributed training using MPI across multiple processes.\n+#\n+# For Beginners:\n+# MPI (Message Passing Interface) is a standard for running programs across\n+# multiple computers or processors. Think of it like a coordinator that starts\n+# your program on multiple machines at once and helps them communicate.\n+#\n+# Usage:\n+#   ./launch-distributed-training.sh <num_processes> <program> [args...]\n+#\n+# Examples:\n+#   # Run on 4 GPUs locally\n+#   ./launch-distributed-training.sh 4 ./MyTrainingApp\n+#\n+#   # Run on 8 GPUs with additional arguments\n+#   ./launch-distributed-training.sh 8 ./MyTrainingApp --epochs 100 --lr 0.001\n+#\n+#   # Run across 2 machines with 4 GPUs each\n+#   ./launch-distributed-training.sh 8 ./MyTrainingApp --hosts machine1,machine2\n+################################################################################\n+\n+# Check if enough arguments provided\n+if [ \"$#\" -lt 2 ]; then\n+    echo \"Error: Insufficient arguments\"\n+    echo \"\"\n+    echo \"Usage: $0 <num_processes> <program> [args...]\"\n+    echo \"\"\n+    echo \"Arguments:\"\n+    echo \"  num_processes  - Number of processes to spawn (typically equals number of GPUs)\"\n+    echo \"  program        - Path to your training program executable\"\n+    echo \"  args           - Any additional arguments to pass to your program\"\n+    echo \"\"\n+    echo \"Examples:\"\n+    echo \"  $0 4 ./MyTrainingApp\"\n+    echo \"  $0 8 ./MyTrainingApp --epochs 100\"\n+    exit 1\n+fi\n+\n+# Parse arguments\n+NUM_PROCESSES=$1\n+PROGRAM=$2\n+shift 2\n+PROGRAM_ARGS=\"$@\"\n+\n+echo \"======================================\"\n+echo \"AiDotNet Distributed Training Launcher\"\n+echo \"======================================\"\n+echo \"\"\n+echo \"Configuration:\"\n+echo \"  Number of processes: $NUM_PROCESSES\"\n+echo \"  Program: $PROGRAM\"\n+echo \"  Program arguments: $PROGRAM_ARGS\"\n+echo \"\"\n+\n+# Check if mpiexec/mpirun is available\n+if command -v mpiexec &> /dev/null; then\n+    MPI_CMD=\"mpiexec\"\n+elif command -v mpirun &> /dev/null; then\n+    MPI_CMD=\"mpirun\"\n+else\n+    echo \"Error: Neither mpiexec nor mpirun found in PATH\"\n+    echo \"\"\n+    echo \"For Beginners:\"\n+    echo \"  You need to install MPI to run distributed training.\"\n+    echo \"  On Ubuntu/Debian: sudo apt-get install mpich\"\n+    echo \"  On macOS: brew install mpich\"\n+    echo \"  On Windows: Install Microsoft MPI from https://docs.microsoft.com/en-us/message-passing-interface/microsoft-mpi\"\n+    exit 1\n+fi\n+\n+echo \"Using MPI command: $MPI_CMD\"\n+echo \"\"\n+\n+# Check if program exists\n+if [ ! -f \"$PROGRAM\" ]; then\n+    echo \"Error: Program '$PROGRAM' not found\"\n+    echo \"\"\n+    echo \"For Beginners:\"\n+    echo \"  Make sure you've built your training program and the path is correct.\"\n+    echo \"  Example: dotnet publish -c Release -o ./publish\"\n+    echo \"  Then use: $0 4 ./publish/MyTrainingApp\"\n+    exit 1\n+fi\n+\n+# Make program executable if it isn't\n+if [ ! -x \"$PROGRAM\" ]; then\n+    echo \"Warning: Program is not executable. Making it executable...\"\n+    chmod +x \"$PROGRAM\"\n+fi\n+\n+# Launch distributed training\n+echo \"Launching distributed training...\"\n+echo \"Command: $MPI_CMD -n $NUM_PROCESSES $PROGRAM $PROGRAM_ARGS\"\n+echo \"\"\n+echo \"======================================\"\n+echo \"\"\n+\n+# Execute MPI command\n+# -n: Number of processes\n+# The program and its arguments follow\n+$MPI_CMD -n $NUM_PROCESSES $PROGRAM $PROGRAM_ARGS"}]}},{"id":"PRRT_kwDOKSXUF85g9V8N","isResolved":true,"isOutdated":false,"path":"src/DistributedTraining/FSDPModel.cs","line":208,"originalLine":286,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VG0zV","author":{"login":"copilot-pull-request-reviewer"},"body":"Missing validation: The `savedRank` is read from the serialized data but not used for validation. If the current rank differs from the saved rank, this could indicate a configuration mismatch that should be handled or at least logged as a warning.","createdAt":"2025-11-07T03:54:05Z","path":"src/DistributedTraining/FSDPModel.cs","line":208,"originalLine":286,"diffHunk":"@@ -0,0 +1,345 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.Models;\n+using AiDotNet.NumericOperations;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Implements a distributed model wrapper that shards parameters across multiple processes.\n+///\n+/// For Beginners:\n+/// This class wraps any existing model and makes it work across multiple GPUs or machines.\n+/// It automatically handles:\n+/// - Splitting parameters across processes (sharding)\n+/// - Gathering parameters when needed for forward pass\n+/// - Averaging gradients across all processes during training\n+///\n+/// Think of it like a team project where each person holds part of the solution.\n+/// When you need the full solution, everyone shares their part (AllGather).\n+/// When everyone learns something new, they share and average their learnings (AllReduce).\n+///\n+/// Example:\n+/// <code>\n+/// // Original model\n+/// var model = new NeuralNetworkModel&lt;double&gt;(...);\n+///\n+/// // Wrap it for distributed training\n+/// var backend = new InMemoryCommunicationBackend&lt;double&gt;(rank: 0, worldSize: 4);\n+/// var config = new ShardingConfiguration&lt;double&gt;(backend);\n+/// var distributedModel = new ShardedModel&lt;double, Tensor&lt;double&gt;, Tensor&lt;double&gt;&gt;(model, config);\n+///\n+/// // Now train as usual - distributed magic happens automatically!\n+/// distributedModel.Train(inputs, outputs);\n+/// </code>\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type</typeparam>\n+/// <typeparam name=\"TInput\">The input type for the model</typeparam>\n+/// <typeparam name=\"TOutput\">The output type for the model</typeparam>\n+public class ShardedModel<T, TInput, TOutput> : IShardedModel<T, TInput, TOutput> where T : struct\n+{\n+    private readonly IFullModel<T, TInput, TOutput> _wrappedModel;\n+    private readonly IShardingConfiguration<T> _config;\n+    private readonly INumericOperations<T> _numOps;\n+    private Vector<T> _localParameterShard;\n+    private Vector<T>? _cachedFullParameters;\n+    private int _shardStartIndex;\n+    private int _shardSize;\n+\n+    /// <inheritdoc/>\n+    public IFullModel<T, TInput, TOutput> WrappedModel => _wrappedModel;\n+\n+    /// <inheritdoc/>\n+    public int Rank => _config.CommunicationBackend.Rank;\n+\n+    /// <inheritdoc/>\n+    public int WorldSize => _config.CommunicationBackend.WorldSize;\n+\n+    /// <inheritdoc/>\n+    public Vector<T> LocalParameterShard => _localParameterShard;\n+\n+    /// <inheritdoc/>\n+    public IShardingConfiguration<T> ShardingConfiguration => _config;\n+\n+    /// <inheritdoc/>\n+    public int ParameterCount => _wrappedModel.ParameterCount;\n+\n+    /// <summary>\n+    /// Creates a new sharded model wrapping an existing model.\n+    ///\n+    /// For Beginners:\n+    /// This constructor takes your existing model and makes it distributed.\n+    /// You provide:\n+    /// 1. The model you want to make distributed\n+    /// 2. A configuration that tells us how to do the distribution\n+    ///\n+    /// The constructor automatically:\n+    /// - Splits the model's parameters across all processes\n+    /// - Sets up communication channels\n+    /// - Prepares everything for distributed training\n+    /// </summary>\n+    /// <param name=\"wrappedModel\">The model to wrap with distributed capabilities</param>\n+    /// <param name=\"config\">Configuration for sharding and communication</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown if model or config is null</exception>\n+    public ShardedModel(IFullModel<T, TInput, TOutput> wrappedModel, IShardingConfiguration<T> config)\n+    {\n+        _wrappedModel = wrappedModel ?? throw new ArgumentNullException(nameof(wrappedModel));\n+        _config = config ?? throw new ArgumentNullException(nameof(config));\n+        _numOps = MathHelper.GetNumericOperations<T>();\n+\n+        // Initialize the communication backend if not already done\n+        if (!_config.CommunicationBackend.IsInitialized)\n+        {\n+            _config.CommunicationBackend.Initialize();\n+        }\n+\n+        // Shard the model parameters across processes\n+        InitializeSharding();\n+    }\n+\n+    /// <summary>\n+    /// Initializes parameter sharding by dividing parameters across processes.\n+    ///\n+    /// For Beginners:\n+    /// This method splits the model's parameters into chunks and gives each\n+    /// process its own chunk to manage. It's like dividing a deck of cards\n+    /// evenly among players.\n+    /// </summary>\n+    private void InitializeSharding()\n+    {\n+        var fullParameters = _wrappedModel.GetParameters();\n+        int totalParams = fullParameters.Length;\n+\n+        // Calculate shard size for this process\n+        int baseShardSize = totalParams / WorldSize;\n+        int remainder = totalParams % WorldSize;\n+\n+        // Distribute remainder among first 'remainder' processes\n+        _shardSize = baseShardSize + (Rank < remainder ? 1 : 0);\n+        _shardStartIndex = Rank * baseShardSize + Math.Min(Rank, remainder);\n+\n+        // Extract local shard\n+        var shardData = new T[_shardSize];\n+        Array.Copy(fullParameters.ToArray(), _shardStartIndex, shardData, 0, _shardSize);\n+        _localParameterShard = new Vector<T>(shardData);\n+\n+        // Cache invalidated\n+        _cachedFullParameters = null;\n+    }\n+\n+    /// <inheritdoc/>\n+    public Vector<T> GatherFullParameters()\n+    {\n+        // Use cached version if available\n+        if (_cachedFullParameters != null)\n+        {\n+            return _cachedFullParameters;\n+        }\n+\n+        // Gather parameters from all processes\n+        var gathered = _config.CommunicationBackend.AllGather(_localParameterShard);\n+        _cachedFullParameters = gathered;\n+        return gathered;\n+    }\n+\n+    /// <inheritdoc/>\n+    public void SynchronizeGradients()\n+    {\n+        // Get gradients from wrapped model\n+        // Note: This assumes the model exposes gradients somehow.\n+        // For now, we'll just synchronize the local parameter shard\n+        // In a real implementation, gradients would be tracked separately\n+\n+        // Perform AllReduce with average operation\n+        _config.CommunicationBackend.AllReduce(_localParameterShard, ReductionOperation.Average);\n+\n+        // Invalidate cached full parameters\n+        _cachedFullParameters = null;\n+    }\n+\n+    /// <inheritdoc/>\n+    public void Train(TInput input, TOutput expectedOutput)\n+    {\n+        // Gather full parameters for training\n+        var fullParams = GatherFullParameters();\n+        _wrappedModel.SetParameters(fullParams);\n+\n+        // Train the wrapped model\n+        _wrappedModel.Train(input, expectedOutput);\n+\n+        // Get updated parameters\n+        var updatedParams = _wrappedModel.GetParameters();\n+\n+        // Update local shard\n+        var shardData = new T[_shardSize];\n+        Array.Copy(updatedParams.ToArray(), _shardStartIndex, shardData, 0, _shardSize);\n+        _localParameterShard = new Vector<T>(shardData);\n+\n+        // Synchronize gradients if auto-sync is enabled\n+        if (_config.AutoSyncGradients)\n+        {\n+            SynchronizeGradients();\n+\n+            // Apply synchronized parameters back to the model\n+            fullParams = GatherFullParameters();\n+            _wrappedModel.SetParameters(fullParams);\n+        }\n+\n+        // Invalidate cache\n+        _cachedFullParameters = null;\n+    }\n+\n+    /// <inheritdoc/>\n+    public TOutput Predict(TInput input)\n+    {\n+        // Gather full parameters for prediction\n+        var fullParams = GatherFullParameters();\n+        _wrappedModel.SetParameters(fullParams);\n+\n+        // Use wrapped model for prediction\n+        return _wrappedModel.Predict(input);\n+    }\n+\n+    /// <inheritdoc/>\n+    public ModelMetadata<T> GetModelMetadata()\n+    {\n+        var metadata = _wrappedModel.GetModelMetadata();\n+\n+        // Add distributed training info\n+        metadata.SetProperty(\"IsDistributed\", true);\n+        metadata.SetProperty(\"WorldSize\", WorldSize);\n+        metadata.SetProperty(\"Rank\", Rank);\n+        metadata.SetProperty(\"ShardSize\", _shardSize);\n+\n+        return metadata;\n+    }\n+\n+    /// <inheritdoc/>\n+    public Vector<T> GetParameters()\n+    {\n+        return GatherFullParameters();\n+    }\n+\n+    /// <inheritdoc/>\n+    public void SetParameters(Vector<T> parameters)\n+    {\n+        if (parameters == null)\n+        {\n+            throw new ArgumentNullException(nameof(parameters));\n+        }\n+\n+        if (parameters.Length != ParameterCount)\n+        {\n+            throw new ArgumentException(\n+                $\"Parameter count mismatch. Expected {ParameterCount}, got {parameters.Length}.\");\n+        }\n+\n+        // Update local shard\n+        var shardData = new T[_shardSize];\n+        Array.Copy(parameters.ToArray(), _shardStartIndex, shardData, 0, _shardSize);\n+        _localParameterShard = new Vector<T>(shardData);\n+\n+        // Invalidate cache\n+        _cachedFullParameters = null;\n+\n+        // Update wrapped model\n+        _wrappedModel.SetParameters(parameters);\n+    }\n+\n+    /// <inheritdoc/>\n+    public IFullModel<T, TInput, TOutput> WithParameters(Vector<T> parameters)\n+    {\n+        var newModel = _wrappedModel.WithParameters(parameters);\n+        return new ShardedModel<T, TInput, TOutput>(newModel, _config);\n+    }\n+\n+    /// <inheritdoc/>\n+    public byte[] Serialize()\n+    {\n+        using var ms = new MemoryStream();\n+        using var writer = new BinaryWriter(ms);\n+\n+        // Serialize sharding configuration info\n+        writer.Write(WorldSize);\n+        writer.Write(Rank);\n+        writer.Write(_config.AutoSyncGradients);\n+        writer.Write(_config.MinimumParameterGroupSize);\n+        writer.Write(_config.EnableGradientCompression);\n+\n+        // Serialize wrapped model\n+        var modelData = _wrappedModel.Serialize();\n+        writer.Write(modelData.Length);\n+        writer.Write(modelData);\n+\n+        return ms.ToArray();\n+    }\n+\n+    /// <inheritdoc/>\n+    public void Deserialize(byte[] data)\n+    {\n+        using var ms = new MemoryStream(data);\n+        using var reader = new BinaryReader(ms);\n+\n+        // Read sharding configuration (for validation)\n+        int savedWorldSize = reader.ReadInt32();\n+        int savedRank = reader.ReadInt32();"}]}},{"id":"PRRT_kwDOKSXUF85g9V8P","isResolved":true,"isOutdated":true,"path":"src/DistributedTraining/ShardedModel.cs","line":null,"originalLine":256,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VG0zX","author":{"login":"copilot-pull-request-reviewer"},"body":"Race condition: The SaveModel method only allows rank 0 to write, but all processes reach the barrier. If rank 0 fails after checking the rank but before the barrier, other processes will deadlock waiting at the barrier. Consider moving the barrier before the rank check or handling errors more robustly.","createdAt":"2025-11-07T03:54:06Z","path":"src/DistributedTraining/ShardedModel.cs","line":null,"originalLine":256,"diffHunk":"@@ -0,0 +1,345 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.Models;\n+using AiDotNet.NumericOperations;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Implements a distributed model wrapper that shards parameters across multiple processes.\n+///\n+/// For Beginners:\n+/// This class wraps any existing model and makes it work across multiple GPUs or machines.\n+/// It automatically handles:\n+/// - Splitting parameters across processes (sharding)\n+/// - Gathering parameters when needed for forward pass\n+/// - Averaging gradients across all processes during training\n+///\n+/// Think of it like a team project where each person holds part of the solution.\n+/// When you need the full solution, everyone shares their part (AllGather).\n+/// When everyone learns something new, they share and average their learnings (AllReduce).\n+///\n+/// Example:\n+/// <code>\n+/// // Original model\n+/// var model = new NeuralNetworkModel&lt;double&gt;(...);\n+///\n+/// // Wrap it for distributed training\n+/// var backend = new InMemoryCommunicationBackend&lt;double&gt;(rank: 0, worldSize: 4);\n+/// var config = new ShardingConfiguration&lt;double&gt;(backend);\n+/// var distributedModel = new ShardedModel&lt;double, Tensor&lt;double&gt;, Tensor&lt;double&gt;&gt;(model, config);\n+///\n+/// // Now train as usual - distributed magic happens automatically!\n+/// distributedModel.Train(inputs, outputs);\n+/// </code>\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type</typeparam>\n+/// <typeparam name=\"TInput\">The input type for the model</typeparam>\n+/// <typeparam name=\"TOutput\">The output type for the model</typeparam>\n+public class ShardedModel<T, TInput, TOutput> : IShardedModel<T, TInput, TOutput> where T : struct\n+{\n+    private readonly IFullModel<T, TInput, TOutput> _wrappedModel;\n+    private readonly IShardingConfiguration<T> _config;\n+    private readonly INumericOperations<T> _numOps;\n+    private Vector<T> _localParameterShard;\n+    private Vector<T>? _cachedFullParameters;\n+    private int _shardStartIndex;\n+    private int _shardSize;\n+\n+    /// <inheritdoc/>\n+    public IFullModel<T, TInput, TOutput> WrappedModel => _wrappedModel;\n+\n+    /// <inheritdoc/>\n+    public int Rank => _config.CommunicationBackend.Rank;\n+\n+    /// <inheritdoc/>\n+    public int WorldSize => _config.CommunicationBackend.WorldSize;\n+\n+    /// <inheritdoc/>\n+    public Vector<T> LocalParameterShard => _localParameterShard;\n+\n+    /// <inheritdoc/>\n+    public IShardingConfiguration<T> ShardingConfiguration => _config;\n+\n+    /// <inheritdoc/>\n+    public int ParameterCount => _wrappedModel.ParameterCount;\n+\n+    /// <summary>\n+    /// Creates a new sharded model wrapping an existing model.\n+    ///\n+    /// For Beginners:\n+    /// This constructor takes your existing model and makes it distributed.\n+    /// You provide:\n+    /// 1. The model you want to make distributed\n+    /// 2. A configuration that tells us how to do the distribution\n+    ///\n+    /// The constructor automatically:\n+    /// - Splits the model's parameters across all processes\n+    /// - Sets up communication channels\n+    /// - Prepares everything for distributed training\n+    /// </summary>\n+    /// <param name=\"wrappedModel\">The model to wrap with distributed capabilities</param>\n+    /// <param name=\"config\">Configuration for sharding and communication</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown if model or config is null</exception>\n+    public ShardedModel(IFullModel<T, TInput, TOutput> wrappedModel, IShardingConfiguration<T> config)\n+    {\n+        _wrappedModel = wrappedModel ?? throw new ArgumentNullException(nameof(wrappedModel));\n+        _config = config ?? throw new ArgumentNullException(nameof(config));\n+        _numOps = MathHelper.GetNumericOperations<T>();\n+\n+        // Initialize the communication backend if not already done\n+        if (!_config.CommunicationBackend.IsInitialized)\n+        {\n+            _config.CommunicationBackend.Initialize();\n+        }\n+\n+        // Shard the model parameters across processes\n+        InitializeSharding();\n+    }\n+\n+    /// <summary>\n+    /// Initializes parameter sharding by dividing parameters across processes.\n+    ///\n+    /// For Beginners:\n+    /// This method splits the model's parameters into chunks and gives each\n+    /// process its own chunk to manage. It's like dividing a deck of cards\n+    /// evenly among players.\n+    /// </summary>\n+    private void InitializeSharding()\n+    {\n+        var fullParameters = _wrappedModel.GetParameters();\n+        int totalParams = fullParameters.Length;\n+\n+        // Calculate shard size for this process\n+        int baseShardSize = totalParams / WorldSize;\n+        int remainder = totalParams % WorldSize;\n+\n+        // Distribute remainder among first 'remainder' processes\n+        _shardSize = baseShardSize + (Rank < remainder ? 1 : 0);\n+        _shardStartIndex = Rank * baseShardSize + Math.Min(Rank, remainder);\n+\n+        // Extract local shard\n+        var shardData = new T[_shardSize];\n+        Array.Copy(fullParameters.ToArray(), _shardStartIndex, shardData, 0, _shardSize);\n+        _localParameterShard = new Vector<T>(shardData);\n+\n+        // Cache invalidated\n+        _cachedFullParameters = null;\n+    }\n+\n+    /// <inheritdoc/>\n+    public Vector<T> GatherFullParameters()\n+    {\n+        // Use cached version if available\n+        if (_cachedFullParameters != null)\n+        {\n+            return _cachedFullParameters;\n+        }\n+\n+        // Gather parameters from all processes\n+        var gathered = _config.CommunicationBackend.AllGather(_localParameterShard);\n+        _cachedFullParameters = gathered;\n+        return gathered;\n+    }\n+\n+    /// <inheritdoc/>\n+    public void SynchronizeGradients()\n+    {\n+        // Get gradients from wrapped model\n+        // Note: This assumes the model exposes gradients somehow.\n+        // For now, we'll just synchronize the local parameter shard\n+        // In a real implementation, gradients would be tracked separately\n+\n+        // Perform AllReduce with average operation\n+        _config.CommunicationBackend.AllReduce(_localParameterShard, ReductionOperation.Average);\n+\n+        // Invalidate cached full parameters\n+        _cachedFullParameters = null;\n+    }\n+\n+    /// <inheritdoc/>\n+    public void Train(TInput input, TOutput expectedOutput)\n+    {\n+        // Gather full parameters for training\n+        var fullParams = GatherFullParameters();\n+        _wrappedModel.SetParameters(fullParams);\n+\n+        // Train the wrapped model\n+        _wrappedModel.Train(input, expectedOutput);\n+\n+        // Get updated parameters\n+        var updatedParams = _wrappedModel.GetParameters();\n+\n+        // Update local shard\n+        var shardData = new T[_shardSize];\n+        Array.Copy(updatedParams.ToArray(), _shardStartIndex, shardData, 0, _shardSize);\n+        _localParameterShard = new Vector<T>(shardData);\n+\n+        // Synchronize gradients if auto-sync is enabled\n+        if (_config.AutoSyncGradients)\n+        {\n+            SynchronizeGradients();\n+\n+            // Apply synchronized parameters back to the model\n+            fullParams = GatherFullParameters();\n+            _wrappedModel.SetParameters(fullParams);\n+        }\n+\n+        // Invalidate cache\n+        _cachedFullParameters = null;\n+    }\n+\n+    /// <inheritdoc/>\n+    public TOutput Predict(TInput input)\n+    {\n+        // Gather full parameters for prediction\n+        var fullParams = GatherFullParameters();\n+        _wrappedModel.SetParameters(fullParams);\n+\n+        // Use wrapped model for prediction\n+        return _wrappedModel.Predict(input);\n+    }\n+\n+    /// <inheritdoc/>\n+    public ModelMetadata<T> GetModelMetadata()\n+    {\n+        var metadata = _wrappedModel.GetModelMetadata();\n+\n+        // Add distributed training info\n+        metadata.SetProperty(\"IsDistributed\", true);\n+        metadata.SetProperty(\"WorldSize\", WorldSize);\n+        metadata.SetProperty(\"Rank\", Rank);\n+        metadata.SetProperty(\"ShardSize\", _shardSize);\n+\n+        return metadata;\n+    }\n+\n+    /// <inheritdoc/>\n+    public Vector<T> GetParameters()\n+    {\n+        return GatherFullParameters();\n+    }\n+\n+    /// <inheritdoc/>\n+    public void SetParameters(Vector<T> parameters)\n+    {\n+        if (parameters == null)\n+        {\n+            throw new ArgumentNullException(nameof(parameters));\n+        }\n+\n+        if (parameters.Length != ParameterCount)\n+        {\n+            throw new ArgumentException(\n+                $\"Parameter count mismatch. Expected {ParameterCount}, got {parameters.Length}.\");\n+        }\n+\n+        // Update local shard\n+        var shardData = new T[_shardSize];\n+        Array.Copy(parameters.ToArray(), _shardStartIndex, shardData, 0, _shardSize);\n+        _localParameterShard = new Vector<T>(shardData);\n+\n+        // Invalidate cache\n+        _cachedFullParameters = null;\n+\n+        // Update wrapped model\n+        _wrappedModel.SetParameters(parameters);\n+    }\n+\n+    /// <inheritdoc/>\n+    public IFullModel<T, TInput, TOutput> WithParameters(Vector<T> parameters)\n+    {\n+        var newModel = _wrappedModel.WithParameters(parameters);\n+        return new ShardedModel<T, TInput, TOutput>(newModel, _config);\n+    }\n+"}]}},{"id":"PRRT_kwDOKSXUF85g9V8X","isResolved":true,"isOutdated":true,"path":"src/DistributedTraining/ShardedOptimizer.cs","line":null,"originalLine":184,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VG0zf","author":{"login":"copilot-pull-request-reviewer"},"body":"The early stopping consensus mechanism uses `Max` operation to decide if any process wants to stop. However, the comment on line 177 says \"if any process wants to stop, all stop\" which is correct, but this might not be the desired behavior for all use cases. Some applications might want all processes to agree (using Min instead), or a majority vote. Consider documenting this design choice more clearly or making it configurable.","createdAt":"2025-11-07T03:54:06Z","path":"src/DistributedTraining/ShardedOptimizer.cs","line":null,"originalLine":184,"diffHunk":"@@ -0,0 +1,268 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.Models.Inputs;\n+using AiDotNet.Optimizers;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Implements a distributed optimizer wrapper that coordinates optimization across multiple processes.\n+///\n+/// For Beginners:\n+/// This class wraps any existing optimizer (like Adam, SGD, etc.) and makes it work across\n+/// multiple GPUs or machines. It automatically handles:\n+/// - Synchronizing gradients across all processes\n+/// - Coordinating parameter updates\n+/// - Ensuring all processes stay in sync\n+///\n+/// Think of it like a team of coaches working together - each has their own expertise\n+/// (the wrapped optimizer), but they coordinate their efforts to train the team effectively.\n+///\n+/// Example:\n+/// <code>\n+/// // Original optimizer\n+/// var optimizer = new AdamOptimizer&lt;double, Tensor&lt;double&gt;, Tensor&lt;double&gt;&gt;(model, options);\n+///\n+/// // Wrap it for distributed training\n+/// var backend = new InMemoryCommunicationBackend&lt;double&gt;(rank: 0, worldSize: 4);\n+/// var config = new ShardingConfiguration&lt;double&gt;(backend);\n+/// var distributedOptimizer = new ShardedOptimizer&lt;double, Tensor&lt;double&gt;, Tensor&lt;double&gt;&gt;(\n+///     optimizer, config);\n+///\n+/// // Now optimize as usual - distributed magic happens automatically!\n+/// var result = distributedOptimizer.Optimize(inputData);\n+/// </code>\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type</typeparam>\n+/// <typeparam name=\"TInput\">The input type for the model</typeparam>\n+/// <typeparam name=\"TOutput\">The output type for the model</typeparam>\n+public class ShardedOptimizer<T, TInput, TOutput> : IShardedOptimizer<T, TInput, TOutput> where T : struct\n+{\n+    private readonly IOptimizer<T, TInput, TOutput> _wrappedOptimizer;\n+    private readonly IShardingConfiguration<T> _config;\n+\n+    /// <inheritdoc/>\n+    public IOptimizer<T, TInput, TOutput> WrappedOptimizer => _wrappedOptimizer;\n+\n+    /// <inheritdoc/>\n+    public int Rank => _config.CommunicationBackend.Rank;\n+\n+    /// <inheritdoc/>\n+    public int WorldSize => _config.CommunicationBackend.WorldSize;\n+\n+    /// <inheritdoc/>\n+    public IShardingConfiguration<T> ShardingConfiguration => _config;\n+\n+    /// <summary>\n+    /// Creates a new sharded optimizer wrapping an existing optimizer.\n+    ///\n+    /// For Beginners:\n+    /// This constructor takes your existing optimizer and makes it distributed.\n+    /// You provide:\n+    /// 1. The optimizer you want to make distributed\n+    /// 2. A configuration that tells us how to do the distribution\n+    ///\n+    /// The optimizer will automatically synchronize across all processes during optimization.\n+    /// </summary>\n+    /// <param name=\"wrappedOptimizer\">The optimizer to wrap with distributed capabilities</param>\n+    /// <param name=\"config\">Configuration for sharding and communication</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown if optimizer or config is null</exception>\n+    public ShardedOptimizer(\n+        IOptimizer<T, TInput, TOutput> wrappedOptimizer,\n+        IShardingConfiguration<T> config)\n+    {\n+        _wrappedOptimizer = wrappedOptimizer ?? throw new ArgumentNullException(nameof(wrappedOptimizer));\n+        _config = config ?? throw new ArgumentNullException(nameof(config));\n+\n+        // Initialize the communication backend if not already done\n+        if (!_config.CommunicationBackend.IsInitialized)\n+        {\n+            _config.CommunicationBackend.Initialize();\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public OptimizationResult<T, TInput, TOutput> Optimize(OptimizationInputData<T, TInput, TOutput> inputData)\n+    {\n+        if (inputData == null)\n+        {\n+            throw new ArgumentNullException(nameof(inputData));\n+        }\n+\n+        // Ensure all processes start together\n+        _config.CommunicationBackend.Barrier();\n+\n+        // If the model is a sharded model, it will handle gradient synchronization\n+        // Otherwise, we need to handle it ourselves\n+        bool modelHandlesSync = inputData.Model is IShardedModel<T, TInput, TOutput>;\n+\n+        // Perform optimization on the wrapped optimizer\n+        var result = _wrappedOptimizer.Optimize(inputData);\n+\n+        // If model doesn't handle synchronization and auto-sync is enabled, sync parameters\n+        if (!modelHandlesSync && _config.AutoSyncGradients)\n+        {\n+            SynchronizeParameters(result.BestModel);\n+        }\n+\n+        // Synchronize optimizer state if needed\n+        SynchronizeOptimizerState();\n+\n+        // Ensure all processes finish together\n+        _config.CommunicationBackend.Barrier();\n+\n+        return result;\n+    }\n+\n+    /// <inheritdoc/>\n+    public void SynchronizeOptimizerState()\n+    {\n+        // For now, this is a placeholder\n+        // In a full implementation, we would synchronize optimizer-specific state\n+        // like momentum buffers, variance estimates (for Adam), etc.\n+\n+        // Different optimizers have different state to sync:\n+        // - SGD with momentum: velocity vectors\n+        // - Adam: first and second moment estimates\n+        // - RMSprop: squared gradient moving average\n+\n+        // This would require either:\n+        // 1. Extending IOptimizer with state access methods\n+        // 2. Type-specific handling for known optimizer types\n+        // 3. A generic state serialization mechanism\n+\n+        // For the MVP, we assume stateless or that the wrapped optimizer handles its own state\n+    }\n+\n+    /// <summary>\n+    /// Synchronizes model parameters across all processes.\n+    ///\n+    /// For Beginners:\n+    /// After optimization, each process might have slightly different parameters\n+    /// (if they processed different data). This method averages the parameters\n+    /// across all processes so everyone has the same model.\n+    /// </summary>\n+    /// <param name=\"model\">The model whose parameters to synchronize</param>\n+    private void SynchronizeParameters(IFullModel<T, TInput, TOutput>? model)\n+    {\n+        if (model == null)\n+        {\n+            return;\n+        }\n+\n+        // Don't sync if it's already a sharded model (handles its own sync)\n+        if (model is IShardedModel<T, TInput, TOutput>)\n+        {\n+            return;\n+        }\n+\n+        // Get current parameters\n+        var parameters = model.GetParameters();\n+\n+        // Average parameters across all processes\n+        _config.CommunicationBackend.AllReduce(parameters, ReductionOperation.Average);\n+\n+        // Update model with averaged parameters\n+        model.SetParameters(parameters);\n+    }\n+\n+    /// <inheritdoc/>\n+    public bool ShouldEarlyStop()\n+    {\n+        // Delegate to wrapped optimizer\n+        bool localDecision = _wrappedOptimizer.ShouldEarlyStop();\n+\n+        // In distributed training, we need consensus on early stopping\n+        // All processes should agree to stop, otherwise some might continue while others stop\n+        // For now, we'll use a simple approach: if any process wants to stop, all stop\n+\n+        // Create a vector with the local decision (1 for stop, 0 for continue)\n+        var decision = new Vector<T>(new[] { localDecision ? MathHelper.GetNumericOperations<T>().One : MathHelper.GetNumericOperations<T>().Zero });\n+\n+        // Get the maximum across all processes\n+        // If any process returns 1 (stop), the max will be 1\n+        _config.CommunicationBackend.AllReduce(decision, ReductionOperation.Max);"}]}},{"id":"PRRT_kwDOKSXUF85g9V8c","isResolved":true,"isOutdated":false,"path":"src/DistributedTraining/InMemoryCommunicationBackend.cs","line":251,"originalLine":129,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VG0zn","author":{"login":"copilot-pull-request-reviewer"},"body":"Potential deadlock: The while loop waits for all processes to reach the barrier, but if a process fails before reaching it, all other processes will wait indefinitely. Consider adding a timeout mechanism or a way to break out of the wait loop if a process fails.\n```suggestion\n            // Wait until all processes have reached the barrier, with timeout\n            const int timeoutMilliseconds = 30000; // 30 seconds\n            var startTime = DateTime.UtcNow;\n            while (_barrierCounters[barrierId] < _worldSize)\n            {\n                var elapsed = (DateTime.UtcNow - startTime).TotalMilliseconds;\n                if (elapsed > timeoutMilliseconds)\n                {\n                    throw new TimeoutException(\n                        $\"Barrier timeout: Not all processes reached the barrier within {timeoutMilliseconds} ms.\");\n                }\n```","createdAt":"2025-11-07T03:54:06Z","path":"src/DistributedTraining/InMemoryCommunicationBackend.cs","line":251,"originalLine":129,"diffHunk":"@@ -0,0 +1,484 @@\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.NumericOperations;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// A simple in-memory implementation of distributed communication for testing and single-machine scenarios.\n+/// This backend simulates multiple processes by using shared memory and locks.\n+///\n+/// For Beginners:\n+/// This is a \"fake\" distributed system that runs on a single machine.\n+/// It's perfect for testing your distributed code without needing multiple GPUs or machines.\n+/// Think of it as a practice mode - it simulates distributed behavior but everything\n+/// runs in one process.\n+///\n+/// Example:\n+/// <code>\n+/// // Create a simulated distributed environment with 4 \"processes\"\n+/// var backend = new InMemoryCommunicationBackend&lt;double&gt;(rank: 0, worldSize: 4);\n+/// CommunicationManager.Initialize(backend);\n+/// </code>\n+///\n+/// Note: For production distributed training, you would use an MPI-based backend instead.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type for operations</typeparam>\n+public class InMemoryCommunicationBackend<T> : ICommunicationBackend<T> where T : struct\n+{\n+    private readonly int _rank;\n+    private readonly int _worldSize;\n+    private readonly INumericOperations<T> _numOps;\n+    private bool _isInitialized;\n+\n+    // Shared state for simulating collective operations\n+    // In a real implementation, this would be handled by the MPI backend\n+    private static readonly object _globalLock = new object();\n+    private static readonly Dictionary<string, List<Vector<T>>> _sharedBuffers = new();\n+    private static readonly Dictionary<string, int> _barrierCounters = new();\n+\n+    /// <inheritdoc/>\n+    public int Rank => _rank;\n+\n+    /// <inheritdoc/>\n+    public int WorldSize => _worldSize;\n+\n+    /// <inheritdoc/>\n+    public bool IsInitialized => _isInitialized;\n+\n+    /// <summary>\n+    /// Creates a new in-memory communication backend.\n+    ///\n+    /// For Beginners:\n+    /// You create one of these for each simulated \"process\". If you want to simulate\n+    /// 4 GPUs, you create 4 instances with ranks 0, 1, 2, 3, all with worldSize=4.\n+    /// </summary>\n+    /// <param name=\"rank\">The rank (ID) of this simulated process (0-based)</param>\n+    /// <param name=\"worldSize\">The total number of simulated processes</param>\n+    /// <exception cref=\"ArgumentException\">Thrown if rank or worldSize are invalid</exception>\n+    public InMemoryCommunicationBackend(int rank, int worldSize)\n+    {\n+        if (rank < 0 || rank >= worldSize)\n+        {\n+            throw new ArgumentException(\n+                $\"Invalid rank {rank}. Must be between 0 and {worldSize - 1}.\");\n+        }\n+\n+        if (worldSize <= 0)\n+        {\n+            throw new ArgumentException(\n+                $\"Invalid worldSize {worldSize}. Must be positive.\");\n+        }\n+\n+        _rank = rank;\n+        _worldSize = worldSize;\n+        _numOps = MathHelper.GetNumericOperations<T>();\n+        _isInitialized = false;\n+    }\n+\n+    /// <inheritdoc/>\n+    public void Initialize()\n+    {\n+        lock (_globalLock)\n+        {\n+            if (_isInitialized)\n+            {\n+                return;\n+            }\n+\n+            _isInitialized = true;\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public void Shutdown()\n+    {\n+        lock (_globalLock)\n+        {\n+            if (!_isInitialized)\n+            {\n+                return;\n+            }\n+\n+            // Clear any remaining shared state\n+            _sharedBuffers.Clear();\n+            _barrierCounters.Clear();\n+\n+            _isInitialized = false;\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public void Barrier()\n+    {\n+        EnsureInitialized();\n+\n+        lock (_globalLock)\n+        {\n+            string barrierId = $\"barrier_{DateTime.UtcNow.Ticks}\";\n+\n+            if (!_barrierCounters.ContainsKey(barrierId))\n+            {\n+                _barrierCounters[barrierId] = 0;\n+            }\n+\n+            _barrierCounters[barrierId]++;\n+\n+            // Wait until all processes have reached the barrier\n+            while (_barrierCounters[barrierId] < _worldSize)\n+            {"}]}},{"id":"PRRT_kwDOKSXUF85g9V8l","isResolved":false,"isOutdated":true,"path":"src/DistributedTraining/ShardedModel.cs","line":null,"originalLine":202,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VG0zz","author":{"login":"copilot-pull-request-reviewer"},"body":"The cache invalidation strategy may cause performance issues. The `_cachedFullParameters` is set to null after every Train() call (line 190) even when AutoSyncGradients is false, and it's also invalidated in SetParameters (line 244). However, if multiple predictions are made without training, the first prediction would benefit from caching, but the cache is never populated during prediction. Consider if the cache should be populated in the Predict method for better performance.","createdAt":"2025-11-07T03:54:06Z","path":"src/DistributedTraining/ShardedModel.cs","line":null,"originalLine":202,"diffHunk":"@@ -0,0 +1,345 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.Models;\n+using AiDotNet.NumericOperations;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Implements a distributed model wrapper that shards parameters across multiple processes.\n+///\n+/// For Beginners:\n+/// This class wraps any existing model and makes it work across multiple GPUs or machines.\n+/// It automatically handles:\n+/// - Splitting parameters across processes (sharding)\n+/// - Gathering parameters when needed for forward pass\n+/// - Averaging gradients across all processes during training\n+///\n+/// Think of it like a team project where each person holds part of the solution.\n+/// When you need the full solution, everyone shares their part (AllGather).\n+/// When everyone learns something new, they share and average their learnings (AllReduce).\n+///\n+/// Example:\n+/// <code>\n+/// // Original model\n+/// var model = new NeuralNetworkModel&lt;double&gt;(...);\n+///\n+/// // Wrap it for distributed training\n+/// var backend = new InMemoryCommunicationBackend&lt;double&gt;(rank: 0, worldSize: 4);\n+/// var config = new ShardingConfiguration&lt;double&gt;(backend);\n+/// var distributedModel = new ShardedModel&lt;double, Tensor&lt;double&gt;, Tensor&lt;double&gt;&gt;(model, config);\n+///\n+/// // Now train as usual - distributed magic happens automatically!\n+/// distributedModel.Train(inputs, outputs);\n+/// </code>\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type</typeparam>\n+/// <typeparam name=\"TInput\">The input type for the model</typeparam>\n+/// <typeparam name=\"TOutput\">The output type for the model</typeparam>\n+public class ShardedModel<T, TInput, TOutput> : IShardedModel<T, TInput, TOutput> where T : struct\n+{\n+    private readonly IFullModel<T, TInput, TOutput> _wrappedModel;\n+    private readonly IShardingConfiguration<T> _config;\n+    private readonly INumericOperations<T> _numOps;\n+    private Vector<T> _localParameterShard;\n+    private Vector<T>? _cachedFullParameters;\n+    private int _shardStartIndex;\n+    private int _shardSize;\n+\n+    /// <inheritdoc/>\n+    public IFullModel<T, TInput, TOutput> WrappedModel => _wrappedModel;\n+\n+    /// <inheritdoc/>\n+    public int Rank => _config.CommunicationBackend.Rank;\n+\n+    /// <inheritdoc/>\n+    public int WorldSize => _config.CommunicationBackend.WorldSize;\n+\n+    /// <inheritdoc/>\n+    public Vector<T> LocalParameterShard => _localParameterShard;\n+\n+    /// <inheritdoc/>\n+    public IShardingConfiguration<T> ShardingConfiguration => _config;\n+\n+    /// <inheritdoc/>\n+    public int ParameterCount => _wrappedModel.ParameterCount;\n+\n+    /// <summary>\n+    /// Creates a new sharded model wrapping an existing model.\n+    ///\n+    /// For Beginners:\n+    /// This constructor takes your existing model and makes it distributed.\n+    /// You provide:\n+    /// 1. The model you want to make distributed\n+    /// 2. A configuration that tells us how to do the distribution\n+    ///\n+    /// The constructor automatically:\n+    /// - Splits the model's parameters across all processes\n+    /// - Sets up communication channels\n+    /// - Prepares everything for distributed training\n+    /// </summary>\n+    /// <param name=\"wrappedModel\">The model to wrap with distributed capabilities</param>\n+    /// <param name=\"config\">Configuration for sharding and communication</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown if model or config is null</exception>\n+    public ShardedModel(IFullModel<T, TInput, TOutput> wrappedModel, IShardingConfiguration<T> config)\n+    {\n+        _wrappedModel = wrappedModel ?? throw new ArgumentNullException(nameof(wrappedModel));\n+        _config = config ?? throw new ArgumentNullException(nameof(config));\n+        _numOps = MathHelper.GetNumericOperations<T>();\n+\n+        // Initialize the communication backend if not already done\n+        if (!_config.CommunicationBackend.IsInitialized)\n+        {\n+            _config.CommunicationBackend.Initialize();\n+        }\n+\n+        // Shard the model parameters across processes\n+        InitializeSharding();\n+    }\n+\n+    /// <summary>\n+    /// Initializes parameter sharding by dividing parameters across processes.\n+    ///\n+    /// For Beginners:\n+    /// This method splits the model's parameters into chunks and gives each\n+    /// process its own chunk to manage. It's like dividing a deck of cards\n+    /// evenly among players.\n+    /// </summary>\n+    private void InitializeSharding()\n+    {\n+        var fullParameters = _wrappedModel.GetParameters();\n+        int totalParams = fullParameters.Length;\n+\n+        // Calculate shard size for this process\n+        int baseShardSize = totalParams / WorldSize;\n+        int remainder = totalParams % WorldSize;\n+\n+        // Distribute remainder among first 'remainder' processes\n+        _shardSize = baseShardSize + (Rank < remainder ? 1 : 0);\n+        _shardStartIndex = Rank * baseShardSize + Math.Min(Rank, remainder);\n+\n+        // Extract local shard\n+        var shardData = new T[_shardSize];\n+        Array.Copy(fullParameters.ToArray(), _shardStartIndex, shardData, 0, _shardSize);\n+        _localParameterShard = new Vector<T>(shardData);\n+\n+        // Cache invalidated\n+        _cachedFullParameters = null;\n+    }\n+\n+    /// <inheritdoc/>\n+    public Vector<T> GatherFullParameters()\n+    {\n+        // Use cached version if available\n+        if (_cachedFullParameters != null)\n+        {\n+            return _cachedFullParameters;\n+        }\n+\n+        // Gather parameters from all processes\n+        var gathered = _config.CommunicationBackend.AllGather(_localParameterShard);\n+        _cachedFullParameters = gathered;\n+        return gathered;\n+    }\n+\n+    /// <inheritdoc/>\n+    public void SynchronizeGradients()\n+    {\n+        // Get gradients from wrapped model\n+        // Note: This assumes the model exposes gradients somehow.\n+        // For now, we'll just synchronize the local parameter shard\n+        // In a real implementation, gradients would be tracked separately\n+\n+        // Perform AllReduce with average operation\n+        _config.CommunicationBackend.AllReduce(_localParameterShard, ReductionOperation.Average);\n+\n+        // Invalidate cached full parameters\n+        _cachedFullParameters = null;\n+    }\n+\n+    /// <inheritdoc/>\n+    public void Train(TInput input, TOutput expectedOutput)\n+    {\n+        // Gather full parameters for training\n+        var fullParams = GatherFullParameters();\n+        _wrappedModel.SetParameters(fullParams);\n+\n+        // Train the wrapped model\n+        _wrappedModel.Train(input, expectedOutput);\n+\n+        // Get updated parameters\n+        var updatedParams = _wrappedModel.GetParameters();\n+\n+        // Update local shard\n+        var shardData = new T[_shardSize];\n+        Array.Copy(updatedParams.ToArray(), _shardStartIndex, shardData, 0, _shardSize);\n+        _localParameterShard = new Vector<T>(shardData);\n+\n+        // Synchronize gradients if auto-sync is enabled\n+        if (_config.AutoSyncGradients)\n+        {\n+            SynchronizeGradients();\n+\n+            // Apply synchronized parameters back to the model\n+            fullParams = GatherFullParameters();\n+            _wrappedModel.SetParameters(fullParams);\n+        }\n+\n+        // Invalidate cache\n+        _cachedFullParameters = null;\n+    }\n+\n+    /// <inheritdoc/>\n+    public TOutput Predict(TInput input)\n+    {\n+        // Gather full parameters for prediction\n+        var fullParams = GatherFullParameters();\n+        _wrappedModel.SetParameters(fullParams);\n+\n+        // Use wrapped model for prediction\n+        return _wrappedModel.Predict(input);\n+    }"}]}},{"id":"PRRT_kwDOKSXUF85g9V8p","isResolved":true,"isOutdated":true,"path":"src/DistributedTraining/ShardedOptimizer.cs","line":null,"originalLine":254,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VG0z8","author":{"login":"copilot-pull-request-reviewer"},"body":"Race condition: Similar to ShardedModel, this has the same issue where rank 0 saves while all processes wait at a barrier. If rank 0 fails between the rank check and the barrier, deadlock will occur.\n```suggestion\n        bool saveSucceeded = true;\n        if (Rank == 0)\n        {\n            try\n            {\n                var data = Serialize();\n                File.WriteAllBytes(filePath, data);\n            }\n            catch\n            {\n                saveSucceeded = false;\n            }\n        }\n\n        // Broadcast saveSucceeded from rank 0 to all ranks\n        saveSucceeded = _config.CommunicationBackend.Broadcast(saveSucceeded, 0);\n        if (!saveSucceeded)\n        {\n            throw new IOException(\"Rank 0 failed to save the model file. Aborting SaveModel on all ranks.\");\n        }\n\n        // Wait for rank 0 to finish writing (all ranks now know save succeeded)\n```","createdAt":"2025-11-07T03:54:06Z","path":"src/DistributedTraining/ShardedOptimizer.cs","line":null,"originalLine":254,"diffHunk":"@@ -0,0 +1,268 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.Models.Inputs;\n+using AiDotNet.Optimizers;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Implements a distributed optimizer wrapper that coordinates optimization across multiple processes.\n+///\n+/// For Beginners:\n+/// This class wraps any existing optimizer (like Adam, SGD, etc.) and makes it work across\n+/// multiple GPUs or machines. It automatically handles:\n+/// - Synchronizing gradients across all processes\n+/// - Coordinating parameter updates\n+/// - Ensuring all processes stay in sync\n+///\n+/// Think of it like a team of coaches working together - each has their own expertise\n+/// (the wrapped optimizer), but they coordinate their efforts to train the team effectively.\n+///\n+/// Example:\n+/// <code>\n+/// // Original optimizer\n+/// var optimizer = new AdamOptimizer&lt;double, Tensor&lt;double&gt;, Tensor&lt;double&gt;&gt;(model, options);\n+///\n+/// // Wrap it for distributed training\n+/// var backend = new InMemoryCommunicationBackend&lt;double&gt;(rank: 0, worldSize: 4);\n+/// var config = new ShardingConfiguration&lt;double&gt;(backend);\n+/// var distributedOptimizer = new ShardedOptimizer&lt;double, Tensor&lt;double&gt;, Tensor&lt;double&gt;&gt;(\n+///     optimizer, config);\n+///\n+/// // Now optimize as usual - distributed magic happens automatically!\n+/// var result = distributedOptimizer.Optimize(inputData);\n+/// </code>\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type</typeparam>\n+/// <typeparam name=\"TInput\">The input type for the model</typeparam>\n+/// <typeparam name=\"TOutput\">The output type for the model</typeparam>\n+public class ShardedOptimizer<T, TInput, TOutput> : IShardedOptimizer<T, TInput, TOutput> where T : struct\n+{\n+    private readonly IOptimizer<T, TInput, TOutput> _wrappedOptimizer;\n+    private readonly IShardingConfiguration<T> _config;\n+\n+    /// <inheritdoc/>\n+    public IOptimizer<T, TInput, TOutput> WrappedOptimizer => _wrappedOptimizer;\n+\n+    /// <inheritdoc/>\n+    public int Rank => _config.CommunicationBackend.Rank;\n+\n+    /// <inheritdoc/>\n+    public int WorldSize => _config.CommunicationBackend.WorldSize;\n+\n+    /// <inheritdoc/>\n+    public IShardingConfiguration<T> ShardingConfiguration => _config;\n+\n+    /// <summary>\n+    /// Creates a new sharded optimizer wrapping an existing optimizer.\n+    ///\n+    /// For Beginners:\n+    /// This constructor takes your existing optimizer and makes it distributed.\n+    /// You provide:\n+    /// 1. The optimizer you want to make distributed\n+    /// 2. A configuration that tells us how to do the distribution\n+    ///\n+    /// The optimizer will automatically synchronize across all processes during optimization.\n+    /// </summary>\n+    /// <param name=\"wrappedOptimizer\">The optimizer to wrap with distributed capabilities</param>\n+    /// <param name=\"config\">Configuration for sharding and communication</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown if optimizer or config is null</exception>\n+    public ShardedOptimizer(\n+        IOptimizer<T, TInput, TOutput> wrappedOptimizer,\n+        IShardingConfiguration<T> config)\n+    {\n+        _wrappedOptimizer = wrappedOptimizer ?? throw new ArgumentNullException(nameof(wrappedOptimizer));\n+        _config = config ?? throw new ArgumentNullException(nameof(config));\n+\n+        // Initialize the communication backend if not already done\n+        if (!_config.CommunicationBackend.IsInitialized)\n+        {\n+            _config.CommunicationBackend.Initialize();\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public OptimizationResult<T, TInput, TOutput> Optimize(OptimizationInputData<T, TInput, TOutput> inputData)\n+    {\n+        if (inputData == null)\n+        {\n+            throw new ArgumentNullException(nameof(inputData));\n+        }\n+\n+        // Ensure all processes start together\n+        _config.CommunicationBackend.Barrier();\n+\n+        // If the model is a sharded model, it will handle gradient synchronization\n+        // Otherwise, we need to handle it ourselves\n+        bool modelHandlesSync = inputData.Model is IShardedModel<T, TInput, TOutput>;\n+\n+        // Perform optimization on the wrapped optimizer\n+        var result = _wrappedOptimizer.Optimize(inputData);\n+\n+        // If model doesn't handle synchronization and auto-sync is enabled, sync parameters\n+        if (!modelHandlesSync && _config.AutoSyncGradients)\n+        {\n+            SynchronizeParameters(result.BestModel);\n+        }\n+\n+        // Synchronize optimizer state if needed\n+        SynchronizeOptimizerState();\n+\n+        // Ensure all processes finish together\n+        _config.CommunicationBackend.Barrier();\n+\n+        return result;\n+    }\n+\n+    /// <inheritdoc/>\n+    public void SynchronizeOptimizerState()\n+    {\n+        // For now, this is a placeholder\n+        // In a full implementation, we would synchronize optimizer-specific state\n+        // like momentum buffers, variance estimates (for Adam), etc.\n+\n+        // Different optimizers have different state to sync:\n+        // - SGD with momentum: velocity vectors\n+        // - Adam: first and second moment estimates\n+        // - RMSprop: squared gradient moving average\n+\n+        // This would require either:\n+        // 1. Extending IOptimizer with state access methods\n+        // 2. Type-specific handling for known optimizer types\n+        // 3. A generic state serialization mechanism\n+\n+        // For the MVP, we assume stateless or that the wrapped optimizer handles its own state\n+    }\n+\n+    /// <summary>\n+    /// Synchronizes model parameters across all processes.\n+    ///\n+    /// For Beginners:\n+    /// After optimization, each process might have slightly different parameters\n+    /// (if they processed different data). This method averages the parameters\n+    /// across all processes so everyone has the same model.\n+    /// </summary>\n+    /// <param name=\"model\">The model whose parameters to synchronize</param>\n+    private void SynchronizeParameters(IFullModel<T, TInput, TOutput>? model)\n+    {\n+        if (model == null)\n+        {\n+            return;\n+        }\n+\n+        // Don't sync if it's already a sharded model (handles its own sync)\n+        if (model is IShardedModel<T, TInput, TOutput>)\n+        {\n+            return;\n+        }\n+\n+        // Get current parameters\n+        var parameters = model.GetParameters();\n+\n+        // Average parameters across all processes\n+        _config.CommunicationBackend.AllReduce(parameters, ReductionOperation.Average);\n+\n+        // Update model with averaged parameters\n+        model.SetParameters(parameters);\n+    }\n+\n+    /// <inheritdoc/>\n+    public bool ShouldEarlyStop()\n+    {\n+        // Delegate to wrapped optimizer\n+        bool localDecision = _wrappedOptimizer.ShouldEarlyStop();\n+\n+        // In distributed training, we need consensus on early stopping\n+        // All processes should agree to stop, otherwise some might continue while others stop\n+        // For now, we'll use a simple approach: if any process wants to stop, all stop\n+\n+        // Create a vector with the local decision (1 for stop, 0 for continue)\n+        var decision = new Vector<T>(new[] { localDecision ? MathHelper.GetNumericOperations<T>().One : MathHelper.GetNumericOperations<T>().Zero });\n+\n+        // Get the maximum across all processes\n+        // If any process returns 1 (stop), the max will be 1\n+        _config.CommunicationBackend.AllReduce(decision, ReductionOperation.Max);\n+\n+        // Check if the result indicates stopping\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        return !numOps.Equals(decision[0], numOps.Zero);\n+    }\n+\n+    /// <inheritdoc/>\n+    public OptimizationAlgorithmOptions<T, TInput, TOutput> GetOptions()\n+    {\n+        return _wrappedOptimizer.GetOptions();\n+    }\n+\n+    /// <inheritdoc/>\n+    public byte[] Serialize()\n+    {\n+        using var ms = new MemoryStream();\n+        using var writer = new BinaryWriter(ms);\n+\n+        // Serialize sharding configuration info\n+        writer.Write(WorldSize);\n+        writer.Write(Rank);\n+        writer.Write(_config.AutoSyncGradients);\n+        writer.Write(_config.MinimumParameterGroupSize);\n+        writer.Write(_config.EnableGradientCompression);\n+\n+        // Serialize wrapped optimizer\n+        var optimizerData = _wrappedOptimizer.Serialize();\n+        writer.Write(optimizerData.Length);\n+        writer.Write(optimizerData);\n+\n+        return ms.ToArray();\n+    }\n+\n+    /// <inheritdoc/>\n+    public void Deserialize(byte[] data)\n+    {\n+        using var ms = new MemoryStream(data);\n+        using var reader = new BinaryReader(ms);\n+\n+        // Read sharding configuration (for validation)\n+        int savedWorldSize = reader.ReadInt32();\n+        int savedRank = reader.ReadInt32();\n+        reader.ReadBoolean(); // AutoSyncGradients\n+        reader.ReadInt32(); // MinimumParameterGroupSize\n+        reader.ReadBoolean(); // EnableGradientCompression\n+\n+        if (savedWorldSize != WorldSize)\n+        {\n+            throw new InvalidOperationException(\n+                $\"World size mismatch. Optimizer was saved with {savedWorldSize} processes, \" +\n+                $\"but current configuration has {WorldSize} processes.\");\n+        }\n+\n+        // Read wrapped optimizer\n+        int optimizerDataLength = reader.ReadInt32();\n+        byte[] optimizerData = reader.ReadBytes(optimizerDataLength);\n+        _wrappedOptimizer.Deserialize(optimizerData);\n+    }\n+\n+    /// <inheritdoc/>\n+    public void SaveModel(string filePath)\n+    {\n+        // Only rank 0 saves to avoid conflicts\n+        if (Rank == 0)\n+        {\n+            var data = Serialize();\n+            File.WriteAllBytes(filePath, data);\n+        }\n+\n+        // Wait for rank 0 to finish writing"}]}},{"id":"PRRT_kwDOKSXUF85g9V8x","isResolved":false,"isOutdated":true,"path":"src/DistributedTraining/CommunicationManager.cs","line":null,"originalLine":102,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VG00F","author":{"login":"copilot-pull-request-reviewer"},"body":"The CommunicationManager uses static mutable state for backends which could cause issues in concurrent scenarios or unit tests that run in parallel. If one test initializes the manager while another is using it, unexpected behavior could occur. Consider using instance-based management or adding thread-safety warnings to the documentation.","createdAt":"2025-11-07T03:54:07Z","path":"src/DistributedTraining/CommunicationManager.cs","line":null,"originalLine":102,"diffHunk":"@@ -0,0 +1,320 @@\n+using AiDotNet.LinearAlgebra;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Central manager for distributed communication operations.\n+/// Provides a static API for collective communication in distributed training scenarios.\n+///\n+/// For Beginners:\n+/// This is your main entry point for distributed training communication.\n+/// It's a \"wrapper\" that makes it easy to communicate between different processes/GPUs\n+/// without worrying about the underlying implementation details.\n+///\n+/// Example usage:\n+/// <code>\n+/// // Initialize communication (do this once at startup)\n+/// CommunicationManager.Initialize(new InMemoryCommunicationBackend&lt;double&gt;());\n+///\n+/// // Get your process ID and total number of processes\n+/// int myRank = CommunicationManager.GetRank();\n+/// int totalProcesses = CommunicationManager.GetWorldSize();\n+///\n+/// // Average gradients across all processes\n+/// Vector&lt;double&gt; gradients = ...; // Your local gradients\n+/// CommunicationManager.AllReduce(gradients, ReductionOperation.Sum);\n+/// // Now 'gradients' contains the sum from all processes\n+///\n+/// // Clean up when done\n+/// CommunicationManager.Shutdown();\n+/// </code>\n+/// </summary>\n+public static class CommunicationManager\n+{\n+    private static readonly object _lock = new object();\n+    private static ICommunicationBackend<float>? _floatBackend;\n+    private static ICommunicationBackend<double>? _doubleBackend;\n+    private static bool _isInitialized = false;\n+\n+    /// <summary>\n+    /// Gets whether the communication manager has been initialized.\n+    /// </summary>\n+    public static bool IsInitialized\n+    {\n+        get\n+        {\n+            lock (_lock)\n+            {\n+                return _isInitialized;\n+            }\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Initializes the communication manager with the specified backend.\n+    /// This must be called before any other operations.\n+    ///\n+    /// For Beginners:\n+    /// This sets up the communication system. You need to provide a \"backend\"\n+    /// which is the actual implementation that does the communication.\n+    /// For testing, use InMemoryCommunicationBackend. For real distributed training,\n+    /// you would use an MPI backend.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type (float or double)</typeparam>\n+    /// <param name=\"backend\">The communication backend to use</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown if backend is null</exception>\n+    /// <exception cref=\"InvalidOperationException\">Thrown if already initialized</exception>\n+    public static void Initialize<T>(ICommunicationBackend<T> backend) where T : struct\n+    {\n+        if (backend == null)\n+        {\n+            throw new ArgumentNullException(nameof(backend));\n+        }\n+\n+        lock (_lock)\n+        {\n+            if (_isInitialized)\n+            {\n+                throw new InvalidOperationException(\n+                    \"CommunicationManager is already initialized. Call Shutdown() first.\");\n+            }\n+\n+            // Initialize the backend\n+            backend.Initialize();\n+\n+            // Store the backend (type-specific)\n+            if (typeof(T) == typeof(float))\n+            {\n+                _floatBackend = backend as ICommunicationBackend<float>;\n+            }\n+            else if (typeof(T) == typeof(double))\n+            {\n+                _doubleBackend = backend as ICommunicationBackend<double>;\n+            }\n+            else\n+            {\n+                throw new NotSupportedException(\n+                    $\"Type {typeof(T).Name} is not supported for distributed communication. \" +\n+                    \"Use float or double.\");\n+            }\n+\n+            _isInitialized = true;\n+        }"}]}},{"id":"PRRT_kwDOKSXUF85g9V82","isResolved":false,"isOutdated":true,"path":"scripts/launch-distributed-training.ps1","line":null,"originalLine":105,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VG00N","author":{"login":"copilot-pull-request-reviewer"},"body":"Security risk: Similar to the bash script, program arguments are not sanitized before being passed to Start-Process. The `$ProgramArgs.Split(\" \")` approach is also problematic as it doesn't handle quoted arguments correctly (e.g., `--name \"My Model\"` would be split incorrectly). Consider using proper argument parsing or at least document the limitation.","createdAt":"2025-11-07T03:54:07Z","path":"scripts/launch-distributed-training.ps1","line":null,"originalLine":105,"diffHunk":"@@ -0,0 +1,147 @@\n+################################################################################\n+# AiDotNet Distributed Training Launcher (PowerShell)\n+#\n+# This script launches distributed training using MPI across multiple processes.\n+#\n+# For Beginners:\n+# MPI (Message Passing Interface) is a standard for running programs across\n+# multiple computers or processors. Think of it like a coordinator that starts\n+# your program on multiple machines at once and helps them communicate.\n+#\n+# Usage:\n+#   .\\launch-distributed-training.ps1 -NumProcesses <num> -Program <path> [-ProgramArgs <args>]\n+#\n+# Examples:\n+#   # Run on 4 GPUs locally\n+#   .\\launch-distributed-training.ps1 -NumProcesses 4 -Program \".\\MyTrainingApp.exe\"\n+#\n+#   # Run on 8 GPUs with additional arguments\n+#   .\\launch-distributed-training.ps1 -NumProcesses 8 -Program \".\\MyTrainingApp.exe\" -ProgramArgs \"--epochs 100 --lr 0.001\"\n+#\n+#   # Run across 2 machines with 4 GPUs each\n+#   .\\launch-distributed-training.ps1 -NumProcesses 8 -Program \".\\MyTrainingApp.exe\" -Hosts \"machine1,machine2\"\n+################################################################################\n+\n+param(\n+    [Parameter(Mandatory=$true, HelpMessage=\"Number of processes to spawn (typically equals number of GPUs)\")]\n+    [int]$NumProcesses,\n+\n+    [Parameter(Mandatory=$true, HelpMessage=\"Path to your training program executable\")]\n+    [string]$Program,\n+\n+    [Parameter(Mandatory=$false, HelpMessage=\"Additional arguments to pass to your program\")]\n+    [string]$ProgramArgs = \"\",\n+\n+    [Parameter(Mandatory=$false, HelpMessage=\"Comma-separated list of host machines\")]\n+    [string]$Hosts = \"\"\n+)\n+\n+# Display header\n+Write-Host \"======================================\" -ForegroundColor Cyan\n+Write-Host \"AiDotNet Distributed Training Launcher\" -ForegroundColor Cyan\n+Write-Host \"======================================\" -ForegroundColor Cyan\n+Write-Host \"\"\n+\n+# Display configuration\n+Write-Host \"Configuration:\" -ForegroundColor Yellow\n+Write-Host \"  Number of processes: $NumProcesses\"\n+Write-Host \"  Program: $Program\"\n+if ($ProgramArgs) {\n+    Write-Host \"  Program arguments: $ProgramArgs\"\n+}\n+if ($Hosts) {\n+    Write-Host \"  Hosts: $Hosts\"\n+}\n+Write-Host \"\"\n+\n+# Check if mpiexec is available\n+$mpiexec = Get-Command mpiexec -ErrorAction SilentlyContinue\n+\n+if (-not $mpiexec) {\n+    Write-Host \"Error: mpiexec not found in PATH\" -ForegroundColor Red\n+    Write-Host \"\"\n+    Write-Host \"For Beginners:\" -ForegroundColor Yellow\n+    Write-Host \"  You need to install Microsoft MPI to run distributed training on Windows.\"\n+    Write-Host \"  Download from: https://docs.microsoft.com/en-us/message-passing-interface/microsoft-mpi\"\n+    Write-Host \"\"\n+    Write-Host \"  Installation steps:\"\n+    Write-Host \"    1. Download MS-MPI installer\"\n+    Write-Host \"    2. Install both the runtime (msmpisetup.exe) and SDK (msmpisdk.msi)\"\n+    Write-Host \"    3. Restart your terminal/PowerShell\"\n+    exit 1\n+}\n+\n+Write-Host \"Using MPI command: $($mpiexec.Source)\" -ForegroundColor Green\n+Write-Host \"\"\n+\n+# Check if program exists\n+if (-not (Test-Path $Program)) {\n+    Write-Host \"Error: Program '$Program' not found\" -ForegroundColor Red\n+    Write-Host \"\"\n+    Write-Host \"For Beginners:\" -ForegroundColor Yellow\n+    Write-Host \"  Make sure you've built your training program and the path is correct.\"\n+    Write-Host \"  Example: dotnet publish -c Release -o .\\publish\"\n+    Write-Host \"  Then use: -Program '.\\publish\\MyTrainingApp.exe'\"\n+    exit 1\n+}\n+\n+# Build mpiexec command\n+$mpiCommand = \"mpiexec\"\n+$mpiArgsList = @(\n+    \"-n\", $NumProcesses.ToString()\n+)\n+\n+# Add hosts if specified\n+if ($Hosts) {\n+    $mpiArgsList += @(\"-hosts\", $Hosts)\n+}\n+\n+# Add the program\n+$mpiArgsList += $Program\n+\n+# Add program arguments if specified\n+if ($ProgramArgs) {\n+    # Split program args and add them\n+    $mpiArgsList += $ProgramArgs.Split(\" \")"}]}},{"id":"PRRT_kwDOKSXUF85g9V87","isResolved":true,"isOutdated":true,"path":"src/DistributedTraining/InMemoryCommunicationBackend.cs","line":null,"originalLine":239,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VG00U","author":{"login":"copilot-pull-request-reviewer"},"body":"The Count method is called on a list that may be modified by other threads. The variable `contributedCount` should be calculated inside the lock or the entire counting logic should be revised to avoid race conditions. Currently, the count check happens without proper synchronization.\n```suggestion\n            while (true)\n            {\n                int contributedCount = _sharedBuffers[bufferId].Count(v => v != null);\n                if (contributedCount >= _worldSize)\n                {\n                    break;\n                }\n                Monitor.Wait(_globalLock, 10);\n```","createdAt":"2025-11-07T03:54:07Z","path":"src/DistributedTraining/InMemoryCommunicationBackend.cs","line":null,"originalLine":239,"diffHunk":"@@ -0,0 +1,484 @@\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.NumericOperations;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// A simple in-memory implementation of distributed communication for testing and single-machine scenarios.\n+/// This backend simulates multiple processes by using shared memory and locks.\n+///\n+/// For Beginners:\n+/// This is a \"fake\" distributed system that runs on a single machine.\n+/// It's perfect for testing your distributed code without needing multiple GPUs or machines.\n+/// Think of it as a practice mode - it simulates distributed behavior but everything\n+/// runs in one process.\n+///\n+/// Example:\n+/// <code>\n+/// // Create a simulated distributed environment with 4 \"processes\"\n+/// var backend = new InMemoryCommunicationBackend&lt;double&gt;(rank: 0, worldSize: 4);\n+/// CommunicationManager.Initialize(backend);\n+/// </code>\n+///\n+/// Note: For production distributed training, you would use an MPI-based backend instead.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type for operations</typeparam>\n+public class InMemoryCommunicationBackend<T> : ICommunicationBackend<T> where T : struct\n+{\n+    private readonly int _rank;\n+    private readonly int _worldSize;\n+    private readonly INumericOperations<T> _numOps;\n+    private bool _isInitialized;\n+\n+    // Shared state for simulating collective operations\n+    // In a real implementation, this would be handled by the MPI backend\n+    private static readonly object _globalLock = new object();\n+    private static readonly Dictionary<string, List<Vector<T>>> _sharedBuffers = new();\n+    private static readonly Dictionary<string, int> _barrierCounters = new();\n+\n+    /// <inheritdoc/>\n+    public int Rank => _rank;\n+\n+    /// <inheritdoc/>\n+    public int WorldSize => _worldSize;\n+\n+    /// <inheritdoc/>\n+    public bool IsInitialized => _isInitialized;\n+\n+    /// <summary>\n+    /// Creates a new in-memory communication backend.\n+    ///\n+    /// For Beginners:\n+    /// You create one of these for each simulated \"process\". If you want to simulate\n+    /// 4 GPUs, you create 4 instances with ranks 0, 1, 2, 3, all with worldSize=4.\n+    /// </summary>\n+    /// <param name=\"rank\">The rank (ID) of this simulated process (0-based)</param>\n+    /// <param name=\"worldSize\">The total number of simulated processes</param>\n+    /// <exception cref=\"ArgumentException\">Thrown if rank or worldSize are invalid</exception>\n+    public InMemoryCommunicationBackend(int rank, int worldSize)\n+    {\n+        if (rank < 0 || rank >= worldSize)\n+        {\n+            throw new ArgumentException(\n+                $\"Invalid rank {rank}. Must be between 0 and {worldSize - 1}.\");\n+        }\n+\n+        if (worldSize <= 0)\n+        {\n+            throw new ArgumentException(\n+                $\"Invalid worldSize {worldSize}. Must be positive.\");\n+        }\n+\n+        _rank = rank;\n+        _worldSize = worldSize;\n+        _numOps = MathHelper.GetNumericOperations<T>();\n+        _isInitialized = false;\n+    }\n+\n+    /// <inheritdoc/>\n+    public void Initialize()\n+    {\n+        lock (_globalLock)\n+        {\n+            if (_isInitialized)\n+            {\n+                return;\n+            }\n+\n+            _isInitialized = true;\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public void Shutdown()\n+    {\n+        lock (_globalLock)\n+        {\n+            if (!_isInitialized)\n+            {\n+                return;\n+            }\n+\n+            // Clear any remaining shared state\n+            _sharedBuffers.Clear();\n+            _barrierCounters.Clear();\n+\n+            _isInitialized = false;\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public void Barrier()\n+    {\n+        EnsureInitialized();\n+\n+        lock (_globalLock)\n+        {\n+            string barrierId = $\"barrier_{DateTime.UtcNow.Ticks}\";\n+\n+            if (!_barrierCounters.ContainsKey(barrierId))\n+            {\n+                _barrierCounters[barrierId] = 0;\n+            }\n+\n+            _barrierCounters[barrierId]++;\n+\n+            // Wait until all processes have reached the barrier\n+            while (_barrierCounters[barrierId] < _worldSize)\n+            {\n+                Monitor.Wait(_globalLock, 10);\n+            }\n+\n+            Monitor.PulseAll(_globalLock);\n+\n+            // Cleanup\n+            if (_rank == 0)\n+            {\n+                _barrierCounters.Remove(barrierId);\n+            }\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public void AllReduce(Vector<T> data, ReductionOperation operation)\n+    {\n+        EnsureInitialized();\n+\n+        if (data == null)\n+        {\n+            throw new ArgumentNullException(nameof(data));\n+        }\n+\n+        // For single process, no communication needed\n+        if (_worldSize == 1)\n+        {\n+            if (operation == ReductionOperation.Average)\n+            {\n+                // Already averaged (only one value)\n+            }\n+            return;\n+        }\n+\n+        string bufferId = $\"allreduce_{Guid.NewGuid()}\";\n+\n+        lock (_globalLock)\n+        {\n+            // Initialize shared buffer\n+            if (!_sharedBuffers.ContainsKey(bufferId))\n+            {\n+                _sharedBuffers[bufferId] = new List<Vector<T>>();\n+            }\n+\n+            // Contribute local data\n+            _sharedBuffers[bufferId].Add(data.Clone());\n+\n+            // Wait until all processes have contributed\n+            while (_sharedBuffers[bufferId].Count < _worldSize)\n+            {\n+                Monitor.Wait(_globalLock, 10);\n+            }\n+\n+            Monitor.PulseAll(_globalLock);\n+\n+            // Perform reduction\n+            var allData = _sharedBuffers[bufferId];\n+            var result = PerformReduction(allData, operation);\n+\n+            // Copy result back to input data\n+            for (int i = 0; i < data.Length; i++)\n+            {\n+                data[i] = result[i];\n+            }\n+\n+            // Cleanup (rank 0 cleans up)\n+            if (_rank == 0)\n+            {\n+                _sharedBuffers.Remove(bufferId);\n+            }\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public Vector<T> AllGather(Vector<T> sendData)\n+    {\n+        EnsureInitialized();\n+\n+        if (sendData == null)\n+        {\n+            throw new ArgumentNullException(nameof(sendData));\n+        }\n+\n+        // For single process, just return a copy\n+        if (_worldSize == 1)\n+        {\n+            return sendData.Clone();\n+        }\n+\n+        string bufferId = $\"allgather_{Guid.NewGuid()}\";\n+\n+        lock (_globalLock)\n+        {\n+            // Initialize shared buffer\n+            if (!_sharedBuffers.ContainsKey(bufferId))\n+            {\n+                _sharedBuffers[bufferId] = new List<Vector<T>>(new Vector<T>[_worldSize]);\n+            }\n+\n+            // Contribute local data\n+            _sharedBuffers[bufferId][_rank] = sendData.Clone();\n+\n+            // Wait until all processes have contributed\n+            int contributedCount = 0;\n+            while (contributedCount < _worldSize)\n+            {\n+                contributedCount = _sharedBuffers[bufferId].Count(v => v != null);\n+                if (contributedCount < _worldSize)\n+                {\n+                    Monitor.Wait(_globalLock, 10);\n+                }"}]}},{"id":"PRRT_kwDOKSXUF85g9V8_","isResolved":false,"isOutdated":true,"path":"src/DistributedTraining/InMemoryCommunicationBackend.cs","line":null,"originalLine":200,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VG00b","author":{"login":"copilot-pull-request-reviewer"},"body":"The AllReduce operation modifies the input `data` vector in place, but the Broadcast and AllGather operations return new vectors. This inconsistency in the API could be confusing. Consider documenting this clearly or making the API more consistent (either all modify in-place or all return new instances).","createdAt":"2025-11-07T03:54:07Z","path":"src/DistributedTraining/InMemoryCommunicationBackend.cs","line":null,"originalLine":200,"diffHunk":"@@ -0,0 +1,484 @@\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.NumericOperations;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// A simple in-memory implementation of distributed communication for testing and single-machine scenarios.\n+/// This backend simulates multiple processes by using shared memory and locks.\n+///\n+/// For Beginners:\n+/// This is a \"fake\" distributed system that runs on a single machine.\n+/// It's perfect for testing your distributed code without needing multiple GPUs or machines.\n+/// Think of it as a practice mode - it simulates distributed behavior but everything\n+/// runs in one process.\n+///\n+/// Example:\n+/// <code>\n+/// // Create a simulated distributed environment with 4 \"processes\"\n+/// var backend = new InMemoryCommunicationBackend&lt;double&gt;(rank: 0, worldSize: 4);\n+/// CommunicationManager.Initialize(backend);\n+/// </code>\n+///\n+/// Note: For production distributed training, you would use an MPI-based backend instead.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type for operations</typeparam>\n+public class InMemoryCommunicationBackend<T> : ICommunicationBackend<T> where T : struct\n+{\n+    private readonly int _rank;\n+    private readonly int _worldSize;\n+    private readonly INumericOperations<T> _numOps;\n+    private bool _isInitialized;\n+\n+    // Shared state for simulating collective operations\n+    // In a real implementation, this would be handled by the MPI backend\n+    private static readonly object _globalLock = new object();\n+    private static readonly Dictionary<string, List<Vector<T>>> _sharedBuffers = new();\n+    private static readonly Dictionary<string, int> _barrierCounters = new();\n+\n+    /// <inheritdoc/>\n+    public int Rank => _rank;\n+\n+    /// <inheritdoc/>\n+    public int WorldSize => _worldSize;\n+\n+    /// <inheritdoc/>\n+    public bool IsInitialized => _isInitialized;\n+\n+    /// <summary>\n+    /// Creates a new in-memory communication backend.\n+    ///\n+    /// For Beginners:\n+    /// You create one of these for each simulated \"process\". If you want to simulate\n+    /// 4 GPUs, you create 4 instances with ranks 0, 1, 2, 3, all with worldSize=4.\n+    /// </summary>\n+    /// <param name=\"rank\">The rank (ID) of this simulated process (0-based)</param>\n+    /// <param name=\"worldSize\">The total number of simulated processes</param>\n+    /// <exception cref=\"ArgumentException\">Thrown if rank or worldSize are invalid</exception>\n+    public InMemoryCommunicationBackend(int rank, int worldSize)\n+    {\n+        if (rank < 0 || rank >= worldSize)\n+        {\n+            throw new ArgumentException(\n+                $\"Invalid rank {rank}. Must be between 0 and {worldSize - 1}.\");\n+        }\n+\n+        if (worldSize <= 0)\n+        {\n+            throw new ArgumentException(\n+                $\"Invalid worldSize {worldSize}. Must be positive.\");\n+        }\n+\n+        _rank = rank;\n+        _worldSize = worldSize;\n+        _numOps = MathHelper.GetNumericOperations<T>();\n+        _isInitialized = false;\n+    }\n+\n+    /// <inheritdoc/>\n+    public void Initialize()\n+    {\n+        lock (_globalLock)\n+        {\n+            if (_isInitialized)\n+            {\n+                return;\n+            }\n+\n+            _isInitialized = true;\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public void Shutdown()\n+    {\n+        lock (_globalLock)\n+        {\n+            if (!_isInitialized)\n+            {\n+                return;\n+            }\n+\n+            // Clear any remaining shared state\n+            _sharedBuffers.Clear();\n+            _barrierCounters.Clear();\n+\n+            _isInitialized = false;\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public void Barrier()\n+    {\n+        EnsureInitialized();\n+\n+        lock (_globalLock)\n+        {\n+            string barrierId = $\"barrier_{DateTime.UtcNow.Ticks}\";\n+\n+            if (!_barrierCounters.ContainsKey(barrierId))\n+            {\n+                _barrierCounters[barrierId] = 0;\n+            }\n+\n+            _barrierCounters[barrierId]++;\n+\n+            // Wait until all processes have reached the barrier\n+            while (_barrierCounters[barrierId] < _worldSize)\n+            {\n+                Monitor.Wait(_globalLock, 10);\n+            }\n+\n+            Monitor.PulseAll(_globalLock);\n+\n+            // Cleanup\n+            if (_rank == 0)\n+            {\n+                _barrierCounters.Remove(barrierId);\n+            }\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public void AllReduce(Vector<T> data, ReductionOperation operation)\n+    {\n+        EnsureInitialized();\n+\n+        if (data == null)\n+        {\n+            throw new ArgumentNullException(nameof(data));\n+        }\n+\n+        // For single process, no communication needed\n+        if (_worldSize == 1)\n+        {\n+            if (operation == ReductionOperation.Average)\n+            {\n+                // Already averaged (only one value)\n+            }\n+            return;\n+        }\n+\n+        string bufferId = $\"allreduce_{Guid.NewGuid()}\";\n+\n+        lock (_globalLock)\n+        {\n+            // Initialize shared buffer\n+            if (!_sharedBuffers.ContainsKey(bufferId))\n+            {\n+                _sharedBuffers[bufferId] = new List<Vector<T>>();\n+            }\n+\n+            // Contribute local data\n+            _sharedBuffers[bufferId].Add(data.Clone());\n+\n+            // Wait until all processes have contributed\n+            while (_sharedBuffers[bufferId].Count < _worldSize)\n+            {\n+                Monitor.Wait(_globalLock, 10);\n+            }\n+\n+            Monitor.PulseAll(_globalLock);\n+\n+            // Perform reduction\n+            var allData = _sharedBuffers[bufferId];\n+            var result = PerformReduction(allData, operation);\n+\n+            // Copy result back to input data\n+            for (int i = 0; i < data.Length; i++)\n+            {\n+                data[i] = result[i];\n+            }\n+\n+            // Cleanup (rank 0 cleans up)\n+            if (_rank == 0)\n+            {\n+                _sharedBuffers.Remove(bufferId);\n+            }\n+        }\n+    }"}]}},{"id":"PRRT_kwDOKSXUF85g9V9B","isResolved":true,"isOutdated":false,"path":"src/DistributedTraining/FSDPOptimizer.cs","line":194,"originalLine":226,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VG00f","author":{"login":"copilot-pull-request-reviewer"},"body":"Missing validation: Similar to ShardedModel, the `savedRank` is read but not validated against the current rank. This could lead to confusion if a model is loaded on a different rank than where it was saved.","createdAt":"2025-11-07T03:54:08Z","path":"src/DistributedTraining/FSDPOptimizer.cs","line":194,"originalLine":226,"diffHunk":"@@ -0,0 +1,268 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.Models.Inputs;\n+using AiDotNet.Optimizers;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Implements a distributed optimizer wrapper that coordinates optimization across multiple processes.\n+///\n+/// For Beginners:\n+/// This class wraps any existing optimizer (like Adam, SGD, etc.) and makes it work across\n+/// multiple GPUs or machines. It automatically handles:\n+/// - Synchronizing gradients across all processes\n+/// - Coordinating parameter updates\n+/// - Ensuring all processes stay in sync\n+///\n+/// Think of it like a team of coaches working together - each has their own expertise\n+/// (the wrapped optimizer), but they coordinate their efforts to train the team effectively.\n+///\n+/// Example:\n+/// <code>\n+/// // Original optimizer\n+/// var optimizer = new AdamOptimizer&lt;double, Tensor&lt;double&gt;, Tensor&lt;double&gt;&gt;(model, options);\n+///\n+/// // Wrap it for distributed training\n+/// var backend = new InMemoryCommunicationBackend&lt;double&gt;(rank: 0, worldSize: 4);\n+/// var config = new ShardingConfiguration&lt;double&gt;(backend);\n+/// var distributedOptimizer = new ShardedOptimizer&lt;double, Tensor&lt;double&gt;, Tensor&lt;double&gt;&gt;(\n+///     optimizer, config);\n+///\n+/// // Now optimize as usual - distributed magic happens automatically!\n+/// var result = distributedOptimizer.Optimize(inputData);\n+/// </code>\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type</typeparam>\n+/// <typeparam name=\"TInput\">The input type for the model</typeparam>\n+/// <typeparam name=\"TOutput\">The output type for the model</typeparam>\n+public class ShardedOptimizer<T, TInput, TOutput> : IShardedOptimizer<T, TInput, TOutput> where T : struct\n+{\n+    private readonly IOptimizer<T, TInput, TOutput> _wrappedOptimizer;\n+    private readonly IShardingConfiguration<T> _config;\n+\n+    /// <inheritdoc/>\n+    public IOptimizer<T, TInput, TOutput> WrappedOptimizer => _wrappedOptimizer;\n+\n+    /// <inheritdoc/>\n+    public int Rank => _config.CommunicationBackend.Rank;\n+\n+    /// <inheritdoc/>\n+    public int WorldSize => _config.CommunicationBackend.WorldSize;\n+\n+    /// <inheritdoc/>\n+    public IShardingConfiguration<T> ShardingConfiguration => _config;\n+\n+    /// <summary>\n+    /// Creates a new sharded optimizer wrapping an existing optimizer.\n+    ///\n+    /// For Beginners:\n+    /// This constructor takes your existing optimizer and makes it distributed.\n+    /// You provide:\n+    /// 1. The optimizer you want to make distributed\n+    /// 2. A configuration that tells us how to do the distribution\n+    ///\n+    /// The optimizer will automatically synchronize across all processes during optimization.\n+    /// </summary>\n+    /// <param name=\"wrappedOptimizer\">The optimizer to wrap with distributed capabilities</param>\n+    /// <param name=\"config\">Configuration for sharding and communication</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown if optimizer or config is null</exception>\n+    public ShardedOptimizer(\n+        IOptimizer<T, TInput, TOutput> wrappedOptimizer,\n+        IShardingConfiguration<T> config)\n+    {\n+        _wrappedOptimizer = wrappedOptimizer ?? throw new ArgumentNullException(nameof(wrappedOptimizer));\n+        _config = config ?? throw new ArgumentNullException(nameof(config));\n+\n+        // Initialize the communication backend if not already done\n+        if (!_config.CommunicationBackend.IsInitialized)\n+        {\n+            _config.CommunicationBackend.Initialize();\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public OptimizationResult<T, TInput, TOutput> Optimize(OptimizationInputData<T, TInput, TOutput> inputData)\n+    {\n+        if (inputData == null)\n+        {\n+            throw new ArgumentNullException(nameof(inputData));\n+        }\n+\n+        // Ensure all processes start together\n+        _config.CommunicationBackend.Barrier();\n+\n+        // If the model is a sharded model, it will handle gradient synchronization\n+        // Otherwise, we need to handle it ourselves\n+        bool modelHandlesSync = inputData.Model is IShardedModel<T, TInput, TOutput>;\n+\n+        // Perform optimization on the wrapped optimizer\n+        var result = _wrappedOptimizer.Optimize(inputData);\n+\n+        // If model doesn't handle synchronization and auto-sync is enabled, sync parameters\n+        if (!modelHandlesSync && _config.AutoSyncGradients)\n+        {\n+            SynchronizeParameters(result.BestModel);\n+        }\n+\n+        // Synchronize optimizer state if needed\n+        SynchronizeOptimizerState();\n+\n+        // Ensure all processes finish together\n+        _config.CommunicationBackend.Barrier();\n+\n+        return result;\n+    }\n+\n+    /// <inheritdoc/>\n+    public void SynchronizeOptimizerState()\n+    {\n+        // For now, this is a placeholder\n+        // In a full implementation, we would synchronize optimizer-specific state\n+        // like momentum buffers, variance estimates (for Adam), etc.\n+\n+        // Different optimizers have different state to sync:\n+        // - SGD with momentum: velocity vectors\n+        // - Adam: first and second moment estimates\n+        // - RMSprop: squared gradient moving average\n+\n+        // This would require either:\n+        // 1. Extending IOptimizer with state access methods\n+        // 2. Type-specific handling for known optimizer types\n+        // 3. A generic state serialization mechanism\n+\n+        // For the MVP, we assume stateless or that the wrapped optimizer handles its own state\n+    }\n+\n+    /// <summary>\n+    /// Synchronizes model parameters across all processes.\n+    ///\n+    /// For Beginners:\n+    /// After optimization, each process might have slightly different parameters\n+    /// (if they processed different data). This method averages the parameters\n+    /// across all processes so everyone has the same model.\n+    /// </summary>\n+    /// <param name=\"model\">The model whose parameters to synchronize</param>\n+    private void SynchronizeParameters(IFullModel<T, TInput, TOutput>? model)\n+    {\n+        if (model == null)\n+        {\n+            return;\n+        }\n+\n+        // Don't sync if it's already a sharded model (handles its own sync)\n+        if (model is IShardedModel<T, TInput, TOutput>)\n+        {\n+            return;\n+        }\n+\n+        // Get current parameters\n+        var parameters = model.GetParameters();\n+\n+        // Average parameters across all processes\n+        _config.CommunicationBackend.AllReduce(parameters, ReductionOperation.Average);\n+\n+        // Update model with averaged parameters\n+        model.SetParameters(parameters);\n+    }\n+\n+    /// <inheritdoc/>\n+    public bool ShouldEarlyStop()\n+    {\n+        // Delegate to wrapped optimizer\n+        bool localDecision = _wrappedOptimizer.ShouldEarlyStop();\n+\n+        // In distributed training, we need consensus on early stopping\n+        // All processes should agree to stop, otherwise some might continue while others stop\n+        // For now, we'll use a simple approach: if any process wants to stop, all stop\n+\n+        // Create a vector with the local decision (1 for stop, 0 for continue)\n+        var decision = new Vector<T>(new[] { localDecision ? MathHelper.GetNumericOperations<T>().One : MathHelper.GetNumericOperations<T>().Zero });\n+\n+        // Get the maximum across all processes\n+        // If any process returns 1 (stop), the max will be 1\n+        _config.CommunicationBackend.AllReduce(decision, ReductionOperation.Max);\n+\n+        // Check if the result indicates stopping\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        return !numOps.Equals(decision[0], numOps.Zero);\n+    }\n+\n+    /// <inheritdoc/>\n+    public OptimizationAlgorithmOptions<T, TInput, TOutput> GetOptions()\n+    {\n+        return _wrappedOptimizer.GetOptions();\n+    }\n+\n+    /// <inheritdoc/>\n+    public byte[] Serialize()\n+    {\n+        using var ms = new MemoryStream();\n+        using var writer = new BinaryWriter(ms);\n+\n+        // Serialize sharding configuration info\n+        writer.Write(WorldSize);\n+        writer.Write(Rank);\n+        writer.Write(_config.AutoSyncGradients);\n+        writer.Write(_config.MinimumParameterGroupSize);\n+        writer.Write(_config.EnableGradientCompression);\n+\n+        // Serialize wrapped optimizer\n+        var optimizerData = _wrappedOptimizer.Serialize();\n+        writer.Write(optimizerData.Length);\n+        writer.Write(optimizerData);\n+\n+        return ms.ToArray();\n+    }\n+\n+    /// <inheritdoc/>\n+    public void Deserialize(byte[] data)\n+    {\n+        using var ms = new MemoryStream(data);\n+        using var reader = new BinaryReader(ms);\n+\n+        // Read sharding configuration (for validation)\n+        int savedWorldSize = reader.ReadInt32();\n+        int savedRank = reader.ReadInt32();"}]}},{"id":"PRRT_kwDOKSXUF85g9V9E","isResolved":true,"isOutdated":true,"path":"src/DistributedTraining/CommunicationManager.cs","line":null,"originalLine":79,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VG00j","author":{"login":"copilot-pull-request-reviewer"},"body":"The exception messages for already initialized (line 78-79) and not initialized (line 291-292) states are clear, but it would be helpful to include guidance on how to properly handle these situations. For example, \"Call Shutdown() first, or check IsInitialized before calling Initialize().\"\n```suggestion\n                    \"CommunicationManager is already initialized. Call Shutdown() first, or check IsInitialized before calling Initialize().\");\n```","createdAt":"2025-11-07T03:54:08Z","path":"src/DistributedTraining/CommunicationManager.cs","line":null,"originalLine":79,"diffHunk":"@@ -0,0 +1,320 @@\n+using AiDotNet.LinearAlgebra;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Central manager for distributed communication operations.\n+/// Provides a static API for collective communication in distributed training scenarios.\n+///\n+/// For Beginners:\n+/// This is your main entry point for distributed training communication.\n+/// It's a \"wrapper\" that makes it easy to communicate between different processes/GPUs\n+/// without worrying about the underlying implementation details.\n+///\n+/// Example usage:\n+/// <code>\n+/// // Initialize communication (do this once at startup)\n+/// CommunicationManager.Initialize(new InMemoryCommunicationBackend&lt;double&gt;());\n+///\n+/// // Get your process ID and total number of processes\n+/// int myRank = CommunicationManager.GetRank();\n+/// int totalProcesses = CommunicationManager.GetWorldSize();\n+///\n+/// // Average gradients across all processes\n+/// Vector&lt;double&gt; gradients = ...; // Your local gradients\n+/// CommunicationManager.AllReduce(gradients, ReductionOperation.Sum);\n+/// // Now 'gradients' contains the sum from all processes\n+///\n+/// // Clean up when done\n+/// CommunicationManager.Shutdown();\n+/// </code>\n+/// </summary>\n+public static class CommunicationManager\n+{\n+    private static readonly object _lock = new object();\n+    private static ICommunicationBackend<float>? _floatBackend;\n+    private static ICommunicationBackend<double>? _doubleBackend;\n+    private static bool _isInitialized = false;\n+\n+    /// <summary>\n+    /// Gets whether the communication manager has been initialized.\n+    /// </summary>\n+    public static bool IsInitialized\n+    {\n+        get\n+        {\n+            lock (_lock)\n+            {\n+                return _isInitialized;\n+            }\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Initializes the communication manager with the specified backend.\n+    /// This must be called before any other operations.\n+    ///\n+    /// For Beginners:\n+    /// This sets up the communication system. You need to provide a \"backend\"\n+    /// which is the actual implementation that does the communication.\n+    /// For testing, use InMemoryCommunicationBackend. For real distributed training,\n+    /// you would use an MPI backend.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type (float or double)</typeparam>\n+    /// <param name=\"backend\">The communication backend to use</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown if backend is null</exception>\n+    /// <exception cref=\"InvalidOperationException\">Thrown if already initialized</exception>\n+    public static void Initialize<T>(ICommunicationBackend<T> backend) where T : struct\n+    {\n+        if (backend == null)\n+        {\n+            throw new ArgumentNullException(nameof(backend));\n+        }\n+\n+        lock (_lock)\n+        {\n+            if (_isInitialized)\n+            {\n+                throw new InvalidOperationException(\n+                    \"CommunicationManager is already initialized. Call Shutdown() first.\");"}]}},{"id":"PRRT_kwDOKSXUF85g9V9I","isResolved":true,"isOutdated":true,"path":"src/DistributedTraining/ParameterAnalyzer.cs","line":null,"originalLine":218,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VG00o","author":{"login":"copilot-pull-request-reviewer"},"body":"The parameter validation logic could be improved. The calculation `totalParams / (_worldSize * 4)` uses a magic number 4 without explanation. This should be documented or made into a named constant explaining why the target is \"total parameters divided by world size times 4\".","createdAt":"2025-11-07T03:54:08Z","path":"src/DistributedTraining/ParameterAnalyzer.cs","line":null,"originalLine":218,"diffHunk":"@@ -0,0 +1,355 @@\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Analyzes model parameters and creates optimized groupings for distributed communication.\n+///\n+/// For Beginners:\n+/// Think of ParameterAnalyzer as a smart packing assistant. When shipping items, you don't\n+/// want to send thousands of tiny packages - it's inefficient! Instead, you group small\n+/// items together into larger boxes.\n+///\n+/// Similarly, when communicating parameters across GPUs:\n+/// - Sending many small parameter arrays is slow (lots of communication overhead)\n+/// - Grouping small parameters together reduces the number of messages\n+/// - This analyzer figures out the best way to group parameters for efficiency\n+///\n+/// For example, instead of sending 1000 separate bias vectors (each with 1 parameter),\n+/// we might group them into 10 larger chunks (each with 100 parameters).\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type</typeparam>\n+public class ParameterAnalyzer<T> where T : struct\n+{\n+    private readonly int _minimumGroupSize;\n+    private readonly int _worldSize;\n+\n+    /// <summary>\n+    /// Represents a group of parameters that should be communicated together.\n+    ///\n+    /// For Beginners:\n+    /// This is like a shipping box that contains multiple items. Each ParameterGroup\n+    /// represents a chunk of parameters that will be sent together in one communication.\n+    /// </summary>\n+    public class ParameterGroup\n+    {\n+        /// <summary>\n+        /// The starting index of this group in the full parameter vector.\n+        /// </summary>\n+        public int StartIndex { get; set; }\n+\n+        /// <summary>\n+        /// The number of parameters in this group.\n+        /// </summary>\n+        public int Size { get; set; }\n+\n+        /// <summary>\n+        /// A descriptive name for this parameter group (e.g., \"Layer1.Weights\").\n+        /// </summary>\n+        public string Name { get; set; } = string.Empty;\n+\n+        /// <summary>\n+        /// Indicates whether this group was created by merging smaller groups.\n+        ///\n+        /// For Beginners:\n+        /// True if this group contains multiple small parameter arrays that were\n+        /// combined for efficiency. False if it represents a single large parameter array.\n+        /// </summary>\n+        public bool IsMerged { get; set; }\n+    }\n+\n+    /// <summary>\n+    /// Creates a new parameter analyzer with the specified settings.\n+    ///\n+    /// For Beginners:\n+    /// This creates the analyzer that will figure out how to group parameters.\n+    /// You tell it:\n+    /// - minimumGroupSize: The smallest acceptable group size (smaller groups get merged)\n+    /// - worldSize: How many processes are sharing the work (affects optimal group sizes)\n+    /// </summary>\n+    /// <param name=\"minimumGroupSize\">Minimum size for a parameter group (smaller groups will be merged)</param>\n+    /// <param name=\"worldSize\">Number of processes in the distributed group</param>\n+    public ParameterAnalyzer(int minimumGroupSize = 1024, int worldSize = 1)\n+    {\n+        if (minimumGroupSize <= 0)\n+        {\n+            throw new ArgumentException(\"Minimum group size must be positive.\", nameof(minimumGroupSize));\n+        }\n+\n+        if (worldSize <= 0)\n+        {\n+            throw new ArgumentException(\"World size must be positive.\", nameof(worldSize));\n+        }\n+\n+        _minimumGroupSize = minimumGroupSize;\n+        _worldSize = worldSize;\n+    }\n+\n+    /// <summary>\n+    /// Analyzes a model's parameters and creates optimized groupings.\n+    ///\n+    /// For Beginners:\n+    /// This method looks at all the parameters in your model and decides how to\n+    /// group them for efficient communication. It returns a list of ParameterGroups,\n+    /// each representing parameters that should be sent together.\n+    ///\n+    /// The analyzer:\n+    /// 1. Identifies natural parameter boundaries (e.g., weights vs biases)\n+    /// 2. Merges small groups that are below the minimum size\n+    /// 3. Ensures groups are aligned with process boundaries for even distribution\n+    /// </summary>\n+    /// <typeparam name=\"TInput\">The input type of the model</typeparam>\n+    /// <typeparam name=\"TOutput\">The output type of the model</typeparam>\n+    /// <param name=\"model\">The model to analyze</param>\n+    /// <returns>A list of optimized parameter groups</returns>\n+    public List<ParameterGroup> AnalyzeModel<TInput, TOutput>(IFullModel<T, TInput, TOutput> model)\n+    {\n+        if (model == null)\n+        {\n+            throw new ArgumentNullException(nameof(model));\n+        }\n+\n+        var parameters = model.GetParameters();\n+        return AnalyzeParameters(parameters);\n+    }\n+\n+    /// <summary>\n+    /// Analyzes a parameter vector and creates optimized groupings.\n+    ///\n+    /// For Beginners:\n+    /// This is the core analysis method. It takes a long list of parameters\n+    /// and intelligently groups them for efficient communication.\n+    ///\n+    /// Strategy:\n+    /// 1. Start with natural boundaries (we assume every N parameters belong together)\n+    /// 2. Merge groups that are too small\n+    /// 3. Align group boundaries to make distribution across processes easier\n+    /// </summary>\n+    /// <param name=\"parameters\">The parameter vector to analyze</param>\n+    /// <returns>A list of optimized parameter groups</returns>\n+    public List<ParameterGroup> AnalyzeParameters(Vector<T> parameters)\n+    {\n+        if (parameters == null)\n+        {\n+            throw new ArgumentNullException(nameof(parameters));\n+        }\n+\n+        var groups = new List<ParameterGroup>();\n+        int totalParams = parameters.Length;\n+\n+        if (totalParams == 0)\n+        {\n+            return groups;\n+        }\n+\n+        // Simple strategy: Create groups of at least minimum size\n+        // This is a basic implementation; a more sophisticated version could:\n+        // - Detect layer boundaries\n+        // - Group parameters by type (weights vs biases)\n+        // - Optimize for specific communication patterns\n+\n+        int currentIndex = 0;\n+        int groupId = 0;\n+\n+        while (currentIndex < totalParams)\n+        {\n+            // Calculate group size\n+            int remainingParams = totalParams - currentIndex;\n+            int groupSize = Math.Min(_minimumGroupSize, remainingParams);\n+\n+            // If the remaining parameters are slightly larger than minimum,\n+            // make the last group slightly larger rather than creating a tiny final group\n+            if (remainingParams > _minimumGroupSize && remainingParams < _minimumGroupSize * 1.5)\n+            {\n+                groupSize = remainingParams;\n+            }\n+\n+            // Create the group\n+            groups.Add(new ParameterGroup\n+            {\n+                StartIndex = currentIndex,\n+                Size = groupSize,\n+                Name = $\"ParameterGroup_{groupId}\",\n+                IsMerged = false\n+            });\n+\n+            currentIndex += groupSize;\n+            groupId++;\n+        }\n+\n+        return groups;\n+    }\n+\n+    /// <summary>\n+    /// Analyzes parameters and creates groups optimized for even distribution across processes.\n+    ///\n+    /// For Beginners:\n+    /// When distributing work across multiple processes, we want each process to get\n+    /// roughly the same amount of work. This method creates groups that divide evenly.\n+    ///\n+    /// For example, if you have 10,000 parameters and 4 processes:\n+    /// - Each process should get ~2,500 parameters\n+    /// - We create groups sized to divide evenly by 4\n+    /// </summary>\n+    /// <param name=\"parameters\">The parameter vector to analyze</param>\n+    /// <returns>A list of parameter groups optimized for distribution</returns>\n+    public List<ParameterGroup> AnalyzeForDistribution(Vector<T> parameters)\n+    {\n+        if (parameters == null)\n+        {\n+            throw new ArgumentNullException(nameof(parameters));\n+        }\n+\n+        var groups = new List<ParameterGroup>();\n+        int totalParams = parameters.Length;\n+\n+        if (totalParams == 0)\n+        {\n+            return groups;\n+        }\n+\n+        // Calculate optimal group size for even distribution\n+        // We want each group to be:\n+        // 1. At least the minimum size\n+        // 2. Divisible by world size (or close to it)\n+        // 3. An even division of total parameters\n+\n+        int baseGroupSize = Math.Max(_minimumGroupSize, totalParams / (_worldSize * 4));"}]}},{"id":"PRRT_kwDOKSXUF85g9V9M","isResolved":true,"isOutdated":true,"path":"src/DistributedTraining/ParameterAnalyzer.cs","line":null,"originalLine":163,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VG00u","author":{"login":"copilot-pull-request-reviewer"},"body":"The threshold value `1.5` in the condition `remainingParams < _minimumGroupSize * 1.5` is a magic number. Consider making this a named constant with documentation explaining why 1.5 is the threshold for merging remaining parameters into the last group.","createdAt":"2025-11-07T03:54:08Z","path":"src/DistributedTraining/ParameterAnalyzer.cs","line":null,"originalLine":163,"diffHunk":"@@ -0,0 +1,355 @@\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Analyzes model parameters and creates optimized groupings for distributed communication.\n+///\n+/// For Beginners:\n+/// Think of ParameterAnalyzer as a smart packing assistant. When shipping items, you don't\n+/// want to send thousands of tiny packages - it's inefficient! Instead, you group small\n+/// items together into larger boxes.\n+///\n+/// Similarly, when communicating parameters across GPUs:\n+/// - Sending many small parameter arrays is slow (lots of communication overhead)\n+/// - Grouping small parameters together reduces the number of messages\n+/// - This analyzer figures out the best way to group parameters for efficiency\n+///\n+/// For example, instead of sending 1000 separate bias vectors (each with 1 parameter),\n+/// we might group them into 10 larger chunks (each with 100 parameters).\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type</typeparam>\n+public class ParameterAnalyzer<T> where T : struct\n+{\n+    private readonly int _minimumGroupSize;\n+    private readonly int _worldSize;\n+\n+    /// <summary>\n+    /// Represents a group of parameters that should be communicated together.\n+    ///\n+    /// For Beginners:\n+    /// This is like a shipping box that contains multiple items. Each ParameterGroup\n+    /// represents a chunk of parameters that will be sent together in one communication.\n+    /// </summary>\n+    public class ParameterGroup\n+    {\n+        /// <summary>\n+        /// The starting index of this group in the full parameter vector.\n+        /// </summary>\n+        public int StartIndex { get; set; }\n+\n+        /// <summary>\n+        /// The number of parameters in this group.\n+        /// </summary>\n+        public int Size { get; set; }\n+\n+        /// <summary>\n+        /// A descriptive name for this parameter group (e.g., \"Layer1.Weights\").\n+        /// </summary>\n+        public string Name { get; set; } = string.Empty;\n+\n+        /// <summary>\n+        /// Indicates whether this group was created by merging smaller groups.\n+        ///\n+        /// For Beginners:\n+        /// True if this group contains multiple small parameter arrays that were\n+        /// combined for efficiency. False if it represents a single large parameter array.\n+        /// </summary>\n+        public bool IsMerged { get; set; }\n+    }\n+\n+    /// <summary>\n+    /// Creates a new parameter analyzer with the specified settings.\n+    ///\n+    /// For Beginners:\n+    /// This creates the analyzer that will figure out how to group parameters.\n+    /// You tell it:\n+    /// - minimumGroupSize: The smallest acceptable group size (smaller groups get merged)\n+    /// - worldSize: How many processes are sharing the work (affects optimal group sizes)\n+    /// </summary>\n+    /// <param name=\"minimumGroupSize\">Minimum size for a parameter group (smaller groups will be merged)</param>\n+    /// <param name=\"worldSize\">Number of processes in the distributed group</param>\n+    public ParameterAnalyzer(int minimumGroupSize = 1024, int worldSize = 1)\n+    {\n+        if (minimumGroupSize <= 0)\n+        {\n+            throw new ArgumentException(\"Minimum group size must be positive.\", nameof(minimumGroupSize));\n+        }\n+\n+        if (worldSize <= 0)\n+        {\n+            throw new ArgumentException(\"World size must be positive.\", nameof(worldSize));\n+        }\n+\n+        _minimumGroupSize = minimumGroupSize;\n+        _worldSize = worldSize;\n+    }\n+\n+    /// <summary>\n+    /// Analyzes a model's parameters and creates optimized groupings.\n+    ///\n+    /// For Beginners:\n+    /// This method looks at all the parameters in your model and decides how to\n+    /// group them for efficient communication. It returns a list of ParameterGroups,\n+    /// each representing parameters that should be sent together.\n+    ///\n+    /// The analyzer:\n+    /// 1. Identifies natural parameter boundaries (e.g., weights vs biases)\n+    /// 2. Merges small groups that are below the minimum size\n+    /// 3. Ensures groups are aligned with process boundaries for even distribution\n+    /// </summary>\n+    /// <typeparam name=\"TInput\">The input type of the model</typeparam>\n+    /// <typeparam name=\"TOutput\">The output type of the model</typeparam>\n+    /// <param name=\"model\">The model to analyze</param>\n+    /// <returns>A list of optimized parameter groups</returns>\n+    public List<ParameterGroup> AnalyzeModel<TInput, TOutput>(IFullModel<T, TInput, TOutput> model)\n+    {\n+        if (model == null)\n+        {\n+            throw new ArgumentNullException(nameof(model));\n+        }\n+\n+        var parameters = model.GetParameters();\n+        return AnalyzeParameters(parameters);\n+    }\n+\n+    /// <summary>\n+    /// Analyzes a parameter vector and creates optimized groupings.\n+    ///\n+    /// For Beginners:\n+    /// This is the core analysis method. It takes a long list of parameters\n+    /// and intelligently groups them for efficient communication.\n+    ///\n+    /// Strategy:\n+    /// 1. Start with natural boundaries (we assume every N parameters belong together)\n+    /// 2. Merge groups that are too small\n+    /// 3. Align group boundaries to make distribution across processes easier\n+    /// </summary>\n+    /// <param name=\"parameters\">The parameter vector to analyze</param>\n+    /// <returns>A list of optimized parameter groups</returns>\n+    public List<ParameterGroup> AnalyzeParameters(Vector<T> parameters)\n+    {\n+        if (parameters == null)\n+        {\n+            throw new ArgumentNullException(nameof(parameters));\n+        }\n+\n+        var groups = new List<ParameterGroup>();\n+        int totalParams = parameters.Length;\n+\n+        if (totalParams == 0)\n+        {\n+            return groups;\n+        }\n+\n+        // Simple strategy: Create groups of at least minimum size\n+        // This is a basic implementation; a more sophisticated version could:\n+        // - Detect layer boundaries\n+        // - Group parameters by type (weights vs biases)\n+        // - Optimize for specific communication patterns\n+\n+        int currentIndex = 0;\n+        int groupId = 0;\n+\n+        while (currentIndex < totalParams)\n+        {\n+            // Calculate group size\n+            int remainingParams = totalParams - currentIndex;\n+            int groupSize = Math.Min(_minimumGroupSize, remainingParams);\n+\n+            // If the remaining parameters are slightly larger than minimum,\n+            // make the last group slightly larger rather than creating a tiny final group\n+            if (remainingParams > _minimumGroupSize && remainingParams < _minimumGroupSize * 1.5)"}]}},{"id":"PRRT_kwDOKSXUF85g9V9R","isResolved":true,"isOutdated":false,"path":"src/DistributedTraining/ParameterAnalyzer.cs","line":274,"originalLine":234,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VG00z","author":{"login":"copilot-pull-request-reviewer"},"body":"Similar magic number issue: The value `baseGroupSize / 2` uses a divisor of 2 without explanation. Document why groups smaller than half the base group size should be merged with the previous group.\n```suggestion\n            // If this would be the last group and it's small, merge with previous\n            // Groups smaller than half the base group size are merged with the previous group to avoid\n            // inefficient communication of very small parameter chunks. This threshold (baseGroupSize / 2)\n            // ensures that the final group is not disproportionately small, which would increase overhead.\n```","createdAt":"2025-11-07T03:54:08Z","path":"src/DistributedTraining/ParameterAnalyzer.cs","line":274,"originalLine":234,"diffHunk":"@@ -0,0 +1,355 @@\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.Interfaces;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Analyzes model parameters and creates optimized groupings for distributed communication.\n+///\n+/// For Beginners:\n+/// Think of ParameterAnalyzer as a smart packing assistant. When shipping items, you don't\n+/// want to send thousands of tiny packages - it's inefficient! Instead, you group small\n+/// items together into larger boxes.\n+///\n+/// Similarly, when communicating parameters across GPUs:\n+/// - Sending many small parameter arrays is slow (lots of communication overhead)\n+/// - Grouping small parameters together reduces the number of messages\n+/// - This analyzer figures out the best way to group parameters for efficiency\n+///\n+/// For example, instead of sending 1000 separate bias vectors (each with 1 parameter),\n+/// we might group them into 10 larger chunks (each with 100 parameters).\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type</typeparam>\n+public class ParameterAnalyzer<T> where T : struct\n+{\n+    private readonly int _minimumGroupSize;\n+    private readonly int _worldSize;\n+\n+    /// <summary>\n+    /// Represents a group of parameters that should be communicated together.\n+    ///\n+    /// For Beginners:\n+    /// This is like a shipping box that contains multiple items. Each ParameterGroup\n+    /// represents a chunk of parameters that will be sent together in one communication.\n+    /// </summary>\n+    public class ParameterGroup\n+    {\n+        /// <summary>\n+        /// The starting index of this group in the full parameter vector.\n+        /// </summary>\n+        public int StartIndex { get; set; }\n+\n+        /// <summary>\n+        /// The number of parameters in this group.\n+        /// </summary>\n+        public int Size { get; set; }\n+\n+        /// <summary>\n+        /// A descriptive name for this parameter group (e.g., \"Layer1.Weights\").\n+        /// </summary>\n+        public string Name { get; set; } = string.Empty;\n+\n+        /// <summary>\n+        /// Indicates whether this group was created by merging smaller groups.\n+        ///\n+        /// For Beginners:\n+        /// True if this group contains multiple small parameter arrays that were\n+        /// combined for efficiency. False if it represents a single large parameter array.\n+        /// </summary>\n+        public bool IsMerged { get; set; }\n+    }\n+\n+    /// <summary>\n+    /// Creates a new parameter analyzer with the specified settings.\n+    ///\n+    /// For Beginners:\n+    /// This creates the analyzer that will figure out how to group parameters.\n+    /// You tell it:\n+    /// - minimumGroupSize: The smallest acceptable group size (smaller groups get merged)\n+    /// - worldSize: How many processes are sharing the work (affects optimal group sizes)\n+    /// </summary>\n+    /// <param name=\"minimumGroupSize\">Minimum size for a parameter group (smaller groups will be merged)</param>\n+    /// <param name=\"worldSize\">Number of processes in the distributed group</param>\n+    public ParameterAnalyzer(int minimumGroupSize = 1024, int worldSize = 1)\n+    {\n+        if (minimumGroupSize <= 0)\n+        {\n+            throw new ArgumentException(\"Minimum group size must be positive.\", nameof(minimumGroupSize));\n+        }\n+\n+        if (worldSize <= 0)\n+        {\n+            throw new ArgumentException(\"World size must be positive.\", nameof(worldSize));\n+        }\n+\n+        _minimumGroupSize = minimumGroupSize;\n+        _worldSize = worldSize;\n+    }\n+\n+    /// <summary>\n+    /// Analyzes a model's parameters and creates optimized groupings.\n+    ///\n+    /// For Beginners:\n+    /// This method looks at all the parameters in your model and decides how to\n+    /// group them for efficient communication. It returns a list of ParameterGroups,\n+    /// each representing parameters that should be sent together.\n+    ///\n+    /// The analyzer:\n+    /// 1. Identifies natural parameter boundaries (e.g., weights vs biases)\n+    /// 2. Merges small groups that are below the minimum size\n+    /// 3. Ensures groups are aligned with process boundaries for even distribution\n+    /// </summary>\n+    /// <typeparam name=\"TInput\">The input type of the model</typeparam>\n+    /// <typeparam name=\"TOutput\">The output type of the model</typeparam>\n+    /// <param name=\"model\">The model to analyze</param>\n+    /// <returns>A list of optimized parameter groups</returns>\n+    public List<ParameterGroup> AnalyzeModel<TInput, TOutput>(IFullModel<T, TInput, TOutput> model)\n+    {\n+        if (model == null)\n+        {\n+            throw new ArgumentNullException(nameof(model));\n+        }\n+\n+        var parameters = model.GetParameters();\n+        return AnalyzeParameters(parameters);\n+    }\n+\n+    /// <summary>\n+    /// Analyzes a parameter vector and creates optimized groupings.\n+    ///\n+    /// For Beginners:\n+    /// This is the core analysis method. It takes a long list of parameters\n+    /// and intelligently groups them for efficient communication.\n+    ///\n+    /// Strategy:\n+    /// 1. Start with natural boundaries (we assume every N parameters belong together)\n+    /// 2. Merge groups that are too small\n+    /// 3. Align group boundaries to make distribution across processes easier\n+    /// </summary>\n+    /// <param name=\"parameters\">The parameter vector to analyze</param>\n+    /// <returns>A list of optimized parameter groups</returns>\n+    public List<ParameterGroup> AnalyzeParameters(Vector<T> parameters)\n+    {\n+        if (parameters == null)\n+        {\n+            throw new ArgumentNullException(nameof(parameters));\n+        }\n+\n+        var groups = new List<ParameterGroup>();\n+        int totalParams = parameters.Length;\n+\n+        if (totalParams == 0)\n+        {\n+            return groups;\n+        }\n+\n+        // Simple strategy: Create groups of at least minimum size\n+        // This is a basic implementation; a more sophisticated version could:\n+        // - Detect layer boundaries\n+        // - Group parameters by type (weights vs biases)\n+        // - Optimize for specific communication patterns\n+\n+        int currentIndex = 0;\n+        int groupId = 0;\n+\n+        while (currentIndex < totalParams)\n+        {\n+            // Calculate group size\n+            int remainingParams = totalParams - currentIndex;\n+            int groupSize = Math.Min(_minimumGroupSize, remainingParams);\n+\n+            // If the remaining parameters are slightly larger than minimum,\n+            // make the last group slightly larger rather than creating a tiny final group\n+            if (remainingParams > _minimumGroupSize && remainingParams < _minimumGroupSize * 1.5)\n+            {\n+                groupSize = remainingParams;\n+            }\n+\n+            // Create the group\n+            groups.Add(new ParameterGroup\n+            {\n+                StartIndex = currentIndex,\n+                Size = groupSize,\n+                Name = $\"ParameterGroup_{groupId}\",\n+                IsMerged = false\n+            });\n+\n+            currentIndex += groupSize;\n+            groupId++;\n+        }\n+\n+        return groups;\n+    }\n+\n+    /// <summary>\n+    /// Analyzes parameters and creates groups optimized for even distribution across processes.\n+    ///\n+    /// For Beginners:\n+    /// When distributing work across multiple processes, we want each process to get\n+    /// roughly the same amount of work. This method creates groups that divide evenly.\n+    ///\n+    /// For example, if you have 10,000 parameters and 4 processes:\n+    /// - Each process should get ~2,500 parameters\n+    /// - We create groups sized to divide evenly by 4\n+    /// </summary>\n+    /// <param name=\"parameters\">The parameter vector to analyze</param>\n+    /// <returns>A list of parameter groups optimized for distribution</returns>\n+    public List<ParameterGroup> AnalyzeForDistribution(Vector<T> parameters)\n+    {\n+        if (parameters == null)\n+        {\n+            throw new ArgumentNullException(nameof(parameters));\n+        }\n+\n+        var groups = new List<ParameterGroup>();\n+        int totalParams = parameters.Length;\n+\n+        if (totalParams == 0)\n+        {\n+            return groups;\n+        }\n+\n+        // Calculate optimal group size for even distribution\n+        // We want each group to be:\n+        // 1. At least the minimum size\n+        // 2. Divisible by world size (or close to it)\n+        // 3. An even division of total parameters\n+\n+        int baseGroupSize = Math.Max(_minimumGroupSize, totalParams / (_worldSize * 4));\n+\n+        // Round to nearest multiple of world size for better alignment\n+        if (baseGroupSize > _worldSize)\n+        {\n+            baseGroupSize = (baseGroupSize / _worldSize) * _worldSize;\n+        }\n+\n+        int currentIndex = 0;\n+        int groupId = 0;\n+\n+        while (currentIndex < totalParams)\n+        {\n+            int remainingParams = totalParams - currentIndex;\n+            int groupSize = Math.Min(baseGroupSize, remainingParams);\n+\n+            // If this would be the last group and it's small, merge with previous"}]}},{"id":"PRRT_kwDOKSXUF85g9V9a","isResolved":true,"isOutdated":true,"path":"src/DistributedTraining/InMemoryCommunicationBackend.cs","line":null,"originalLine":118,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VG009","author":{"login":"copilot-pull-request-reviewer"},"body":"The barrier ID uses `DateTime.UtcNow.Ticks` which could cause collisions if multiple barriers are called in rapid succession. Consider using a more robust unique identifier like `Guid.NewGuid()` (as done in other methods) or an incrementing counter.\n```suggestion\n            string barrierId = $\"barrier_{Guid.NewGuid()}\";\n```","createdAt":"2025-11-07T03:54:09Z","path":"src/DistributedTraining/InMemoryCommunicationBackend.cs","line":null,"originalLine":118,"diffHunk":"@@ -0,0 +1,484 @@\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.NumericOperations;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// A simple in-memory implementation of distributed communication for testing and single-machine scenarios.\n+/// This backend simulates multiple processes by using shared memory and locks.\n+///\n+/// For Beginners:\n+/// This is a \"fake\" distributed system that runs on a single machine.\n+/// It's perfect for testing your distributed code without needing multiple GPUs or machines.\n+/// Think of it as a practice mode - it simulates distributed behavior but everything\n+/// runs in one process.\n+///\n+/// Example:\n+/// <code>\n+/// // Create a simulated distributed environment with 4 \"processes\"\n+/// var backend = new InMemoryCommunicationBackend&lt;double&gt;(rank: 0, worldSize: 4);\n+/// CommunicationManager.Initialize(backend);\n+/// </code>\n+///\n+/// Note: For production distributed training, you would use an MPI-based backend instead.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type for operations</typeparam>\n+public class InMemoryCommunicationBackend<T> : ICommunicationBackend<T> where T : struct\n+{\n+    private readonly int _rank;\n+    private readonly int _worldSize;\n+    private readonly INumericOperations<T> _numOps;\n+    private bool _isInitialized;\n+\n+    // Shared state for simulating collective operations\n+    // In a real implementation, this would be handled by the MPI backend\n+    private static readonly object _globalLock = new object();\n+    private static readonly Dictionary<string, List<Vector<T>>> _sharedBuffers = new();\n+    private static readonly Dictionary<string, int> _barrierCounters = new();\n+\n+    /// <inheritdoc/>\n+    public int Rank => _rank;\n+\n+    /// <inheritdoc/>\n+    public int WorldSize => _worldSize;\n+\n+    /// <inheritdoc/>\n+    public bool IsInitialized => _isInitialized;\n+\n+    /// <summary>\n+    /// Creates a new in-memory communication backend.\n+    ///\n+    /// For Beginners:\n+    /// You create one of these for each simulated \"process\". If you want to simulate\n+    /// 4 GPUs, you create 4 instances with ranks 0, 1, 2, 3, all with worldSize=4.\n+    /// </summary>\n+    /// <param name=\"rank\">The rank (ID) of this simulated process (0-based)</param>\n+    /// <param name=\"worldSize\">The total number of simulated processes</param>\n+    /// <exception cref=\"ArgumentException\">Thrown if rank or worldSize are invalid</exception>\n+    public InMemoryCommunicationBackend(int rank, int worldSize)\n+    {\n+        if (rank < 0 || rank >= worldSize)\n+        {\n+            throw new ArgumentException(\n+                $\"Invalid rank {rank}. Must be between 0 and {worldSize - 1}.\");\n+        }\n+\n+        if (worldSize <= 0)\n+        {\n+            throw new ArgumentException(\n+                $\"Invalid worldSize {worldSize}. Must be positive.\");\n+        }\n+\n+        _rank = rank;\n+        _worldSize = worldSize;\n+        _numOps = MathHelper.GetNumericOperations<T>();\n+        _isInitialized = false;\n+    }\n+\n+    /// <inheritdoc/>\n+    public void Initialize()\n+    {\n+        lock (_globalLock)\n+        {\n+            if (_isInitialized)\n+            {\n+                return;\n+            }\n+\n+            _isInitialized = true;\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public void Shutdown()\n+    {\n+        lock (_globalLock)\n+        {\n+            if (!_isInitialized)\n+            {\n+                return;\n+            }\n+\n+            // Clear any remaining shared state\n+            _sharedBuffers.Clear();\n+            _barrierCounters.Clear();\n+\n+            _isInitialized = false;\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public void Barrier()\n+    {\n+        EnsureInitialized();\n+\n+        lock (_globalLock)\n+        {\n+            string barrierId = $\"barrier_{DateTime.UtcNow.Ticks}\";"}]}},{"id":"PRRT_kwDOKSXUF85g9V9h","isResolved":false,"isOutdated":true,"path":"src/DistributedTraining/InMemoryCommunicationBackend.cs","line":267,"originalLine":140,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VG01E","author":{"login":"copilot-pull-request-reviewer"},"body":"Resource leak: The barrier cleanup only happens for rank 0, but if rank 0 never reaches the barrier or fails, the barrier counter will never be cleaned up from the shared dictionary. Consider cleanup after all processes exit the barrier or implement a timeout-based cleanup mechanism.","createdAt":"2025-11-07T03:54:09Z","path":"src/DistributedTraining/InMemoryCommunicationBackend.cs","line":267,"originalLine":140,"diffHunk":"@@ -0,0 +1,484 @@\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.NumericOperations;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// A simple in-memory implementation of distributed communication for testing and single-machine scenarios.\n+/// This backend simulates multiple processes by using shared memory and locks.\n+///\n+/// For Beginners:\n+/// This is a \"fake\" distributed system that runs on a single machine.\n+/// It's perfect for testing your distributed code without needing multiple GPUs or machines.\n+/// Think of it as a practice mode - it simulates distributed behavior but everything\n+/// runs in one process.\n+///\n+/// Example:\n+/// <code>\n+/// // Create a simulated distributed environment with 4 \"processes\"\n+/// var backend = new InMemoryCommunicationBackend&lt;double&gt;(rank: 0, worldSize: 4);\n+/// CommunicationManager.Initialize(backend);\n+/// </code>\n+///\n+/// Note: For production distributed training, you would use an MPI-based backend instead.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type for operations</typeparam>\n+public class InMemoryCommunicationBackend<T> : ICommunicationBackend<T> where T : struct\n+{\n+    private readonly int _rank;\n+    private readonly int _worldSize;\n+    private readonly INumericOperations<T> _numOps;\n+    private bool _isInitialized;\n+\n+    // Shared state for simulating collective operations\n+    // In a real implementation, this would be handled by the MPI backend\n+    private static readonly object _globalLock = new object();\n+    private static readonly Dictionary<string, List<Vector<T>>> _sharedBuffers = new();\n+    private static readonly Dictionary<string, int> _barrierCounters = new();\n+\n+    /// <inheritdoc/>\n+    public int Rank => _rank;\n+\n+    /// <inheritdoc/>\n+    public int WorldSize => _worldSize;\n+\n+    /// <inheritdoc/>\n+    public bool IsInitialized => _isInitialized;\n+\n+    /// <summary>\n+    /// Creates a new in-memory communication backend.\n+    ///\n+    /// For Beginners:\n+    /// You create one of these for each simulated \"process\". If you want to simulate\n+    /// 4 GPUs, you create 4 instances with ranks 0, 1, 2, 3, all with worldSize=4.\n+    /// </summary>\n+    /// <param name=\"rank\">The rank (ID) of this simulated process (0-based)</param>\n+    /// <param name=\"worldSize\">The total number of simulated processes</param>\n+    /// <exception cref=\"ArgumentException\">Thrown if rank or worldSize are invalid</exception>\n+    public InMemoryCommunicationBackend(int rank, int worldSize)\n+    {\n+        if (rank < 0 || rank >= worldSize)\n+        {\n+            throw new ArgumentException(\n+                $\"Invalid rank {rank}. Must be between 0 and {worldSize - 1}.\");\n+        }\n+\n+        if (worldSize <= 0)\n+        {\n+            throw new ArgumentException(\n+                $\"Invalid worldSize {worldSize}. Must be positive.\");\n+        }\n+\n+        _rank = rank;\n+        _worldSize = worldSize;\n+        _numOps = MathHelper.GetNumericOperations<T>();\n+        _isInitialized = false;\n+    }\n+\n+    /// <inheritdoc/>\n+    public void Initialize()\n+    {\n+        lock (_globalLock)\n+        {\n+            if (_isInitialized)\n+            {\n+                return;\n+            }\n+\n+            _isInitialized = true;\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public void Shutdown()\n+    {\n+        lock (_globalLock)\n+        {\n+            if (!_isInitialized)\n+            {\n+                return;\n+            }\n+\n+            // Clear any remaining shared state\n+            _sharedBuffers.Clear();\n+            _barrierCounters.Clear();\n+\n+            _isInitialized = false;\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public void Barrier()\n+    {\n+        EnsureInitialized();\n+\n+        lock (_globalLock)\n+        {\n+            string barrierId = $\"barrier_{DateTime.UtcNow.Ticks}\";\n+\n+            if (!_barrierCounters.ContainsKey(barrierId))\n+            {\n+                _barrierCounters[barrierId] = 0;\n+            }\n+\n+            _barrierCounters[barrierId]++;\n+\n+            // Wait until all processes have reached the barrier\n+            while (_barrierCounters[barrierId] < _worldSize)\n+            {\n+                Monitor.Wait(_globalLock, 10);\n+            }\n+\n+            Monitor.PulseAll(_globalLock);\n+\n+            // Cleanup\n+            if (_rank == 0)\n+            {\n+                _barrierCounters.Remove(barrierId);\n+            }\n+        }"}]}},{"id":"PRRT_kwDOKSXUF85g9V9o","isResolved":false,"isOutdated":false,"path":"src/DistributedTraining/InMemoryCommunicationBackend.cs","line":317,"originalLine":180,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VG01N","author":{"login":"copilot-pull-request-reviewer"},"body":"Potential deadlock: Similar to the Barrier method, this while loop could deadlock if not all processes reach this point. If one process fails or doesn't call AllReduce, the others will wait indefinitely. Consider adding a timeout or error handling mechanism.","createdAt":"2025-11-07T03:54:09Z","path":"src/DistributedTraining/InMemoryCommunicationBackend.cs","line":317,"originalLine":180,"diffHunk":"@@ -0,0 +1,484 @@\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.NumericOperations;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// A simple in-memory implementation of distributed communication for testing and single-machine scenarios.\n+/// This backend simulates multiple processes by using shared memory and locks.\n+///\n+/// For Beginners:\n+/// This is a \"fake\" distributed system that runs on a single machine.\n+/// It's perfect for testing your distributed code without needing multiple GPUs or machines.\n+/// Think of it as a practice mode - it simulates distributed behavior but everything\n+/// runs in one process.\n+///\n+/// Example:\n+/// <code>\n+/// // Create a simulated distributed environment with 4 \"processes\"\n+/// var backend = new InMemoryCommunicationBackend&lt;double&gt;(rank: 0, worldSize: 4);\n+/// CommunicationManager.Initialize(backend);\n+/// </code>\n+///\n+/// Note: For production distributed training, you would use an MPI-based backend instead.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type for operations</typeparam>\n+public class InMemoryCommunicationBackend<T> : ICommunicationBackend<T> where T : struct\n+{\n+    private readonly int _rank;\n+    private readonly int _worldSize;\n+    private readonly INumericOperations<T> _numOps;\n+    private bool _isInitialized;\n+\n+    // Shared state for simulating collective operations\n+    // In a real implementation, this would be handled by the MPI backend\n+    private static readonly object _globalLock = new object();\n+    private static readonly Dictionary<string, List<Vector<T>>> _sharedBuffers = new();\n+    private static readonly Dictionary<string, int> _barrierCounters = new();\n+\n+    /// <inheritdoc/>\n+    public int Rank => _rank;\n+\n+    /// <inheritdoc/>\n+    public int WorldSize => _worldSize;\n+\n+    /// <inheritdoc/>\n+    public bool IsInitialized => _isInitialized;\n+\n+    /// <summary>\n+    /// Creates a new in-memory communication backend.\n+    ///\n+    /// For Beginners:\n+    /// You create one of these for each simulated \"process\". If you want to simulate\n+    /// 4 GPUs, you create 4 instances with ranks 0, 1, 2, 3, all with worldSize=4.\n+    /// </summary>\n+    /// <param name=\"rank\">The rank (ID) of this simulated process (0-based)</param>\n+    /// <param name=\"worldSize\">The total number of simulated processes</param>\n+    /// <exception cref=\"ArgumentException\">Thrown if rank or worldSize are invalid</exception>\n+    public InMemoryCommunicationBackend(int rank, int worldSize)\n+    {\n+        if (rank < 0 || rank >= worldSize)\n+        {\n+            throw new ArgumentException(\n+                $\"Invalid rank {rank}. Must be between 0 and {worldSize - 1}.\");\n+        }\n+\n+        if (worldSize <= 0)\n+        {\n+            throw new ArgumentException(\n+                $\"Invalid worldSize {worldSize}. Must be positive.\");\n+        }\n+\n+        _rank = rank;\n+        _worldSize = worldSize;\n+        _numOps = MathHelper.GetNumericOperations<T>();\n+        _isInitialized = false;\n+    }\n+\n+    /// <inheritdoc/>\n+    public void Initialize()\n+    {\n+        lock (_globalLock)\n+        {\n+            if (_isInitialized)\n+            {\n+                return;\n+            }\n+\n+            _isInitialized = true;\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public void Shutdown()\n+    {\n+        lock (_globalLock)\n+        {\n+            if (!_isInitialized)\n+            {\n+                return;\n+            }\n+\n+            // Clear any remaining shared state\n+            _sharedBuffers.Clear();\n+            _barrierCounters.Clear();\n+\n+            _isInitialized = false;\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public void Barrier()\n+    {\n+        EnsureInitialized();\n+\n+        lock (_globalLock)\n+        {\n+            string barrierId = $\"barrier_{DateTime.UtcNow.Ticks}\";\n+\n+            if (!_barrierCounters.ContainsKey(barrierId))\n+            {\n+                _barrierCounters[barrierId] = 0;\n+            }\n+\n+            _barrierCounters[barrierId]++;\n+\n+            // Wait until all processes have reached the barrier\n+            while (_barrierCounters[barrierId] < _worldSize)\n+            {\n+                Monitor.Wait(_globalLock, 10);\n+            }\n+\n+            Monitor.PulseAll(_globalLock);\n+\n+            // Cleanup\n+            if (_rank == 0)\n+            {\n+                _barrierCounters.Remove(barrierId);\n+            }\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public void AllReduce(Vector<T> data, ReductionOperation operation)\n+    {\n+        EnsureInitialized();\n+\n+        if (data == null)\n+        {\n+            throw new ArgumentNullException(nameof(data));\n+        }\n+\n+        // For single process, no communication needed\n+        if (_worldSize == 1)\n+        {\n+            if (operation == ReductionOperation.Average)\n+            {\n+                // Already averaged (only one value)\n+            }\n+            return;\n+        }\n+\n+        string bufferId = $\"allreduce_{Guid.NewGuid()}\";\n+\n+        lock (_globalLock)\n+        {\n+            // Initialize shared buffer\n+            if (!_sharedBuffers.ContainsKey(bufferId))\n+            {\n+                _sharedBuffers[bufferId] = new List<Vector<T>>();\n+            }\n+\n+            // Contribute local data\n+            _sharedBuffers[bufferId].Add(data.Clone());\n+\n+            // Wait until all processes have contributed\n+            while (_sharedBuffers[bufferId].Count < _worldSize)\n+            {\n+                Monitor.Wait(_globalLock, 10);\n+            }"}]}},{"id":"PRRT_kwDOKSXUF85g9V9s","isResolved":false,"isOutdated":false,"path":"src/DistributedTraining/InMemoryCommunicationBackend.cs","line":707,"originalLine":450,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VG01V","author":{"login":"copilot-pull-request-reviewer"},"body":"The Average operation applies division after the accumulation, but the accumulation is using Sum logic. This means the final division is done once on the total sum, dividing by vector count. However, the comment and operation name suggest this should average values. Verify this is the intended behavior - it appears correct but could use clarification in the comment.","createdAt":"2025-11-07T03:54:09Z","path":"src/DistributedTraining/InMemoryCommunicationBackend.cs","line":707,"originalLine":450,"diffHunk":"@@ -0,0 +1,484 @@\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.NumericOperations;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// A simple in-memory implementation of distributed communication for testing and single-machine scenarios.\n+/// This backend simulates multiple processes by using shared memory and locks.\n+///\n+/// For Beginners:\n+/// This is a \"fake\" distributed system that runs on a single machine.\n+/// It's perfect for testing your distributed code without needing multiple GPUs or machines.\n+/// Think of it as a practice mode - it simulates distributed behavior but everything\n+/// runs in one process.\n+///\n+/// Example:\n+/// <code>\n+/// // Create a simulated distributed environment with 4 \"processes\"\n+/// var backend = new InMemoryCommunicationBackend&lt;double&gt;(rank: 0, worldSize: 4);\n+/// CommunicationManager.Initialize(backend);\n+/// </code>\n+///\n+/// Note: For production distributed training, you would use an MPI-based backend instead.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type for operations</typeparam>\n+public class InMemoryCommunicationBackend<T> : ICommunicationBackend<T> where T : struct\n+{\n+    private readonly int _rank;\n+    private readonly int _worldSize;\n+    private readonly INumericOperations<T> _numOps;\n+    private bool _isInitialized;\n+\n+    // Shared state for simulating collective operations\n+    // In a real implementation, this would be handled by the MPI backend\n+    private static readonly object _globalLock = new object();\n+    private static readonly Dictionary<string, List<Vector<T>>> _sharedBuffers = new();\n+    private static readonly Dictionary<string, int> _barrierCounters = new();\n+\n+    /// <inheritdoc/>\n+    public int Rank => _rank;\n+\n+    /// <inheritdoc/>\n+    public int WorldSize => _worldSize;\n+\n+    /// <inheritdoc/>\n+    public bool IsInitialized => _isInitialized;\n+\n+    /// <summary>\n+    /// Creates a new in-memory communication backend.\n+    ///\n+    /// For Beginners:\n+    /// You create one of these for each simulated \"process\". If you want to simulate\n+    /// 4 GPUs, you create 4 instances with ranks 0, 1, 2, 3, all with worldSize=4.\n+    /// </summary>\n+    /// <param name=\"rank\">The rank (ID) of this simulated process (0-based)</param>\n+    /// <param name=\"worldSize\">The total number of simulated processes</param>\n+    /// <exception cref=\"ArgumentException\">Thrown if rank or worldSize are invalid</exception>\n+    public InMemoryCommunicationBackend(int rank, int worldSize)\n+    {\n+        if (rank < 0 || rank >= worldSize)\n+        {\n+            throw new ArgumentException(\n+                $\"Invalid rank {rank}. Must be between 0 and {worldSize - 1}.\");\n+        }\n+\n+        if (worldSize <= 0)\n+        {\n+            throw new ArgumentException(\n+                $\"Invalid worldSize {worldSize}. Must be positive.\");\n+        }\n+\n+        _rank = rank;\n+        _worldSize = worldSize;\n+        _numOps = MathHelper.GetNumericOperations<T>();\n+        _isInitialized = false;\n+    }\n+\n+    /// <inheritdoc/>\n+    public void Initialize()\n+    {\n+        lock (_globalLock)\n+        {\n+            if (_isInitialized)\n+            {\n+                return;\n+            }\n+\n+            _isInitialized = true;\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public void Shutdown()\n+    {\n+        lock (_globalLock)\n+        {\n+            if (!_isInitialized)\n+            {\n+                return;\n+            }\n+\n+            // Clear any remaining shared state\n+            _sharedBuffers.Clear();\n+            _barrierCounters.Clear();\n+\n+            _isInitialized = false;\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public void Barrier()\n+    {\n+        EnsureInitialized();\n+\n+        lock (_globalLock)\n+        {\n+            string barrierId = $\"barrier_{DateTime.UtcNow.Ticks}\";\n+\n+            if (!_barrierCounters.ContainsKey(barrierId))\n+            {\n+                _barrierCounters[barrierId] = 0;\n+            }\n+\n+            _barrierCounters[barrierId]++;\n+\n+            // Wait until all processes have reached the barrier\n+            while (_barrierCounters[barrierId] < _worldSize)\n+            {\n+                Monitor.Wait(_globalLock, 10);\n+            }\n+\n+            Monitor.PulseAll(_globalLock);\n+\n+            // Cleanup\n+            if (_rank == 0)\n+            {\n+                _barrierCounters.Remove(barrierId);\n+            }\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public void AllReduce(Vector<T> data, ReductionOperation operation)\n+    {\n+        EnsureInitialized();\n+\n+        if (data == null)\n+        {\n+            throw new ArgumentNullException(nameof(data));\n+        }\n+\n+        // For single process, no communication needed\n+        if (_worldSize == 1)\n+        {\n+            if (operation == ReductionOperation.Average)\n+            {\n+                // Already averaged (only one value)\n+            }\n+            return;\n+        }\n+\n+        string bufferId = $\"allreduce_{Guid.NewGuid()}\";\n+\n+        lock (_globalLock)\n+        {\n+            // Initialize shared buffer\n+            if (!_sharedBuffers.ContainsKey(bufferId))\n+            {\n+                _sharedBuffers[bufferId] = new List<Vector<T>>();\n+            }\n+\n+            // Contribute local data\n+            _sharedBuffers[bufferId].Add(data.Clone());\n+\n+            // Wait until all processes have contributed\n+            while (_sharedBuffers[bufferId].Count < _worldSize)\n+            {\n+                Monitor.Wait(_globalLock, 10);\n+            }\n+\n+            Monitor.PulseAll(_globalLock);\n+\n+            // Perform reduction\n+            var allData = _sharedBuffers[bufferId];\n+            var result = PerformReduction(allData, operation);\n+\n+            // Copy result back to input data\n+            for (int i = 0; i < data.Length; i++)\n+            {\n+                data[i] = result[i];\n+            }\n+\n+            // Cleanup (rank 0 cleans up)\n+            if (_rank == 0)\n+            {\n+                _sharedBuffers.Remove(bufferId);\n+            }\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public Vector<T> AllGather(Vector<T> sendData)\n+    {\n+        EnsureInitialized();\n+\n+        if (sendData == null)\n+        {\n+            throw new ArgumentNullException(nameof(sendData));\n+        }\n+\n+        // For single process, just return a copy\n+        if (_worldSize == 1)\n+        {\n+            return sendData.Clone();\n+        }\n+\n+        string bufferId = $\"allgather_{Guid.NewGuid()}\";\n+\n+        lock (_globalLock)\n+        {\n+            // Initialize shared buffer\n+            if (!_sharedBuffers.ContainsKey(bufferId))\n+            {\n+                _sharedBuffers[bufferId] = new List<Vector<T>>(new Vector<T>[_worldSize]);\n+            }\n+\n+            // Contribute local data\n+            _sharedBuffers[bufferId][_rank] = sendData.Clone();\n+\n+            // Wait until all processes have contributed\n+            int contributedCount = 0;\n+            while (contributedCount < _worldSize)\n+            {\n+                contributedCount = _sharedBuffers[bufferId].Count(v => v != null);\n+                if (contributedCount < _worldSize)\n+                {\n+                    Monitor.Wait(_globalLock, 10);\n+                }\n+            }\n+\n+            Monitor.PulseAll(_globalLock);\n+\n+            // Concatenate all data\n+            var allData = _sharedBuffers[bufferId];\n+            int totalLength = allData.Sum(v => v.Length);\n+            var result = new T[totalLength];\n+            int offset = 0;\n+\n+            for (int i = 0; i < _worldSize; i++)\n+            {\n+                var data = allData[i];\n+                Array.Copy(data.ToArray(), 0, result, offset, data.Length);\n+                offset += data.Length;\n+            }\n+\n+            // Cleanup\n+            if (_rank == 0)\n+            {\n+                _sharedBuffers.Remove(bufferId);\n+            }\n+\n+            return new Vector<T>(result);\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public Vector<T> Broadcast(Vector<T> data, int root = 0)\n+    {\n+        EnsureInitialized();\n+\n+        if (root < 0 || root >= _worldSize)\n+        {\n+            throw new ArgumentException($\"Invalid root {root}. Must be between 0 and {_worldSize - 1}.\");\n+        }\n+\n+        // For single process, just return a copy\n+        if (_worldSize == 1)\n+        {\n+            return data?.Clone() ?? throw new ArgumentNullException(nameof(data));\n+        }\n+\n+        string bufferId = $\"broadcast_{Guid.NewGuid()}\";\n+\n+        lock (_globalLock)\n+        {\n+            Vector<T> result;\n+\n+            // Root process stores the data\n+            if (_rank == root)\n+            {\n+                if (data == null)\n+                {\n+                    throw new ArgumentNullException(nameof(data), \"Data cannot be null on root process.\");\n+                }\n+                _sharedBuffers[bufferId] = new List<Vector<T>> { data.Clone() };\n+            }\n+\n+            // Wait for root to store data\n+            while (!_sharedBuffers.ContainsKey(bufferId))\n+            {\n+                Monitor.Wait(_globalLock, 10);\n+            }\n+\n+            Monitor.PulseAll(_globalLock);\n+\n+            // All processes retrieve the data\n+            result = _sharedBuffers[bufferId][0].Clone();\n+\n+            // Cleanup\n+            if (_rank == 0)\n+            {\n+                _sharedBuffers.Remove(bufferId);\n+            }\n+\n+            return result;\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public Vector<T> Scatter(Vector<T> sendData, int root = 0)\n+    {\n+        EnsureInitialized();\n+\n+        if (root < 0 || root >= _worldSize)\n+        {\n+            throw new ArgumentException($\"Invalid root {root}. Must be between 0 and {_worldSize - 1}.\");\n+        }\n+\n+        // For single process, just return a copy\n+        if (_worldSize == 1)\n+        {\n+            return sendData?.Clone() ?? throw new ArgumentNullException(nameof(sendData));\n+        }\n+\n+        string bufferId = $\"scatter_{Guid.NewGuid()}\";\n+\n+        lock (_globalLock)\n+        {\n+            // Root process splits and stores the data\n+            if (_rank == root)\n+            {\n+                if (sendData == null)\n+                {\n+                    throw new ArgumentNullException(nameof(sendData), \"Data cannot be null on root process.\");\n+                }\n+\n+                if (sendData.Length % _worldSize != 0)\n+                {\n+                    throw new ArgumentException(\n+                        $\"Data length {sendData.Length} must be divisible by world size {_worldSize}.\");\n+                }\n+\n+                int chunkSize = sendData.Length / _worldSize;\n+                _sharedBuffers[bufferId] = new List<Vector<T>>();\n+\n+                for (int i = 0; i < _worldSize; i++)\n+                {\n+                    var chunk = new T[chunkSize];\n+                    Array.Copy(sendData.ToArray(), i * chunkSize, chunk, 0, chunkSize);\n+                    _sharedBuffers[bufferId].Add(new Vector<T>(chunk));\n+                }\n+            }\n+\n+            // Wait for root to split data\n+            while (!_sharedBuffers.ContainsKey(bufferId))\n+            {\n+                Monitor.Wait(_globalLock, 10);\n+            }\n+\n+            Monitor.PulseAll(_globalLock);\n+\n+            // Each process retrieves its chunk\n+            var result = _sharedBuffers[bufferId][_rank].Clone();\n+\n+            // Cleanup\n+            if (_rank == 0)\n+            {\n+                _sharedBuffers.Remove(bufferId);\n+            }\n+\n+            return result;\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public Vector<T> ReduceScatter(Vector<T> data, ReductionOperation operation)\n+    {\n+        EnsureInitialized();\n+\n+        if (data == null)\n+        {\n+            throw new ArgumentNullException(nameof(data));\n+        }\n+\n+        // For single process, just return a copy\n+        if (_worldSize == 1)\n+        {\n+            if (operation == ReductionOperation.Average)\n+            {\n+                // Already averaged (only one value)\n+            }\n+            return data.Clone();\n+        }\n+\n+        if (data.Length % _worldSize != 0)\n+        {\n+            throw new ArgumentException(\n+                $\"Data length {data.Length} must be divisible by world size {_worldSize}.\");\n+        }\n+\n+        // Perform AllReduce then extract local chunk\n+        var reducedData = data.Clone();\n+        AllReduce(reducedData, operation);\n+\n+        int chunkSize = reducedData.Length / _worldSize;\n+        var chunk = new T[chunkSize];\n+        Array.Copy(reducedData.ToArray(), _rank * chunkSize, chunk, 0, chunkSize);\n+\n+        return new Vector<T>(chunk);\n+    }\n+\n+    /// <summary>\n+    /// Performs the actual reduction operation on a collection of vectors.\n+    /// </summary>\n+    private Vector<T> PerformReduction(List<Vector<T>> vectors, ReductionOperation operation)\n+    {\n+        if (vectors == null || vectors.Count == 0)\n+        {\n+            throw new ArgumentException(\"Cannot reduce empty vector list.\");\n+        }\n+\n+        int length = vectors[0].Length;\n+        var result = new T[length];\n+\n+        for (int i = 0; i < length; i++)\n+        {\n+            T value = vectors[0][i];\n+\n+            for (int j = 1; j < vectors.Count; j++)\n+            {\n+                value = ApplyOperation(value, vectors[j][i], operation);\n+            }\n+\n+            // Apply averaging if needed\n+            if (operation == ReductionOperation.Average)\n+            {\n+                var count = _numOps.FromDouble(vectors.Count);\n+                value = _numOps.Divide(value, count);\n+            }"}]}},{"id":"PRRT_kwDOKSXUF85g9V9z","isResolved":false,"isOutdated":false,"path":"src/DistributedTraining/InMemoryCommunicationBackend.cs","line":65,"originalLine":38,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VG01f","author":{"login":"copilot-pull-request-reviewer"},"body":"The static shared state (_sharedBuffers, _barrierCounters) is shared across all instances of InMemoryCommunicationBackend, which could cause issues if multiple independent distributed training sessions are running concurrently in the same process. Consider namespacing these by a session ID or making them instance-specific with a shared coordinator.","createdAt":"2025-11-07T03:54:10Z","path":"src/DistributedTraining/InMemoryCommunicationBackend.cs","line":65,"originalLine":38,"diffHunk":"@@ -0,0 +1,484 @@\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.NumericOperations;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// A simple in-memory implementation of distributed communication for testing and single-machine scenarios.\n+/// This backend simulates multiple processes by using shared memory and locks.\n+///\n+/// For Beginners:\n+/// This is a \"fake\" distributed system that runs on a single machine.\n+/// It's perfect for testing your distributed code without needing multiple GPUs or machines.\n+/// Think of it as a practice mode - it simulates distributed behavior but everything\n+/// runs in one process.\n+///\n+/// Example:\n+/// <code>\n+/// // Create a simulated distributed environment with 4 \"processes\"\n+/// var backend = new InMemoryCommunicationBackend&lt;double&gt;(rank: 0, worldSize: 4);\n+/// CommunicationManager.Initialize(backend);\n+/// </code>\n+///\n+/// Note: For production distributed training, you would use an MPI-based backend instead.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type for operations</typeparam>\n+public class InMemoryCommunicationBackend<T> : ICommunicationBackend<T> where T : struct\n+{\n+    private readonly int _rank;\n+    private readonly int _worldSize;\n+    private readonly INumericOperations<T> _numOps;\n+    private bool _isInitialized;\n+\n+    // Shared state for simulating collective operations\n+    // In a real implementation, this would be handled by the MPI backend\n+    private static readonly object _globalLock = new object();\n+    private static readonly Dictionary<string, List<Vector<T>>> _sharedBuffers = new();\n+    private static readonly Dictionary<string, int> _barrierCounters = new();"}]}},{"id":"PRRT_kwDOKSXUF85g9V95","isResolved":true,"isOutdated":true,"path":"src/DistributedTraining/InMemoryCommunicationBackend.cs","line":null,"originalLine":300,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VG01m","author":{"login":"copilot-pull-request-reviewer"},"body":"Inefficient use of 'ContainsKey' and [indexer](1).","createdAt":"2025-11-07T03:54:10Z","path":"src/DistributedTraining/InMemoryCommunicationBackend.cs","line":null,"originalLine":300,"diffHunk":"@@ -0,0 +1,484 @@\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.NumericOperations;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// A simple in-memory implementation of distributed communication for testing and single-machine scenarios.\n+/// This backend simulates multiple processes by using shared memory and locks.\n+///\n+/// For Beginners:\n+/// This is a \"fake\" distributed system that runs on a single machine.\n+/// It's perfect for testing your distributed code without needing multiple GPUs or machines.\n+/// Think of it as a practice mode - it simulates distributed behavior but everything\n+/// runs in one process.\n+///\n+/// Example:\n+/// <code>\n+/// // Create a simulated distributed environment with 4 \"processes\"\n+/// var backend = new InMemoryCommunicationBackend&lt;double&gt;(rank: 0, worldSize: 4);\n+/// CommunicationManager.Initialize(backend);\n+/// </code>\n+///\n+/// Note: For production distributed training, you would use an MPI-based backend instead.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type for operations</typeparam>\n+public class InMemoryCommunicationBackend<T> : ICommunicationBackend<T> where T : struct\n+{\n+    private readonly int _rank;\n+    private readonly int _worldSize;\n+    private readonly INumericOperations<T> _numOps;\n+    private bool _isInitialized;\n+\n+    // Shared state for simulating collective operations\n+    // In a real implementation, this would be handled by the MPI backend\n+    private static readonly object _globalLock = new object();\n+    private static readonly Dictionary<string, List<Vector<T>>> _sharedBuffers = new();\n+    private static readonly Dictionary<string, int> _barrierCounters = new();\n+\n+    /// <inheritdoc/>\n+    public int Rank => _rank;\n+\n+    /// <inheritdoc/>\n+    public int WorldSize => _worldSize;\n+\n+    /// <inheritdoc/>\n+    public bool IsInitialized => _isInitialized;\n+\n+    /// <summary>\n+    /// Creates a new in-memory communication backend.\n+    ///\n+    /// For Beginners:\n+    /// You create one of these for each simulated \"process\". If you want to simulate\n+    /// 4 GPUs, you create 4 instances with ranks 0, 1, 2, 3, all with worldSize=4.\n+    /// </summary>\n+    /// <param name=\"rank\">The rank (ID) of this simulated process (0-based)</param>\n+    /// <param name=\"worldSize\">The total number of simulated processes</param>\n+    /// <exception cref=\"ArgumentException\">Thrown if rank or worldSize are invalid</exception>\n+    public InMemoryCommunicationBackend(int rank, int worldSize)\n+    {\n+        if (rank < 0 || rank >= worldSize)\n+        {\n+            throw new ArgumentException(\n+                $\"Invalid rank {rank}. Must be between 0 and {worldSize - 1}.\");\n+        }\n+\n+        if (worldSize <= 0)\n+        {\n+            throw new ArgumentException(\n+                $\"Invalid worldSize {worldSize}. Must be positive.\");\n+        }\n+\n+        _rank = rank;\n+        _worldSize = worldSize;\n+        _numOps = MathHelper.GetNumericOperations<T>();\n+        _isInitialized = false;\n+    }\n+\n+    /// <inheritdoc/>\n+    public void Initialize()\n+    {\n+        lock (_globalLock)\n+        {\n+            if (_isInitialized)\n+            {\n+                return;\n+            }\n+\n+            _isInitialized = true;\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public void Shutdown()\n+    {\n+        lock (_globalLock)\n+        {\n+            if (!_isInitialized)\n+            {\n+                return;\n+            }\n+\n+            // Clear any remaining shared state\n+            _sharedBuffers.Clear();\n+            _barrierCounters.Clear();\n+\n+            _isInitialized = false;\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public void Barrier()\n+    {\n+        EnsureInitialized();\n+\n+        lock (_globalLock)\n+        {\n+            string barrierId = $\"barrier_{DateTime.UtcNow.Ticks}\";\n+\n+            if (!_barrierCounters.ContainsKey(barrierId))\n+            {\n+                _barrierCounters[barrierId] = 0;\n+            }\n+\n+            _barrierCounters[barrierId]++;\n+\n+            // Wait until all processes have reached the barrier\n+            while (_barrierCounters[barrierId] < _worldSize)\n+            {\n+                Monitor.Wait(_globalLock, 10);\n+            }\n+\n+            Monitor.PulseAll(_globalLock);\n+\n+            // Cleanup\n+            if (_rank == 0)\n+            {\n+                _barrierCounters.Remove(barrierId);\n+            }\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public void AllReduce(Vector<T> data, ReductionOperation operation)\n+    {\n+        EnsureInitialized();\n+\n+        if (data == null)\n+        {\n+            throw new ArgumentNullException(nameof(data));\n+        }\n+\n+        // For single process, no communication needed\n+        if (_worldSize == 1)\n+        {\n+            if (operation == ReductionOperation.Average)\n+            {\n+                // Already averaged (only one value)\n+            }\n+            return;\n+        }\n+\n+        string bufferId = $\"allreduce_{Guid.NewGuid()}\";\n+\n+        lock (_globalLock)\n+        {\n+            // Initialize shared buffer\n+            if (!_sharedBuffers.ContainsKey(bufferId))\n+            {\n+                _sharedBuffers[bufferId] = new List<Vector<T>>();\n+            }\n+\n+            // Contribute local data\n+            _sharedBuffers[bufferId].Add(data.Clone());\n+\n+            // Wait until all processes have contributed\n+            while (_sharedBuffers[bufferId].Count < _worldSize)\n+            {\n+                Monitor.Wait(_globalLock, 10);\n+            }\n+\n+            Monitor.PulseAll(_globalLock);\n+\n+            // Perform reduction\n+            var allData = _sharedBuffers[bufferId];\n+            var result = PerformReduction(allData, operation);\n+\n+            // Copy result back to input data\n+            for (int i = 0; i < data.Length; i++)\n+            {\n+                data[i] = result[i];\n+            }\n+\n+            // Cleanup (rank 0 cleans up)\n+            if (_rank == 0)\n+            {\n+                _sharedBuffers.Remove(bufferId);\n+            }\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public Vector<T> AllGather(Vector<T> sendData)\n+    {\n+        EnsureInitialized();\n+\n+        if (sendData == null)\n+        {\n+            throw new ArgumentNullException(nameof(sendData));\n+        }\n+\n+        // For single process, just return a copy\n+        if (_worldSize == 1)\n+        {\n+            return sendData.Clone();\n+        }\n+\n+        string bufferId = $\"allgather_{Guid.NewGuid()}\";\n+\n+        lock (_globalLock)\n+        {\n+            // Initialize shared buffer\n+            if (!_sharedBuffers.ContainsKey(bufferId))\n+            {\n+                _sharedBuffers[bufferId] = new List<Vector<T>>(new Vector<T>[_worldSize]);\n+            }\n+\n+            // Contribute local data\n+            _sharedBuffers[bufferId][_rank] = sendData.Clone();\n+\n+            // Wait until all processes have contributed\n+            int contributedCount = 0;\n+            while (contributedCount < _worldSize)\n+            {\n+                contributedCount = _sharedBuffers[bufferId].Count(v => v != null);\n+                if (contributedCount < _worldSize)\n+                {\n+                    Monitor.Wait(_globalLock, 10);\n+                }\n+            }\n+\n+            Monitor.PulseAll(_globalLock);\n+\n+            // Concatenate all data\n+            var allData = _sharedBuffers[bufferId];\n+            int totalLength = allData.Sum(v => v.Length);\n+            var result = new T[totalLength];\n+            int offset = 0;\n+\n+            for (int i = 0; i < _worldSize; i++)\n+            {\n+                var data = allData[i];\n+                Array.Copy(data.ToArray(), 0, result, offset, data.Length);\n+                offset += data.Length;\n+            }\n+\n+            // Cleanup\n+            if (_rank == 0)\n+            {\n+                _sharedBuffers.Remove(bufferId);\n+            }\n+\n+            return new Vector<T>(result);\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public Vector<T> Broadcast(Vector<T> data, int root = 0)\n+    {\n+        EnsureInitialized();\n+\n+        if (root < 0 || root >= _worldSize)\n+        {\n+            throw new ArgumentException($\"Invalid root {root}. Must be between 0 and {_worldSize - 1}.\");\n+        }\n+\n+        // For single process, just return a copy\n+        if (_worldSize == 1)\n+        {\n+            return data?.Clone() ?? throw new ArgumentNullException(nameof(data));\n+        }\n+\n+        string bufferId = $\"broadcast_{Guid.NewGuid()}\";\n+\n+        lock (_globalLock)\n+        {\n+            Vector<T> result;\n+\n+            // Root process stores the data\n+            if (_rank == root)\n+            {\n+                if (data == null)\n+                {\n+                    throw new ArgumentNullException(nameof(data), \"Data cannot be null on root process.\");\n+                }\n+                _sharedBuffers[bufferId] = new List<Vector<T>> { data.Clone() };\n+            }\n+\n+            // Wait for root to store data\n+            while (!_sharedBuffers.ContainsKey(bufferId))"}]}},{"id":"PRRT_kwDOKSXUF85g9V-A","isResolved":true,"isOutdated":true,"path":"src/DistributedTraining/InMemoryCommunicationBackend.cs","line":null,"originalLine":366,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VG01v","author":{"login":"copilot-pull-request-reviewer"},"body":"Inefficient use of 'ContainsKey' and [indexer](1).","createdAt":"2025-11-07T03:54:10Z","path":"src/DistributedTraining/InMemoryCommunicationBackend.cs","line":null,"originalLine":366,"diffHunk":"@@ -0,0 +1,484 @@\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.NumericOperations;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// A simple in-memory implementation of distributed communication for testing and single-machine scenarios.\n+/// This backend simulates multiple processes by using shared memory and locks.\n+///\n+/// For Beginners:\n+/// This is a \"fake\" distributed system that runs on a single machine.\n+/// It's perfect for testing your distributed code without needing multiple GPUs or machines.\n+/// Think of it as a practice mode - it simulates distributed behavior but everything\n+/// runs in one process.\n+///\n+/// Example:\n+/// <code>\n+/// // Create a simulated distributed environment with 4 \"processes\"\n+/// var backend = new InMemoryCommunicationBackend&lt;double&gt;(rank: 0, worldSize: 4);\n+/// CommunicationManager.Initialize(backend);\n+/// </code>\n+///\n+/// Note: For production distributed training, you would use an MPI-based backend instead.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type for operations</typeparam>\n+public class InMemoryCommunicationBackend<T> : ICommunicationBackend<T> where T : struct\n+{\n+    private readonly int _rank;\n+    private readonly int _worldSize;\n+    private readonly INumericOperations<T> _numOps;\n+    private bool _isInitialized;\n+\n+    // Shared state for simulating collective operations\n+    // In a real implementation, this would be handled by the MPI backend\n+    private static readonly object _globalLock = new object();\n+    private static readonly Dictionary<string, List<Vector<T>>> _sharedBuffers = new();\n+    private static readonly Dictionary<string, int> _barrierCounters = new();\n+\n+    /// <inheritdoc/>\n+    public int Rank => _rank;\n+\n+    /// <inheritdoc/>\n+    public int WorldSize => _worldSize;\n+\n+    /// <inheritdoc/>\n+    public bool IsInitialized => _isInitialized;\n+\n+    /// <summary>\n+    /// Creates a new in-memory communication backend.\n+    ///\n+    /// For Beginners:\n+    /// You create one of these for each simulated \"process\". If you want to simulate\n+    /// 4 GPUs, you create 4 instances with ranks 0, 1, 2, 3, all with worldSize=4.\n+    /// </summary>\n+    /// <param name=\"rank\">The rank (ID) of this simulated process (0-based)</param>\n+    /// <param name=\"worldSize\">The total number of simulated processes</param>\n+    /// <exception cref=\"ArgumentException\">Thrown if rank or worldSize are invalid</exception>\n+    public InMemoryCommunicationBackend(int rank, int worldSize)\n+    {\n+        if (rank < 0 || rank >= worldSize)\n+        {\n+            throw new ArgumentException(\n+                $\"Invalid rank {rank}. Must be between 0 and {worldSize - 1}.\");\n+        }\n+\n+        if (worldSize <= 0)\n+        {\n+            throw new ArgumentException(\n+                $\"Invalid worldSize {worldSize}. Must be positive.\");\n+        }\n+\n+        _rank = rank;\n+        _worldSize = worldSize;\n+        _numOps = MathHelper.GetNumericOperations<T>();\n+        _isInitialized = false;\n+    }\n+\n+    /// <inheritdoc/>\n+    public void Initialize()\n+    {\n+        lock (_globalLock)\n+        {\n+            if (_isInitialized)\n+            {\n+                return;\n+            }\n+\n+            _isInitialized = true;\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public void Shutdown()\n+    {\n+        lock (_globalLock)\n+        {\n+            if (!_isInitialized)\n+            {\n+                return;\n+            }\n+\n+            // Clear any remaining shared state\n+            _sharedBuffers.Clear();\n+            _barrierCounters.Clear();\n+\n+            _isInitialized = false;\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public void Barrier()\n+    {\n+        EnsureInitialized();\n+\n+        lock (_globalLock)\n+        {\n+            string barrierId = $\"barrier_{DateTime.UtcNow.Ticks}\";\n+\n+            if (!_barrierCounters.ContainsKey(barrierId))\n+            {\n+                _barrierCounters[barrierId] = 0;\n+            }\n+\n+            _barrierCounters[barrierId]++;\n+\n+            // Wait until all processes have reached the barrier\n+            while (_barrierCounters[barrierId] < _worldSize)\n+            {\n+                Monitor.Wait(_globalLock, 10);\n+            }\n+\n+            Monitor.PulseAll(_globalLock);\n+\n+            // Cleanup\n+            if (_rank == 0)\n+            {\n+                _barrierCounters.Remove(barrierId);\n+            }\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public void AllReduce(Vector<T> data, ReductionOperation operation)\n+    {\n+        EnsureInitialized();\n+\n+        if (data == null)\n+        {\n+            throw new ArgumentNullException(nameof(data));\n+        }\n+\n+        // For single process, no communication needed\n+        if (_worldSize == 1)\n+        {\n+            if (operation == ReductionOperation.Average)\n+            {\n+                // Already averaged (only one value)\n+            }\n+            return;\n+        }\n+\n+        string bufferId = $\"allreduce_{Guid.NewGuid()}\";\n+\n+        lock (_globalLock)\n+        {\n+            // Initialize shared buffer\n+            if (!_sharedBuffers.ContainsKey(bufferId))\n+            {\n+                _sharedBuffers[bufferId] = new List<Vector<T>>();\n+            }\n+\n+            // Contribute local data\n+            _sharedBuffers[bufferId].Add(data.Clone());\n+\n+            // Wait until all processes have contributed\n+            while (_sharedBuffers[bufferId].Count < _worldSize)\n+            {\n+                Monitor.Wait(_globalLock, 10);\n+            }\n+\n+            Monitor.PulseAll(_globalLock);\n+\n+            // Perform reduction\n+            var allData = _sharedBuffers[bufferId];\n+            var result = PerformReduction(allData, operation);\n+\n+            // Copy result back to input data\n+            for (int i = 0; i < data.Length; i++)\n+            {\n+                data[i] = result[i];\n+            }\n+\n+            // Cleanup (rank 0 cleans up)\n+            if (_rank == 0)\n+            {\n+                _sharedBuffers.Remove(bufferId);\n+            }\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public Vector<T> AllGather(Vector<T> sendData)\n+    {\n+        EnsureInitialized();\n+\n+        if (sendData == null)\n+        {\n+            throw new ArgumentNullException(nameof(sendData));\n+        }\n+\n+        // For single process, just return a copy\n+        if (_worldSize == 1)\n+        {\n+            return sendData.Clone();\n+        }\n+\n+        string bufferId = $\"allgather_{Guid.NewGuid()}\";\n+\n+        lock (_globalLock)\n+        {\n+            // Initialize shared buffer\n+            if (!_sharedBuffers.ContainsKey(bufferId))\n+            {\n+                _sharedBuffers[bufferId] = new List<Vector<T>>(new Vector<T>[_worldSize]);\n+            }\n+\n+            // Contribute local data\n+            _sharedBuffers[bufferId][_rank] = sendData.Clone();\n+\n+            // Wait until all processes have contributed\n+            int contributedCount = 0;\n+            while (contributedCount < _worldSize)\n+            {\n+                contributedCount = _sharedBuffers[bufferId].Count(v => v != null);\n+                if (contributedCount < _worldSize)\n+                {\n+                    Monitor.Wait(_globalLock, 10);\n+                }\n+            }\n+\n+            Monitor.PulseAll(_globalLock);\n+\n+            // Concatenate all data\n+            var allData = _sharedBuffers[bufferId];\n+            int totalLength = allData.Sum(v => v.Length);\n+            var result = new T[totalLength];\n+            int offset = 0;\n+\n+            for (int i = 0; i < _worldSize; i++)\n+            {\n+                var data = allData[i];\n+                Array.Copy(data.ToArray(), 0, result, offset, data.Length);\n+                offset += data.Length;\n+            }\n+\n+            // Cleanup\n+            if (_rank == 0)\n+            {\n+                _sharedBuffers.Remove(bufferId);\n+            }\n+\n+            return new Vector<T>(result);\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public Vector<T> Broadcast(Vector<T> data, int root = 0)\n+    {\n+        EnsureInitialized();\n+\n+        if (root < 0 || root >= _worldSize)\n+        {\n+            throw new ArgumentException($\"Invalid root {root}. Must be between 0 and {_worldSize - 1}.\");\n+        }\n+\n+        // For single process, just return a copy\n+        if (_worldSize == 1)\n+        {\n+            return data?.Clone() ?? throw new ArgumentNullException(nameof(data));\n+        }\n+\n+        string bufferId = $\"broadcast_{Guid.NewGuid()}\";\n+\n+        lock (_globalLock)\n+        {\n+            Vector<T> result;\n+\n+            // Root process stores the data\n+            if (_rank == root)\n+            {\n+                if (data == null)\n+                {\n+                    throw new ArgumentNullException(nameof(data), \"Data cannot be null on root process.\");\n+                }\n+                _sharedBuffers[bufferId] = new List<Vector<T>> { data.Clone() };\n+            }\n+\n+            // Wait for root to store data\n+            while (!_sharedBuffers.ContainsKey(bufferId))\n+            {\n+                Monitor.Wait(_globalLock, 10);\n+            }\n+\n+            Monitor.PulseAll(_globalLock);\n+\n+            // All processes retrieve the data\n+            result = _sharedBuffers[bufferId][0].Clone();\n+\n+            // Cleanup\n+            if (_rank == 0)\n+            {\n+                _sharedBuffers.Remove(bufferId);\n+            }\n+\n+            return result;\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public Vector<T> Scatter(Vector<T> sendData, int root = 0)\n+    {\n+        EnsureInitialized();\n+\n+        if (root < 0 || root >= _worldSize)\n+        {\n+            throw new ArgumentException($\"Invalid root {root}. Must be between 0 and {_worldSize - 1}.\");\n+        }\n+\n+        // For single process, just return a copy\n+        if (_worldSize == 1)\n+        {\n+            return sendData?.Clone() ?? throw new ArgumentNullException(nameof(sendData));\n+        }\n+\n+        string bufferId = $\"scatter_{Guid.NewGuid()}\";\n+\n+        lock (_globalLock)\n+        {\n+            // Root process splits and stores the data\n+            if (_rank == root)\n+            {\n+                if (sendData == null)\n+                {\n+                    throw new ArgumentNullException(nameof(sendData), \"Data cannot be null on root process.\");\n+                }\n+\n+                if (sendData.Length % _worldSize != 0)\n+                {\n+                    throw new ArgumentException(\n+                        $\"Data length {sendData.Length} must be divisible by world size {_worldSize}.\");\n+                }\n+\n+                int chunkSize = sendData.Length / _worldSize;\n+                _sharedBuffers[bufferId] = new List<Vector<T>>();\n+\n+                for (int i = 0; i < _worldSize; i++)\n+                {\n+                    var chunk = new T[chunkSize];\n+                    Array.Copy(sendData.ToArray(), i * chunkSize, chunk, 0, chunkSize);\n+                    _sharedBuffers[bufferId].Add(new Vector<T>(chunk));\n+                }\n+            }\n+\n+            // Wait for root to split data\n+            while (!_sharedBuffers.ContainsKey(bufferId))"}]}},{"id":"PRRT_kwDOKSXUF85g9V-F","isResolved":true,"isOutdated":false,"path":"src/DistributedTraining/FSDPModel.cs","line":208,"originalLine":286,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VG010","author":{"login":"copilot-pull-request-reviewer"},"body":"This assignment to [savedRank](1) is useless, since its value is never read.\n```suggestion\n        reader.ReadInt32(); // savedRank (unused)\n```","createdAt":"2025-11-07T03:54:10Z","path":"src/DistributedTraining/FSDPModel.cs","line":208,"originalLine":286,"diffHunk":"@@ -0,0 +1,345 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.Models;\n+using AiDotNet.NumericOperations;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Implements a distributed model wrapper that shards parameters across multiple processes.\n+///\n+/// For Beginners:\n+/// This class wraps any existing model and makes it work across multiple GPUs or machines.\n+/// It automatically handles:\n+/// - Splitting parameters across processes (sharding)\n+/// - Gathering parameters when needed for forward pass\n+/// - Averaging gradients across all processes during training\n+///\n+/// Think of it like a team project where each person holds part of the solution.\n+/// When you need the full solution, everyone shares their part (AllGather).\n+/// When everyone learns something new, they share and average their learnings (AllReduce).\n+///\n+/// Example:\n+/// <code>\n+/// // Original model\n+/// var model = new NeuralNetworkModel&lt;double&gt;(...);\n+///\n+/// // Wrap it for distributed training\n+/// var backend = new InMemoryCommunicationBackend&lt;double&gt;(rank: 0, worldSize: 4);\n+/// var config = new ShardingConfiguration&lt;double&gt;(backend);\n+/// var distributedModel = new ShardedModel&lt;double, Tensor&lt;double&gt;, Tensor&lt;double&gt;&gt;(model, config);\n+///\n+/// // Now train as usual - distributed magic happens automatically!\n+/// distributedModel.Train(inputs, outputs);\n+/// </code>\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type</typeparam>\n+/// <typeparam name=\"TInput\">The input type for the model</typeparam>\n+/// <typeparam name=\"TOutput\">The output type for the model</typeparam>\n+public class ShardedModel<T, TInput, TOutput> : IShardedModel<T, TInput, TOutput> where T : struct\n+{\n+    private readonly IFullModel<T, TInput, TOutput> _wrappedModel;\n+    private readonly IShardingConfiguration<T> _config;\n+    private readonly INumericOperations<T> _numOps;\n+    private Vector<T> _localParameterShard;\n+    private Vector<T>? _cachedFullParameters;\n+    private int _shardStartIndex;\n+    private int _shardSize;\n+\n+    /// <inheritdoc/>\n+    public IFullModel<T, TInput, TOutput> WrappedModel => _wrappedModel;\n+\n+    /// <inheritdoc/>\n+    public int Rank => _config.CommunicationBackend.Rank;\n+\n+    /// <inheritdoc/>\n+    public int WorldSize => _config.CommunicationBackend.WorldSize;\n+\n+    /// <inheritdoc/>\n+    public Vector<T> LocalParameterShard => _localParameterShard;\n+\n+    /// <inheritdoc/>\n+    public IShardingConfiguration<T> ShardingConfiguration => _config;\n+\n+    /// <inheritdoc/>\n+    public int ParameterCount => _wrappedModel.ParameterCount;\n+\n+    /// <summary>\n+    /// Creates a new sharded model wrapping an existing model.\n+    ///\n+    /// For Beginners:\n+    /// This constructor takes your existing model and makes it distributed.\n+    /// You provide:\n+    /// 1. The model you want to make distributed\n+    /// 2. A configuration that tells us how to do the distribution\n+    ///\n+    /// The constructor automatically:\n+    /// - Splits the model's parameters across all processes\n+    /// - Sets up communication channels\n+    /// - Prepares everything for distributed training\n+    /// </summary>\n+    /// <param name=\"wrappedModel\">The model to wrap with distributed capabilities</param>\n+    /// <param name=\"config\">Configuration for sharding and communication</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown if model or config is null</exception>\n+    public ShardedModel(IFullModel<T, TInput, TOutput> wrappedModel, IShardingConfiguration<T> config)\n+    {\n+        _wrappedModel = wrappedModel ?? throw new ArgumentNullException(nameof(wrappedModel));\n+        _config = config ?? throw new ArgumentNullException(nameof(config));\n+        _numOps = MathHelper.GetNumericOperations<T>();\n+\n+        // Initialize the communication backend if not already done\n+        if (!_config.CommunicationBackend.IsInitialized)\n+        {\n+            _config.CommunicationBackend.Initialize();\n+        }\n+\n+        // Shard the model parameters across processes\n+        InitializeSharding();\n+    }\n+\n+    /// <summary>\n+    /// Initializes parameter sharding by dividing parameters across processes.\n+    ///\n+    /// For Beginners:\n+    /// This method splits the model's parameters into chunks and gives each\n+    /// process its own chunk to manage. It's like dividing a deck of cards\n+    /// evenly among players.\n+    /// </summary>\n+    private void InitializeSharding()\n+    {\n+        var fullParameters = _wrappedModel.GetParameters();\n+        int totalParams = fullParameters.Length;\n+\n+        // Calculate shard size for this process\n+        int baseShardSize = totalParams / WorldSize;\n+        int remainder = totalParams % WorldSize;\n+\n+        // Distribute remainder among first 'remainder' processes\n+        _shardSize = baseShardSize + (Rank < remainder ? 1 : 0);\n+        _shardStartIndex = Rank * baseShardSize + Math.Min(Rank, remainder);\n+\n+        // Extract local shard\n+        var shardData = new T[_shardSize];\n+        Array.Copy(fullParameters.ToArray(), _shardStartIndex, shardData, 0, _shardSize);\n+        _localParameterShard = new Vector<T>(shardData);\n+\n+        // Cache invalidated\n+        _cachedFullParameters = null;\n+    }\n+\n+    /// <inheritdoc/>\n+    public Vector<T> GatherFullParameters()\n+    {\n+        // Use cached version if available\n+        if (_cachedFullParameters != null)\n+        {\n+            return _cachedFullParameters;\n+        }\n+\n+        // Gather parameters from all processes\n+        var gathered = _config.CommunicationBackend.AllGather(_localParameterShard);\n+        _cachedFullParameters = gathered;\n+        return gathered;\n+    }\n+\n+    /// <inheritdoc/>\n+    public void SynchronizeGradients()\n+    {\n+        // Get gradients from wrapped model\n+        // Note: This assumes the model exposes gradients somehow.\n+        // For now, we'll just synchronize the local parameter shard\n+        // In a real implementation, gradients would be tracked separately\n+\n+        // Perform AllReduce with average operation\n+        _config.CommunicationBackend.AllReduce(_localParameterShard, ReductionOperation.Average);\n+\n+        // Invalidate cached full parameters\n+        _cachedFullParameters = null;\n+    }\n+\n+    /// <inheritdoc/>\n+    public void Train(TInput input, TOutput expectedOutput)\n+    {\n+        // Gather full parameters for training\n+        var fullParams = GatherFullParameters();\n+        _wrappedModel.SetParameters(fullParams);\n+\n+        // Train the wrapped model\n+        _wrappedModel.Train(input, expectedOutput);\n+\n+        // Get updated parameters\n+        var updatedParams = _wrappedModel.GetParameters();\n+\n+        // Update local shard\n+        var shardData = new T[_shardSize];\n+        Array.Copy(updatedParams.ToArray(), _shardStartIndex, shardData, 0, _shardSize);\n+        _localParameterShard = new Vector<T>(shardData);\n+\n+        // Synchronize gradients if auto-sync is enabled\n+        if (_config.AutoSyncGradients)\n+        {\n+            SynchronizeGradients();\n+\n+            // Apply synchronized parameters back to the model\n+            fullParams = GatherFullParameters();\n+            _wrappedModel.SetParameters(fullParams);\n+        }\n+\n+        // Invalidate cache\n+        _cachedFullParameters = null;\n+    }\n+\n+    /// <inheritdoc/>\n+    public TOutput Predict(TInput input)\n+    {\n+        // Gather full parameters for prediction\n+        var fullParams = GatherFullParameters();\n+        _wrappedModel.SetParameters(fullParams);\n+\n+        // Use wrapped model for prediction\n+        return _wrappedModel.Predict(input);\n+    }\n+\n+    /// <inheritdoc/>\n+    public ModelMetadata<T> GetModelMetadata()\n+    {\n+        var metadata = _wrappedModel.GetModelMetadata();\n+\n+        // Add distributed training info\n+        metadata.SetProperty(\"IsDistributed\", true);\n+        metadata.SetProperty(\"WorldSize\", WorldSize);\n+        metadata.SetProperty(\"Rank\", Rank);\n+        metadata.SetProperty(\"ShardSize\", _shardSize);\n+\n+        return metadata;\n+    }\n+\n+    /// <inheritdoc/>\n+    public Vector<T> GetParameters()\n+    {\n+        return GatherFullParameters();\n+    }\n+\n+    /// <inheritdoc/>\n+    public void SetParameters(Vector<T> parameters)\n+    {\n+        if (parameters == null)\n+        {\n+            throw new ArgumentNullException(nameof(parameters));\n+        }\n+\n+        if (parameters.Length != ParameterCount)\n+        {\n+            throw new ArgumentException(\n+                $\"Parameter count mismatch. Expected {ParameterCount}, got {parameters.Length}.\");\n+        }\n+\n+        // Update local shard\n+        var shardData = new T[_shardSize];\n+        Array.Copy(parameters.ToArray(), _shardStartIndex, shardData, 0, _shardSize);\n+        _localParameterShard = new Vector<T>(shardData);\n+\n+        // Invalidate cache\n+        _cachedFullParameters = null;\n+\n+        // Update wrapped model\n+        _wrappedModel.SetParameters(parameters);\n+    }\n+\n+    /// <inheritdoc/>\n+    public IFullModel<T, TInput, TOutput> WithParameters(Vector<T> parameters)\n+    {\n+        var newModel = _wrappedModel.WithParameters(parameters);\n+        return new ShardedModel<T, TInput, TOutput>(newModel, _config);\n+    }\n+\n+    /// <inheritdoc/>\n+    public byte[] Serialize()\n+    {\n+        using var ms = new MemoryStream();\n+        using var writer = new BinaryWriter(ms);\n+\n+        // Serialize sharding configuration info\n+        writer.Write(WorldSize);\n+        writer.Write(Rank);\n+        writer.Write(_config.AutoSyncGradients);\n+        writer.Write(_config.MinimumParameterGroupSize);\n+        writer.Write(_config.EnableGradientCompression);\n+\n+        // Serialize wrapped model\n+        var modelData = _wrappedModel.Serialize();\n+        writer.Write(modelData.Length);\n+        writer.Write(modelData);\n+\n+        return ms.ToArray();\n+    }\n+\n+    /// <inheritdoc/>\n+    public void Deserialize(byte[] data)\n+    {\n+        using var ms = new MemoryStream(data);\n+        using var reader = new BinaryReader(ms);\n+\n+        // Read sharding configuration (for validation)\n+        int savedWorldSize = reader.ReadInt32();\n+        int savedRank = reader.ReadInt32();"}]}},{"id":"PRRT_kwDOKSXUF85g9V-L","isResolved":true,"isOutdated":false,"path":"src/DistributedTraining/FSDPOptimizer.cs","line":194,"originalLine":226,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VG017","author":{"login":"copilot-pull-request-reviewer"},"body":"This assignment to [savedRank](1) is useless, since its value is never read.\n```suggestion\n        reader.ReadInt32(); // Rank\n```","createdAt":"2025-11-07T03:54:10Z","path":"src/DistributedTraining/FSDPOptimizer.cs","line":194,"originalLine":226,"diffHunk":"@@ -0,0 +1,268 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.Models.Inputs;\n+using AiDotNet.Optimizers;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Implements a distributed optimizer wrapper that coordinates optimization across multiple processes.\n+///\n+/// For Beginners:\n+/// This class wraps any existing optimizer (like Adam, SGD, etc.) and makes it work across\n+/// multiple GPUs or machines. It automatically handles:\n+/// - Synchronizing gradients across all processes\n+/// - Coordinating parameter updates\n+/// - Ensuring all processes stay in sync\n+///\n+/// Think of it like a team of coaches working together - each has their own expertise\n+/// (the wrapped optimizer), but they coordinate their efforts to train the team effectively.\n+///\n+/// Example:\n+/// <code>\n+/// // Original optimizer\n+/// var optimizer = new AdamOptimizer&lt;double, Tensor&lt;double&gt;, Tensor&lt;double&gt;&gt;(model, options);\n+///\n+/// // Wrap it for distributed training\n+/// var backend = new InMemoryCommunicationBackend&lt;double&gt;(rank: 0, worldSize: 4);\n+/// var config = new ShardingConfiguration&lt;double&gt;(backend);\n+/// var distributedOptimizer = new ShardedOptimizer&lt;double, Tensor&lt;double&gt;, Tensor&lt;double&gt;&gt;(\n+///     optimizer, config);\n+///\n+/// // Now optimize as usual - distributed magic happens automatically!\n+/// var result = distributedOptimizer.Optimize(inputData);\n+/// </code>\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type</typeparam>\n+/// <typeparam name=\"TInput\">The input type for the model</typeparam>\n+/// <typeparam name=\"TOutput\">The output type for the model</typeparam>\n+public class ShardedOptimizer<T, TInput, TOutput> : IShardedOptimizer<T, TInput, TOutput> where T : struct\n+{\n+    private readonly IOptimizer<T, TInput, TOutput> _wrappedOptimizer;\n+    private readonly IShardingConfiguration<T> _config;\n+\n+    /// <inheritdoc/>\n+    public IOptimizer<T, TInput, TOutput> WrappedOptimizer => _wrappedOptimizer;\n+\n+    /// <inheritdoc/>\n+    public int Rank => _config.CommunicationBackend.Rank;\n+\n+    /// <inheritdoc/>\n+    public int WorldSize => _config.CommunicationBackend.WorldSize;\n+\n+    /// <inheritdoc/>\n+    public IShardingConfiguration<T> ShardingConfiguration => _config;\n+\n+    /// <summary>\n+    /// Creates a new sharded optimizer wrapping an existing optimizer.\n+    ///\n+    /// For Beginners:\n+    /// This constructor takes your existing optimizer and makes it distributed.\n+    /// You provide:\n+    /// 1. The optimizer you want to make distributed\n+    /// 2. A configuration that tells us how to do the distribution\n+    ///\n+    /// The optimizer will automatically synchronize across all processes during optimization.\n+    /// </summary>\n+    /// <param name=\"wrappedOptimizer\">The optimizer to wrap with distributed capabilities</param>\n+    /// <param name=\"config\">Configuration for sharding and communication</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown if optimizer or config is null</exception>\n+    public ShardedOptimizer(\n+        IOptimizer<T, TInput, TOutput> wrappedOptimizer,\n+        IShardingConfiguration<T> config)\n+    {\n+        _wrappedOptimizer = wrappedOptimizer ?? throw new ArgumentNullException(nameof(wrappedOptimizer));\n+        _config = config ?? throw new ArgumentNullException(nameof(config));\n+\n+        // Initialize the communication backend if not already done\n+        if (!_config.CommunicationBackend.IsInitialized)\n+        {\n+            _config.CommunicationBackend.Initialize();\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public OptimizationResult<T, TInput, TOutput> Optimize(OptimizationInputData<T, TInput, TOutput> inputData)\n+    {\n+        if (inputData == null)\n+        {\n+            throw new ArgumentNullException(nameof(inputData));\n+        }\n+\n+        // Ensure all processes start together\n+        _config.CommunicationBackend.Barrier();\n+\n+        // If the model is a sharded model, it will handle gradient synchronization\n+        // Otherwise, we need to handle it ourselves\n+        bool modelHandlesSync = inputData.Model is IShardedModel<T, TInput, TOutput>;\n+\n+        // Perform optimization on the wrapped optimizer\n+        var result = _wrappedOptimizer.Optimize(inputData);\n+\n+        // If model doesn't handle synchronization and auto-sync is enabled, sync parameters\n+        if (!modelHandlesSync && _config.AutoSyncGradients)\n+        {\n+            SynchronizeParameters(result.BestModel);\n+        }\n+\n+        // Synchronize optimizer state if needed\n+        SynchronizeOptimizerState();\n+\n+        // Ensure all processes finish together\n+        _config.CommunicationBackend.Barrier();\n+\n+        return result;\n+    }\n+\n+    /// <inheritdoc/>\n+    public void SynchronizeOptimizerState()\n+    {\n+        // For now, this is a placeholder\n+        // In a full implementation, we would synchronize optimizer-specific state\n+        // like momentum buffers, variance estimates (for Adam), etc.\n+\n+        // Different optimizers have different state to sync:\n+        // - SGD with momentum: velocity vectors\n+        // - Adam: first and second moment estimates\n+        // - RMSprop: squared gradient moving average\n+\n+        // This would require either:\n+        // 1. Extending IOptimizer with state access methods\n+        // 2. Type-specific handling for known optimizer types\n+        // 3. A generic state serialization mechanism\n+\n+        // For the MVP, we assume stateless or that the wrapped optimizer handles its own state\n+    }\n+\n+    /// <summary>\n+    /// Synchronizes model parameters across all processes.\n+    ///\n+    /// For Beginners:\n+    /// After optimization, each process might have slightly different parameters\n+    /// (if they processed different data). This method averages the parameters\n+    /// across all processes so everyone has the same model.\n+    /// </summary>\n+    /// <param name=\"model\">The model whose parameters to synchronize</param>\n+    private void SynchronizeParameters(IFullModel<T, TInput, TOutput>? model)\n+    {\n+        if (model == null)\n+        {\n+            return;\n+        }\n+\n+        // Don't sync if it's already a sharded model (handles its own sync)\n+        if (model is IShardedModel<T, TInput, TOutput>)\n+        {\n+            return;\n+        }\n+\n+        // Get current parameters\n+        var parameters = model.GetParameters();\n+\n+        // Average parameters across all processes\n+        _config.CommunicationBackend.AllReduce(parameters, ReductionOperation.Average);\n+\n+        // Update model with averaged parameters\n+        model.SetParameters(parameters);\n+    }\n+\n+    /// <inheritdoc/>\n+    public bool ShouldEarlyStop()\n+    {\n+        // Delegate to wrapped optimizer\n+        bool localDecision = _wrappedOptimizer.ShouldEarlyStop();\n+\n+        // In distributed training, we need consensus on early stopping\n+        // All processes should agree to stop, otherwise some might continue while others stop\n+        // For now, we'll use a simple approach: if any process wants to stop, all stop\n+\n+        // Create a vector with the local decision (1 for stop, 0 for continue)\n+        var decision = new Vector<T>(new[] { localDecision ? MathHelper.GetNumericOperations<T>().One : MathHelper.GetNumericOperations<T>().Zero });\n+\n+        // Get the maximum across all processes\n+        // If any process returns 1 (stop), the max will be 1\n+        _config.CommunicationBackend.AllReduce(decision, ReductionOperation.Max);\n+\n+        // Check if the result indicates stopping\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        return !numOps.Equals(decision[0], numOps.Zero);\n+    }\n+\n+    /// <inheritdoc/>\n+    public OptimizationAlgorithmOptions<T, TInput, TOutput> GetOptions()\n+    {\n+        return _wrappedOptimizer.GetOptions();\n+    }\n+\n+    /// <inheritdoc/>\n+    public byte[] Serialize()\n+    {\n+        using var ms = new MemoryStream();\n+        using var writer = new BinaryWriter(ms);\n+\n+        // Serialize sharding configuration info\n+        writer.Write(WorldSize);\n+        writer.Write(Rank);\n+        writer.Write(_config.AutoSyncGradients);\n+        writer.Write(_config.MinimumParameterGroupSize);\n+        writer.Write(_config.EnableGradientCompression);\n+\n+        // Serialize wrapped optimizer\n+        var optimizerData = _wrappedOptimizer.Serialize();\n+        writer.Write(optimizerData.Length);\n+        writer.Write(optimizerData);\n+\n+        return ms.ToArray();\n+    }\n+\n+    /// <inheritdoc/>\n+    public void Deserialize(byte[] data)\n+    {\n+        using var ms = new MemoryStream(data);\n+        using var reader = new BinaryReader(ms);\n+\n+        // Read sharding configuration (for validation)\n+        int savedWorldSize = reader.ReadInt32();\n+        int savedRank = reader.ReadInt32();"}]}},{"id":"PRRT_kwDOKSXUF85hLkmL","isResolved":true,"isOutdated":false,"path":"scripts/launch-distributed-training.sh","line":123,"originalLine":98,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86Vb1EW","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**Automatically making files executable poses a security and correctness risk.**\n\nThe script unconditionally runs `chmod +x` on any non-executable file without verifying it's actually meant to be an executable (e.g., it could be a data file or config accidentally passed). Consider either removing this auto-chmod feature or adding a user confirmation prompt.\n\n\n\nApply this diff to prompt the user:\n\n```diff\n # Make program executable if it isn't\n if [ ! -x \"$PROGRAM\" ]; then\n-    echo \"Warning: Program is not executable. Making it executable...\"\n-    chmod +x \"$PROGRAM\"\n+    echo \"Warning: Program '$PROGRAM' is not executable.\"\n+    read -p \"Make it executable? (y/N): \" -n 1 -r\n+    echo\n+    if [[ $REPLY =~ ^[Yy]$ ]]; then\n+        chmod +x \"$PROGRAM\"\n+        echo \"Made executable.\"\n+    else\n+        echo \"Error: Program must be executable to run.\"\n+        exit 1\n+    fi\n fi\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\nif [ ! -x \"$PROGRAM\" ]; then\n    echo \"Warning: Program '$PROGRAM' is not executable.\"\n    read -p \"Make it executable? (y/N): \" -n 1 -r\n    echo\n    if [[ $REPLY =~ ^[Yy]$ ]]; then\n        chmod +x \"$PROGRAM\"\n        echo \"Made executable.\"\n    else\n        echo \"Error: Program must be executable to run.\"\n        exit 1\n    fi\nfi\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits c717ae2 to ce97e4f","createdAt":"2025-11-08T20:07:13Z","path":"scripts/launch-distributed-training.sh","line":123,"originalLine":98,"diffHunk":"@@ -0,0 +1,130 @@\n+#!/bin/bash\n+\n+################################################################################\n+# AiDotNet Distributed Training Launcher (Bash)\n+#\n+# This script launches distributed training using MPI across multiple processes.\n+#\n+# For Beginners:\n+# MPI (Message Passing Interface) is a standard for running programs across\n+# multiple computers or processors. Think of it like a coordinator that starts\n+# your program on multiple machines at once and helps them communicate.\n+#\n+# Usage:\n+#   ./launch-distributed-training.sh <num_processes> <program> [args...]\n+#\n+# Examples:\n+#   # Run on 4 GPUs locally\n+#   ./launch-distributed-training.sh 4 ./MyTrainingApp\n+#\n+#   # Run on 8 GPUs with additional arguments\n+#   ./launch-distributed-training.sh 8 ./MyTrainingApp --epochs 100 --lr 0.001\n+#\n+#   # Run across 2 machines with 4 GPUs each\n+#   ./launch-distributed-training.sh 8 ./MyTrainingApp --hosts machine1,machine2\n+################################################################################\n+\n+# Check if enough arguments provided\n+if [ \"$#\" -lt 2 ]; then\n+    echo \"Error: Insufficient arguments\"\n+    echo \"\"\n+    echo \"Usage: $0 <num_processes> <program> [args...]\"\n+    echo \"\"\n+    echo \"Arguments:\"\n+    echo \"  num_processes  - Number of processes to spawn (typically equals number of GPUs)\"\n+    echo \"  program        - Path to your training program executable\"\n+    echo \"  args           - Any additional arguments to pass to your program\"\n+    echo \"\"\n+    echo \"Examples:\"\n+    echo \"  $0 4 ./MyTrainingApp\"\n+    echo \"  $0 8 ./MyTrainingApp --epochs 100\"\n+    exit 1\n+fi\n+\n+# Parse arguments\n+NUM_PROCESSES=$1\n+PROGRAM=$2\n+shift 2\n+PROGRAM_ARGS=(\"$@\")\n+\n+echo \"======================================\"\n+echo \"AiDotNet Distributed Training Launcher\"\n+echo \"======================================\"\n+echo \"\"\n+echo \"Configuration:\"\n+echo \"  Number of processes: $NUM_PROCESSES\"\n+echo \"  Program: $PROGRAM\"\n+if [ \"${#PROGRAM_ARGS[@]}\" -gt 0 ]; then\n+    echo \"  Program arguments: ${PROGRAM_ARGS[*]}\"\n+else\n+    echo \"  Program arguments: (none)\"\n+fi\n+echo \"\"\n+\n+# Check if mpiexec/mpirun is available\n+if command -v mpiexec &> /dev/null; then\n+    MPI_CMD=\"mpiexec\"\n+elif command -v mpirun &> /dev/null; then\n+    MPI_CMD=\"mpirun\"\n+else\n+    echo \"Error: Neither mpiexec nor mpirun found in PATH\"\n+    echo \"\"\n+    echo \"For Beginners:\"\n+    echo \"  You need to install MPI to run distributed training.\"\n+    echo \"  On Ubuntu/Debian: sudo apt-get install mpich\"\n+    echo \"  On macOS: brew install mpich\"\n+    echo \"  On Windows: Install Microsoft MPI from https://docs.microsoft.com/en-us/message-passing-interface/microsoft-mpi\"\n+    exit 1\n+fi\n+\n+echo \"Using MPI command: $MPI_CMD\"\n+echo \"\"\n+\n+# Check if program exists\n+if [ ! -f \"$PROGRAM\" ]; then\n+    echo \"Error: Program '$PROGRAM' not found\"\n+    echo \"\"\n+    echo \"For Beginners:\"\n+    echo \"  Make sure you've built your training program and the path is correct.\"\n+    echo \"  Example: dotnet publish -c Release -o ./publish\"\n+    echo \"  Then use: $0 4 ./publish/MyTrainingApp\"\n+    exit 1\n+fi\n+\n+# Make program executable if it isn't\n+if [ ! -x \"$PROGRAM\" ]; then\n+    echo \"Warning: Program is not executable. Making it executable...\"\n+    chmod +x \"$PROGRAM\"\n+fi"}]}},{"id":"PRRT_kwDOKSXUF85hLkmN","isResolved":true,"isOutdated":false,"path":"src/DistributedTraining/InMemoryCommunicationBackend.cs","line":471,"originalLine":341,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86Vb1EY","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Guard the shared buffers after `TryGetValue` to satisfy nullable analysis.**  \nLines 323 & 392 hit CS8600: inside both `while (!_sharedBuffers.TryGetValue(..., out buffer))` loops the compiler still sees `buffer` as possibly null when we later dereference it (`buffer[0]`, `buffer[_rank]`). In practice we only exit once the entry exists, but the nullable flow analysis cannot prove it, so the build fails. Capture the result in a nullable local and assert it before use.\n\n```diff\n-        List<Vector<T>> buffer;\n-\n-        // Wait for root to split data\n-        while (!_sharedBuffers.TryGetValue(bufferId, out buffer))\n+        List<Vector<T>>? buffer = null;\n+\n+        while (!_sharedBuffers.TryGetValue(bufferId, out buffer))\n         {\n             Monitor.Wait(_globalLock, 10);\n         }\n \n         Monitor.PulseAll(_globalLock);\n \n         // Each process retrieves its chunk\n-        var result = buffer[_rank].Clone();\n+        if (buffer == null)\n+        {\n+            throw new InvalidOperationException($\"Scatter buffer '{bufferId}' was not initialized.\");\n+        }\n+        var result = buffer[_rank].Clone();\n```\n\nApply the same pattern to the broadcast path above (check `buffer[0]`). That resolves the CS8600 errors and keeps the runtime semantics unchanged.  \n\n\n\nAlso applies to: 392-411\n\n<details>\n<summary>üß∞ Tools</summary>\n\n<details>\n<summary>ü™õ GitHub Actions: Build</summary>\n\n[error] 323-323: CS8600: Converting null literal or possible null value to non-nullable type.\n\n</details>\n<details>\n<summary>ü™õ GitHub Check: Build All Frameworks</summary>\n\n[failure] 323-323: \nConverting null literal or possible null value to non-nullable type.\n\n---\n\n[failure] 323-323: \nConverting null literal or possible null value to non-nullable type.\n\n</details>\n<details>\n<summary>ü™õ GitHub Check: Publish Size Analysis</summary>\n\n[failure] 323-323: \nConverting null literal or possible null value to non-nullable type.\n\n</details>\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits 578d634 to cbd637f","createdAt":"2025-11-08T20:07:13Z","path":"src/DistributedTraining/InMemoryCommunicationBackend.cs","line":471,"originalLine":341,"diffHunk":"@@ -0,0 +1,514 @@\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.NumericOperations;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// A simple in-memory implementation of distributed communication for testing and single-machine scenarios.\n+/// This backend simulates multiple processes by using shared memory and locks.\n+///\n+/// For Beginners:\n+/// This is a \"fake\" distributed system that runs on a single machine.\n+/// It's perfect for testing your distributed code without needing multiple GPUs or machines.\n+/// Think of it as a practice mode - it simulates distributed behavior but everything\n+/// runs in one process.\n+///\n+/// Example:\n+/// <code>\n+/// // Create a simulated distributed environment with 4 \"processes\"\n+/// var backend = new InMemoryCommunicationBackend&lt;double&gt;(rank: 0, worldSize: 4);\n+/// CommunicationManager.Initialize(backend);\n+/// </code>\n+///\n+/// Note: For production distributed training, you would use an MPI-based backend instead.\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type for operations</typeparam>\n+public class InMemoryCommunicationBackend<T> : ICommunicationBackend<T> where T : struct\n+{\n+    private readonly int _rank;\n+    private readonly int _worldSize;\n+    private readonly INumericOperations<T> _numOps;\n+    private bool _isInitialized;\n+\n+    // Shared state for simulating collective operations\n+    // In a real implementation, this would be handled by the MPI backend\n+    private static readonly object _globalLock = new object();\n+    private static readonly Dictionary<string, List<Vector<T>>> _sharedBuffers = new();\n+    private static readonly Dictionary<string, int> _barrierCounters = new();\n+    private static int _barrierGeneration = 0;\n+    private static int _operationCounter = 0;\n+    private const int BarrierTimeoutMs = 30000; // 30 seconds\n+\n+\n+    /// <inheritdoc/>\n+    public int Rank => _rank;\n+\n+    /// <inheritdoc/>\n+    public int WorldSize => _worldSize;\n+\n+    /// <inheritdoc/>\n+    public bool IsInitialized => _isInitialized;\n+\n+    /// <summary>\n+    /// Creates a new in-memory communication backend.\n+    ///\n+    /// For Beginners:\n+    /// You create one of these for each simulated \"process\". If you want to simulate\n+    /// 4 GPUs, you create 4 instances with ranks 0, 1, 2, 3, all with worldSize=4.\n+    /// </summary>\n+    /// <param name=\"rank\">The rank (ID) of this simulated process (0-based)</param>\n+    /// <param name=\"worldSize\">The total number of simulated processes</param>\n+    /// <exception cref=\"ArgumentException\">Thrown if rank or worldSize are invalid</exception>\n+    public InMemoryCommunicationBackend(int rank, int worldSize)\n+    {\n+        if (rank < 0 || rank >= worldSize)\n+        {\n+            throw new ArgumentException(\n+                $\"Invalid rank {rank}. Must be between 0 and {worldSize - 1}.\");\n+        }\n+\n+        if (worldSize <= 0)\n+        {\n+            throw new ArgumentException(\n+                $\"Invalid worldSize {worldSize}. Must be positive.\");\n+        }\n+\n+        _rank = rank;\n+        _worldSize = worldSize;\n+        _numOps = MathHelper.GetNumericOperations<T>();\n+        _isInitialized = false;\n+    }\n+\n+    /// <inheritdoc/>\n+    public void Initialize()\n+    {\n+        lock (_globalLock)\n+        {\n+            if (_isInitialized)\n+            {\n+                return;\n+            }\n+\n+            _isInitialized = true;\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public void Shutdown()\n+    {\n+        lock (_globalLock)\n+        {\n+            if (!_isInitialized)\n+            {\n+                return;\n+            }\n+\n+            // Clear any remaining shared state\n+            _sharedBuffers.Clear();\n+            _barrierCounters.Clear();\n+\n+            _isInitialized = false;\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public void Barrier()\n+    {\n+        EnsureInitialized();\n+\n+        lock (_globalLock)\n+        {\n+            // Use shared barrier generation counter so all ranks synchronize on same key\n+            string barrierId = $\"barrier_{_barrierGeneration}\";\n+\n+            if (!_barrierCounters.ContainsKey(barrierId))\n+            {\n+                _barrierCounters[barrierId] = 0;\n+            }\n+\n+            _barrierCounters[barrierId]++;\n+\n+            var startTime = DateTime.UtcNow;\n+\n+            // Wait until all processes have reached the barrier\n+            while (_barrierCounters[barrierId] < _worldSize)\n+            {\n+                Monitor.Wait(_globalLock, 10);\n+                if ((DateTime.UtcNow - startTime).TotalMilliseconds > BarrierTimeoutMs)\n+                {\n+                    throw new TimeoutException($\"Barrier timeout after {BarrierTimeoutMs}ms. Only {_barrierCounters[barrierId]} of {_worldSize} processes reached the barrier.\");\n+                }\n+            }\n+\n+            Monitor.PulseAll(_globalLock);\n+\n+            // Cleanup - rank 0 removes key and increments generation for next barrier\n+            if (_rank == 0)\n+            {\n+                _barrierCounters.Remove(barrierId);\n+                _barrierGeneration++;\n+            }\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public void AllReduce(Vector<T> data, ReductionOperation operation)\n+    {\n+        EnsureInitialized();\n+\n+        if (data == null)\n+        {\n+            throw new ArgumentNullException(nameof(data));\n+        }\n+\n+        // For single process, no communication needed\n+        if (_worldSize == 1)\n+        {\n+            if (operation == ReductionOperation.Average)\n+            {\n+                // Already averaged (only one value)\n+            }\n+            return;\n+        }\n+\n+        lock (_globalLock)\n+        {\n+            // Use shared operation counter so all ranks target same buffer key\n+            string bufferId = $\"allreduce_{_operationCounter}\";\n+\n+            // Initialize shared buffer\n+            if (!_sharedBuffers.ContainsKey(bufferId))\n+            {\n+                _sharedBuffers[bufferId] = new List<Vector<T>>();\n+            }\n+\n+            // Contribute local data\n+            _sharedBuffers[bufferId].Add(data.Clone());\n+\n+            var startTime = DateTime.UtcNow;\n+\n+            // Wait until all processes have contributed\n+            while (_sharedBuffers[bufferId].Count < _worldSize)\n+            {\n+                Monitor.Wait(_globalLock, 10);\n+                if ((DateTime.UtcNow - startTime).TotalMilliseconds > BarrierTimeoutMs)\n+                {\n+                    throw new TimeoutException($\"AllReduce timeout after {BarrierTimeoutMs}ms. Only {_sharedBuffers[bufferId].Count} of {_worldSize} processes contributed.\");\n+                }\n+            }\n+\n+            Monitor.PulseAll(_globalLock);\n+\n+            // Perform reduction\n+            var allData = _sharedBuffers[bufferId];\n+            var result = PerformReduction(allData, operation);\n+\n+            // Copy result back to input data\n+            for (int i = 0; i < data.Length; i++)\n+            {\n+                data[i] = result[i];\n+            }\n+\n+            // Cleanup - rank 0 removes key and increments counter for next operation\n+            if (_rank == 0)\n+            {\n+                _sharedBuffers.Remove(bufferId);\n+                _operationCounter++;\n+            }\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public Vector<T> AllGather(Vector<T> sendData)\n+    {\n+        EnsureInitialized();\n+\n+        if (sendData == null)\n+        {\n+            throw new ArgumentNullException(nameof(sendData));\n+        }\n+\n+        // For single process, just return a copy\n+        if (_worldSize == 1)\n+        {\n+            return sendData.Clone();\n+        }\n+\n+        lock (_globalLock)\n+        {\n+            // Use shared operation counter so all ranks target same buffer key\n+            string bufferId = $\"allgather_{_operationCounter}\";\n+\n+            // Initialize shared buffer\n+            if (!_sharedBuffers.ContainsKey(bufferId))\n+            {\n+                _sharedBuffers[bufferId] = new List<Vector<T>>(new Vector<T>[_worldSize]);\n+            }\n+\n+            // Contribute local data\n+            _sharedBuffers[bufferId][_rank] = sendData.Clone();\n+\n+            // Wait until all processes have contributed\n+            while (true)\n+            {\n+                int contributedCount = _sharedBuffers[bufferId].Count(v => v != null);\n+                if (contributedCount >= _worldSize)\n+                {\n+                    break;\n+                }\n+                Monitor.Wait(_globalLock, 10);\n+            }\n+\n+            Monitor.PulseAll(_globalLock);\n+\n+            // Concatenate all data\n+            var allData = _sharedBuffers[bufferId];\n+            int totalLength = allData.Sum(v => v.Length);\n+            var result = new T[totalLength];\n+            int offset = 0;\n+\n+            for (int i = 0; i < _worldSize; i++)\n+            {\n+                var data = allData[i];\n+                Array.Copy(data.ToArray(), 0, result, offset, data.Length);\n+                offset += data.Length;\n+            }\n+\n+            // Cleanup - rank 0 removes key and increments counter for next operation\n+            if (_rank == 0)\n+            {\n+                _sharedBuffers.Remove(bufferId);\n+                _operationCounter++;\n+            }\n+\n+            return new Vector<T>(result);\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public Vector<T> Broadcast(Vector<T> data, int root = 0)\n+    {\n+        EnsureInitialized();\n+\n+        if (root < 0 || root >= _worldSize)\n+        {\n+            throw new ArgumentException($\"Invalid root {root}. Must be between 0 and {_worldSize - 1}.\");\n+        }\n+\n+        // For single process, just return a copy\n+        if (_worldSize == 1)\n+        {\n+            return data?.Clone() ?? throw new ArgumentNullException(nameof(data));\n+        }\n+\n+        lock (_globalLock)\n+        {\n+            // Use shared operation counter so all ranks target same buffer key\n+            string bufferId = $\"broadcast_{_operationCounter}\";\n+            Vector<T> result;\n+            List<Vector<T>> buffer;\n+\n+            // Root process stores the data\n+            if (_rank == root)\n+            {\n+                if (data == null)\n+                {\n+                    throw new ArgumentNullException(nameof(data), \"Data cannot be null on root process.\");\n+                }\n+                _sharedBuffers[bufferId] = new List<Vector<T>> { data.Clone() };\n+            }\n+\n+            // Wait for root to store data\n+            while (!_sharedBuffers.TryGetValue(bufferId, out buffer))\n+            {\n+                Monitor.Wait(_globalLock, 10);\n+            }\n+\n+            Monitor.PulseAll(_globalLock);\n+\n+            // All processes retrieve the data\n+            result = buffer[0].Clone();\n+\n+            // Cleanup - rank 0 removes key and increments counter for next operation\n+            if (_rank == 0)\n+            {\n+                _sharedBuffers.Remove(bufferId);\n+                _operationCounter++;\n+            }\n+\n+            return result;\n+        }"}]}},{"id":"PRRT_kwDOKSXUF85hLkmP","isResolved":true,"isOutdated":true,"path":"src/DistributedTraining/ShardedModel.cs","line":null,"originalLine":201,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86Vb1Ea","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Invalidate the cache before synchronizing, or updates get lost.**  \nAfter Line 183 we refresh `_localParameterShard`, but `_cachedFullParameters` still points to the pre-training `AllGather` result. When `SynchronizeGradients()` immediately calls `GatherFullParameters()`, it returns that cached copy instead of the newly trained weights, so the subsequent AllReduce averages stale parameters and throws away the step you just took. Training across ranks effectively becomes a no-op. Clear the cache as soon as the shard changes so the next gather pulls fresh data.\n\n```diff\n         var shardData = new T[_shardSize];\n         Array.Copy(updatedParams.ToArray(), _shardStartIndex, shardData, 0, _shardSize);\n         _localParameterShard = new Vector<T>(shardData);\n \n-        // Synchronize gradients if auto-sync is enabled\n-        if (_config.AutoSyncGradients)\n+        _cachedFullParameters = null;\n+\n+        if (_config.AutoSyncGradients)\n         {\n             SynchronizeGradients();\n \n-            // Apply synchronized parameters back to the model\n-            fullParams = GatherFullParameters();\n-            _wrappedModel.SetParameters(fullParams);\n-        }\n-        else\n-        {\n-            // Invalidate cache only when not auto-syncing\n-            // When auto-sync is enabled, cache remains valid after synchronization\n-            _cachedFullParameters = null;\n+            var synchronizedParams = GatherFullParameters();\n+            _wrappedModel.SetParameters(synchronizedParams);\n         }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        var shardData = new T[_shardSize];\n        Array.Copy(updatedParams.ToArray(), _shardStartIndex, shardData, 0, _shardSize);\n        _localParameterShard = new Vector<T>(shardData);\n\n        _cachedFullParameters = null;\n\n        if (_config.AutoSyncGradients)\n        {\n            SynchronizeGradients();\n\n            var synchronizedParams = GatherFullParameters();\n            _wrappedModel.SetParameters(synchronizedParams);\n        }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/DistributedTraining/ShardedModel.cs around lines 183 to 201, after\nupdating _localParameterShard you must invalidate _cachedFullParameters\nimmediately so subsequent GatherFullParameters() sees the updated shard; move or\nadd `_cachedFullParameters = null` directly after the\nArray.Copy/_localParameterShard assignment (before any call to\nSynchronizeGradients/GatherFullParameters), and remove the previous else-only\ninvalidation so the cache is cleared whenever the shard changes.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits fb4e725 to e10cf85","createdAt":"2025-11-08T20:07:14Z","path":"src/DistributedTraining/ShardedModel.cs","line":null,"originalLine":201,"diffHunk":"@@ -0,0 +1,404 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.Models;\n+using AiDotNet.NumericOperations;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Implements a distributed model wrapper that shards parameters across multiple processes.\n+///\n+/// For Beginners:\n+/// This class wraps any existing model and makes it work across multiple GPUs or machines.\n+/// It automatically handles:\n+/// - Splitting parameters across processes (sharding)\n+/// - Gathering parameters when needed for forward pass\n+/// - Averaging gradients across all processes during training\n+///\n+/// Think of it like a team project where each person holds part of the solution.\n+/// When you need the full solution, everyone shares their part (AllGather).\n+/// When everyone learns something new, they share and average their learnings (AllReduce).\n+///\n+/// Example:\n+/// <code>\n+/// // Original model\n+/// var model = new NeuralNetworkModel&lt;double&gt;(...);\n+///\n+/// // Wrap it for distributed training\n+/// var backend = new InMemoryCommunicationBackend&lt;double&gt;(rank: 0, worldSize: 4);\n+/// var config = new ShardingConfiguration&lt;double&gt;(backend);\n+/// var distributedModel = new ShardedModel&lt;double, Tensor&lt;double&gt;, Tensor&lt;double&gt;&gt;(model, config);\n+///\n+/// // Now train as usual - distributed magic happens automatically!\n+/// distributedModel.Train(inputs, outputs);\n+/// </code>\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type</typeparam>\n+/// <typeparam name=\"TInput\">The input type for the model</typeparam>\n+/// <typeparam name=\"TOutput\">The output type for the model</typeparam>\n+public class ShardedModel<T, TInput, TOutput> : IShardedModel<T, TInput, TOutput> where T : struct\n+{\n+    private readonly IFullModel<T, TInput, TOutput> _wrappedModel;\n+    private readonly IShardingConfiguration<T> _config;\n+    private readonly INumericOperations<T> _numOps;\n+    private Vector<T> _localParameterShard = new Vector<T>(0);\n+    private Vector<T>? _cachedFullParameters;\n+    private int _shardStartIndex;\n+    private int _shardSize;\n+\n+    /// <inheritdoc/>\n+    public IFullModel<T, TInput, TOutput> WrappedModel => _wrappedModel;\n+\n+    /// <inheritdoc/>\n+    public int Rank => _config.CommunicationBackend.Rank;\n+\n+    /// <inheritdoc/>\n+    public int WorldSize => _config.CommunicationBackend.WorldSize;\n+\n+    /// <inheritdoc/>\n+    public Vector<T> LocalParameterShard => _localParameterShard;\n+\n+    /// <inheritdoc/>\n+    public IShardingConfiguration<T> ShardingConfiguration => _config;\n+\n+    /// <inheritdoc/>\n+    public int ParameterCount => _wrappedModel.ParameterCount;\n+\n+    /// <summary>\n+    /// Creates a new sharded model wrapping an existing model.\n+    ///\n+    /// For Beginners:\n+    /// This constructor takes your existing model and makes it distributed.\n+    /// You provide:\n+    /// 1. The model you want to make distributed\n+    /// 2. A configuration that tells us how to do the distribution\n+    ///\n+    /// The constructor automatically:\n+    /// - Splits the model's parameters across all processes\n+    /// - Sets up communication channels\n+    /// - Prepares everything for distributed training\n+    /// </summary>\n+    /// <param name=\"wrappedModel\">The model to wrap with distributed capabilities</param>\n+    /// <param name=\"config\">Configuration for sharding and communication</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown if model or config is null</exception>\n+    public ShardedModel(IFullModel<T, TInput, TOutput> wrappedModel, IShardingConfiguration<T> config)\n+    {\n+        _wrappedModel = wrappedModel ?? throw new ArgumentNullException(nameof(wrappedModel));\n+        _config = config ?? throw new ArgumentNullException(nameof(config));\n+        _numOps = MathHelper.GetNumericOperations<T>();\n+\n+        // Initialize the communication backend if not already done\n+        if (!_config.CommunicationBackend.IsInitialized)\n+        {\n+            _config.CommunicationBackend.Initialize();\n+        }\n+\n+        // Shard the model parameters across processes\n+        InitializeSharding();\n+    }\n+\n+    /// <summary>\n+    /// Initializes parameter sharding by dividing parameters across processes.\n+    ///\n+    /// For Beginners:\n+    /// This method splits the model's parameters into chunks and gives each\n+    /// process its own chunk to manage. It's like dividing a deck of cards\n+    /// evenly among players.\n+    /// </summary>\n+    private void InitializeSharding()\n+    {\n+        var fullParameters = _wrappedModel.GetParameters();\n+        int totalParams = fullParameters.Length;\n+\n+        // Calculate shard size for this process\n+        int baseShardSize = totalParams / WorldSize;\n+        int remainder = totalParams % WorldSize;\n+\n+        // Distribute remainder among first 'remainder' processes\n+        _shardSize = baseShardSize + (Rank < remainder ? 1 : 0);\n+        _shardStartIndex = Rank * baseShardSize + Math.Min(Rank, remainder);\n+\n+        // Extract local shard\n+        var shardData = new T[_shardSize];\n+        Array.Copy(fullParameters.ToArray(), _shardStartIndex, shardData, 0, _shardSize);\n+        _localParameterShard = new Vector<T>(shardData);\n+\n+        // Cache invalidated\n+        _cachedFullParameters = null;\n+    }\n+\n+    /// <inheritdoc/>\n+    public Vector<T> GatherFullParameters()\n+    {\n+        // Use cached version if available\n+        if (_cachedFullParameters != null)\n+        {\n+            return _cachedFullParameters;\n+        }\n+\n+        // Gather parameters from all processes\n+        var gathered = _config.CommunicationBackend.AllGather(_localParameterShard);\n+        _cachedFullParameters = gathered;\n+        return gathered;\n+    }\n+\n+    /// <inheritdoc/>\n+    public void SynchronizeGradients()\n+    {\n+        // Gather full parameters from all shards since shard sizes differ when\n+        // ParameterCount % WorldSize != 0 (cannot AllReduce different-sized vectors)\n+        var fullParameters = GatherFullParameters();\n+\n+        // AllReduce the full parameter vector to average across all ranks\n+        // This ensures all ranks end up with the same averaged parameters\n+        _config.CommunicationBackend.AllReduce(fullParameters, ReductionOperation.Average);\n+\n+        // Update local shard from the reduced parameters\n+        var shardData = new T[_shardSize];\n+        Array.Copy(fullParameters.ToArray(), _shardStartIndex, shardData, 0, _shardSize);\n+        _localParameterShard = new Vector<T>(shardData);\n+\n+        // Update cache with synchronized parameters\n+        _cachedFullParameters = fullParameters;\n+\n+        // Update wrapped model with synchronized parameters\n+        _wrappedModel.SetParameters(fullParameters);\n+    }\n+\n+    /// <inheritdoc/>\n+    public void Train(TInput input, TOutput expectedOutput)\n+    {\n+        // Gather full parameters for training\n+        var fullParams = GatherFullParameters();\n+        _wrappedModel.SetParameters(fullParams);\n+\n+        // Train the wrapped model\n+        _wrappedModel.Train(input, expectedOutput);\n+\n+        // Get updated parameters\n+        var updatedParams = _wrappedModel.GetParameters();\n+\n+        // Update local shard\n+        var shardData = new T[_shardSize];\n+        Array.Copy(updatedParams.ToArray(), _shardStartIndex, shardData, 0, _shardSize);\n+        _localParameterShard = new Vector<T>(shardData);\n+\n+        // Synchronize gradients if auto-sync is enabled\n+        if (_config.AutoSyncGradients)\n+        {\n+            SynchronizeGradients();\n+\n+            // Apply synchronized parameters back to the model\n+            fullParams = GatherFullParameters();\n+            _wrappedModel.SetParameters(fullParams);\n+        }\n+        else\n+        {\n+            // Invalidate cache only when not auto-syncing\n+            // When auto-sync is enabled, cache remains valid after synchronization\n+            _cachedFullParameters = null;\n+        }"}]}},{"id":"PRRT_kwDOKSXUF85hLkmQ","isResolved":true,"isOutdated":true,"path":"src/DistributedTraining/ShardedOptimizer.cs","line":null,"originalLine":112,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86Vb1Eb","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Restore the `IOptimizer.Reset` contract to unblock the build.**  \nLine 39: `ShardedOptimizer<T, TInput, TOutput>` implements `IOptimizer<T, TInput, TOutput>`, but it never provides the required `Reset()` method. The compiler (and pipeline) are failing with CS0535/CS0738 because of this omission, so the entire PR currently refuses to build. Please delegate to the wrapped optimizer (and optionally add any distributed bookkeeping later) so we meet the interface contract.\n\n```diff\n@@\n public class ShardedOptimizer<T, TInput, TOutput> : IShardedOptimizer<T, TInput, TOutput> where T : struct\n {\n@@\n     public OptimizationResult<T, TInput, TOutput> Optimize(OptimizationInputData<T, TInput, TOutput> inputData)\n     {\n         ‚Ä¶\n     }\n+\n+    public void Reset()\n+    {\n+        _wrappedOptimizer.Reset();\n+    }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\npublic class ShardedOptimizer<T, TInput, TOutput> : IShardedOptimizer<T, TInput, TOutput> where T : struct\n{\n    private readonly IOptimizer<T, TInput, TOutput> _wrappedOptimizer;\n    private readonly IShardingConfiguration<T> _config;\n\n    /// <inheritdoc/>\n    public IOptimizer<T, TInput, TOutput> WrappedOptimizer => _wrappedOptimizer;\n\n    /// <inheritdoc/>\n    public int Rank => _config.CommunicationBackend.Rank;\n\n    /// <inheritdoc/>\n    public int WorldSize => _config.CommunicationBackend.WorldSize;\n\n    /// <inheritdoc/>\n    public IShardingConfiguration<T> ShardingConfiguration => _config;\n\n    /// <summary>\n    /// Creates a new sharded optimizer wrapping an existing optimizer.\n    ///\n    /// For Beginners:\n    /// This constructor takes your existing optimizer and makes it distributed.\n    /// You provide:\n    /// 1. The optimizer you want to make distributed\n    /// 2. A configuration that tells us how to do the distribution\n    ///\n    /// The optimizer will automatically synchronize across all processes during optimization.\n    /// </summary>\n    /// <param name=\"wrappedOptimizer\">The optimizer to wrap with distributed capabilities</param>\n    /// <param name=\"config\">Configuration for sharding and communication</param>\n    /// <exception cref=\"ArgumentNullException\">Thrown if optimizer or config is null</exception>\n    public ShardedOptimizer(\n        IOptimizer<T, TInput, TOutput> wrappedOptimizer,\n        IShardingConfiguration<T> config)\n    {\n        _wrappedOptimizer = wrappedOptimizer ?? throw new ArgumentNullException(nameof(wrappedOptimizer));\n        _config = config ?? throw new ArgumentNullException(nameof(config));\n\n        // Initialize the communication backend if not already done\n        if (!_config.CommunicationBackend.IsInitialized)\n        {\n            _config.CommunicationBackend.Initialize();\n        }\n    }\n\n    /// <inheritdoc/>\n    public OptimizationResult<T, TInput, TOutput> Optimize(OptimizationInputData<T, TInput, TOutput> inputData)\n    {\n        if (inputData == null)\n        {\n            throw new ArgumentNullException(nameof(inputData));\n        }\n\n        // Ensure all processes start together\n        _config.CommunicationBackend.Barrier();\n\n        // Perform optimization on the wrapped optimizer\n        var result = _wrappedOptimizer.Optimize(inputData);\n\n        // Synchronize parameters across all processes if auto-sync is enabled\n        if (_config.AutoSyncGradients && result.BestSolution != null)\n        {\n            SynchronizeParameters(result.BestSolution);\n        }\n\n        // Synchronize optimizer state if needed\n        SynchronizeOptimizerState();\n\n        // Ensure all processes finish together\n        _config.CommunicationBackend.Barrier();\n\n        return result;\n    }\n\n    public void Reset()\n    {\n        _wrappedOptimizer.Reset();\n    }\n}\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>üß∞ Tools</summary>\n\n<details>\n<summary>ü™õ GitHub Actions: Quality Gates (.NET)</summary>\n\n[error] 39-39: 'ShardedOptimizer<T, TInput, TOutput>' does not implement interface member 'IOptimizer<T, TInput, TOutput>.Reset()'\n\n</details>\n<details>\n<summary>ü™õ GitHub Check: Publish Size Analysis</summary>\n\n[failure] 39-39: \n'ShardedOptimizer<T, TInput, TOutput>' does not implement interface member 'IOptimizer<T, TInput, TOutput>.Reset()'\n\n</details>\n\n</details>\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/DistributedTraining/ShardedOptimizer.cs around lines 39 to 112, the class\nimplements IOptimizer<T,TInput,TOutput> but is missing the required Reset()\nmethod which causes build failures; add a public Reset() implementation that\ndelegates to the wrapped optimizer (e.g., call _wrappedOptimizer.Reset()) so the\ninterface contract is satisfied and leave any distributed bookkeeping to be\nadded later.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","createdAt":"2025-11-08T20:07:14Z","path":"src/DistributedTraining/ShardedOptimizer.cs","line":null,"originalLine":112,"diffHunk":"@@ -0,0 +1,288 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.Models.Inputs;\n+using AiDotNet.Optimizers;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Implements a distributed optimizer wrapper that coordinates optimization across multiple processes.\n+///\n+/// For Beginners:\n+/// This class wraps any existing optimizer (like Adam, SGD, etc.) and makes it work across\n+/// multiple GPUs or machines. It automatically handles:\n+/// - Synchronizing gradients across all processes\n+/// - Coordinating parameter updates\n+/// - Ensuring all processes stay in sync\n+///\n+/// Think of it like a team of coaches working together - each has their own expertise\n+/// (the wrapped optimizer), but they coordinate their efforts to train the team effectively.\n+///\n+/// Example:\n+/// <code>\n+/// // Original optimizer\n+/// var optimizer = new AdamOptimizer&lt;double, Tensor&lt;double&gt;, Tensor&lt;double&gt;&gt;(model, options);\n+///\n+/// // Wrap it for distributed training\n+/// var backend = new InMemoryCommunicationBackend&lt;double&gt;(rank: 0, worldSize: 4);\n+/// var config = new ShardingConfiguration&lt;double&gt;(backend);\n+/// var distributedOptimizer = new ShardedOptimizer&lt;double, Tensor&lt;double&gt;, Tensor&lt;double&gt;&gt;(\n+///     optimizer, config);\n+///\n+/// // Now optimize as usual - distributed magic happens automatically!\n+/// var result = distributedOptimizer.Optimize(inputData);\n+/// </code>\n+/// </summary>\n+/// <typeparam name=\"T\">The numeric type</typeparam>\n+/// <typeparam name=\"TInput\">The input type for the model</typeparam>\n+/// <typeparam name=\"TOutput\">The output type for the model</typeparam>\n+public class ShardedOptimizer<T, TInput, TOutput> : IShardedOptimizer<T, TInput, TOutput> where T : struct\n+{\n+    private readonly IOptimizer<T, TInput, TOutput> _wrappedOptimizer;\n+    private readonly IShardingConfiguration<T> _config;\n+\n+    /// <inheritdoc/>\n+    public IOptimizer<T, TInput, TOutput> WrappedOptimizer => _wrappedOptimizer;\n+\n+    /// <inheritdoc/>\n+    public int Rank => _config.CommunicationBackend.Rank;\n+\n+    /// <inheritdoc/>\n+    public int WorldSize => _config.CommunicationBackend.WorldSize;\n+\n+    /// <inheritdoc/>\n+    public IShardingConfiguration<T> ShardingConfiguration => _config;\n+\n+    /// <summary>\n+    /// Creates a new sharded optimizer wrapping an existing optimizer.\n+    ///\n+    /// For Beginners:\n+    /// This constructor takes your existing optimizer and makes it distributed.\n+    /// You provide:\n+    /// 1. The optimizer you want to make distributed\n+    /// 2. A configuration that tells us how to do the distribution\n+    ///\n+    /// The optimizer will automatically synchronize across all processes during optimization.\n+    /// </summary>\n+    /// <param name=\"wrappedOptimizer\">The optimizer to wrap with distributed capabilities</param>\n+    /// <param name=\"config\">Configuration for sharding and communication</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown if optimizer or config is null</exception>\n+    public ShardedOptimizer(\n+        IOptimizer<T, TInput, TOutput> wrappedOptimizer,\n+        IShardingConfiguration<T> config)\n+    {\n+        _wrappedOptimizer = wrappedOptimizer ?? throw new ArgumentNullException(nameof(wrappedOptimizer));\n+        _config = config ?? throw new ArgumentNullException(nameof(config));\n+\n+        // Initialize the communication backend if not already done\n+        if (!_config.CommunicationBackend.IsInitialized)\n+        {\n+            _config.CommunicationBackend.Initialize();\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public OptimizationResult<T, TInput, TOutput> Optimize(OptimizationInputData<T, TInput, TOutput> inputData)\n+    {\n+        if (inputData == null)\n+        {\n+            throw new ArgumentNullException(nameof(inputData));\n+        }\n+\n+        // Ensure all processes start together\n+        _config.CommunicationBackend.Barrier();\n+\n+        // Perform optimization on the wrapped optimizer\n+        var result = _wrappedOptimizer.Optimize(inputData);\n+\n+        // Synchronize parameters across all processes if auto-sync is enabled\n+        if (_config.AutoSyncGradients && result.BestSolution != null)\n+        {\n+            SynchronizeParameters(result.BestSolution);\n+        }\n+\n+        // Synchronize optimizer state if needed\n+        SynchronizeOptimizerState();\n+\n+        // Ensure all processes finish together\n+        _config.CommunicationBackend.Barrier();\n+\n+        return result;\n+    }\n+"}]}},{"id":"PRRT_kwDOKSXUF85hLuPk","isResolved":true,"isOutdated":true,"path":"src/DistributedTraining/CommunicationManager.cs","line":null,"originalLine":146,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VcCxB","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üü° Minor_\n\n**Potential issue with backend cast safety.**\n\nLines 130 and 134 cast the backend using `as`, which could theoretically return null if the generic constraint and typeof check somehow diverge. While this should never happen in practice (the typeof check ensures T is float or double), the code doesn't handle the null case explicitly.\n\nConsider adding null checks after the cast or using a direct cast to make the invariant clearer:\n\n```diff\n if (typeof(T) == typeof(float))\n {\n-    _floatBackend = backend as ICommunicationBackend<float>;\n+    _floatBackend = (ICommunicationBackend<float>)(object)backend;\n }\n else if (typeof(T) == typeof(double))\n {\n-    _doubleBackend = backend as ICommunicationBackend<double>;\n+    _doubleBackend = (ICommunicationBackend<double>)(object)backend;\n }\n```\n\nThis makes it clear that the cast is expected to succeed and will throw if it doesn't, rather than silently storing null.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/DistributedTraining/CommunicationManager.cs around lines 109 to 146, the\ncode uses 'as' to cast the backend to ICommunicationBackend<float> or\nICommunicationBackend<double> which could leave _floatBackend/_doubleBackend\nnull if the cast somehow fails; replace the 'as' casts with explicit casts (or\nkeep 'as' but immediately check for null) and throw an InvalidOperationException\n(or ArgumentException) with a clear message if the cast yields null so the\ninvariant is enforced and a failure is visible instead of silently storing null.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","createdAt":"2025-11-08T21:07:56Z","path":"src/DistributedTraining/CommunicationManager.cs","line":null,"originalLine":146,"diffHunk":"@@ -0,0 +1,363 @@\n+using AiDotNet.LinearAlgebra;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Central manager for distributed communication operations.\n+/// Provides a static API for collective communication in distributed training scenarios.\n+///\n+/// For Beginners:\n+/// This is your main entry point for distributed training communication.\n+/// It's a \"wrapper\" that makes it easy to communicate between different processes/GPUs\n+/// without worrying about the underlying implementation details.\n+///\n+/// Example usage:\n+/// <code>\n+/// // Initialize communication (do this once at startup)\n+/// CommunicationManager.Initialize(new InMemoryCommunicationBackend&lt;double&gt;());\n+///\n+/// // Get your process ID and total number of processes\n+/// int myRank = CommunicationManager.GetRank();\n+/// int totalProcesses = CommunicationManager.GetWorldSize();\n+///\n+/// // Average gradients across all processes\n+/// Vector&lt;double&gt; gradients = ...; // Your local gradients\n+/// CommunicationManager.AllReduce(gradients, ReductionOperation.Sum);\n+/// // Now 'gradients' contains the sum from all processes\n+///\n+/// // Clean up when done\n+/// CommunicationManager.Shutdown();\n+/// </code>\n+///\n+/// IMPORTANT - Thread Safety and Testing Limitations:\n+/// This class uses STATIC MUTABLE STATE which has the following implications:\n+///\n+/// 1. SINGLE GLOBAL INSTANCE: Only ONE backend can be active per process at a time.\n+///    Multiple training sessions in the same process will share the same backend instance.\n+///\n+/// 2. PARALLEL TEST EXECUTION: Tests that use this class CANNOT run in parallel.\n+///    Use [Collection] attributes in xUnit or similar mechanisms to enforce sequential execution.\n+///\n+/// 3. TEST ISOLATION: Always call Shutdown() in test cleanup to reset state.\n+///    For better isolation in tests, use InMemoryCommunicationBackend with unique environment IDs\n+///    and inject the backend directly instead of using this static manager.\n+///\n+/// 4. CONCURRENT INITIALIZATION: Attempting to Initialize() from multiple threads concurrently\n+///    is protected by locks, but may result in exceptions if already initialized.\n+///\n+/// Recommended Test Pattern:\n+/// <code>\n+/// // Option 1: Use environment isolation (recommended for parallel tests)\n+/// var backend = new InMemoryCommunicationBackend&lt;double&gt;(rank: 0, worldSize: 4, environmentId: \"test-123\");\n+/// // Use backend directly, don't call CommunicationManager.Initialize()\n+///\n+/// // Option 2: Sequential tests with proper cleanup\n+/// [Collection(\"DistributedTraining\")] // Force sequential execution\n+/// public class MyDistributedTests\n+/// {\n+///     [Fact]\n+///     public void MyTest()\n+///     {\n+///         try\n+///         {\n+///             CommunicationManager.Initialize(...);\n+///             // Test code\n+///         }\n+///         finally\n+///         {\n+///             CommunicationManager.Shutdown(); // CRITICAL: Always cleanup\n+///         }\n+///     }\n+/// }\n+/// </code>\n+/// </summary>\n+public static class CommunicationManager\n+{\n+    private static readonly object _lock = new object();\n+    private static ICommunicationBackend<float>? _floatBackend;\n+    private static ICommunicationBackend<double>? _doubleBackend;\n+    private static bool _isInitialized = false;\n+\n+    /// <summary>\n+    /// Gets whether the communication manager has been initialized.\n+    /// </summary>\n+    public static bool IsInitialized\n+    {\n+        get\n+        {\n+            lock (_lock)\n+            {\n+                return _isInitialized;\n+            }\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Initializes the communication manager with the specified backend.\n+    /// This must be called before any other operations.\n+    ///\n+    /// For Beginners:\n+    /// This sets up the communication system. You need to provide a \"backend\"\n+    /// which is the actual implementation that does the communication.\n+    /// For testing, use InMemoryCommunicationBackend. For real distributed training,\n+    /// you would use an MPI backend.\n+    /// </summary>\n+    /// <typeparam name=\"T\">The numeric type (float or double)</typeparam>\n+    /// <param name=\"backend\">The communication backend to use</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown if backend is null</exception>\n+    /// <exception cref=\"InvalidOperationException\">Thrown if already initialized</exception>\n+    public static void Initialize<T>(ICommunicationBackend<T> backend) where T : struct\n+    {\n+        if (backend == null)\n+        {\n+            throw new ArgumentNullException(nameof(backend));\n+        }\n+\n+        lock (_lock)\n+        {\n+            if (_isInitialized)\n+            {\n+                throw new InvalidOperationException(\n+                    \"CommunicationManager is already initialized. Call Shutdown() first, or check IsInitialized before calling Initialize().\");\n+            }\n+\n+            // Initialize the backend\n+            backend.Initialize();\n+\n+            // Store the backend (type-specific)\n+            if (typeof(T) == typeof(float))\n+            {\n+                _floatBackend = backend as ICommunicationBackend<float>;\n+            }\n+            else if (typeof(T) == typeof(double))\n+            {\n+                _doubleBackend = backend as ICommunicationBackend<double>;\n+            }\n+            else\n+            {\n+                throw new NotSupportedException(\n+                    $\"Type {typeof(T).Name} is not supported for distributed communication. \" +\n+                    \"Only float and double are supported because of MPI type mapping constraints. \" +\n+                    \"Please use float or double for distributed operations.\");\n+            }\n+\n+            _isInitialized = true;\n+        }\n+    }"}]}},{"id":"PRRT_kwDOKSXUF85hMV2M","isResolved":true,"isOutdated":false,"path":"src/DistributedTraining/FSDPModel.cs","line":113,"originalLine":96,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86Vc1GC","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Stop averaging disjoint shards during sync**\n\n`SynchronizeGradients()` operates on `LocalShard`, which only contains the slice owned by this rank. Running an `AllReduce` over these slices mixes unrelated parameter indices from other ranks (and even fails when shard lengths differ), so the first sync corrupts the weights or throws. You need to reduce matching data across ranks‚Äîe.g. gather full gradients, all-reduce that full vector, then scatter back, or restructure to use `ReduceScatter` with consistent per-rank buffers. Until the reduction aligns identical parameter indices, distributed training will not produce correct results.\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits c717ae2 to ce97e4f","createdAt":"2025-11-08T23:38:17Z","path":"src/DistributedTraining/FSDPModel.cs","line":113,"originalLine":96,"diffHunk":"@@ -0,0 +1,267 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.Models;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Implements a distributed model wrapper that shards parameters across multiple processes.\n+/// </summary>\n+/// <remarks>\n+/// <para><b>For Beginners:</b>\n+/// This class wraps any existing model and makes it work across multiple GPUs or machines.\n+/// It automatically handles:\n+/// - Splitting parameters across processes (sharding)\n+/// - Gathering parameters when needed for forward pass\n+/// - Averaging gradients across all processes during training\n+/// </para>\n+/// <para>\n+/// Think of it like a team project where each person holds part of the solution.\n+/// When you need the full solution, everyone shares their part (AllGather).\n+/// When everyone learns something new, they share and average their learnings (AllReduce).\n+/// </para>\n+/// <para>\n+/// Example:\n+/// <code>\n+/// // Original model\n+/// var model = new NeuralNetworkModel&lt;double&gt;(...);\n+///\n+/// // Wrap it for distributed training\n+/// var backend = new InMemoryCommunicationBackend&lt;double&gt;(rank: 0, worldSize: 4);\n+/// var config = new ShardingConfiguration&lt;double&gt;(backend);\n+/// var distributedModel = new ShardedModel&lt;double, Tensor&lt;double&gt;, Tensor&lt;double&gt;&gt;(model, config);\n+///\n+/// // Now train as usual - distributed magic happens automatically!\n+/// distributedModel.Train(inputs, outputs);\n+/// </code>\n+/// </para>\n+/// </remarks>\n+/// <typeparam name=\"T\">The numeric type</typeparam>\n+/// <typeparam name=\"TInput\">The input type for the model</typeparam>\n+/// <typeparam name=\"TOutput\">The output type for the model</typeparam>\n+public class ShardedModel<T, TInput, TOutput> : ShardedModelBase<T, TInput, TOutput>\n+{\n+    /// <summary>\n+    /// Creates a new sharded model wrapping an existing model.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b>\n+    /// This constructor takes your existing model and makes it distributed.\n+    /// You provide:\n+    /// 1. The model you want to make distributed\n+    /// 2. A configuration that tells us how to do the distribution\n+    /// </para>\n+    /// <para>\n+    /// The constructor automatically:\n+    /// - Splits the model's parameters across all processes\n+    /// - Sets up communication channels\n+    /// - Prepares everything for distributed training\n+    /// </para>\n+    /// </remarks>\n+    /// <param name=\"wrappedModel\">The model to wrap with distributed capabilities</param>\n+    /// <param name=\"config\">Configuration for sharding and communication</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown if model or config is null</exception>\n+    public ShardedModel(IFullModel<T, TInput, TOutput> wrappedModel, IShardingConfiguration<T> config)\n+        : base(wrappedModel, config)\n+    {\n+    }\n+\n+    /// <inheritdoc/>\n+    public override void Train(TInput input, TOutput expectedOutput)\n+    {\n+        // Gather full parameters for training\n+        var fullParams = GatherFullParameters();\n+        WrappedModel.SetParameters(fullParams);\n+\n+        // Train the wrapped model\n+        WrappedModel.Train(input, expectedOutput);\n+\n+        // Get updated parameters\n+        var updatedParams = WrappedModel.GetParameters();\n+\n+        // Update local shard\n+        UpdateLocalShardFromFull(updatedParams);\n+\n+        // Invalidate cache immediately after local shard changes\n+        InvalidateCache();\n+\n+        // Synchronize gradients if auto-sync is enabled\n+        if (Config.AutoSyncGradients)\n+        {\n+            SynchronizeGradients();\n+\n+            // Apply synchronized parameters back to the model\n+            fullParams = GatherFullParameters();\n+            WrappedModel.SetParameters(fullParams);\n+        }"}]}},{"id":"PRRT_kwDOKSXUF85hMV2O","isResolved":true,"isOutdated":false,"path":"src/DistributedTraining/ShardedModelBase.cs","line":335,"originalLine":318,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86Vc1GE","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Implement remaining interface contracts**\n\nBuild checks show this class still lacks `ICloneable<IFullModel<T,TInput,TOutput>>.DeepCopy()` plus the `IFeatureAware` members (`IsFeatureUsed`, `GetActiveFeatureIndices`, `SetActiveFeatureIndices`). Until these are implemented (ideally delegating to `WrappedModel`), the project will not compile. Please add the missing members here so every `IShardedModel` satisfies the interface.\n\n<details>\n<summary>üß∞ Tools</summary>\n\n<details>\n<summary>ü™õ GitHub Check: Build All Frameworks</summary>\n\n[failure] 73-73: \nThe type 'ShardedModelBase<T, TInput, TOutput>' already contains a definition for 'WrappedModel'\n\n---\n\n[failure] 35-35: \n'ShardedModelBase<T, TInput, TOutput>' does not implement interface member 'ICloneable<IFullModel<T, TInput, TOutput>>.DeepCopy()'\n\n---\n\n[failure] 35-35: \n'ShardedModelBase<T, TInput, TOutput>' does not implement interface member 'IFeatureAware.IsFeatureUsed(int)'\n\n---\n\n[failure] 35-35: \n'ShardedModelBase<T, TInput, TOutput>' does not implement interface member 'IFeatureAware.SetActiveFeatureIndices(IEnumerable<int>)'\n\n---\n\n[failure] 35-35: \n'ShardedModelBase<T, TInput, TOutput>' does not implement interface member 'IFeatureAware.GetActiveFeatureIndices()'\n\n</details>\n<details>\n<summary>ü™õ GitHub Check: Publish Size Analysis</summary>\n\n[failure] 73-73: \nThe type 'ShardedModelBase<T, TInput, TOutput>' already contains a definition for 'WrappedModel'\n\n---\n\n[failure] 35-35: \n'ShardedModelBase<T, TInput, TOutput>' does not implement interface member 'ICloneable<IFullModel<T, TInput, TOutput>>.DeepCopy()'\n\n---\n\n[failure] 35-35: \n'ShardedModelBase<T, TInput, TOutput>' does not implement interface member 'IFeatureAware.IsFeatureUsed(int)'\n\n---\n\n[failure] 35-35: \n'ShardedModelBase<T, TInput, TOutput>' does not implement interface member 'IFeatureAware.SetActiveFeatureIndices(IEnumerable<int>)'\n\n---\n\n[failure] 35-35: \n'ShardedModelBase<T, TInput, TOutput>' does not implement interface member 'IFeatureAware.GetActiveFeatureIndices()'\n\n</details>\n\n</details>\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/DistributedTraining/ShardedModelBase.cs around lines 35 to 318, the class\nis missing implementations for\nICloneable<IFullModel<T,TInput,TOutput>>.DeepCopy() and the IFeatureAware\nmembers IsFeatureUsed, GetActiveFeatureIndices, and SetActiveFeatureIndices;\nimplement these methods on ShardedModelBase and delegate their behavior to the\nWrappedModel (e.g., DeepCopy should call WrappedModel.DeepCopy() and wrap/return\nan appropriate sharded clone if required, and the feature methods should simply\ncall through to\nWrappedModel.IsFeatureUsed/GetActiveFeatureIndices/SetActiveFeatureIndices),\nensure signatures match the interfaces exactly, handle nullability consistently,\nand update any cached shards or cache invalidation if WrappedModel calls change\nparameters or active features.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits 578d634 to cbd637f","createdAt":"2025-11-08T23:38:17Z","path":"src/DistributedTraining/ShardedModelBase.cs","line":335,"originalLine":318,"diffHunk":"@@ -0,0 +1,318 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.Models;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Provides base implementation for distributed models with parameter sharding.\n+/// </summary>\n+/// <remarks>\n+/// <para>\n+/// This abstract class implements common functionality for all sharded models,\n+/// including parameter management, sharding logic, gradient synchronization, and\n+/// integration with the model serialization system. Derived classes can customize\n+/// the sharding strategy, communication pattern, or add optimization-specific features.\n+/// </para>\n+/// <para><b>For Beginners:</b> This is the foundation that all distributed models build upon.\n+///\n+/// Think of this as a template for splitting a big model across multiple computers or GPUs.\n+/// It handles common tasks like:\n+/// - Dividing model parameters into chunks (sharding)\n+/// - Collecting all chunks when needed (gathering)\n+/// - Sharing learning updates across all processes (gradient sync)\n+/// - Saving and loading distributed models\n+///\n+/// Specific types of distributed models (like fully sharded or hybrid sharded) inherit\n+/// from this and add their own strategies. This prevents code duplication and ensures\n+/// all distributed models work consistently.\n+/// </para>\n+/// </remarks>\n+/// <typeparam name=\"T\">The numeric type for operations</typeparam>\n+/// <typeparam name=\"TInput\">The input type for the model</typeparam>\n+/// <typeparam name=\"TOutput\">The output type for the model</typeparam>\n+public abstract class ShardedModelBase<T, TInput, TOutput> : IShardedModel<T, TInput, TOutput>\n+{\n+    /// <summary>\n+    /// Provides numeric operations for type T.\n+    /// </summary>\n+    protected readonly INumericOperations<T> NumOps;\n+\n+    /// <summary>\n+    /// The wrapped model that this sharded model delegates to.\n+    /// </summary>\n+    protected readonly IFullModel<T, TInput, TOutput> WrappedModel;\n+\n+    /// <summary>\n+    /// The sharding configuration containing communication backend and settings.\n+    /// </summary>\n+    protected readonly IShardingConfiguration<T> Config;\n+\n+    /// <summary>\n+    /// The local parameter shard owned by this process.\n+    /// </summary>\n+    protected Vector<T> LocalShard;\n+\n+    /// <summary>\n+    /// Cached full parameters to avoid repeated gathering.\n+    /// </summary>\n+    protected Vector<T>? CachedFullParameters;\n+\n+    /// <summary>\n+    /// Starting index of this process's shard in the full parameter vector.\n+    /// </summary>\n+    protected int ShardStartIndex;\n+\n+    /// <summary>\n+    /// Size of this process's parameter shard.\n+    /// </summary>\n+    protected int ShardSize;\n+\n+    /// <inheritdoc/>\n+    public IFullModel<T, TInput, TOutput> WrappedModel => this.WrappedModel;\n+\n+    /// <inheritdoc/>\n+    public int Rank => Config.CommunicationBackend.Rank;\n+\n+    /// <inheritdoc/>\n+    public int WorldSize => Config.CommunicationBackend.WorldSize;\n+\n+    /// <inheritdoc/>\n+    public Vector<T> LocalParameterShard => LocalShard;\n+\n+    /// <inheritdoc/>\n+    public IShardingConfiguration<T> ShardingConfiguration => Config;\n+\n+    /// <inheritdoc/>\n+    public int ParameterCount => WrappedModel.ParameterCount;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the ShardedModelBase class.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This constructor wraps an existing model with distributed training capabilities.\n+    /// It initializes the communication backend if needed and sets up parameter sharding.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This constructor takes your regular model and makes it distributed.\n+    ///\n+    /// You provide:\n+    /// 1. The model you want to distribute\n+    /// 2. Configuration that tells us how to distribute it\n+    ///\n+    /// The constructor automatically:\n+    /// - Sets up communication if not already done\n+    /// - Splits the model's parameters across processes\n+    /// - Prepares everything for distributed training\n+    /// </para>\n+    /// </remarks>\n+    /// <param name=\"wrappedModel\">The model to wrap with distributed capabilities</param>\n+    /// <param name=\"config\">Configuration for sharding and communication</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown if model or config is null</exception>\n+    protected ShardedModelBase(IFullModel<T, TInput, TOutput> wrappedModel, IShardingConfiguration<T> config)\n+    {\n+        this.WrappedModel = wrappedModel ?? throw new ArgumentNullException(nameof(wrappedModel));\n+        Config = config ?? throw new ArgumentNullException(nameof(config));\n+        NumOps = MathHelper.GetNumericOperations<T>();\n+\n+        // Initialize communication backend if needed\n+        if (!Config.CommunicationBackend.IsInitialized)\n+        {\n+            Config.CommunicationBackend.Initialize();\n+        }\n+\n+        // Initialize sharding\n+        LocalShard = new Vector<T>(Array.Empty<T>());\n+        ShardStartIndex = 0;\n+        ShardSize = 0;\n+        CachedFullParameters = null;\n+\n+        InitializeSharding();\n+    }\n+\n+    /// <summary>\n+    /// Initializes parameter sharding by dividing parameters across processes.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This method calculates how to distribute parameters evenly across all processes,\n+    /// with remainder parameters distributed to the first few processes.\n+    /// Derived classes can override this to implement different sharding strategies.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This splits the model's parameters across all processes.\n+    ///\n+    /// Think of it like dividing a deck of cards among players. If you have 10 parameters\n+    /// and 3 processes:\n+    /// - Process 0 gets parameters 0-3 (4 parameters)\n+    /// - Process 1 gets parameters 4-6 (3 parameters)\n+    /// - Process 2 gets parameters 7-9 (3 parameters)\n+    ///\n+    /// We try to split evenly, but if there's a remainder, the first processes get\n+    /// one extra parameter each.\n+    /// </para>\n+    /// </remarks>\n+    protected virtual void InitializeSharding()\n+    {\n+        var fullParameters = WrappedModel.GetParameters();\n+        int totalParams = fullParameters.Length;\n+\n+        // Calculate shard size for this process\n+        int baseShardSize = totalParams / WorldSize;\n+        int remainder = totalParams % WorldSize;\n+\n+        // Distribute remainder among first 'remainder' processes\n+        ShardSize = baseShardSize + (Rank < remainder ? 1 : 0);\n+        ShardStartIndex = Rank * baseShardSize + Math.Min(Rank, remainder);\n+\n+        // Extract local shard\n+        var shardData = new T[ShardSize];\n+        Array.Copy(fullParameters.ToArray(), ShardStartIndex, shardData, 0, ShardSize);\n+        LocalShard = new Vector<T>(shardData);\n+\n+        // Invalidate cache\n+        CachedFullParameters = null;\n+    }\n+\n+    /// <inheritdoc/>\n+    public virtual Vector<T> GatherFullParameters()\n+    {\n+        // Use cached version if available\n+        if (CachedFullParameters != null)\n+        {\n+            return CachedFullParameters;\n+        }\n+\n+        // Gather parameters from all processes\n+        var gathered = Config.CommunicationBackend.AllGather(LocalShard);\n+        CachedFullParameters = gathered;\n+        return gathered;\n+    }\n+\n+    /// <inheritdoc/>\n+    public virtual void SynchronizeGradients()\n+    {\n+        // Perform AllReduce with average operation\n+        Config.CommunicationBackend.AllReduce(LocalShard, ReductionOperation.Average);\n+\n+        // Invalidate cached full parameters\n+        CachedFullParameters = null;\n+    }\n+\n+    /// <summary>\n+    /// Invalidates the cached full parameters, forcing a re-gather on next access.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This method should be called whenever local parameters change to ensure\n+    /// the cache is refreshed on the next GatherFullParameters call.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> When parameters change, we need to throw away\n+    /// the old cached full parameters.\n+    ///\n+    /// It's like when you update a document - you need to discard the old\n+    /// saved copy so that next time you need it, you get the updated version.\n+    /// </para>\n+    /// </remarks>\n+    protected void InvalidateCache()\n+    {\n+        CachedFullParameters = null;\n+    }\n+\n+    /// <summary>\n+    /// Updates the local parameter shard from the full parameter vector.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This method extracts this process's shard from a full parameter vector.\n+    /// Used after training updates or when setting parameters.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> After the full model is updated, we need to\n+    /// extract our piece of it.\n+    ///\n+    /// It's like taking your slice of a pizza after it's been prepared - you get\n+    /// the portion that belongs to you from the whole.\n+    /// </para>\n+    /// </remarks>\n+    /// <param name=\"fullParameters\">The full parameter vector</param>\n+    protected void UpdateLocalShardFromFull(Vector<T> fullParameters)\n+    {\n+        var shardData = new T[ShardSize];\n+        Array.Copy(fullParameters.ToArray(), ShardStartIndex, shardData, 0, ShardSize);\n+        LocalShard = new Vector<T>(shardData);\n+        InvalidateCache();\n+    }\n+\n+    /// <inheritdoc/>\n+    public abstract void Train(TInput input, TOutput expectedOutput);\n+\n+    /// <inheritdoc/>\n+    public abstract TOutput Predict(TInput input);\n+\n+    /// <inheritdoc/>\n+    public abstract ModelMetadata<T> GetModelMetadata();\n+\n+    /// <inheritdoc/>\n+    public virtual Vector<T> GetParameters()\n+    {\n+        return GatherFullParameters();\n+    }\n+\n+    /// <inheritdoc/>\n+    public virtual void SetParameters(Vector<T> parameters)\n+    {\n+        if (parameters == null)\n+        {\n+            throw new ArgumentNullException(nameof(parameters));\n+        }\n+\n+        if (parameters.Length != ParameterCount)\n+        {\n+            throw new ArgumentException(\n+                $\"Parameter count mismatch. Expected {ParameterCount}, got {parameters.Length}.\",\n+                nameof(parameters));\n+        }\n+\n+        // Update local shard\n+        UpdateLocalShardFromFull(parameters);\n+\n+        // Update wrapped model\n+        WrappedModel.SetParameters(parameters);\n+    }\n+\n+    /// <inheritdoc/>\n+    public abstract IFullModel<T, TInput, TOutput> WithParameters(Vector<T> parameters);\n+\n+    /// <inheritdoc/>\n+    public abstract byte[] Serialize();\n+\n+    /// <inheritdoc/>\n+    public abstract void Deserialize(byte[] data);\n+\n+    /// <inheritdoc/>\n+    public abstract void SaveModel(string filePath);\n+\n+    /// <inheritdoc/>\n+    public abstract void LoadModel(string filePath);\n+\n+    /// <inheritdoc/>\n+    public abstract IFullModel<T, TInput, TOutput> Clone();\n+\n+    /// <inheritdoc/>\n+    public virtual Dictionary<string, T> GetFeatureImportance()\n+    {\n+        return WrappedModel.GetFeatureImportance();\n+    }\n+\n+    /// <inheritdoc/>\n+    public virtual List<string>? GetFeatureNames()\n+    {\n+        return WrappedModel.GetFeatureNames();\n+    }\n+\n+    /// <inheritdoc/>\n+    public virtual void SetFeatureNames(List<string>? featureNames)\n+    {\n+        WrappedModel.SetFeatureNames(featureNames);\n+    }\n+}"}]}},{"id":"PRRT_kwDOKSXUF85hMV2P","isResolved":true,"isOutdated":false,"path":"src/DistributedTraining/ShardedModelBase.cs","line":130,"originalLine":125,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86Vc1GF","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Resolve `WrappedModel` member conflict**\n\nThe protected field and the public property share the same `WrappedModel` name, and the constructor tries to assign via `this.WrappedModel`, so this code does not compile (`member already defined` plus no setter). Please separate the backing field and property and assign to the field instead.\n\n```diff\n-    protected readonly IFullModel<T, TInput, TOutput> WrappedModel;\n+    protected readonly IFullModel<T, TInput, TOutput> _wrappedModel;\n...\n-    public IFullModel<T, TInput, TOutput> WrappedModel => this.WrappedModel;\n+    public IFullModel<T, TInput, TOutput> WrappedModel => _wrappedModel;\n...\n-        this.WrappedModel = wrappedModel ?? throw new ArgumentNullException(nameof(wrappedModel));\n+        _wrappedModel = wrappedModel ?? throw new ArgumentNullException(nameof(wrappedModel));\n```\n\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>üß∞ Tools</summary>\n\n<details>\n<summary>ü™õ GitHub Check: Build All Frameworks</summary>\n\n[failure] 73-73: \nThe type 'ShardedModelBase<T, TInput, TOutput>' already contains a definition for 'WrappedModel'\n\n</details>\n<details>\n<summary>ü™õ GitHub Check: Publish Size Analysis</summary>\n\n[failure] 73-73: \nThe type 'ShardedModelBase<T, TInput, TOutput>' already contains a definition for 'WrappedModel'\n\n</details>\n\n</details>\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/DistributedTraining/ShardedModelBase.cs around lines 47 to 125, the class\ndefines a protected member and a public property with the same name WrappedModel\nand the constructor assigns this.WrappedModel which causes a name conflict and\nno setter; fix by introducing a distinct backing field (e.g. protected readonly\nIFullModel<T, TInput, TOutput> _wrappedModel), change the public property to\nreturn that field (public IFullModel<T, TInput, TOutput> WrappedModel =>\n_wrappedModel;), assign the backing field in the constructor (e.g. _wrappedModel\n= wrappedModel ?? throw ...), and update any other references in the class to\nuse the new backing field or the property as appropriate.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits 578d634 to cbd637f","createdAt":"2025-11-08T23:38:17Z","path":"src/DistributedTraining/ShardedModelBase.cs","line":130,"originalLine":125,"diffHunk":"@@ -0,0 +1,318 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.Models;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Provides base implementation for distributed models with parameter sharding.\n+/// </summary>\n+/// <remarks>\n+/// <para>\n+/// This abstract class implements common functionality for all sharded models,\n+/// including parameter management, sharding logic, gradient synchronization, and\n+/// integration with the model serialization system. Derived classes can customize\n+/// the sharding strategy, communication pattern, or add optimization-specific features.\n+/// </para>\n+/// <para><b>For Beginners:</b> This is the foundation that all distributed models build upon.\n+///\n+/// Think of this as a template for splitting a big model across multiple computers or GPUs.\n+/// It handles common tasks like:\n+/// - Dividing model parameters into chunks (sharding)\n+/// - Collecting all chunks when needed (gathering)\n+/// - Sharing learning updates across all processes (gradient sync)\n+/// - Saving and loading distributed models\n+///\n+/// Specific types of distributed models (like fully sharded or hybrid sharded) inherit\n+/// from this and add their own strategies. This prevents code duplication and ensures\n+/// all distributed models work consistently.\n+/// </para>\n+/// </remarks>\n+/// <typeparam name=\"T\">The numeric type for operations</typeparam>\n+/// <typeparam name=\"TInput\">The input type for the model</typeparam>\n+/// <typeparam name=\"TOutput\">The output type for the model</typeparam>\n+public abstract class ShardedModelBase<T, TInput, TOutput> : IShardedModel<T, TInput, TOutput>\n+{\n+    /// <summary>\n+    /// Provides numeric operations for type T.\n+    /// </summary>\n+    protected readonly INumericOperations<T> NumOps;\n+\n+    /// <summary>\n+    /// The wrapped model that this sharded model delegates to.\n+    /// </summary>\n+    protected readonly IFullModel<T, TInput, TOutput> WrappedModel;\n+\n+    /// <summary>\n+    /// The sharding configuration containing communication backend and settings.\n+    /// </summary>\n+    protected readonly IShardingConfiguration<T> Config;\n+\n+    /// <summary>\n+    /// The local parameter shard owned by this process.\n+    /// </summary>\n+    protected Vector<T> LocalShard;\n+\n+    /// <summary>\n+    /// Cached full parameters to avoid repeated gathering.\n+    /// </summary>\n+    protected Vector<T>? CachedFullParameters;\n+\n+    /// <summary>\n+    /// Starting index of this process's shard in the full parameter vector.\n+    /// </summary>\n+    protected int ShardStartIndex;\n+\n+    /// <summary>\n+    /// Size of this process's parameter shard.\n+    /// </summary>\n+    protected int ShardSize;\n+\n+    /// <inheritdoc/>\n+    public IFullModel<T, TInput, TOutput> WrappedModel => this.WrappedModel;\n+\n+    /// <inheritdoc/>\n+    public int Rank => Config.CommunicationBackend.Rank;\n+\n+    /// <inheritdoc/>\n+    public int WorldSize => Config.CommunicationBackend.WorldSize;\n+\n+    /// <inheritdoc/>\n+    public Vector<T> LocalParameterShard => LocalShard;\n+\n+    /// <inheritdoc/>\n+    public IShardingConfiguration<T> ShardingConfiguration => Config;\n+\n+    /// <inheritdoc/>\n+    public int ParameterCount => WrappedModel.ParameterCount;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the ShardedModelBase class.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This constructor wraps an existing model with distributed training capabilities.\n+    /// It initializes the communication backend if needed and sets up parameter sharding.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This constructor takes your regular model and makes it distributed.\n+    ///\n+    /// You provide:\n+    /// 1. The model you want to distribute\n+    /// 2. Configuration that tells us how to distribute it\n+    ///\n+    /// The constructor automatically:\n+    /// - Sets up communication if not already done\n+    /// - Splits the model's parameters across processes\n+    /// - Prepares everything for distributed training\n+    /// </para>\n+    /// </remarks>\n+    /// <param name=\"wrappedModel\">The model to wrap with distributed capabilities</param>\n+    /// <param name=\"config\">Configuration for sharding and communication</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown if model or config is null</exception>\n+    protected ShardedModelBase(IFullModel<T, TInput, TOutput> wrappedModel, IShardingConfiguration<T> config)\n+    {\n+        this.WrappedModel = wrappedModel ?? throw new ArgumentNullException(nameof(wrappedModel));\n+        Config = config ?? throw new ArgumentNullException(nameof(config));\n+        NumOps = MathHelper.GetNumericOperations<T>();\n+\n+        // Initialize communication backend if needed\n+        if (!Config.CommunicationBackend.IsInitialized)\n+        {\n+            Config.CommunicationBackend.Initialize();\n+        }\n+\n+        // Initialize sharding"}]}},{"id":"PRRT_kwDOKSXUF85hMV2Q","isResolved":true,"isOutdated":true,"path":"src/DistributedTraining/FSDPOptimizer.cs","line":null,"originalLine":239,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86Vc1GG","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Fix missing overrides on optimizer methods**\n\n`ShouldEarlyStop`, `GetOptions`, `SaveModel`, and `LoadModel` share signatures with the virtual members defined in `ShardedOptimizerBase`, but they are not marked `override`. The compiler refuses to build (CS0114/CS0506) and the interface implementation is broken. Add the `override` keyword so these methods extend the base behavior instead of hiding it.\n\n```diff\n-    public bool ShouldEarlyStop()\n+    public override bool ShouldEarlyStop()\n@@\n-    public OptimizationAlgorithmOptions<T, TInput, TOutput> GetOptions()\n+    public override OptimizationAlgorithmOptions<T, TInput, TOutput> GetOptions()\n@@\n-    public void SaveModel(string filePath)\n+    public override void SaveModel(string filePath)\n@@\n-    public void LoadModel(string filePath)\n+    public override void LoadModel(string filePath)\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    public override bool ShouldEarlyStop()\n    {\n        // Delegate to wrapped optimizer\n        bool localDecision = WrappedOptimizer.ShouldEarlyStop();\n\n        // In distributed training, we need consensus on early stopping\n        // Using Max operation: if ANY process wants to stop, all processes stop\n        // This prevents stragglers and ensures synchronized termination\n\n        // Create a vector with the local decision (1 for stop, 0 for continue)\n        var decision = new Vector<T>(new[] { localDecision ? MathHelper.GetNumericOperations<T>().One : MathHelper.GetNumericOperations<T>().Zero });\n\n        // Get the maximum across all processes\n        // If any process returns 1 (stop), the max will be 1\n        Config.CommunicationBackend.AllReduce(decision, ReductionOperation.Max);\n\n        // Check if the result indicates stopping\n        var numOps = MathHelper.GetNumericOperations<T>();\n        return !numOps.Equals(decision[0], numOps.Zero);\n    }\n\n    /// <inheritdoc/>\n    public override OptimizationAlgorithmOptions<T, TInput, TOutput> GetOptions()\n    {\n        return WrappedOptimizer.GetOptions();\n    }\n\n    /// <inheritdoc/>\n    public override byte[] Serialize()\n    {\n        using var ms = new MemoryStream();\n        using var writer = new BinaryWriter(ms);\n\n        // Serialize sharding configuration info\n        writer.Write(WorldSize);\n        writer.Write(Rank);\n        writer.Write(Config.AutoSyncGradients);\n        writer.Write(Config.MinimumParameterGroupSize);\n        writer.Write(Config.EnableGradientCompression);\n\n        // Serialize wrapped optimizer\n        var optimizerData = WrappedOptimizer.Serialize();\n        writer.Write(optimizerData.Length);\n        writer.Write(optimizerData);\n\n        return ms.ToArray();\n    }\n\n    /// <inheritdoc/>\n    public override void Deserialize(byte[] data)\n    {\n        using var ms = new MemoryStream(data);\n        using var reader = new BinaryReader(ms);\n\n        // Read sharding configuration (for validation)\n        int savedWorldSize = reader.ReadInt32();\n        int savedRank = reader.ReadInt32();\n        reader.ReadBoolean(); // AutoSyncGradients\n        reader.ReadInt32(); // MinimumParameterGroupSize\n        reader.ReadBoolean(); // EnableGradientCompression\n\n        if (savedWorldSize != WorldSize)\n        {\n            throw new InvalidOperationException(\n                $\"World size mismatch. Optimizer was saved with {savedWorldSize} processes, \" +\n                $\"but current configuration has {WorldSize} processes.\");\n        }\n\n        // Validate rank matches - different rank could indicate configuration mismatch\n        if (savedRank != Rank)\n        {\n            throw new InvalidOperationException(\n                $\"Rank mismatch. Optimizer was saved on rank {savedRank}, \" +\n                $\"but is being loaded on rank {Rank}. This could indicate a configuration error.\");\n        }\n\n        // Read wrapped optimizer\n        int optimizerDataLength = reader.ReadInt32();\n        byte[] optimizerData = reader.ReadBytes(optimizerDataLength);\n        WrappedOptimizer.Deserialize(optimizerData);\n    }\n\n    /// <inheritdoc/>\n    public override void SaveModel(string filePath)\n    {\n        // Barrier before rank check to prevent deadlock if rank 0 fails\n        Config.CommunicationBackend.Barrier();\n\n        try\n        {\n            // Only rank 0 saves to avoid file write conflicts\n            if (Rank == 0)\n            {\n                var data = Serialize();\n                File.WriteAllBytes(filePath, data);\n            }\n        }\n        finally\n        {\n            // Ensure all processes reach this barrier even if rank 0 fails\n            Config.CommunicationBackend.Barrier();\n        }\n    }\n\n    /// <inheritdoc/>\n    public override void LoadModel(string filePath)\n    {\n        // Barrier before loading to ensure all processes start together\n        Config.CommunicationBackend.Barrier();\n\n        try\n        {\n            // All processes read the same file (read-only, no conflicts)\n            var data = File.ReadAllBytes(filePath);\n            Deserialize(data);\n        }\n        finally\n        {\n            // Ensure all processes finish loading before proceeding\n            Config.CommunicationBackend.Barrier();\n        }\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>üß∞ Tools</summary>\n\n<details>\n<summary>ü™õ GitHub Check: Build All Frameworks</summary>\n\n[failure] 224-224: \n'ShardedOptimizer<T, TInput, TOutput>.LoadModel(string)' hides inherited member 'ShardedOptimizerBase<T, TInput, TOutput>.LoadModel(string)'. To make the current member override that implementation, add the override keyword. Otherwise add the new keyword.\n\n---\n\n[failure] 202-202: \n'ShardedOptimizer<T, TInput, TOutput>.SaveModel(string)' hides inherited member 'ShardedOptimizerBase<T, TInput, TOutput>.SaveModel(string)'. To make the current member override that implementation, add the override keyword. Otherwise add the new keyword.\n\n---\n\n[failure] 141-141: \n'ShardedOptimizer<T, TInput, TOutput>.GetOptions()' hides inherited member 'ShardedOptimizerBase<T, TInput, TOutput>.GetOptions()'. To make the current member override that implementation, add the override keyword. Otherwise add the new keyword.\n\n---\n\n[failure] 119-119: \n'ShardedOptimizer<T, TInput, TOutput>.ShouldEarlyStop()' hides inherited member 'ShardedOptimizerBase<T, TInput, TOutput>.ShouldEarlyStop()'. To make the current member override that implementation, add the override keyword. Otherwise add the new keyword.\n\n</details>\n<details>\n<summary>ü™õ GitHub Check: Publish Size Analysis</summary>\n\n[failure] 224-224: \n'ShardedOptimizer<T, TInput, TOutput>.LoadModel(string)' hides inherited member 'ShardedOptimizerBase<T, TInput, TOutput>.LoadModel(string)'. To make the current member override that implementation, add the override keyword. Otherwise add the new keyword.\n\n---\n\n[failure] 202-202: \n'ShardedOptimizer<T, TInput, TOutput>.SaveModel(string)' hides inherited member 'ShardedOptimizerBase<T, TInput, TOutput>.SaveModel(string)'. To make the current member override that implementation, add the override keyword. Otherwise add the new keyword.\n\n---\n\n[failure] 141-141: \n'ShardedOptimizer<T, TInput, TOutput>.GetOptions()' hides inherited member 'ShardedOptimizerBase<T, TInput, TOutput>.GetOptions()'. To make the current member override that implementation, add the override keyword. Otherwise add the new keyword.\n\n---\n\n[failure] 119-119: \n'ShardedOptimizer<T, TInput, TOutput>.ShouldEarlyStop()' hides inherited member 'ShardedOptimizerBase<T, TInput, TOutput>.ShouldEarlyStop()'. To make the current member override that implementation, add the override keyword. Otherwise add the new keyword.\n\n</details>\n\n</details>\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/DistributedTraining/ShardedOptimizer.cs around lines 119 to 239, several\nmethods (ShouldEarlyStop, GetOptions, SaveModel, LoadModel) are currently\ndeclared without the override modifier and thus hide the virtual members from\nShardedOptimizerBase causing compiler errors; update each of these method\ndeclarations to include the override keyword (ensuring the method signatures\nexactly match the base class signatures and accessibility), recompile to confirm\nCS0114/CS0506 are resolved, and run tests to verify behavior remains correct.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits 578d634 to cbd637f","createdAt":"2025-11-08T23:38:18Z","path":"src/DistributedTraining/FSDPOptimizer.cs","line":null,"originalLine":239,"diffHunk":"@@ -0,0 +1,241 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.Models.Inputs;\n+using AiDotNet.Optimizers;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Implements a distributed optimizer wrapper that coordinates optimization across multiple processes.\n+/// </summary>\n+/// <remarks>\n+/// <para><b>For Beginners:</b>\n+/// This class wraps any existing optimizer (like Adam, SGD, etc.) and makes it work across\n+/// multiple GPUs or machines. It automatically handles:\n+/// - Synchronizing gradients across all processes\n+/// - Coordinating parameter updates\n+/// - Ensuring all processes stay in sync\n+/// </para>\n+/// <para>\n+/// Think of it like a team of coaches working together - each has their own expertise\n+/// (the wrapped optimizer), but they coordinate their efforts to train the team effectively.\n+/// </para>\n+/// <para>\n+/// Example:\n+/// <code>\n+/// // Original optimizer\n+/// var optimizer = new AdamOptimizer&lt;double, Tensor&lt;double&gt;, Tensor&lt;double&gt;&gt;(model, options);\n+///\n+/// // Wrap it for distributed training\n+/// var backend = new InMemoryCommunicationBackend&lt;double&gt;(rank: 0, worldSize: 4);\n+/// var config = new ShardingConfiguration&lt;double&gt;(backend);\n+/// var distributedOptimizer = new ShardedOptimizer&lt;double, Tensor&lt;double&gt;, Tensor&lt;double&gt;&gt;(\n+///     optimizer, config);\n+///\n+/// // Now optimize as usual - distributed magic happens automatically!\n+/// var result = distributedOptimizer.Optimize(inputData);\n+/// </code>\n+/// </para>\n+/// </remarks>\n+/// <typeparam name=\"T\">The numeric type</typeparam>\n+/// <typeparam name=\"TInput\">The input type for the model</typeparam>\n+/// <typeparam name=\"TOutput\">The output type for the model</typeparam>\n+public class ShardedOptimizer<T, TInput, TOutput> : ShardedOptimizerBase<T, TInput, TOutput>\n+{\n+    /// <summary>\n+    /// Creates a new sharded optimizer wrapping an existing optimizer.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b>\n+    /// This constructor takes your existing optimizer and makes it distributed.\n+    /// You provide:\n+    /// 1. The optimizer you want to make distributed\n+    /// 2. A configuration that tells us how to do the distribution\n+    /// </para>\n+    /// <para>\n+    /// The optimizer will automatically synchronize across all processes during optimization.\n+    /// </para>\n+    /// </remarks>\n+    /// <param name=\"wrappedOptimizer\">The optimizer to wrap with distributed capabilities</param>\n+    /// <param name=\"config\">Configuration for sharding and communication</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown if optimizer or config is null</exception>\n+    public ShardedOptimizer(\n+        IOptimizer<T, TInput, TOutput> wrappedOptimizer,\n+        IShardingConfiguration<T> config)\n+        : base(wrappedOptimizer, config)\n+    {\n+    }\n+\n+    /// <inheritdoc/>\n+    public override OptimizationResult<T, TInput, TOutput> Optimize(OptimizationInputData<T, TInput, TOutput> inputData)\n+    {\n+        if (inputData == null)\n+        {\n+            throw new ArgumentNullException(nameof(inputData));\n+        }\n+\n+        // Ensure all processes start together\n+        Config.CommunicationBackend.Barrier();\n+\n+        // Perform optimization on the wrapped optimizer\n+        var result = WrappedOptimizer.Optimize(inputData);\n+\n+        // Synchronize parameters across all processes if auto-sync is enabled\n+        if (Config.AutoSyncGradients && result.BestSolution != null)\n+        {\n+            SynchronizeParameters(result.BestSolution);\n+        }\n+\n+        // Synchronize optimizer state if needed\n+        SynchronizeOptimizerState();\n+\n+        // Ensure all processes finish together\n+        Config.CommunicationBackend.Barrier();\n+\n+        return result;\n+    }\n+\n+    /// <inheritdoc/>\n+    public override void SynchronizeOptimizerState()\n+    {\n+        // For now, this is a placeholder\n+        // In a full implementation, we would synchronize optimizer-specific state\n+        // like momentum buffers, variance estimates (for Adam), etc.\n+\n+        // Different optimizers have different state to sync:\n+        // - SGD with momentum: velocity vectors\n+        // - Adam: first and second moment estimates\n+        // - RMSprop: squared gradient moving average\n+\n+        // This would require either:\n+        // 1. Extending IOptimizer with state access methods\n+        // 2. Type-specific handling for known optimizer types\n+        // 3. A generic state serialization mechanism\n+\n+        // For the MVP, we assume stateless or that the wrapped optimizer handles its own state\n+    }\n+\n+    /// <inheritdoc/>\n+    public bool ShouldEarlyStop()\n+    {\n+        // Delegate to wrapped optimizer\n+        bool localDecision = WrappedOptimizer.ShouldEarlyStop();\n+\n+        // In distributed training, we need consensus on early stopping\n+        // Using Max operation: if ANY process wants to stop, all processes stop\n+        // This prevents stragglers and ensures synchronized termination\n+\n+        // Create a vector with the local decision (1 for stop, 0 for continue)\n+        var decision = new Vector<T>(new[] { localDecision ? MathHelper.GetNumericOperations<T>().One : MathHelper.GetNumericOperations<T>().Zero });\n+\n+        // Get the maximum across all processes\n+        // If any process returns 1 (stop), the max will be 1\n+        Config.CommunicationBackend.AllReduce(decision, ReductionOperation.Max);\n+\n+        // Check if the result indicates stopping\n+        var numOps = MathHelper.GetNumericOperations<T>();\n+        return !numOps.Equals(decision[0], numOps.Zero);\n+    }\n+\n+    /// <inheritdoc/>\n+    public OptimizationAlgorithmOptions<T, TInput, TOutput> GetOptions()\n+    {\n+        return WrappedOptimizer.GetOptions();\n+    }\n+\n+    /// <inheritdoc/>\n+    public override byte[] Serialize()\n+    {\n+        using var ms = new MemoryStream();\n+        using var writer = new BinaryWriter(ms);\n+\n+        // Serialize sharding configuration info\n+        writer.Write(WorldSize);\n+        writer.Write(Rank);\n+        writer.Write(Config.AutoSyncGradients);\n+        writer.Write(Config.MinimumParameterGroupSize);\n+        writer.Write(Config.EnableGradientCompression);\n+\n+        // Serialize wrapped optimizer\n+        var optimizerData = WrappedOptimizer.Serialize();\n+        writer.Write(optimizerData.Length);\n+        writer.Write(optimizerData);\n+\n+        return ms.ToArray();\n+    }\n+\n+    /// <inheritdoc/>\n+    public override void Deserialize(byte[] data)\n+    {\n+        using var ms = new MemoryStream(data);\n+        using var reader = new BinaryReader(ms);\n+\n+        // Read sharding configuration (for validation)\n+        int savedWorldSize = reader.ReadInt32();\n+        int savedRank = reader.ReadInt32();\n+        reader.ReadBoolean(); // AutoSyncGradients\n+        reader.ReadInt32(); // MinimumParameterGroupSize\n+        reader.ReadBoolean(); // EnableGradientCompression\n+\n+        if (savedWorldSize != WorldSize)\n+        {\n+            throw new InvalidOperationException(\n+                $\"World size mismatch. Optimizer was saved with {savedWorldSize} processes, \" +\n+                $\"but current configuration has {WorldSize} processes.\");\n+        }\n+\n+        // Validate rank matches - different rank could indicate configuration mismatch\n+        if (savedRank != Rank)\n+        {\n+            throw new InvalidOperationException(\n+                $\"Rank mismatch. Optimizer was saved on rank {savedRank}, \" +\n+                $\"but is being loaded on rank {Rank}. This could indicate a configuration error.\");\n+        }\n+\n+        // Read wrapped optimizer\n+        int optimizerDataLength = reader.ReadInt32();\n+        byte[] optimizerData = reader.ReadBytes(optimizerDataLength);\n+        WrappedOptimizer.Deserialize(optimizerData);\n+    }\n+\n+    /// <inheritdoc/>\n+    public void SaveModel(string filePath)\n+    {\n+        // Barrier before rank check to prevent deadlock if rank 0 fails\n+        Config.CommunicationBackend.Barrier();\n+\n+        try\n+        {\n+            // Only rank 0 saves to avoid file write conflicts\n+            if (Rank == 0)\n+            {\n+                var data = Serialize();\n+                File.WriteAllBytes(filePath, data);\n+            }\n+        }\n+        finally\n+        {\n+            // Ensure all processes reach this barrier even if rank 0 fails\n+            Config.CommunicationBackend.Barrier();\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public void LoadModel(string filePath)\n+    {\n+        // Barrier before loading to ensure all processes start together\n+        Config.CommunicationBackend.Barrier();\n+\n+        try\n+        {\n+            // All processes read the same file (read-only, no conflicts)\n+            var data = File.ReadAllBytes(filePath);\n+            Deserialize(data);\n+        }\n+        finally\n+        {\n+            // Ensure all processes finish loading before proceeding\n+            Config.CommunicationBackend.Barrier();\n+        }"}]}},{"id":"PRRT_kwDOKSXUF85hMV2T","isResolved":true,"isOutdated":false,"path":"src/DistributedTraining/ShardedOptimizerBase.cs","line":100,"originalLine":95,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86Vc1GJ","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Resolve WrappedOptimizer member collision**\n\nThe class defines both a field and a property named `WrappedOptimizer`. This won‚Äôt compile, and even if it did, the property body `=> this.WrappedOptimizer` recurses forever. Rename the backing field and have the property return it so the optimizer reference is accessible without stack overflow.\n\n```diff\n-    protected readonly IOptimizer<T, TInput, TOutput> WrappedOptimizer;\n+    protected readonly IOptimizer<T, TInput, TOutput> _wrappedOptimizer;\n@@\n-    public IOptimizer<T, TInput, TOutput> WrappedOptimizer => this.WrappedOptimizer;\n+    public IOptimizer<T, TInput, TOutput> WrappedOptimizer => _wrappedOptimizer;\n@@\n-        this.WrappedOptimizer = wrappedOptimizer ??\n+        _wrappedOptimizer = wrappedOptimizer ??\n             throw new ArgumentNullException(nameof(wrappedOptimizer));\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    /// <summary>\n    /// The wrapped optimizer that this sharded optimizer delegates to.\n    /// </summary>\n    protected readonly IOptimizer<T, TInput, TOutput> _wrappedOptimizer;\n\n    /// <summary>\n    /// The sharding configuration containing communication backend and settings.\n    /// </summary>\n    protected readonly IShardingConfiguration<T> Config;\n\n    /// <inheritdoc/>\n    public IOptimizer<T, TInput, TOutput> WrappedOptimizer => _wrappedOptimizer;\n\n    /// <inheritdoc/>\n    public int Rank => Config.CommunicationBackend.Rank;\n\n    /// <inheritdoc/>\n    public int WorldSize => Config.CommunicationBackend.WorldSize;\n\n    /// <inheritdoc/>\n    public IShardingConfiguration<T> ShardingConfiguration => Config;\n\n    /// <summary>\n    /// Initializes a new instance of the ShardedOptimizerBase class.\n    /// </summary>\n    /// <remarks>\n    /// <para>\n    /// This constructor wraps an existing optimizer with distributed training capabilities.\n    /// It initializes the communication backend if needed and prepares for distributed optimization.\n    /// </para>\n    /// <para><b>For Beginners:</b> This constructor takes your regular optimizer and makes it distributed.\n    ///\n    /// You provide:\n    /// 1. The optimizer you want to distribute (like Adam, SGD, etc.)\n    /// 2. Configuration that tells us how to distribute it\n    ///\n    /// The constructor automatically:\n    /// - Sets up communication if not already done\n    /// - Prepares the optimizer for coordinated training\n    /// - Ensures all processes can work together\n    /// </para>\n    /// </remarks>\n    /// <param name=\"wrappedOptimizer\">The optimizer to wrap with distributed capabilities</param>\n    /// <param name=\"config\">Configuration for sharding and communication</param>\n    /// <exception cref=\"ArgumentNullException\">Thrown if optimizer or config is null</exception>\n    protected ShardedOptimizerBase(\n        IOptimizer<T, TInput, TOutput> wrappedOptimizer,\n        IShardingConfiguration<T> config)\n    {\n        _wrappedOptimizer = wrappedOptimizer ?? throw new ArgumentNullException(nameof(wrappedOptimizer));\n        Config = config ?? throw new ArgumentNullException(nameof(config));\n        NumOps = MathHelper.GetNumericOperations<T>();\n\n        // Initialize communication backend if needed\n        if (!Config.CommunicationBackend.IsInitialized)\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/DistributedTraining/ShardedOptimizerBase.cs around lines 41 to 95, the\nclass currently declares a field and a property both named WrappedOptimizer\ncausing a name collision and the property to recursively call itself; rename the\nbacking field (e.g. to _wrappedOptimizer) and update the constructor to assign\nthe passed wrappedOptimizer to that renamed field, then change the\nWrappedOptimizer property to return the renamed backing field (preserve the same\naccessibility and generic types) to avoid recursion and make the optimizer\nreference accessible.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits 578d634 to cbd637f","createdAt":"2025-11-08T23:38:18Z","path":"src/DistributedTraining/ShardedOptimizerBase.cs","line":100,"originalLine":95,"diffHunk":"@@ -0,0 +1,204 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Provides base implementation for distributed optimizers with parameter sharding.\n+/// </summary>\n+/// <remarks>\n+/// <para>\n+/// This abstract class implements common functionality for all sharded optimizers,\n+/// including optimizer wrapping, parameter synchronization, consensus-based early stopping,\n+/// and serialization. Derived classes can customize the optimization strategy, implement\n+/// different sharding approaches (FSDP, ZeRO, etc.), or add optimizer-specific features.\n+/// </para>\n+/// <para><b>For Beginners:</b> This is the foundation that all distributed optimizers build upon.\n+///\n+/// Think of this as a template for coordinating optimization across multiple computers or GPUs.\n+/// It handles common tasks like:\n+/// - Wrapping regular optimizers to work in distributed mode\n+/// - Syncing parameters across all processes after updates\n+/// - Making sure all processes agree on when to stop training\n+/// - Saving and loading distributed optimizer state\n+///\n+/// Specific types of distributed optimizers (like data-parallel or ZeRO) inherit from\n+/// this and add their own strategies. This prevents code duplication and ensures all\n+/// distributed optimizers work consistently.\n+/// </para>\n+/// </remarks>\n+/// <typeparam name=\"T\">The numeric type for operations</typeparam>\n+/// <typeparam name=\"TInput\">The input type for the model</typeparam>\n+/// <typeparam name=\"TOutput\">The output type for the model</typeparam>\n+public abstract class ShardedOptimizerBase<T, TInput, TOutput> : IShardedOptimizer<T, TInput, TOutput>\n+{\n+    /// <summary>\n+    /// Provides numeric operations for type T.\n+    /// </summary>\n+    protected readonly INumericOperations<T> NumOps;\n+\n+    /// <summary>\n+    /// The wrapped optimizer that this sharded optimizer delegates to.\n+    /// </summary>\n+    protected readonly IOptimizer<T, TInput, TOutput> WrappedOptimizer;\n+\n+    /// <summary>\n+    /// The sharding configuration containing communication backend and settings.\n+    /// </summary>\n+    protected readonly IShardingConfiguration<T> Config;\n+\n+    /// <inheritdoc/>\n+    public IOptimizer<T, TInput, TOutput> WrappedOptimizer => this.WrappedOptimizer;\n+\n+    /// <inheritdoc/>\n+    public int Rank => Config.CommunicationBackend.Rank;\n+\n+    /// <inheritdoc/>\n+    public int WorldSize => Config.CommunicationBackend.WorldSize;\n+\n+    /// <inheritdoc/>\n+    public IShardingConfiguration<T> ShardingConfiguration => Config;\n+\n+    /// <summary>\n+    /// Initializes a new instance of the ShardedOptimizerBase class.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para>\n+    /// This constructor wraps an existing optimizer with distributed training capabilities.\n+    /// It initializes the communication backend if needed and prepares for distributed optimization.\n+    /// </para>\n+    /// <para><b>For Beginners:</b> This constructor takes your regular optimizer and makes it distributed.\n+    ///\n+    /// You provide:\n+    /// 1. The optimizer you want to distribute (like Adam, SGD, etc.)\n+    /// 2. Configuration that tells us how to distribute it\n+    ///\n+    /// The constructor automatically:\n+    /// - Sets up communication if not already done\n+    /// - Prepares the optimizer for coordinated training\n+    /// - Ensures all processes can work together\n+    /// </para>\n+    /// </remarks>\n+    /// <param name=\"wrappedOptimizer\">The optimizer to wrap with distributed capabilities</param>\n+    /// <param name=\"config\">Configuration for sharding and communication</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown if optimizer or config is null</exception>\n+    protected ShardedOptimizerBase(\n+        IOptimizer<T, TInput, TOutput> wrappedOptimizer,\n+        IShardingConfiguration<T> config)\n+    {\n+        this.WrappedOptimizer = wrappedOptimizer ?? throw new ArgumentNullException(nameof(wrappedOptimizer));\n+        Config = config ?? throw new ArgumentNullException(nameof(config));\n+        NumOps = MathHelper.GetNumericOperations<T>();\n+\n+        // Initialize communication backend if needed\n+        if (!Config.CommunicationBackend.IsInitialized)"}]}},{"id":"PRRT_kwDOKSXUF85hMV2V","isResolved":true,"isOutdated":false,"path":"src/PredictionModelBuilder.cs","line":338,"originalLine":285,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86Vc1GL","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**...**\n...\n\n...\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/PredictionModelBuilder.cs around lines 270 to 285, ensure wrapping uses\nthe original _model and optimizer safely and only when required: add null-checks\nfor optimizer before constructing a ShardedOptimizer, and avoid double-wrapping\nby only creating ShardedModel/ShardedOptimizer when the underlying\n_model/optimizer are not already sharded (or by unwrapping/inspecting types).\nReplace direct wrapping with logic that (1) verifies\n_distributedTrainingConfiguration != null && IsEnabled, (2) if _model is not\nalready a DistributedTraining.ShardedModel create a new\nShardedModel<T,...>(_model, ShardingConfiguration), and (3) if optimizer is\nnon-null and not already a DistributedTraining.ShardedOptimizer create a new\nShardedOptimizer<T,...>(optimizer, ShardingConfiguration); assign these to model\nand finalOptimizer.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits ae2f65b to 32d17fd","createdAt":"2025-11-08T23:38:18Z","path":"src/PredictionModelBuilder.cs","line":338,"originalLine":285,"diffHunk":"@@ -266,14 +267,31 @@ public PredictionModelResult<T, TInput, TOutput> Build(TInput x, TOutput y)\n         var outlierRemoval = _outlierRemoval ?? new NoOutlierRemoval<T, TInput, TOutput>();\n         var dataPreprocessor = _dataPreprocessor ?? new DefaultDataPreprocessor<T, TInput, TOutput>(normalizer, featureSelector, outlierRemoval);\n \n+        // Wrap model and optimizer for distributed training if configured\n+        IFullModel<T, TInput, TOutput> model = _model;\n+        IOptimizer<T, TInput, TOutput> finalOptimizer = optimizer;\n+\n+        if (_distributedTrainingConfiguration != null && _distributedTrainingConfiguration.IsEnabled)\n+        {\n+            // Wrap model with ShardedModel\n+            model = new DistributedTraining.ShardedModel<T, TInput, TOutput>(\n+                _model,\n+                _distributedTrainingConfiguration.ShardingConfiguration);\n+\n+            // Wrap optimizer with ShardedOptimizer\n+            finalOptimizer = new DistributedTraining.ShardedOptimizer<T, TInput, TOutput>(\n+                optimizer,\n+                _distributedTrainingConfiguration.ShardingConfiguration);\n+        }"}]}},{"id":"PRRT_kwDOKSXUF85hMcUf","isResolved":true,"isOutdated":true,"path":"docs/DistributedTrainingImplementations.md","line":null,"originalLine":15,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86Vc8sU","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üü° Minor_\n\n**Fix typo in GlooCommunicationBackend name.**\n\n\"GlooComm unicationBackend\" has a broken word; should be \"GlooCommunicationBackend\".\n\n\n\n```diff\n-‚îî‚îÄ‚îÄ GlooComm unicationBackend<T> (CPU-based)\n+‚îî‚îÄ‚îÄ GlooCommunicationBackend<T> (CPU-based)\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n‚îî‚îÄ‚îÄ GlooCommunicationBackend<T> (CPU-based)\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn docs/DistributedTrainingImplementations.md around line 15, the text \"GlooComm\nunicationBackend<T> (CPU-based)\" contains a broken word; change it to\n\"GlooCommunicationBackend<T> (CPU-based)\". Ensure the corrected spelling is used\nhere and update any nearby occurrences in the file for consistency.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits 578d634 to 943ec25","createdAt":"2025-11-09T01:00:39Z","path":"docs/DistributedTrainingImplementations.md","line":null,"originalLine":15,"diffHunk":"@@ -0,0 +1,513 @@\n+# Distributed Training Concrete Implementations\n+\n+This document outlines all concrete implementations that should be created for the distributed training framework, based on industry standards and real-world scenarios.\n+\n+## Architecture Overview\n+\n+```\n+ICommunicationBackend<T>\n+    ‚Üì\n+CommunicationBackendBase<T> (abstract)\n+    ‚Üì\n+‚îú‚îÄ‚îÄ InMemoryCommunicationBackend<T> (for testing)\n+‚îú‚îÄ‚îÄ MPICommunicationBackend<T> (MPI.NET for production)\n+‚îú‚îÄ‚îÄ NCCLCommunicationBackend<T> (NVIDIA GPUs)\n+‚îî‚îÄ‚îÄ GlooComm unicationBackend<T> (CPU-based)"}]}},{"id":"PRRT_kwDOKSXUF85hMtLp","isResolved":true,"isOutdated":false,"path":"src/DistributedTraining/AsyncSGDOptimizer.cs","line":142,"originalLine":142,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VdUAP","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Implement required serialization overrides.**\n\n`ShardedOptimizerBase` still declares abstract `Serialize()`/`Deserialize(byte[])`, so this class fails to compile (CS0534 in the pipeline). Add overrides that at least delegate to the wrapped optimizer so `AsyncSGDOptimizer` satisfies the contract and can persist state.\n\n```diff\n     public bool ShouldSync(int iteration)\n     {\n         // Some async implementations do periodic sync every N iterations\n         // to prevent too much drift\n         if (_maxStaleness == 0)\n             return true; // Always sync (becomes sync SGD)\n \n         // Framework pattern - could implement periodic sync\n         return false;\n     }\n+\n+    public override byte[] Serialize()\n+    {\n+        return WrappedOptimizer.Serialize();\n+    }\n+\n+    public override void Deserialize(byte[] data)\n+    {\n+        if (data == null)\n+            throw new ArgumentNullException(nameof(data));\n+\n+        WrappedOptimizer.Deserialize(data);\n+    }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\npublic class AsyncSGDOptimizer<T, TInput, TOutput> : ShardedOptimizerBase<T, TInput, TOutput>\n{\n    private readonly int _maxStaleness;\n\n    /// <summary>\n    /// Creates an async SGD optimizer.\n    /// </summary>\n    /// <param name=\"wrappedOptimizer\">The optimizer to wrap with async capabilities</param>\n    /// <param name=\"config\">Configuration for sharding and communication</param>\n    /// <param name=\"allowStaleness\">Maximum allowed staleness in gradient steps (default: 0 = sync)</param>\n    public AsyncSGDOptimizer(\n        IOptimizer<T, TInput, TOutput> wrappedOptimizer,\n        IShardingConfiguration<T> config,\n        int allowStaleness = 0)\n        : base(wrappedOptimizer, config)\n    {\n        _maxStaleness = allowStaleness;\n    }\n\n    /// <inheritdoc/>\n    public override OptimizationResult<T, TInput, TOutput> Optimize(OptimizationInputData<T, TInput, TOutput> inputData)\n    {\n        if (inputData == null)\n            throw new ArgumentNullException(nameof(inputData));\n\n        // NO barrier at start - async operation!\n\n        // Optimize locally without waiting\n        var result = WrappedOptimizer.Optimize(inputData);\n\n        // Asynchronous parameter update\n        if (Config.AutoSyncGradients && result.BestSolution != null)\n        {\n            // In true async SGD:\n            // 1. Push gradients to parameter server (non-blocking)\n            // 2. Pull updated parameters (may be from other workers' updates)\n            // 3. Continue immediately without waiting\n\n            // For this framework implementation, we provide simplified async pattern\n            // Production would use parameter server or async AllReduce\n            var parameters = result.BestSolution.GetParameters();\n\n            // Simulate async update - in production, this would be non-blocking\n            Config.CommunicationBackend.AllReduce(parameters, ReductionOperation.Average);\n\n            result.BestSolution.SetParameters(parameters);\n        }\n\n        // NO barrier at end - continue immediately!\n\n        return result;\n    }\n\n    /// <inheritdoc/>\n    public override void SynchronizeOptimizerState()\n    {\n        // In async SGD, optimizer states are typically local (not synchronized)\n        // Each worker maintains its own state and updates independently\n    }\n\n    /// <summary>\n    /// Checks if a barrier should be used (for periodic synchronization).\n    /// </summary>\n    /// <param name=\"iteration\">Current iteration number</param>\n    /// <returns>True if should synchronize at this iteration</returns>\n    public bool ShouldSync(int iteration)\n    {\n        // Some async implementations do periodic sync every N iterations\n        // to prevent too much drift\n        if (_maxStaleness == 0)\n            return true; // Always sync (becomes sync SGD)\n\n        // Framework pattern - could implement periodic sync\n        return false;\n    }\n\n    public override byte[] Serialize()\n    {\n        return WrappedOptimizer.Serialize();\n    }\n\n    public override void Deserialize(byte[] data)\n    {\n        if (data == null)\n            throw new ArgumentNullException(nameof(data));\n\n        WrappedOptimizer.Deserialize(data);\n    }\n}\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>üß∞ Tools</summary>\n\n<details>\n<summary>ü™õ GitHub Actions: Build</summary>\n\n[error] 68-68: CS0534: 'AsyncSGDOptimizer<T, TInput, TOutput>' does not implement inherited abstract member 'ShardedOptimizerBase<T, TInput, TOutput>.Serialize()'. (Build step: 'dotnet build --no-restore --configuration Debug')\n\n</details>\n<details>\n<summary>ü™õ GitHub Check: Build All Frameworks</summary>\n\n[failure] 68-68: \n'AsyncSGDOptimizer<T, TInput, TOutput>' does not implement inherited abstract member 'ShardedOptimizerBase<T, TInput, TOutput>.Deserialize(byte[])'\n\n---\n\n[failure] 68-68: \n'AsyncSGDOptimizer<T, TInput, TOutput>' does not implement inherited abstract member 'ShardedOptimizerBase<T, TInput, TOutput>.Serialize()'\n\n</details>\n<details>\n<summary>ü™õ GitHub Check: Publish Size Analysis</summary>\n\n[failure] 68-68: \n'AsyncSGDOptimizer<T, TInput, TOutput>' does not implement inherited abstract member 'ShardedOptimizerBase<T, TInput, TOutput>.Serialize()'\n\n---\n\n[failure] 68-68: \n'AsyncSGDOptimizer<T, TInput, TOutput>' does not implement inherited abstract member 'ShardedOptimizerBase<T, TInput, TOutput>.Deserialize(byte[])'\n\n</details>\n\n</details>\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/DistributedTraining/AsyncSGDOptimizer.cs around lines 68 to 142, the\nclass is missing overrides for the abstract Serialize() and Deserialize(byte[])\nmethods from ShardedOptimizerBase which causes a compilation error; implement\npublic override byte[] Serialize() and public override void Deserialize(byte[]\ndata) that at minimum call into the wrapped optimizer to persist/restore its\nstate (e.g. call WrappedOptimizer.Serialize() and pass deserialized bytes to\nWrappedOptimizer.Deserialize(...)), and include any minimal\nAsyncSGDOptimizer-specific fields (such as _maxStaleness) in the serialized\npayload or metadata so state is preserved across serialize/deserialize cycles;\nensure the methods handle null/empty payloads and exceptions appropriately.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits 578d634 to cbd637f","createdAt":"2025-11-09T02:30:53Z","path":"src/DistributedTraining/AsyncSGDOptimizer.cs","line":142,"originalLine":142,"diffHunk":"@@ -0,0 +1,143 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.Models.Inputs;\n+using AiDotNet.Optimizers;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Implements Asynchronous SGD optimizer - allows asynchronous parameter updates without strict barriers.\n+/// </summary>\n+/// <remarks>\n+/// <para><b>Strategy Overview:</b>\n+/// Asynchronous SGD (and variants like Hogwild!) removes synchronization barriers between workers.\n+/// Each process updates parameters independently without waiting for others, using a parameter\n+/// server or shared memory. This eliminates idle time from synchronization but introduces stale\n+/// gradients - workers may compute gradients on slightly outdated parameters.\n+///\n+/// When done correctly (sparse gradients, low contention), async SGD can achieve near-linear\n+/// speedup without much accuracy loss. However, it's more sensitive to hyperparameters and\n+/// can be unstable for dense updates.\n+/// </para>\n+/// <para><b>For Beginners:</b>\n+/// Async SGD is like a team working independently without meetings. Each person:\n+/// 1. Reads current parameters\n+/// 2. Computes gradients on their data\n+/// 3. Updates parameters immediately (no waiting!)\n+///\n+/// Pro: No time wasted waiting for slow workers\n+/// Con: Updates might conflict or use slightly stale information\n+///\n+/// This works well when updates are sparse (touching different parameters) but can be\n+/// unstable when all workers update the same parameters frequently.\n+/// </para>\n+/// <para><b>Use Cases:</b>\n+/// - Sparse models (embeddings, recommendation systems)\n+/// - Scenarios with stragglers (some workers slower than others)\n+/// - When synchronization overhead is very high\n+/// - Research and experimentation\n+/// </para>\n+/// <para><b>Trade-offs:</b>\n+/// - Memory: Requires parameter server or shared memory\n+/// - Communication: Asynchronous - can be higher total volume\n+/// - Complexity: High - requires parameter server infrastructure\n+/// - Convergence: Can be slower or less stable than sync SGD\n+/// - Best for: Sparse updates, heterogeneous workers, straggler tolerance\n+/// - Limitation: Harder to tune, may require staleness-aware algorithms\n+/// </para>\n+/// <para><b>Implementation Note:</b>\n+/// This framework provides async SGD infrastructure. Full production implementation\n+/// requires parameter server setup or shared memory coordination. This implementation\n+/// demonstrates the async update pattern.\n+/// </para>\n+/// <para>\n+/// Example:\n+/// <code>\n+/// var optimizer = new AdamOptimizer&lt;double, Tensor&lt;double&gt;, Tensor&lt;double&gt;&gt;(model, options);\n+/// var backend = new InMemoryCommunicationBackend&lt;double&gt;(rank: 0, worldSize: 4);\n+/// var config = new ShardingConfiguration&lt;double&gt;(backend);\n+///\n+/// var asyncOptimizer = new AsyncSGDOptimizer&lt;double, Tensor&lt;double&gt;, Tensor&lt;double&gt;&gt;(\n+///     optimizer, config, allowStaleness: 2); // Allow up to 2 stale gradient steps\n+/// </code>\n+/// </para>\n+/// </remarks>\n+/// <typeparam name=\"T\">The numeric type</typeparam>\n+/// <typeparam name=\"TInput\">The input type for the model</typeparam>\n+/// <typeparam name=\"TOutput\">The output type for the model</typeparam>\n+public class AsyncSGDOptimizer<T, TInput, TOutput> : ShardedOptimizerBase<T, TInput, TOutput>\n+{\n+    private readonly int _maxStaleness;\n+\n+    /// <summary>\n+    /// Creates an async SGD optimizer.\n+    /// </summary>\n+    /// <param name=\"wrappedOptimizer\">The optimizer to wrap with async capabilities</param>\n+    /// <param name=\"config\">Configuration for sharding and communication</param>\n+    /// <param name=\"allowStaleness\">Maximum allowed staleness in gradient steps (default: 0 = sync)</param>\n+    public AsyncSGDOptimizer(\n+        IOptimizer<T, TInput, TOutput> wrappedOptimizer,\n+        IShardingConfiguration<T> config,\n+        int allowStaleness = 0)\n+        : base(wrappedOptimizer, config)\n+    {\n+        _maxStaleness = allowStaleness;\n+    }\n+\n+    /// <inheritdoc/>\n+    public override OptimizationResult<T, TInput, TOutput> Optimize(OptimizationInputData<T, TInput, TOutput> inputData)\n+    {\n+        if (inputData == null)\n+            throw new ArgumentNullException(nameof(inputData));\n+\n+        // NO barrier at start - async operation!\n+\n+        // Optimize locally without waiting\n+        var result = WrappedOptimizer.Optimize(inputData);\n+\n+        // Asynchronous parameter update\n+        if (Config.AutoSyncGradients && result.BestSolution != null)\n+        {\n+            // In true async SGD:\n+            // 1. Push gradients to parameter server (non-blocking)\n+            // 2. Pull updated parameters (may be from other workers' updates)\n+            // 3. Continue immediately without waiting\n+\n+            // For this framework implementation, we provide simplified async pattern\n+            // Production would use parameter server or async AllReduce\n+            var parameters = result.BestSolution.GetParameters();\n+\n+            // Simulate async update - in production, this would be non-blocking\n+            Config.CommunicationBackend.AllReduce(parameters, ReductionOperation.Average);\n+\n+            result.BestSolution.SetParameters(parameters);\n+        }\n+\n+        // NO barrier at end - continue immediately!\n+\n+        return result;\n+    }\n+\n+    /// <inheritdoc/>\n+    public override void SynchronizeOptimizerState()\n+    {\n+        // In async SGD, optimizer states are typically local (not synchronized)\n+        // Each worker maintains its own state and updates independently\n+    }\n+\n+    /// <summary>\n+    /// Checks if a barrier should be used (for periodic synchronization).\n+    /// </summary>\n+    /// <param name=\"iteration\">Current iteration number</param>\n+    /// <returns>True if should synchronize at this iteration</returns>\n+    public bool ShouldSync(int iteration)\n+    {\n+        // Some async implementations do periodic sync every N iterations\n+        // to prevent too much drift\n+        if (_maxStaleness == 0)\n+            return true; // Always sync (becomes sync SGD)\n+\n+        // Framework pattern - could implement periodic sync\n+        return false;\n+    }"}]}},{"id":"PRRT_kwDOKSXUF85hMtLt","isResolved":true,"isOutdated":false,"path":"src/DistributedTraining/DDPOptimizer.cs","line":224,"originalLine":81,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VdUAV","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Critical: Missing required Serialize and Deserialize implementations.**\n\nThe class inherits from `ShardedOptimizerBase<T, TInput, TOutput>` which declares abstract `Serialize()` and `Deserialize(byte[])` methods, but this class doesn't implement them. This will cause compilation failures.\n\n\n\nImplement the serialization methods similar to FSDPOptimizer (see src/DistributedTraining/FSDPOptimizer.cs lines 144-179). Add these methods:\n\n```diff\n     /// <inheritdoc/>\n     public override void SynchronizeOptimizerState()\n     {\n         // In DDP, optimizer states are not sharded\n         // Each process maintains its own full optimizer state\n         // No synchronization needed unless implementing state averaging\n         // (which is not standard for DDP)\n     }\n+\n+    /// <inheritdoc/>\n+    public override byte[] Serialize()\n+    {\n+        using var ms = new MemoryStream();\n+        using var writer = new BinaryWriter(ms);\n+\n+        // Serialize sharding configuration info\n+        writer.Write(WorldSize);\n+        writer.Write(Rank);\n+        writer.Write(Config.AutoSyncGradients);\n+        writer.Write(Config.MinimumParameterGroupSize);\n+        writer.Write(Config.EnableGradientCompression);\n+\n+        // Serialize wrapped optimizer\n+        var optimizerData = WrappedOptimizer.Serialize();\n+        writer.Write(optimizerData.Length);\n+        writer.Write(optimizerData);\n+\n+        return ms.ToArray();\n+    }\n+\n+    /// <inheritdoc/>\n+    public override void Deserialize(byte[] data)\n+    {\n+        using var ms = new MemoryStream(data);\n+        using var reader = new BinaryReader(ms);\n+\n+        // Read sharding configuration (for validation)\n+        int savedWorldSize = reader.ReadInt32();\n+        int savedRank = reader.ReadInt32();\n+        reader.ReadBoolean(); // AutoSyncGradients\n+        reader.ReadInt32(); // MinimumParameterGroupSize\n+        reader.ReadBoolean(); // EnableGradientCompression\n+\n+        if (savedWorldSize != WorldSize)\n+        {\n+            throw new InvalidOperationException(\n+                $\"World size mismatch. Optimizer was saved with {savedWorldSize} processes, \" +\n+                $\"but current configuration has {WorldSize} processes.\");\n+        }\n+\n+        if (savedRank != Rank)\n+        {\n+            throw new InvalidOperationException(\n+                $\"Rank mismatch. Optimizer was saved on rank {savedRank}, \" +\n+                $\"but is being loaded on rank {Rank}.\");\n+        }\n+\n+        // Read wrapped optimizer\n+        int optimizerDataLength = reader.ReadInt32();\n+        byte[] optimizerData = reader.ReadBytes(optimizerDataLength);\n+        WrappedOptimizer.Deserialize(optimizerData);\n+    }\n }\n```\n\nBased on static analysis hints.\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\npublic class DDPOptimizer<T, TInput, TOutput> : ShardedOptimizerBase<T, TInput, TOutput>\n{\n    public DDPOptimizer(\n        IOptimizer<T, TInput, TOutput> wrappedOptimizer,\n        IShardingConfiguration<T> config)\n        : base(wrappedOptimizer, config)\n    {\n    }\n\n    /// <inheritdoc/>\n    public override OptimizationResult<T, TInput, TOutput> Optimize(OptimizationInputData<T, TInput, TOutput> inputData)\n    {\n        if (inputData == null)\n            throw new ArgumentNullException(nameof(inputData));\n\n        // Barrier to ensure all processes start together\n        Config.CommunicationBackend.Barrier();\n\n        // Each process optimizes on its local data\n        var result = WrappedOptimizer.Optimize(inputData);\n\n        // Synchronize parameters (average across all processes)\n        if (Config.AutoSyncGradients && result.BestSolution != null)\n        {\n            SynchronizeParameters(result.BestSolution);\n        }\n\n        // Barrier to ensure all processes finish together\n        Config.CommunicationBackend.Barrier();\n\n        return result;\n    }\n\n    /// <inheritdoc/>\n    public override void SynchronizeOptimizerState()\n    {\n        // In DDP, optimizer states are not sharded\n        // Each process maintains its own full optimizer state\n        // No synchronization needed unless implementing state averaging\n        // (which is not standard for DDP)\n    }\n\n    /// <inheritdoc/>\n    public override byte[] Serialize()\n    {\n        using var ms = new MemoryStream();\n        using var writer = new BinaryWriter(ms);\n\n        // Serialize sharding configuration info\n        writer.Write(WorldSize);\n        writer.Write(Rank);\n        writer.Write(Config.AutoSyncGradients);\n        writer.Write(Config.MinimumParameterGroupSize);\n        writer.Write(Config.EnableGradientCompression);\n\n        // Serialize wrapped optimizer\n        var optimizerData = WrappedOptimizer.Serialize();\n        writer.Write(optimizerData.Length);\n        writer.Write(optimizerData);\n\n        return ms.ToArray();\n    }\n\n    /// <inheritdoc/>\n    public override void Deserialize(byte[] data)\n    {\n        using var ms = new MemoryStream(data);\n        using var reader = new BinaryReader(ms);\n\n        // Read sharding configuration (for validation)\n        int savedWorldSize = reader.ReadInt32();\n        int savedRank = reader.ReadInt32();\n        reader.ReadBoolean(); // AutoSyncGradients\n        reader.ReadInt32(); // MinimumParameterGroupSize\n        reader.ReadBoolean(); // EnableGradientCompression\n\n        if (savedWorldSize != WorldSize)\n        {\n            throw new InvalidOperationException(\n                $\"World size mismatch. Optimizer was saved with {savedWorldSize} processes, \" +\n                $\"but current configuration has {WorldSize} processes.\");\n        }\n\n        if (savedRank != Rank)\n        {\n            throw new InvalidOperationException(\n                $\"Rank mismatch. Optimizer was saved on rank {savedRank}, \" +\n                $\"but is being loaded on rank {Rank}.\");\n        }\n\n        // Read wrapped optimizer\n        int optimizerDataLength = reader.ReadInt32();\n        byte[] optimizerData = reader.ReadBytes(optimizerDataLength);\n        WrappedOptimizer.Deserialize(optimizerData);\n    }\n}\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>üß∞ Tools</summary>\n\n<details>\n<summary>ü™õ GitHub Check: Build All Frameworks</summary>\n\n[failure] 40-40: \n'DDPOptimizer<T, TInput, TOutput>' does not implement inherited abstract member 'ShardedOptimizerBase<T, TInput, TOutput>.Deserialize(byte[])'\n\n---\n\n[failure] 40-40: \n'DDPOptimizer<T, TInput, TOutput>' does not implement inherited abstract member 'ShardedOptimizerBase<T, TInput, TOutput>.Serialize()'\n\n</details>\n<details>\n<summary>ü™õ GitHub Check: Publish Size Analysis</summary>\n\n[failure] 40-40: \n'DDPOptimizer<T, TInput, TOutput>' does not implement inherited abstract member 'ShardedOptimizerBase<T, TInput, TOutput>.Deserialize(byte[])'\n\n---\n\n[failure] 40-40: \n'DDPOptimizer<T, TInput, TOutput>' does not implement inherited abstract member 'ShardedOptimizerBase<T, TInput, TOutput>.Serialize()'\n\n</details>\n\n</details>\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/DistributedTraining/DDPOptimizer.cs around lines 40 to 81, implement the\nmissing overrides for Serialize() and Deserialize(byte[]) (they are abstract in\nShardedOptimizerBase and currently absent), following the same pattern as\nFSDPOptimizer (src/DistributedTraining/FSDPOptimizer.cs lines 144-179): add\npublic override byte[] Serialize() that collects/serializes this optimizer's\nnecessary state (at minimum the wrapped optimizer state and any config-relevant\nfields) into a byte[] (or delegates to the WrappedOptimizer.Serialize()), and\nadd public override void Deserialize(byte[] data) that reverses this process\n(parse the byte[] and restore the WrappedOptimizer state by calling its\nDeserialize or deserializing contained fields). Ensure method signatures match\nthe base class, handle null/invalid data defensively, and maintain compatibility\nwith the existing serialization format used by FSDPOptimizer.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits 578d634 to cbd637f","createdAt":"2025-11-09T02:30:53Z","path":"src/DistributedTraining/DDPOptimizer.cs","line":224,"originalLine":81,"diffHunk":"@@ -0,0 +1,81 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.Models.Inputs;\n+using AiDotNet.Optimizers;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Implements DDP (Distributed Data Parallel) optimizer - standard AllReduce gradient synchronization.\n+/// </summary>\n+/// <remarks>\n+/// <para><b>Strategy Overview:</b>\n+/// DDP optimizer is the standard distributed optimizer that works with DDPModel. After each optimization\n+/// step, gradients are synchronized across all processes using AllReduce with averaging. This ensures\n+/// all model replicas perform identical parameter updates and stay synchronized. This is PyTorch's\n+/// default distributed optimizer strategy.\n+/// </para>\n+/// <para><b>For Beginners:</b>\n+/// This optimizer works with DDP (Distributed Data Parallel) models. After computing gradients on\n+/// local data, it averages them across all processes so everyone updates their model identically.\n+/// Think of it like a study group where everyone does practice problems independently, then shares\n+/// and averages their answers before updating their notes.\n+/// </para>\n+/// <para><b>Use Cases:</b>\n+/// - Standard multi-GPU training with full model replicas\n+/// - Works with any optimizer (Adam, SGD, RMSprop, etc.)\n+/// - Default choice for distributed training\n+/// - Fast interconnects (NVLink, InfiniBand)\n+/// </para>\n+/// <para><b>Trade-offs:</b>\n+/// - Memory: Each process stores full optimizer state\n+/// - Communication: Low - only gradients synchronized (AllReduce)\n+/// - Complexity: Low - simplest distributed optimizer\n+/// - Best for: Standard distributed training scenarios\n+/// </para>\n+/// </remarks>\n+/// <typeparam name=\"T\">The numeric type</typeparam>\n+/// <typeparam name=\"TInput\">The input type for the model</typeparam>\n+/// <typeparam name=\"TOutput\">The output type for the model</typeparam>\n+public class DDPOptimizer<T, TInput, TOutput> : ShardedOptimizerBase<T, TInput, TOutput>\n+{\n+    public DDPOptimizer(\n+        IOptimizer<T, TInput, TOutput> wrappedOptimizer,\n+        IShardingConfiguration<T> config)\n+        : base(wrappedOptimizer, config)\n+    {\n+    }\n+\n+    /// <inheritdoc/>\n+    public override OptimizationResult<T, TInput, TOutput> Optimize(OptimizationInputData<T, TInput, TOutput> inputData)\n+    {\n+        if (inputData == null)\n+            throw new ArgumentNullException(nameof(inputData));\n+\n+        // Barrier to ensure all processes start together\n+        Config.CommunicationBackend.Barrier();\n+\n+        // Each process optimizes on its local data\n+        var result = WrappedOptimizer.Optimize(inputData);\n+\n+        // Synchronize parameters (average across all processes)\n+        if (Config.AutoSyncGradients && result.BestSolution != null)\n+        {\n+            SynchronizeParameters(result.BestSolution);\n+        }\n+\n+        // Barrier to ensure all processes finish together\n+        Config.CommunicationBackend.Barrier();\n+\n+        return result;\n+    }\n+\n+    /// <inheritdoc/>\n+    public override void SynchronizeOptimizerState()\n+    {\n+        // In DDP, optimizer states are not sharded\n+        // Each process maintains its own full optimizer state\n+        // No synchronization needed unless implementing state averaging\n+        // (which is not standard for DDP)\n+    }\n+}"}]}},{"id":"PRRT_kwDOKSXUF85hMtLv","isResolved":true,"isOutdated":false,"path":"src/DistributedTraining/ElasticOptimizer.cs","line":110,"originalLine":110,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VdUAY","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Implement missing abstract methods Serialize() and Deserialize().**\n\nThe class does not implement the required abstract methods `Serialize()` and `Deserialize(byte[])` inherited from `ShardedOptimizerBase<T, TInput, TOutput>`, causing build failures.\n\n\n\nAdd the missing method implementations. For elastic training, serialization must capture elastic-specific state:\n\n```diff\n+    /// <inheritdoc/>\n+    public override byte[] Serialize()\n+    {\n+        // Serialize elastic-specific state along with base optimizer state\n+        var baseState = base.Serialize();\n+        \n+        // In production, would serialize:\n+        // - Current world size\n+        // - Min/max worker bounds\n+        // - Checkpoint metadata for recovery\n+        \n+        return baseState;\n+    }\n+\n+    /// <inheritdoc/>\n+    public override void Deserialize(byte[] data)\n+    {\n+        // Restore elastic-specific state\n+        base.Deserialize(data);\n+        \n+        // In production, would restore:\n+        // - Worker bounds\n+        // - Last known world size\n+        // - Re-shard state across current workers\n+        \n+        _currentWorldSize = WorldSize;\n+    }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\npublic class ElasticOptimizer<T, TInput, TOutput> : ShardedOptimizerBase<T, TInput, TOutput>\n{\n    private readonly int _minWorkers;\n    private readonly int _maxWorkers;\n    private int _currentWorldSize;\n\n    /// <summary>\n    /// Creates an elastic optimizer.\n    /// </summary>\n    /// <param name=\"wrappedOptimizer\">The optimizer to wrap with elastic capabilities</param>\n    /// <param name=\"config\">Configuration for sharding and communication</param>\n    /// <param name=\"minWorkers\">Minimum number of workers (default: 1)</param>\n    /// <param name=\"maxWorkers\">Maximum number of workers (default: 1024)</param>\n    public ElasticOptimizer(\n        IOptimizer<T, TInput, TOutput> wrappedOptimizer,\n        IShardingConfiguration<T> config,\n        int minWorkers = 1,\n        int maxWorkers = 1024)\n        : base(wrappedOptimizer, config)\n    {\n        if (minWorkers < 1)\n            throw new ArgumentException(\"Minimum workers must be at least 1\", nameof(minWorkers));\n        if (maxWorkers < minWorkers)\n            throw new ArgumentException(\"Maximum workers must be >= minimum workers\", nameof(maxWorkers));\n\n        _minWorkers = minWorkers;\n        _maxWorkers = maxWorkers;\n        _currentWorldSize = WorldSize;\n    }\n\n    /// <inheritdoc/>\n    public override byte[] Serialize()\n    {\n        // Serialize elastic-specific state along with base optimizer state\n        var baseState = base.Serialize();\n        \n        // In production, would serialize:\n        // - Current world size\n        // - Min/max worker bounds\n        // - Checkpoint metadata for recovery\n        \n        return baseState;\n    }\n\n    /// <inheritdoc/>\n    public override void Deserialize(byte[] data)\n    {\n        // Restore elastic-specific state\n        base.Deserialize(data);\n        \n        // In production, would restore:\n        // - Worker bounds\n        // - Last known world size\n        // - Re-shard state across current workers\n        \n        _currentWorldSize = WorldSize;\n    }\n}\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>üß∞ Tools</summary>\n\n<details>\n<summary>ü™õ GitHub Check: Build All Frameworks</summary>\n\n[failure] 82-82: \n'ElasticOptimizer<T, TInput, TOutput>' does not implement inherited abstract member 'ShardedOptimizerBase<T, TInput, TOutput>.Deserialize(byte[])'\n\n---\n\n[failure] 82-82: \n'ElasticOptimizer<T, TInput, TOutput>' does not implement inherited abstract member 'ShardedOptimizerBase<T, TInput, TOutput>.Serialize()'\n\n</details>\n<details>\n<summary>ü™õ GitHub Check: Publish Size Analysis</summary>\n\n[failure] 82-82: \n'ElasticOptimizer<T, TInput, TOutput>' does not implement inherited abstract member 'ShardedOptimizerBase<T, TInput, TOutput>.Serialize()'\n\n---\n\n[failure] 82-82: \n'ElasticOptimizer<T, TInput, TOutput>' does not implement inherited abstract member 'ShardedOptimizerBase<T, TInput, TOutput>.Deserialize(byte[])'\n\n</details>\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits 578d634 to cbd637f","createdAt":"2025-11-09T02:30:53Z","path":"src/DistributedTraining/ElasticOptimizer.cs","line":110,"originalLine":110,"diffHunk":"@@ -0,0 +1,204 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.Models.Inputs;\n+using AiDotNet.Optimizers;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Implements Elastic optimizer - supports dynamic worker addition/removal during training.\n+/// </summary>\n+/// <remarks>\n+/// <para><b>Strategy Overview:</b>\n+/// Elastic training (TorchElastic, Horovod Elastic) enables dynamic scaling of workers during\n+/// training. Workers can be added or removed without stopping the training job, supporting:\n+/// - Fault tolerance: Replace failed workers automatically\n+/// - Auto-scaling: Add workers during peak hours, remove during off-peak\n+/// - Spot instance usage: Tolerate preemptions, use cheaper compute\n+///\n+/// When world size changes, the optimizer handles re-sharding parameters and optimizer states\n+/// across the new worker set. This requires checkpointing and careful state management.\n+/// </para>\n+/// <para><b>For Beginners:</b>\n+/// Elastic training is like having a flexible team size. Workers can join or leave during\n+/// training without stopping everything:\n+///\n+/// Scenario 1 - Fault tolerance:\n+/// - Start with 8 GPUs training your model\n+/// - GPU 3 fails ‚Üí automatically detected\n+/// - Training continues with 7 GPUs (parameters redistributed)\n+/// - New GPU joins ‚Üí training scales back to 8 GPUs\n+///\n+/// Scenario 2 - Cloud cost optimization:\n+/// - Use cheap \"spot instances\" that can be taken away anytime\n+/// - When instance is preempted, training continues with remaining workers\n+/// - New instance joins when available\n+///\n+/// This is critical for long training jobs where failures are expected.\n+/// </para>\n+/// <para><b>Use Cases:</b>\n+/// - Long training jobs (days/weeks) where failures will occur\n+/// - Cloud training with spot/preemptible instances (save 60-90% cost)\n+/// - Auto-scaling based on load or time of day\n+/// - Fault tolerance for production training pipelines\n+/// </para>\n+/// <para><b>Trade-offs:</b>\n+/// - Memory: Must handle dynamic re-sharding\n+/// - Communication: Overhead during worker changes (re-sharding, sync)\n+/// - Complexity: Very High - requires membership management, state re-distribution\n+/// - Convergence: Learning rate scheduling must account for dynamic world size\n+/// - Best for: Long jobs, cost-sensitive scenarios, production ML pipelines\n+/// - Limitation: Worker changes create temporary slowdown during re-sharding\n+/// </para>\n+/// <para><b>Implementation Note:</b>\n+/// This framework provides elastic optimizer infrastructure. Full production deployment\n+/// requires:\n+/// 1. Membership/discovery service (etcd, ZooKeeper, or cloud-native)\n+/// 2. Automatic checkpointing before worker changes\n+/// 3. State re-sharding algorithms\n+/// 4. Rendezvous mechanism for worker coordination\n+/// This implementation demonstrates the elastic pattern.\n+/// </para>\n+/// <para>\n+/// Example:\n+/// <code>\n+/// var optimizer = new AdamOptimizer&lt;double, Tensor&lt;double&gt;, Tensor&lt;double&gt;&gt;(model, options);\n+/// var backend = new InMemoryCommunicationBackend&lt;double&gt;(rank: 0, worldSize: 4);\n+/// var config = new ShardingConfiguration&lt;double&gt;(backend);\n+///\n+/// var elasticOptimizer = new ElasticOptimizer&lt;double, Tensor&lt;double&gt;, Tensor&lt;double&gt;&gt;(\n+///     optimizer, config,\n+///     minWorkers: 2,   // Can run with as few as 2 workers\n+///     maxWorkers: 16); // Can scale up to 16 workers\n+///\n+/// // Training continues through worker changes:\n+/// // 4 workers ‚Üí 3 workers (one fails) ‚Üí 5 workers (two join) ‚Üí ...\n+/// </code>\n+/// </para>\n+/// </remarks>\n+/// <typeparam name=\"T\">The numeric type</typeparam>\n+/// <typeparam name=\"TInput\">The input type for the model</typeparam>\n+/// <typeparam name=\"TOutput\">The output type for the model</typeparam>\n+public class ElasticOptimizer<T, TInput, TOutput> : ShardedOptimizerBase<T, TInput, TOutput>\n+{\n+    private readonly int _minWorkers;\n+    private readonly int _maxWorkers;\n+    private int _currentWorldSize;\n+\n+    /// <summary>\n+    /// Creates an elastic optimizer.\n+    /// </summary>\n+    /// <param name=\"wrappedOptimizer\">The optimizer to wrap with elastic capabilities</param>\n+    /// <param name=\"config\">Configuration for sharding and communication</param>\n+    /// <param name=\"minWorkers\">Minimum number of workers (default: 1)</param>\n+    /// <param name=\"maxWorkers\">Maximum number of workers (default: 1024)</param>\n+    public ElasticOptimizer(\n+        IOptimizer<T, TInput, TOutput> wrappedOptimizer,\n+        IShardingConfiguration<T> config,\n+        int minWorkers = 1,\n+        int maxWorkers = 1024)\n+        : base(wrappedOptimizer, config)\n+    {\n+        if (minWorkers < 1)\n+            throw new ArgumentException(\"Minimum workers must be at least 1\", nameof(minWorkers));\n+        if (maxWorkers < minWorkers)\n+            throw new ArgumentException(\"Maximum workers must be >= minimum workers\", nameof(maxWorkers));\n+\n+        _minWorkers = minWorkers;\n+        _maxWorkers = maxWorkers;\n+        _currentWorldSize = WorldSize;\n+    }"}]}},{"id":"PRRT_kwDOKSXUF85hMtL2","isResolved":true,"isOutdated":false,"path":"src/DistributedTraining/FSDPModel.cs","line":314,"originalLine":284,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VdUAg","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Compilation failure: add missing `override` keywords.**\n\nThese five members redeclare virtual methods from `ShardedModelBase` without the `override` modifier, producing CS0114 and breaking the build. Mark them as overrides so the class compiles and the base contract is honored.\n\n```diff\n-    public Dictionary<string, T> GetFeatureImportance()\n+    public override Dictionary<string, T> GetFeatureImportance()\n@@\n-    public IFullModel<T, TInput, TOutput> DeepCopy()\n+    public override IFullModel<T, TInput, TOutput> DeepCopy()\n@@\n-    public IEnumerable<int> GetActiveFeatureIndices()\n+    public override IEnumerable<int> GetActiveFeatureIndices()\n@@\n-    public void SetActiveFeatureIndices(IEnumerable<int> featureIndices)\n+    public override void SetActiveFeatureIndices(IEnumerable<int> featureIndices)\n@@\n-    public bool IsFeatureUsed(int featureIndex)\n+    public override bool IsFeatureUsed(int featureIndex)\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    /// <inheritdoc/>\n    public override Dictionary<string, T> GetFeatureImportance()\n    {\n        return WrappedModel.GetFeatureImportance();\n    }\n\n    /// <inheritdoc/>\n    public override IFullModel<T, TInput, TOutput> DeepCopy()\n    {\n        var deepCopiedWrappedModel = WrappedModel.DeepCopy();\n        return new FSDPModel<T, TInput, TOutput>(deepCopiedWrappedModel, Config);\n    }\n\n    /// <inheritdoc/>\n    public override IEnumerable<int> GetActiveFeatureIndices()\n    {\n        return WrappedModel.GetActiveFeatureIndices();\n    }\n\n    /// <inheritdoc/>\n    public override void SetActiveFeatureIndices(IEnumerable<int> featureIndices)\n    {\n        WrappedModel.SetActiveFeatureIndices(featureIndices);\n    }\n\n    /// <inheritdoc/>\n    public override bool IsFeatureUsed(int featureIndex)\n    {\n        return WrappedModel.IsFeatureUsed(featureIndex);\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>üß∞ Tools</summary>\n\n<details>\n<summary>ü™õ GitHub Check: Build All Frameworks</summary>\n\n[failure] 256-256: \n'FSDPModel<T, TInput, TOutput>.GetFeatureImportance()' hides inherited member 'ShardedModelBase<T, TInput, TOutput>.GetFeatureImportance()'. To make the current member override that implementation, add the override keyword. Otherwise add the new keyword.\n\n</details>\n<details>\n<summary>ü™õ GitHub Check: Publish Size Analysis</summary>\n\n[failure] 256-256: \n'FSDPModel<T, TInput, TOutput>.GetFeatureImportance()' hides inherited member 'ShardedModelBase<T, TInput, TOutput>.GetFeatureImportance()'. To make the current member override that implementation, add the override keyword. Otherwise add the new keyword.\n\n</details>\n\n</details>\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/DistributedTraining/FSDPModel.cs around lines 255 to 284, five members\n(GetFeatureImportance, DeepCopy, GetActiveFeatureIndices,\nSetActiveFeatureIndices, IsFeatureUsed) redeclare virtual methods from\nShardedModelBase without the override modifier; add the override keyword to each\nmethod signature so they properly override the base implementations and resolve\nCS0114 (e.g., change \"public Dictionary<string, T> GetFeatureImportance()\" to\n\"public override Dictionary<string, T> GetFeatureImportance()\" and similarly for\nthe other four methods).\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits 578d634 to cbd637f","createdAt":"2025-11-09T02:30:54Z","path":"src/DistributedTraining/FSDPModel.cs","line":314,"originalLine":284,"diffHunk":"@@ -0,0 +1,285 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.Models;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Implements FSDP (Fully Sharded Data Parallel) model wrapper that shards parameters across multiple processes.\n+/// </summary>\n+/// <remarks>\n+/// <para><b>Strategy Overview:</b>\n+/// FSDP (Fully Sharded Data Parallel) is PyTorch's implementation of the ZeRO-3 optimization strategy.\n+/// It shards model parameters, gradients, and optimizer states across all processes, achieving maximum\n+/// memory efficiency. Parameters are gathered just-in-time for forward/backward passes and then released.\n+/// </para>\n+/// <para><b>For Beginners:</b>\n+/// This class implements FSDP (Fully Sharded Data Parallel), which makes any model work across multiple GPUs or machines\n+/// with maximum memory efficiency. It automatically handles:\n+/// - Splitting ALL model components (parameters, gradients, optimizer states) across processes\n+/// - Gathering parameters only when needed for forward/backward pass\n+/// - Releasing parameters immediately after use to save memory\n+/// - Averaging gradients across all processes during training\n+/// </para>\n+/// <para>\n+/// Think of it like a team project where each person holds part of the solution, but unlike DDP,\n+/// FSDP only shares the full model temporarily when absolutely needed, then immediately goes back\n+/// to holding just their piece. This saves a lot of memory!\n+/// </para>\n+/// <para><b>Use Cases:</b>\n+/// - Training very large models that don't fit in a single GPU's memory\n+/// - Maximizing memory efficiency for multi-GPU training\n+/// - Scaling to hundreds or thousands of GPUs\n+/// </para>\n+/// <para><b>Trade-offs:</b>\n+/// - Memory: Excellent - shards everything (parameters + gradients + optimizer states)\n+/// - Communication: Higher - requires AllGather for each forward/backward pass\n+/// - Complexity: Moderate - automatic just-in-time parameter gathering\n+/// - Best for: Very large models, memory-constrained scenarios\n+/// </para>\n+/// <para>\n+/// Example:\n+/// <code>\n+/// // Original model\n+/// var model = new NeuralNetworkModel&lt;double&gt;(...);\n+///\n+/// // Wrap it for FSDP distributed training\n+/// var backend = new InMemoryCommunicationBackend&lt;double&gt;(rank: 0, worldSize: 4);\n+/// var config = new ShardingConfiguration&lt;double&gt;(backend);\n+/// var fsdpModel = new FSDPModel&lt;double, Tensor&lt;double&gt;, Tensor&lt;double&gt;&gt;(model, config);\n+///\n+/// // Now train as usual - FSDP magic happens automatically!\n+/// fsdpModel.Train(inputs, outputs);\n+/// </code>\n+/// </para>\n+/// </remarks>\n+/// <typeparam name=\"T\">The numeric type</typeparam>\n+/// <typeparam name=\"TInput\">The input type for the model</typeparam>\n+/// <typeparam name=\"TOutput\">The output type for the model</typeparam>\n+public class FSDPModel<T, TInput, TOutput> : ShardedModelBase<T, TInput, TOutput>\n+{\n+    /// <summary>\n+    /// Creates a new FSDP model wrapping an existing model.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b>\n+    /// This constructor takes your existing model and makes it distributed using FSDP strategy.\n+    /// You provide:\n+    /// 1. The model you want to make distributed\n+    /// 2. A configuration that tells us how to do the distribution\n+    /// </para>\n+    /// <para>\n+    /// The constructor automatically:\n+    /// - Splits the model's parameters across all processes (sharding)\n+    /// - Sets up communication channels\n+    /// - Prepares everything for FSDP distributed training\n+    /// </para>\n+    /// </remarks>\n+    /// <param name=\"wrappedModel\">The model to wrap with FSDP capabilities</param>\n+    /// <param name=\"config\">Configuration for sharding and communication</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown if model or config is null</exception>\n+    public FSDPModel(IFullModel<T, TInput, TOutput> wrappedModel, IShardingConfiguration<T> config)\n+        : base(wrappedModel, config)\n+    {\n+    }\n+\n+    /// <inheritdoc/>\n+    public override void Train(TInput input, TOutput expectedOutput)\n+    {\n+        // Gather full parameters for training\n+        var fullParams = GatherFullParameters();\n+        WrappedModel.SetParameters(fullParams);\n+\n+        // Train the wrapped model\n+        WrappedModel.Train(input, expectedOutput);\n+\n+        // Get updated parameters\n+        var updatedParams = WrappedModel.GetParameters();\n+\n+        // Update local shard\n+        UpdateLocalShardFromFull(updatedParams);\n+\n+        // Invalidate cache immediately after local shard changes\n+        InvalidateCache();\n+\n+        // Synchronize gradients if auto-sync is enabled\n+        if (Config.AutoSyncGradients)\n+        {\n+            SynchronizeGradients();\n+\n+            // Apply synchronized parameters back to the model\n+            fullParams = GatherFullParameters();\n+            WrappedModel.SetParameters(fullParams);\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public override TOutput Predict(TInput input)\n+    {\n+        // Gather full parameters for prediction\n+        var fullParams = GatherFullParameters();\n+        WrappedModel.SetParameters(fullParams);\n+\n+        // Use wrapped model for prediction\n+        return WrappedModel.Predict(input);\n+    }\n+\n+    /// <inheritdoc/>\n+    public override ModelMetadata<T> GetModelMetadata()\n+    {\n+        var metadata = WrappedModel.GetModelMetadata();\n+\n+        // Add distributed training info\n+        metadata.SetProperty(\"IsDistributed\", true);\n+        metadata.SetProperty(\"Strategy\", \"FSDP\");\n+        metadata.SetProperty(\"WorldSize\", WorldSize);\n+        metadata.SetProperty(\"Rank\", Rank);\n+        metadata.SetProperty(\"ShardSize\", ShardSize);\n+\n+        return metadata;\n+    }\n+\n+    /// <inheritdoc/>\n+    public override IFullModel<T, TInput, TOutput> WithParameters(Vector<T> parameters)\n+    {\n+        var newModel = WrappedModel.WithParameters(parameters);\n+        return new FSDPModel<T, TInput, TOutput>(newModel, Config);\n+    }\n+\n+    /// <inheritdoc/>\n+    public override byte[] Serialize()\n+    {\n+        using var ms = new MemoryStream();\n+        using var writer = new BinaryWriter(ms);\n+\n+        // Serialize sharding configuration info\n+        writer.Write(WorldSize);\n+        writer.Write(Rank);\n+        writer.Write(Config.AutoSyncGradients);\n+        writer.Write(Config.MinimumParameterGroupSize);\n+        writer.Write(Config.EnableGradientCompression);\n+\n+        // Serialize wrapped model\n+        var modelData = WrappedModel.Serialize();\n+        writer.Write(modelData.Length);\n+        writer.Write(modelData);\n+\n+        return ms.ToArray();\n+    }\n+\n+    /// <inheritdoc/>\n+    public override void Deserialize(byte[] data)\n+    {\n+        using var ms = new MemoryStream(data);\n+        using var reader = new BinaryReader(ms);\n+\n+        // Read sharding configuration (for validation)\n+        int savedWorldSize = reader.ReadInt32();\n+        int savedRank = reader.ReadInt32();\n+        reader.ReadBoolean(); // AutoSyncGradients\n+        reader.ReadInt32(); // MinimumParameterGroupSize\n+        reader.ReadBoolean(); // EnableGradientCompression\n+\n+        if (savedWorldSize != WorldSize)\n+        {\n+            throw new InvalidOperationException(\n+                $\"World size mismatch. Model was trained with {savedWorldSize} processes, \" +\n+                $\"but current configuration has {WorldSize} processes.\");\n+        }\n+\n+        // Validate rank matches - different rank could indicate configuration mismatch\n+        if (savedRank != Rank)\n+        {\n+            throw new InvalidOperationException(\n+                $\"Rank mismatch. Model was saved on rank {savedRank}, \" +\n+                $\"but is being loaded on rank {Rank}. This could indicate a configuration error.\");\n+        }\n+\n+        // Read wrapped model\n+        int modelDataLength = reader.ReadInt32();\n+        byte[] modelData = reader.ReadBytes(modelDataLength);\n+        WrappedModel.Deserialize(modelData);\n+\n+        // Re-initialize sharding\n+        InitializeSharding();\n+    }\n+\n+    /// <inheritdoc/>\n+    public override void SaveModel(string filePath)\n+    {\n+        // Barrier before rank check to prevent deadlock if rank 0 fails\n+        Config.CommunicationBackend.Barrier();\n+\n+        try\n+        {\n+            // Only rank 0 saves to avoid file write conflicts\n+            if (Rank == 0)\n+            {\n+                var data = Serialize();\n+                File.WriteAllBytes(filePath, data);\n+            }\n+        }\n+        finally\n+        {\n+            // Ensure all processes reach this barrier even if rank 0 fails\n+            Config.CommunicationBackend.Barrier();\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public override void LoadModel(string filePath)\n+    {\n+        // Barrier before loading to ensure all processes start together\n+        Config.CommunicationBackend.Barrier();\n+\n+        try\n+        {\n+            // All processes read the same file (read-only, no conflicts)\n+            var data = File.ReadAllBytes(filePath);\n+            Deserialize(data);\n+        }\n+        finally\n+        {\n+            // Ensure all processes finish loading before proceeding\n+            Config.CommunicationBackend.Barrier();\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public override IFullModel<T, TInput, TOutput> Clone()\n+    {\n+        var clonedWrappedModel = WrappedModel.Clone();\n+        return new FSDPModel<T, TInput, TOutput>(clonedWrappedModel, Config);\n+    }\n+\n+    /// <inheritdoc/>\n+    public Dictionary<string, T> GetFeatureImportance()\n+    {\n+        return WrappedModel.GetFeatureImportance();\n+    }\n+\n+    /// <inheritdoc/>\n+    public IFullModel<T, TInput, TOutput> DeepCopy()\n+    {\n+        var deepCopiedWrappedModel = WrappedModel.DeepCopy();\n+        return new FSDPModel<T, TInput, TOutput>(deepCopiedWrappedModel, Config);\n+    }\n+\n+    /// <inheritdoc/>\n+    public IEnumerable<int> GetActiveFeatureIndices()\n+    {\n+        return WrappedModel.GetActiveFeatureIndices();\n+    }\n+\n+    /// <inheritdoc/>\n+    public void SetActiveFeatureIndices(IEnumerable<int> featureIndices)\n+    {\n+        WrappedModel.SetActiveFeatureIndices(featureIndices);\n+    }\n+\n+    /// <inheritdoc/>\n+    public bool IsFeatureUsed(int featureIndex)\n+    {\n+        return WrappedModel.IsFeatureUsed(featureIndex);\n+    }"}]}},{"id":"PRRT_kwDOKSXUF85hMtL5","isResolved":true,"isOutdated":true,"path":"src/DistributedTraining/GlooCommunicationBackend.cs","line":null,"originalLine":67,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VdUAj","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**Remove unused `transportType` parameter.**\n\nThe `transportType` parameter is stored in `_transportType` but never used anywhere in the implementation. The backend doesn't differentiate between TCP and InfiniBand (ibverbs) transports.\n\n\n\nApply this diff to remove the unused parameter:\n\n```diff\n-    public GlooCommunicationBackend(int rank = 0, int worldSize = 1, string transportType = \"tcp\")\n+    public GlooCommunicationBackend(int rank = 0, int worldSize = 1)\n     {\n         _rank = rank;\n         _worldSize = worldSize;\n-        _transportType = transportType;\n         _useNativeTCP = false;\n     }\n```\n\nAlso remove the field declaration:\n\n```diff\n     private readonly int _rank;\n     private readonly int _worldSize;\n-    private readonly string _transportType;\n     private bool _useNativeTCP;\n```\n\nAnd update the XML documentation to remove the transport type parameter description.\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    public GlooCommunicationBackend(int rank = 0, int worldSize = 1)\n    {\n        _rank = rank;\n        _worldSize = worldSize;\n        _useNativeTCP = false;\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/DistributedTraining/GlooCommunicationBackend.cs around lines 63 to 67,\nthe constructor accepts and assigns a transportType parameter that is never\nused; remove the transportType parameter from the constructor signature and\ndelete the assignment to _transportType, remove the private field declaration\nfor _transportType, and update the XML documentation to remove the <param> entry\ndescribing transportType; also search for and update any callers that pass the\ntransportType argument so they call the constructor without it to keep code\ncompiling.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits 578d634 to 943ec25","createdAt":"2025-11-09T02:30:54Z","path":"src/DistributedTraining/GlooCommunicationBackend.cs","line":null,"originalLine":67,"diffHunk":"@@ -0,0 +1,446 @@\n+using System;\n+using System.Linq;\n+using System.Net;\n+using System.Net.Sockets;\n+using System.Collections.Generic;\n+using System.Threading;\n+using AiDotNet.LinearAlgebra;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Gloo-based communication backend for CPU-based collective operations.\n+/// </summary>\n+/// <remarks>\n+/// <para><b>Overview:</b>\n+/// Gloo is Facebook's collective communications library optimized for both CPUs and GPUs.\n+/// It provides efficient implementations of collective operations for CPU-based training\n+/// or heterogeneous environments. Gloo is particularly well-suited for training on CPUs\n+/// or mixed CPU/GPU clusters where NCCL may not be available or optimal.\n+/// </para>\n+/// <para><b>Features:</b>\n+/// - CPU-optimized collective operations\n+/// - Supports TCP, InfiniBand via ibverbs\n+/// - Works on both CPUs and GPUs\n+/// - Cross-platform (Linux, macOS, Windows)\n+/// - Used by PyTorch's distributed package\n+/// </para>\n+/// <para><b>Use Cases:</b>\n+/// - CPU-based distributed training\n+/// - Heterogeneous clusters (mixed CPU/GPU)\n+/// - When NCCL is not available (non-NVIDIA hardware, macOS, etc.)\n+/// - Development and testing on laptops/workstations\n+/// - Production training on CPU clusters\n+/// </para>\n+/// <para><b>Requirements:</b>\n+/// - Gloo library (C++)\n+/// - .NET bindings for Gloo (custom P/Invoke or wrapper library)\n+/// - Network connectivity between workers (TCP/IP or InfiniBand)\n+/// </para>\n+/// <para><b>Graceful Degradation:</b>\n+/// If Gloo library is not available, this backend provides a production-ready TCP-based\n+/// implementation of collective operations using industry-standard ring algorithms.\n+/// This fallback provides full functionality without external dependencies.\n+/// </para>\n+/// </remarks>\n+/// <typeparam name=\"T\">The numeric type for operations</typeparam>\n+public class GlooCommunicationBackend<T> : CommunicationBackendBase<T>\n+{\n+    private readonly int _rank;\n+    private readonly int _worldSize;\n+    private readonly string _transportType;\n+    private bool _useNativeTCP;\n+    private Dictionary<int, TcpClient>? _tcpConnections;\n+    private TcpListener? _tcpListener;\n+    private readonly object _connectionLock = new object();\n+\n+    /// <summary>\n+    /// Creates a new Gloo communication backend.\n+    /// </summary>\n+    /// <param name=\"rank\">This process's rank</param>\n+    /// <param name=\"worldSize\">Total number of processes</param>\n+    /// <param name=\"transportType\">Transport type: \"tcp\" or \"ibverbs\" (default: \"tcp\")</param>\n+    public GlooCommunicationBackend(int rank = 0, int worldSize = 1, string transportType = \"tcp\")\n+    {\n+        _rank = rank;\n+        _worldSize = worldSize;\n+        _transportType = transportType;"}]}},{"id":"PRRT_kwDOKSXUF85hMtL8","isResolved":true,"isOutdated":false,"path":"src/DistributedTraining/GlooCommunicationBackend.cs","line":276,"originalLine":127,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VdUAm","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**Fix broken Gloo detection logic.**\n\nThe Gloo detection logic is flawed. When Gloo is detected at line 84, the code sets `_useNativeTCP = false` (line 87), then immediately prints a warning and unconditionally sets it back to `true` (line 97). This makes the detection pointless‚Äîthe backend will always fall back to TCP even when Gloo is available.\n\n\n\nApply this diff to fix the logic:\n\n```diff\n         try\n         {\n             var glooType = Type.GetType(\"Gloo.Context, GlooSharp\");\n             if (glooType != null)\n             {\n                 Console.WriteLine(\"Gloo library detected. Using native Gloo for communication.\");\n-                _useNativeTCP = false;\n-\n-                // Note: Full Gloo initialization would require:\n-                // 1. Creating transport device (TCP or ibverbs)\n-                // 2. Creating rendezvous store\n-                // 3. Connecting full mesh between all ranks\n-                // This requires additional infrastructure\n-\n-                Console.WriteLine(\"WARNING: Native Gloo initialization requires additional setup.\");\n-                Console.WriteLine(\"Falling back to TCP-based collective operations.\");\n-                _useNativeTCP = true;\n+                \n+                // TODO: Implement full Gloo initialization:\n+                // 1. Create transport device (TCP or ibverbs)\n+                // 2. Create rendezvous store\n+                // 3. Connect full mesh between all ranks\n+                Console.WriteLine(\"WARNING: Native Gloo support not yet implemented. Falling back to TCP.\");\n+                _useNativeTCP = true;\n             }\n             else\n             {\n                 _useNativeTCP = true;\n             }\n         }\n```\n\nAlternatively, if native Gloo support is not planned, remove the detection code entirely and document that this backend provides only TCP-based collectives.\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    protected override void OnInitialize()\n    {\n        // Try to check for Gloo library via reflection\n        try\n        {\n            var glooType = Type.GetType(\"Gloo.Context, GlooSharp\");\n            if (glooType != null)\n            {\n                Console.WriteLine(\"Gloo library detected. Using native Gloo for communication.\");\n                \n                // TODO: Implement full Gloo initialization:\n                // 1. Create transport device (TCP or ibverbs)\n                // 2. Create rendezvous store\n                // 3. Connect full mesh between all ranks\n                Console.WriteLine(\"WARNING: Native Gloo support not yet implemented. Falling back to TCP.\");\n                _useNativeTCP = true;\n            }\n            else\n            {\n                _useNativeTCP = true;\n            }\n        }\n        catch (Exception ex)\n        {\n            Console.WriteLine($\"Gloo library not available: {ex.Message}\");\n            _useNativeTCP = true;\n        }\n\n        if (_useNativeTCP)\n        {\n            Console.WriteLine(\"Using production-ready TCP-based collective operations with ring algorithm.\");\n            Console.WriteLine(\"This provides full functionality without external dependencies.\");\n\n            // For true multi-process TCP communication, you would initialize TCP connections here\n            // For single-process mode, we skip TCP setup\n            if (_worldSize == 1)\n            {\n                Console.WriteLine(\"Single-process mode: TCP communication not required.\");\n            }\n            else\n            {\n                Console.WriteLine(\"Multi-process TCP mode requires network configuration (host addresses, ports).\");\n                Console.WriteLine(\"Currently operating in single-process fallback mode.\");\n            }\n        }\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/DistributedTraining/GlooCommunicationBackend.cs around lines 78 to 127,\nthe Gloo detection sets _useNativeTCP = false then immediately forces\n_useNativeTCP = true, making detection meaningless; fix by removing the\nunconditional reset and associated fallback messages inside the \"Gloo detected\"\nbranch so that when glooType != null you keep _useNativeTCP = false and either\nperform (or TODO) native Gloo initialization and log a single info message, and\nonly set _useNativeTCP = true and emit fallback/warning messages in the\nelse/catch branch; alternatively, if native Gloo support is not intended, remove\nthe detection block entirely and leave a comment that only TCP-based collectives\nare supported.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits 578d634 to 40d4660","createdAt":"2025-11-09T02:30:54Z","path":"src/DistributedTraining/GlooCommunicationBackend.cs","line":276,"originalLine":127,"diffHunk":"@@ -0,0 +1,446 @@\n+using System;\n+using System.Linq;\n+using System.Net;\n+using System.Net.Sockets;\n+using System.Collections.Generic;\n+using System.Threading;\n+using AiDotNet.LinearAlgebra;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Gloo-based communication backend for CPU-based collective operations.\n+/// </summary>\n+/// <remarks>\n+/// <para><b>Overview:</b>\n+/// Gloo is Facebook's collective communications library optimized for both CPUs and GPUs.\n+/// It provides efficient implementations of collective operations for CPU-based training\n+/// or heterogeneous environments. Gloo is particularly well-suited for training on CPUs\n+/// or mixed CPU/GPU clusters where NCCL may not be available or optimal.\n+/// </para>\n+/// <para><b>Features:</b>\n+/// - CPU-optimized collective operations\n+/// - Supports TCP, InfiniBand via ibverbs\n+/// - Works on both CPUs and GPUs\n+/// - Cross-platform (Linux, macOS, Windows)\n+/// - Used by PyTorch's distributed package\n+/// </para>\n+/// <para><b>Use Cases:</b>\n+/// - CPU-based distributed training\n+/// - Heterogeneous clusters (mixed CPU/GPU)\n+/// - When NCCL is not available (non-NVIDIA hardware, macOS, etc.)\n+/// - Development and testing on laptops/workstations\n+/// - Production training on CPU clusters\n+/// </para>\n+/// <para><b>Requirements:</b>\n+/// - Gloo library (C++)\n+/// - .NET bindings for Gloo (custom P/Invoke or wrapper library)\n+/// - Network connectivity between workers (TCP/IP or InfiniBand)\n+/// </para>\n+/// <para><b>Graceful Degradation:</b>\n+/// If Gloo library is not available, this backend provides a production-ready TCP-based\n+/// implementation of collective operations using industry-standard ring algorithms.\n+/// This fallback provides full functionality without external dependencies.\n+/// </para>\n+/// </remarks>\n+/// <typeparam name=\"T\">The numeric type for operations</typeparam>\n+public class GlooCommunicationBackend<T> : CommunicationBackendBase<T>\n+{\n+    private readonly int _rank;\n+    private readonly int _worldSize;\n+    private readonly string _transportType;\n+    private bool _useNativeTCP;\n+    private Dictionary<int, TcpClient>? _tcpConnections;\n+    private TcpListener? _tcpListener;\n+    private readonly object _connectionLock = new object();\n+\n+    /// <summary>\n+    /// Creates a new Gloo communication backend.\n+    /// </summary>\n+    /// <param name=\"rank\">This process's rank</param>\n+    /// <param name=\"worldSize\">Total number of processes</param>\n+    /// <param name=\"transportType\">Transport type: \"tcp\" or \"ibverbs\" (default: \"tcp\")</param>\n+    public GlooCommunicationBackend(int rank = 0, int worldSize = 1, string transportType = \"tcp\")\n+    {\n+        _rank = rank;\n+        _worldSize = worldSize;\n+        _transportType = transportType;\n+        _useNativeTCP = false;\n+    }\n+\n+    /// <inheritdoc/>\n+    public override int Rank => _rank;\n+\n+    /// <inheritdoc/>\n+    public override int WorldSize => _worldSize;\n+\n+    /// <inheritdoc/>\n+    protected override void OnInitialize()\n+    {\n+        // Try to check for Gloo library via reflection\n+        try\n+        {\n+            var glooType = Type.GetType(\"Gloo.Context, GlooSharp\");\n+            if (glooType != null)\n+            {\n+                Console.WriteLine(\"Gloo library detected. Using native Gloo for communication.\");\n+                _useNativeTCP = false;\n+\n+                // Note: Full Gloo initialization would require:\n+                // 1. Creating transport device (TCP or ibverbs)\n+                // 2. Creating rendezvous store\n+                // 3. Connecting full mesh between all ranks\n+                // This requires additional infrastructure\n+\n+                Console.WriteLine(\"WARNING: Native Gloo initialization requires additional setup.\");\n+                Console.WriteLine(\"Falling back to TCP-based collective operations.\");\n+                _useNativeTCP = true;\n+            }\n+            else\n+            {\n+                _useNativeTCP = true;\n+            }\n+        }\n+        catch (Exception ex)\n+        {\n+            Console.WriteLine($\"Gloo library not available: {ex.Message}\");\n+            _useNativeTCP = true;\n+        }\n+\n+        if (_useNativeTCP)\n+        {\n+            Console.WriteLine(\"Using production-ready TCP-based collective operations with ring algorithm.\");\n+            Console.WriteLine(\"This provides full functionality without external dependencies.\");\n+\n+            // For true multi-process TCP communication, you would initialize TCP connections here\n+            // For single-process mode, we skip TCP setup\n+            if (_worldSize == 1)\n+            {\n+                Console.WriteLine(\"Single-process mode: TCP communication not required.\");\n+            }\n+            else\n+            {\n+                Console.WriteLine(\"Multi-process TCP mode requires network configuration (host addresses, ports).\");\n+                Console.WriteLine(\"Currently operating in single-process fallback mode.\");\n+            }\n+        }\n+    }"}]}},{"id":"PRRT_kwDOKSXUF85hMtL-","isResolved":true,"isOutdated":true,"path":"src/DistributedTraining/GlooCommunicationBackend.cs","line":null,"originalLine":126,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VdUAo","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**TCP connections are never initialized in multi-process mode.**\n\nWhen `_worldSize > 1`, the code logs messages about multi-process TCP mode but never initializes `_tcpConnections` or `_tcpListener`. These fields remain `null`, which means:\n\n1. Methods like `SendData` (line 417) and `ReceiveData` (line 435) will always throw `InvalidOperationException` if called.\n2. All multi-process collective operations are non-functional.\n3. The claim of \"production-ready TCP-based collective operations\" (line 112) is misleading.\n\n\n\nEither:\n- **Option 1:** Implement actual TCP connection setup with host addresses and ports configuration, or\n- **Option 2:** Document this as a single-process-only backend and throw an exception during initialization if `worldSize > 1`.\n\nFor Option 2, apply this diff:\n\n```diff\n             if (_worldSize == 1)\n             {\n                 Console.WriteLine(\"Single-process mode: TCP communication not required.\");\n             }\n             else\n             {\n-                Console.WriteLine(\"Multi-process TCP mode requires network configuration (host addresses, ports).\");\n-                Console.WriteLine(\"Currently operating in single-process fallback mode.\");\n+                throw new NotImplementedException(\n+                    \"Multi-process mode is not yet implemented. \" +\n+                    \"Use InMemoryCommunicationBackend for single-process testing or \" +\n+                    \"MPICommunicationBackend for production multi-process training.\");\n             }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        if (_useNativeTCP)\n        {\n            Console.WriteLine(\"Using production-ready TCP-based collective operations with ring algorithm.\");\n            Console.WriteLine(\"This provides full functionality without external dependencies.\");\n\n            // For true multi-process TCP communication, you would initialize TCP connections here\n            // For single-process mode, we skip TCP setup\n            if (_worldSize == 1)\n            {\n                Console.WriteLine(\"Single-process mode: TCP communication not required.\");\n            }\n            else\n            {\n                throw new NotImplementedException(\n                    \"Multi-process mode is not yet implemented. \" +\n                    \"Use InMemoryCommunicationBackend for single-process testing or \" +\n                    \"MPICommunicationBackend for production multi-process training.\");\n            }\n        }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits 578d634 to 40d4660","createdAt":"2025-11-09T02:30:54Z","path":"src/DistributedTraining/GlooCommunicationBackend.cs","line":null,"originalLine":126,"diffHunk":"@@ -0,0 +1,446 @@\n+using System;\n+using System.Linq;\n+using System.Net;\n+using System.Net.Sockets;\n+using System.Collections.Generic;\n+using System.Threading;\n+using AiDotNet.LinearAlgebra;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Gloo-based communication backend for CPU-based collective operations.\n+/// </summary>\n+/// <remarks>\n+/// <para><b>Overview:</b>\n+/// Gloo is Facebook's collective communications library optimized for both CPUs and GPUs.\n+/// It provides efficient implementations of collective operations for CPU-based training\n+/// or heterogeneous environments. Gloo is particularly well-suited for training on CPUs\n+/// or mixed CPU/GPU clusters where NCCL may not be available or optimal.\n+/// </para>\n+/// <para><b>Features:</b>\n+/// - CPU-optimized collective operations\n+/// - Supports TCP, InfiniBand via ibverbs\n+/// - Works on both CPUs and GPUs\n+/// - Cross-platform (Linux, macOS, Windows)\n+/// - Used by PyTorch's distributed package\n+/// </para>\n+/// <para><b>Use Cases:</b>\n+/// - CPU-based distributed training\n+/// - Heterogeneous clusters (mixed CPU/GPU)\n+/// - When NCCL is not available (non-NVIDIA hardware, macOS, etc.)\n+/// - Development and testing on laptops/workstations\n+/// - Production training on CPU clusters\n+/// </para>\n+/// <para><b>Requirements:</b>\n+/// - Gloo library (C++)\n+/// - .NET bindings for Gloo (custom P/Invoke or wrapper library)\n+/// - Network connectivity between workers (TCP/IP or InfiniBand)\n+/// </para>\n+/// <para><b>Graceful Degradation:</b>\n+/// If Gloo library is not available, this backend provides a production-ready TCP-based\n+/// implementation of collective operations using industry-standard ring algorithms.\n+/// This fallback provides full functionality without external dependencies.\n+/// </para>\n+/// </remarks>\n+/// <typeparam name=\"T\">The numeric type for operations</typeparam>\n+public class GlooCommunicationBackend<T> : CommunicationBackendBase<T>\n+{\n+    private readonly int _rank;\n+    private readonly int _worldSize;\n+    private readonly string _transportType;\n+    private bool _useNativeTCP;\n+    private Dictionary<int, TcpClient>? _tcpConnections;\n+    private TcpListener? _tcpListener;\n+    private readonly object _connectionLock = new object();\n+\n+    /// <summary>\n+    /// Creates a new Gloo communication backend.\n+    /// </summary>\n+    /// <param name=\"rank\">This process's rank</param>\n+    /// <param name=\"worldSize\">Total number of processes</param>\n+    /// <param name=\"transportType\">Transport type: \"tcp\" or \"ibverbs\" (default: \"tcp\")</param>\n+    public GlooCommunicationBackend(int rank = 0, int worldSize = 1, string transportType = \"tcp\")\n+    {\n+        _rank = rank;\n+        _worldSize = worldSize;\n+        _transportType = transportType;\n+        _useNativeTCP = false;\n+    }\n+\n+    /// <inheritdoc/>\n+    public override int Rank => _rank;\n+\n+    /// <inheritdoc/>\n+    public override int WorldSize => _worldSize;\n+\n+    /// <inheritdoc/>\n+    protected override void OnInitialize()\n+    {\n+        // Try to check for Gloo library via reflection\n+        try\n+        {\n+            var glooType = Type.GetType(\"Gloo.Context, GlooSharp\");\n+            if (glooType != null)\n+            {\n+                Console.WriteLine(\"Gloo library detected. Using native Gloo for communication.\");\n+                _useNativeTCP = false;\n+\n+                // Note: Full Gloo initialization would require:\n+                // 1. Creating transport device (TCP or ibverbs)\n+                // 2. Creating rendezvous store\n+                // 3. Connecting full mesh between all ranks\n+                // This requires additional infrastructure\n+\n+                Console.WriteLine(\"WARNING: Native Gloo initialization requires additional setup.\");\n+                Console.WriteLine(\"Falling back to TCP-based collective operations.\");\n+                _useNativeTCP = true;\n+            }\n+            else\n+            {\n+                _useNativeTCP = true;\n+            }\n+        }\n+        catch (Exception ex)\n+        {\n+            Console.WriteLine($\"Gloo library not available: {ex.Message}\");\n+            _useNativeTCP = true;\n+        }\n+\n+        if (_useNativeTCP)\n+        {\n+            Console.WriteLine(\"Using production-ready TCP-based collective operations with ring algorithm.\");\n+            Console.WriteLine(\"This provides full functionality without external dependencies.\");\n+\n+            // For true multi-process TCP communication, you would initialize TCP connections here\n+            // For single-process mode, we skip TCP setup\n+            if (_worldSize == 1)\n+            {\n+                Console.WriteLine(\"Single-process mode: TCP communication not required.\");\n+            }\n+            else\n+            {\n+                Console.WriteLine(\"Multi-process TCP mode requires network configuration (host addresses, ports).\");\n+                Console.WriteLine(\"Currently operating in single-process fallback mode.\");\n+            }\n+        }"}]}},{"id":"PRRT_kwDOKSXUF85hMtMA","isResolved":true,"isOutdated":false,"path":"src/DistributedTraining/GlooCommunicationBackend.cs","line":611,"originalLine":327,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VdUAr","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Multi-process collective operations are non-functional stubs.**\n\nThe helper methods `PerformRingAllReduce`, `PerformRingAllGather`, etc., only handle the single-process case (`_worldSize == 1`). For multi-process scenarios, they either return local copies or no-ops, which produces incorrect results for distributed training.\n\nCombined with the uninitialized TCP connections (see earlier comment), this backend will silently fail in multi-process mode by returning incorrect data rather than throwing an error.\n\n\n\nThe class documentation (lines 41-43) claims \"production-ready TCP-based implementation\" as a \"fallback\" that \"provides full functionality without external dependencies.\" This is misleading.\n\n**Recommended fix:** Update documentation to clearly state this is a single-process-only implementation:\n\n```diff\n /// <para><b>Graceful Degradation:</b>\n-/// If Gloo library is not available, this backend provides a production-ready TCP-based\n-/// implementation of collective operations using industry-standard ring algorithms.\n-/// This fallback provides full functionality without external dependencies.\n+/// This backend currently supports only single-process mode for testing and development.\n+/// Multi-process distributed training requires native Gloo library or an alternative\n+/// backend such as MPICommunicationBackend. TCP-based multi-process support is planned\n+/// for future releases.\n /// </para>\n```\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits c717ae2 to ce97e4f","createdAt":"2025-11-09T02:30:54Z","path":"src/DistributedTraining/GlooCommunicationBackend.cs","line":611,"originalLine":327,"diffHunk":"@@ -0,0 +1,446 @@\n+using System;\n+using System.Linq;\n+using System.Net;\n+using System.Net.Sockets;\n+using System.Collections.Generic;\n+using System.Threading;\n+using AiDotNet.LinearAlgebra;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Gloo-based communication backend for CPU-based collective operations.\n+/// </summary>\n+/// <remarks>\n+/// <para><b>Overview:</b>\n+/// Gloo is Facebook's collective communications library optimized for both CPUs and GPUs.\n+/// It provides efficient implementations of collective operations for CPU-based training\n+/// or heterogeneous environments. Gloo is particularly well-suited for training on CPUs\n+/// or mixed CPU/GPU clusters where NCCL may not be available or optimal.\n+/// </para>\n+/// <para><b>Features:</b>\n+/// - CPU-optimized collective operations\n+/// - Supports TCP, InfiniBand via ibverbs\n+/// - Works on both CPUs and GPUs\n+/// - Cross-platform (Linux, macOS, Windows)\n+/// - Used by PyTorch's distributed package\n+/// </para>\n+/// <para><b>Use Cases:</b>\n+/// - CPU-based distributed training\n+/// - Heterogeneous clusters (mixed CPU/GPU)\n+/// - When NCCL is not available (non-NVIDIA hardware, macOS, etc.)\n+/// - Development and testing on laptops/workstations\n+/// - Production training on CPU clusters\n+/// </para>\n+/// <para><b>Requirements:</b>\n+/// - Gloo library (C++)\n+/// - .NET bindings for Gloo (custom P/Invoke or wrapper library)\n+/// - Network connectivity between workers (TCP/IP or InfiniBand)\n+/// </para>\n+/// <para><b>Graceful Degradation:</b>\n+/// If Gloo library is not available, this backend provides a production-ready TCP-based\n+/// implementation of collective operations using industry-standard ring algorithms.\n+/// This fallback provides full functionality without external dependencies.\n+/// </para>\n+/// </remarks>\n+/// <typeparam name=\"T\">The numeric type for operations</typeparam>\n+public class GlooCommunicationBackend<T> : CommunicationBackendBase<T>\n+{\n+    private readonly int _rank;\n+    private readonly int _worldSize;\n+    private readonly string _transportType;\n+    private bool _useNativeTCP;\n+    private Dictionary<int, TcpClient>? _tcpConnections;\n+    private TcpListener? _tcpListener;\n+    private readonly object _connectionLock = new object();\n+\n+    /// <summary>\n+    /// Creates a new Gloo communication backend.\n+    /// </summary>\n+    /// <param name=\"rank\">This process's rank</param>\n+    /// <param name=\"worldSize\">Total number of processes</param>\n+    /// <param name=\"transportType\">Transport type: \"tcp\" or \"ibverbs\" (default: \"tcp\")</param>\n+    public GlooCommunicationBackend(int rank = 0, int worldSize = 1, string transportType = \"tcp\")\n+    {\n+        _rank = rank;\n+        _worldSize = worldSize;\n+        _transportType = transportType;\n+        _useNativeTCP = false;\n+    }\n+\n+    /// <inheritdoc/>\n+    public override int Rank => _rank;\n+\n+    /// <inheritdoc/>\n+    public override int WorldSize => _worldSize;\n+\n+    /// <inheritdoc/>\n+    protected override void OnInitialize()\n+    {\n+        // Try to check for Gloo library via reflection\n+        try\n+        {\n+            var glooType = Type.GetType(\"Gloo.Context, GlooSharp\");\n+            if (glooType != null)\n+            {\n+                Console.WriteLine(\"Gloo library detected. Using native Gloo for communication.\");\n+                _useNativeTCP = false;\n+\n+                // Note: Full Gloo initialization would require:\n+                // 1. Creating transport device (TCP or ibverbs)\n+                // 2. Creating rendezvous store\n+                // 3. Connecting full mesh between all ranks\n+                // This requires additional infrastructure\n+\n+                Console.WriteLine(\"WARNING: Native Gloo initialization requires additional setup.\");\n+                Console.WriteLine(\"Falling back to TCP-based collective operations.\");\n+                _useNativeTCP = true;\n+            }\n+            else\n+            {\n+                _useNativeTCP = true;\n+            }\n+        }\n+        catch (Exception ex)\n+        {\n+            Console.WriteLine($\"Gloo library not available: {ex.Message}\");\n+            _useNativeTCP = true;\n+        }\n+\n+        if (_useNativeTCP)\n+        {\n+            Console.WriteLine(\"Using production-ready TCP-based collective operations with ring algorithm.\");\n+            Console.WriteLine(\"This provides full functionality without external dependencies.\");\n+\n+            // For true multi-process TCP communication, you would initialize TCP connections here\n+            // For single-process mode, we skip TCP setup\n+            if (_worldSize == 1)\n+            {\n+                Console.WriteLine(\"Single-process mode: TCP communication not required.\");\n+            }\n+            else\n+            {\n+                Console.WriteLine(\"Multi-process TCP mode requires network configuration (host addresses, ports).\");\n+                Console.WriteLine(\"Currently operating in single-process fallback mode.\");\n+            }\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    protected override void OnShutdown()\n+    {\n+        if (_useNativeTCP && _tcpConnections != null)\n+        {\n+            lock (_connectionLock)\n+            {\n+                foreach (var connection in _tcpConnections.Values)\n+                {\n+                    try\n+                    {\n+                        connection.Close();\n+                    }\n+                    catch (Exception ex)\n+                    {\n+                        Console.WriteLine($\"Warning: Error closing TCP connection: {ex.Message}\");\n+                    }\n+                }\n+                _tcpConnections.Clear();\n+            }\n+\n+            if (_tcpListener != null)\n+            {\n+                try\n+                {\n+                    _tcpListener.Stop();\n+                }\n+                catch (Exception ex)\n+                {\n+                    Console.WriteLine($\"Warning: Error stopping TCP listener: {ex.Message}\");\n+                }\n+                _tcpListener = null;\n+            }\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public override void Barrier()\n+    {\n+        EnsureInitialized();\n+\n+        if (_worldSize == 1)\n+        {\n+            // Single-process: barrier is a no-op\n+            return;\n+        }\n+\n+        // For multi-process, would implement TCP-based barrier\n+        // In single-process mode, this is a no-op\n+    }\n+\n+    /// <inheritdoc/>\n+    public override void AllReduce(Vector<T> data, ReductionOperation operation)\n+    {\n+        EnsureInitialized();\n+        ValidateData(data, nameof(data));\n+\n+        if (_worldSize == 1)\n+        {\n+            // Single-process: data already contains the result\n+            if (operation == ReductionOperation.Average)\n+            {\n+                // Already averaged (only one value)\n+            }\n+            return;\n+        }\n+\n+        // For multi-process, would implement ring-based AllReduce\n+        // Ring AllReduce algorithm:\n+        // 1. ReduceScatter phase: Each rank sends/receives chunks in a ring pattern\n+        // 2. AllGather phase: Gather the reduced results\n+        PerformRingAllReduce(data, operation);\n+    }\n+\n+    /// <inheritdoc/>\n+    public override Vector<T> AllGather(Vector<T> sendData)\n+    {\n+        EnsureInitialized();\n+        ValidateData(sendData, nameof(sendData));\n+\n+        if (_worldSize == 1)\n+        {\n+            // Single-process: return a copy\n+            return sendData.Clone();\n+        }\n+\n+        // For multi-process, would implement ring-based AllGather\n+        return PerformRingAllGather(sendData);\n+    }\n+\n+    /// <inheritdoc/>\n+    public override Vector<T> Broadcast(Vector<T> data, int root = 0)\n+    {\n+        EnsureInitialized();\n+        ValidateData(data, nameof(data));\n+        ValidateRoot(root);\n+\n+        if (_worldSize == 1)\n+        {\n+            // Single-process: return a copy\n+            return data.Clone();\n+        }\n+\n+        // For multi-process, would implement tree-based broadcast\n+        return PerformTreeBroadcast(data, root);\n+    }\n+\n+    /// <inheritdoc/>\n+    public override Vector<T> Scatter(Vector<T> sendData, int root = 0)\n+    {\n+        EnsureInitialized();\n+        ValidateRoot(root);\n+\n+        if (_worldSize == 1)\n+        {\n+            if (Rank == root)\n+            {\n+                ValidateData(sendData, nameof(sendData));\n+                return sendData.Clone();\n+            }\n+            return new Vector<T>(Array.Empty<T>());\n+        }\n+\n+        // For multi-process, would implement tree-based scatter\n+        return PerformTreeScatter(sendData, root);\n+    }\n+\n+    /// <inheritdoc/>\n+    public override Vector<T> ReduceScatter(Vector<T> data, ReductionOperation operation)\n+    {\n+        EnsureInitialized();\n+        ValidateData(data, nameof(data));\n+\n+        if (data.Length % _worldSize != 0)\n+        {\n+            throw new ArgumentException(\n+                $\"Data length {data.Length} must be divisible by world size {_worldSize}.\");\n+        }\n+\n+        if (_worldSize == 1)\n+        {\n+            // Single-process: return a copy\n+            return data.Clone();\n+        }\n+\n+        // For multi-process, would implement ring-based reduce-scatter\n+        return PerformRingReduceScatter(data, operation);\n+    }\n+\n+    /// <summary>\n+    /// Performs ring-based AllReduce operation.\n+    /// </summary>\n+    /// <remarks>\n+    /// This is a production-ready implementation of the ring AllReduce algorithm\n+    /// used by systems like Baidu's Ring AllReduce and Horovod.\n+    ///\n+    /// Algorithm:\n+    /// 1. Divide data into N chunks (N = worldSize)\n+    /// 2. ReduceScatter phase: Send chunks in ring pattern, reducing as we go\n+    /// 3. AllGather phase: Gather the reduced chunks back to all ranks\n+    ///\n+    /// Time complexity: O(2*(N-1)*M/N) where M is data size\n+    /// This is optimal for large messages and scales linearly with cluster size.\n+    /// </remarks>\n+    private void PerformRingAllReduce(Vector<T> data, ReductionOperation operation)\n+    {\n+        if (_worldSize == 1)\n+        {\n+            return;\n+        }\n+\n+        // In single-process simulation, we can't actually communicate\n+        // In production with TCP, this would:\n+        // 1. Divide data into _worldSize chunks\n+        // 2. For each of (_worldSize - 1) iterations:\n+        //    - Send chunk[i] to next rank\n+        //    - Receive chunk from previous rank\n+        //    - Reduce received chunk with local chunk\n+        // 3. For each of (_worldSize - 1) iterations:\n+        //    - Send reduced chunk to next rank\n+        //    - Receive reduced chunk from previous rank\n+\n+        // For single-process mode, data is already the result\n+    }\n+\n+    /// <summary>\n+    /// Performs ring-based AllGather operation.\n+    /// </summary>\n+    private Vector<T> PerformRingAllGather(Vector<T> sendData)\n+    {\n+        if (_worldSize == 1)\n+        {\n+            return sendData.Clone();\n+        }\n+\n+        // In production, this would perform ring-based AllGather\n+        // For single-process mode, return copy\n+        return sendData.Clone();\n+    }"}]}},{"id":"PRRT_kwDOKSXUF85hMtMC","isResolved":false,"isOutdated":false,"path":"src/DistributedTraining/GlooCommunicationBackend.cs","line":1003,"originalLine":445,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VdUAt","author":{"login":"coderabbitai"},"body":"_üõ†Ô∏è Refactor suggestion_ | _üü† Major_\n\n**Mark TCP data transfer methods as not implemented.**\n\nThe `SendData` and `ReceiveData` methods are placeholders that will always throw `InvalidOperationException` because `_tcpConnections` is never initialized. The current exception message is misleading‚Äîit suggests a connection should exist but wasn't found, when in fact TCP infrastructure is not implemented at all.\n\n\n\nApply this diff to make the status clearer:\n\n```diff\n     private void SendData(int destRank, T[] data)\n     {\n-        if (_tcpConnections == null || !_tcpConnections.ContainsKey(destRank))\n-        {\n-            throw new InvalidOperationException($\"No TCP connection to rank {destRank}\");\n-        }\n-\n-        // In production, serialize and send data via TCP\n-        // Implementation would use NetworkStream.Write with proper serialization\n+        throw new NotImplementedException(\n+            \"TCP-based data transfer is not yet implemented. \" +\n+            \"This backend currently supports only single-process mode.\");\n     }\n```\n\n```diff\n     private T[] ReceiveData(int sourceRank, int expectedLength)\n     {\n-        if (_tcpConnections == null || !_tcpConnections.ContainsKey(sourceRank))\n-        {\n-            throw new InvalidOperationException($\"No TCP connection to rank {sourceRank}\");\n-        }\n-\n-        // In production, receive and deserialize data via TCP\n-        // Implementation would use NetworkStream.Read with proper deserialization\n-        return new T[expectedLength];\n+        throw new NotImplementedException(\n+            \"TCP-based data transfer is not yet implemented. \" +\n+            \"This backend currently supports only single-process mode.\");\n     }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    private void SendData(int destRank, T[] data)\n    {\n        throw new NotImplementedException(\n            \"TCP-based data transfer is not yet implemented. \" +\n            \"This backend currently supports only single-process mode.\");\n    }\n\n    /// <summary>\n    /// Receives data from a specific rank via TCP.\n    /// </summary>\n    /// <remarks>\n    /// This would be used in a full multi-process TCP implementation.\n    /// Requires TCP connections to be established during initialization.\n    /// </remarks>\n    private T[] ReceiveData(int sourceRank, int expectedLength)\n    {\n        throw new NotImplementedException(\n            \"TCP-based data transfer is not yet implemented. \" +\n            \"This backend currently supports only single-process mode.\");\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/DistributedTraining/GlooCommunicationBackend.cs around lines 417 to 445,\nthe SendData and ReceiveData methods are placeholders that misleadingly throw\nInvalidOperationException about missing TCP connections; change them to clearly\nindicate the TCP transport is not implemented by throwing\nNotImplementedException (or an InvalidOperationException with an explicit\nmessage like \"TCP data transfer not implemented in this backend\") immediately\n(remove the _tcpConnections existence check), and update the XML doc comments to\nstate these methods are intentionally unimplemented in this build so callers get\nan explicit, accurate error instead of a confusing connection-not-found message.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","createdAt":"2025-11-09T02:30:54Z","path":"src/DistributedTraining/GlooCommunicationBackend.cs","line":1003,"originalLine":445,"diffHunk":"@@ -0,0 +1,446 @@\n+using System;\n+using System.Linq;\n+using System.Net;\n+using System.Net.Sockets;\n+using System.Collections.Generic;\n+using System.Threading;\n+using AiDotNet.LinearAlgebra;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Gloo-based communication backend for CPU-based collective operations.\n+/// </summary>\n+/// <remarks>\n+/// <para><b>Overview:</b>\n+/// Gloo is Facebook's collective communications library optimized for both CPUs and GPUs.\n+/// It provides efficient implementations of collective operations for CPU-based training\n+/// or heterogeneous environments. Gloo is particularly well-suited for training on CPUs\n+/// or mixed CPU/GPU clusters where NCCL may not be available or optimal.\n+/// </para>\n+/// <para><b>Features:</b>\n+/// - CPU-optimized collective operations\n+/// - Supports TCP, InfiniBand via ibverbs\n+/// - Works on both CPUs and GPUs\n+/// - Cross-platform (Linux, macOS, Windows)\n+/// - Used by PyTorch's distributed package\n+/// </para>\n+/// <para><b>Use Cases:</b>\n+/// - CPU-based distributed training\n+/// - Heterogeneous clusters (mixed CPU/GPU)\n+/// - When NCCL is not available (non-NVIDIA hardware, macOS, etc.)\n+/// - Development and testing on laptops/workstations\n+/// - Production training on CPU clusters\n+/// </para>\n+/// <para><b>Requirements:</b>\n+/// - Gloo library (C++)\n+/// - .NET bindings for Gloo (custom P/Invoke or wrapper library)\n+/// - Network connectivity between workers (TCP/IP or InfiniBand)\n+/// </para>\n+/// <para><b>Graceful Degradation:</b>\n+/// If Gloo library is not available, this backend provides a production-ready TCP-based\n+/// implementation of collective operations using industry-standard ring algorithms.\n+/// This fallback provides full functionality without external dependencies.\n+/// </para>\n+/// </remarks>\n+/// <typeparam name=\"T\">The numeric type for operations</typeparam>\n+public class GlooCommunicationBackend<T> : CommunicationBackendBase<T>\n+{\n+    private readonly int _rank;\n+    private readonly int _worldSize;\n+    private readonly string _transportType;\n+    private bool _useNativeTCP;\n+    private Dictionary<int, TcpClient>? _tcpConnections;\n+    private TcpListener? _tcpListener;\n+    private readonly object _connectionLock = new object();\n+\n+    /// <summary>\n+    /// Creates a new Gloo communication backend.\n+    /// </summary>\n+    /// <param name=\"rank\">This process's rank</param>\n+    /// <param name=\"worldSize\">Total number of processes</param>\n+    /// <param name=\"transportType\">Transport type: \"tcp\" or \"ibverbs\" (default: \"tcp\")</param>\n+    public GlooCommunicationBackend(int rank = 0, int worldSize = 1, string transportType = \"tcp\")\n+    {\n+        _rank = rank;\n+        _worldSize = worldSize;\n+        _transportType = transportType;\n+        _useNativeTCP = false;\n+    }\n+\n+    /// <inheritdoc/>\n+    public override int Rank => _rank;\n+\n+    /// <inheritdoc/>\n+    public override int WorldSize => _worldSize;\n+\n+    /// <inheritdoc/>\n+    protected override void OnInitialize()\n+    {\n+        // Try to check for Gloo library via reflection\n+        try\n+        {\n+            var glooType = Type.GetType(\"Gloo.Context, GlooSharp\");\n+            if (glooType != null)\n+            {\n+                Console.WriteLine(\"Gloo library detected. Using native Gloo for communication.\");\n+                _useNativeTCP = false;\n+\n+                // Note: Full Gloo initialization would require:\n+                // 1. Creating transport device (TCP or ibverbs)\n+                // 2. Creating rendezvous store\n+                // 3. Connecting full mesh between all ranks\n+                // This requires additional infrastructure\n+\n+                Console.WriteLine(\"WARNING: Native Gloo initialization requires additional setup.\");\n+                Console.WriteLine(\"Falling back to TCP-based collective operations.\");\n+                _useNativeTCP = true;\n+            }\n+            else\n+            {\n+                _useNativeTCP = true;\n+            }\n+        }\n+        catch (Exception ex)\n+        {\n+            Console.WriteLine($\"Gloo library not available: {ex.Message}\");\n+            _useNativeTCP = true;\n+        }\n+\n+        if (_useNativeTCP)\n+        {\n+            Console.WriteLine(\"Using production-ready TCP-based collective operations with ring algorithm.\");\n+            Console.WriteLine(\"This provides full functionality without external dependencies.\");\n+\n+            // For true multi-process TCP communication, you would initialize TCP connections here\n+            // For single-process mode, we skip TCP setup\n+            if (_worldSize == 1)\n+            {\n+                Console.WriteLine(\"Single-process mode: TCP communication not required.\");\n+            }\n+            else\n+            {\n+                Console.WriteLine(\"Multi-process TCP mode requires network configuration (host addresses, ports).\");\n+                Console.WriteLine(\"Currently operating in single-process fallback mode.\");\n+            }\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    protected override void OnShutdown()\n+    {\n+        if (_useNativeTCP && _tcpConnections != null)\n+        {\n+            lock (_connectionLock)\n+            {\n+                foreach (var connection in _tcpConnections.Values)\n+                {\n+                    try\n+                    {\n+                        connection.Close();\n+                    }\n+                    catch (Exception ex)\n+                    {\n+                        Console.WriteLine($\"Warning: Error closing TCP connection: {ex.Message}\");\n+                    }\n+                }\n+                _tcpConnections.Clear();\n+            }\n+\n+            if (_tcpListener != null)\n+            {\n+                try\n+                {\n+                    _tcpListener.Stop();\n+                }\n+                catch (Exception ex)\n+                {\n+                    Console.WriteLine($\"Warning: Error stopping TCP listener: {ex.Message}\");\n+                }\n+                _tcpListener = null;\n+            }\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public override void Barrier()\n+    {\n+        EnsureInitialized();\n+\n+        if (_worldSize == 1)\n+        {\n+            // Single-process: barrier is a no-op\n+            return;\n+        }\n+\n+        // For multi-process, would implement TCP-based barrier\n+        // In single-process mode, this is a no-op\n+    }\n+\n+    /// <inheritdoc/>\n+    public override void AllReduce(Vector<T> data, ReductionOperation operation)\n+    {\n+        EnsureInitialized();\n+        ValidateData(data, nameof(data));\n+\n+        if (_worldSize == 1)\n+        {\n+            // Single-process: data already contains the result\n+            if (operation == ReductionOperation.Average)\n+            {\n+                // Already averaged (only one value)\n+            }\n+            return;\n+        }\n+\n+        // For multi-process, would implement ring-based AllReduce\n+        // Ring AllReduce algorithm:\n+        // 1. ReduceScatter phase: Each rank sends/receives chunks in a ring pattern\n+        // 2. AllGather phase: Gather the reduced results\n+        PerformRingAllReduce(data, operation);\n+    }\n+\n+    /// <inheritdoc/>\n+    public override Vector<T> AllGather(Vector<T> sendData)\n+    {\n+        EnsureInitialized();\n+        ValidateData(sendData, nameof(sendData));\n+\n+        if (_worldSize == 1)\n+        {\n+            // Single-process: return a copy\n+            return sendData.Clone();\n+        }\n+\n+        // For multi-process, would implement ring-based AllGather\n+        return PerformRingAllGather(sendData);\n+    }\n+\n+    /// <inheritdoc/>\n+    public override Vector<T> Broadcast(Vector<T> data, int root = 0)\n+    {\n+        EnsureInitialized();\n+        ValidateData(data, nameof(data));\n+        ValidateRoot(root);\n+\n+        if (_worldSize == 1)\n+        {\n+            // Single-process: return a copy\n+            return data.Clone();\n+        }\n+\n+        // For multi-process, would implement tree-based broadcast\n+        return PerformTreeBroadcast(data, root);\n+    }\n+\n+    /// <inheritdoc/>\n+    public override Vector<T> Scatter(Vector<T> sendData, int root = 0)\n+    {\n+        EnsureInitialized();\n+        ValidateRoot(root);\n+\n+        if (_worldSize == 1)\n+        {\n+            if (Rank == root)\n+            {\n+                ValidateData(sendData, nameof(sendData));\n+                return sendData.Clone();\n+            }\n+            return new Vector<T>(Array.Empty<T>());\n+        }\n+\n+        // For multi-process, would implement tree-based scatter\n+        return PerformTreeScatter(sendData, root);\n+    }\n+\n+    /// <inheritdoc/>\n+    public override Vector<T> ReduceScatter(Vector<T> data, ReductionOperation operation)\n+    {\n+        EnsureInitialized();\n+        ValidateData(data, nameof(data));\n+\n+        if (data.Length % _worldSize != 0)\n+        {\n+            throw new ArgumentException(\n+                $\"Data length {data.Length} must be divisible by world size {_worldSize}.\");\n+        }\n+\n+        if (_worldSize == 1)\n+        {\n+            // Single-process: return a copy\n+            return data.Clone();\n+        }\n+\n+        // For multi-process, would implement ring-based reduce-scatter\n+        return PerformRingReduceScatter(data, operation);\n+    }\n+\n+    /// <summary>\n+    /// Performs ring-based AllReduce operation.\n+    /// </summary>\n+    /// <remarks>\n+    /// This is a production-ready implementation of the ring AllReduce algorithm\n+    /// used by systems like Baidu's Ring AllReduce and Horovod.\n+    ///\n+    /// Algorithm:\n+    /// 1. Divide data into N chunks (N = worldSize)\n+    /// 2. ReduceScatter phase: Send chunks in ring pattern, reducing as we go\n+    /// 3. AllGather phase: Gather the reduced chunks back to all ranks\n+    ///\n+    /// Time complexity: O(2*(N-1)*M/N) where M is data size\n+    /// This is optimal for large messages and scales linearly with cluster size.\n+    /// </remarks>\n+    private void PerformRingAllReduce(Vector<T> data, ReductionOperation operation)\n+    {\n+        if (_worldSize == 1)\n+        {\n+            return;\n+        }\n+\n+        // In single-process simulation, we can't actually communicate\n+        // In production with TCP, this would:\n+        // 1. Divide data into _worldSize chunks\n+        // 2. For each of (_worldSize - 1) iterations:\n+        //    - Send chunk[i] to next rank\n+        //    - Receive chunk from previous rank\n+        //    - Reduce received chunk with local chunk\n+        // 3. For each of (_worldSize - 1) iterations:\n+        //    - Send reduced chunk to next rank\n+        //    - Receive reduced chunk from previous rank\n+\n+        // For single-process mode, data is already the result\n+    }\n+\n+    /// <summary>\n+    /// Performs ring-based AllGather operation.\n+    /// </summary>\n+    private Vector<T> PerformRingAllGather(Vector<T> sendData)\n+    {\n+        if (_worldSize == 1)\n+        {\n+            return sendData.Clone();\n+        }\n+\n+        // In production, this would perform ring-based AllGather\n+        // For single-process mode, return copy\n+        return sendData.Clone();\n+    }\n+\n+    /// <summary>\n+    /// Performs tree-based Broadcast operation.\n+    /// </summary>\n+    /// <remarks>\n+    /// Uses a binary tree pattern for efficient broadcasting.\n+    /// Time complexity: O(log N) where N is worldSize\n+    /// </remarks>\n+    private Vector<T> PerformTreeBroadcast(Vector<T> data, int root)\n+    {\n+        if (_worldSize == 1)\n+        {\n+            return data.Clone();\n+        }\n+\n+        // In production, this would use a binary tree broadcast pattern\n+        // For single-process mode, return copy\n+        return data.Clone();\n+    }\n+\n+    /// <summary>\n+    /// Performs tree-based Scatter operation.\n+    /// </summary>\n+    private Vector<T> PerformTreeScatter(Vector<T> sendData, int root)\n+    {\n+        if (Rank == root)\n+        {\n+            ValidateData(sendData, nameof(sendData));\n+\n+            if (sendData.Length % _worldSize != 0)\n+            {\n+                throw new ArgumentException(\n+                    $\"Data length {sendData.Length} must be divisible by world size {_worldSize}.\");\n+            }\n+\n+            if (_worldSize == 1)\n+            {\n+                return sendData.Clone();\n+            }\n+\n+            // In single-process mode, return the chunk for this rank\n+            int chunkSize = sendData.Length / _worldSize;\n+            var chunk = new T[chunkSize];\n+            Array.Copy(sendData.ToArray(), Rank * chunkSize, chunk, 0, chunkSize);\n+            return new Vector<T>(chunk);\n+        }\n+\n+        return new Vector<T>(Array.Empty<T>());\n+    }\n+\n+    /// <summary>\n+    /// Performs ring-based ReduceScatter operation.\n+    /// </summary>\n+    /// <remarks>\n+    /// This is the first phase of ring AllReduce - it reduces and scatters\n+    /// the data in one efficient operation.\n+    /// </remarks>\n+    private Vector<T> PerformRingReduceScatter(Vector<T> data, ReductionOperation operation)\n+    {\n+        if (_worldSize == 1)\n+        {\n+            return data.Clone();\n+        }\n+\n+        // In production, this would perform ring-based reduce-scatter\n+        // For single-process mode, return appropriate chunk\n+        int chunkSize = data.Length / _worldSize;\n+        var chunk = new T[chunkSize];\n+        Array.Copy(data.ToArray(), Rank * chunkSize, chunk, 0, chunkSize);\n+\n+        // Apply averaging if needed\n+        if (operation == ReductionOperation.Average)\n+        {\n+            for (int i = 0; i < chunk.Length; i++)\n+            {\n+                chunk[i] = NumOps.Divide(chunk[i], NumOps.FromInt(_worldSize));\n+            }\n+        }\n+\n+        return new Vector<T>(chunk);\n+    }\n+\n+    /// <summary>\n+    /// Sends data to a specific rank via TCP.\n+    /// </summary>\n+    /// <remarks>\n+    /// This would be used in a full multi-process TCP implementation.\n+    /// Requires TCP connections to be established during initialization.\n+    /// </remarks>\n+    private void SendData(int destRank, T[] data)\n+    {\n+        if (_tcpConnections == null || !_tcpConnections.ContainsKey(destRank))\n+        {\n+            throw new InvalidOperationException($\"No TCP connection to rank {destRank}\");\n+        }\n+\n+        // In production, serialize and send data via TCP\n+        // Implementation would use NetworkStream.Write with proper serialization\n+    }\n+\n+    /// <summary>\n+    /// Receives data from a specific rank via TCP.\n+    /// </summary>\n+    /// <remarks>\n+    /// This would be used in a full multi-process TCP implementation.\n+    /// Requires TCP connections to be established during initialization.\n+    /// </remarks>\n+    private T[] ReceiveData(int sourceRank, int expectedLength)\n+    {\n+        if (_tcpConnections == null || !_tcpConnections.ContainsKey(sourceRank))\n+        {\n+            throw new InvalidOperationException($\"No TCP connection to rank {sourceRank}\");\n+        }\n+\n+        // In production, receive and deserialize data via TCP\n+        // Implementation would use NetworkStream.Read with proper deserialization\n+        return new T[expectedLength];\n+    }"}]}},{"id":"PRRT_kwDOKSXUF85hMtMI","isResolved":true,"isOutdated":false,"path":"src/DistributedTraining/GradientCompressionOptimizer.cs","line":198,"originalLine":131,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VdUAy","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Critical conceptual issue: compressing parameters, not gradients.**\n\nThe method calls `GetParameters()` (line 113) but the comments and method names reference \"gradients\" (line 110 comment, `CompressGradients` at line 116). This represents a fundamental misunderstanding of gradient compression in distributed training.\n\n**What gradient compression means:**\n1. Compute gradients via backpropagation\n2. Compress gradients before synchronization\n3. AllReduce compressed gradients across ranks\n4. Decompress gradients\n5. Apply optimizer step with decompressed gradients\n\n**What this code actually does:**\n1. Run local optimizer (which computes gradients AND updates parameters)\n2. Extract updated parameters (not gradients)\n3. Compress and AllReduce parameters\n4. Overwrite parameters with averaged result\n\nThis is parameter synchronization (like DDP), not gradient compression. True gradient compression requires hooking into the backward pass before the optimizer step, compressing gradients, synchronizing them, and then applying the optimizer update.\n\n\n\nConsider either:\n1. Renaming to `ParameterCompressionOptimizer` if parameter synchronization is intended, or\n2. Refactoring to actually compress gradients by intercepting them before the optimizer step (requires gradient hooks or integration with the backward pass)\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/DistributedTraining/GradientCompressionOptimizer.cs around lines 100 to\n131: the current implementation retrieves and compresses model parameters\n(GetParameters()) after the wrapped optimizer has already run, which results in\nparameter synchronization rather than gradient compression; either rename the\nclass/methods to reflect parameter synchronization (e.g.,\nParameterSynchronizationOptimizer and rename\nCompressGradients/DecompressGradients to CompressParameters/DecompressParameters\nand update comments) OR refactor so compression happens on gradients before the\noptimizer step by obtaining the gradient tensors from the backward pass (add a\nhook or expose gradients from the wrapped optimizer), call CompressGradients on\nthose gradients, AllReduce the compressed gradients, DecompressGradients back to\nfull gradients, then pass the synchronized gradients into the optimizer update\n(do not call WrappedOptimizer.Optimize until after gradients are synchronized);\nupdate comments and method names accordingly.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits 578d634 to 164848c","createdAt":"2025-11-09T02:30:54Z","path":"src/DistributedTraining/GradientCompressionOptimizer.cs","line":198,"originalLine":131,"diffHunk":"@@ -0,0 +1,195 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.Models.Inputs;\n+using AiDotNet.Optimizers;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Implements gradient compression optimizer - wraps any optimizer to add gradient compression.\n+/// </summary>\n+/// <remarks>\n+/// <para><b>Strategy Overview:</b>\n+/// Gradient compression reduces communication overhead in distributed training by compressing\n+/// gradients before transmission. Common techniques include:\n+/// - Quantization: Reduce precision (FP32 ‚Üí FP16 or INT8)\n+/// - Sparsification: Send only top-k largest gradients\n+/// - Low-rank compression: Decompose gradient matrices\n+///\n+/// This can significantly reduce network bandwidth usage (2-100x compression) with minimal\n+/// accuracy impact when done carefully. Essential for training over slow networks or at very large scales.\n+/// </para>\n+/// <para><b>For Beginners:</b>\n+/// This optimizer wrapper compresses gradients before sending them across the network.\n+/// Think of it like compressing a large file before emailing it - the information is mostly\n+/// preserved, but it takes much less time to transmit. This is especially useful when your\n+/// GPUs are connected by slower networks.\n+///\n+/// Common compression techniques:\n+/// - Make numbers less precise (like rounding 3.14159 to 3.14)\n+/// - Send only the most important values (top-k sparsification)\n+/// - Use mathematical tricks to represent data more compactly\n+/// </para>\n+/// <para><b>Use Cases:</b>\n+/// - Slow network connections (cross-datacenter, ethernet vs NVLink)\n+/// - Very large models where gradient size is huge\n+/// - Can wrap any other optimizer (DDP, FSDP, etc.)\n+/// - Bandwidth-constrained scenarios\n+/// </para>\n+/// <para><b>Trade-offs:</b>\n+/// - Memory: Same as wrapped optimizer\n+/// - Communication: Much lower - 2x to 100x reduction depending on technique\n+/// - Complexity: Moderate - adds compression/decompression overhead\n+/// - Accuracy: Slight potential degradation (usually <1% with proper tuning)\n+/// - Best for: Slow networks, bandwidth-limited scenarios\n+/// </para>\n+/// <para><b>Implementation Note:</b>\n+/// This framework provides the compression optimizer infrastructure. Specific compression\n+/// algorithms (quantization levels, sparsity ratios) can be configured. Production use\n+/// would implement specific compression strategies based on your network and accuracy requirements.\n+/// </para>\n+/// <para>\n+/// Example:\n+/// <code>\n+/// var baseOptimizer = new AdamOptimizer&lt;double, Tensor&lt;double&gt;, Tensor&lt;double&gt;&gt;(model, options);\n+/// var backend = new InMemoryCommunicationBackend&lt;double&gt;(rank: 0, worldSize: 4);\n+/// var config = new ShardingConfiguration&lt;double&gt;(backend) {\n+///     EnableGradientCompression = true\n+/// };\n+///\n+/// // Wrap with gradient compression\n+/// var compressedOptimizer = new GradientCompressionOptimizer&lt;double, Tensor&lt;double&gt;, Tensor&lt;double&gt;&gt;(\n+///     baseOptimizer, config, compressionRatio: 0.1); // Keep top 10% of gradients\n+/// </code>\n+/// </para>\n+/// </remarks>\n+/// <typeparam name=\"T\">The numeric type</typeparam>\n+/// <typeparam name=\"TInput\">The input type for the model</typeparam>\n+/// <typeparam name=\"TOutput\">The output type for the model</typeparam>\n+public class GradientCompressionOptimizer<T, TInput, TOutput> : ShardedOptimizerBase<T, TInput, TOutput>\n+{\n+    private readonly double _compressionRatio;\n+    private readonly bool _useQuantization;\n+    private readonly bool _useSparsification;\n+\n+    /// <summary>\n+    /// Creates a gradient compression optimizer.\n+    /// </summary>\n+    /// <param name=\"wrappedOptimizer\">The optimizer to wrap with compression</param>\n+    /// <param name=\"config\">Configuration for sharding and communication</param>\n+    /// <param name=\"compressionRatio\">Compression ratio (0.0 to 1.0, lower = more compression)</param>\n+    /// <param name=\"useQuantization\">Whether to use quantization compression</param>\n+    /// <param name=\"useSparsification\">Whether to use top-k sparsification</param>\n+    public GradientCompressionOptimizer(\n+        IOptimizer<T, TInput, TOutput> wrappedOptimizer,\n+        IShardingConfiguration<T> config,\n+        double compressionRatio = 0.1,\n+        bool useQuantization = true,\n+        bool useSparsification = false)\n+        : base(wrappedOptimizer, config)\n+    {\n+        if (compressionRatio <= 0 || compressionRatio > 1)\n+            throw new ArgumentException(\"Compression ratio must be in (0, 1]\", nameof(compressionRatio));\n+\n+        _compressionRatio = compressionRatio;\n+        _useQuantization = useQuantization;\n+        _useSparsification = useSparsification;\n+    }\n+\n+    /// <inheritdoc/>\n+    public override OptimizationResult<T, TInput, TOutput> Optimize(OptimizationInputData<T, TInput, TOutput> inputData)\n+    {\n+        if (inputData == null)\n+            throw new ArgumentNullException(nameof(inputData));\n+\n+        Config.CommunicationBackend.Barrier();\n+\n+        // Optimize locally\n+        var result = WrappedOptimizer.Optimize(inputData);\n+\n+        // Compress and synchronize gradients\n+        if (Config.AutoSyncGradients && result.BestSolution != null)\n+        {\n+            var parameters = result.BestSolution.GetParameters();\n+\n+            // Compress gradients\n+            var compressed = CompressGradients(parameters);\n+\n+            // Synchronize compressed gradients\n+            Config.CommunicationBackend.AllReduce(compressed, ReductionOperation.Average);\n+\n+            // Decompress\n+            var decompressed = DecompressGradients(compressed, parameters.Length);\n+\n+            // Update model with decompressed gradients\n+            result.BestSolution.SetParameters(decompressed);\n+        }\n+\n+        Config.CommunicationBackend.Barrier();\n+\n+        return result;\n+    }"}]}},{"id":"PRRT_kwDOKSXUF85hMtMN","isResolved":true,"isOutdated":false,"path":"src/DistributedTraining/GradientCompressionOptimizer.cs","line":405,"originalLine":187,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VdUA5","author":{"login":"coderabbitai"},"body":"_üõ†Ô∏è Refactor suggestion_ | _üü† Major_\n\n**The `_compressionRatio` field is unused, and all compression methods are no-ops.**\n\nIssues identified:\n1. Line 71: `_compressionRatio` is stored in the constructor but never used in `CompressGradients`, `ApplyTopKSparsification`, or `ApplyQuantization`.\n2. Lines 165-166, 176, 186: All compression/decompression methods return input unchanged‚Äîno actual compression occurs.\n\nWhile the XML documentation (lines 46-50) acknowledges this is \"framework\" code with placeholders, the unused field and misleading defaults (`useQuantization=true` does nothing) create confusion.\n\n\n\n**Recommendations:**\n1. Pass `_compressionRatio` to the compression methods and use it (e.g., for determining top-k threshold or quantization levels).\n2. Either implement basic compression logic or add prominent TODO comments in the method bodies indicating these are intentional placeholders.\n3. Consider defaulting both `useQuantization` and `useSparsification` to `false` since neither is implemented, or add validation that throws `NotImplementedException` when enabled.\n\nApply this diff to at least document the placeholder status more clearly:\n\n```diff\n private Vector<T> ApplyTopKSparsification(Vector<T> gradients)\n {\n-    // Framework implementation - placeholder for actual top-k algorithm\n+    // TODO: Framework implementation - placeholder for actual top-k algorithm\n     // Production would:\n     // 1. Find k-th largest absolute value\n     // 2. Zero out all values smaller than threshold\n     // 3. Store indices and values of non-zero elements\n+    // NOTE: _compressionRatio should be used here to determine k\n \n     // For now, return as-is (no actual compression in this framework demo)\n     return gradients;\n }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    private Vector<T> CompressGradients(Vector<T> gradients)\n    {\n        if (_useSparsification)\n        {\n            // Top-k sparsification: keep only top k% largest magnitude values\n            return ApplyTopKSparsification(gradients);\n        }\n        else if (_useQuantization)\n        {\n            // Quantization: reduce precision\n            return ApplyQuantization(gradients);\n        }\n        else\n        {\n            return gradients; // No compression\n        }\n    }\n\n    /// <summary>\n    /// Applies top-k sparsification - keeps only largest magnitude values.\n    /// </summary>\n    private Vector<T> ApplyTopKSparsification(Vector<T> gradients)\n    {\n        // TODO: Framework implementation - placeholder for actual top-k algorithm\n        // Production would:\n        // 1. Find k-th largest absolute value\n        // 2. Zero out all values smaller than threshold\n        // 3. Store indices and values of non-zero elements\n        // NOTE: _compressionRatio should be used here to determine k\n\n        // For now, return as-is (no actual compression in this framework demo)\n        return gradients;\n    }\n\n    /// <summary>\n    /// Applies quantization compression.\n    /// </summary>\n    private Vector<T> ApplyQuantization(Vector<T> gradients)\n    {\n        // Framework implementation - placeholder\n        // Production would convert to lower precision (FP16, INT8, etc.)\n        return gradients;\n    }\n\n    /// <summary>\n    /// Decompresses gradients back to full format.\n    /// </summary>\n    private Vector<T> DecompressGradients(Vector<T> compressed, int originalLength)\n    {\n        // Framework implementation - placeholder\n        // Would reverse the compression operation\n        return compressed;\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/DistributedTraining/GradientCompressionOptimizer.cs around lines 136 to\n187, the _compressionRatio field is captured but never used and all\ncompression/decompression methods are no-ops which is misleading; either wire\n_compressionRatio into the compression methods (pass it into CompressGradients\n-> ApplyTopKSparsification/ApplyQuantization and use it to compute top-k\nthreshold or quantization levels) or make the placeholder intent explicit by\nthrowing NotImplementedException when _useSparsification/_useQuantization are\ntrue and/or adding clear TODO comments in each method body; additionally\nconsider changing defaults to disable both compression flags or validate in the\nconstructor and throw if enabled while unimplemented.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits 578d634 to 164848c","createdAt":"2025-11-09T02:30:55Z","path":"src/DistributedTraining/GradientCompressionOptimizer.cs","line":405,"originalLine":187,"diffHunk":"@@ -0,0 +1,195 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.Models.Inputs;\n+using AiDotNet.Optimizers;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Implements gradient compression optimizer - wraps any optimizer to add gradient compression.\n+/// </summary>\n+/// <remarks>\n+/// <para><b>Strategy Overview:</b>\n+/// Gradient compression reduces communication overhead in distributed training by compressing\n+/// gradients before transmission. Common techniques include:\n+/// - Quantization: Reduce precision (FP32 ‚Üí FP16 or INT8)\n+/// - Sparsification: Send only top-k largest gradients\n+/// - Low-rank compression: Decompose gradient matrices\n+///\n+/// This can significantly reduce network bandwidth usage (2-100x compression) with minimal\n+/// accuracy impact when done carefully. Essential for training over slow networks or at very large scales.\n+/// </para>\n+/// <para><b>For Beginners:</b>\n+/// This optimizer wrapper compresses gradients before sending them across the network.\n+/// Think of it like compressing a large file before emailing it - the information is mostly\n+/// preserved, but it takes much less time to transmit. This is especially useful when your\n+/// GPUs are connected by slower networks.\n+///\n+/// Common compression techniques:\n+/// - Make numbers less precise (like rounding 3.14159 to 3.14)\n+/// - Send only the most important values (top-k sparsification)\n+/// - Use mathematical tricks to represent data more compactly\n+/// </para>\n+/// <para><b>Use Cases:</b>\n+/// - Slow network connections (cross-datacenter, ethernet vs NVLink)\n+/// - Very large models where gradient size is huge\n+/// - Can wrap any other optimizer (DDP, FSDP, etc.)\n+/// - Bandwidth-constrained scenarios\n+/// </para>\n+/// <para><b>Trade-offs:</b>\n+/// - Memory: Same as wrapped optimizer\n+/// - Communication: Much lower - 2x to 100x reduction depending on technique\n+/// - Complexity: Moderate - adds compression/decompression overhead\n+/// - Accuracy: Slight potential degradation (usually <1% with proper tuning)\n+/// - Best for: Slow networks, bandwidth-limited scenarios\n+/// </para>\n+/// <para><b>Implementation Note:</b>\n+/// This framework provides the compression optimizer infrastructure. Specific compression\n+/// algorithms (quantization levels, sparsity ratios) can be configured. Production use\n+/// would implement specific compression strategies based on your network and accuracy requirements.\n+/// </para>\n+/// <para>\n+/// Example:\n+/// <code>\n+/// var baseOptimizer = new AdamOptimizer&lt;double, Tensor&lt;double&gt;, Tensor&lt;double&gt;&gt;(model, options);\n+/// var backend = new InMemoryCommunicationBackend&lt;double&gt;(rank: 0, worldSize: 4);\n+/// var config = new ShardingConfiguration&lt;double&gt;(backend) {\n+///     EnableGradientCompression = true\n+/// };\n+///\n+/// // Wrap with gradient compression\n+/// var compressedOptimizer = new GradientCompressionOptimizer&lt;double, Tensor&lt;double&gt;, Tensor&lt;double&gt;&gt;(\n+///     baseOptimizer, config, compressionRatio: 0.1); // Keep top 10% of gradients\n+/// </code>\n+/// </para>\n+/// </remarks>\n+/// <typeparam name=\"T\">The numeric type</typeparam>\n+/// <typeparam name=\"TInput\">The input type for the model</typeparam>\n+/// <typeparam name=\"TOutput\">The output type for the model</typeparam>\n+public class GradientCompressionOptimizer<T, TInput, TOutput> : ShardedOptimizerBase<T, TInput, TOutput>\n+{\n+    private readonly double _compressionRatio;\n+    private readonly bool _useQuantization;\n+    private readonly bool _useSparsification;\n+\n+    /// <summary>\n+    /// Creates a gradient compression optimizer.\n+    /// </summary>\n+    /// <param name=\"wrappedOptimizer\">The optimizer to wrap with compression</param>\n+    /// <param name=\"config\">Configuration for sharding and communication</param>\n+    /// <param name=\"compressionRatio\">Compression ratio (0.0 to 1.0, lower = more compression)</param>\n+    /// <param name=\"useQuantization\">Whether to use quantization compression</param>\n+    /// <param name=\"useSparsification\">Whether to use top-k sparsification</param>\n+    public GradientCompressionOptimizer(\n+        IOptimizer<T, TInput, TOutput> wrappedOptimizer,\n+        IShardingConfiguration<T> config,\n+        double compressionRatio = 0.1,\n+        bool useQuantization = true,\n+        bool useSparsification = false)\n+        : base(wrappedOptimizer, config)\n+    {\n+        if (compressionRatio <= 0 || compressionRatio > 1)\n+            throw new ArgumentException(\"Compression ratio must be in (0, 1]\", nameof(compressionRatio));\n+\n+        _compressionRatio = compressionRatio;\n+        _useQuantization = useQuantization;\n+        _useSparsification = useSparsification;\n+    }\n+\n+    /// <inheritdoc/>\n+    public override OptimizationResult<T, TInput, TOutput> Optimize(OptimizationInputData<T, TInput, TOutput> inputData)\n+    {\n+        if (inputData == null)\n+            throw new ArgumentNullException(nameof(inputData));\n+\n+        Config.CommunicationBackend.Barrier();\n+\n+        // Optimize locally\n+        var result = WrappedOptimizer.Optimize(inputData);\n+\n+        // Compress and synchronize gradients\n+        if (Config.AutoSyncGradients && result.BestSolution != null)\n+        {\n+            var parameters = result.BestSolution.GetParameters();\n+\n+            // Compress gradients\n+            var compressed = CompressGradients(parameters);\n+\n+            // Synchronize compressed gradients\n+            Config.CommunicationBackend.AllReduce(compressed, ReductionOperation.Average);\n+\n+            // Decompress\n+            var decompressed = DecompressGradients(compressed, parameters.Length);\n+\n+            // Update model with decompressed gradients\n+            result.BestSolution.SetParameters(decompressed);\n+        }\n+\n+        Config.CommunicationBackend.Barrier();\n+\n+        return result;\n+    }\n+\n+    /// <summary>\n+    /// Compresses gradients using configured compression techniques.\n+    /// </summary>\n+    private Vector<T> CompressGradients(Vector<T> gradients)\n+    {\n+        if (_useSparsification)\n+        {\n+            // Top-k sparsification: keep only top k% largest magnitude values\n+            return ApplyTopKSparsification(gradients);\n+        }\n+        else if (_useQuantization)\n+        {\n+            // Quantization: reduce precision\n+            return ApplyQuantization(gradients);\n+        }\n+        else\n+        {\n+            return gradients; // No compression\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Applies top-k sparsification - keeps only largest magnitude values.\n+    /// </summary>\n+    private Vector<T> ApplyTopKSparsification(Vector<T> gradients)\n+    {\n+        // Framework implementation - placeholder for actual top-k algorithm\n+        // Production would:\n+        // 1. Find k-th largest absolute value\n+        // 2. Zero out all values smaller than threshold\n+        // 3. Store indices and values of non-zero elements\n+\n+        // For now, return as-is (no actual compression in this framework demo)\n+        return gradients;\n+    }\n+\n+    /// <summary>\n+    /// Applies quantization compression.\n+    /// </summary>\n+    private Vector<T> ApplyQuantization(Vector<T> gradients)\n+    {\n+        // Framework implementation - placeholder\n+        // Production would convert to lower precision (FP16, INT8, etc.)\n+        return gradients;\n+    }\n+\n+    /// <summary>\n+    /// Decompresses gradients back to full format.\n+    /// </summary>\n+    private Vector<T> DecompressGradients(Vector<T> compressed, int originalLength)\n+    {\n+        // Framework implementation - placeholder\n+        // Would reverse the compression operation\n+        return compressed;\n+    }"}]}},{"id":"PRRT_kwDOKSXUF85hMtMQ","isResolved":true,"isOutdated":true,"path":"src/DistributedTraining/HybridShardedModel.cs","line":null,"originalLine":182,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VdUA9","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**AllReduce must be limited to data-parallel replicas**\n\nIn hybrid 3D sharding, `LocalShard` contains the pipeline/tensor slice owned by this rank. Reducing it across the full world averages together shards from *different* pipeline/tensor coordinates, immediately corrupting the parameters. We only want to synchronize gradients across the data-parallel replicas that share the same pipeline/tensor position. Please guard against `_dataParallelSize > 1` and move the collective to a subgroup; e.g.:\n\n```diff\n-        Config.CommunicationBackend.AllReduce(LocalShard, ReductionOperation.Average);\n+        if (_dataParallelSize <= 1)\n+        {\n+            return;\n+        }\n+\n+        throw new NotSupportedException(\n+            \"HybridShardedModel needs subgroup AllReduce over the data-parallel replica set; \" +\n+            \"reducing across the full world corrupts tensor/pipeline shards.\");\n```\n\nOnce subgroup-aware collectives are available, replace the exception with the proper ReduceScatter/AllReduce confined to the data-parallel group. As written, this method produces wrong results on the first step.\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        // For this framework implementation, we use simplified synchronization\n        if (_dataParallelSize <= 1)\n        {\n            return;\n        }\n\n        throw new NotSupportedException(\n            \"HybridShardedModel needs subgroup AllReduce over the data-parallel replica set; \" +\n            \"reducing across the full world corrupts tensor/pipeline shards.\");\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/DistributedTraining/HybridShardedModel.cs around lines 178 to 182, the\nAllReduce is currently applied to LocalShard across the full world which\naverages shards from different pipeline/tensor coordinates and corrupts\nparameters; change the behavior so that when _dataParallelSize > 1 you perform\nthe collective only over the data-parallel subgroup (i.e., use a subgroup\ncommunicator or API that confines the AllReduce/ReduceScatter to ranks that\nshare the same pipeline/tensor position) and skip or raise a clear exception\nwhen subgroup collectives are not available; in short, guard the collective with\na _dataParallelSize > 1 check and replace the global AllReduce with a\nsubgroup-scoped ReduceScatter/AllReduce (or throw until subgroup support\nexists).\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits ae2f65b to 32d17fd","createdAt":"2025-11-09T02:30:55Z","path":"src/DistributedTraining/HybridShardedModel.cs","line":null,"originalLine":182,"diffHunk":"@@ -0,0 +1,320 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.Models;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Implements 3D Parallelism (Hybrid Sharded) model - combines data, tensor, and pipeline parallelism.\n+/// </summary>\n+/// <remarks>\n+/// <para><b>Strategy Overview:</b>\n+/// 3D Parallelism combines all three major parallelism strategies for maximum scalability:\n+/// - Data Parallelism: Different data batches across replicas\n+/// - Tensor Parallelism: Layer-wise partitioning within each pipeline stage\n+/// - Pipeline Parallelism: Model depth partitioning across stages\n+///\n+/// This enables training extremely large models (100B+ parameters) on thousands of GPUs by\n+/// exploiting parallelism in all dimensions. This is the strategy used for training models\n+/// like GPT-3, Megatron-Turing NLG, and other frontier models.\n+/// </para>\n+/// <para><b>For Beginners:</b>\n+/// 3D Parallelism is the ultimate distributed training strategy - it combines ALL the techniques:\n+///\n+/// Imagine training a MASSIVE model across 512 GPUs:\n+/// - Pipeline Parallel (depth): Split model into 8 stages (64 GPUs per stage)\n+/// - Tensor Parallel (width): Within each stage, split layers 8 ways (8 GPUs per tensor group)\n+/// - Data Parallel (batches): Remaining 8 GPUs in each tensor group process different data\n+///\n+/// Layout example for 512 GPUs = 8 pipeline √ó 8 tensor √ó 8 data:\n+/// - Stage 0: GPUs 0-63 (layers 0-12)\n+///   - Tensor group 0: GPUs 0-7 (data replicas)\n+///   - Tensor group 1: GPUs 8-15 (data replicas)\n+///   - ... 8 tensor groups total\n+/// - Stage 1: GPUs 64-127 (layers 13-25)\n+///   - ...and so on\n+/// </para>\n+/// <para><b>Use Cases:</b>\n+/// - Training frontier models (GPT-3 scale: 100B-1T parameters)\n+/// - Requires 100s to 1000s of GPUs\n+/// - When single parallelism dimension isn't enough\n+/// - Production training at largest scales (OpenAI, Google, Meta)\n+/// </para>\n+/// <para><b>Trade-offs:</b>\n+/// - Memory: Excellent - exploits all memory-saving strategies\n+/// - Communication: Complex - requires careful network topology optimization\n+/// - Complexity: Very High - most complex distributed strategy\n+/// - Best for: Frontier-scale models (100B+ params), massive GPU clusters\n+/// - Requires: Careful tuning of all three parallelism dimensions for efficiency\n+/// </para>\n+/// <para><b>Implementation Note:</b>\n+/// This is a production-ready framework providing the 3D parallelism infrastructure.\n+/// Full production deployment requires:\n+/// 1. Process group management (separate groups for data/tensor/pipeline)\n+/// 2. Model-specific layer partitioning\n+/// 3. Careful configuration tuning for your specific cluster topology\n+/// This implementation demonstrates the pattern and provides the foundation.\n+/// </para>\n+/// <para>\n+/// Example:\n+/// <code>\n+/// // Training a 175B parameter model on 512 GPUs\n+/// // 8 pipeline stages √ó 8 tensor parallel √ó 8 data parallel = 512\n+/// var model = new MassiveTransformer&lt;double&gt;(...);\n+/// var backend = new InMemoryCommunicationBackend&lt;double&gt;(rank: myRank, worldSize: 512);\n+/// var config = new ShardingConfiguration&lt;double&gt;(backend);\n+///\n+/// var hybridModel = new HybridShardedModel&lt;double, Tensor&lt;double&gt;, Tensor&lt;double&gt;&gt;(\n+///     model, config,\n+///     pipelineParallelSize: 8,\n+///     tensorParallelSize: 8,\n+///     dataParallelSize: 8);\n+/// </code>\n+/// </para>\n+/// </remarks>\n+/// <typeparam name=\"T\">The numeric type</typeparam>\n+/// <typeparam name=\"TInput\">The input type for the model</typeparam>\n+/// <typeparam name=\"TOutput\">The output type for the model</typeparam>\n+public class HybridShardedModel<T, TInput, TOutput> : ShardedModelBase<T, TInput, TOutput>\n+{\n+    private readonly int _pipelineParallelSize;\n+    private readonly int _tensorParallelSize;\n+    private readonly int _dataParallelSize;\n+    private readonly int _pipelineRank;\n+    private readonly int _tensorRank;\n+    private readonly int _dataRank;\n+\n+    /// <summary>\n+    /// Creates a new 3D Parallel (Hybrid Sharded) model.\n+    /// </summary>\n+    /// <param name=\"wrappedModel\">The model to partition with 3D parallelism</param>\n+    /// <param name=\"config\">Configuration for sharding and communication</param>\n+    /// <param name=\"pipelineParallelSize\">Number of pipeline stages (default: 1)</param>\n+    /// <param name=\"tensorParallelSize\">Tensor parallelism degree (default: 1)</param>\n+    /// <param name=\"dataParallelSize\">Data parallelism degree (default: uses remaining GPUs)</param>\n+    public HybridShardedModel(\n+        IFullModel<T, TInput, TOutput> wrappedModel,\n+        IShardingConfiguration<T> config,\n+        int pipelineParallelSize = 1,\n+        int tensorParallelSize = 1,\n+        int dataParallelSize = -1)\n+        : base(wrappedModel, config)\n+    {\n+        _pipelineParallelSize = pipelineParallelSize;\n+        _tensorParallelSize = tensorParallelSize;\n+\n+        // Calculate data parallel size if not specified\n+        if (dataParallelSize == -1)\n+        {\n+            int totalGpus = WorldSize;\n+            if (totalGpus % (pipelineParallelSize * tensorParallelSize) != 0)\n+            {\n+                throw new ArgumentException(\n+                    $\"WorldSize ({totalGpus}) must be divisible by \" +\n+                    $\"pipelineParallelSize ({pipelineParallelSize}) √ó tensorParallelSize ({tensorParallelSize})\");\n+            }\n+            _dataParallelSize = totalGpus / (pipelineParallelSize * tensorParallelSize);\n+        }\n+        else\n+        {\n+            _dataParallelSize = dataParallelSize;\n+        }\n+\n+        // Verify configuration\n+        if (_pipelineParallelSize * _tensorParallelSize * _dataParallelSize != WorldSize)\n+        {\n+            throw new ArgumentException(\n+                $\"Pipeline ({_pipelineParallelSize}) √ó Tensor ({_tensorParallelSize}) √ó \" +\n+                $\"Data ({_dataParallelSize}) must equal WorldSize ({WorldSize})\");\n+        }\n+\n+        // Calculate this process's position in the 3D grid\n+        // Layout: [pipeline][tensor][data]\n+        int tensorGroupSize = _tensorParallelSize * _dataParallelSize;\n+        _pipelineRank = Rank / tensorGroupSize;\n+        int withinStage = Rank % tensorGroupSize;\n+        _tensorRank = withinStage / _dataParallelSize;\n+        _dataRank = withinStage % _dataParallelSize;\n+    }\n+\n+    /// <summary>\n+    /// Initializes 3D parallelism by partitioning along all dimensions.\n+    /// </summary>\n+    protected override void InitializeSharding()\n+    {\n+        var fullParameters = WrappedModel.GetParameters();\n+        int totalParams = fullParameters.Length;\n+\n+        // Apply pipeline partitioning first (depth-wise)\n+        int pipelineShardSize = totalParams / _pipelineParallelSize;\n+        int pipelineRemainder = totalParams % _pipelineParallelSize;\n+        int myPipelineSize = pipelineShardSize + (_pipelineRank < pipelineRemainder ? 1 : 0);\n+        int myPipelineStart = _pipelineRank * pipelineShardSize + Math.Min(_pipelineRank, pipelineRemainder);\n+\n+        // Then apply tensor partitioning within the pipeline stage (width-wise)\n+        int tensorShardSize = myPipelineSize / _tensorParallelSize;\n+        int tensorRemainder = myPipelineSize % _tensorParallelSize;\n+        ShardSize = tensorShardSize + (_tensorRank < tensorRemainder ? 1 : 0);\n+        int tensorOffset = _tensorRank * tensorShardSize + Math.Min(_tensorRank, tensorRemainder);\n+\n+        ShardStartIndex = myPipelineStart + tensorOffset;\n+\n+        // Data parallelism doesn't affect parameter sharding (parameters replicated across data-parallel group)\n+        var shardData = new T[ShardSize];\n+        Array.Copy(fullParameters.ToArray(), ShardStartIndex, shardData, 0, ShardSize);\n+        LocalShard = new Vector<T>(shardData);\n+\n+        CachedFullParameters = null;\n+    }\n+\n+    /// <inheritdoc/>\n+    public override void SynchronizeGradients()\n+    {\n+        // In 3D parallelism, synchronization happens in multiple steps:\n+        // 1. AllReduce within tensor-parallel group (sum partial gradients)\n+        // 2. AllReduce within data-parallel group (average across data replicas)\n+        // 3. Pipeline parallel stages handle their own gradients\n+\n+        // For this framework implementation, we use simplified synchronization\n+        Config.CommunicationBackend.AllReduce(LocalShard, ReductionOperation.Average);\n+\n+        CachedFullParameters = null;\n+    }"}]}},{"id":"PRRT_kwDOKSXUF85hMtMT","isResolved":true,"isOutdated":true,"path":"src/DistributedTraining/HybridShardedOptimizer.cs","line":null,"originalLine":107,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VdUBA","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**Ensure the post-step barrier always executes.**\n\nIf `WrappedOptimizer.Optimize` throws on any rank, that rank skips the second barrier while the others block indefinitely, deadlocking the job. Wrap the optimize block in `try/finally` so the trailing barrier (or a corresponding abort) always runs. For example:\n\n```diff\n-        Config.CommunicationBackend.Barrier();\n-\n-        var result = WrappedOptimizer.Optimize(inputData);\n-        ...\n-\n-        Config.CommunicationBackend.Barrier();\n-\n-        return result;\n+        Config.CommunicationBackend.Barrier();\n+\n+        try\n+        {\n+            var result = WrappedOptimizer.Optimize(inputData);\n+            ...\n+            return result;\n+        }\n+        finally\n+        {\n+            Config.CommunicationBackend.Barrier();\n+        }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        Config.CommunicationBackend.Barrier();\n\n        // 3D parallel optimization requires careful coordination:\n        // 1. Pipeline: gradient accumulation across micro-batches\n        // 2. Tensor: synchronization within tensor-parallel group\n        // 3. Data: synchronization across data-parallel replicas\n\n        try\n        {\n            var result = WrappedOptimizer.Optimize(inputData);\n\n            if (Config.AutoSyncGradients && result.BestSolution != null)\n            {\n                // Synchronization order matters:\n                // 1. First sync within tensor-parallel group (sum partial results)\n                // 2. Then sync across data-parallel replicas (average gradients)\n                // 3. Pipeline stages handle their own gradient accumulation\n\n                SynchronizeParameters(result.BestSolution);\n            }\n\n            return result;\n        }\n        finally\n        {\n            Config.CommunicationBackend.Barrier();\n        }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/DistributedTraining/HybridShardedOptimizer.cs around lines 86 to 107, the\ntrailing Config.CommunicationBackend.Barrier() can be skipped if\nWrappedOptimizer.Optimize throws, causing deadlock; wrap the call to\nWrappedOptimizer.Optimize and the following conditional synchronization in a\ntry/finally and place the final Barrier() (or a call to a safe abort/cleanup\nthat itself performs the barrier) in the finally block so the barrier always\nexecutes regardless of exceptions; ensure any exceptions are rethrown after the\nfinally so upstream error handling still sees them.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits ae2f65b to 32d17fd","createdAt":"2025-11-09T02:30:55Z","path":"src/DistributedTraining/HybridShardedOptimizer.cs","line":null,"originalLine":107,"diffHunk":"@@ -0,0 +1,121 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.Models.Inputs;\n+using AiDotNet.Optimizers;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Implements 3D Parallelism optimizer - coordinates across data, tensor, and pipeline dimensions.\n+/// </summary>\n+/// <remarks>\n+/// <para><b>Strategy Overview:</b>\n+/// 3D Parallelism optimizer coordinates optimization across all three parallelism dimensions:\n+/// - Data parallel: synchronizes gradients across data-parallel replicas\n+/// - Tensor parallel: synchronizes within tensor-parallel groups\n+/// - Pipeline parallel: handles gradient accumulation across micro-batches\n+///\n+/// This requires managing separate communication groups for each dimension and ensuring\n+/// proper synchronization order to maintain correctness and efficiency.\n+/// </para>\n+/// <para><b>For Beginners:</b>\n+/// This is the most complex optimizer, coordinating all three types of parallelism.\n+/// It needs to handle:\n+/// 1. Averaging gradients across data-parallel replicas (GPUs processing different batches)\n+/// 2. Synchronizing tensor-parallel groups (GPUs sharing layer computations)\n+/// 3. Accumulating gradients from pipeline micro-batches\n+///\n+/// Think of it like coordinating a massive team split into departments (pipeline stages),\n+/// work groups (tensor parallel), and shifts (data parallel) - all need to sync at the right times.\n+/// </para>\n+/// <para><b>Use Cases:</b>\n+/// - Frontier-scale models (100B+ parameters)\n+/// - 100s to 1000s of GPUs\n+/// - Works with HybridShardedModel\n+/// </para>\n+/// <para><b>Trade-offs:</b>\n+/// - Memory: Excellent - exploits all dimensions\n+/// - Communication: Complex - multiple sync patterns\n+/// - Complexity: Very High - most complex optimizer\n+/// - Best for: Largest scale training\n+/// </para>\n+/// </remarks>\n+/// <typeparam name=\"T\">The numeric type</typeparam>\n+/// <typeparam name=\"TInput\">The input type for the model</typeparam>\n+/// <typeparam name=\"TOutput\">The output type for the model</typeparam>\n+public class HybridShardedOptimizer<T, TInput, TOutput> : ShardedOptimizerBase<T, TInput, TOutput>\n+{\n+    private readonly int _pipelineParallelSize;\n+    private readonly int _tensorParallelSize;\n+    private readonly int _dataParallelSize;\n+\n+    public HybridShardedOptimizer(\n+        IOptimizer<T, TInput, TOutput> wrappedOptimizer,\n+        IShardingConfiguration<T> config,\n+        int pipelineParallelSize = 1,\n+        int tensorParallelSize = 1,\n+        int dataParallelSize = -1)\n+        : base(wrappedOptimizer, config)\n+    {\n+        _pipelineParallelSize = pipelineParallelSize;\n+        _tensorParallelSize = tensorParallelSize;\n+\n+        if (dataParallelSize == -1)\n+        {\n+            _dataParallelSize = WorldSize / (pipelineParallelSize * tensorParallelSize);\n+        }\n+        else\n+        {\n+            _dataParallelSize = dataParallelSize;\n+        }\n+\n+        if (_pipelineParallelSize * _tensorParallelSize * _dataParallelSize != WorldSize)\n+        {\n+            throw new ArgumentException(\n+                $\"Pipeline ({_pipelineParallelSize}) √ó Tensor ({_tensorParallelSize}) √ó \" +\n+                $\"Data ({_dataParallelSize}) must equal WorldSize ({WorldSize})\");\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public override OptimizationResult<T, TInput, TOutput> Optimize(OptimizationInputData<T, TInput, TOutput> inputData)\n+    {\n+        if (inputData == null)\n+            throw new ArgumentNullException(nameof(inputData));\n+\n+        Config.CommunicationBackend.Barrier();\n+\n+        // 3D parallel optimization requires careful coordination:\n+        // 1. Pipeline: gradient accumulation across micro-batches\n+        // 2. Tensor: synchronization within tensor-parallel group\n+        // 3. Data: synchronization across data-parallel replicas\n+\n+        var result = WrappedOptimizer.Optimize(inputData);\n+\n+        if (Config.AutoSyncGradients && result.BestSolution != null)\n+        {\n+            // Synchronization order matters:\n+            // 1. First sync within tensor-parallel group (sum partial results)\n+            // 2. Then sync across data-parallel replicas (average gradients)\n+            // 3. Pipeline stages handle their own gradient accumulation\n+\n+            SynchronizeParameters(result.BestSolution);\n+        }\n+\n+        Config.CommunicationBackend.Barrier();\n+\n+        return result;"}]}},{"id":"PRRT_kwDOKSXUF85hMtMV","isResolved":false,"isOutdated":true,"path":"src/DistributedTraining/HybridShardedOptimizer.cs","line":null,"originalLine":103,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VdUBE","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Stop averaging parameters across all ranks.**\n\nLine 102 calls `SynchronizeParameters`, which in `ShardedOptimizerBase` performs a world-size AllReduce over the entire parameter vector (and becomes a no-op if the model already implements `IShardedModel`). That neither honors the tensor/data groups nor touches gradients at all, so gradients remain unsynchronized (or worse, fully averaged parameters overwrite the local shards), breaking the 3D parallel semantics you outlined. Replace this with explicit gradient synchronization per dimension‚Äîfirst reduce across the tensor-parallel group, then across the data-parallel group, using the `_tensorParallelSize` / `_dataParallelSize` information and the appropriate communication groups. Only touch parameters when the wrapped optimizer actually requires a parameter broadcast.\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","createdAt":"2025-11-09T02:30:55Z","path":"src/DistributedTraining/HybridShardedOptimizer.cs","line":null,"originalLine":103,"diffHunk":"@@ -0,0 +1,121 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.Models.Inputs;\n+using AiDotNet.Optimizers;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Implements 3D Parallelism optimizer - coordinates across data, tensor, and pipeline dimensions.\n+/// </summary>\n+/// <remarks>\n+/// <para><b>Strategy Overview:</b>\n+/// 3D Parallelism optimizer coordinates optimization across all three parallelism dimensions:\n+/// - Data parallel: synchronizes gradients across data-parallel replicas\n+/// - Tensor parallel: synchronizes within tensor-parallel groups\n+/// - Pipeline parallel: handles gradient accumulation across micro-batches\n+///\n+/// This requires managing separate communication groups for each dimension and ensuring\n+/// proper synchronization order to maintain correctness and efficiency.\n+/// </para>\n+/// <para><b>For Beginners:</b>\n+/// This is the most complex optimizer, coordinating all three types of parallelism.\n+/// It needs to handle:\n+/// 1. Averaging gradients across data-parallel replicas (GPUs processing different batches)\n+/// 2. Synchronizing tensor-parallel groups (GPUs sharing layer computations)\n+/// 3. Accumulating gradients from pipeline micro-batches\n+///\n+/// Think of it like coordinating a massive team split into departments (pipeline stages),\n+/// work groups (tensor parallel), and shifts (data parallel) - all need to sync at the right times.\n+/// </para>\n+/// <para><b>Use Cases:</b>\n+/// - Frontier-scale models (100B+ parameters)\n+/// - 100s to 1000s of GPUs\n+/// - Works with HybridShardedModel\n+/// </para>\n+/// <para><b>Trade-offs:</b>\n+/// - Memory: Excellent - exploits all dimensions\n+/// - Communication: Complex - multiple sync patterns\n+/// - Complexity: Very High - most complex optimizer\n+/// - Best for: Largest scale training\n+/// </para>\n+/// </remarks>\n+/// <typeparam name=\"T\">The numeric type</typeparam>\n+/// <typeparam name=\"TInput\">The input type for the model</typeparam>\n+/// <typeparam name=\"TOutput\">The output type for the model</typeparam>\n+public class HybridShardedOptimizer<T, TInput, TOutput> : ShardedOptimizerBase<T, TInput, TOutput>\n+{\n+    private readonly int _pipelineParallelSize;\n+    private readonly int _tensorParallelSize;\n+    private readonly int _dataParallelSize;\n+\n+    public HybridShardedOptimizer(\n+        IOptimizer<T, TInput, TOutput> wrappedOptimizer,\n+        IShardingConfiguration<T> config,\n+        int pipelineParallelSize = 1,\n+        int tensorParallelSize = 1,\n+        int dataParallelSize = -1)\n+        : base(wrappedOptimizer, config)\n+    {\n+        _pipelineParallelSize = pipelineParallelSize;\n+        _tensorParallelSize = tensorParallelSize;\n+\n+        if (dataParallelSize == -1)\n+        {\n+            _dataParallelSize = WorldSize / (pipelineParallelSize * tensorParallelSize);\n+        }\n+        else\n+        {\n+            _dataParallelSize = dataParallelSize;\n+        }\n+\n+        if (_pipelineParallelSize * _tensorParallelSize * _dataParallelSize != WorldSize)\n+        {\n+            throw new ArgumentException(\n+                $\"Pipeline ({_pipelineParallelSize}) √ó Tensor ({_tensorParallelSize}) √ó \" +\n+                $\"Data ({_dataParallelSize}) must equal WorldSize ({WorldSize})\");\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public override OptimizationResult<T, TInput, TOutput> Optimize(OptimizationInputData<T, TInput, TOutput> inputData)\n+    {\n+        if (inputData == null)\n+            throw new ArgumentNullException(nameof(inputData));\n+\n+        Config.CommunicationBackend.Barrier();\n+\n+        // 3D parallel optimization requires careful coordination:\n+        // 1. Pipeline: gradient accumulation across micro-batches\n+        // 2. Tensor: synchronization within tensor-parallel group\n+        // 3. Data: synchronization across data-parallel replicas\n+\n+        var result = WrappedOptimizer.Optimize(inputData);\n+\n+        if (Config.AutoSyncGradients && result.BestSolution != null)\n+        {\n+            // Synchronization order matters:\n+            // 1. First sync within tensor-parallel group (sum partial results)\n+            // 2. Then sync across data-parallel replicas (average gradients)\n+            // 3. Pipeline stages handle their own gradient accumulation\n+\n+            SynchronizeParameters(result.BestSolution);\n+        }"}]}},{"id":"PRRT_kwDOKSXUF85hMtMY","isResolved":true,"isOutdated":true,"path":"src/DistributedTraining/MPICommunicationBackend.cs","line":null,"originalLine":113,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VdUBH","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Fix Rank/WorldSize: MPI path always reports single-rank**  \n`_rank` and `_worldSize` are captured once in the constructor (defaults: 0/1) and never updated when MPI is actually available. Downstream invariants break immediately: `ValidateRoot` rejects any non-zero root, `ReductionOperation.Average` divides by 1 regardless of real world size, and every collective behaves as if the job were single-process. Please promote these fields to mutable state and refresh them from the MPI communicator (e.g., via `Rank`/`Size` properties) inside `OnInitialize()` after `_mpiCommunicator` is resolved, so they reflect the real MPI topology.  \n\nApply this diff to address the issue:\n\n```diff\n-    private readonly int _rank;\n-    private readonly int _worldSize;\n+    private int _rank;\n+    private int _worldSize;\n@@\n         _worldSize = worldSize;\n@@\n                     if (isInitialized == true)\n                     {\n                         _useMPI = true;\n@@\n                         if (intracommunicatorType != null)\n                         {\n                             var worldProp = intracommunicatorType.GetProperty(\"World\", BindingFlags.Public | BindingFlags.Static);\n                             _mpiCommunicator = worldProp?.GetValue(null);\n+                            if (_mpiCommunicator != null)\n+                            {\n+                                var rankProp = _mpiCommunicator.GetType().GetProperty(\"Rank\", BindingFlags.Public | BindingFlags.Instance);\n+                                var sizeProp = _mpiCommunicator.GetType().GetProperty(\"Size\", BindingFlags.Public | BindingFlags.Instance);\n+                                if (rankProp != null)\n+                                {\n+                                    _rank = (int)rankProp.GetValue(_mpiCommunicator)!;\n+                                }\n+                                if (sizeProp != null)\n+                                {\n+                                    _worldSize = (int)sizeProp.GetValue(_mpiCommunicator)!;\n+                                }\n+                            }\n                         }\n \n                         return;\n                     }\n```\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits 578d634 to 943ec25","createdAt":"2025-11-09T02:30:55Z","path":"src/DistributedTraining/MPICommunicationBackend.cs","line":null,"originalLine":113,"diffHunk":"@@ -0,0 +1,454 @@\n+using System;\n+using System.Linq;\n+using System.Reflection;\n+using AiDotNet.LinearAlgebra;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// MPI.NET-based communication backend for production distributed training.\n+/// </summary>\n+/// <remarks>\n+/// <para><b>Overview:</b>\n+/// MPI (Message Passing Interface) is the industry-standard communication framework for\n+/// high-performance computing. MPI.NET provides .NET bindings for MPI, enabling production-grade\n+/// distributed training on HPC clusters and supercomputers.\n+/// </para>\n+/// <para><b>Features:</b>\n+/// - Optimized collective operations (AllReduce, AllGather, etc.)\n+/// - Support for InfiniBand and other high-speed interconnects\n+/// - Battle-tested in HPC for decades\n+/// - Excellent performance and scalability\n+/// </para>\n+/// <para><b>Use Cases:</b>\n+/// - HPC cluster deployment\n+/// - Large-scale training (100s-1000s of nodes)\n+/// - InfiniBand or high-speed network infrastructure\n+/// - Production distributed training pipelines\n+/// </para>\n+/// <para><b>Requirements:</b>\n+/// - MPI.NET NuGet package\n+/// - MPI implementation (OpenMPI, MPICH, Intel MPI, etc.)\n+/// - MPI runtime environment\n+/// </para>\n+/// <para><b>Graceful Degradation:</b>\n+/// If MPI.NET is not available, this backend falls back to single-process mode\n+/// where all operations work correctly but without actual inter-process communication.\n+/// A warning is logged when fallback mode is active.\n+/// </para>\n+/// </remarks>\n+/// <typeparam name=\"T\">The numeric type for operations</typeparam>\n+public class MPICommunicationBackend<T> : CommunicationBackendBase<T>\n+{\n+    private readonly int _rank;\n+    private readonly int _worldSize;\n+    private bool _useMPI;\n+    private object? _mpiCommunicator;\n+    private Type? _mpiEnvironmentType;\n+    private Type? _mpiOperationType;\n+\n+    /// <summary>\n+    /// Creates a new MPI communication backend.\n+    /// </summary>\n+    /// <param name=\"rank\">This process's rank (will be obtained from MPI if available)</param>\n+    /// <param name=\"worldSize\">Total number of processes (will be obtained from MPI if available)</param>\n+    public MPICommunicationBackend(int rank = 0, int worldSize = 1)\n+    {\n+        _rank = rank;\n+        _worldSize = worldSize;\n+        _useMPI = false;\n+    }\n+\n+    /// <inheritdoc/>\n+    public override int Rank => _rank;\n+\n+    /// <inheritdoc/>\n+    public override int WorldSize => _worldSize;\n+\n+    /// <inheritdoc/>\n+    protected override void OnInitialize()\n+    {\n+        // Try to use MPI.NET if available via reflection\n+        try\n+        {\n+            // Attempt to load MPI.NET assembly\n+            _mpiEnvironmentType = Type.GetType(\"MPI.Environment, MPI\");\n+\n+            if (_mpiEnvironmentType != null)\n+            {\n+                // Check if MPI is already initialized\n+                var initializedProp = _mpiEnvironmentType.GetProperty(\"Initialized\", BindingFlags.Public | BindingFlags.Static);\n+                if (initializedProp != null)\n+                {\n+                    var isInitialized = (bool?)initializedProp.GetValue(null);\n+\n+                    if (isInitialized == true)\n+                    {\n+                        _useMPI = true;\n+                        Console.WriteLine(\"MPI.NET detected and initialized. Using MPI for communication.\");\n+\n+                        // Get communicator type and world communicator\n+                        var intracommunicatorType = Type.GetType(\"MPI.Intracommunicator, MPI\");\n+                        if (intracommunicatorType != null)\n+                        {\n+                            var worldProp = intracommunicatorType.GetProperty(\"World\", BindingFlags.Public | BindingFlags.Static);\n+                            _mpiCommunicator = worldProp?.GetValue(null);\n+                        }\n+\n+                        return;\n+                    }\n+                }\n+            }\n+        }\n+        catch (Exception ex)\n+        {\n+            // MPI.NET not available, will use fallback\n+            Console.WriteLine($\"MPI.NET not available: {ex.Message}\");\n+        }\n+\n+        // Fallback to single-process mode\n+        _useMPI = false;\n+        Console.WriteLine(\"WARNING: MPI.NET not available. Running in single-process fallback mode.\");\n+        Console.WriteLine(\"For production HPC deployment, install MPI.NET and an MPI implementation (OpenMPI, MPICH, etc.).\");\n+    }"}]}},{"id":"PRRT_kwDOKSXUF85hMtMa","isResolved":true,"isOutdated":false,"path":"src/DistributedTraining/NCCLCommunicationBackend.cs","line":161,"originalLine":160,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VdUBK","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Fail fast when NCCL is unavailable for multi-rank runs.**\n\nIf NCCL cannot be initialized, `_ncclAvailable` is reset to false but initialization still succeeds. The CPU ‚Äúfallback‚Äù helpers are no-ops for `WorldSize > 1`, so AllReduce/AllGather/ReduceScatter never exchange data‚Äîgradients silently diverge the moment you launch more than one rank without NCCL. Please throw when `_worldSize > 1` and NCCL isn‚Äôt available (or wire up a real CPU backend) instead of pretending the backend works.\n\n```diff\n         if (!_ncclAvailable)\n         {\n             Console.WriteLine(\"WARNING: NCCL not available. Falling back to CPU-based collective operations.\");\n             Console.WriteLine(\"For production GPU training, install NCCL library and CUDA toolkit.\");\n+\n+            if (_worldSize > 1)\n+            {\n+                throw new InvalidOperationException(\n+                    \"NCCLCommunicationBackend requires the NCCL runtime for multi-rank execution. \" +\n+                    \"Use a CPU-capable backend (MPI, Gloo, InMemory) or install NCCL before initializing.\");\n+            }\n         }\n```\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/DistributedTraining/NCCLCommunicationBackend.cs around lines 121 to 160,\nthe current OnInitialize swallows NCCL failures and leaves initialization\nsucceeding while _ncclAvailable is false; this silently breaks multi-rank runs\nbecause the CPU fallback is a no-op for WorldSize > 1. Change the method so that\nafter attempting NCCL detection/initialization if _ncclAvailable is false and\n_worldSize > 1 you throw an exception (e.g. InvalidOperationException with a\nclear message) to fail fast; alternatively, if you intend to support multi-rank\nwithout NCCL, wire up a real CPU backend and set _ncclAvailable accordingly.\nEnsure DllNotFoundException and other exceptions still set _ncclAvailable =\nfalse but do not suppress the final check that throws when running with multiple\nranks.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits 578d634 to 943ec25","createdAt":"2025-11-09T02:30:55Z","path":"src/DistributedTraining/NCCLCommunicationBackend.cs","line":161,"originalLine":160,"diffHunk":"@@ -0,0 +1,407 @@\n+using System;\n+using System.Linq;\n+using System.Runtime.InteropServices;\n+using AiDotNet.LinearAlgebra;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// NVIDIA NCCL-based communication backend for GPU-to-GPU communication.\n+/// </summary>\n+/// <remarks>\n+/// <para><b>Overview:</b>\n+/// NCCL (NVIDIA Collective Communications Library) is optimized for multi-GPU communication\n+/// on NVIDIA GPUs. It provides highly optimized implementations of collective operations\n+/// that take advantage of NVLink, PCIe, and network topology for maximum throughput.\n+/// </para>\n+/// <para><b>Features:</b>\n+/// - Optimized for NVIDIA GPUs (NVLink, NVSwitch awareness)\n+/// - Near-optimal bandwidth utilization\n+/// - Supports multi-node multi-GPU configurations\n+/// - Ring and tree algorithms for different collective operations\n+/// - Essential for high-performance multi-GPU training\n+/// </para>\n+/// <para><b>Use Cases:</b>\n+/// - Multi-GPU training on NVIDIA hardware\n+/// - DGX systems and GPU clusters\n+/// - When maximum GPU-to-GPU communication performance is critical\n+/// - Production training on NVIDIA infrastructure\n+/// </para>\n+/// <para><b>Requirements:</b>\n+/// - NVIDIA GPUs (compute capability 3.0+)\n+/// - CUDA toolkit\n+/// - NCCL library\n+/// - .NET bindings for NCCL (custom P/Invoke or wrapper library)\n+/// </para>\n+/// <para><b>Graceful Degradation:</b>\n+/// If NCCL library is not available, this backend falls back to CPU-based collective operations.\n+/// A warning is logged when fallback mode is active. This allows code to work on systems\n+/// without NVIDIA GPUs or NCCL, albeit with reduced performance.\n+/// </para>\n+/// </remarks>\n+/// <typeparam name=\"T\">The numeric type for operations</typeparam>\n+public class NCCLCommunicationBackend<T> : CommunicationBackendBase<T>\n+{\n+    // NCCL P/Invoke declarations\n+    private const string NcclLibrary = \"nccl\";\n+\n+    // NCCL data types\n+    private enum ncclDataType_t\n+    {\n+        ncclInt8 = 0, ncclChar = 0,\n+        ncclUint8 = 1,\n+        ncclInt32 = 2, ncclInt = 2,\n+        ncclUint32 = 3,\n+        ncclInt64 = 4,\n+        ncclUint64 = 5,\n+        ncclFloat16 = 6, ncclHalf = 6,\n+        ncclFloat32 = 7, ncclFloat = 7,\n+        ncclFloat64 = 8, ncclDouble = 8,\n+        ncclBfloat16 = 9,\n+        ncclNumTypes = 10\n+    }\n+\n+    // NCCL reduction operations\n+    private enum ncclRedOp_t\n+    {\n+        ncclSum = 0,\n+        ncclProd = 1,\n+        ncclMax = 2,\n+        ncclMin = 3,\n+        ncclAvg = 4,\n+        ncclNumOps = 5\n+    }\n+\n+    // NCCL result codes\n+    private enum ncclResult_t\n+    {\n+        ncclSuccess = 0,\n+        ncclUnhandledCudaError = 1,\n+        ncclSystemError = 2,\n+        ncclInternalError = 3,\n+        ncclInvalidArgument = 4,\n+        ncclInvalidUsage = 5,\n+        ncclRemoteError = 6,\n+        ncclInProgress = 7,\n+        ncclNumResults = 8\n+    }\n+\n+    // NCCL P/Invoke methods (will fail gracefully if library not found)\n+    [DllImport(NcclLibrary, CallingConvention = CallingConvention.Cdecl)]\n+    private static extern ncclResult_t ncclGetVersion(out int version);\n+\n+    private readonly int _rank;\n+    private readonly int _worldSize;\n+    private readonly int _deviceId;\n+    private bool _ncclAvailable;\n+    private IntPtr _ncclComm;\n+\n+    /// <summary>\n+    /// Creates a new NCCL communication backend.\n+    /// </summary>\n+    /// <param name=\"rank\">This process's rank</param>\n+    /// <param name=\"worldSize\">Total number of processes</param>\n+    /// <param name=\"deviceId\">CUDA device ID for this process (default: use rank)</param>\n+    public NCCLCommunicationBackend(int rank = 0, int worldSize = 1, int deviceId = -1)\n+    {\n+        _rank = rank;\n+        _worldSize = worldSize;\n+        _deviceId = deviceId >= 0 ? deviceId : rank;\n+        _ncclAvailable = false;\n+        _ncclComm = IntPtr.Zero;\n+    }\n+\n+    /// <inheritdoc/>\n+    public override int Rank => _rank;\n+\n+    /// <inheritdoc/>\n+    public override int WorldSize => _worldSize;\n+\n+    /// <inheritdoc/>\n+    protected override void OnInitialize()\n+    {\n+        // Try to use NCCL if available\n+        try\n+        {\n+            // Check if NCCL library is available\n+            ncclResult_t result = ncclGetVersion(out int version);\n+\n+            if (result == ncclResult_t.ncclSuccess)\n+            {\n+                _ncclAvailable = true;\n+                Console.WriteLine($\"NCCL library detected (version: {version}). Using NCCL for GPU communication.\");\n+\n+                // Note: Full NCCL initialization requires:\n+                // 1. ncclGetUniqueId() on rank 0\n+                // 2. Broadcast unique ID to all ranks (via separate mechanism like TCP)\n+                // 3. ncclCommInitRank() on all ranks\n+                // This is complex and requires additional infrastructure, so we log a warning\n+\n+                Console.WriteLine(\"WARNING: NCCL communicator initialization requires additional setup.\");\n+                Console.WriteLine(\"For full NCCL support, implement unique ID distribution and call ncclCommInitRank.\");\n+                _ncclAvailable = false; // Disable NCCL until full initialization is implemented\n+            }\n+        }\n+        catch (DllNotFoundException)\n+        {\n+            // NCCL library not found\n+            _ncclAvailable = false;\n+        }\n+        catch (Exception ex)\n+        {\n+            Console.WriteLine($\"NCCL initialization failed: {ex.Message}\");\n+            _ncclAvailable = false;\n+        }\n+\n+        if (!_ncclAvailable)\n+        {\n+            Console.WriteLine(\"WARNING: NCCL not available. Falling back to CPU-based collective operations.\");\n+            Console.WriteLine(\"For production GPU training, install NCCL library and CUDA toolkit.\");\n+        }"}]}},{"id":"PRRT_kwDOKSXUF85hMtMe","isResolved":true,"isOutdated":true,"path":"src/DistributedTraining/PipelineParallelModel.cs","line":null,"originalLine":153,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VdUBP","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**Local shard passed into full-model parameter setter**\n\n`LocalShard` only contains this stage‚Äôs contiguous chunk, but `IFullModel.SetParameters` expects the *entire* parameter vector. With the provided usage (wrapping a full model), this call either throws (length mismatch) or silently maps the shard to the wrong weights. On the later sync path we never reapply the other stages‚Äô parameters, so every non-zero rank trains with truncated weights. Please gather/apply the full parameter vector, similar to the FSDP implementation, before trimming back to the local shard:\n\n```diff\n-            WrappedModel.SetParameters(LocalShard);\n-            WrappedModel.Train(input, expectedOutput);\n-            LocalShard = WrappedModel.GetParameters();\n+            var fullParams = GatherFullParameters();\n+            WrappedModel.SetParameters(fullParams);\n+            WrappedModel.Train(input, expectedOutput);\n+            var updatedParams = WrappedModel.GetParameters();\n+            UpdateLocalShardFromFull(updatedParams);\n```\n\nWithout this change pipeline stages beyond rank 0 can‚Äôt run correctly. Follow up by wiring the TODO activation flow, but first fix the parameter mapping so the basics work.\n\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits ae2f65b to 32d17fd","createdAt":"2025-11-09T02:30:56Z","path":"src/DistributedTraining/PipelineParallelModel.cs","line":null,"originalLine":153,"diffHunk":"@@ -0,0 +1,286 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.Models;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Implements Pipeline Parallel model wrapper - splits model into stages across ranks.\n+/// </summary>\n+/// <remarks>\n+/// <para><b>Strategy Overview:</b>\n+/// Pipeline Parallelism (GPipe-style) divides the model vertically into stages, with each process\n+/// owning specific layers. Input mini-batches are divided into micro-batches that flow through\n+/// the pipeline stages sequentially. This enables training models too large to fit on a single device\n+/// while maintaining good hardware utilization through micro-batch pipelining.\n+/// </para>\n+/// <para><b>For Beginners:</b>\n+/// Pipeline parallelism is like an assembly line for training. Imagine a deep neural network as\n+/// a tall building - instead of one person (GPU) handling all floors, we assign different floors\n+/// to different people. Process 0 handles layers 0-10, Process 1 handles layers 11-20, etc.\n+///\n+/// To keep everyone busy (avoid idle time), we split each batch into smaller \"micro-batches\" that\n+/// flow through the pipeline like cars on an assembly line. While Process 1 is working on micro-batch 1,\n+/// Process 0 can start on micro-batch 2.\n+/// </para>\n+/// <para><b>Use Cases:</b>\n+/// - Very deep models that don't fit on a single GPU\n+/// - When model depth (layers) >> width (parameters per layer)\n+/// - Transformer models with many layers\n+/// - Complementary to data parallelism (can combine them)\n+/// </para>\n+/// <para><b>Trade-offs:</b>\n+/// - Memory: Excellent for deep models - each rank stores only its layers\n+/// - Communication: Low - only activations passed between adjacent stages\n+/// - Complexity: High - requires micro-batching, careful scheduling, pipeline bubble overhead\n+/// - Best for: Very deep models, limited per-device memory\n+/// - Limitation: Pipeline \"bubble\" (idle time) reduces efficiency, typically ~12-25% for GPipe\n+/// </para>\n+/// <para><b>Implementation Note:</b>\n+/// This is a production-ready framework implementation. Full pipeline parallelism requires\n+/// model-specific layer partitioning logic. This implementation provides the infrastructure\n+/// and demonstrates the pattern. For production use, extend this class with your specific\n+/// layer assignment strategy.\n+/// </para>\n+/// <para>\n+/// Example:\n+/// <code>\n+/// var model = new DeepNeuralNetwork&lt;double&gt;(...); // 100 layers\n+/// var backend = new InMemoryCommunicationBackend&lt;double&gt;(rank: 0, worldSize: 4);\n+/// var config = new ShardingConfiguration&lt;double&gt;(backend);\n+///\n+/// // Rank 0: layers 0-24, Rank 1: layers 25-49, Rank 2: layers 50-74, Rank 3: layers 75-99\n+/// var pipelineModel = new PipelineParallelModel&lt;double, Tensor&lt;double&gt;, Tensor&lt;double&gt;&gt;(\n+///     model, config, microBatchSize: 4);\n+/// </code>\n+/// </para>\n+/// </remarks>\n+/// <typeparam name=\"T\">The numeric type</typeparam>\n+/// <typeparam name=\"TInput\">The input type for the model</typeparam>\n+/// <typeparam name=\"TOutput\">The output type for the model</typeparam>\n+public class PipelineParallelModel<T, TInput, TOutput> : ShardedModelBase<T, TInput, TOutput>\n+{\n+    private readonly int _microBatchSize;\n+    private readonly int _stageId;\n+    private readonly int _numStages;\n+\n+    /// <summary>\n+    /// Creates a new Pipeline Parallel model.\n+    /// </summary>\n+    /// <param name=\"wrappedModel\">The model to split into pipeline stages</param>\n+    /// <param name=\"config\">Configuration for sharding and communication</param>\n+    /// <param name=\"microBatchSize\">Size of micro-batches for pipeline execution (default: 1)</param>\n+    public PipelineParallelModel(\n+        IFullModel<T, TInput, TOutput> wrappedModel,\n+        IShardingConfiguration<T> config,\n+        int microBatchSize = 1)\n+        : base(wrappedModel, config)\n+    {\n+        _microBatchSize = microBatchSize;\n+        _stageId = Rank;\n+        _numStages = WorldSize;\n+    }\n+\n+    /// <summary>\n+    /// Initializes pipeline parallelism by partitioning parameters into stages.\n+    /// </summary>\n+    protected override void InitializeSharding()\n+    {\n+        var fullParameters = WrappedModel.GetParameters();\n+        int totalParams = fullParameters.Length;\n+\n+        // Divide parameters into pipeline stages\n+        // Each stage owns a contiguous chunk of parameters (representing layers)\n+        int baseShardSize = totalParams / _numStages;\n+        int remainder = totalParams % _numStages;\n+\n+        ShardSize = baseShardSize + (_stageId < remainder ? 1 : 0);\n+        ShardStartIndex = _stageId * baseShardSize + Math.Min(_stageId, remainder);\n+\n+        // Extract this stage's parameters\n+        var shardData = new T[ShardSize];\n+        Array.Copy(fullParameters.ToArray(), ShardStartIndex, shardData, 0, ShardSize);\n+        LocalShard = new Vector<T>(shardData);\n+\n+        CachedFullParameters = null;\n+    }\n+\n+    /// <inheritdoc/>\n+    public override void Train(TInput input, TOutput expectedOutput)\n+    {\n+        // Pipeline parallel training requires:\n+        // 1. Split batch into micro-batches\n+        // 2. Forward pass: stage i receives activations from stage i-1, sends to stage i+1\n+        // 3. Backward pass: stage i receives gradients from stage i+1, sends to stage i-1\n+        // 4. Synchronize gradients across micro-batches\n+\n+        // For this production framework, we provide a simplified implementation\n+        // that demonstrates the pattern. Full implementation would require\n+        // model-specific micro-batch splitting and activation passing.\n+\n+        // Stage 0 starts with input\n+        if (_stageId == 0)\n+        {\n+            // First stage processes input\n+            WrappedModel.SetParameters(LocalShard);\n+            WrappedModel.Train(input, expectedOutput);\n+            LocalShard = WrappedModel.GetParameters();\n+\n+            // TODO: Send activations to next stage\n+            // In full implementation: send forward activations to rank+1\n+        }\n+        else if (_stageId == _numStages - 1)\n+        {\n+            // Last stage\n+            // TODO: Receive activations from previous stage\n+            // In full implementation: receive from rank-1\n+\n+            WrappedModel.SetParameters(LocalShard);\n+            WrappedModel.Train(input, expectedOutput);\n+            LocalShard = WrappedModel.GetParameters();\n+\n+            // TODO: Send gradients backward to previous stage\n+        }\n+        else\n+        {\n+            // Middle stages\n+            // TODO: Receive from rank-1, process, send to rank+1 (forward)\n+            // TODO: Receive gradients from rank+1, process, send to rank-1 (backward)\n+\n+            WrappedModel.SetParameters(LocalShard);\n+            WrappedModel.Train(input, expectedOutput);\n+            LocalShard = WrappedModel.GetParameters();\n+        }"}]}},{"id":"PRRT_kwDOKSXUF85hMtMh","isResolved":true,"isOutdated":true,"path":"src/DistributedTraining/TensorParallelModel.cs","line":null,"originalLine":123,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VdUBS","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**AllReduce corrupts tensor-parallel shards**\n\nEach rank‚Äôs `LocalShard` holds a disjoint slice of the parameter vector. Calling `AllReduce` across the whole world sums unrelated indices, so after this line the shard contains other ranks‚Äô parameters instead of its own. The result diverges immediately once `_tensorParallelSize > 1`. Please restrict synchronization to the data-parallel dimension (or skip it entirely for pure tensor parallel) instead of reducing across the full world. One minimal guard would be:\n\n```diff\n-        Config.CommunicationBackend.AllReduce(LocalShard, ReductionOperation.Sum);\n+        if (_tensorParallelSize > 1)\n+        {\n+            throw new NotSupportedException(\"TensorParallelModel requires subgroup gradient sync; AllReduce over the full world corrupts the shard.\");\n+        }\n+        Config.CommunicationBackend.AllReduce(LocalShard, ReductionOperation.Sum);\n```\n\nand then follow up by implementing subgroup-aware collectives per tensor-parallel group. Otherwise the current code corrupts the parameter shards on the first step.\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        // In tensor parallelism, we typically need to synchronize within tensor-parallel group\n        // For column-parallel layers: AllReduce gradients\n        // For row-parallel layers: Different synchronization pattern\n        if (_tensorParallelSize > 1)\n        {\n            throw new NotSupportedException(\"TensorParallelModel requires subgroup gradient sync; AllReduce over the full world corrupts the shard.\");\n        }\n        Config.CommunicationBackend.AllReduce(LocalShard, ReductionOperation.Sum);\n\n        CachedFullParameters = null;\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/DistributedTraining/TensorParallelModel.cs around lines 118 to 123, the\ncall Config.CommunicationBackend.AllReduce(LocalShard, ReductionOperation.Sum)\nis reducing across the full world and corrupts tensor-parallel shards; change\nthe logic so AllReduce is only performed on the data-parallel communicator (or\nskipped entirely when running pure tensor-parallel) rather than the global\nworld. Detect the tensor-parallel group size (e.g. _tensorParallelSize > 1) and:\nif in a combined data+tensor parallel run, call AllReduce using the\ndata-parallel subgroup/communicator; if purely tensor-parallel, do not AllReduce\nthe LocalShard (or use a tensor-parallel-aware collectives implementation such\nas a subgroup-aware AllReduce/ReduceScatter for column/row-parallel layers).\nEnsure CachedFullParameters handling remains correct after the conditional and\nadd a TODO to implement proper subgroup collectives if missing.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits ae2f65b to 32d17fd","createdAt":"2025-11-09T02:30:56Z","path":"src/DistributedTraining/TensorParallelModel.cs","line":null,"originalLine":123,"diffHunk":"@@ -0,0 +1,258 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.Models;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Implements Tensor Parallel model wrapper - splits individual layers across ranks (Megatron-LM style).\n+/// </summary>\n+/// <remarks>\n+/// <para><b>Strategy Overview:</b>\n+/// Tensor Parallelism (Megatron-LM style) partitions individual layers horizontally across processes.\n+/// For example, a large matrix multiplication is split so each GPU computes only a portion of the output,\n+/// then results are combined. This is particularly effective for transformer models where attention and\n+/// feed-forward layers can be partitioned along specific dimensions (column-parallel and row-parallel).\n+/// </para>\n+/// <para><b>For Beginners:</b>\n+/// Tensor parallelism is like splitting a single large calculation across multiple workers.\n+/// Imagine a huge spreadsheet calculation - instead of one person doing all the math, we divide\n+/// the spreadsheet columns across multiple people, each computing their portion simultaneously.\n+///\n+/// For example, in a neural network layer with a 10000x10000 weight matrix:\n+/// - GPU 0 handles columns 0-2499\n+/// - GPU 1 handles columns 2500-4999\n+/// - GPU 2 handles columns 5000-7499\n+/// - GPU 3 handles columns 7500-9999\n+///\n+/// They compute in parallel, then combine results.\n+/// </para>\n+/// <para><b>Use Cases:</b>\n+/// - Very wide models (large hidden dimensions)\n+/// - Transformer models (BERT, GPT) with large attention/FFN layers\n+/// - When individual layers are too large for single GPU\n+/// - Often combined with pipeline parallelism for maximum scalability\n+/// </para>\n+/// <para><b>Trade-offs:</b>\n+/// - Memory: Excellent for wide layers - each rank stores only portion of weights\n+/// - Communication: High - requires AllReduce or AllGather within each layer\n+/// - Complexity: Very High - requires model-aware partitioning, specific to layer types\n+/// - Best for: Transformer models, very wide layers, fast interconnects (NVLink)\n+/// - Limitation: Requires fast communication (high overhead on slow networks)\n+/// </para>\n+/// <para><b>Implementation Note:</b>\n+/// This is a production-ready framework implementation. Full tensor parallelism requires\n+/// model-specific layer partitioning (column-parallel vs row-parallel strategy for different\n+/// layer types). This implementation provides the infrastructure. For production use with\n+/// specific models (e.g., transformers), extend this class with layer-aware partitioning.\n+/// </para>\n+/// <para>\n+/// Example:\n+/// <code>\n+/// var model = new TransformerModel&lt;double&gt;(...); // Large transformer\n+/// var backend = new InMemoryCommunicationBackend&lt;double&gt;(rank: 0, worldSize: 4);\n+/// var config = new ShardingConfiguration&lt;double&gt;(backend);\n+///\n+/// // Each rank handles 1/4 of each layer's width\n+/// var tensorParallelModel = new TensorParallelModel&lt;double, Tensor&lt;double&gt;, Tensor&lt;double&gt;&gt;(\n+///     model, config);\n+/// </code>\n+/// </para>\n+/// </remarks>\n+/// <typeparam name=\"T\">The numeric type</typeparam>\n+/// <typeparam name=\"TInput\">The input type for the model</typeparam>\n+/// <typeparam name=\"TOutput\">The output type for the model</typeparam>\n+public class TensorParallelModel<T, TInput, TOutput> : ShardedModelBase<T, TInput, TOutput>\n+{\n+    private readonly int _tensorParallelSize;\n+\n+    /// <summary>\n+    /// Creates a new Tensor Parallel model.\n+    /// </summary>\n+    /// <param name=\"wrappedModel\">The model to partition with tensor parallelism</param>\n+    /// <param name=\"config\">Configuration for sharding and communication</param>\n+    public TensorParallelModel(\n+        IFullModel<T, TInput, TOutput> wrappedModel,\n+        IShardingConfiguration<T> config)\n+        : base(wrappedModel, config)\n+    {\n+        _tensorParallelSize = WorldSize;\n+    }\n+\n+    /// <summary>\n+    /// Initializes tensor parallelism by partitioning layer weights.\n+    /// </summary>\n+    protected override void InitializeSharding()\n+    {\n+        var fullParameters = WrappedModel.GetParameters();\n+        int totalParams = fullParameters.Length;\n+\n+        // In tensor parallelism, we partition weights within layers\n+        // For this framework implementation, we use a simplified column-wise partitioning\n+        // Production usage would require layer-specific partitioning logic\n+\n+        int baseShardSize = totalParams / _tensorParallelSize;\n+        int remainder = totalParams % _tensorParallelSize;\n+\n+        ShardSize = baseShardSize + (Rank < remainder ? 1 : 0);\n+        ShardStartIndex = Rank * baseShardSize + Math.Min(Rank, remainder);\n+\n+        var shardData = new T[ShardSize];\n+        Array.Copy(fullParameters.ToArray(), ShardStartIndex, shardData, 0, ShardSize);\n+        LocalShard = new Vector<T>(shardData);\n+\n+        CachedFullParameters = null;\n+    }\n+\n+    /// <summary>\n+    /// Synchronizes tensor-parallel computation results.\n+    /// </summary>\n+    /// <remarks>\n+    /// In tensor parallelism, different layers require different synchronization patterns:\n+    /// - Column-parallel layers: AllReduce after computation\n+    /// - Row-parallel layers: AllGather before computation\n+    /// This simplified implementation uses AllReduce.\n+    /// </remarks>\n+    public override void SynchronizeGradients()\n+    {\n+        // In tensor parallelism, we typically need to synchronize within tensor-parallel group\n+        // For column-parallel layers: AllReduce gradients\n+        // For row-parallel layers: Different synchronization pattern\n+        Config.CommunicationBackend.AllReduce(LocalShard, ReductionOperation.Sum);\n+\n+        CachedFullParameters = null;"}]}},{"id":"PRRT_kwDOKSXUF85hMtMl","isResolved":true,"isOutdated":false,"path":"src/DistributedTraining/ZeRO2Model.cs","line":126,"originalLine":106,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VdUBW","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Fix ReduceScatter precondition so ZeRO‚Äë2 doesn‚Äôt crash on uneven sizes.**\n\n`LocalShard.Length` equals the full parameter count. Whenever that count isn‚Äôt divisible by `WorldSize`, `GlooCommunicationBackend.ReduceScatter` throws (`data.Length % _worldSize != 0`). This makes ZeRO‚Äë2 unusable for most models. Pad the buffer before the call and trim the result back to the expected shard length for each rank.\n\n```diff\n     public override void SynchronizeGradients()\n     {\n-        // Use ReduceScatter to reduce gradients and distribute shards\n-        // Each process will receive only its portion of the reduced gradients\n-        _gradientShard = Config.CommunicationBackend.ReduceScatter(LocalShard, ReductionOperation.Average);\n+        var totalParams = LocalShard.Length;\n+        var remainder = totalParams % WorldSize;\n+\n+        // Pad to satisfy ReduceScatter‚Äôs divisibility requirement\n+        Vector<T> reduceInput = LocalShard;\n+        if (remainder != 0)\n+        {\n+            var paddedLength = totalParams + (WorldSize - remainder);\n+            var padded = new T[paddedLength];\n+            Array.Copy(LocalShard.ToArray(), padded, totalParams);\n+            reduceInput = new Vector<T>(padded);\n+        }\n+\n+        var reducedChunk = Config.CommunicationBackend.ReduceScatter(reduceInput, ReductionOperation.Average);\n+\n+        // Trim padding so each rank keeps only its logical shard\n+        var shardLength = totalParams / WorldSize + (Rank < remainder ? 1 : 0);\n+        var shardData = new T[shardLength];\n+        Array.Copy(reducedChunk.ToArray(), 0, shardData, 0, shardLength);\n+        _gradientShard = new Vector<T>(shardData);\n \n         CachedFullParameters = null;\n     }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    /// <summary>\n    /// Synchronizes gradients using ReduceScatter - each process gets its shard of reduced gradients.\n    /// </summary>\n    public override void SynchronizeGradients()\n    {\n        var totalParams = LocalShard.Length;\n        var remainder = totalParams % WorldSize;\n\n        // Pad to satisfy ReduceScatter's divisibility requirement\n        Vector<T> reduceInput = LocalShard;\n        if (remainder != 0)\n        {\n            var paddedLength = totalParams + (WorldSize - remainder);\n            var padded = new T[paddedLength];\n            Array.Copy(LocalShard.ToArray(), padded, totalParams);\n            reduceInput = new Vector<T>(padded);\n        }\n\n        var reducedChunk = Config.CommunicationBackend.ReduceScatter(reduceInput, ReductionOperation.Average);\n\n        // Trim padding so each rank keeps only its logical shard\n        var shardLength = totalParams / WorldSize + (Rank < remainder ? 1 : 0);\n        var shardData = new T[shardLength];\n        Array.Copy(reducedChunk.ToArray(), 0, shardData, 0, shardLength);\n        _gradientShard = new Vector<T>(shardData);\n\n        CachedFullParameters = null;\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits c717ae2 to 9ed6627","createdAt":"2025-11-09T02:30:56Z","path":"src/DistributedTraining/ZeRO2Model.cs","line":126,"originalLine":106,"diffHunk":"@@ -0,0 +1,213 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.Models;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Implements ZeRO Stage 2 model wrapper - shards optimizer states and gradients.\n+/// </summary>\n+/// <remarks>\n+/// <para><b>Strategy Overview:</b>\n+/// ZeRO Stage 2 builds on ZeRO-1 by additionally sharding gradients across processes.\n+/// Parameters are still replicated for the forward pass, but gradients are reduced and scattered\n+/// (ReduceScatter) so each process only stores a portion. This saves significant memory compared\n+/// to ZeRO-1, especially for large models.\n+/// </para>\n+/// <para><b>For Beginners:</b>\n+/// This implements ZeRO Stage 2, which saves even more memory than ZeRO-1. The model parameters\n+/// are still fully replicated (like DDP and ZeRO-1), but now both the optimizer state AND the\n+/// gradients are split across processes. After computing gradients, they're immediately reduced\n+/// and scattered so each process only keeps its portion.\n+/// </para>\n+/// <para>\n+/// Think of it like a team where everyone has the full playbook (parameters), but when taking\n+/// notes during practice (gradients), they divide up the note-taking so each person is responsible\n+/// for recording only certain plays. This saves everyone from having to write everything down.\n+/// </para>\n+/// <para><b>Use Cases:</b>\n+/// - Larger models where gradient memory becomes significant\n+/// - Want substantial memory savings with moderate communication cost\n+/// - Preparing for ZeRO-3/FSDP migration\n+/// </para>\n+/// <para><b>Trade-offs:</b>\n+/// - Memory: Very Good - saves both optimizer states and gradients\n+/// - Communication: Moderate - uses ReduceScatter instead of AllReduce\n+/// - Complexity: Moderate - gradient sharding adds some complexity\n+/// - Best for: Large models where gradient memory is significant\n+/// </para>\n+/// <para>\n+/// Example:\n+/// <code>\n+/// var model = new NeuralNetworkModel&lt;double&gt;(...);\n+/// var backend = new InMemoryCommunicationBackend&lt;double&gt;(rank: 0, worldSize: 4);\n+/// var config = new ShardingConfiguration&lt;double&gt;(backend);\n+/// var zero2Model = new ZeRO2Model&lt;double, Tensor&lt;double&gt;, Tensor&lt;double&gt;&gt;(model, config);\n+///\n+/// // Use with ZeRO2Optimizer for full ZeRO-2 benefits\n+/// var zero2Optimizer = new ZeRO2Optimizer&lt;double, Tensor&lt;double&gt;, Tensor&lt;double&gt;&gt;(optimizer, config);\n+/// </code>\n+/// </para>\n+/// </remarks>\n+/// <typeparam name=\"T\">The numeric type</typeparam>\n+/// <typeparam name=\"TInput\">The input type for the model</typeparam>\n+/// <typeparam name=\"TOutput\">The output type for the model</typeparam>\n+public class ZeRO2Model<T, TInput, TOutput> : ShardedModelBase<T, TInput, TOutput>\n+{\n+    private Vector<T>? _gradientShard;\n+\n+    public ZeRO2Model(IFullModel<T, TInput, TOutput> wrappedModel, IShardingConfiguration<T> config)\n+        : base(wrappedModel, config)\n+    {\n+    }\n+\n+    protected override void InitializeSharding()\n+    {\n+        var fullParameters = WrappedModel.GetParameters();\n+\n+        // Parameters still replicated (like ZeRO-1)\n+        ShardStartIndex = 0;\n+        ShardSize = fullParameters.Length;\n+        LocalShard = new Vector<T>(fullParameters.ToArray());\n+\n+        // Calculate gradient shard size\n+        int totalParams = fullParameters.Length;\n+        int baseShardSize = totalParams / WorldSize;\n+        int remainder = totalParams % WorldSize;\n+        int gradShardSize = baseShardSize + (Rank < remainder ? 1 : 0);\n+\n+        _gradientShard = new Vector<T>(new T[gradShardSize]);\n+        CachedFullParameters = null;\n+    }\n+\n+    /// <summary>\n+    /// Synchronizes gradients using ReduceScatter - each process gets its shard of reduced gradients.\n+    /// </summary>\n+    public override void SynchronizeGradients()\n+    {\n+        // Use ReduceScatter to reduce gradients and distribute shards\n+        // Each process will receive only its portion of the reduced gradients\n+        _gradientShard = Config.CommunicationBackend.ReduceScatter(LocalShard, ReductionOperation.Average);\n+\n+        CachedFullParameters = null;\n+    }\n+\n+    /// <inheritdoc/>\n+    public override void Train(TInput input, TOutput expectedOutput)\n+    {\n+        // Full parameters available locally\n+        WrappedModel.SetParameters(LocalShard);\n+        WrappedModel.Train(input, expectedOutput);\n+        LocalShard = WrappedModel.GetParameters();\n+        InvalidateCache();\n+\n+        if (Config.AutoSyncGradients)\n+        {\n+            SynchronizeGradients();"}]}},{"id":"PRRT_kwDOKSXUF85hMtMt","isResolved":true,"isOutdated":true,"path":"src/DistributedTraining/ZeRO2Optimizer.cs","line":null,"originalLine":70,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VdUBg","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n<details>\n<summary>üß© Analysis chain</summary>\n\n**Unused ReduceScatter result suggests incomplete implementation.**\n\nLine 66 computes `reducedShard` via `ReduceScatter`, but this result is never used. The comment at line 69 indicates this requires \"optimizer state integration.\" This creates dead code that performs communication without applying the results.\n\n\n\n\nEither:\n1. Apply the reduced shard to update parameters, or\n2. Add a TODO comment and consider removing the ReduceScatter call until the full implementation is ready (to avoid unnecessary communication overhead)\n\n```diff\n         // In ZeRO-2, we use ReduceScatter to reduce gradients and distribute shards\n         // Each process receives only its portion of the reduced gradients\n         if (Config.AutoSyncGradients && result.BestSolution != null)\n         {\n-            // Instead of AllReduce, use ReduceScatter for gradient sharding\n-            var parameters = result.BestSolution.GetParameters();\n-            var reducedShard = Config.CommunicationBackend.ReduceScatter(parameters, ReductionOperation.Average);\n-\n-            // Each process now has only its shard of gradients\n-            // Update would use only this shard (requires optimizer state integration)\n+            // TODO: Implement ReduceScatter gradient sharding\n+            // This requires deeper optimizer state integration:\n+            // 1. var parameters = result.BestSolution.GetParameters();\n+            // 2. var reducedShard = Config.CommunicationBackend.ReduceScatter(parameters, ReductionOperation.Average);\n+            // 3. Apply reducedShard to update only local parameter shard\n+            // 4. AllGather updated parameters for next forward pass\n         }\n```\n\nPlease verify whether this placeholder implementation is intentional or if the ReduceScatter result should be applied.\n\n---\n\nI need to investigate the codebase to verify the review comment. Let me search for information about the unused variable and the broader context.\n\n---\n\nüèÅ Script executed:\n\n```shell\n#!/bin/bash\n# First, find and inspect the ZeRO2Optimizer.cs file to verify the dead code issue\nfd \"ZeRO2Optimizer.cs\" --type f\n```\n\nLength of output: 41\n\n---\n\nüèÅ Script executed:\n\n```shell\n#!/bin/bash\n# Search for references to reducedShard to confirm it's unused\nrg -n \"reducedShard\" --type cs -C 5\n```\n\nLength of output: 1068\n\n---\n\nüèÅ Script executed:\n\n```shell\n#!/bin/bash\n# Inspect the full method context around lines 60-70\nfd \"ZeRO2Optimizer.cs\" --type f -x cat {} | head -100\n```\n\nLength of output: 3733\n\n---\n\n\n\n</details>\n\n**Remove the unused ReduceScatter call or complete the implementation.**\n\nThe `reducedShard` variable is confirmed to be unused‚Äîit appears only once in the entire file (line 66 assignment) and is never referenced again. Since `ReduceScatter` is a distributed communication operation, executing it and discarding the results wastes network bandwidth unnecessarily.\n\nThe placeholder comment in `SynchronizeOptimizerState()` confirms this is incomplete work-in-progress code. Either:\n1. Complete the implementation by applying `reducedShard` to update parameters locally, or\n2. Remove the ReduceScatter call and replace with a TODO comment documenting the required steps\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/DistributedTraining/ZeRO2Optimizer.cs around lines 60 to 70, the\nReduceScatter result (reducedShard) is assigned but never used which wastes a\ndistributed communication call; either complete the ZeRO-2 shard update or\nremove the call: to complete, replace the placeholder by mapping reducedShard\nback into the local parameter shard and invoke the optimizer state update method\n(e.g., apply/update gradients for the local shard, ensuring shard indices and\nsizes match and any optimizer momentum/exp avg state is updated and\nsynchronized), or if not ready, remove the ReduceScatter call and replace it\nwith a TODO comment that documents the intended steps (perform ReduceScatter to\nreceive local gradient shard, apply that shard to local parameters and optimizer\nstate, and handle shard indexing/shape checks) so no unnecessary network traffic\noccurs.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits 578d634 to 5b45fed","createdAt":"2025-11-09T02:30:56Z","path":"src/DistributedTraining/ZeRO2Optimizer.cs","line":null,"originalLine":70,"diffHunk":"@@ -0,0 +1,91 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.Models.Inputs;\n+using AiDotNet.Optimizers;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Implements ZeRO Stage 2 optimizer - shards optimizer states and gradients.\n+/// </summary>\n+/// <remarks>\n+/// <para><b>Strategy Overview:</b>\n+/// ZeRO-2 optimizer builds on ZeRO-1 by additionally sharding gradients using ReduceScatter.\n+/// After backward pass, gradients are reduced and scattered so each process only stores its\n+/// portion. This further reduces memory compared to ZeRO-1, as gradients can be as large as\n+/// the model itself. Parameters remain replicated for the forward pass.\n+/// </para>\n+/// <para><b>For Beginners:</b>\n+/// ZeRO-2 saves even more memory than ZeRO-1. Not only is the optimizer state split across\n+/// processes, but the gradients are too. After computing gradients, we immediately use\n+/// ReduceScatter to average them across processes AND split them up, so each process only\n+/// keeps its assigned portion. This is like having a team where each person is responsible\n+/// for updating only certain parameters.\n+/// </para>\n+/// <para><b>Use Cases:</b>\n+/// - Large models where gradient memory is significant\n+/// - Want substantial memory savings\n+/// - Works well with ZeRO2Model\n+/// </para>\n+/// <para><b>Trade-offs:</b>\n+/// - Memory: Very Good - saves optimizer states + gradients\n+/// - Communication: Moderate - uses ReduceScatter instead of AllReduce\n+/// - Complexity: Moderate - gradient and state sharding\n+/// - Best for: Large models with significant gradient memory\n+/// </para>\n+/// </remarks>\n+/// <typeparam name=\"T\">The numeric type</typeparam>\n+/// <typeparam name=\"TInput\">The input type for the model</typeparam>\n+/// <typeparam name=\"TOutput\">The output type for the model</typeparam>\n+public class ZeRO2Optimizer<T, TInput, TOutput> : ShardedOptimizerBase<T, TInput, TOutput>\n+{\n+    public ZeRO2Optimizer(\n+        IOptimizer<T, TInput, TOutput> wrappedOptimizer,\n+        IShardingConfiguration<T> config)\n+        : base(wrappedOptimizer, config)\n+    {\n+    }\n+\n+    /// <inheritdoc/>\n+    public override OptimizationResult<T, TInput, TOutput> Optimize(OptimizationInputData<T, TInput, TOutput> inputData)\n+    {\n+        if (inputData == null)\n+            throw new ArgumentNullException(nameof(inputData));\n+\n+        Config.CommunicationBackend.Barrier();\n+\n+        // Optimize on local data\n+        var result = WrappedOptimizer.Optimize(inputData);\n+\n+        // In ZeRO-2, we use ReduceScatter to reduce gradients and distribute shards\n+        // Each process receives only its portion of the reduced gradients\n+        if (Config.AutoSyncGradients && result.BestSolution != null)\n+        {\n+            // Instead of AllReduce, use ReduceScatter for gradient sharding\n+            var parameters = result.BestSolution.GetParameters();\n+            var reducedShard = Config.CommunicationBackend.ReduceScatter(parameters, ReductionOperation.Average);\n+\n+            // Each process now has only its shard of gradients\n+            // Update would use only this shard (requires optimizer state integration)\n+        }"}]}},{"id":"PRRT_kwDOKSXUF85hMtMz","isResolved":true,"isOutdated":true,"path":"src/Interfaces/IPredictionModelBuilder.cs","line":505,"originalLine":461,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VdUBl","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üü° Minor_\n\n**Fix XML doc parameters to match the signature.**\n\nThe comment still documents `strategy`, `autoSyncGradients`, `minimumParameterGroupSize`, and `enableGradientCompression`, but the method only takes the optional `backend`. Update the docs (or reintroduce the parameters) so tooling and readers aren‚Äôt misled.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/Interfaces/IPredictionModelBuilder.cs around lines 401 to 461, the XML\ndoc lists parameters (strategy, autoSyncGradients, minimumParameterGroupSize,\nenableGradientCompression) that no longer exist on the method and therefore\nmislead tooling; remove the obsolete <param> entries and any example text that\nrefers to those parameters, update remarks/examples to describe only the actual\noptional backend parameter and the default behavior\n(InMemoryCommunicationBackend and default strategy), and ensure a single <param\nname=\"backend\"> entry remains and matches the method signature and examples.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commit 433e396","createdAt":"2025-11-09T02:30:56Z","path":"src/Interfaces/IPredictionModelBuilder.cs","line":505,"originalLine":461,"diffHunk":"@@ -398,6 +398,69 @@\n     /// <returns>The builder instance for method chaining.</returns>\n     IPredictionModelBuilder<T, TInput, TOutput> ConfigureMetaLearning(IMetaLearner<T, TInput, TOutput> metaLearner);\n \n+    /// <summary>\n+    /// Configures distributed training across multiple GPUs or machines.\n+    /// </summary>\n+    /// <param name=\"backend\">Communication backend to use. If null, uses InMemoryCommunicationBackend.</param>\n+    /// <param name=\"strategy\">Distributed training strategy. Default is FSDP.</param>\n+    /// <param name=\"autoSyncGradients\">Whether to automatically synchronize gradients. Default is true.</param>\n+    /// <param name=\"minimumParameterGroupSize\">Minimum parameter group size for communication. Default is 1024.</param>\n+    /// <param name=\"enableGradientCompression\">Whether to enable gradient compression. Default is false.</param>\n+    /// <returns>This builder instance for method chaining.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// When distributed training is configured, the builder automatically wraps the model and optimizer\n+    /// with their distributed counterparts based on the chosen strategy. This enables:\n+    /// - Training models too large to fit on a single GPU\n+    /// - Faster training by distributing work across multiple processes\n+    /// - Automatic gradient synchronization and parameter sharding\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> Call this method to enable distributed training across multiple GPUs.\n+    /// You can use this with no parameters for sensible defaults, or provide a custom communication backend.\n+    /// </para>\n+    /// <para>\n+    /// <b>Beginner Usage (no parameters):</b>\n+    /// <code>\n+    /// var result = builder\n+    ///     .ConfigureModel(myModel)\n+    ///     .ConfigureOptimizer(myOptimizer)\n+    ///     .ConfigureDistributedTraining()  // Uses InMemory backend, DDP strategy\n+    ///     .Build(xTrain, yTrain);\n+    /// </code>\n+    /// </para>\n+    /// <para>\n+    /// <b>Advanced Usage (custom backend):</b>\n+    /// <code>\n+    /// var backend = new MPICommunicationBackend&lt;double&gt;();\n+    /// var result = builder\n+    ///     .ConfigureModel(myModel)\n+    ///     .ConfigureDistributedTraining(backend)  // Uses MPI backend, DDP strategy\n+    ///     .Build(xTrain, yTrain);\n+    /// </code>\n+    /// </para>\n+    /// <para>\n+    /// <b>Expert Usage (full control with specific distributed model):</b>\n+    /// <code>\n+    /// // For full control over strategy and options, configure the distributed model directly\n+    /// var backend = new NCCLCommunicationBackend&lt;double&gt;();\n+    /// var shardingConfig = new ShardingConfiguration&lt;double&gt;(backend)\n+    /// {\n+    ///     AutoSyncGradients = true,\n+    ///     MinimumParameterGroupSize = 2048,\n+    ///     EnableGradientCompression = true\n+    /// };\n+    /// var distributedModel = new FSDPModel&lt;double, Matrix&lt;double&gt;, Vector&lt;double&gt;&gt;(baseModel, shardingConfig);\n+    /// var result = builder\n+    ///     .ConfigureModel(distributedModel)  // Use FSDP instead of default DDP\n+    ///     .Build(xTrain, yTrain);\n+    /// </code>\n+    /// </para>\n+    /// </remarks>\n+    /// <param name=\"backend\">Optional communication backend. If null, uses InMemoryCommunicationBackend.</param>\n+    /// <returns>This builder instance for method chaining.</returns>"}]}},{"id":"PRRT_kwDOKSXUF85hMtM3","isResolved":true,"isOutdated":false,"path":"src/PredictionModelBuilder.cs","line":51,"originalLine":49,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VdUBn","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Critical: Missing using directive causes compilation failure.**\n\nThe field declaration for `ICommunicationBackend<T>` requires a using directive that is not present in this file.\n\n\n\nAdd the following using directive at the top of the file (likely around lines 1-10 with the other global usings):\n\n```diff\n+using AiDotNet.DistributedTraining;\n```\n\nBased on pipeline failures.\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\nusing AiDotNet.DistributedTraining;\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>üß∞ Tools</summary>\n\n<details>\n<summary>ü™õ GitHub Check: Publish Size Analysis</summary>\n\n[failure] 49-49: \nThe type or namespace name 'ICommunicationBackend<>' could not be found (are you missing a using directive or an assembly reference?)\n\n</details>\n\n</details>\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/PredictionModelBuilder.cs around line 49, the field declaration for\nICommunicationBackend<T> fails to compile because the file is missing the using\ndirective for the namespace that declares ICommunicationBackend<T>; locate the\ninterface declaration in the project (or referenced package) and add its\nnamespace as a using at the top of this file alongside the other usings (or\nalternatively fully-qualify the type where the field is declared), then rebuild\nto confirm the compilation error is resolved.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commit 433e396","createdAt":"2025-11-09T02:30:56Z","path":"src/PredictionModelBuilder.cs","line":51,"originalLine":49,"diffHunk":"@@ -46,6 +46,7 @@\n     private IGenerator<T>? _ragGenerator;\n     private IEnumerable<IQueryProcessor>? _queryProcessors;\n     private IMetaLearner<T, TInput, TOutput>? _metaLearner;\n+    private ICommunicationBackend<T>? _distributedBackend;"}]}},{"id":"PRRT_kwDOKSXUF85hMtM-","isResolved":true,"isOutdated":true,"path":"src/PredictionModelBuilder.cs","line":null,"originalLine":286,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VdUBw","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üü° Minor_\n\n**Minor: Redundant null-coalescing operator.**\n\nAt line 277, the code checks `_distributedBackend ?? new InMemory...`, but line 274 already verifies `_distributedBackend != null`. The null-coalescing operator on line 277 is unreachable.\n\n\n\nApply this diff to simplify the logic:\n\n```diff\n         if (_distributedBackend != null)\n         {\n-            // Use provided backend or default to InMemory for single-process\n-            var backend = _distributedBackend ?? new DistributedTraining.InMemoryCommunicationBackend<T>(rank: 0, worldSize: 1);\n+            // Use the configured backend\n+            var backend = _distributedBackend;\n \n             // Create sharding configuration with defaults\n             var shardingConfig = new DistributedTraining.ShardingConfiguration<T>(backend);\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        if (_distributedBackend != null)\n        {\n            // Use the configured backend\n            var backend = _distributedBackend;\n\n            // Create sharding configuration with defaults\n            var shardingConfig = new DistributedTraining.ShardingConfiguration<T>(backend);\n\n            // Wrap with DDP (Distributed Data Parallel) - industry standard for 90% of use cases\n            // For other strategies, users can configure the distributed model directly via ConfigureModel()\n            model = new DistributedTraining.DDPModel<T, TInput, TOutput>(_model, shardingConfig);\n            finalOptimizer = new DistributedTraining.DDPOptimizer<T, TInput, TOutput>(optimizer, shardingConfig);\n        }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/PredictionModelBuilder.cs around lines 274 to 286, the code uses a null\ncheck on _distributedBackend and then uses the null-coalescing operator when\nassigning backend, which is redundant and unreachable; change the assignment to\nsimply use the already-validated value (e.g., var backend =\n_distributedBackend;) and remove or adjust the comment about defaulting to\nInMemory so it no longer implies runtime fallback in this branch.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits 578d634 to 943ec25","createdAt":"2025-11-09T02:30:56Z","path":"src/PredictionModelBuilder.cs","line":null,"originalLine":286,"diffHunk":"@@ -266,14 +267,32 @@\n         var outlierRemoval = _outlierRemoval ?? new NoOutlierRemoval<T, TInput, TOutput>();\n         var dataPreprocessor = _dataPreprocessor ?? new DefaultDataPreprocessor<T, TInput, TOutput>(normalizer, featureSelector, outlierRemoval);\n \n+        // Wrap model and optimizer for distributed training if configured\n+        IFullModel<T, TInput, TOutput> model = _model;\n+        IOptimizer<T, TInput, TOutput> finalOptimizer = optimizer;\n+\n+        if (_distributedBackend != null)\n+        {\n+            // Use provided backend or default to InMemory for single-process\n+            var backend = _distributedBackend ?? new DistributedTraining.InMemoryCommunicationBackend<T>(rank: 0, worldSize: 1);\n+\n+            // Create sharding configuration with defaults\n+            var shardingConfig = new DistributedTraining.ShardingConfiguration<T>(backend);\n+\n+            // Wrap with DDP (Distributed Data Parallel) - industry standard for 90% of use cases\n+            // For other strategies, users can configure the distributed model directly via ConfigureModel()\n+            model = new DistributedTraining.DDPModel<T, TInput, TOutput>(_model, shardingConfig);\n+            finalOptimizer = new DistributedTraining.DDPOptimizer<T, TInput, TOutput>(optimizer, shardingConfig);\n+        }"}]}},{"id":"PRRT_kwDOKSXUF85hMtND","isResolved":true,"isOutdated":true,"path":"src/PredictionModelBuilder.cs","line":585,"originalLine":530,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VdUB0","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üü° Minor_\n\n**Minor: Documentation mentions non-existent parameter.**\n\nLine 502 in the XML documentation mentions a `strategy` parameter that doesn't exist in the method signature. The current implementation always uses DDP as noted in the comment at line 282.\n\n\n\nApply this diff to fix the documentation:\n\n```diff\n     /// <param name=\"backend\">Communication backend to use. If null, uses InMemoryCommunicationBackend.</param>\n-    /// <param name=\"strategy\">Distributed training strategy. Default is FSDP.</param>\n-    /// <param name=\"autoSyncGradients\">Whether to automatically synchronize gradients. Default is true.</param>\n-    /// <param name=\"minimumParameterGroupSize\">Minimum parameter group size for communication. Default is 1024.</param>\n-    /// <param name=\"enableGradientCompression\">Whether to enable gradient compression. Default is false.</param>\n     /// <returns>This builder instance for method chaining.</returns>\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    /// <summary>\n    /// Configures distributed training across multiple GPUs or machines.\n    /// </summary>\n    /// <param name=\"backend\">Communication backend to use. If null, uses InMemoryCommunicationBackend.</param>\n    /// <returns>This builder instance for method chaining.</returns>\n    /// <remarks>\n    /// <para>\n    /// When distributed training is configured, the Build() method will automatically wrap\n    /// the model and optimizer with their distributed counterparts based on the chosen strategy.\n    /// This enables training across multiple GPUs or machines with automatic parameter\n    /// sharding and gradient synchronization.\n    /// </para>\n    /// <para>\n    /// <b>For Beginners:</b> This enables distributed training across multiple GPUs or machines.\n    /// You can call it with no parameters for sensible defaults, or customize as needed.\n    ///\n    /// When you configure this, the builder automatically handles all the complexity:\n    /// - Your model gets split across GPUs (parameter sharding)\n    /// - Gradients are synchronized automatically\n    /// - Training is coordinated across all processes\n    ///\n    /// You just train as normal - the distributed magic happens behind the scenes!\n    /// </para>\n    /// </remarks>\n    public IPredictionModelBuilder<T, TInput, TOutput> ConfigureDistributedTraining(ICommunicationBackend<T>? backend = null)\n    {\n        _distributedBackend = backend;  // Can be null - will use InMemory default in Build()\n        return this;\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>üß∞ Tools</summary>\n\n<details>\n<summary>ü™õ GitHub Actions: Quality Gates (.NET)</summary>\n\n[error] 526-526: CS0246: The type or namespace name 'ICommunicationBackend<>' could not be found (are you missing a using directive or an assembly reference?)\n\n</details>\n<details>\n<summary>ü™õ GitHub Check: Publish Size Analysis</summary>\n\n[failure] 526-526: \nThe type or namespace name 'ICommunicationBackend<>' could not be found (are you missing a using directive or an assembly reference?)\n\n</details>\n\n</details>\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/PredictionModelBuilder.cs around lines 498 to 530, the XML doc lists\nseveral non-existent parameters (strategy, autoSyncGradients,\nminimumParameterGroupSize, enableGradientCompression) and incorrectly references\na configurable strategy; update the documentation to only describe the actual\nmethod parameter (backend) and clarify behavior: state that backend may be null\n(in which case an InMemory backend is used) and note the implementation uses DDP\nas the default distributed strategy; remove or replace all references to the\nnonexistent parameters so the summary, param tags, and remarks accurately\nreflect the single-parameter method signature and actual runtime behavior.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits 578d634 to 943ec25","createdAt":"2025-11-09T02:30:57Z","path":"src/PredictionModelBuilder.cs","line":585,"originalLine":530,"diffHunk":"@@ -475,4 +494,38 @@\n         _metaLearner = metaLearner;\n         return this;\n     }\n+\n+    /// <summary>\n+    /// Configures distributed training across multiple GPUs or machines.\n+    /// </summary>\n+    /// <param name=\"backend\">Communication backend to use. If null, uses InMemoryCommunicationBackend.</param>\n+    /// <param name=\"strategy\">Distributed training strategy. Default is FSDP.</param>\n+    /// <param name=\"autoSyncGradients\">Whether to automatically synchronize gradients. Default is true.</param>\n+    /// <param name=\"minimumParameterGroupSize\">Minimum parameter group size for communication. Default is 1024.</param>\n+    /// <param name=\"enableGradientCompression\">Whether to enable gradient compression. Default is false.</param>\n+    /// <returns>This builder instance for method chaining.</returns>\n+    /// <remarks>\n+    /// <para>\n+    /// When distributed training is configured, the Build() method will automatically wrap\n+    /// the model and optimizer with their distributed counterparts based on the chosen strategy.\n+    /// This enables training across multiple GPUs or machines with automatic parameter\n+    /// sharding and gradient synchronization.\n+    /// </para>\n+    /// <para>\n+    /// <b>For Beginners:</b> This enables distributed training across multiple GPUs or machines.\n+    /// You can call it with no parameters for sensible defaults, or customize as needed.\n+    ///\n+    /// When you configure this, the builder automatically handles all the complexity:\n+    /// - Your model gets split across GPUs (parameter sharding)\n+    /// - Gradients are synchronized automatically\n+    /// - Training is coordinated across all processes\n+    ///\n+    /// You just train as normal - the distributed magic happens behind the scenes!\n+    /// </para>\n+    /// </remarks>\n+    public IPredictionModelBuilder<T, TInput, TOutput> ConfigureDistributedTraining(ICommunicationBackend<T>? backend = null)\n+    {\n+        _distributedBackend = backend;  // Can be null - will use InMemory default in Build()\n+        return this;\n+    }"}]}},{"id":"PRRT_kwDOKSXUF85hNV-J","isResolved":false,"isOutdated":false,"path":"src/DistributedTraining/DDPOptimizer.cs","line":123,"originalLine":123,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VeGGG","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Restore pre-update weights before applying averaged gradients.**\n\nAfter local optimization the wrapped model still holds the locally-updated parameters. Because `originalParams` is computed but never applied, `ApplyGradients` runs on already-updated weights, effectively performing two steps (`params_old - lr*localGrad - lr*avgGrad`) instead of the intended `params_old - lr*avgGrad`. Reset the model to `originalParams` before applying the averaged gradients so DDP stays numerically correct.\n\n```diff\n                 // Average gradients across all workers (true DDP)\n                 Config.CommunicationBackend.AllReduce(localGradients, ReductionOperation.Average);\n\n                 // Apply averaged gradients to original parameters\n                 // This ensures all workers apply the same averaged gradients\n-                var finalModel = gradientOptimizer.ApplyGradients(localGradients, localResult.BestSolution);\n+                localResult.BestSolution.SetParameters(originalParams);\n+                var finalModel = gradientOptimizer.ApplyGradients(localGradients, localResult.BestSolution);\n```\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/DistributedTraining/DDPOptimizer.cs around lines 109 to 123, the code\ncomputes originalParams but never restores them before applying the averaged\ngradients, causing a double-update; modify the flow to set the model's\nparameters back to originalParams (i.e., restore the wrapped model or\nlocalResult.BestSolution to originalParams) immediately after computing\noriginalParams and before calling Config.CommunicationBackend.AllReduce and\ngradientOptimizer.ApplyGradients so that ApplyGradients operates on the\npre-update weights; finally ensure localResult.BestSolution is replaced with the\nreturned finalModel (the model with averaged gradients applied).\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","createdAt":"2025-11-09T08:20:16Z","path":"src/DistributedTraining/DDPOptimizer.cs","line":123,"originalLine":123,"diffHunk":"@@ -0,0 +1,224 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.Models.Inputs;\n+using AiDotNet.Optimizers;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Implements true DDP (Distributed Data Parallel) optimizer - industry-standard gradient averaging.\n+/// </summary>\n+/// <remarks>\n+/// <para><b>Strategy Overview:</b>\n+/// True DDP is the industry-standard distributed training approach used by PyTorch, TensorFlow, and JAX.\n+/// After computing gradients on local data, gradients are averaged across all workers using AllReduce,\n+/// then the averaged gradients are applied to update model parameters. This ensures all workers\n+/// stay perfectly synchronized with identical parameter updates at every step.\n+/// </para>\n+/// <para><b>For Beginners:</b>\n+/// DDP works by having each worker compute gradients on their local batch of data, then averaging\n+/// those gradients across all workers before updating the model. It's like a study group where everyone\n+/// works on different practice problems, shares their solutions, averages the feedback, and everyone\n+/// applies the same averaged correction to their understanding.\n+/// </para>\n+/// <para><b>Key Difference from Local SGD:</b>\n+/// - **True DDP (this class)**: Compute gradients ‚Üí Average GRADIENTS ‚Üí Apply averaged gradients\n+/// - **Local SGD**: Optimize locally ‚Üí Average PARAMETERS after multiple steps\n+///\n+/// DDP maintains tighter synchronization but requires more frequent communication.\n+/// </para>\n+/// <para><b>How It Works:</b>\n+/// 1. Each worker computes gradients on local data batch\n+/// 2. Gradients are synchronized via AllReduce (averaging across all workers)\n+/// 3. Each worker applies the same averaged gradients to their model\n+/// 4. All workers now have identical parameters\n+/// 5. Repeat for next iteration\n+/// </para>\n+/// <para><b>Use Cases:</b>\n+/// - Standard multi-GPU distributed training (PyTorch DDP, TensorFlow MirroredStrategy)\n+/// - Fast interconnects (NVLink, InfiniBand) where communication is cheap\n+/// - Training where tight synchronization is critical\n+/// - Works with any optimizer (SGD, Adam, RMSprop, etc.)\n+/// - Default choice for distributed training with good network\n+/// </para>\n+/// <para><b>Trade-offs:</b>\n+/// - Memory: Each process stores full model and optimizer state\n+/// - Communication: Moderate - gradients synchronized every step (can use gradient compression)\n+/// - Synchronization: Perfect - all workers always have identical parameters\n+/// - Convergence: Identical to single-GPU training (mathematically equivalent)\n+/// - Complexity: Low - straightforward gradient averaging\n+/// - Best for: Fast networks, standard distributed training scenarios\n+/// </para>\n+/// <para><b>Production Implementation:</b>\n+/// This implementation uses the gradient access infrastructure (LastComputedGradients, ApplyGradients)\n+/// to properly average gradients before parameter updates. It reverses local gradient applications\n+/// to recover original parameters, applies averaged gradients, ensuring true DDP semantics.\n+/// </para>\n+/// <para><b>Industry Standard:</b>\n+/// This implementation matches PyTorch's DistributedDataParallel, TensorFlow's MirroredStrategy,\n+/// and JAX's pmap with gradient averaging. It is the gold standard for distributed training.\n+/// </para>\n+/// </remarks>\n+/// <typeparam name=\"T\">The numeric type</typeparam>\n+/// <typeparam name=\"TInput\">The input type for the model</typeparam>\n+/// <typeparam name=\"TOutput\">The output type for the model</typeparam>\n+public class DDPOptimizer<T, TInput, TOutput> : ShardedOptimizerBase<T, TInput, TOutput>\n+{\n+    /// <summary>\n+    /// Creates a true DDP optimizer that averages gradients across workers.\n+    /// </summary>\n+    /// <param name=\"wrappedOptimizer\">The base optimizer to wrap (must be gradient-based: SGD, Adam, etc.)</param>\n+    /// <param name=\"config\">Configuration for distributed training communication</param>\n+    /// <exception cref=\"ArgumentException\">If wrapped optimizer is not gradient-based</exception>\n+    public DDPOptimizer(\n+        IOptimizer<T, TInput, TOutput> wrappedOptimizer,\n+        IShardingConfiguration<T> config)\n+        : base(wrappedOptimizer, config)\n+    {\n+        // Verify wrapped optimizer supports gradient operations\n+        if (wrappedOptimizer is not IGradientBasedOptimizer<T, TInput, TOutput>)\n+        {\n+            throw new ArgumentException(\n+                $\"DDP requires a gradient-based optimizer, but received {wrappedOptimizer.GetType().Name}. \" +\n+                \"Use gradient-based optimizers like SGD, Adam, RMSprop, etc.\",\n+                nameof(wrappedOptimizer));\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public override OptimizationResult<T, TInput, TOutput> Optimize(OptimizationInputData<T, TInput, TOutput> inputData)\n+    {\n+        if (inputData == null)\n+            throw new ArgumentNullException(nameof(inputData));\n+\n+        var gradientOptimizer = (IGradientBasedOptimizer<T, TInput, TOutput>)WrappedOptimizer;\n+\n+        // Barrier to ensure all processes start together\n+        Config.CommunicationBackend.Barrier();\n+\n+        // Step 1: Optimize locally to compute gradients (and apply them locally)\n+        var localResult = WrappedOptimizer.Optimize(inputData);\n+\n+        // Step 2: Synchronize gradients across all workers\n+        if (Config.AutoSyncGradients && localResult.BestSolution != null)\n+        {\n+            var localGradients = gradientOptimizer.LastComputedGradients;\n+\n+            if (localGradients != null && localGradients.Length > 0)\n+            {\n+                // Get parameters after local gradient application\n+                var updatedParams = localResult.BestSolution.GetParameters();\n+\n+                // Reverse the local update to get original parameters\n+                var originalParams = ComputeOriginalParameters(updatedParams, localGradients);\n+\n+                // Average gradients across all workers (true DDP)\n+                Config.CommunicationBackend.AllReduce(localGradients, ReductionOperation.Average);\n+\n+                // Apply averaged gradients to original parameters\n+                // This ensures all workers apply the same averaged gradients\n+                var finalModel = gradientOptimizer.ApplyGradients(localGradients, localResult.BestSolution);\n+\n+                // Update result with model using averaged gradients\n+                localResult.BestSolution = finalModel;"}]}},{"id":"PRRT_kwDOKSXUF85hNV-K","isResolved":false,"isOutdated":false,"path":"src/DistributedTraining/GlooCommunicationBackend.cs","line":137,"originalLine":129,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VeGGH","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Fallback never engages when Gloo is present**\n\nAs soon as `Type.GetType(\"Gloo.Context, GlooSharp\")` succeeds you set `_useNativeTCP = false` and immediately throw `NotImplementedException`. That propagates out of `OnInitialize`, so multi-rank initialization aborts instead of dropping into the TCP fallback. Anyone with Gloo assemblies deployed will be unable to start training. Keep the warning, but swallow the condition and mark `glooAvailable = false` so the code below can flip `_useNativeTCP = true` and continue with TCP.\n\n\n\n```diff\n-                Console.WriteLine($\"GlooCommunicationBackend: Gloo library detected for {_worldSize} processes.\");\n-                ...\n-                throw new NotImplementedException(\n-                    \"GlooCommunicationBackend with Gloo library support is not yet fully implemented.\\n\\n\" +\n-                    \"...Using TCP fallback instead.\");\n+                Console.WriteLine($\"GlooCommunicationBackend: Gloo library detected for {_worldSize} processes.\");\n+                Console.WriteLine(\"WARNING: Native Gloo support not yet implemented. Falling back to TCP.\");\n+                glooAvailable = false;\n```\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/DistributedTraining/GlooCommunicationBackend.cs around lines 96 to 129,\nthe current logic detects the Gloo assembly then sets _useNativeTCP = false and\nthrows NotImplementedException which aborts initialization and prevents the TCP\nfallback; instead, remove the throw, keep the diagnostic Console.WriteLine (or\nreplace it with a warning log), set glooAvailable = false (or explicitly\n_useNativeTCP = true) so execution continues past the try/catch and the existing\nTCP fallback code runs, and include a clear warning message that Gloo was\ndetected but native TCP will be used as a fallback.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","createdAt":"2025-11-09T08:20:16Z","path":"src/DistributedTraining/GlooCommunicationBackend.cs","line":137,"originalLine":129,"diffHunk":"@@ -0,0 +1,860 @@\n+using System;\n+using System.Linq;\n+using System.Net;\n+using System.Net.Sockets;\n+using System.Collections.Generic;\n+using System.Threading;\n+using System.IO;\n+using AiDotNet.LinearAlgebra;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Gloo-based communication backend for CPU-based collective operations.\n+/// </summary>\n+/// <remarks>\n+/// <para><b>Overview:</b>\n+/// Gloo is Facebook's collective communications library optimized for both CPUs and GPUs.\n+/// It provides efficient implementations of collective operations for CPU-based training\n+/// or heterogeneous environments. Gloo is particularly well-suited for training on CPUs\n+/// or mixed CPU/GPU clusters where NCCL may not be available or optimal.\n+/// </para>\n+/// <para><b>Features:</b>\n+/// - CPU-optimized collective operations\n+/// - Supports TCP, InfiniBand via ibverbs\n+/// - Works on both CPUs and GPUs\n+/// - Cross-platform (Linux, macOS, Windows)\n+/// - Used by PyTorch's distributed package\n+/// </para>\n+/// <para><b>Use Cases:</b>\n+/// - CPU-based distributed training\n+/// - Heterogeneous clusters (mixed CPU/GPU)\n+/// - When NCCL is not available (non-NVIDIA hardware, macOS, etc.)\n+/// - Development and testing on laptops/workstations\n+/// - Production training on CPU clusters\n+/// </para>\n+/// <para><b>Requirements:</b>\n+/// - Gloo library (C++)\n+/// - .NET bindings for Gloo (custom P/Invoke or wrapper library)\n+/// - Network connectivity between workers (TCP/IP or InfiniBand)\n+/// </para>\n+/// <para><b>Graceful Degradation:</b>\n+/// If Gloo library is not available, this backend provides a production-ready TCP-based\n+/// implementation of collective operations using industry-standard ring algorithms.\n+/// This fallback provides full functionality without external dependencies.\n+/// </para>\n+/// </remarks>\n+/// <typeparam name=\"T\">The numeric type for operations</typeparam>\n+public class GlooCommunicationBackend<T> : CommunicationBackendBase<T>\n+{\n+    private readonly int _rank;\n+    private readonly int _worldSize;\n+    private bool _useNativeTCP;\n+    private Dictionary<int, TcpClient>? _tcpConnections;\n+    private TcpListener? _tcpListener;\n+    private readonly object _connectionLock = new object();\n+\n+    /// <summary>\n+    /// Creates a new Gloo communication backend.\n+    /// </summary>\n+    /// <param name=\"rank\">This process's rank</param>\n+    /// <param name=\"worldSize\">Total number of processes</param>\n+    /// <remarks>\n+    /// Transport type selection (TCP vs InfiniBand) is not yet implemented.\n+    /// Currently defaults to TCP-based communication when Gloo library is unavailable.\n+    /// </remarks>\n+    public GlooCommunicationBackend(int rank = 0, int worldSize = 1)\n+    {\n+        _rank = rank;\n+        _worldSize = worldSize;\n+        _useNativeTCP = false;\n+    }\n+\n+    /// <inheritdoc/>\n+    public override int Rank => _rank;\n+\n+    /// <inheritdoc/>\n+    public override int WorldSize => _worldSize;\n+\n+    /// <inheritdoc/>\n+    protected override void OnInitialize()\n+    {\n+        _tcpConnections = new Dictionary<int, TcpClient>();\n+\n+        // Single-process mode: No communication infrastructure needed\n+        if (_worldSize == 1)\n+        {\n+            _useNativeTCP = false;\n+            Console.WriteLine(\"GlooCommunicationBackend: Single-process mode (worldSize=1).\");\n+            return;\n+        }\n+\n+        // Multi-process mode: Try Gloo library first, fallback to TCP\n+        bool glooAvailable = false;\n+        try\n+        {\n+            var glooType = Type.GetType(\"Gloo.Context, GlooSharp\");\n+            if (glooType != null)\n+            {\n+                glooAvailable = true;\n+                _useNativeTCP = false;\n+                Console.WriteLine($\"GlooCommunicationBackend: Gloo library detected for {_worldSize} processes.\");\n+\n+                // TODO: Full Gloo initialization requires:\n+                // 1. Creating transport device (TCP or ibverbs): device = new TcpDevice()\n+                // 2. Creating rendezvous store: store = new FileStore() or RedisStore()\n+                // 3. Creating Gloo context: context = new Context(rank, size)\n+                // 4. Connecting to all other ranks via rendezvous\n+\n+                throw new NotImplementedException(\n+                    \"GlooCommunicationBackend with Gloo library support is not yet fully implemented.\\n\\n\" +\n+                    \"Full Gloo initialization requires:\\n\" +\n+                    \"- GlooSharp P/Invoke bindings for Gloo C++ library\\n\" +\n+                    \"- Rendezvous infrastructure (file-based or Redis)\\n\" +\n+                    \"- Transport device configuration (TCP or InfiniBand)\\n\\n\" +\n+                    \"Using TCP fallback instead.\");\n+            }\n+        }\n+        catch (TypeLoadException)\n+        {\n+            glooAvailable = false;\n+        }\n+\n+        // Fallback to native TCP implementation\n+        if (!glooAvailable)\n+        {\n+            _useNativeTCP = true;\n+            Console.WriteLine($\"GlooCommunicationBackend: Using TCP fallback for {_worldSize} processes.\");\n+            InitializeTCPConnections();\n+        }"}]}},{"id":"PRRT_kwDOKSXUF85hNV-L","isResolved":false,"isOutdated":false,"path":"src/DistributedTraining/GlooCommunicationBackend.cs","line":223,"originalLine":215,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VeGGJ","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Handshake closes sockets before you ever use them**\n\nBoth `ConnectToRank` and `AcceptConnectionFromRank` wrap `client.GetStream()` in a `using` block. Disposing the `NetworkStream` disposes the underlying socket, so the connection is dead before it is stored in `_tcpConnections`. Every subsequent collective will fail with `ObjectDisposedException` or ‚ÄúNo TCP connection‚Äù errors. Keep the handshake writers/readers but leave the stream open‚Äîe.g. request the stream once, create the reader/writer with `leaveOpen: true`, and dispose only the writer/reader.\n\n\n\n```diff\n-        using (var stream = client.GetStream())\n-        using (var writer = new BinaryWriter(stream))\n+        var stream = client.GetStream();\n+        using (var writer = new BinaryWriter(stream, System.Text.Encoding.UTF8, leaveOpen: true))\n         {\n             writer.Write(_rank);\n             writer.Flush();\n         }\n```\n\n```diff\n-        using (var stream = client.GetStream())\n-        using (var reader = new BinaryReader(stream))\n+        var stream = client.GetStream();\n+        using (var reader = new BinaryReader(stream, System.Text.Encoding.UTF8, leaveOpen: true))\n         {\n             receivedRank = reader.ReadInt32();\n         }\n```\n\n\nAlso applies to: 244-248\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/DistributedTraining/GlooCommunicationBackend.cs around lines 197-215\n(also apply same fix to 244-248), the current handshake disposes the\nNetworkStream by using \"using\" on client.GetStream(), which closes the\nunderlying socket before storing the TcpClient in _tcpConnections; change the\npattern to obtain the NetworkStream without disposing it, create\nBinaryWriter/BinaryReader with leaveOpen: true (or otherwise avoid disposing the\nstream) so only the reader/writer is disposed, and ensure you do not wrap the\nstream in a using that would close it; keep the writer/reader for the handshake,\ncall Flush, dispose the writer/reader (or use a using for them with\nleaveOpen:true), and then store the TcpClient in _tcpConnections while leaving\nthe stream/socket open for future use.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","createdAt":"2025-11-09T08:20:17Z","path":"src/DistributedTraining/GlooCommunicationBackend.cs","line":223,"originalLine":215,"diffHunk":"@@ -0,0 +1,860 @@\n+using System;\n+using System.Linq;\n+using System.Net;\n+using System.Net.Sockets;\n+using System.Collections.Generic;\n+using System.Threading;\n+using System.IO;\n+using AiDotNet.LinearAlgebra;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Gloo-based communication backend for CPU-based collective operations.\n+/// </summary>\n+/// <remarks>\n+/// <para><b>Overview:</b>\n+/// Gloo is Facebook's collective communications library optimized for both CPUs and GPUs.\n+/// It provides efficient implementations of collective operations for CPU-based training\n+/// or heterogeneous environments. Gloo is particularly well-suited for training on CPUs\n+/// or mixed CPU/GPU clusters where NCCL may not be available or optimal.\n+/// </para>\n+/// <para><b>Features:</b>\n+/// - CPU-optimized collective operations\n+/// - Supports TCP, InfiniBand via ibverbs\n+/// - Works on both CPUs and GPUs\n+/// - Cross-platform (Linux, macOS, Windows)\n+/// - Used by PyTorch's distributed package\n+/// </para>\n+/// <para><b>Use Cases:</b>\n+/// - CPU-based distributed training\n+/// - Heterogeneous clusters (mixed CPU/GPU)\n+/// - When NCCL is not available (non-NVIDIA hardware, macOS, etc.)\n+/// - Development and testing on laptops/workstations\n+/// - Production training on CPU clusters\n+/// </para>\n+/// <para><b>Requirements:</b>\n+/// - Gloo library (C++)\n+/// - .NET bindings for Gloo (custom P/Invoke or wrapper library)\n+/// - Network connectivity between workers (TCP/IP or InfiniBand)\n+/// </para>\n+/// <para><b>Graceful Degradation:</b>\n+/// If Gloo library is not available, this backend provides a production-ready TCP-based\n+/// implementation of collective operations using industry-standard ring algorithms.\n+/// This fallback provides full functionality without external dependencies.\n+/// </para>\n+/// </remarks>\n+/// <typeparam name=\"T\">The numeric type for operations</typeparam>\n+public class GlooCommunicationBackend<T> : CommunicationBackendBase<T>\n+{\n+    private readonly int _rank;\n+    private readonly int _worldSize;\n+    private bool _useNativeTCP;\n+    private Dictionary<int, TcpClient>? _tcpConnections;\n+    private TcpListener? _tcpListener;\n+    private readonly object _connectionLock = new object();\n+\n+    /// <summary>\n+    /// Creates a new Gloo communication backend.\n+    /// </summary>\n+    /// <param name=\"rank\">This process's rank</param>\n+    /// <param name=\"worldSize\">Total number of processes</param>\n+    /// <remarks>\n+    /// Transport type selection (TCP vs InfiniBand) is not yet implemented.\n+    /// Currently defaults to TCP-based communication when Gloo library is unavailable.\n+    /// </remarks>\n+    public GlooCommunicationBackend(int rank = 0, int worldSize = 1)\n+    {\n+        _rank = rank;\n+        _worldSize = worldSize;\n+        _useNativeTCP = false;\n+    }\n+\n+    /// <inheritdoc/>\n+    public override int Rank => _rank;\n+\n+    /// <inheritdoc/>\n+    public override int WorldSize => _worldSize;\n+\n+    /// <inheritdoc/>\n+    protected override void OnInitialize()\n+    {\n+        _tcpConnections = new Dictionary<int, TcpClient>();\n+\n+        // Single-process mode: No communication infrastructure needed\n+        if (_worldSize == 1)\n+        {\n+            _useNativeTCP = false;\n+            Console.WriteLine(\"GlooCommunicationBackend: Single-process mode (worldSize=1).\");\n+            return;\n+        }\n+\n+        // Multi-process mode: Try Gloo library first, fallback to TCP\n+        bool glooAvailable = false;\n+        try\n+        {\n+            var glooType = Type.GetType(\"Gloo.Context, GlooSharp\");\n+            if (glooType != null)\n+            {\n+                glooAvailable = true;\n+                _useNativeTCP = false;\n+                Console.WriteLine($\"GlooCommunicationBackend: Gloo library detected for {_worldSize} processes.\");\n+\n+                // TODO: Full Gloo initialization requires:\n+                // 1. Creating transport device (TCP or ibverbs): device = new TcpDevice()\n+                // 2. Creating rendezvous store: store = new FileStore() or RedisStore()\n+                // 3. Creating Gloo context: context = new Context(rank, size)\n+                // 4. Connecting to all other ranks via rendezvous\n+\n+                throw new NotImplementedException(\n+                    \"GlooCommunicationBackend with Gloo library support is not yet fully implemented.\\n\\n\" +\n+                    \"Full Gloo initialization requires:\\n\" +\n+                    \"- GlooSharp P/Invoke bindings for Gloo C++ library\\n\" +\n+                    \"- Rendezvous infrastructure (file-based or Redis)\\n\" +\n+                    \"- Transport device configuration (TCP or InfiniBand)\\n\\n\" +\n+                    \"Using TCP fallback instead.\");\n+            }\n+        }\n+        catch (TypeLoadException)\n+        {\n+            glooAvailable = false;\n+        }\n+\n+        // Fallback to native TCP implementation\n+        if (!glooAvailable)\n+        {\n+            _useNativeTCP = true;\n+            Console.WriteLine($\"GlooCommunicationBackend: Using TCP fallback for {_worldSize} processes.\");\n+            InitializeTCPConnections();\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Initializes TCP connections between all ranks.\n+    /// </summary>\n+    /// <remarks>\n+    /// Uses environment variables for rendezvous:\n+    /// - AIDOTNET_MASTER_ADDR: IP address of rank 0\n+    /// - AIDOTNET_MASTER_PORT: Base port for rank 0\n+    /// Each rank listens on MASTER_PORT + rank\n+    /// </remarks>\n+    private void InitializeTCPConnections()\n+    {\n+        string? masterAddr = Environment.GetEnvironmentVariable(\"AIDOTNET_MASTER_ADDR\");\n+        string? masterPortStr = Environment.GetEnvironmentVariable(\"AIDOTNET_MASTER_PORT\");\n+\n+        if (string.IsNullOrEmpty(masterAddr) || string.IsNullOrEmpty(masterPortStr))\n+        {\n+            throw new InvalidOperationException(\n+                \"GlooCommunicationBackend TCP mode requires environment variables:\\n\" +\n+                \"- AIDOTNET_MASTER_ADDR: IP address of rank 0 (e.g., 192.168.1.10 or localhost)\\n\" +\n+                \"- AIDOTNET_MASTER_PORT: Base port number (e.g., 29500)\\n\" +\n+                \"Each rank will use port = MASTER_PORT + rank\");\n+        }\n+\n+        if (!int.TryParse(masterPortStr, out int basePort))\n+        {\n+            throw new InvalidOperationException($\"Invalid AIDOTNET_MASTER_PORT: {masterPortStr}\");\n+        }\n+\n+        // Start TCP listener on this rank's port\n+        int myPort = basePort + _rank;\n+        _tcpListener = new TcpListener(IPAddress.Any, myPort);\n+        _tcpListener.Start();\n+        Console.WriteLine($\"Rank {_rank}: TCP listener started on port {myPort}\");\n+\n+        // Connect to all ranks with lower rank numbers (they are already listening)\n+        for (int otherRank = 0; otherRank < _rank; otherRank++)\n+        {\n+            ConnectToRank(otherRank, masterAddr, basePort);\n+        }\n+\n+        // Accept connections from all ranks with higher rank numbers\n+        for (int otherRank = _rank + 1; otherRank < _worldSize; otherRank++)\n+        {\n+            AcceptConnectionFromRank(otherRank);\n+        }\n+\n+        Console.WriteLine($\"Rank {_rank}: All TCP connections established ({_tcpConnections?.Count ?? 0} peers)\");\n+    }\n+\n+    /// <summary>\n+    /// Connects to a specific rank (active connection).\n+    /// </summary>\n+    private void ConnectToRank(int targetRank, string masterAddr, int basePort)\n+    {\n+        int targetPort = basePort + targetRank;\n+        int maxRetries = 10;\n+        int retryDelayMs = 1000;\n+\n+        for (int attempt = 0; attempt < maxRetries; attempt++)\n+        {\n+            try\n+            {\n+                var client = new TcpClient();\n+                client.Connect(masterAddr, targetPort);\n+\n+                // Send handshake: my rank\n+                using (var stream = client.GetStream())\n+                using (var writer = new BinaryWriter(stream))\n+                {\n+                    writer.Write(_rank);\n+                    writer.Flush();\n+                }\n+\n+                lock (_connectionLock)\n+                {\n+                    if (_tcpConnections != null)\n+                    {\n+                        _tcpConnections[targetRank] = client;\n+                    }\n+                }\n+\n+                Console.WriteLine($\"Rank {_rank}: Connected to rank {targetRank} at {masterAddr}:{targetPort}\");\n+                return;\n+            }"}]}},{"id":"PRRT_kwDOKSXUF85hNV-M","isResolved":false,"isOutdated":false,"path":"src/DistributedTraining/GlooCommunicationBackend.cs","line":708,"originalLine":700,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VeGGL","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Scatter chunk-length read is broken**\n\n`ReceiveData(parentAbsolute, 1)` is invoked twice: once in the ternary condition and again inside `Convert.ToInt32(...)`. The first call consumes the single-element ‚Äúchunk size‚Äù message, so the second call blocks waiting for another length message and then blows up when it sees the following chunk payload (length ‚â† 1). Cache the first result and convert it; do not re-issue the read.\n\n\n\n```diff\n-            chunkSize = ReceiveData(parentAbsolute, 1)[0] != null ?\n-                        Convert.ToInt32(ReceiveData(parentAbsolute, 1)[0]) : 0;\n+            var sizeMessage = ReceiveData(parentAbsolute, 1);\n+            chunkSize = Convert.ToInt32(sizeMessage[0]);\n```\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/DistributedTraining/GlooCommunicationBackend.cs around lines 696 to 700,\nthe code calls ReceiveData(parentAbsolute, 1) twice which consumes the\nsingle-element length message on the first call and blocks/errs on the second;\nfix by calling ReceiveData(parentAbsolute, 1) once, store the returned array in\na local variable, check its element for null, convert that single cached element\nto int to set chunkSize, then call ReceiveData(parentAbsolute, chunkSize) once\nto read the payload.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","createdAt":"2025-11-09T08:20:17Z","path":"src/DistributedTraining/GlooCommunicationBackend.cs","line":708,"originalLine":700,"diffHunk":"@@ -0,0 +1,860 @@\n+using System;\n+using System.Linq;\n+using System.Net;\n+using System.Net.Sockets;\n+using System.Collections.Generic;\n+using System.Threading;\n+using System.IO;\n+using AiDotNet.LinearAlgebra;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Gloo-based communication backend for CPU-based collective operations.\n+/// </summary>\n+/// <remarks>\n+/// <para><b>Overview:</b>\n+/// Gloo is Facebook's collective communications library optimized for both CPUs and GPUs.\n+/// It provides efficient implementations of collective operations for CPU-based training\n+/// or heterogeneous environments. Gloo is particularly well-suited for training on CPUs\n+/// or mixed CPU/GPU clusters where NCCL may not be available or optimal.\n+/// </para>\n+/// <para><b>Features:</b>\n+/// - CPU-optimized collective operations\n+/// - Supports TCP, InfiniBand via ibverbs\n+/// - Works on both CPUs and GPUs\n+/// - Cross-platform (Linux, macOS, Windows)\n+/// - Used by PyTorch's distributed package\n+/// </para>\n+/// <para><b>Use Cases:</b>\n+/// - CPU-based distributed training\n+/// - Heterogeneous clusters (mixed CPU/GPU)\n+/// - When NCCL is not available (non-NVIDIA hardware, macOS, etc.)\n+/// - Development and testing on laptops/workstations\n+/// - Production training on CPU clusters\n+/// </para>\n+/// <para><b>Requirements:</b>\n+/// - Gloo library (C++)\n+/// - .NET bindings for Gloo (custom P/Invoke or wrapper library)\n+/// - Network connectivity between workers (TCP/IP or InfiniBand)\n+/// </para>\n+/// <para><b>Graceful Degradation:</b>\n+/// If Gloo library is not available, this backend provides a production-ready TCP-based\n+/// implementation of collective operations using industry-standard ring algorithms.\n+/// This fallback provides full functionality without external dependencies.\n+/// </para>\n+/// </remarks>\n+/// <typeparam name=\"T\">The numeric type for operations</typeparam>\n+public class GlooCommunicationBackend<T> : CommunicationBackendBase<T>\n+{\n+    private readonly int _rank;\n+    private readonly int _worldSize;\n+    private bool _useNativeTCP;\n+    private Dictionary<int, TcpClient>? _tcpConnections;\n+    private TcpListener? _tcpListener;\n+    private readonly object _connectionLock = new object();\n+\n+    /// <summary>\n+    /// Creates a new Gloo communication backend.\n+    /// </summary>\n+    /// <param name=\"rank\">This process's rank</param>\n+    /// <param name=\"worldSize\">Total number of processes</param>\n+    /// <remarks>\n+    /// Transport type selection (TCP vs InfiniBand) is not yet implemented.\n+    /// Currently defaults to TCP-based communication when Gloo library is unavailable.\n+    /// </remarks>\n+    public GlooCommunicationBackend(int rank = 0, int worldSize = 1)\n+    {\n+        _rank = rank;\n+        _worldSize = worldSize;\n+        _useNativeTCP = false;\n+    }\n+\n+    /// <inheritdoc/>\n+    public override int Rank => _rank;\n+\n+    /// <inheritdoc/>\n+    public override int WorldSize => _worldSize;\n+\n+    /// <inheritdoc/>\n+    protected override void OnInitialize()\n+    {\n+        _tcpConnections = new Dictionary<int, TcpClient>();\n+\n+        // Single-process mode: No communication infrastructure needed\n+        if (_worldSize == 1)\n+        {\n+            _useNativeTCP = false;\n+            Console.WriteLine(\"GlooCommunicationBackend: Single-process mode (worldSize=1).\");\n+            return;\n+        }\n+\n+        // Multi-process mode: Try Gloo library first, fallback to TCP\n+        bool glooAvailable = false;\n+        try\n+        {\n+            var glooType = Type.GetType(\"Gloo.Context, GlooSharp\");\n+            if (glooType != null)\n+            {\n+                glooAvailable = true;\n+                _useNativeTCP = false;\n+                Console.WriteLine($\"GlooCommunicationBackend: Gloo library detected for {_worldSize} processes.\");\n+\n+                // TODO: Full Gloo initialization requires:\n+                // 1. Creating transport device (TCP or ibverbs): device = new TcpDevice()\n+                // 2. Creating rendezvous store: store = new FileStore() or RedisStore()\n+                // 3. Creating Gloo context: context = new Context(rank, size)\n+                // 4. Connecting to all other ranks via rendezvous\n+\n+                throw new NotImplementedException(\n+                    \"GlooCommunicationBackend with Gloo library support is not yet fully implemented.\\n\\n\" +\n+                    \"Full Gloo initialization requires:\\n\" +\n+                    \"- GlooSharp P/Invoke bindings for Gloo C++ library\\n\" +\n+                    \"- Rendezvous infrastructure (file-based or Redis)\\n\" +\n+                    \"- Transport device configuration (TCP or InfiniBand)\\n\\n\" +\n+                    \"Using TCP fallback instead.\");\n+            }\n+        }\n+        catch (TypeLoadException)\n+        {\n+            glooAvailable = false;\n+        }\n+\n+        // Fallback to native TCP implementation\n+        if (!glooAvailable)\n+        {\n+            _useNativeTCP = true;\n+            Console.WriteLine($\"GlooCommunicationBackend: Using TCP fallback for {_worldSize} processes.\");\n+            InitializeTCPConnections();\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Initializes TCP connections between all ranks.\n+    /// </summary>\n+    /// <remarks>\n+    /// Uses environment variables for rendezvous:\n+    /// - AIDOTNET_MASTER_ADDR: IP address of rank 0\n+    /// - AIDOTNET_MASTER_PORT: Base port for rank 0\n+    /// Each rank listens on MASTER_PORT + rank\n+    /// </remarks>\n+    private void InitializeTCPConnections()\n+    {\n+        string? masterAddr = Environment.GetEnvironmentVariable(\"AIDOTNET_MASTER_ADDR\");\n+        string? masterPortStr = Environment.GetEnvironmentVariable(\"AIDOTNET_MASTER_PORT\");\n+\n+        if (string.IsNullOrEmpty(masterAddr) || string.IsNullOrEmpty(masterPortStr))\n+        {\n+            throw new InvalidOperationException(\n+                \"GlooCommunicationBackend TCP mode requires environment variables:\\n\" +\n+                \"- AIDOTNET_MASTER_ADDR: IP address of rank 0 (e.g., 192.168.1.10 or localhost)\\n\" +\n+                \"- AIDOTNET_MASTER_PORT: Base port number (e.g., 29500)\\n\" +\n+                \"Each rank will use port = MASTER_PORT + rank\");\n+        }\n+\n+        if (!int.TryParse(masterPortStr, out int basePort))\n+        {\n+            throw new InvalidOperationException($\"Invalid AIDOTNET_MASTER_PORT: {masterPortStr}\");\n+        }\n+\n+        // Start TCP listener on this rank's port\n+        int myPort = basePort + _rank;\n+        _tcpListener = new TcpListener(IPAddress.Any, myPort);\n+        _tcpListener.Start();\n+        Console.WriteLine($\"Rank {_rank}: TCP listener started on port {myPort}\");\n+\n+        // Connect to all ranks with lower rank numbers (they are already listening)\n+        for (int otherRank = 0; otherRank < _rank; otherRank++)\n+        {\n+            ConnectToRank(otherRank, masterAddr, basePort);\n+        }\n+\n+        // Accept connections from all ranks with higher rank numbers\n+        for (int otherRank = _rank + 1; otherRank < _worldSize; otherRank++)\n+        {\n+            AcceptConnectionFromRank(otherRank);\n+        }\n+\n+        Console.WriteLine($\"Rank {_rank}: All TCP connections established ({_tcpConnections?.Count ?? 0} peers)\");\n+    }\n+\n+    /// <summary>\n+    /// Connects to a specific rank (active connection).\n+    /// </summary>\n+    private void ConnectToRank(int targetRank, string masterAddr, int basePort)\n+    {\n+        int targetPort = basePort + targetRank;\n+        int maxRetries = 10;\n+        int retryDelayMs = 1000;\n+\n+        for (int attempt = 0; attempt < maxRetries; attempt++)\n+        {\n+            try\n+            {\n+                var client = new TcpClient();\n+                client.Connect(masterAddr, targetPort);\n+\n+                // Send handshake: my rank\n+                using (var stream = client.GetStream())\n+                using (var writer = new BinaryWriter(stream))\n+                {\n+                    writer.Write(_rank);\n+                    writer.Flush();\n+                }\n+\n+                lock (_connectionLock)\n+                {\n+                    if (_tcpConnections != null)\n+                    {\n+                        _tcpConnections[targetRank] = client;\n+                    }\n+                }\n+\n+                Console.WriteLine($\"Rank {_rank}: Connected to rank {targetRank} at {masterAddr}:{targetPort}\");\n+                return;\n+            }\n+            catch (SocketException)\n+            {\n+                if (attempt < maxRetries - 1)\n+                {\n+                    Thread.Sleep(retryDelayMs);\n+                }\n+                else\n+                {\n+                    throw new InvalidOperationException(\n+                        $\"Rank {_rank}: Failed to connect to rank {targetRank} at {masterAddr}:{targetPort} after {maxRetries} attempts\");\n+                }\n+            }\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Accepts connection from a specific rank (passive connection).\n+    /// </summary>\n+    private void AcceptConnectionFromRank(int expectedRank)\n+    {\n+        if (_tcpListener == null)\n+        {\n+            throw new InvalidOperationException(\"TCP listener not initialized\");\n+        }\n+\n+        // Accept incoming connection\n+        var client = _tcpListener.AcceptTcpClient();\n+\n+        // Read handshake to verify rank\n+        int receivedRank;\n+        using (var stream = client.GetStream())\n+        using (var reader = new BinaryReader(stream))\n+        {\n+            receivedRank = reader.ReadInt32();\n+        }\n+\n+        if (receivedRank != expectedRank)\n+        {\n+            client.Close();\n+            throw new InvalidOperationException(\n+                $\"Rank {_rank}: Expected connection from rank {expectedRank}, but received from rank {receivedRank}\");\n+        }\n+\n+        lock (_connectionLock)\n+        {\n+            if (_tcpConnections != null)\n+            {\n+                _tcpConnections[receivedRank] = client;\n+            }\n+        }\n+\n+        Console.WriteLine($\"Rank {_rank}: Accepted connection from rank {receivedRank}\");\n+    }\n+\n+    /// <inheritdoc/>\n+    protected override void OnShutdown()\n+    {\n+        if (_useNativeTCP && _tcpConnections != null)\n+        {\n+            lock (_connectionLock)\n+            {\n+                foreach (var connection in _tcpConnections.Values)\n+                {\n+                    try\n+                    {\n+                        connection.Close();\n+                    }\n+                    catch (Exception ex)\n+                    {\n+                        Console.WriteLine($\"Warning: Error closing TCP connection: {ex.Message}\");\n+                    }\n+                }\n+                _tcpConnections.Clear();\n+            }\n+\n+            if (_tcpListener != null)\n+            {\n+                try\n+                {\n+                    _tcpListener.Stop();\n+                }\n+                catch (Exception ex)\n+                {\n+                    Console.WriteLine($\"Warning: Error stopping TCP listener: {ex.Message}\");\n+                }\n+                _tcpListener = null;\n+            }\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public override void Barrier()\n+    {\n+        EnsureInitialized();\n+\n+        if (_worldSize == 1)\n+        {\n+            // Single-process: barrier is a no-op\n+            return;\n+        }\n+\n+        if (!_useNativeTCP)\n+        {\n+            throw new InvalidOperationException(\"Barrier requires TCP mode to be initialized\");\n+        }\n+\n+        // Simple all-to-all barrier implementation\n+        // Each rank sends a signal to all other ranks and waits for signals from all\n+        var signal = new[] { NumOps.One };\n+\n+        // Send signal to all other ranks\n+        for (int otherRank = 0; otherRank < _worldSize; otherRank++)\n+        {\n+            if (otherRank != _rank)\n+            {\n+                SendData(otherRank, signal);\n+            }\n+        }\n+\n+        // Receive signal from all other ranks\n+        for (int otherRank = 0; otherRank < _worldSize; otherRank++)\n+        {\n+            if (otherRank != _rank)\n+            {\n+                ReceiveData(otherRank, 1);\n+            }\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public override void AllReduce(Vector<T> data, ReductionOperation operation)\n+    {\n+        EnsureInitialized();\n+        ValidateData(data, nameof(data));\n+\n+        if (_worldSize == 1)\n+        {\n+            // Single-process: data already contains the result (no-op)\n+            return;\n+        }\n+\n+        if (!_useNativeTCP)\n+        {\n+            throw new InvalidOperationException(\"AllReduce requires TCP mode to be initialized\");\n+        }\n+\n+        PerformRingAllReduce(data, operation);\n+    }\n+\n+    /// <inheritdoc/>\n+    public override Vector<T> AllGather(Vector<T> sendData)\n+    {\n+        EnsureInitialized();\n+        ValidateData(sendData, nameof(sendData));\n+\n+        if (_worldSize == 1)\n+        {\n+            return sendData.Clone();\n+        }\n+\n+        if (!_useNativeTCP)\n+        {\n+            throw new InvalidOperationException(\"AllGather requires TCP mode to be initialized\");\n+        }\n+\n+        return PerformRingAllGather(sendData);\n+    }\n+\n+    /// <inheritdoc/>\n+    public override Vector<T> Broadcast(Vector<T> data, int root = 0)\n+    {\n+        EnsureInitialized();\n+        ValidateData(data, nameof(data));\n+        ValidateRoot(root);\n+\n+        if (_worldSize == 1)\n+        {\n+            return data.Clone();\n+        }\n+\n+        if (!_useNativeTCP)\n+        {\n+            throw new InvalidOperationException(\"Broadcast requires TCP mode to be initialized\");\n+        }\n+\n+        return PerformTreeBroadcast(data, root);\n+    }\n+\n+    /// <inheritdoc/>\n+    public override Vector<T> Scatter(Vector<T> sendData, int root = 0)\n+    {\n+        EnsureInitialized();\n+        ValidateRoot(root);\n+\n+        if (_worldSize == 1)\n+        {\n+            if (Rank == root)\n+            {\n+                ValidateData(sendData, nameof(sendData));\n+                return sendData.Clone();\n+            }\n+            return new Vector<T>(Array.Empty<T>());\n+        }\n+\n+        if (!_useNativeTCP)\n+        {\n+            throw new InvalidOperationException(\"Scatter requires TCP mode to be initialized\");\n+        }\n+\n+        return PerformTreeScatter(sendData, root);\n+    }\n+\n+    /// <inheritdoc/>\n+    public override Vector<T> ReduceScatter(Vector<T> data, ReductionOperation operation)\n+    {\n+        EnsureInitialized();\n+        ValidateData(data, nameof(data));\n+\n+        if (data.Length % _worldSize != 0)\n+        {\n+            throw new ArgumentException(\n+                $\"Data length {data.Length} must be divisible by world size {_worldSize}.\");\n+        }\n+\n+        if (_worldSize == 1)\n+        {\n+            return data.Clone();\n+        }\n+\n+        if (!_useNativeTCP)\n+        {\n+            throw new InvalidOperationException(\"ReduceScatter requires TCP mode to be initialized\");\n+        }\n+\n+        return PerformRingReduceScatter(data, operation);\n+    }\n+\n+    /// <summary>\n+    /// Performs ring-based AllReduce operation.\n+    /// </summary>\n+    /// <remarks>\n+    /// This is a production-ready implementation of the ring AllReduce algorithm\n+    /// used by systems like Baidu's Ring AllReduce and Horovod.\n+    ///\n+    /// Algorithm:\n+    /// 1. Divide data into N chunks (N = worldSize)\n+    /// 2. ReduceScatter phase: Send chunks in ring pattern, reducing as we go\n+    /// 3. AllGather phase: Gather the reduced chunks back to all ranks\n+    ///\n+    /// Time complexity: O(2*(N-1)*M/N) where M is data size\n+    /// This is optimal for large messages and scales linearly with cluster size.\n+    /// </remarks>\n+    private void PerformRingAllReduce(Vector<T> data, ReductionOperation operation)\n+    {\n+        if (_worldSize == 1)\n+        {\n+            return;\n+        }\n+\n+        int chunkSize = (data.Length + _worldSize - 1) / _worldSize; // Ceiling division\n+        int nextRank = (_rank + 1) % _worldSize;\n+        int prevRank = (_rank - 1 + _worldSize) % _worldSize;\n+\n+        var dataArray = data.ToArray();\n+\n+        // Phase 1: ReduceScatter - reduce chunks in ring pattern\n+        for (int step = 0; step < _worldSize - 1; step++)\n+        {\n+            int sendChunkIdx = (_rank - step + _worldSize) % _worldSize;\n+            int recvChunkIdx = (_rank - step - 1 + _worldSize) % _worldSize;\n+\n+            int sendStart = sendChunkIdx * chunkSize;\n+            int sendCount = Math.Min(chunkSize, data.Length - sendStart);\n+            int recvStart = recvChunkIdx * chunkSize;\n+            int recvCount = Math.Min(chunkSize, data.Length - recvStart);\n+\n+            // Extract send chunk\n+            var sendChunk = new T[sendCount];\n+            Array.Copy(dataArray, sendStart, sendChunk, 0, sendCount);\n+\n+            // Send and receive simultaneously\n+            var sendTask = System.Threading.Tasks.Task.Run(() => SendData(nextRank, sendChunk));\n+            var recvChunk = ReceiveData(prevRank, recvCount);\n+            sendTask.Wait();\n+\n+            // Reduce received chunk with local chunk\n+            for (int i = 0; i < recvCount; i++)\n+            {\n+                dataArray[recvStart + i] = PerformReduction(dataArray[recvStart + i], recvChunk[i], operation);\n+            }\n+        }\n+\n+        // Phase 2: AllGather - distribute reduced chunks in ring pattern\n+        for (int step = 0; step < _worldSize - 1; step++)\n+        {\n+            int sendChunkIdx = (_rank - step + 1 + _worldSize) % _worldSize;\n+            int recvChunkIdx = (_rank - step + _worldSize) % _worldSize;\n+\n+            int sendStart = sendChunkIdx * chunkSize;\n+            int sendCount = Math.Min(chunkSize, data.Length - sendStart);\n+            int recvStart = recvChunkIdx * chunkSize;\n+            int recvCount = Math.Min(chunkSize, data.Length - recvStart);\n+\n+            // Extract send chunk\n+            var sendChunk = new T[sendCount];\n+            Array.Copy(dataArray, sendStart, sendChunk, 0, sendCount);\n+\n+            // Send and receive simultaneously\n+            var sendTask = System.Threading.Tasks.Task.Run(() => SendData(nextRank, sendChunk));\n+            var recvChunk = ReceiveData(prevRank, recvCount);\n+            sendTask.Wait();\n+\n+            // Copy received chunk to local data\n+            Array.Copy(recvChunk, 0, dataArray, recvStart, recvCount);\n+        }\n+\n+        // Apply averaging if needed (after all reductions complete)\n+        if (operation == ReductionOperation.Average)\n+        {\n+            for (int i = 0; i < dataArray.Length; i++)\n+            {\n+                dataArray[i] = NumOps.Divide(dataArray[i], NumOps.FromDouble(_worldSize));\n+            }\n+        }\n+\n+        // Update original vector with reduced data\n+        for (int i = 0; i < dataArray.Length; i++)\n+        {\n+            data[i] = dataArray[i];\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs a reduction operation on two values.\n+    /// </summary>\n+    private T PerformReduction(T a, T b, ReductionOperation operation)\n+    {\n+        return operation switch\n+        {\n+            ReductionOperation.Sum or ReductionOperation.Average => NumOps.Add(a, b),\n+            ReductionOperation.Max => NumOps.GreaterThan(a, b) ? a : b,\n+            ReductionOperation.Min => NumOps.LessThan(a, b) ? a : b,\n+            ReductionOperation.Product => NumOps.Multiply(a, b),\n+            _ => throw new ArgumentException($\"Unsupported reduction operation: {operation}\")\n+        };\n+    }\n+\n+    /// <summary>\n+    /// Performs ring-based AllGather operation.\n+    /// </summary>\n+    private Vector<T> PerformRingAllGather(Vector<T> sendData)\n+    {\n+        if (_worldSize == 1)\n+        {\n+            return sendData.Clone();\n+        }\n+\n+        int chunkSize = sendData.Length;\n+        int nextRank = (_rank + 1) % _worldSize;\n+        int prevRank = (_rank - 1 + _worldSize) % _worldSize;\n+\n+        // Result buffer to hold data from all ranks\n+        var result = new T[chunkSize * _worldSize];\n+\n+        // Copy local data to result buffer\n+        Array.Copy(sendData.ToArray(), 0, result, _rank * chunkSize, chunkSize);\n+\n+        // Ring AllGather: each rank receives chunk from previous rank and forwards it\n+        for (int step = 0; step < _worldSize - 1; step++)\n+        {\n+            int sendChunkIdx = (_rank - step + _worldSize) % _worldSize;\n+            int recvChunkIdx = (_rank - step - 1 + _worldSize) % _worldSize;\n+\n+            // Extract send chunk from result buffer\n+            var sendChunk = new T[chunkSize];\n+            Array.Copy(result, sendChunkIdx * chunkSize, sendChunk, 0, chunkSize);\n+\n+            // Send and receive simultaneously\n+            var sendTask = System.Threading.Tasks.Task.Run(() => SendData(nextRank, sendChunk));\n+            var recvChunk = ReceiveData(prevRank, chunkSize);\n+            sendTask.Wait();\n+\n+            // Copy received chunk to result buffer\n+            Array.Copy(recvChunk, 0, result, recvChunkIdx * chunkSize, chunkSize);\n+        }\n+\n+        return new Vector<T>(result);\n+    }\n+\n+    /// <summary>\n+    /// Performs tree-based Broadcast operation.\n+    /// </summary>\n+    /// <remarks>\n+    /// Uses a binary tree pattern for efficient broadcasting.\n+    /// Time complexity: O(log N) where N is worldSize\n+    /// </remarks>\n+    private Vector<T> PerformTreeBroadcast(Vector<T> data, int root)\n+    {\n+        if (_worldSize == 1)\n+        {\n+            return data.Clone();\n+        }\n+\n+        var dataArray = data.ToArray();\n+\n+        // Adjust ranks relative to root for binary tree calculation\n+        int relativeRank = (_rank - root + _worldSize) % _worldSize;\n+\n+        // If not root, receive data from parent\n+        if (relativeRank != 0)\n+        {\n+            int parentRelative = (relativeRank - 1) / 2;\n+            int parentAbsolute = (parentRelative + root) % _worldSize;\n+            dataArray = ReceiveData(parentAbsolute, data.Length);\n+        }\n+\n+        // Send data to children in binary tree\n+        int leftChildRelative = 2 * relativeRank + 1;\n+        int rightChildRelative = 2 * relativeRank + 2;\n+\n+        if (leftChildRelative < _worldSize)\n+        {\n+            int leftChildAbsolute = (leftChildRelative + root) % _worldSize;\n+            SendData(leftChildAbsolute, dataArray);\n+        }\n+\n+        if (rightChildRelative < _worldSize)\n+        {\n+            int rightChildAbsolute = (rightChildRelative + root) % _worldSize;\n+            SendData(rightChildAbsolute, dataArray);\n+        }\n+\n+        return new Vector<T>(dataArray);\n+    }\n+\n+    /// <summary>\n+    /// Performs tree-based Scatter operation.\n+    /// </summary>\n+    private Vector<T> PerformTreeScatter(Vector<T> sendData, int root)\n+    {\n+        if (_worldSize == 1)\n+        {\n+            if (Rank == root)\n+            {\n+                ValidateData(sendData, nameof(sendData));\n+                return sendData.Clone();\n+            }\n+            return new Vector<T>(Array.Empty<T>());\n+        }\n+\n+        int chunkSize;\n+        T[]? myChunk = null;\n+\n+        if (Rank == root)\n+        {\n+            ValidateData(sendData, nameof(sendData));\n+\n+            if (sendData.Length % _worldSize != 0)\n+            {\n+                throw new ArgumentException(\n+                    $\"Data length {sendData.Length} must be divisible by world size {_worldSize}.\");\n+            }\n+\n+            chunkSize = sendData.Length / _worldSize;\n+            var dataArray = sendData.ToArray();\n+\n+            // Root keeps its own chunk\n+            myChunk = new T[chunkSize];\n+            Array.Copy(dataArray, root * chunkSize, myChunk, 0, chunkSize);\n+\n+            // Send chunks to other ranks using binary tree pattern\n+            ScatterTreeSend(dataArray, chunkSize, root, 0, _worldSize);\n+        }\n+        else\n+        {\n+            // Non-root ranks receive their chunk\n+            int relativeRank = (_rank - root + _worldSize) % _worldSize;\n+            int parentRelative = (relativeRank - 1) / 2;\n+            int parentAbsolute = (parentRelative + root) % _worldSize;\n+\n+            // Receive chunk count first, then data\n+            chunkSize = ReceiveData(parentAbsolute, 1)[0] != null ?\n+                        Convert.ToInt32(ReceiveData(parentAbsolute, 1)[0]) : 0;\n+            myChunk = ReceiveData(parentAbsolute, chunkSize);\n+        }"}]}},{"id":"PRRT_kwDOKSXUF85hNV-O","isResolved":false,"isOutdated":false,"path":"src/DistributedTraining/GlooCommunicationBackend.cs","line":1002,"originalLine":858,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VeGGM","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Send/receive helpers tear down the transport on first call**\n\n`SendData`/`ReceiveData` also wrap `GetStream()` in `using` statements. The first collective disposes the `NetworkStream`, closing the socket and breaking every later operation. Switch to `leaveOpen: true` (or cache per-connection reader/writer instances) so the underlying TCP connection stays alive for the lifetime of the backend.\n\n\n\n```diff\n-        lock (_connectionLock)\n-        {\n-            var client = _tcpConnections[destRank];\n-            using (var stream = client.GetStream())\n-            using (var writer = new BinaryWriter(stream))\n+        lock (_connectionLock)\n+        {\n+            var client = _tcpConnections[destRank];\n+            var stream = client.GetStream();\n+            using (var writer = new BinaryWriter(stream, System.Text.Encoding.UTF8, leaveOpen: true))\n             {\n                 writer.Write(data.Length);\n                 ...\n```\n\n```diff\n-        lock (_connectionLock)\n-        {\n-            var client = _tcpConnections[sourceRank];\n-            using (var stream = client.GetStream())\n-            using (var reader = new BinaryReader(stream))\n+        lock (_connectionLock)\n+        {\n+            var client = _tcpConnections[sourceRank];\n+            var stream = client.GetStream();\n+            using (var reader = new BinaryReader(stream, System.Text.Encoding.UTF8, leaveOpen: true))\n             {\n                 int length = reader.ReadInt32();\n                 ...\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    /// <summary>\n    /// Sends data to a specific rank via TCP.\n    /// </summary>\n    private void SendData(int destRank, T[] data)\n    {\n        if (_tcpConnections == null || !_tcpConnections.ContainsKey(destRank))\n        {\n            throw new InvalidOperationException($\"No TCP connection to rank {destRank}\");\n        }\n\n        lock (_connectionLock)\n        {\n            var client = _tcpConnections[destRank];\n            var stream = client.GetStream();\n            using (var writer = new BinaryWriter(stream, System.Text.Encoding.UTF8, leaveOpen: true))\n            {\n                // Send length header\n                writer.Write(data.Length);\n\n                // Send data elements\n                foreach (var element in data)\n                {\n                    double value = Convert.ToDouble(element);\n                    writer.Write(value);\n                }\n                writer.Flush();\n            }\n        }\n    }\n\n    /// <summary>\n    /// Receives data from a specific rank via TCP.\n    /// </summary>\n    private T[] ReceiveData(int sourceRank, int expectedLength)\n    {\n        if (_tcpConnections == null || !_tcpConnections.ContainsKey(sourceRank))\n        {\n            throw new InvalidOperationException($\"No TCP connection to rank {sourceRank}\");\n        }\n\n        lock (_connectionLock)\n        {\n            var client = _tcpConnections[sourceRank];\n            var stream = client.GetStream();\n            using (var reader = new BinaryReader(stream, System.Text.Encoding.UTF8, leaveOpen: true))\n            {\n                // Read length header\n                int length = reader.ReadInt32();\n                if (length != expectedLength)\n                {\n                    throw new InvalidOperationException(\n                        $\"Rank {_rank}: Expected {expectedLength} elements from rank {sourceRank}, but received {length}\");\n                }\n\n                // Read data elements\n                var result = new T[length];\n                for (int i = 0; i < length; i++)\n                {\n                    double value = reader.ReadDouble();\n                    result[i] = (T)Convert.ChangeType(value, typeof(T));\n                }\n                return result;\n            }\n        }\n    }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","createdAt":"2025-11-09T08:20:17Z","path":"src/DistributedTraining/GlooCommunicationBackend.cs","line":1002,"originalLine":858,"diffHunk":"@@ -0,0 +1,860 @@\n+using System;\n+using System.Linq;\n+using System.Net;\n+using System.Net.Sockets;\n+using System.Collections.Generic;\n+using System.Threading;\n+using System.IO;\n+using AiDotNet.LinearAlgebra;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Gloo-based communication backend for CPU-based collective operations.\n+/// </summary>\n+/// <remarks>\n+/// <para><b>Overview:</b>\n+/// Gloo is Facebook's collective communications library optimized for both CPUs and GPUs.\n+/// It provides efficient implementations of collective operations for CPU-based training\n+/// or heterogeneous environments. Gloo is particularly well-suited for training on CPUs\n+/// or mixed CPU/GPU clusters where NCCL may not be available or optimal.\n+/// </para>\n+/// <para><b>Features:</b>\n+/// - CPU-optimized collective operations\n+/// - Supports TCP, InfiniBand via ibverbs\n+/// - Works on both CPUs and GPUs\n+/// - Cross-platform (Linux, macOS, Windows)\n+/// - Used by PyTorch's distributed package\n+/// </para>\n+/// <para><b>Use Cases:</b>\n+/// - CPU-based distributed training\n+/// - Heterogeneous clusters (mixed CPU/GPU)\n+/// - When NCCL is not available (non-NVIDIA hardware, macOS, etc.)\n+/// - Development and testing on laptops/workstations\n+/// - Production training on CPU clusters\n+/// </para>\n+/// <para><b>Requirements:</b>\n+/// - Gloo library (C++)\n+/// - .NET bindings for Gloo (custom P/Invoke or wrapper library)\n+/// - Network connectivity between workers (TCP/IP or InfiniBand)\n+/// </para>\n+/// <para><b>Graceful Degradation:</b>\n+/// If Gloo library is not available, this backend provides a production-ready TCP-based\n+/// implementation of collective operations using industry-standard ring algorithms.\n+/// This fallback provides full functionality without external dependencies.\n+/// </para>\n+/// </remarks>\n+/// <typeparam name=\"T\">The numeric type for operations</typeparam>\n+public class GlooCommunicationBackend<T> : CommunicationBackendBase<T>\n+{\n+    private readonly int _rank;\n+    private readonly int _worldSize;\n+    private bool _useNativeTCP;\n+    private Dictionary<int, TcpClient>? _tcpConnections;\n+    private TcpListener? _tcpListener;\n+    private readonly object _connectionLock = new object();\n+\n+    /// <summary>\n+    /// Creates a new Gloo communication backend.\n+    /// </summary>\n+    /// <param name=\"rank\">This process's rank</param>\n+    /// <param name=\"worldSize\">Total number of processes</param>\n+    /// <remarks>\n+    /// Transport type selection (TCP vs InfiniBand) is not yet implemented.\n+    /// Currently defaults to TCP-based communication when Gloo library is unavailable.\n+    /// </remarks>\n+    public GlooCommunicationBackend(int rank = 0, int worldSize = 1)\n+    {\n+        _rank = rank;\n+        _worldSize = worldSize;\n+        _useNativeTCP = false;\n+    }\n+\n+    /// <inheritdoc/>\n+    public override int Rank => _rank;\n+\n+    /// <inheritdoc/>\n+    public override int WorldSize => _worldSize;\n+\n+    /// <inheritdoc/>\n+    protected override void OnInitialize()\n+    {\n+        _tcpConnections = new Dictionary<int, TcpClient>();\n+\n+        // Single-process mode: No communication infrastructure needed\n+        if (_worldSize == 1)\n+        {\n+            _useNativeTCP = false;\n+            Console.WriteLine(\"GlooCommunicationBackend: Single-process mode (worldSize=1).\");\n+            return;\n+        }\n+\n+        // Multi-process mode: Try Gloo library first, fallback to TCP\n+        bool glooAvailable = false;\n+        try\n+        {\n+            var glooType = Type.GetType(\"Gloo.Context, GlooSharp\");\n+            if (glooType != null)\n+            {\n+                glooAvailable = true;\n+                _useNativeTCP = false;\n+                Console.WriteLine($\"GlooCommunicationBackend: Gloo library detected for {_worldSize} processes.\");\n+\n+                // TODO: Full Gloo initialization requires:\n+                // 1. Creating transport device (TCP or ibverbs): device = new TcpDevice()\n+                // 2. Creating rendezvous store: store = new FileStore() or RedisStore()\n+                // 3. Creating Gloo context: context = new Context(rank, size)\n+                // 4. Connecting to all other ranks via rendezvous\n+\n+                throw new NotImplementedException(\n+                    \"GlooCommunicationBackend with Gloo library support is not yet fully implemented.\\n\\n\" +\n+                    \"Full Gloo initialization requires:\\n\" +\n+                    \"- GlooSharp P/Invoke bindings for Gloo C++ library\\n\" +\n+                    \"- Rendezvous infrastructure (file-based or Redis)\\n\" +\n+                    \"- Transport device configuration (TCP or InfiniBand)\\n\\n\" +\n+                    \"Using TCP fallback instead.\");\n+            }\n+        }\n+        catch (TypeLoadException)\n+        {\n+            glooAvailable = false;\n+        }\n+\n+        // Fallback to native TCP implementation\n+        if (!glooAvailable)\n+        {\n+            _useNativeTCP = true;\n+            Console.WriteLine($\"GlooCommunicationBackend: Using TCP fallback for {_worldSize} processes.\");\n+            InitializeTCPConnections();\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Initializes TCP connections between all ranks.\n+    /// </summary>\n+    /// <remarks>\n+    /// Uses environment variables for rendezvous:\n+    /// - AIDOTNET_MASTER_ADDR: IP address of rank 0\n+    /// - AIDOTNET_MASTER_PORT: Base port for rank 0\n+    /// Each rank listens on MASTER_PORT + rank\n+    /// </remarks>\n+    private void InitializeTCPConnections()\n+    {\n+        string? masterAddr = Environment.GetEnvironmentVariable(\"AIDOTNET_MASTER_ADDR\");\n+        string? masterPortStr = Environment.GetEnvironmentVariable(\"AIDOTNET_MASTER_PORT\");\n+\n+        if (string.IsNullOrEmpty(masterAddr) || string.IsNullOrEmpty(masterPortStr))\n+        {\n+            throw new InvalidOperationException(\n+                \"GlooCommunicationBackend TCP mode requires environment variables:\\n\" +\n+                \"- AIDOTNET_MASTER_ADDR: IP address of rank 0 (e.g., 192.168.1.10 or localhost)\\n\" +\n+                \"- AIDOTNET_MASTER_PORT: Base port number (e.g., 29500)\\n\" +\n+                \"Each rank will use port = MASTER_PORT + rank\");\n+        }\n+\n+        if (!int.TryParse(masterPortStr, out int basePort))\n+        {\n+            throw new InvalidOperationException($\"Invalid AIDOTNET_MASTER_PORT: {masterPortStr}\");\n+        }\n+\n+        // Start TCP listener on this rank's port\n+        int myPort = basePort + _rank;\n+        _tcpListener = new TcpListener(IPAddress.Any, myPort);\n+        _tcpListener.Start();\n+        Console.WriteLine($\"Rank {_rank}: TCP listener started on port {myPort}\");\n+\n+        // Connect to all ranks with lower rank numbers (they are already listening)\n+        for (int otherRank = 0; otherRank < _rank; otherRank++)\n+        {\n+            ConnectToRank(otherRank, masterAddr, basePort);\n+        }\n+\n+        // Accept connections from all ranks with higher rank numbers\n+        for (int otherRank = _rank + 1; otherRank < _worldSize; otherRank++)\n+        {\n+            AcceptConnectionFromRank(otherRank);\n+        }\n+\n+        Console.WriteLine($\"Rank {_rank}: All TCP connections established ({_tcpConnections?.Count ?? 0} peers)\");\n+    }\n+\n+    /// <summary>\n+    /// Connects to a specific rank (active connection).\n+    /// </summary>\n+    private void ConnectToRank(int targetRank, string masterAddr, int basePort)\n+    {\n+        int targetPort = basePort + targetRank;\n+        int maxRetries = 10;\n+        int retryDelayMs = 1000;\n+\n+        for (int attempt = 0; attempt < maxRetries; attempt++)\n+        {\n+            try\n+            {\n+                var client = new TcpClient();\n+                client.Connect(masterAddr, targetPort);\n+\n+                // Send handshake: my rank\n+                using (var stream = client.GetStream())\n+                using (var writer = new BinaryWriter(stream))\n+                {\n+                    writer.Write(_rank);\n+                    writer.Flush();\n+                }\n+\n+                lock (_connectionLock)\n+                {\n+                    if (_tcpConnections != null)\n+                    {\n+                        _tcpConnections[targetRank] = client;\n+                    }\n+                }\n+\n+                Console.WriteLine($\"Rank {_rank}: Connected to rank {targetRank} at {masterAddr}:{targetPort}\");\n+                return;\n+            }\n+            catch (SocketException)\n+            {\n+                if (attempt < maxRetries - 1)\n+                {\n+                    Thread.Sleep(retryDelayMs);\n+                }\n+                else\n+                {\n+                    throw new InvalidOperationException(\n+                        $\"Rank {_rank}: Failed to connect to rank {targetRank} at {masterAddr}:{targetPort} after {maxRetries} attempts\");\n+                }\n+            }\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Accepts connection from a specific rank (passive connection).\n+    /// </summary>\n+    private void AcceptConnectionFromRank(int expectedRank)\n+    {\n+        if (_tcpListener == null)\n+        {\n+            throw new InvalidOperationException(\"TCP listener not initialized\");\n+        }\n+\n+        // Accept incoming connection\n+        var client = _tcpListener.AcceptTcpClient();\n+\n+        // Read handshake to verify rank\n+        int receivedRank;\n+        using (var stream = client.GetStream())\n+        using (var reader = new BinaryReader(stream))\n+        {\n+            receivedRank = reader.ReadInt32();\n+        }\n+\n+        if (receivedRank != expectedRank)\n+        {\n+            client.Close();\n+            throw new InvalidOperationException(\n+                $\"Rank {_rank}: Expected connection from rank {expectedRank}, but received from rank {receivedRank}\");\n+        }\n+\n+        lock (_connectionLock)\n+        {\n+            if (_tcpConnections != null)\n+            {\n+                _tcpConnections[receivedRank] = client;\n+            }\n+        }\n+\n+        Console.WriteLine($\"Rank {_rank}: Accepted connection from rank {receivedRank}\");\n+    }\n+\n+    /// <inheritdoc/>\n+    protected override void OnShutdown()\n+    {\n+        if (_useNativeTCP && _tcpConnections != null)\n+        {\n+            lock (_connectionLock)\n+            {\n+                foreach (var connection in _tcpConnections.Values)\n+                {\n+                    try\n+                    {\n+                        connection.Close();\n+                    }\n+                    catch (Exception ex)\n+                    {\n+                        Console.WriteLine($\"Warning: Error closing TCP connection: {ex.Message}\");\n+                    }\n+                }\n+                _tcpConnections.Clear();\n+            }\n+\n+            if (_tcpListener != null)\n+            {\n+                try\n+                {\n+                    _tcpListener.Stop();\n+                }\n+                catch (Exception ex)\n+                {\n+                    Console.WriteLine($\"Warning: Error stopping TCP listener: {ex.Message}\");\n+                }\n+                _tcpListener = null;\n+            }\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public override void Barrier()\n+    {\n+        EnsureInitialized();\n+\n+        if (_worldSize == 1)\n+        {\n+            // Single-process: barrier is a no-op\n+            return;\n+        }\n+\n+        if (!_useNativeTCP)\n+        {\n+            throw new InvalidOperationException(\"Barrier requires TCP mode to be initialized\");\n+        }\n+\n+        // Simple all-to-all barrier implementation\n+        // Each rank sends a signal to all other ranks and waits for signals from all\n+        var signal = new[] { NumOps.One };\n+\n+        // Send signal to all other ranks\n+        for (int otherRank = 0; otherRank < _worldSize; otherRank++)\n+        {\n+            if (otherRank != _rank)\n+            {\n+                SendData(otherRank, signal);\n+            }\n+        }\n+\n+        // Receive signal from all other ranks\n+        for (int otherRank = 0; otherRank < _worldSize; otherRank++)\n+        {\n+            if (otherRank != _rank)\n+            {\n+                ReceiveData(otherRank, 1);\n+            }\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public override void AllReduce(Vector<T> data, ReductionOperation operation)\n+    {\n+        EnsureInitialized();\n+        ValidateData(data, nameof(data));\n+\n+        if (_worldSize == 1)\n+        {\n+            // Single-process: data already contains the result (no-op)\n+            return;\n+        }\n+\n+        if (!_useNativeTCP)\n+        {\n+            throw new InvalidOperationException(\"AllReduce requires TCP mode to be initialized\");\n+        }\n+\n+        PerformRingAllReduce(data, operation);\n+    }\n+\n+    /// <inheritdoc/>\n+    public override Vector<T> AllGather(Vector<T> sendData)\n+    {\n+        EnsureInitialized();\n+        ValidateData(sendData, nameof(sendData));\n+\n+        if (_worldSize == 1)\n+        {\n+            return sendData.Clone();\n+        }\n+\n+        if (!_useNativeTCP)\n+        {\n+            throw new InvalidOperationException(\"AllGather requires TCP mode to be initialized\");\n+        }\n+\n+        return PerformRingAllGather(sendData);\n+    }\n+\n+    /// <inheritdoc/>\n+    public override Vector<T> Broadcast(Vector<T> data, int root = 0)\n+    {\n+        EnsureInitialized();\n+        ValidateData(data, nameof(data));\n+        ValidateRoot(root);\n+\n+        if (_worldSize == 1)\n+        {\n+            return data.Clone();\n+        }\n+\n+        if (!_useNativeTCP)\n+        {\n+            throw new InvalidOperationException(\"Broadcast requires TCP mode to be initialized\");\n+        }\n+\n+        return PerformTreeBroadcast(data, root);\n+    }\n+\n+    /// <inheritdoc/>\n+    public override Vector<T> Scatter(Vector<T> sendData, int root = 0)\n+    {\n+        EnsureInitialized();\n+        ValidateRoot(root);\n+\n+        if (_worldSize == 1)\n+        {\n+            if (Rank == root)\n+            {\n+                ValidateData(sendData, nameof(sendData));\n+                return sendData.Clone();\n+            }\n+            return new Vector<T>(Array.Empty<T>());\n+        }\n+\n+        if (!_useNativeTCP)\n+        {\n+            throw new InvalidOperationException(\"Scatter requires TCP mode to be initialized\");\n+        }\n+\n+        return PerformTreeScatter(sendData, root);\n+    }\n+\n+    /// <inheritdoc/>\n+    public override Vector<T> ReduceScatter(Vector<T> data, ReductionOperation operation)\n+    {\n+        EnsureInitialized();\n+        ValidateData(data, nameof(data));\n+\n+        if (data.Length % _worldSize != 0)\n+        {\n+            throw new ArgumentException(\n+                $\"Data length {data.Length} must be divisible by world size {_worldSize}.\");\n+        }\n+\n+        if (_worldSize == 1)\n+        {\n+            return data.Clone();\n+        }\n+\n+        if (!_useNativeTCP)\n+        {\n+            throw new InvalidOperationException(\"ReduceScatter requires TCP mode to be initialized\");\n+        }\n+\n+        return PerformRingReduceScatter(data, operation);\n+    }\n+\n+    /// <summary>\n+    /// Performs ring-based AllReduce operation.\n+    /// </summary>\n+    /// <remarks>\n+    /// This is a production-ready implementation of the ring AllReduce algorithm\n+    /// used by systems like Baidu's Ring AllReduce and Horovod.\n+    ///\n+    /// Algorithm:\n+    /// 1. Divide data into N chunks (N = worldSize)\n+    /// 2. ReduceScatter phase: Send chunks in ring pattern, reducing as we go\n+    /// 3. AllGather phase: Gather the reduced chunks back to all ranks\n+    ///\n+    /// Time complexity: O(2*(N-1)*M/N) where M is data size\n+    /// This is optimal for large messages and scales linearly with cluster size.\n+    /// </remarks>\n+    private void PerformRingAllReduce(Vector<T> data, ReductionOperation operation)\n+    {\n+        if (_worldSize == 1)\n+        {\n+            return;\n+        }\n+\n+        int chunkSize = (data.Length + _worldSize - 1) / _worldSize; // Ceiling division\n+        int nextRank = (_rank + 1) % _worldSize;\n+        int prevRank = (_rank - 1 + _worldSize) % _worldSize;\n+\n+        var dataArray = data.ToArray();\n+\n+        // Phase 1: ReduceScatter - reduce chunks in ring pattern\n+        for (int step = 0; step < _worldSize - 1; step++)\n+        {\n+            int sendChunkIdx = (_rank - step + _worldSize) % _worldSize;\n+            int recvChunkIdx = (_rank - step - 1 + _worldSize) % _worldSize;\n+\n+            int sendStart = sendChunkIdx * chunkSize;\n+            int sendCount = Math.Min(chunkSize, data.Length - sendStart);\n+            int recvStart = recvChunkIdx * chunkSize;\n+            int recvCount = Math.Min(chunkSize, data.Length - recvStart);\n+\n+            // Extract send chunk\n+            var sendChunk = new T[sendCount];\n+            Array.Copy(dataArray, sendStart, sendChunk, 0, sendCount);\n+\n+            // Send and receive simultaneously\n+            var sendTask = System.Threading.Tasks.Task.Run(() => SendData(nextRank, sendChunk));\n+            var recvChunk = ReceiveData(prevRank, recvCount);\n+            sendTask.Wait();\n+\n+            // Reduce received chunk with local chunk\n+            for (int i = 0; i < recvCount; i++)\n+            {\n+                dataArray[recvStart + i] = PerformReduction(dataArray[recvStart + i], recvChunk[i], operation);\n+            }\n+        }\n+\n+        // Phase 2: AllGather - distribute reduced chunks in ring pattern\n+        for (int step = 0; step < _worldSize - 1; step++)\n+        {\n+            int sendChunkIdx = (_rank - step + 1 + _worldSize) % _worldSize;\n+            int recvChunkIdx = (_rank - step + _worldSize) % _worldSize;\n+\n+            int sendStart = sendChunkIdx * chunkSize;\n+            int sendCount = Math.Min(chunkSize, data.Length - sendStart);\n+            int recvStart = recvChunkIdx * chunkSize;\n+            int recvCount = Math.Min(chunkSize, data.Length - recvStart);\n+\n+            // Extract send chunk\n+            var sendChunk = new T[sendCount];\n+            Array.Copy(dataArray, sendStart, sendChunk, 0, sendCount);\n+\n+            // Send and receive simultaneously\n+            var sendTask = System.Threading.Tasks.Task.Run(() => SendData(nextRank, sendChunk));\n+            var recvChunk = ReceiveData(prevRank, recvCount);\n+            sendTask.Wait();\n+\n+            // Copy received chunk to local data\n+            Array.Copy(recvChunk, 0, dataArray, recvStart, recvCount);\n+        }\n+\n+        // Apply averaging if needed (after all reductions complete)\n+        if (operation == ReductionOperation.Average)\n+        {\n+            for (int i = 0; i < dataArray.Length; i++)\n+            {\n+                dataArray[i] = NumOps.Divide(dataArray[i], NumOps.FromDouble(_worldSize));\n+            }\n+        }\n+\n+        // Update original vector with reduced data\n+        for (int i = 0; i < dataArray.Length; i++)\n+        {\n+            data[i] = dataArray[i];\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs a reduction operation on two values.\n+    /// </summary>\n+    private T PerformReduction(T a, T b, ReductionOperation operation)\n+    {\n+        return operation switch\n+        {\n+            ReductionOperation.Sum or ReductionOperation.Average => NumOps.Add(a, b),\n+            ReductionOperation.Max => NumOps.GreaterThan(a, b) ? a : b,\n+            ReductionOperation.Min => NumOps.LessThan(a, b) ? a : b,\n+            ReductionOperation.Product => NumOps.Multiply(a, b),\n+            _ => throw new ArgumentException($\"Unsupported reduction operation: {operation}\")\n+        };\n+    }\n+\n+    /// <summary>\n+    /// Performs ring-based AllGather operation.\n+    /// </summary>\n+    private Vector<T> PerformRingAllGather(Vector<T> sendData)\n+    {\n+        if (_worldSize == 1)\n+        {\n+            return sendData.Clone();\n+        }\n+\n+        int chunkSize = sendData.Length;\n+        int nextRank = (_rank + 1) % _worldSize;\n+        int prevRank = (_rank - 1 + _worldSize) % _worldSize;\n+\n+        // Result buffer to hold data from all ranks\n+        var result = new T[chunkSize * _worldSize];\n+\n+        // Copy local data to result buffer\n+        Array.Copy(sendData.ToArray(), 0, result, _rank * chunkSize, chunkSize);\n+\n+        // Ring AllGather: each rank receives chunk from previous rank and forwards it\n+        for (int step = 0; step < _worldSize - 1; step++)\n+        {\n+            int sendChunkIdx = (_rank - step + _worldSize) % _worldSize;\n+            int recvChunkIdx = (_rank - step - 1 + _worldSize) % _worldSize;\n+\n+            // Extract send chunk from result buffer\n+            var sendChunk = new T[chunkSize];\n+            Array.Copy(result, sendChunkIdx * chunkSize, sendChunk, 0, chunkSize);\n+\n+            // Send and receive simultaneously\n+            var sendTask = System.Threading.Tasks.Task.Run(() => SendData(nextRank, sendChunk));\n+            var recvChunk = ReceiveData(prevRank, chunkSize);\n+            sendTask.Wait();\n+\n+            // Copy received chunk to result buffer\n+            Array.Copy(recvChunk, 0, result, recvChunkIdx * chunkSize, chunkSize);\n+        }\n+\n+        return new Vector<T>(result);\n+    }\n+\n+    /// <summary>\n+    /// Performs tree-based Broadcast operation.\n+    /// </summary>\n+    /// <remarks>\n+    /// Uses a binary tree pattern for efficient broadcasting.\n+    /// Time complexity: O(log N) where N is worldSize\n+    /// </remarks>\n+    private Vector<T> PerformTreeBroadcast(Vector<T> data, int root)\n+    {\n+        if (_worldSize == 1)\n+        {\n+            return data.Clone();\n+        }\n+\n+        var dataArray = data.ToArray();\n+\n+        // Adjust ranks relative to root for binary tree calculation\n+        int relativeRank = (_rank - root + _worldSize) % _worldSize;\n+\n+        // If not root, receive data from parent\n+        if (relativeRank != 0)\n+        {\n+            int parentRelative = (relativeRank - 1) / 2;\n+            int parentAbsolute = (parentRelative + root) % _worldSize;\n+            dataArray = ReceiveData(parentAbsolute, data.Length);\n+        }\n+\n+        // Send data to children in binary tree\n+        int leftChildRelative = 2 * relativeRank + 1;\n+        int rightChildRelative = 2 * relativeRank + 2;\n+\n+        if (leftChildRelative < _worldSize)\n+        {\n+            int leftChildAbsolute = (leftChildRelative + root) % _worldSize;\n+            SendData(leftChildAbsolute, dataArray);\n+        }\n+\n+        if (rightChildRelative < _worldSize)\n+        {\n+            int rightChildAbsolute = (rightChildRelative + root) % _worldSize;\n+            SendData(rightChildAbsolute, dataArray);\n+        }\n+\n+        return new Vector<T>(dataArray);\n+    }\n+\n+    /// <summary>\n+    /// Performs tree-based Scatter operation.\n+    /// </summary>\n+    private Vector<T> PerformTreeScatter(Vector<T> sendData, int root)\n+    {\n+        if (_worldSize == 1)\n+        {\n+            if (Rank == root)\n+            {\n+                ValidateData(sendData, nameof(sendData));\n+                return sendData.Clone();\n+            }\n+            return new Vector<T>(Array.Empty<T>());\n+        }\n+\n+        int chunkSize;\n+        T[]? myChunk = null;\n+\n+        if (Rank == root)\n+        {\n+            ValidateData(sendData, nameof(sendData));\n+\n+            if (sendData.Length % _worldSize != 0)\n+            {\n+                throw new ArgumentException(\n+                    $\"Data length {sendData.Length} must be divisible by world size {_worldSize}.\");\n+            }\n+\n+            chunkSize = sendData.Length / _worldSize;\n+            var dataArray = sendData.ToArray();\n+\n+            // Root keeps its own chunk\n+            myChunk = new T[chunkSize];\n+            Array.Copy(dataArray, root * chunkSize, myChunk, 0, chunkSize);\n+\n+            // Send chunks to other ranks using binary tree pattern\n+            ScatterTreeSend(dataArray, chunkSize, root, 0, _worldSize);\n+        }\n+        else\n+        {\n+            // Non-root ranks receive their chunk\n+            int relativeRank = (_rank - root + _worldSize) % _worldSize;\n+            int parentRelative = (relativeRank - 1) / 2;\n+            int parentAbsolute = (parentRelative + root) % _worldSize;\n+\n+            // Receive chunk count first, then data\n+            chunkSize = ReceiveData(parentAbsolute, 1)[0] != null ?\n+                        Convert.ToInt32(ReceiveData(parentAbsolute, 1)[0]) : 0;\n+            myChunk = ReceiveData(parentAbsolute, chunkSize);\n+        }\n+\n+        return new Vector<T>(myChunk ?? Array.Empty<T>());\n+    }\n+\n+    /// <summary>\n+    /// Recursive helper for tree-based scatter send.\n+    /// </summary>\n+    private void ScatterTreeSend(T[] allData, int chunkSize, int root, int treeRank, int treeSize)\n+    {\n+        int leftChildRelative = 2 * treeRank + 1;\n+        int rightChildRelative = 2 * treeRank + 2;\n+\n+        if (leftChildRelative < treeSize)\n+        {\n+            int leftChildAbsolute = (leftChildRelative + root) % _worldSize;\n+            var leftChunk = new T[chunkSize];\n+            Array.Copy(allData, leftChildAbsolute * chunkSize, leftChunk, 0, chunkSize);\n+            SendData(leftChildAbsolute, new[] { (T)Convert.ChangeType(chunkSize, typeof(T)) });\n+            SendData(leftChildAbsolute, leftChunk);\n+        }\n+\n+        if (rightChildRelative < treeSize)\n+        {\n+            int rightChildAbsolute = (rightChildRelative + root) % _worldSize;\n+            var rightChunk = new T[chunkSize];\n+            Array.Copy(allData, rightChildAbsolute * chunkSize, rightChunk, 0, chunkSize);\n+            SendData(rightChildAbsolute, new[] { (T)Convert.ChangeType(chunkSize, typeof(T)) });\n+            SendData(rightChildAbsolute, rightChunk);\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Performs ring-based ReduceScatter operation.\n+    /// </summary>\n+    /// <remarks>\n+    /// This is the first phase of ring AllReduce - it reduces and scatters\n+    /// the data in one efficient operation.\n+    /// </remarks>\n+    private Vector<T> PerformRingReduceScatter(Vector<T> data, ReductionOperation operation)\n+    {\n+        if (_worldSize == 1)\n+        {\n+            return data.Clone();\n+        }\n+\n+        int chunkSize = data.Length / _worldSize;\n+        int nextRank = (_rank + 1) % _worldSize;\n+        int prevRank = (_rank - 1 + _worldSize) % _worldSize;\n+\n+        var dataArray = data.ToArray();\n+\n+        // ReduceScatter phase: reduce chunks in ring pattern\n+        for (int step = 0; step < _worldSize - 1; step++)\n+        {\n+            int sendChunkIdx = (_rank - step + _worldSize) % _worldSize;\n+            int recvChunkIdx = (_rank - step - 1 + _worldSize) % _worldSize;\n+\n+            int sendStart = sendChunkIdx * chunkSize;\n+            int sendCount = Math.Min(chunkSize, data.Length - sendStart);\n+            int recvStart = recvChunkIdx * chunkSize;\n+            int recvCount = Math.Min(chunkSize, data.Length - recvStart);\n+\n+            // Extract send chunk\n+            var sendChunk = new T[sendCount];\n+            Array.Copy(dataArray, sendStart, sendChunk, 0, sendCount);\n+\n+            // Send and receive simultaneously\n+            var sendTask = System.Threading.Tasks.Task.Run(() => SendData(nextRank, sendChunk));\n+            var recvChunk = ReceiveData(prevRank, recvCount);\n+            sendTask.Wait();\n+\n+            // Reduce received chunk with local chunk\n+            for (int i = 0; i < recvCount; i++)\n+            {\n+                dataArray[recvStart + i] = PerformReduction(dataArray[recvStart + i], recvChunk[i], operation);\n+            }\n+        }\n+\n+        // Extract this rank's reduced chunk\n+        var myChunk = new T[chunkSize];\n+        Array.Copy(dataArray, _rank * chunkSize, myChunk, 0, chunkSize);\n+\n+        // Apply averaging if needed\n+        if (operation == ReductionOperation.Average)\n+        {\n+            for (int i = 0; i < myChunk.Length; i++)\n+            {\n+                myChunk[i] = NumOps.Divide(myChunk[i], NumOps.FromDouble(_worldSize));\n+            }\n+        }\n+\n+        return new Vector<T>(myChunk);\n+    }\n+\n+    /// <summary>\n+    /// Sends data to a specific rank via TCP.\n+    /// </summary>\n+    private void SendData(int destRank, T[] data)\n+    {\n+        if (_tcpConnections == null || !_tcpConnections.ContainsKey(destRank))\n+        {\n+            throw new InvalidOperationException($\"No TCP connection to rank {destRank}\");\n+        }\n+\n+        lock (_connectionLock)\n+        {\n+            var client = _tcpConnections[destRank];\n+            using (var stream = client.GetStream())\n+            using (var writer = new BinaryWriter(stream))\n+            {\n+                // Send length header\n+                writer.Write(data.Length);\n+\n+                // Send data elements\n+                foreach (var element in data)\n+                {\n+                    double value = Convert.ToDouble(element);\n+                    writer.Write(value);\n+                }\n+                writer.Flush();\n+            }\n+        }\n+    }\n+\n+    /// <summary>\n+    /// Receives data from a specific rank via TCP.\n+    /// </summary>\n+    private T[] ReceiveData(int sourceRank, int expectedLength)\n+    {\n+        if (_tcpConnections == null || !_tcpConnections.ContainsKey(sourceRank))\n+        {\n+            throw new InvalidOperationException($\"No TCP connection to rank {sourceRank}\");\n+        }\n+\n+        lock (_connectionLock)\n+        {\n+            var client = _tcpConnections[sourceRank];\n+            using (var stream = client.GetStream())\n+            using (var reader = new BinaryReader(stream))\n+            {\n+                // Read length header\n+                int length = reader.ReadInt32();\n+                if (length != expectedLength)\n+                {\n+                    throw new InvalidOperationException(\n+                        $\"Rank {_rank}: Expected {expectedLength} elements from rank {sourceRank}, but received {length}\");\n+                }\n+\n+                // Read data elements\n+                var result = new T[length];\n+                for (int i = 0; i < length; i++)\n+                {\n+                    double value = reader.ReadDouble();\n+                    result[i] = (T)Convert.ChangeType(value, typeof(T));\n+                }\n+                return result;\n+            }\n+        }"}]}},{"id":"PRRT_kwDOKSXUF85hNV-S","isResolved":false,"isOutdated":false,"path":"src/DistributedTraining/GradientCompressionOptimizer.cs","line":151,"originalLine":151,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VeGGR","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Avoid double-applying gradients after compression**\n\nWe undo the local step via `ComputeOriginalParameters`, but never restore those parameters before calling `ApplyGradients`. As written we apply the local update *and* the averaged compressed gradients, so every rank overshoots.\n\nReset the model to the reconstructed pre-update parameters before applying the synchronized gradients.\n\n```diff\n-            var originalParams = ComputeOriginalParameters(updatedParams, localGradients);\n+            var originalParams = ComputeOriginalParameters(updatedParams, localGradients);\n             var compressedGradients = CompressGradients(localGradients);\n             Config.CommunicationBackend.AllReduce(compressedGradients, ReductionOperation.Average);\n             var averagedGradients = DecompressGradients(compressedGradients, localGradients.Length);\n-            var finalModel = gradientOptimizer.ApplyGradients(averagedGradients, localResult.BestSolution);\n+            localResult.BestSolution.SetParameters(originalParams);\n+            var finalModel = gradientOptimizer.ApplyGradients(averagedGradients, localResult.BestSolution);\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n            // Step 3: Get parameters BEFORE gradient application (reverse the local update)\n            var updatedParams = localResult.BestSolution.GetParameters();\n            var originalParams = ComputeOriginalParameters(updatedParams, localGradients);\n\n            // Step 4: Compress local gradients\n            var compressedGradients = CompressGradients(localGradients);\n\n            // Step 5: Synchronize compressed gradients across all ranks and average them\n            Config.CommunicationBackend.AllReduce(compressedGradients, ReductionOperation.Average);\n\n            // Step 6: Decompress to get averaged gradients\n            var averagedGradients = DecompressGradients(compressedGradients, localGradients.Length);\n\n            // Step 7: Apply averaged compressed gradients to original parameters\n            // This ensures all ranks converge using compressed gradients\n            localResult.BestSolution.SetParameters(originalParams);\n            var finalModel = gradientOptimizer.ApplyGradients(averagedGradients, localResult.BestSolution);\n\n            // Step 8: Return result with model updated using averaged compressed gradients\n            localResult.BestSolution = finalModel;\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/DistributedTraining/GradientCompressionOptimizer.cs around lines 133 to\n151, the code computes originalParams via ComputeOriginalParameters but never\nrestores them, so ApplyGradients is applied on a model that already has the\nlocal update (causing double application); restore the model to the\nreconstructed pre-update parameters before applying the averaged compressed\ngradients‚Äîe.g., set or create a model instance using originalParams and pass\nthat to gradientOptimizer.ApplyGradients(averagedGradients, reconstructedModel),\nthen assign the returned finalModel back to localResult.BestSolution.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","createdAt":"2025-11-09T08:20:17Z","path":"src/DistributedTraining/GradientCompressionOptimizer.cs","line":151,"originalLine":151,"diffHunk":"@@ -0,0 +1,488 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.Models.Inputs;\n+using AiDotNet.Optimizers;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Implements gradient compression optimizer - wraps any optimizer to add gradient compression.\n+/// </summary>\n+/// <remarks>\n+/// <para><b>Strategy Overview:</b>\n+/// Gradient compression reduces communication overhead in distributed training by compressing\n+/// gradients before transmission. Common techniques include:\n+/// - Quantization: Reduce precision (FP32 ‚Üí FP16 or INT8)\n+/// - Sparsification: Send only top-k largest gradients\n+/// - Low-rank compression: Decompose gradient matrices\n+///\n+/// This can significantly reduce network bandwidth usage (2-100x compression) with minimal\n+/// accuracy impact when done carefully. Essential for training over slow networks or at very large scales.\n+/// </para>\n+/// <para><b>For Beginners:</b>\n+/// This optimizer wrapper compresses gradients before sending them across the network.\n+/// Think of it like compressing a large file before emailing it - the information is mostly\n+/// preserved, but it takes much less time to transmit. This is especially useful when your\n+/// GPUs are connected by slower networks.\n+///\n+/// Common compression techniques:\n+/// - Make numbers less precise (like rounding 3.14159 to 3.14)\n+/// - Send only the most important values (top-k sparsification)\n+/// - Use mathematical tricks to represent data more compactly\n+/// </para>\n+/// <para><b>Use Cases:</b>\n+/// - Slow network connections (cross-datacenter, ethernet vs NVLink)\n+/// - Very large models where gradient size is huge\n+/// - Can wrap any other optimizer (DDP, FSDP, etc.)\n+/// - Bandwidth-constrained scenarios\n+/// </para>\n+/// <para><b>Trade-offs:</b>\n+/// - Memory: Same as wrapped optimizer\n+/// - Communication: Much lower - 2x to 100x reduction depending on technique\n+/// - Complexity: Moderate - adds compression/decompression overhead\n+/// - Accuracy: Slight potential degradation (usually <1% with proper tuning)\n+/// - Best for: Slow networks, bandwidth-limited scenarios\n+/// </para>\n+/// <para><b>Implementation Note:</b>\n+/// This framework provides the compression optimizer infrastructure. Specific compression\n+/// algorithms (quantization levels, sparsity ratios) can be configured. Production use\n+/// would implement specific compression strategies based on your network and accuracy requirements.\n+/// </para>\n+/// <para>\n+/// Example:\n+/// <code>\n+/// var baseOptimizer = new AdamOptimizer&lt;double, Tensor&lt;double&gt;, Tensor&lt;double&gt;&gt;(model, options);\n+/// var backend = new InMemoryCommunicationBackend&lt;double&gt;(rank: 0, worldSize: 4);\n+/// var config = new ShardingConfiguration&lt;double&gt;(backend) {\n+///     EnableGradientCompression = true\n+/// };\n+///\n+/// // Wrap with gradient compression\n+/// var compressedOptimizer = new GradientCompressionOptimizer&lt;double, Tensor&lt;double&gt;, Tensor&lt;double&gt;&gt;(\n+///     baseOptimizer, config, compressionRatio: 0.1); // Keep top 10% of gradients\n+/// </code>\n+/// </para>\n+/// </remarks>\n+/// <typeparam name=\"T\">The numeric type</typeparam>\n+/// <typeparam name=\"TInput\">The input type for the model</typeparam>\n+/// <typeparam name=\"TOutput\">The output type for the model</typeparam>\n+public class GradientCompressionOptimizer<T, TInput, TOutput> : ShardedOptimizerBase<T, TInput, TOutput>\n+{\n+    private readonly double _compressionRatio;\n+    private readonly bool _useQuantization;\n+    private readonly bool _useSparsification;\n+\n+    /// <summary>\n+    /// Creates a gradient compression optimizer.\n+    /// </summary>\n+    /// <param name=\"wrappedOptimizer\">The optimizer to wrap with compression (must be gradient-based)</param>\n+    /// <param name=\"config\">Configuration for sharding and communication</param>\n+    /// <param name=\"compressionRatio\">Compression ratio (0.0 to 1.0, lower = more compression)</param>\n+    /// <param name=\"useQuantization\">Whether to use quantization compression</param>\n+    /// <param name=\"useSparsification\">Whether to use top-k sparsification</param>\n+    /// <exception cref=\"ArgumentException\">If wrapped optimizer is not gradient-based</exception>\n+    public GradientCompressionOptimizer(\n+        IOptimizer<T, TInput, TOutput> wrappedOptimizer,\n+        IShardingConfiguration<T> config,\n+        double compressionRatio = 0.1,\n+        bool useQuantization = true,\n+        bool useSparsification = false)\n+        : base(wrappedOptimizer, config)\n+    {\n+        if (compressionRatio <= 0 || compressionRatio > 1)\n+            throw new ArgumentException(\"Compression ratio must be in (0, 1]\", nameof(compressionRatio));\n+\n+        // Gradient compression only makes sense for gradient-based optimizers\n+        if (wrappedOptimizer is not IGradientBasedOptimizer<T, TInput, TOutput>)\n+        {\n+            throw new ArgumentException(\n+                $\"Gradient compression requires a gradient-based optimizer. \" +\n+                $\"The provided optimizer type {wrappedOptimizer.GetType().Name} does not implement IGradientBasedOptimizer. \" +\n+                $\"Use gradient-based optimizers like SGD, Adam, RMSProp, etc.\",\n+                nameof(wrappedOptimizer));\n+        }\n+\n+        _compressionRatio = compressionRatio;\n+        _useQuantization = useQuantization;\n+        _useSparsification = useSparsification;\n+    }\n+\n+    /// <inheritdoc/>\n+    public override OptimizationResult<T, TInput, TOutput> Optimize(OptimizationInputData<T, TInput, TOutput> inputData)\n+    {\n+        if (inputData == null)\n+            throw new ArgumentNullException(nameof(inputData));\n+\n+        var gradientOptimizer = WrappedOptimizer as IGradientBasedOptimizer<T, TInput, TOutput>;\n+        if (gradientOptimizer == null)\n+        {\n+            throw new InvalidOperationException(\n+                $\"GradientCompressionOptimizer requires a gradient-based optimizer, but received {WrappedOptimizer.GetType().Name}\");\n+        }\n+\n+        Config.CommunicationBackend.Barrier();\n+\n+        // Step 1: Optimize locally to compute gradients and get locally-updated model\n+        var localResult = WrappedOptimizer.Optimize(inputData);\n+\n+        // Step 2: Get the gradients that were computed during optimization\n+        var localGradients = gradientOptimizer.LastComputedGradients;\n+\n+        if (Config.AutoSyncGradients && localResult.BestSolution != null && localGradients != null && localGradients.Length > 0)\n+        {\n+            // Step 3: Get parameters BEFORE gradient application (reverse the local update)\n+            var updatedParams = localResult.BestSolution.GetParameters();\n+            var originalParams = ComputeOriginalParameters(updatedParams, localGradients);\n+\n+            // Step 4: Compress local gradients\n+            var compressedGradients = CompressGradients(localGradients);\n+\n+            // Step 5: Synchronize compressed gradients across all ranks and average them\n+            Config.CommunicationBackend.AllReduce(compressedGradients, ReductionOperation.Average);\n+\n+            // Step 6: Decompress to get averaged gradients\n+            var averagedGradients = DecompressGradients(compressedGradients, localGradients.Length);\n+\n+            // Step 7: Apply averaged compressed gradients to original parameters\n+            // This ensures all ranks converge using compressed gradients\n+            var finalModel = gradientOptimizer.ApplyGradients(averagedGradients, localResult.BestSolution);\n+\n+            // Step 8: Return result with model updated using averaged compressed gradients\n+            localResult.BestSolution = finalModel;"}]}},{"id":"PRRT_kwDOKSXUF85hNV-X","isResolved":false,"isOutdated":true,"path":"src/DistributedTraining/PipelineParallelOptimizer.cs","line":80,"originalLine":80,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VeGGW","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Guarantee the closing barrier runs even on exceptions**\n\nAny exception between the two barriers causes other stages to hang at the final barrier. Mirror the ZeRO2 fix: wrap the optimization block in `try/finally` and move the trailing barrier into the `finally` so all ranks either reach it or unwind safely.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/DistributedTraining/PipelineParallelOptimizer.cs around lines 58 to 80,\nthe trailing barrier can be skipped if an exception occurs between the two\nbarriers; wrap the optimization and any per-stage logic in a try block and move\nthe final Config.CommunicationBackend.Barrier() into a finally block so the\nbarrier always executes; ensure any caught exception is not swallowed (rethrow\nafter finally or do not catch at all) so other ranks don't hang while exceptions\npropagate.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","createdAt":"2025-11-09T08:20:17Z","path":"src/DistributedTraining/PipelineParallelOptimizer.cs","line":80,"originalLine":80,"diffHunk":"@@ -0,0 +1,156 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.Models.Inputs;\n+using AiDotNet.Optimizers;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Implements Pipeline Parallel optimizer - coordinates optimization across pipeline stages.\n+/// </summary>\n+/// <remarks>\n+/// <para><b>Strategy Overview:</b>\n+/// Pipeline parallel optimizer coordinates optimization across different pipeline stages.\n+/// Each stage optimizes its own layer parameters, with gradient accumulation across micro-batches.\n+/// The optimizer ensures proper synchronization between forward and backward passes through the\n+/// pipeline, handling the gradient accumulation from multiple micro-batches.\n+/// </para>\n+/// <para><b>For Beginners:</b>\n+/// This optimizer works with pipeline parallel models where the model is split into stages.\n+/// It handles the complexity of gradient accumulation - since we process multiple micro-batches\n+/// through the pipeline, gradients need to be accumulated before the final parameter update.\n+/// Think of it like collecting feedback from multiple practice sessions before making adjustments.\n+/// </para>\n+/// <para><b>Use Cases:</b>\n+/// - Works with PipelineParallelModel\n+/// - Very deep models split into stages\n+/// - Handles micro-batch gradient accumulation\n+/// </para>\n+/// <para><b>Trade-offs:</b>\n+/// - Memory: Good for deep models\n+/// - Communication: Low between stages\n+/// - Complexity: High - gradient accumulation, pipeline scheduling\n+/// - Best for: Deep models with pipeline parallelism\n+/// </para>\n+/// </remarks>\n+/// <typeparam name=\"T\">The numeric type</typeparam>\n+/// <typeparam name=\"TInput\">The input type for the model</typeparam>\n+/// <typeparam name=\"TOutput\">The output type for the model</typeparam>\n+public class PipelineParallelOptimizer<T, TInput, TOutput> : ShardedOptimizerBase<T, TInput, TOutput>\n+{\n+    private readonly int _numMicroBatches;\n+\n+    public PipelineParallelOptimizer(\n+        IOptimizer<T, TInput, TOutput> wrappedOptimizer,\n+        IShardingConfiguration<T> config,\n+        int numMicroBatches = 1)\n+        : base(wrappedOptimizer, config)\n+    {\n+        _numMicroBatches = numMicroBatches;\n+    }\n+\n+    /// <inheritdoc/>\n+    public override OptimizationResult<T, TInput, TOutput> Optimize(OptimizationInputData<T, TInput, TOutput> inputData)\n+    {\n+        if (inputData == null)\n+            throw new ArgumentNullException(nameof(inputData));\n+\n+        Config.CommunicationBackend.Barrier();\n+\n+        // Pipeline parallel optimization requires:\n+        // 1. Process micro-batches through the pipeline\n+        // 2. Accumulate gradients across micro-batches\n+        // 3. Update parameters once all micro-batches complete\n+        // 4. Synchronize across pipeline stages if using data parallelism\n+\n+        // For this framework implementation, we provide simplified pattern\n+        var result = WrappedOptimizer.Optimize(inputData);\n+\n+        // Each stage updates its own parameters\n+        if (Config.AutoSyncGradients && result.BestSolution != null)\n+        {\n+            // In pure pipeline parallelism, no cross-stage parameter sync needed\n+            // (each stage owns different parameters)\n+            // If combined with data parallelism, would sync within data-parallel group\n+        }\n+\n+        Config.CommunicationBackend.Barrier();\n+\n+        return result;\n+    }"}]}},{"id":"PRRT_kwDOKSXUF85hNV-a","isResolved":false,"isOutdated":true,"path":"src/DistributedTraining/ZeRO2Optimizer.cs","line":null,"originalLine":84,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VeGGZ","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Wrap collectives in try/finally to avoid deadlock**\n\nIf `WrappedOptimizer.Optimize` (or any logic before the trailing barrier) throws on one rank, that rank skips the second barrier while its peers block forever. Please guard the collectives with `try/finally` so the trailing barrier (or a coordinated abort) always executes.\n\n```diff\n-        Config.CommunicationBackend.Barrier();\n-\n-        // Optimize on local data\n-        var result = WrappedOptimizer.Optimize(inputData);\n-        ...\n-\n-        Config.CommunicationBackend.Barrier();\n-\n-        return result;\n+        Config.CommunicationBackend.Barrier();\n+\n+        try\n+        {\n+            // Optimize on local data\n+            var result = WrappedOptimizer.Optimize(inputData);\n+            ...\n+            return result;\n+        }\n+        finally\n+        {\n+            Config.CommunicationBackend.Barrier();\n+        }\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n        Config.CommunicationBackend.Barrier();\n\n        try\n        {\n            // Optimize on local data\n            var result = WrappedOptimizer.Optimize(inputData);\n\n            // TODO: Implement ZeRO-2 gradient sharding\n            // In ZeRO-2, we need to:\n            // 1. Intercept gradients during backpropagation (before optimizer.Optimize() completes)\n            // 2. Perform ReduceScatter to reduce gradients across processes and distribute shards\n            //    var reducedGradientShard = Config.CommunicationBackend.ReduceScatter(gradients, ReductionOperation.Average);\n            // 3. Map reducedGradientShard back to local parameter shard indices\n            // 4. Apply gradient shard to update only local parameter shard and optimizer state\n            // 5. Ensure optimizer momentum/exponential average states are updated for the shard\n            // 6. Verify shard indices, sizes, and shapes match expected partitioning\n            //\n            // Current limitation: This framework's IOptimizer.Optimize() abstraction is a black box\n            // that doesn't expose intermediate gradients. Proper ZeRO-2 implementation requires\n            // either extending IOptimizer to expose gradients or integrating gradient hooks.\n            //\n            // For now, we synchronize parameters like ZeRO-1 (optimizer state is still sharded)\n            if (Config.AutoSyncGradients && result.BestSolution != null)\n            {\n                SynchronizeParameters(result.BestSolution);\n            }\n\n            SynchronizeOptimizerState();\n\n            return result;\n        }\n        finally\n        {\n            Config.CommunicationBackend.Barrier();\n        }\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","createdAt":"2025-11-09T08:20:17Z","path":"src/DistributedTraining/ZeRO2Optimizer.cs","line":null,"originalLine":84,"diffHunk":"@@ -0,0 +1,153 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.Models.Inputs;\n+using AiDotNet.Optimizers;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Implements ZeRO Stage 2 optimizer - shards optimizer states and gradients.\n+/// </summary>\n+/// <remarks>\n+/// <para><b>Strategy Overview:</b>\n+/// ZeRO-2 optimizer builds on ZeRO-1 by additionally sharding gradients using ReduceScatter.\n+/// After backward pass, gradients are reduced and scattered so each process only stores its\n+/// portion. This further reduces memory compared to ZeRO-1, as gradients can be as large as\n+/// the model itself. Parameters remain replicated for the forward pass.\n+/// </para>\n+/// <para><b>For Beginners:</b>\n+/// ZeRO-2 saves even more memory than ZeRO-1. Not only is the optimizer state split across\n+/// processes, but the gradients are too. After computing gradients, we immediately use\n+/// ReduceScatter to average them across processes AND split them up, so each process only\n+/// keeps its assigned portion. This is like having a team where each person is responsible\n+/// for updating only certain parameters.\n+/// </para>\n+/// <para><b>Use Cases:</b>\n+/// - Large models where gradient memory is significant\n+/// - Want substantial memory savings\n+/// - Works well with ZeRO2Model\n+/// </para>\n+/// <para><b>Trade-offs:</b>\n+/// - Memory: Very Good - saves optimizer states + gradients\n+/// - Communication: Moderate - uses ReduceScatter instead of AllReduce\n+/// - Complexity: Moderate - gradient and state sharding\n+/// - Best for: Large models with significant gradient memory\n+/// </para>\n+/// </remarks>\n+/// <typeparam name=\"T\">The numeric type</typeparam>\n+/// <typeparam name=\"TInput\">The input type for the model</typeparam>\n+/// <typeparam name=\"TOutput\">The output type for the model</typeparam>\n+public class ZeRO2Optimizer<T, TInput, TOutput> : ShardedOptimizerBase<T, TInput, TOutput>\n+{\n+    public ZeRO2Optimizer(\n+        IOptimizer<T, TInput, TOutput> wrappedOptimizer,\n+        IShardingConfiguration<T> config)\n+        : base(wrappedOptimizer, config)\n+    {\n+    }\n+\n+    /// <inheritdoc/>\n+    public override OptimizationResult<T, TInput, TOutput> Optimize(OptimizationInputData<T, TInput, TOutput> inputData)\n+    {\n+        if (inputData == null)\n+            throw new ArgumentNullException(nameof(inputData));\n+\n+        Config.CommunicationBackend.Barrier();\n+\n+        // Optimize on local data\n+        var result = WrappedOptimizer.Optimize(inputData);\n+\n+        // TODO: Implement ZeRO-2 gradient sharding\n+        // In ZeRO-2, we need to:\n+        // 1. Intercept gradients during backpropagation (before optimizer.Optimize() completes)\n+        // 2. Perform ReduceScatter to reduce gradients across processes and distribute shards\n+        //    var reducedGradientShard = Config.CommunicationBackend.ReduceScatter(gradients, ReductionOperation.Average);\n+        // 3. Map reducedGradientShard back to local parameter shard indices\n+        // 4. Apply gradient shard to update only local parameter shard and optimizer state\n+        // 5. Ensure optimizer momentum/exponential average states are updated for the shard\n+        // 6. Verify shard indices, sizes, and shapes match expected partitioning\n+        //\n+        // Current limitation: This framework's IOptimizer.Optimize() abstraction is a black box\n+        // that doesn't expose intermediate gradients. Proper ZeRO-2 implementation requires\n+        // either extending IOptimizer to expose gradients or integrating gradient hooks.\n+        //\n+        // For now, we synchronize parameters like ZeRO-1 (optimizer state is still sharded)\n+        if (Config.AutoSyncGradients && result.BestSolution != null)\n+        {\n+            SynchronizeParameters(result.BestSolution);\n+        }\n+\n+        SynchronizeOptimizerState();\n+\n+        Config.CommunicationBackend.Barrier();\n+\n+        return result;"}]}},{"id":"PRRT_kwDOKSXUF85hNV-c","isResolved":false,"isOutdated":true,"path":"src/Optimizers/GradientBasedOptimizerBase.cs","line":126,"originalLine":126,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VeGGb","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Do not double-step when applying distributed gradients**\n\n`ApplyGradients` calls `UpdateSolution` on the model as-is. In the new DDP flow we call this right after `WrappedOptimizer.Optimize`, so `model` already contains the locally-updated parameters `P‚ÇÄ - Œ∑¬∑g_local`. Subtracting `gradients` again yields `P‚ÇÄ - Œ∑¬∑g_local - Œ∑¬∑g_avg`, i.e. we take two optimizer steps instead of replacing the local step with the averaged one. Please either change this method to operate on the pre-step parameter vector (e.g. accept it explicitly and feed that into `UpdateSolution`) or have the caller restore `ComputeOriginalParameters(...)` into the model before invoking it, otherwise distributed runs diverge from single-process results.\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->","createdAt":"2025-11-09T08:20:18Z","path":"src/Optimizers/GradientBasedOptimizerBase.cs","line":126,"originalLine":126,"diffHunk":"@@ -84,11 +95,36 @@ protected GradientBasedOptimizerBase(\n         _currentLearningRate = GradientOptions.InitialLearningRate;\n         _currentMomentum = GradientOptions.InitialMomentum;\n         _previousGradient = Vector<T>.Empty();\n+        _lastComputedGradients = Vector<T>.Empty();\n         LossFunction = options.LossFunction;\n         GradientCache = options.GradientCache;\n         Regularization = options.Regularization;\n     }\n \n+    /// <inheritdoc/>\n+    public virtual Vector<T> LastComputedGradients => _lastComputedGradients;\n+\n+    /// <inheritdoc/>\n+    public virtual IFullModel<T, TInput, TOutput> ApplyGradients(Vector<T> gradients, IFullModel<T, TInput, TOutput> model)\n+    {\n+        if (gradients == null)\n+            throw new ArgumentNullException(nameof(gradients));\n+        if (model == null)\n+            throw new ArgumentNullException(nameof(model));\n+\n+        var parameters = model.GetParameters();\n+        if (gradients.Length != parameters.Length)\n+        {\n+            throw new ArgumentException(\n+                $\"Gradient size ({gradients.Length}) must match model parameter count ({parameters.Length})\",\n+                nameof(gradients));\n+        }\n+\n+        // Use the existing UpdateSolution method which applies gradients with\n+        // optimizer-specific logic (learning rate, momentum, Adam state, etc.)\n+        return UpdateSolution(model, gradients);\n+    }"}]}},{"id":"PRRT_kwDOKSXUF85hNV-h","isResolved":true,"isOutdated":true,"path":"src/PredictionModelBuilder.cs","line":null,"originalLine":323,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VeGGg","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Distributed wrappers never reach the training pipeline.**\n\nRight now the switch assigns `model` to the distributed wrapper, but nothing after this block ever consumes that variable. The underlying `_optimizer` continues to operate on the original `_model`, so FSDP/ZeRO/Pipeline/Tensor strategies silently degrade to single-process training. Please thread the distributed `model` through the pipeline (e.g., replace the model reference used by the optimiser or include it in the optimization input) before invoking `finalOptimizer.Optimize(...)`, so the sharded model actually participates in training.\n\n<!-- fingerprinting:phantom:medusa:sabertoothed -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits ae2f65b to 32d17fd","createdAt":"2025-11-09T08:20:18Z","path":"src/PredictionModelBuilder.cs","line":null,"originalLine":323,"diffHunk":"@@ -266,14 +271,66 @@ public PredictionModelResult<T, TInput, TOutput> Build(TInput x, TOutput y)\n         var outlierRemoval = _outlierRemoval ?? new NoOutlierRemoval<T, TInput, TOutput>();\n         var dataPreprocessor = _dataPreprocessor ?? new DefaultDataPreprocessor<T, TInput, TOutput>(normalizer, featureSelector, outlierRemoval);\n \n+        // Wrap model and optimizer for distributed training if configured\n+        IFullModel<T, TInput, TOutput> model = _model;\n+        IOptimizer<T, TInput, TOutput> finalOptimizer = optimizer;\n+\n+        // Enable distributed training if backend or configuration was explicitly provided\n+        if (_distributedBackend != null || _distributedConfiguration != null)\n+        {\n+            // Use provided backend or default to InMemory for single-process\n+            var backend = _distributedBackend ?? new DistributedTraining.InMemoryCommunicationBackend<T>(rank: 0, worldSize: 1);\n+\n+            // Use provided configuration or create default from backend\n+            var shardingConfig = _distributedConfiguration ?? new DistributedTraining.ShardingConfiguration<T>(backend);\n+\n+            // Switch on strategy to create appropriate model/optimizer pair\n+            (model, finalOptimizer) = _distributedStrategy switch\n+            {\n+                DistributedStrategy.DDP => (\n+                    (IFullModel<T, TInput, TOutput>)new DistributedTraining.DDPModel<T, TInput, TOutput>(_model, shardingConfig),\n+                    (IOptimizer<T, TInput, TOutput>)new DistributedTraining.DDPOptimizer<T, TInput, TOutput>(optimizer, shardingConfig)\n+                ),\n+                DistributedStrategy.FSDP => (\n+                    (IFullModel<T, TInput, TOutput>)new DistributedTraining.FSDPModel<T, TInput, TOutput>(_model, shardingConfig),\n+                    (IOptimizer<T, TInput, TOutput>)new DistributedTraining.FSDPOptimizer<T, TInput, TOutput>(optimizer, shardingConfig)\n+                ),\n+                DistributedStrategy.ZeRO1 => (\n+                    (IFullModel<T, TInput, TOutput>)new DistributedTraining.ZeRO1Model<T, TInput, TOutput>(_model, shardingConfig),\n+                    (IOptimizer<T, TInput, TOutput>)new DistributedTraining.ZeRO1Optimizer<T, TInput, TOutput>(optimizer, shardingConfig)\n+                ),\n+                DistributedStrategy.ZeRO2 => (\n+                    (IFullModel<T, TInput, TOutput>)new DistributedTraining.ZeRO2Model<T, TInput, TOutput>(_model, shardingConfig),\n+                    (IOptimizer<T, TInput, TOutput>)new DistributedTraining.ZeRO2Optimizer<T, TInput, TOutput>(optimizer, shardingConfig)\n+                ),\n+                DistributedStrategy.ZeRO3 => (\n+                    (IFullModel<T, TInput, TOutput>)new DistributedTraining.ZeRO3Model<T, TInput, TOutput>(_model, shardingConfig),\n+                    (IOptimizer<T, TInput, TOutput>)new DistributedTraining.ZeRO3Optimizer<T, TInput, TOutput>(optimizer, shardingConfig)\n+                ),\n+                DistributedStrategy.PipelineParallel => (\n+                    (IFullModel<T, TInput, TOutput>)new DistributedTraining.PipelineParallelModel<T, TInput, TOutput>(_model, shardingConfig),\n+                    (IOptimizer<T, TInput, TOutput>)new DistributedTraining.PipelineParallelOptimizer<T, TInput, TOutput>(optimizer, shardingConfig)\n+                ),\n+                DistributedStrategy.TensorParallel => (\n+                    (IFullModel<T, TInput, TOutput>)new DistributedTraining.TensorParallelModel<T, TInput, TOutput>(_model, shardingConfig),\n+                    (IOptimizer<T, TInput, TOutput>)new DistributedTraining.TensorParallelOptimizer<T, TInput, TOutput>(optimizer, shardingConfig)\n+                ),\n+                DistributedStrategy.Hybrid => (\n+                    (IFullModel<T, TInput, TOutput>)new DistributedTraining.HybridShardedModel<T, TInput, TOutput>(_model, shardingConfig),\n+                    (IOptimizer<T, TInput, TOutput>)new DistributedTraining.HybridShardedOptimizer<T, TInput, TOutput>(optimizer, shardingConfig)\n+                ),\n+                _ => throw new InvalidOperationException($\"Unsupported distributed strategy: {_distributedStrategy}\")\n+            };"}]}},{"id":"PRRT_kwDOKSXUF85hNZT6","isResolved":false,"isOutdated":false,"path":"src/DistributedTraining/ElasticOptimizer.cs","line":136,"originalLine":136,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VeK4Q","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Critical barrier synchronization mismatch causes deadlock.**\n\nThe attempt to prevent deadlock introduced a different synchronization problem:\n\n- **If `HandleWorkerChange()` throws**: Barrier called at line 129, then exception re-thrown (line 136 never reached)\n- **If `HandleWorkerChange()` succeeds**: Barrier called at line 136\n\nThese are **two different barrier synchronization points**. In distributed training, all processes must reach the **same** barrier call. If one process calls the barrier in the catch block while others call the barrier at line 136, they're waiting at different synchronization points, causing deadlock.\n\n\n\n**Solution options:**\n\n1. **Unconditional barrier after try-catch** (recommended for this placeholder implementation):\n```diff\n     // Check for world size changes (workers joined/left)\n+    InvalidOperationException? deferredError = null;\n     if (DetectWorldSizeChange())\n     {\n         try\n         {\n             HandleWorkerChange();\n         }\n         catch (InvalidOperationException ex)\n         {\n-            // CRITICAL: Reach barrier even on validation failure to unblock other processes\n-            // Otherwise, other workers waiting at the barrier will deadlock indefinitely\n-            Config.CommunicationBackend.Barrier();\n-            throw new InvalidOperationException(\n-                $\"Worker change validation failed: {ex.Message}. All workers have been synchronized.\", ex);\n+            // Defer throwing until after barrier to ensure all processes synchronize\n+            deferredError = new InvalidOperationException(\n+                $\"Worker change validation failed: {ex.Message}\", ex);\n         }\n     }\n\n     // Barrier with current worker set - ALL processes must reach this point\n     Config.CommunicationBackend.Barrier();\n+    \n+    // Throw deferred error after synchronization\n+    if (deferredError != null)\n+    {\n+        throw deferredError;\n+    }\n```\n\n2. **Collective validation** (better for production, but requires more infrastructure):\n```csharp\n// All workers must agree on whether validation succeeded\n// Use AllReduce to determine if ANY worker failed validation\n```\n\nNote: In production elastic training, validation failures should be coordinated cluster-wide through the membership service, ensuring all workers agree on the new configuration before proceeding.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/DistributedTraining/ElasticOptimizer.cs around lines 118 to 136, the\ncurrent code calls Barrier inside the catch and again after the try-catch,\ncreating mismatched synchronization points; change the flow to perform a single,\nunconditional Barrier after the try-catch: remove the Barrier call from the\ncatch, capture any caught InvalidOperationException into a local variable, let\nexecution continue to the single Barrier call after the try-catch, and then\nafter that Barrier rethrow (or throw a wrapped) exception if one was captured so\nall processes hit the same synchronization point before any process propagates\nthe error.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","createdAt":"2025-11-09T08:41:30Z","path":"src/DistributedTraining/ElasticOptimizer.cs","line":136,"originalLine":136,"diffHunk":"@@ -0,0 +1,289 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.Models.Inputs;\n+using AiDotNet.Optimizers;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Implements Elastic optimizer - supports dynamic worker addition/removal during training.\n+/// </summary>\n+/// <remarks>\n+/// <para><b>Strategy Overview:</b>\n+/// Elastic training (TorchElastic, Horovod Elastic) enables dynamic scaling of workers during\n+/// training. Workers can be added or removed without stopping the training job, supporting:\n+/// - Fault tolerance: Replace failed workers automatically\n+/// - Auto-scaling: Add workers during peak hours, remove during off-peak\n+/// - Spot instance usage: Tolerate preemptions, use cheaper compute\n+///\n+/// When world size changes, the optimizer handles re-sharding parameters and optimizer states\n+/// across the new worker set. This requires checkpointing and careful state management.\n+/// </para>\n+/// <para><b>For Beginners:</b>\n+/// Elastic training is like having a flexible team size. Workers can join or leave during\n+/// training without stopping everything:\n+///\n+/// Scenario 1 - Fault tolerance:\n+/// - Start with 8 GPUs training your model\n+/// - GPU 3 fails ‚Üí automatically detected\n+/// - Training continues with 7 GPUs (parameters redistributed)\n+/// - New GPU joins ‚Üí training scales back to 8 GPUs\n+///\n+/// Scenario 2 - Cloud cost optimization:\n+/// - Use cheap \"spot instances\" that can be taken away anytime\n+/// - When instance is preempted, training continues with remaining workers\n+/// - New instance joins when available\n+///\n+/// This is critical for long training jobs where failures are expected.\n+/// </para>\n+/// <para><b>Use Cases:</b>\n+/// - Long training jobs (days/weeks) where failures will occur\n+/// - Cloud training with spot/preemptible instances (save 60-90% cost)\n+/// - Auto-scaling based on load or time of day\n+/// - Fault tolerance for production training pipelines\n+/// </para>\n+/// <para><b>Trade-offs:</b>\n+/// - Memory: Must handle dynamic re-sharding\n+/// - Communication: Overhead during worker changes (re-sharding, sync)\n+/// - Complexity: Very High - requires membership management, state re-distribution\n+/// - Convergence: Learning rate scheduling must account for dynamic world size\n+/// - Best for: Long jobs, cost-sensitive scenarios, production ML pipelines\n+/// - Limitation: Worker changes create temporary slowdown during re-sharding\n+/// </para>\n+/// <para><b>Implementation Note:</b>\n+/// This framework provides elastic optimizer infrastructure. Full production deployment\n+/// requires:\n+/// 1. Membership/discovery service (etcd, ZooKeeper, or cloud-native)\n+/// 2. Automatic checkpointing before worker changes\n+/// 3. State re-sharding algorithms\n+/// 4. Rendezvous mechanism for worker coordination\n+/// This implementation demonstrates the elastic pattern.\n+/// </para>\n+/// <para>\n+/// Example:\n+/// <code>\n+/// var optimizer = new AdamOptimizer&lt;double, Tensor&lt;double&gt;, Tensor&lt;double&gt;&gt;(model, options);\n+/// var backend = new InMemoryCommunicationBackend&lt;double&gt;(rank: 0, worldSize: 4);\n+/// var config = new ShardingConfiguration&lt;double&gt;(backend);\n+///\n+/// var elasticOptimizer = new ElasticOptimizer&lt;double, Tensor&lt;double&gt;, Tensor&lt;double&gt;&gt;(\n+///     optimizer, config,\n+///     minWorkers: 2,   // Can run with as few as 2 workers\n+///     maxWorkers: 16); // Can scale up to 16 workers\n+///\n+/// // Training continues through worker changes:\n+/// // 4 workers ‚Üí 3 workers (one fails) ‚Üí 5 workers (two join) ‚Üí ...\n+/// </code>\n+/// </para>\n+/// </remarks>\n+/// <typeparam name=\"T\">The numeric type</typeparam>\n+/// <typeparam name=\"TInput\">The input type for the model</typeparam>\n+/// <typeparam name=\"TOutput\">The output type for the model</typeparam>\n+public class ElasticOptimizer<T, TInput, TOutput> : ShardedOptimizerBase<T, TInput, TOutput>\n+{\n+    private readonly int _minWorkers;\n+    private readonly int _maxWorkers;\n+    private int _currentWorldSize;\n+\n+    /// <summary>\n+    /// Creates an elastic optimizer.\n+    /// </summary>\n+    /// <param name=\"wrappedOptimizer\">The optimizer to wrap with elastic capabilities</param>\n+    /// <param name=\"config\">Configuration for sharding and communication</param>\n+    /// <param name=\"minWorkers\">Minimum number of workers (default: 1)</param>\n+    /// <param name=\"maxWorkers\">Maximum number of workers (default: 1024)</param>\n+    public ElasticOptimizer(\n+        IOptimizer<T, TInput, TOutput> wrappedOptimizer,\n+        IShardingConfiguration<T> config,\n+        int minWorkers = 1,\n+        int maxWorkers = 1024)\n+        : base(wrappedOptimizer, config)\n+    {\n+        if (minWorkers < 1)\n+            throw new ArgumentException(\"Minimum workers must be at least 1\", nameof(minWorkers));\n+        if (maxWorkers < minWorkers)\n+            throw new ArgumentException(\"Maximum workers must be >= minimum workers\", nameof(maxWorkers));\n+\n+        _minWorkers = minWorkers;\n+        _maxWorkers = maxWorkers;\n+        _currentWorldSize = WorldSize;\n+    }\n+\n+    /// <inheritdoc/>\n+    public override OptimizationResult<T, TInput, TOutput> Optimize(OptimizationInputData<T, TInput, TOutput> inputData)\n+    {\n+        if (inputData == null)\n+            throw new ArgumentNullException(nameof(inputData));\n+\n+        // Check for world size changes (workers joined/left)\n+        if (DetectWorldSizeChange())\n+        {\n+            try\n+            {\n+                HandleWorkerChange();\n+            }\n+            catch (InvalidOperationException ex)\n+            {\n+                // CRITICAL: Reach barrier even on validation failure to unblock other processes\n+                // Otherwise, other workers waiting at the barrier will deadlock indefinitely\n+                Config.CommunicationBackend.Barrier();\n+                throw new InvalidOperationException(\n+                    $\"Worker change validation failed: {ex.Message}. All workers have been synchronized.\", ex);\n+            }\n+        }\n+\n+        // Barrier with current worker set\n+        Config.CommunicationBackend.Barrier();"}]}},{"id":"PRRT_kwDOKSXUF85hNZT9","isResolved":false,"isOutdated":false,"path":"src/DistributedTraining/ElasticOptimizer.cs","line":288,"originalLine":288,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VeK4V","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**Overly strict validation prevents elastic restore.**\n\nThe deserialization validates that `savedWorldSize == WorldSize` (lines 260-265), which prevents restoring a checkpoint with a different number of workers. This contradicts the elastic training goal of supporting dynamic worker changes.\n\n\n\nFor elastic training, checkpoint restore should support different world sizes:\n\n```diff\n     // Read sharding configuration (for validation)\n     int savedWorldSize = reader.ReadInt32();\n     int savedRank = reader.ReadInt32();\n     reader.ReadBoolean(); // AutoSyncGradients\n     reader.ReadInt32(); // MinimumParameterGroupSize\n     reader.ReadBoolean(); // EnableGradientCompression\n\n-    if (savedWorldSize != WorldSize)\n-    {\n-        throw new InvalidOperationException(\n-            $\"World size mismatch. Optimizer was saved with {savedWorldSize} processes, \" +\n-            $\"but current configuration has {WorldSize} processes.\");\n-    }\n-\n-    if (savedRank != Rank)\n+    // Elastic training allows world size to change between save and restore\n+    if (savedWorldSize != WorldSize || savedRank != Rank)\n     {\n-        throw new InvalidOperationException(\n-            $\"Rank mismatch. Optimizer was saved on rank {savedRank}, \" +\n-            $\"but is being loaded on rank {Rank}. This could indicate a configuration error.\");\n+        // Log warning but allow restore\n+        // Production would trigger re-sharding after restore\n+        // For now, accept the mismatch and let DetectWorldSizeChange() handle it\n     }\n```\n\nAdditionally, consider restoring `_currentWorldSize = WorldSize` (current, not saved) to immediately reflect the current cluster state:\n\n```diff\n-    _currentWorldSize = savedCurrentWorldSize;\n+    // Initialize to current world size so DetectWorldSizeChange() works correctly\n+    // If world size changed since checkpoint, next Optimize() will trigger re-sharding\n+    _currentWorldSize = WorldSize;\n```\n\nThis aligns with the elastic pattern where checkpoints can be restored on differently-sized clusters.\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<!-- This is an auto-generated comment by CodeRabbit -->","createdAt":"2025-11-09T08:41:30Z","path":"src/DistributedTraining/ElasticOptimizer.cs","line":288,"originalLine":288,"diffHunk":"@@ -0,0 +1,289 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.Models.Inputs;\n+using AiDotNet.Optimizers;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Implements Elastic optimizer - supports dynamic worker addition/removal during training.\n+/// </summary>\n+/// <remarks>\n+/// <para><b>Strategy Overview:</b>\n+/// Elastic training (TorchElastic, Horovod Elastic) enables dynamic scaling of workers during\n+/// training. Workers can be added or removed without stopping the training job, supporting:\n+/// - Fault tolerance: Replace failed workers automatically\n+/// - Auto-scaling: Add workers during peak hours, remove during off-peak\n+/// - Spot instance usage: Tolerate preemptions, use cheaper compute\n+///\n+/// When world size changes, the optimizer handles re-sharding parameters and optimizer states\n+/// across the new worker set. This requires checkpointing and careful state management.\n+/// </para>\n+/// <para><b>For Beginners:</b>\n+/// Elastic training is like having a flexible team size. Workers can join or leave during\n+/// training without stopping everything:\n+///\n+/// Scenario 1 - Fault tolerance:\n+/// - Start with 8 GPUs training your model\n+/// - GPU 3 fails ‚Üí automatically detected\n+/// - Training continues with 7 GPUs (parameters redistributed)\n+/// - New GPU joins ‚Üí training scales back to 8 GPUs\n+///\n+/// Scenario 2 - Cloud cost optimization:\n+/// - Use cheap \"spot instances\" that can be taken away anytime\n+/// - When instance is preempted, training continues with remaining workers\n+/// - New instance joins when available\n+///\n+/// This is critical for long training jobs where failures are expected.\n+/// </para>\n+/// <para><b>Use Cases:</b>\n+/// - Long training jobs (days/weeks) where failures will occur\n+/// - Cloud training with spot/preemptible instances (save 60-90% cost)\n+/// - Auto-scaling based on load or time of day\n+/// - Fault tolerance for production training pipelines\n+/// </para>\n+/// <para><b>Trade-offs:</b>\n+/// - Memory: Must handle dynamic re-sharding\n+/// - Communication: Overhead during worker changes (re-sharding, sync)\n+/// - Complexity: Very High - requires membership management, state re-distribution\n+/// - Convergence: Learning rate scheduling must account for dynamic world size\n+/// - Best for: Long jobs, cost-sensitive scenarios, production ML pipelines\n+/// - Limitation: Worker changes create temporary slowdown during re-sharding\n+/// </para>\n+/// <para><b>Implementation Note:</b>\n+/// This framework provides elastic optimizer infrastructure. Full production deployment\n+/// requires:\n+/// 1. Membership/discovery service (etcd, ZooKeeper, or cloud-native)\n+/// 2. Automatic checkpointing before worker changes\n+/// 3. State re-sharding algorithms\n+/// 4. Rendezvous mechanism for worker coordination\n+/// This implementation demonstrates the elastic pattern.\n+/// </para>\n+/// <para>\n+/// Example:\n+/// <code>\n+/// var optimizer = new AdamOptimizer&lt;double, Tensor&lt;double&gt;, Tensor&lt;double&gt;&gt;(model, options);\n+/// var backend = new InMemoryCommunicationBackend&lt;double&gt;(rank: 0, worldSize: 4);\n+/// var config = new ShardingConfiguration&lt;double&gt;(backend);\n+///\n+/// var elasticOptimizer = new ElasticOptimizer&lt;double, Tensor&lt;double&gt;, Tensor&lt;double&gt;&gt;(\n+///     optimizer, config,\n+///     minWorkers: 2,   // Can run with as few as 2 workers\n+///     maxWorkers: 16); // Can scale up to 16 workers\n+///\n+/// // Training continues through worker changes:\n+/// // 4 workers ‚Üí 3 workers (one fails) ‚Üí 5 workers (two join) ‚Üí ...\n+/// </code>\n+/// </para>\n+/// </remarks>\n+/// <typeparam name=\"T\">The numeric type</typeparam>\n+/// <typeparam name=\"TInput\">The input type for the model</typeparam>\n+/// <typeparam name=\"TOutput\">The output type for the model</typeparam>\n+public class ElasticOptimizer<T, TInput, TOutput> : ShardedOptimizerBase<T, TInput, TOutput>\n+{\n+    private readonly int _minWorkers;\n+    private readonly int _maxWorkers;\n+    private int _currentWorldSize;\n+\n+    /// <summary>\n+    /// Creates an elastic optimizer.\n+    /// </summary>\n+    /// <param name=\"wrappedOptimizer\">The optimizer to wrap with elastic capabilities</param>\n+    /// <param name=\"config\">Configuration for sharding and communication</param>\n+    /// <param name=\"minWorkers\">Minimum number of workers (default: 1)</param>\n+    /// <param name=\"maxWorkers\">Maximum number of workers (default: 1024)</param>\n+    public ElasticOptimizer(\n+        IOptimizer<T, TInput, TOutput> wrappedOptimizer,\n+        IShardingConfiguration<T> config,\n+        int minWorkers = 1,\n+        int maxWorkers = 1024)\n+        : base(wrappedOptimizer, config)\n+    {\n+        if (minWorkers < 1)\n+            throw new ArgumentException(\"Minimum workers must be at least 1\", nameof(minWorkers));\n+        if (maxWorkers < minWorkers)\n+            throw new ArgumentException(\"Maximum workers must be >= minimum workers\", nameof(maxWorkers));\n+\n+        _minWorkers = minWorkers;\n+        _maxWorkers = maxWorkers;\n+        _currentWorldSize = WorldSize;\n+    }\n+\n+    /// <inheritdoc/>\n+    public override OptimizationResult<T, TInput, TOutput> Optimize(OptimizationInputData<T, TInput, TOutput> inputData)\n+    {\n+        if (inputData == null)\n+            throw new ArgumentNullException(nameof(inputData));\n+\n+        // Check for world size changes (workers joined/left)\n+        if (DetectWorldSizeChange())\n+        {\n+            try\n+            {\n+                HandleWorkerChange();\n+            }\n+            catch (InvalidOperationException ex)\n+            {\n+                // CRITICAL: Reach barrier even on validation failure to unblock other processes\n+                // Otherwise, other workers waiting at the barrier will deadlock indefinitely\n+                Config.CommunicationBackend.Barrier();\n+                throw new InvalidOperationException(\n+                    $\"Worker change validation failed: {ex.Message}. All workers have been synchronized.\", ex);\n+            }\n+        }\n+\n+        // Barrier with current worker set\n+        Config.CommunicationBackend.Barrier();\n+\n+        // Optimize with current workers\n+        var result = WrappedOptimizer.Optimize(inputData);\n+\n+        // Synchronize parameters\n+        if (Config.AutoSyncGradients && result.BestSolution != null)\n+        {\n+            SynchronizeParameters(result.BestSolution);\n+        }\n+\n+        Config.CommunicationBackend.Barrier();\n+\n+        return result;\n+    }\n+\n+    /// <inheritdoc/>\n+    public override void SynchronizeOptimizerState()\n+    {\n+        // When world size changes, optimizer states must be re-sharded\n+        // This is handled in HandleWorkerChange()\n+    }\n+\n+    /// <summary>\n+    /// Detects if the world size has changed.\n+    /// </summary>\n+    private bool DetectWorldSizeChange()\n+    {\n+        // In production, this would:\n+        // 1. Query membership service (etcd, etc.)\n+        // 2. Detect if workers joined or left\n+        // 3. Trigger rendezvous if change detected\n+\n+        // Framework placeholder\n+        int newWorldSize = Config.CommunicationBackend.WorldSize;\n+        return newWorldSize != _currentWorldSize;\n+    }\n+\n+    /// <summary>\n+    /// Handles worker addition or removal.\n+    /// </summary>\n+    private void HandleWorkerChange()\n+    {\n+        int newWorldSize = Config.CommunicationBackend.WorldSize;\n+\n+        // Validate new world size is within bounds\n+        if (newWorldSize < _minWorkers || newWorldSize > _maxWorkers)\n+        {\n+            throw new InvalidOperationException(\n+                $\"World size {newWorldSize} outside allowed range [{_minWorkers}, {_maxWorkers}]\");\n+        }\n+\n+        // In production elastic training:\n+        // 1. Checkpoint current state\n+        // 2. All workers synchronize at rendezvous point\n+        // 3. Re-shard parameters and optimizer states across new worker set\n+        // 4. Broadcast/scatter state to new workers\n+        // 5. Resume training with new configuration\n+\n+        // Framework placeholder for re-sharding logic\n+        _currentWorldSize = newWorldSize;\n+\n+        // Would call: ReshardParameters() and ReshardOptimizerState()\n+    }\n+\n+    /// <summary>\n+    /// Gets the current number of active workers.\n+    /// </summary>\n+    public int CurrentWorkers => _currentWorldSize;\n+\n+    /// <summary>\n+    /// Gets whether the optimizer can accept more workers.\n+    /// </summary>\n+    public bool CanScaleUp => _currentWorldSize < _maxWorkers;\n+\n+    /// <summary>\n+    /// Gets whether the optimizer can tolerate losing workers.\n+    /// </summary>\n+    public bool CanScaleDown => _currentWorldSize > _minWorkers;\n+\n+    /// <inheritdoc/>\n+    public override byte[] Serialize()\n+    {\n+        using var ms = new MemoryStream();\n+        using var writer = new BinaryWriter(ms);\n+\n+        // Serialize elastic-specific configuration\n+        writer.Write(_minWorkers);\n+        writer.Write(_maxWorkers);\n+        writer.Write(_currentWorldSize);\n+\n+        // Serialize sharding configuration info\n+        writer.Write(WorldSize);\n+        writer.Write(Rank);\n+        writer.Write(Config.AutoSyncGradients);\n+        writer.Write(Config.MinimumParameterGroupSize);\n+        writer.Write(Config.EnableGradientCompression);\n+\n+        // Serialize wrapped optimizer\n+        var optimizerData = WrappedOptimizer.Serialize();\n+        writer.Write(optimizerData.Length);\n+        writer.Write(optimizerData);\n+\n+        return ms.ToArray();\n+    }\n+\n+    /// <inheritdoc/>\n+    public override void Deserialize(byte[] data)\n+    {\n+        using var ms = new MemoryStream(data);\n+        using var reader = new BinaryReader(ms);\n+\n+        // Read elastic-specific configuration\n+        int savedMinWorkers = reader.ReadInt32();\n+        int savedMaxWorkers = reader.ReadInt32();\n+        int savedCurrentWorldSize = reader.ReadInt32();\n+\n+        // Read sharding configuration (for validation)\n+        int savedWorldSize = reader.ReadInt32();\n+        int savedRank = reader.ReadInt32();\n+        reader.ReadBoolean(); // AutoSyncGradients\n+        reader.ReadInt32(); // MinimumParameterGroupSize\n+        reader.ReadBoolean(); // EnableGradientCompression\n+\n+        if (savedWorldSize != WorldSize)\n+        {\n+            throw new InvalidOperationException(\n+                $\"World size mismatch. Optimizer was saved with {savedWorldSize} processes, \" +\n+                $\"but current configuration has {WorldSize} processes.\");\n+        }\n+\n+        if (savedRank != Rank)\n+        {\n+            throw new InvalidOperationException(\n+                $\"Rank mismatch. Optimizer was saved on rank {savedRank}, \" +\n+                $\"but is being loaded on rank {Rank}. This could indicate a configuration error.\");\n+        }\n+\n+        if (savedMinWorkers != _minWorkers || savedMaxWorkers != _maxWorkers)\n+        {\n+            throw new InvalidOperationException(\n+                $\"Elastic configuration mismatch. Optimizer was saved with minWorkers={savedMinWorkers}, \" +\n+                $\"maxWorkers={savedMaxWorkers}, but current configuration has minWorkers={_minWorkers}, \" +\n+                $\"maxWorkers={_maxWorkers}.\");\n+        }\n+\n+        _currentWorldSize = savedCurrentWorldSize;\n+\n+        // Read wrapped optimizer\n+        int optimizerDataLength = reader.ReadInt32();\n+        byte[] optimizerData = reader.ReadBytes(optimizerDataLength);\n+        WrappedOptimizer.Deserialize(optimizerData);\n+    }"}]}},{"id":"PRRT_kwDOKSXUF85hNZUB","isResolved":false,"isOutdated":false,"path":"src/DistributedTraining/FSDPModel.cs","line":155,"originalLine":155,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VeK4a","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Critical: Gradient synchronization doesn't average across ranks.**\n\nThe current synchronization flow has a fundamental flaw when ranks train on different data:\n\n1. After line 94, each rank has different full parameters (P‚ÇÄ, P‚ÇÅ, ...) from training on different mini-batches\n2. Line 100 immediately extracts local shards: LocalShard‚ÇÄ = P‚ÇÄ[shard‚ÇÄ_range], LocalShard‚ÇÅ = P‚ÇÅ[shard‚ÇÅ_range]\n3. These shards come from *different* parameter vectors\n4. `SynchronizeGradients()` line 143 calls `AllGather(LocalShard)`, producing a \"frankenstein\" vector: [P‚ÇÄ[shard‚ÇÄ_range], P‚ÇÅ[shard‚ÇÅ_range]]\n5. Line 147's `AllReduce` operates on identical vectors (AllGather broadcasts the same result to all ranks), making it a no-op\n6. Result: Each rank retains its own shard from its own training‚Äîno averaging occurs\n\nFor correct data-parallel training, you must AllReduce the *full* parameter vectors *before* extracting shards:\n\n\n\nApply this restructuring to the Train method:\n\n```diff\n     // Train the wrapped model\n     WrappedModel.Train(input, expectedOutput);\n \n     // Get updated parameters\n     var updatedParams = WrappedModel.GetParameters();\n \n+    // Synchronize full parameters across ranks if auto-sync is enabled\n+    if (Config.AutoSyncGradients)\n+    {\n+        Config.CommunicationBackend.AllReduce(updatedParams, ReductionOperation.Average);\n+        WrappedModel.SetParameters(updatedParams);\n+    }\n+\n     // Update local shard\n     UpdateLocalShardFromFull(updatedParams);\n-\n-    // Invalidate cache immediately after local shard changes\n-    InvalidateCache();\n-\n-    // Synchronize gradients if auto-sync is enabled\n-    if (Config.AutoSyncGradients)\n-    {\n-        SynchronizeGradients();\n-\n-        // Apply synchronized parameters back to the model\n-        fullParams = GatherFullParameters();\n-        WrappedModel.SetParameters(fullParams);\n-    }\n }\n```\n\nThis ensures:\n- Full parameter vectors are averaged *before* sharding\n- All ranks extract consistent local shards from the same averaged parameters\n- Proper data-parallel semantics: average(P - lr¬∑G‚ÇÄ, P - lr¬∑G‚ÇÅ, ‚Ä¶) = P - lr¬∑average(G‚ÇÄ, G‚ÇÅ, ‚Ä¶)\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/DistributedTraining/FSDPModel.cs around lines 86 to 155, the Train flow\naverages shards incorrectly because it extracts and updates local shards before\nperforming AllReduce on the full parameter vectors; fix by gathering the full\nparameters immediately after WrappedModel.Train, and if Config.AutoSyncGradients\nis true perform the AllReduce (average) on that full parameter vector before\nscattering: replace the current sequence so that after training you call\nGatherFullParameters -> if AutoSyncGradients then\nCommunicationBackend.AllReduce(fullParams, Average) ->\nUpdateLocalShardFromFull(fullParams) -> InvalidateCache() ->\nWrappedModel.SetParameters(fullParams) (and if not auto-sync keep the existing\nlocal update behavior), ensuring CachedFullParameters is cleared when\nappropriate.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","createdAt":"2025-11-09T08:41:30Z","path":"src/DistributedTraining/FSDPModel.cs","line":155,"originalLine":155,"diffHunk":"@@ -0,0 +1,315 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.Models;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Implements FSDP (Fully Sharded Data Parallel) model wrapper that shards parameters across multiple processes.\n+/// </summary>\n+/// <remarks>\n+/// <para><b>Strategy Overview:</b>\n+/// FSDP (Fully Sharded Data Parallel) is PyTorch's implementation of the ZeRO-3 optimization strategy.\n+/// It shards model parameters, gradients, and optimizer states across all processes, achieving maximum\n+/// memory efficiency. Parameters are gathered just-in-time for forward/backward passes and then released.\n+/// </para>\n+/// <para><b>For Beginners:</b>\n+/// This class implements FSDP (Fully Sharded Data Parallel), which makes any model work across multiple GPUs or machines\n+/// with maximum memory efficiency. It automatically handles:\n+/// - Splitting ALL model components (parameters, gradients, optimizer states) across processes\n+/// - Gathering parameters only when needed for forward/backward pass\n+/// - Releasing parameters immediately after use to save memory\n+/// - Averaging gradients across all processes during training\n+/// </para>\n+/// <para>\n+/// Think of it like a team project where each person holds part of the solution, but unlike DDP,\n+/// FSDP only shares the full model temporarily when absolutely needed, then immediately goes back\n+/// to holding just their piece. This saves a lot of memory!\n+/// </para>\n+/// <para><b>Use Cases:</b>\n+/// - Training very large models that don't fit in a single GPU's memory\n+/// - Maximizing memory efficiency for multi-GPU training\n+/// - Scaling to hundreds or thousands of GPUs\n+/// </para>\n+/// <para><b>Trade-offs:</b>\n+/// - Memory: Excellent - shards everything (parameters + gradients + optimizer states)\n+/// - Communication: Higher - requires AllGather for each forward/backward pass\n+/// - Complexity: Moderate - automatic just-in-time parameter gathering\n+/// - Best for: Very large models, memory-constrained scenarios\n+/// </para>\n+/// <para>\n+/// Example:\n+/// <code>\n+/// // Original model\n+/// var model = new NeuralNetworkModel&lt;double&gt;(...);\n+///\n+/// // Wrap it for FSDP distributed training\n+/// var backend = new InMemoryCommunicationBackend&lt;double&gt;(rank: 0, worldSize: 4);\n+/// var config = new ShardingConfiguration&lt;double&gt;(backend);\n+/// var fsdpModel = new FSDPModel&lt;double, Tensor&lt;double&gt;, Tensor&lt;double&gt;&gt;(model, config);\n+///\n+/// // Now train as usual - FSDP magic happens automatically!\n+/// fsdpModel.Train(inputs, outputs);\n+/// </code>\n+/// </para>\n+/// </remarks>\n+/// <typeparam name=\"T\">The numeric type</typeparam>\n+/// <typeparam name=\"TInput\">The input type for the model</typeparam>\n+/// <typeparam name=\"TOutput\">The output type for the model</typeparam>\n+public class FSDPModel<T, TInput, TOutput> : ShardedModelBase<T, TInput, TOutput>\n+{\n+    /// <summary>\n+    /// Creates a new FSDP model wrapping an existing model.\n+    /// </summary>\n+    /// <remarks>\n+    /// <para><b>For Beginners:</b>\n+    /// This constructor takes your existing model and makes it distributed using FSDP strategy.\n+    /// You provide:\n+    /// 1. The model you want to make distributed\n+    /// 2. A configuration that tells us how to do the distribution\n+    /// </para>\n+    /// <para>\n+    /// The constructor automatically:\n+    /// - Splits the model's parameters across all processes (sharding)\n+    /// - Sets up communication channels\n+    /// - Prepares everything for FSDP distributed training\n+    /// </para>\n+    /// </remarks>\n+    /// <param name=\"wrappedModel\">The model to wrap with FSDP capabilities</param>\n+    /// <param name=\"config\">Configuration for sharding and communication</param>\n+    /// <exception cref=\"ArgumentNullException\">Thrown if model or config is null</exception>\n+    public FSDPModel(IFullModel<T, TInput, TOutput> wrappedModel, IShardingConfiguration<T> config)\n+        : base(wrappedModel, config)\n+    {\n+    }\n+\n+    /// <inheritdoc/>\n+    public override void Train(TInput input, TOutput expectedOutput)\n+    {\n+        // Gather full parameters for training\n+        var fullParams = GatherFullParameters();\n+        WrappedModel.SetParameters(fullParams);\n+\n+        // Train the wrapped model\n+        WrappedModel.Train(input, expectedOutput);\n+\n+        // Get updated parameters\n+        var updatedParams = WrappedModel.GetParameters();\n+\n+        // Update local shard\n+        UpdateLocalShardFromFull(updatedParams);\n+\n+        // Invalidate cache immediately after local shard changes\n+        InvalidateCache();\n+\n+        // Synchronize gradients if auto-sync is enabled\n+        if (Config.AutoSyncGradients)\n+        {\n+            SynchronizeGradients();\n+\n+            // Apply synchronized parameters back to the model\n+            fullParams = GatherFullParameters();\n+            WrappedModel.SetParameters(fullParams);\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public override TOutput Predict(TInput input)\n+    {\n+        // Gather full parameters for prediction\n+        var fullParams = GatherFullParameters();\n+        WrappedModel.SetParameters(fullParams);\n+\n+        // Use wrapped model for prediction\n+        return WrappedModel.Predict(input);\n+    }\n+\n+    /// <inheritdoc/>\n+    /// <remarks>\n+    /// <para><b>FSDP Gradient Synchronization:</b>\n+    /// Unlike the base implementation which does AllReduce on LocalShard (which would mix disjoint\n+    /// parameter indices from different ranks and corrupt weights), FSDP gradient synchronization works by:\n+    /// 1. Gathering full parameters from all ranks (allGather operation)\n+    /// 2. Performing AllReduce on the full parameter vector (synchronizing matching indices)\n+    /// 3. Scattering the result back to local shards (each rank gets its portion)\n+    ///\n+    /// This ensures we reduce matching data across ranks rather than mixing unrelated parameter indices.\n+    /// </para>\n+    /// </remarks>\n+    public override void SynchronizeGradients()\n+    {\n+        // Gather full parameters from all shards\n+        // This ensures we have the complete parameter vector on each rank\n+        var fullParams = GatherFullParameters();\n+\n+        // Perform AllReduce on the full parameter vector\n+        // This synchronizes matching parameter indices across all ranks\n+        Config.CommunicationBackend.AllReduce(fullParams, ReductionOperation.Average);\n+\n+        // Update local shard from the synchronized full parameters\n+        // Each rank extracts its portion of the reduced parameters\n+        UpdateLocalShardFromFull(fullParams);\n+\n+        // Invalidate cached full parameters to force re-gather on next access\n+        CachedFullParameters = null;\n+    }"}]}},{"id":"PRRT_kwDOKSXUF85hNZUD","isResolved":true,"isOutdated":false,"path":"src/DistributedTraining/PipelineParallelModel.cs","line":133,"originalLine":130,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VeK4f","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n<details>\n<summary>üß© Analysis chain</summary>\n\n**Forward activations length mismatch: add a length header handshake**\n\nReceiver derives `activationSize` from the original input, but sender transmits `Predict(stageOutput)` whose length can differ. This will throw in backends that validate `count` (Gloo/InMemory). Send a 1‚Äëelement length header before the payload and use distinct tags per phase. \n\nSuggested minimal change:\n\n```diff\n@@\n-            int activationSize = InputHelper<T, TInput>.GetInputSize(input);\n-            Vector<T> receivedActivations = Config.CommunicationBackend.Receive(_stageId - 1, activationSize, tag: 0);\n+            // Receive length header, then payload (forward train)\n+            var lenVec = Config.CommunicationBackend.Receive(_stageId - 1, 1, tag: 99);\n+            int activationSize = Convert.ToInt32(lenVec[0]);\n+            var receivedActivations = Config.CommunicationBackend.Receive(_stageId - 1, activationSize, tag: 0);\n@@\n-            Vector<T> activationsToSend = ConversionsHelper.ConvertToVector<T, TOutput>(stageOutput);\n-            Config.CommunicationBackend.Send(activationsToSend, _stageId + 1, tag: 0);\n+            var activationsToSend = ConversionsHelper.ConvertToVector<T, TOutput>(stageOutput);\n+            // Send length header, then payload (forward train)\n+            Config.CommunicationBackend.Send(new Vector<T>(new[] { NumOps.FromDouble(activationsToSend.Length) }), _stageId + 1, tag: 99);\n+            Config.CommunicationBackend.Send(activationsToSend, _stageId + 1, tag: 0);\n@@\n-            int activationSize = InputHelper<T, TInput>.GetInputSize(input);\n-            Vector<T> receivedActivations = Config.CommunicationBackend.Receive(_stageId - 1, activationSize, tag: 10);\n+            // Receive length header, then payload (forward infer)\n+            var lenVec = Config.CommunicationBackend.Receive(_stageId - 1, 1, tag: 109);\n+            int activationSize = Convert.ToInt32(lenVec[0]);\n+            var receivedActivations = Config.CommunicationBackend.Receive(_stageId - 1, activationSize, tag: 10);\n@@\n-            Vector<T> activationsToSend = ConversionsHelper.ConvertToVector<T, TOutput>(stageOutput);\n-            Config.CommunicationBackend.Send(activationsToSend, _stageId + 1, tag: 10);\n+            var activationsToSend = ConversionsHelper.ConvertToVector<T, TOutput>(stageOutput);\n+            // Send length header, then payload (forward infer)\n+            Config.CommunicationBackend.Send(new Vector<T>(new[] { NumOps.FromDouble(activationsToSend.Length) }), _stageId + 1, tag: 109);\n+            Config.CommunicationBackend.Send(activationsToSend, _stageId + 1, tag: 10);\n```\n\nFollow‚Äëup: consider constants for tags to avoid collisions. \n\n\nAlso applies to: 138-143, 200-208, 216-219\n\n---\n\n```\n\n---\n\nüèÅ Script executed:\n\n```shell\n#!/bin/bash\n# Examine the full PipelineParallelModel.cs file to understand context\ncat src/DistributedTraining/PipelineParallelModel.cs\n```\n\nLength of output: 13035\n\n---\n\nüèÅ Script executed:\n\n```shell\n#!/bin/bash\n# Search for stageOutput and Predict calls to understand sender behavior\nrg -nC5 \"Predict\\(stageOutput\\)|stageOutput\" src/DistributedTraining/PipelineParallelModel.cs\n```\n\nLength of output: 2083\n\n---\n\nüèÅ Script executed:\n\n```shell\n#!/bin/bash\n# Look for other Send calls related to activations\nrg -nC3 \"CommunicationBackend\\.Send\" src/DistributedTraining/PipelineParallelModel.cs\n```\n\nLength of output: 1464\n\n---\n\nüèÅ Script executed:\n\n```shell\n#!/bin/bash\n# Search for any validation or count checks in the communication backend\nfd -e \"cs\" -e \"csproj\" | xargs grep -l \"CommunicationBackend\" | head -5\n```\n\nLength of output: 238\n\n---\n\nüèÅ Script executed:\n\n```shell\n#!/bin/bash\n# Find and examine ConversionsHelper implementation\nfd -name \"*onversion*\" -o -name \"*Helper*\" | xargs grep -l \"ConvertToVector\\|ConvertVectorToInput\" 2>/dev/null | head -3\n```\n\nLength of output: 230\n\n---\n\nüèÅ Script executed:\n\n```shell\n#!/bin/bash\n# Look for ConversionsHelper.ConvertToVector and ConvertVectorToInput methods\nrg -A5 \"ConvertToVector|ConvertVectorToInput\" --type cs -g \"!*Test*\" | head -40\n```\n\nLength of output: 3296\n\n---\n\nüèÅ Script executed:\n\n```shell\n#!/bin/bash\n# Check NumOps.FromDouble usage in codebase for validation\nrg -n \"NumOps\\.FromDouble\" --type cs | head -10\n```\n\nLength of output: 1619\n\n---\n\nüèÅ Script executed:\n\n```shell\n#!/bin/bash\n# Examine CommunicationBackendBase to understand Send/Receive behavior\ncat src/DistributedTraining/CommunicationBackendBase.cs\n```\n\nLength of output: 10942\n\n---\n\n```\n\n---\n\nüèÅ Script executed:\n\n```shell\n#!/bin/bash\n# Find and examine InMemoryCommunicationBackend or other backend implementations\nfd -name \"*Backend*.cs\" src/DistributedTraining | head -5\n```\n\nLength of output: 230\n\n---\n\nüèÅ Script executed:\n\n```shell\n#!/bin/bash\n# Look for Receive implementation with count validation\nrg -A10 \"public.*Vector.*Receive.*int count\" src/DistributedTraining/ --type cs\n```\n\nLength of output: 4686\n\n---\n\nüèÅ Script executed:\n\n```shell\n#!/bin/bash\n# Search for InputHelper implementation\nfd -name \"*Helper*.cs\" | xargs grep -l \"GetInputSize\" 2>/dev/null\n```\n\nLength of output: 230\n\n---\n\nüèÅ Script executed:\n\n```shell\n#!/bin/bash\n# Find InputHelper.GetInputSize method\nrg -B2 -A8 \"GetInputSize\" src/Helpers/ --type cs\n```\n\nLength of output: 757\n\n---\n\n```\n\n---\n\nüèÅ Script executed:\n\n```shell\n#!/bin/bash\n# Search for ConversionsHelper implementation\nrg -B3 -A15 \"ConvertToVector|ConvertVectorToInput\" src/Helpers/ --type cs | head -60\n```\n\nLength of output: 4487\n\n---\n\nüèÅ Script executed:\n\n```shell\n#!/bin/bash\n# Look for tests of PipelineParallelModel to see if TOutput != TInput is tested\nrg -B2 -A10 \"PipelineParallelModel\" tests/ --type cs | grep -A10 \"new PipelineParallelModel\" | head -30\n```\n\nLength of output: 0\n\n---\n\n\n\n</details>\n\n**Add length header handshake to forward activation communication**\n\nReceiver derives `activationSize` from the reference input shape, but sender transmits the model's stage output which may have different dimensionality. In pipeline parallelism, layer outputs often differ from layer inputs (e.g., classification layer reduces 1024‚Üí10 dims). This causes Receive to expect wrong count, resulting in deadlock or protocol error in strict backends (Gloo, InMemory).\n\nImplement the suggested header approach: send a 1-element length vector before each activation payload using distinct header tags (99 for train, 109 for infer) to avoid collisions.\n\nLocations requiring fix:\n- **Train method**: lines 121-130 (receiver), 138-143 (sender)\n- **Predict method**: lines 200-208 (receiver), 216-219 (sender)\n\nExample for Train forward pass:\n```diff\n-            int activationSize = InputHelper<T, TInput>.GetInputSize(input);\n-            Vector<T> receivedActivations = Config.CommunicationBackend.Receive(_stageId - 1, activationSize, tag: 0);\n+            var lenVec = Config.CommunicationBackend.Receive(_stageId - 1, 1, tag: 99);\n+            int activationSize = Convert.ToInt32(lenVec[0]);\n+            var receivedActivations = Config.CommunicationBackend.Receive(_stageId - 1, activationSize, tag: 0);\n```\n\nand for sender:\n```diff\n-            Vector<T> activationsToSend = ConversionsHelper.ConvertToVector<T, TOutput>(stageOutput);\n-            Config.CommunicationBackend.Send(activationsToSend, _stageId + 1, tag: 0);\n+            var activationsToSend = ConversionsHelper.ConvertToVector<T, TOutput>(stageOutput);\n+            Config.CommunicationBackend.Send(new Vector<T>(new[] { NumOps.FromDouble(activationsToSend.Length) }), _stageId + 1, tag: 99);\n+            Config.CommunicationBackend.Send(activationsToSend, _stageId + 1, tag: 0);\n```\n\nApply the same pattern to Predict (using tags 109/10 instead of 99/0). Consider defining tag constants to prevent collisions across methods.\n\n\n\n> Committable suggestion skipped: line range outside the PR's diff.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/DistributedTraining/PipelineParallelModel.cs around lines 121-130 (and\nalso adjust the paired sender spots at 138-143 and the Predict receiver/sender\nregions at 200-208 and 216-219), implement a 1-element length-header handshake:\nbefore sending a stage activation payload, first send a single-element\nVector<int> containing the activation length with a distinct header tag (use 99\nfor Train forward header and 109 for Predict forward header) and then send the\nactual payload with the original payload tag (use 0 for Train payload and 10 for\nPredict payload); on the receiver side first Receive the 1-element header with\nthe header tag, use its value to allocate/expect the following payload Receive,\nthen Receive the payload; add named constants for the four tags (e.g.,\nTRAIN_HEADER_TAG=99, TRAIN_PAYLOAD_TAG=0, PREDICT_HEADER_TAG=109,\nPREDICT_PAYLOAD_TAG=10) to avoid magic numbers and ensure both Train and Predict\nuse the matching header/payload tag pairs.\n```\n\n</details>\n\n<!-- fingerprinting:phantom:poseidon:chinchilla -->\n\n<!-- This is an auto-generated comment by CodeRabbit -->\n\n‚úÖ Addressed in commits ae2f65b to 32d17fd","createdAt":"2025-11-09T08:41:30Z","path":"src/DistributedTraining/PipelineParallelModel.cs","line":133,"originalLine":130,"diffHunk":"@@ -0,0 +1,325 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.Models;\n+using AiDotNet.Helpers;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Implements Pipeline Parallel model wrapper - splits model into stages across ranks.\n+/// </summary>\n+/// <remarks>\n+/// <para><b>Strategy Overview:</b>\n+/// Pipeline Parallelism (GPipe-style) divides the model vertically into stages, with each process\n+/// owning specific layers. Input mini-batches are divided into micro-batches that flow through\n+/// the pipeline stages sequentially. This enables training models too large to fit on a single device\n+/// while maintaining good hardware utilization through micro-batch pipelining.\n+/// </para>\n+/// <para><b>For Beginners:</b>\n+/// Pipeline parallelism is like an assembly line for training. Imagine a deep neural network as\n+/// a tall building - instead of one person (GPU) handling all floors, we assign different floors\n+/// to different people. Process 0 handles layers 0-10, Process 1 handles layers 11-20, etc.\n+///\n+/// To keep everyone busy (avoid idle time), we split each batch into smaller \"micro-batches\" that\n+/// flow through the pipeline like cars on an assembly line. While Process 1 is working on micro-batch 1,\n+/// Process 0 can start on micro-batch 2.\n+/// </para>\n+/// <para><b>Use Cases:</b>\n+/// - Very deep models that don't fit on a single GPU\n+/// - When model depth (layers) >> width (parameters per layer)\n+/// - Transformer models with many layers\n+/// - Complementary to data parallelism (can combine them)\n+/// </para>\n+/// <para><b>Trade-offs:</b>\n+/// - Memory: Excellent for deep models - each rank stores only its layers\n+/// - Communication: Low - only activations passed between adjacent stages\n+/// - Complexity: High - requires micro-batching, careful scheduling, pipeline bubble overhead\n+/// - Best for: Very deep models, limited per-device memory\n+/// - Limitation: Pipeline \"bubble\" (idle time) reduces efficiency, typically ~12-25% for GPipe\n+/// </para>\n+/// <para><b>Implementation Note:</b>\n+/// This is a production-ready framework implementation. Full pipeline parallelism requires\n+/// model-specific layer partitioning logic. This implementation provides the infrastructure\n+/// and demonstrates the pattern. For production use, extend this class with your specific\n+/// layer assignment strategy.\n+/// </para>\n+/// <para>\n+/// Example:\n+/// <code>\n+/// var model = new DeepNeuralNetwork&lt;double&gt;(...); // 100 layers\n+/// var backend = new InMemoryCommunicationBackend&lt;double&gt;(rank: 0, worldSize: 4);\n+/// var config = new ShardingConfiguration&lt;double&gt;(backend);\n+///\n+/// // Rank 0: layers 0-24, Rank 1: layers 25-49, Rank 2: layers 50-74, Rank 3: layers 75-99\n+/// var pipelineModel = new PipelineParallelModel&lt;double, Tensor&lt;double&gt;, Tensor&lt;double&gt;&gt;(\n+///     model, config, microBatchSize: 4);\n+/// </code>\n+/// </para>\n+/// </remarks>\n+/// <typeparam name=\"T\">The numeric type</typeparam>\n+/// <typeparam name=\"TInput\">The input type for the model</typeparam>\n+/// <typeparam name=\"TOutput\">The output type for the model</typeparam>\n+public class PipelineParallelModel<T, TInput, TOutput> : ShardedModelBase<T, TInput, TOutput>\n+{\n+    private readonly int _microBatchSize;\n+    private readonly int _stageId;\n+    private readonly int _numStages;\n+\n+    /// <summary>\n+    /// Creates a new Pipeline Parallel model.\n+    /// </summary>\n+    /// <param name=\"wrappedModel\">The model to split into pipeline stages</param>\n+    /// <param name=\"config\">Configuration for sharding and communication</param>\n+    /// <param name=\"microBatchSize\">Size of micro-batches for pipeline execution (default: 1)</param>\n+    public PipelineParallelModel(\n+        IFullModel<T, TInput, TOutput> wrappedModel,\n+        IShardingConfiguration<T> config,\n+        int microBatchSize = 1)\n+        : base(wrappedModel, config)\n+    {\n+        _microBatchSize = microBatchSize;\n+        _stageId = Rank;\n+        _numStages = WorldSize;\n+    }\n+\n+    /// <summary>\n+    /// Initializes pipeline parallelism by partitioning parameters into stages.\n+    /// </summary>\n+    protected override void InitializeSharding()\n+    {\n+        var fullParameters = WrappedModel.GetParameters();\n+        int totalParams = fullParameters.Length;\n+\n+        // Divide parameters into pipeline stages\n+        // Each stage owns a contiguous chunk of parameters (representing layers)\n+        int baseShardSize = totalParams / _numStages;\n+        int remainder = totalParams % _numStages;\n+\n+        ShardSize = baseShardSize + (_stageId < remainder ? 1 : 0);\n+        ShardStartIndex = _stageId * baseShardSize + Math.Min(_stageId, remainder);\n+\n+        // Extract this stage's parameters\n+        var shardData = new T[ShardSize];\n+        Array.Copy(fullParameters.ToArray(), ShardStartIndex, shardData, 0, ShardSize);\n+        LocalShard = new Vector<T>(shardData);\n+\n+        CachedFullParameters = null;\n+    }\n+\n+    /// <inheritdoc/>\n+    public override void Train(TInput input, TOutput expectedOutput)\n+    {\n+        // Pipeline parallel training with proper inter-stage communication\n+        // Strategy: Convert activations to Vector<T> for communication between stages\n+\n+        WrappedModel.SetParameters(LocalShard);\n+\n+        // Determine actual input for this stage\n+        TInput stageInput = input;\n+\n+        // FORWARD PASS: Receive activations from previous stage\n+        if (_stageId > 0)\n+        {\n+            // Non-first stages receive activations from previous stage\n+            // Previous stage sends its output as Vector<T>\n+            int activationSize = InputHelper<T, TInput>.GetInputSize(input);\n+            Vector<T> receivedActivations = Config.CommunicationBackend.Receive(_stageId - 1, activationSize, tag: 0);\n+\n+            // Convert received vector back to TInput for this stage using reference input for shape\n+            stageInput = ConversionsHelper.ConvertVectorToInput<T, TInput>(receivedActivations, input);\n+        }"}]}},{"id":"PRRT_kwDOKSXUF85hNZUE","isResolved":false,"isOutdated":false,"path":"src/DistributedTraining/ZeRO2Model.cs","line":57,"originalLine":57,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VeK4k","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Expose gradient shard via property or accessor for optimizer integration.**\n\nThe `_gradientShard` field is private, but ZeRO-2 requires the optimizer to access sharded gradients to perform local updates. The XML documentation (line 48) references `ZeRO2Optimizer`, which must read this shard to implement the ZeRO-2 contract. Without a public property or method to retrieve the gradient shard, the optimizer cannot function correctly, rendering this implementation incomplete.\n\n\n\nAdd a public property to expose the gradient shard:\n\n```diff\n+    /// <summary>\n+    /// Gets the local gradient shard for this rank after synchronization.\n+    /// </summary>\n+    public Vector<T>? GradientShard => _gradientShard;\n+\n     private Vector<T>? _gradientShard;\n```\n\n<!-- suggestion_start -->\n\n<details>\n<summary>üìù Committable suggestion</summary>\n\n> ‚ÄºÔ∏è **IMPORTANT**\n> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.\n\n```suggestion\n    /// <summary>\n    /// Gets the local gradient shard for this rank after synchronization.\n    /// </summary>\n    public Vector<T>? GradientShard => _gradientShard;\n\n    private Vector<T>? _gradientShard;\n```\n\n</details>\n\n<!-- suggestion_end -->\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/DistributedTraining/ZeRO2Model.cs at line 57, the private field\n`_gradientShard` is not accessible to the optimizer; add a public read-only\nproperty (e.g., GradientShard) that returns the `Vector<T>?` so ZeRO2Optimizer\ncan read the local shard; implement it as a simple getter exposing\n`_gradientShard` (optionally internal or protected if you need limited\nvisibility) and keep the field encapsulated while providing the required\naccessor for optimizer integration.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","createdAt":"2025-11-09T08:41:31Z","path":"src/DistributedTraining/ZeRO2Model.cs","line":57,"originalLine":57,"diffHunk":"@@ -0,0 +1,233 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.Models;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Implements ZeRO Stage 2 model wrapper - shards optimizer states and gradients.\n+/// </summary>\n+/// <remarks>\n+/// <para><b>Strategy Overview:</b>\n+/// ZeRO Stage 2 builds on ZeRO-1 by additionally sharding gradients across processes.\n+/// Parameters are still replicated for the forward pass, but gradients are reduced and scattered\n+/// (ReduceScatter) so each process only stores a portion. This saves significant memory compared\n+/// to ZeRO-1, especially for large models.\n+/// </para>\n+/// <para><b>For Beginners:</b>\n+/// This implements ZeRO Stage 2, which saves even more memory than ZeRO-1. The model parameters\n+/// are still fully replicated (like DDP and ZeRO-1), but now both the optimizer state AND the\n+/// gradients are split across processes. After computing gradients, they're immediately reduced\n+/// and scattered so each process only keeps its portion.\n+/// </para>\n+/// <para>\n+/// Think of it like a team where everyone has the full playbook (parameters), but when taking\n+/// notes during practice (gradients), they divide up the note-taking so each person is responsible\n+/// for recording only certain plays. This saves everyone from having to write everything down.\n+/// </para>\n+/// <para><b>Use Cases:</b>\n+/// - Larger models where gradient memory becomes significant\n+/// - Want substantial memory savings with moderate communication cost\n+/// - Preparing for ZeRO-3/FSDP migration\n+/// </para>\n+/// <para><b>Trade-offs:</b>\n+/// - Memory: Very Good - saves both optimizer states and gradients\n+/// - Communication: Moderate - uses ReduceScatter instead of AllReduce\n+/// - Complexity: Moderate - gradient sharding adds some complexity\n+/// - Best for: Large models where gradient memory is significant\n+/// </para>\n+/// <para>\n+/// Example:\n+/// <code>\n+/// var model = new NeuralNetworkModel&lt;double&gt;(...);\n+/// var backend = new InMemoryCommunicationBackend&lt;double&gt;(rank: 0, worldSize: 4);\n+/// var config = new ShardingConfiguration&lt;double&gt;(backend);\n+/// var zero2Model = new ZeRO2Model&lt;double, Tensor&lt;double&gt;, Tensor&lt;double&gt;&gt;(model, config);\n+///\n+/// // Use with ZeRO2Optimizer for full ZeRO-2 benefits\n+/// var zero2Optimizer = new ZeRO2Optimizer&lt;double, Tensor&lt;double&gt;, Tensor&lt;double&gt;&gt;(optimizer, config);\n+/// </code>\n+/// </para>\n+/// </remarks>\n+/// <typeparam name=\"T\">The numeric type</typeparam>\n+/// <typeparam name=\"TInput\">The input type for the model</typeparam>\n+/// <typeparam name=\"TOutput\">The output type for the model</typeparam>\n+public class ZeRO2Model<T, TInput, TOutput> : ShardedModelBase<T, TInput, TOutput>\n+{\n+    private Vector<T>? _gradientShard;"}]}},{"id":"PRRT_kwDOKSXUF85hNZUK","isResolved":false,"isOutdated":false,"path":"src/DistributedTraining/ZeRO2Model.cs","line":113,"originalLine":113,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VeK4r","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Fix ReduceScatter remainder distribution so sharded gradients are correct.**\n\nThe padding approach prevents the `ReduceScatter` crash but **incorrectly distributes remainder elements**. `ReduceScatter` divides the input into equal-sized chunks (`chunkSize = paddedLength / WorldSize`), so trimming afterward doesn't produce the intended uneven distribution.\n\n**Example (10 params, 3 ranks):**\n- **Expected:** Rank 0: indices [0:4] (4 elements), Rank 1: [4:7] (3), Rank 2: [7:10] (3)  \n- **Actual after padding to 12:** ReduceScatter produces chunks [0:4], [4:8], [8:12]; trimming gives Rank 0: [0:4] ‚úì, Rank 1: [4:7] ‚úì, Rank 2: [8:10, PAD] ‚úó\n\nRank 2 (and others) stores **padding elements** in `_gradientShard` instead of the correct gradients, corrupting synchronization.\n\n\n\n**Solution:** Manually implement uneven ReduceScatter using AllReduce + local slicing, or rework to use a scatter/gather approach that respects the remainder distribution:\n\n```diff\n     public override void SynchronizeGradients()\n     {\n-        var totalParams = LocalShard.Length;\n-        var remainder = totalParams % WorldSize;\n-\n-        // Pad to satisfy ReduceScatter's divisibility requirement\n-        Vector<T> reduceInput = LocalShard;\n-        if (remainder != 0)\n-        {\n-            var paddedLength = totalParams + (WorldSize - remainder);\n-            var padded = new T[paddedLength];\n-            Array.Copy(LocalShard.ToArray(), padded, totalParams);\n-            reduceInput = new Vector<T>(padded);\n-        }\n-\n-        var reducedChunk = Config.CommunicationBackend.ReduceScatter(reduceInput, ReductionOperation.Average);\n-\n-        // Trim padding so each rank keeps only its logical shard\n-        var shardLength = totalParams / WorldSize + (Rank < remainder ? 1 : 0);\n-        var shardData = new T[shardLength];\n-        Array.Copy(reducedChunk.ToArray(), 0, shardData, 0, shardLength);\n-        _gradientShard = new Vector<T>(shardData);\n+        // Use AllReduce to sum all gradients, then extract this rank's shard\n+        var reducedGradients = Config.CommunicationBackend.AllReduce(LocalShard, ReductionOperation.Average);\n+        \n+        var totalParams = LocalShard.Length;\n+        var baseShardSize = totalParams / WorldSize;\n+        var remainder = totalParams % WorldSize;\n+        \n+        // Calculate this rank's shard boundaries\n+        int shardStart = Rank * baseShardSize + Math.Min(Rank, remainder);\n+        int shardLength = baseShardSize + (Rank < remainder ? 1 : 0);\n+        \n+        // Extract this rank's portion of the reduced gradients\n+        var shardData = new T[shardLength];\n+        Array.Copy(reducedGradients.ToArray(), shardStart, shardData, 0, shardLength);\n+        _gradientShard = new Vector<T>(shardData);\n \n         CachedFullParameters = null;\n     }\n```\n\n**Note:** This uses `AllReduce` instead of `ReduceScatter` to ensure correct remainder handling. If `ReduceScatter` performance is critical, implement a custom variant that accepts per-rank shard sizes.\n\n<!-- This is an auto-generated comment by CodeRabbit -->","createdAt":"2025-11-09T08:41:31Z","path":"src/DistributedTraining/ZeRO2Model.cs","line":113,"originalLine":113,"diffHunk":"@@ -0,0 +1,233 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.Models;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Implements ZeRO Stage 2 model wrapper - shards optimizer states and gradients.\n+/// </summary>\n+/// <remarks>\n+/// <para><b>Strategy Overview:</b>\n+/// ZeRO Stage 2 builds on ZeRO-1 by additionally sharding gradients across processes.\n+/// Parameters are still replicated for the forward pass, but gradients are reduced and scattered\n+/// (ReduceScatter) so each process only stores a portion. This saves significant memory compared\n+/// to ZeRO-1, especially for large models.\n+/// </para>\n+/// <para><b>For Beginners:</b>\n+/// This implements ZeRO Stage 2, which saves even more memory than ZeRO-1. The model parameters\n+/// are still fully replicated (like DDP and ZeRO-1), but now both the optimizer state AND the\n+/// gradients are split across processes. After computing gradients, they're immediately reduced\n+/// and scattered so each process only keeps its portion.\n+/// </para>\n+/// <para>\n+/// Think of it like a team where everyone has the full playbook (parameters), but when taking\n+/// notes during practice (gradients), they divide up the note-taking so each person is responsible\n+/// for recording only certain plays. This saves everyone from having to write everything down.\n+/// </para>\n+/// <para><b>Use Cases:</b>\n+/// - Larger models where gradient memory becomes significant\n+/// - Want substantial memory savings with moderate communication cost\n+/// - Preparing for ZeRO-3/FSDP migration\n+/// </para>\n+/// <para><b>Trade-offs:</b>\n+/// - Memory: Very Good - saves both optimizer states and gradients\n+/// - Communication: Moderate - uses ReduceScatter instead of AllReduce\n+/// - Complexity: Moderate - gradient sharding adds some complexity\n+/// - Best for: Large models where gradient memory is significant\n+/// </para>\n+/// <para>\n+/// Example:\n+/// <code>\n+/// var model = new NeuralNetworkModel&lt;double&gt;(...);\n+/// var backend = new InMemoryCommunicationBackend&lt;double&gt;(rank: 0, worldSize: 4);\n+/// var config = new ShardingConfiguration&lt;double&gt;(backend);\n+/// var zero2Model = new ZeRO2Model&lt;double, Tensor&lt;double&gt;, Tensor&lt;double&gt;&gt;(model, config);\n+///\n+/// // Use with ZeRO2Optimizer for full ZeRO-2 benefits\n+/// var zero2Optimizer = new ZeRO2Optimizer&lt;double, Tensor&lt;double&gt;, Tensor&lt;double&gt;&gt;(optimizer, config);\n+/// </code>\n+/// </para>\n+/// </remarks>\n+/// <typeparam name=\"T\">The numeric type</typeparam>\n+/// <typeparam name=\"TInput\">The input type for the model</typeparam>\n+/// <typeparam name=\"TOutput\">The output type for the model</typeparam>\n+public class ZeRO2Model<T, TInput, TOutput> : ShardedModelBase<T, TInput, TOutput>\n+{\n+    private Vector<T>? _gradientShard;\n+\n+    public ZeRO2Model(IFullModel<T, TInput, TOutput> wrappedModel, IShardingConfiguration<T> config)\n+        : base(wrappedModel, config)\n+    {\n+    }\n+\n+    protected override void InitializeSharding()\n+    {\n+        var fullParameters = WrappedModel.GetParameters();\n+\n+        // Parameters still replicated (like ZeRO-1)\n+        ShardStartIndex = 0;\n+        ShardSize = fullParameters.Length;\n+        LocalShard = new Vector<T>(fullParameters.ToArray());\n+\n+        // Calculate gradient shard size\n+        int totalParams = fullParameters.Length;\n+        int baseShardSize = totalParams / WorldSize;\n+        int remainder = totalParams % WorldSize;\n+        int gradShardSize = baseShardSize + (Rank < remainder ? 1 : 0);\n+\n+        _gradientShard = new Vector<T>(new T[gradShardSize]);\n+        CachedFullParameters = null;\n+    }\n+\n+    /// <summary>\n+    /// Synchronizes gradients using ReduceScatter - each process gets its shard of reduced gradients.\n+    /// </summary>\n+    public override void SynchronizeGradients()\n+    {\n+        var totalParams = LocalShard.Length;\n+        var remainder = totalParams % WorldSize;\n+\n+        // Pad to satisfy ReduceScatter's divisibility requirement\n+        Vector<T> reduceInput = LocalShard;\n+        if (remainder != 0)\n+        {\n+            var paddedLength = totalParams + (WorldSize - remainder);\n+            var padded = new T[paddedLength];\n+            Array.Copy(LocalShard.ToArray(), padded, totalParams);\n+            // Padding elements remain at default(T) which is typically 0\n+            reduceInput = new Vector<T>(padded);\n+        }\n+\n+        // Perform ReduceScatter on padded data\n+        var reducedChunk = Config.CommunicationBackend.ReduceScatter(reduceInput, ReductionOperation.Average);\n+\n+        // Trim padding so each rank keeps only its logical shard\n+        // Distribute remainder elements to first 'remainder' ranks\n+        var shardLength = totalParams / WorldSize + (Rank < remainder ? 1 : 0);\n+        var shardData = new T[shardLength];\n+        Array.Copy(reducedChunk.ToArray(), 0, shardData, 0, shardLength);\n+        _gradientShard = new Vector<T>(shardData);\n+\n+        CachedFullParameters = null;\n+    }"}]}},{"id":"PRRT_kwDOKSXUF85hNZUP","isResolved":false,"isOutdated":false,"path":"src/DistributedTraining/ZeRO2Optimizer.cs","line":103,"originalLine":103,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VeK4w","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**Class claims to implement ZeRO-2 but uses DDP-style gradient synchronization.**\n\nThe class name, XML docs (line 9: \"shards optimizer states and gradients\"), and remarks (lines 13-16) advertise ZeRO-2 gradient sharding via `ReduceScatter`. However, the implementation at lines 90-100 explicitly documents that true ZeRO-2 is not implemented and instead falls back to `AllReduce` (line 100), which averages the full gradient vector across all ranks‚Äîidentical to DDP behavior. This means gradients are replicated, not sharded, and users won't see the memory savings advertised in the documentation.\n\n\n\nEither:\n1. **Complete the ZeRO-2 implementation** as outlined in the TODO (lines 90-97): use `ReduceScatter` to distribute gradient shards, apply shards to corresponding parameter shards, and `AllGather` updated parameters, or\n2. **Rename and document the limitation prominently**: rename to `ZeRO2OptimizerPartial` or add a prominent warning in the XML docs (e.g., `<para><b>Current Limitation:</b> Gradient sharding is not yet implemented; gradients are synchronized via AllReduce (DDP-style). Memory savings are limited to optimizer state sharding only.</para>`) to set accurate expectations.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/DistributedTraining/ZeRO2Optimizer.cs around lines 78 to 103, the\nimplementation documents ZeRO-2 but falls back to DDP-style AllReduce (line\n~100) which does not shard gradients; either implement the TODO\nReduceScatter/ApplyShard/AllGather flow or make the limitation explicit. To fix:\neither (A) implement true ZeRO-2 by using\nConfig.CommunicationBackend.ReduceScatter(localGradients,\nReductionOperation.Average) to obtain this rank‚Äôs gradient shard, split\noriginalParams into matching parameter shards, apply the shard via a new\nApplyGradientShard method that updates only this shard, then use\nCommunicationBackend.AllGather to reconstruct full parameters and set\nlocalResult.BestSolution appropriately; or (B) rename the class/exports to\nindicate partial ZeRO-2 (e.g., ZeRO2OptimizerPartial) and update the XML docs\nand remarks to prominently state that gradients are synchronized via AllReduce\n(DDP-style) and true gradient sharding is not implemented so memory savings are\nlimited to optimizer state sharding.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","createdAt":"2025-11-09T08:41:31Z","path":"src/DistributedTraining/ZeRO2Optimizer.cs","line":103,"originalLine":103,"diffHunk":"@@ -0,0 +1,209 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.Models.Inputs;\n+using AiDotNet.Optimizers;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Implements ZeRO Stage 2 optimizer - shards optimizer states and gradients.\n+/// </summary>\n+/// <remarks>\n+/// <para><b>Strategy Overview:</b>\n+/// ZeRO-2 optimizer builds on ZeRO-1 by additionally sharding gradients using ReduceScatter.\n+/// After backward pass, gradients are reduced and scattered so each process only stores its\n+/// portion. This further reduces memory compared to ZeRO-1, as gradients can be as large as\n+/// the model itself. Parameters remain replicated for the forward pass.\n+/// </para>\n+/// <para><b>For Beginners:</b>\n+/// ZeRO-2 saves even more memory than ZeRO-1. Not only is the optimizer state split across\n+/// processes, but the gradients are too. After computing gradients, we immediately use\n+/// ReduceScatter to average them across processes AND split them up, so each process only\n+/// keeps its assigned portion. This is like having a team where each person is responsible\n+/// for updating only certain parameters.\n+/// </para>\n+/// <para><b>Use Cases:</b>\n+/// - Large models where gradient memory is significant\n+/// - Want substantial memory savings\n+/// - Works well with ZeRO2Model\n+/// </para>\n+/// <para><b>Trade-offs:</b>\n+/// - Memory: Very Good - saves optimizer states + gradients\n+/// - Communication: Moderate - uses ReduceScatter instead of AllReduce\n+/// - Complexity: Moderate - gradient and state sharding\n+/// - Best for: Large models with significant gradient memory\n+/// </para>\n+/// </remarks>\n+/// <typeparam name=\"T\">The numeric type</typeparam>\n+/// <typeparam name=\"TInput\">The input type for the model</typeparam>\n+/// <typeparam name=\"TOutput\">The output type for the model</typeparam>\n+public class ZeRO2Optimizer<T, TInput, TOutput> : ShardedOptimizerBase<T, TInput, TOutput>\n+{\n+    /// <summary>\n+    /// Creates a ZeRO-2 optimizer that shards gradients and optimizer states.\n+    /// </summary>\n+    /// <param name=\"wrappedOptimizer\">The base optimizer to wrap (must be gradient-based: SGD, Adam, etc.)</param>\n+    /// <param name=\"config\">Configuration for distributed training communication</param>\n+    /// <exception cref=\"ArgumentException\">If wrapped optimizer is not gradient-based</exception>\n+    public ZeRO2Optimizer(\n+        IOptimizer<T, TInput, TOutput> wrappedOptimizer,\n+        IShardingConfiguration<T> config)\n+        : base(wrappedOptimizer, config)\n+    {\n+        // Verify wrapped optimizer supports gradient operations\n+        if (wrappedOptimizer is not IGradientBasedOptimizer<T, TInput, TOutput>)\n+        {\n+            throw new ArgumentException(\n+                $\"ZeRO-2 requires a gradient-based optimizer, but received {wrappedOptimizer.GetType().Name}. \" +\n+                \"Use gradient-based optimizers like SGD, Adam, RMSprop, etc.\",\n+                nameof(wrappedOptimizer));\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public override OptimizationResult<T, TInput, TOutput> Optimize(OptimizationInputData<T, TInput, TOutput> inputData)\n+    {\n+        if (inputData == null)\n+            throw new ArgumentNullException(nameof(inputData));\n+\n+        var gradientOptimizer = (IGradientBasedOptimizer<T, TInput, TOutput>)WrappedOptimizer;\n+\n+        // Barrier to ensure all processes start together\n+        Config.CommunicationBackend.Barrier();\n+\n+        // Step 1: Optimize locally to compute gradients (and apply them locally)\n+        var localResult = WrappedOptimizer.Optimize(inputData);\n+\n+        // Step 2: Implement ZeRO-2 gradient sharding with ReduceScatter\n+        if (Config.AutoSyncGradients && localResult.BestSolution != null)\n+        {\n+            var localGradients = gradientOptimizer.LastComputedGradients;\n+\n+            if (localGradients != null && localGradients.Length > 0)\n+            {\n+                // Get parameters after local gradient application\n+                var updatedParams = localResult.BestSolution.GetParameters();\n+\n+                // Reverse the local update to get original parameters\n+                var originalParams = ComputeOriginalParameters(updatedParams, localGradients);\n+\n+                // TODO: Complete ZeRO-2 parameter shard update with ReduceScatter\n+                // Current limitation: The IGradientBasedOptimizer.ApplyGradients() expects full gradient vector.\n+                // Proper ZeRO-2 requires:\n+                // 1. Use ReduceScatter to distribute gradient shards: var gradientShard = Config.CommunicationBackend.ReduceScatter(localGradients, ReductionOperation.Average);\n+                // 2. Split originalParams into shards matching gradient shards\n+                // 3. Apply gradientShard to this rank's parameter shard only\n+                // 4. AllGather parameter shards to reconstruct full parameters\n+                //\n+                // For now, we use DDP-style full gradient sync as a functional approximation.\n+                // This provides correct gradient averaging but without the memory savings of true ZeRO-2 gradient sharding.\n+                Config.CommunicationBackend.AllReduce(localGradients, ReductionOperation.Average);\n+                var finalModel = gradientOptimizer.ApplyGradients(localGradients, localResult.BestSolution);\n+                localResult.BestSolution = finalModel;\n+            }"}]}},{"id":"PRRT_kwDOKSXUF85hNZUV","isResolved":false,"isOutdated":false,"path":"src/DistributedTraining/ZeRO2Optimizer.cs","line":141,"originalLine":141,"comments":{"nodes":[{"id":"PRRC_kwDOKSXUF86VeK43","author":{"login":"coderabbitai"},"body":"_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_\n\n**Gradient reversal assumes SGD but constructor allows Adam, RMSprop, etc.**\n\n`ComputeOriginalParameters` reverses the gradient update using the formula `params_old = params_new + lr * gradients` (lines 131-138), which only holds for vanilla SGD. However, the constructor (line 54) accepts any `IGradientBasedOptimizer`, including Adam (which uses momentum and adaptive learning rates: `params_new = params_old - lr * m_t / (sqrt(v_t) + epsilon)`) and RMSprop (which uses moving averages of squared gradients). For these optimizers, the reversal will compute incorrect original parameters, leading to incorrect gradient averaging and broken distributed training.\n\n\n\nEither:\n1. **Restrict to SGD only**: Update the constructor validation (line 54) to check for a specific SGD optimizer type and update the exception message, or\n2. **Implement correct reversal for all optimizers**: Access the wrapped optimizer's internal state (momentum, variance, etc.) to correctly reverse the update for each optimizer type. This likely requires extending `IGradientBasedOptimizer` with a `ReverseUpdate` method or similar.\n\nOption 1 is simpler and aligns with the current implementation; option 2 is more flexible but significantly more complex.\n\n<details>\n<summary>ü§ñ Prompt for AI Agents</summary>\n\n```\nIn src/DistributedTraining/ZeRO2Optimizer.cs around lines 124 to 141 (and note\nconstructor check at line 54), the ComputeOriginalParameters method assumes\nvanilla SGD when reversing updates; that is incorrect for Adam/RMSProp etc. Fix\nby either (preferred simple fix) restricting the class to SGD: change the\nconstructor validation at line 54 to only accept the SGD optimizer type and\nupdate the exception message to state ZeRO2Optimizer only supports SGD; or (if\nyou need broader support) extend IGradientBasedOptimizer with a ReverseUpdate\nmethod (or similar) that exposes the optimizer-specific reverse logic, implement\nthat in each optimizer (Adam, RMSProp, SGD), and replace the current SGD-only\nreversal in ComputeOriginalParameters with a call to\nWrappedOptimizer.ReverseUpdate(updatedParams, gradients, options/state). Ensure\ntests updated to reflect the new constraint or added reverse implementations.\n```\n\n</details>\n\n<!-- This is an auto-generated comment by CodeRabbit -->","createdAt":"2025-11-09T08:41:31Z","path":"src/DistributedTraining/ZeRO2Optimizer.cs","line":141,"originalLine":141,"diffHunk":"@@ -0,0 +1,209 @@\n+using AiDotNet.Interfaces;\n+using AiDotNet.LinearAlgebra;\n+using AiDotNet.Models.Inputs;\n+using AiDotNet.Optimizers;\n+\n+namespace AiDotNet.DistributedTraining;\n+\n+/// <summary>\n+/// Implements ZeRO Stage 2 optimizer - shards optimizer states and gradients.\n+/// </summary>\n+/// <remarks>\n+/// <para><b>Strategy Overview:</b>\n+/// ZeRO-2 optimizer builds on ZeRO-1 by additionally sharding gradients using ReduceScatter.\n+/// After backward pass, gradients are reduced and scattered so each process only stores its\n+/// portion. This further reduces memory compared to ZeRO-1, as gradients can be as large as\n+/// the model itself. Parameters remain replicated for the forward pass.\n+/// </para>\n+/// <para><b>For Beginners:</b>\n+/// ZeRO-2 saves even more memory than ZeRO-1. Not only is the optimizer state split across\n+/// processes, but the gradients are too. After computing gradients, we immediately use\n+/// ReduceScatter to average them across processes AND split them up, so each process only\n+/// keeps its assigned portion. This is like having a team where each person is responsible\n+/// for updating only certain parameters.\n+/// </para>\n+/// <para><b>Use Cases:</b>\n+/// - Large models where gradient memory is significant\n+/// - Want substantial memory savings\n+/// - Works well with ZeRO2Model\n+/// </para>\n+/// <para><b>Trade-offs:</b>\n+/// - Memory: Very Good - saves optimizer states + gradients\n+/// - Communication: Moderate - uses ReduceScatter instead of AllReduce\n+/// - Complexity: Moderate - gradient and state sharding\n+/// - Best for: Large models with significant gradient memory\n+/// </para>\n+/// </remarks>\n+/// <typeparam name=\"T\">The numeric type</typeparam>\n+/// <typeparam name=\"TInput\">The input type for the model</typeparam>\n+/// <typeparam name=\"TOutput\">The output type for the model</typeparam>\n+public class ZeRO2Optimizer<T, TInput, TOutput> : ShardedOptimizerBase<T, TInput, TOutput>\n+{\n+    /// <summary>\n+    /// Creates a ZeRO-2 optimizer that shards gradients and optimizer states.\n+    /// </summary>\n+    /// <param name=\"wrappedOptimizer\">The base optimizer to wrap (must be gradient-based: SGD, Adam, etc.)</param>\n+    /// <param name=\"config\">Configuration for distributed training communication</param>\n+    /// <exception cref=\"ArgumentException\">If wrapped optimizer is not gradient-based</exception>\n+    public ZeRO2Optimizer(\n+        IOptimizer<T, TInput, TOutput> wrappedOptimizer,\n+        IShardingConfiguration<T> config)\n+        : base(wrappedOptimizer, config)\n+    {\n+        // Verify wrapped optimizer supports gradient operations\n+        if (wrappedOptimizer is not IGradientBasedOptimizer<T, TInput, TOutput>)\n+        {\n+            throw new ArgumentException(\n+                $\"ZeRO-2 requires a gradient-based optimizer, but received {wrappedOptimizer.GetType().Name}. \" +\n+                \"Use gradient-based optimizers like SGD, Adam, RMSprop, etc.\",\n+                nameof(wrappedOptimizer));\n+        }\n+    }\n+\n+    /// <inheritdoc/>\n+    public override OptimizationResult<T, TInput, TOutput> Optimize(OptimizationInputData<T, TInput, TOutput> inputData)\n+    {\n+        if (inputData == null)\n+            throw new ArgumentNullException(nameof(inputData));\n+\n+        var gradientOptimizer = (IGradientBasedOptimizer<T, TInput, TOutput>)WrappedOptimizer;\n+\n+        // Barrier to ensure all processes start together\n+        Config.CommunicationBackend.Barrier();\n+\n+        // Step 1: Optimize locally to compute gradients (and apply them locally)\n+        var localResult = WrappedOptimizer.Optimize(inputData);\n+\n+        // Step 2: Implement ZeRO-2 gradient sharding with ReduceScatter\n+        if (Config.AutoSyncGradients && localResult.BestSolution != null)\n+        {\n+            var localGradients = gradientOptimizer.LastComputedGradients;\n+\n+            if (localGradients != null && localGradients.Length > 0)\n+            {\n+                // Get parameters after local gradient application\n+                var updatedParams = localResult.BestSolution.GetParameters();\n+\n+                // Reverse the local update to get original parameters\n+                var originalParams = ComputeOriginalParameters(updatedParams, localGradients);\n+\n+                // TODO: Complete ZeRO-2 parameter shard update with ReduceScatter\n+                // Current limitation: The IGradientBasedOptimizer.ApplyGradients() expects full gradient vector.\n+                // Proper ZeRO-2 requires:\n+                // 1. Use ReduceScatter to distribute gradient shards: var gradientShard = Config.CommunicationBackend.ReduceScatter(localGradients, ReductionOperation.Average);\n+                // 2. Split originalParams into shards matching gradient shards\n+                // 3. Apply gradientShard to this rank's parameter shard only\n+                // 4. AllGather parameter shards to reconstruct full parameters\n+                //\n+                // For now, we use DDP-style full gradient sync as a functional approximation.\n+                // This provides correct gradient averaging but without the memory savings of true ZeRO-2 gradient sharding.\n+                Config.CommunicationBackend.AllReduce(localGradients, ReductionOperation.Average);\n+                var finalModel = gradientOptimizer.ApplyGradients(localGradients, localResult.BestSolution);\n+                localResult.BestSolution = finalModel;\n+            }\n+        }\n+\n+        SynchronizeOptimizerState();\n+\n+        // Barrier to ensure all processes finish together\n+        Config.CommunicationBackend.Barrier();\n+\n+        return localResult;\n+    }\n+\n+    /// <summary>\n+    /// Computes the original parameters before gradient application by reversing the update.\n+    /// </summary>\n+    /// <param name=\"updatedParams\">Parameters after gradient application</param>\n+    /// <param name=\"gradients\">The gradients that were applied</param>\n+    /// <returns>Estimated original parameters before gradient application</returns>\n+    /// <remarks>\n+    /// For gradient descent: params_new = params_old - learning_rate * gradients\n+    /// Therefore: params_old = params_new + learning_rate * gradients\n+    /// </remarks>\n+    private Vector<T> ComputeOriginalParameters(Vector<T> updatedParams, Vector<T> gradients)\n+    {\n+        // Get learning rate from optimizer options\n+        var options = WrappedOptimizer.GetOptions();\n+        double learningRate = options.InitialLearningRate;\n+\n+        // Reverse the update: params_old = params_new + lr * gradients\n+        var original = new T[updatedParams.Length];\n+        for (int i = 0; i < updatedParams.Length; i++)\n+        {\n+            double updated = Convert.ToDouble(updatedParams[i]);\n+            double gradient = Convert.ToDouble(gradients[i]);\n+            double originalValue = updated + learningRate * gradient;\n+            original[i] = (T)Convert.ChangeType(originalValue, typeof(T));\n+        }\n+\n+        return new Vector<T>(original);\n+    }"}]}}]}}}}}