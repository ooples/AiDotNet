
===== COMMENT 1 =====
ThreadID: PRRT_kwDOKSXUF85g9V8l
File: src/DistributedTraining/ShardedModel.cs
Line: None
Body:
The cache invalidation strategy may cause performance issues. The `_cachedFullParameters` is set to null after every Train() call (line 190) even when AutoSyncGradients is false, and it's also invalidated in SetParameters (line 244). However, if multiple predictions are made without training, the first prediction would benefit from caching, but the cache is never populated during prediction. Consider if the cache should be populated in the Predict method for better performance.
============================================================

===== COMMENT 2 =====
ThreadID: PRRT_kwDOKSXUF85g9V8x
File: src/DistributedTraining/CommunicationManager.cs
Line: None
Body:
The CommunicationManager uses static mutable state for backends which could cause issues in concurrent scenarios or unit tests that run in parallel. If one test initializes the manager while another is using it, unexpected behavior could occur. Consider using instance-based management or adding thread-safety warnings to the documentation.
============================================================

===== COMMENT 3 =====
ThreadID: PRRT_kwDOKSXUF85g9V82
File: scripts/launch-distributed-training.ps1
Line: None
Body:
Security risk: Similar to the bash script, program arguments are not sanitized before being passed to Start-Process. The `$ProgramArgs.Split(" ")` approach is also problematic as it doesn't handle quoted arguments correctly (e.g., `--name "My Model"` would be split incorrectly). Consider using proper argument parsing or at least document the limitation.
============================================================

===== COMMENT 4 =====
ThreadID: PRRT_kwDOKSXUF85g9V8_
File: src/DistributedTraining/InMemoryCommunicationBackend.cs
Line: None
Body:
The AllReduce operation modifies the input `data` vector in place, but the Broadcast and AllGather operations return new vectors. This inconsistency in the API could be confusing. Consider documenting this clearly or making the API more consistent (either all modify in-place or all return new instances).
============================================================

===== COMMENT 5 =====
ThreadID: PRRT_kwDOKSXUF85g9V9h
File: src/DistributedTraining/InMemoryCommunicationBackend.cs
Line: 267
Body:
Resource leak: The barrier cleanup only happens for rank 0, but if rank 0 never reaches the barrier or fails, the barrier counter will never be cleaned up from the shared dictionary. Consider cleanup after all processes exit the barrier or implement a timeout-based cleanup mechanism.
============================================================

===== COMMENT 6 =====
ThreadID: PRRT_kwDOKSXUF85g9V9o
File: src/DistributedTraining/InMemoryCommunicationBackend.cs
Line: 317
Body:
Potential deadlock: Similar to the Barrier method, this while loop could deadlock if not all processes reach this point. If one process fails or doesn't call AllReduce, the others will wait indefinitely. Consider adding a timeout or error handling mechanism.
============================================================

===== COMMENT 7 =====
ThreadID: PRRT_kwDOKSXUF85g9V9s
File: src/DistributedTraining/InMemoryCommunicationBackend.cs
Line: 707
Body:
The Average operation applies division after the accumulation, but the accumulation is using Sum logic. This means the final division is done once on the total sum, dividing by vector count. However, the comment and operation name suggest this should average values. Verify this is the intended behavior - it appears correct but could use clarification in the comment.
============================================================

===== COMMENT 8 =====
ThreadID: PRRT_kwDOKSXUF85g9V9z
File: src/DistributedTraining/InMemoryCommunicationBackend.cs
Line: 65
Body:
The static shared state (_sharedBuffers, _barrierCounters) is shared across all instances of InMemoryCommunicationBackend, which could cause issues if multiple independent distributed training sessions are running concurrently in the same process. Consider namespacing these by a session ID or making them instance-specific with a shared coordinator.
============================================================

===== COMMENT 9 =====
ThreadID: PRRT_kwDOKSXUF85hMtMC
File: src/DistributedTraining/GlooCommunicationBackend.cs
Line: 1003
Body:
_üõ†Ô∏è Refactor suggestion_ | _üü† Major_

**Mark TCP data transfer methods as not implemented.**

The `SendData` and `ReceiveData` methods are placeholders that will always throw `InvalidOperationException` because `_tcpConnections` is never initialized. The current exception message is misleading‚Äîit suggests a connection should exist but wasn't found, when in fact TCP infrastructure is not implemented at all.



Apply this diff to make the status clearer:

```diff
     private void SendData(int destRank, T[] data)
     {
-        if (_tcpConnections == null || !_tcpConnections.ContainsKey(destRank))
-        {
-            throw new InvalidOperationException($"No TCP connection to rank {destRank}");
-        }
-
-        // In production, serialize and send data via TCP
-        // Implementation would use NetworkStream.Write with proper serialization
+        throw new NotImplementedException(
+            "TCP-based data transfer is not yet implemented. " +
+            "This backend currently supports only single-process mode.");
     }
```

```diff
     private T[] ReceiveData(int sourceRank, int expectedLength)
     {
-        if (_tcpConnections == null || !_tcpConnections.ContainsKey(sourceRank))
-        {
-            throw new InvalidOperationException($"No TCP connection to rank {sourceRank}");
-        }
-
-        // In production, receive and deserialize data via TCP
-        // Implementation would use NetworkStream.Read with proper deserialization
-        return new T[expectedLength];
+        throw new NotImplementedException(
+            "TCP-based data transfer is not yet implemented. " +
+            "This backend currently supports only single-process mode.");
     }
```

<!-- suggestion_start -->

<details>
<summary>üìù Committable suggestion</summary>

> ‚ÄºÔ∏è **IMPORTANT**
> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.

```suggestion
    private void SendData(int destRank, T[] data)
    {
        throw new NotImplementedException(
            "TCP-based data transfer is not yet implemented. " +
            "This backend currently supports only single-process mode.");
    }

    /// <summary>
    /// Receives data from a specific rank via TCP.
    /// </summary>
    /// <remarks>
    /// This would be used in a full multi-process TCP implementation.
    /// Requires TCP connections to be established during initialization.
    /// </remarks>
    private T[] ReceiveData(int sourceRank, int expectedLength)
    {
        throw new NotImplementedException(
            "TCP-based data transfer is not yet implemented. " +
            "This backend currently supports only single-process mode.");
    }
```

</details>

<!-- suggestion_end -->

<details>
<summary>ü§ñ Prompt for AI Agents</summary>

```
In src/DistributedTraining/GlooCommunicationBackend.cs around lines 417 to 445,
the SendData and ReceiveData methods are placeholders that misleadingly throw
InvalidOperationException about missing TCP connections; change them to clearly
indicate the TCP transport is not implemented by throwing
NotImplementedException (or an InvalidOperationException with an explicit
message like "TCP data transfer not implemented in this backend") immediately
(remove the _tcpConnections existence check), and update the XML doc comments to
state these methods are intentionally unimplemented in this build so callers get
an explicit, accurate error instead of a confusing connection-not-found message.
```

</details>

<!-- This is an auto-generated comment by CodeRabbit -->
============================================================

===== COMMENT 10 =====
ThreadID: PRRT_kwDOKSXUF85hMtMV
File: src/DistributedTraining/HybridShardedOptimizer.cs
Line: None
Body:
_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_

**Stop averaging parameters across all ranks.**

Line 102 calls `SynchronizeParameters`, which in `ShardedOptimizerBase` performs a world-size AllReduce over the entire parameter vector (and becomes a no-op if the model already implements `IShardedModel`). That neither honors the tensor/data groups nor touches gradients at all, so gradients remain unsynchronized (or worse, fully averaged parameters overwrite the local shards), breaking the 3D parallel semantics you outlined. Replace this with explicit gradient synchronization per dimension‚Äîfirst reduce across the tensor-parallel group, then across the data-parallel group, using the `_tensorParallelSize` / `_dataParallelSize` information and the appropriate communication groups. Only touch parameters when the wrapped optimizer actually requires a parameter broadcast.

<!-- fingerprinting:phantom:medusa:sabertoothed -->

<!-- This is an auto-generated comment by CodeRabbit -->
============================================================

===== COMMENT 11 =====
ThreadID: PRRT_kwDOKSXUF85hNV-J
File: src/DistributedTraining/DDPOptimizer.cs
Line: 123
Body:
_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_

**Restore pre-update weights before applying averaged gradients.**

After local optimization the wrapped model still holds the locally-updated parameters. Because `originalParams` is computed but never applied, `ApplyGradients` runs on already-updated weights, effectively performing two steps (`params_old - lr*localGrad - lr*avgGrad`) instead of the intended `params_old - lr*avgGrad`. Reset the model to `originalParams` before applying the averaged gradients so DDP stays numerically correct.

```diff
                 // Average gradients across all workers (true DDP)
                 Config.CommunicationBackend.AllReduce(localGradients, ReductionOperation.Average);

                 // Apply averaged gradients to original parameters
                 // This ensures all workers apply the same averaged gradients
-                var finalModel = gradientOptimizer.ApplyGradients(localGradients, localResult.BestSolution);
+                localResult.BestSolution.SetParameters(originalParams);
+                var finalModel = gradientOptimizer.ApplyGradients(localGradients, localResult.BestSolution);
```

<details>
<summary>ü§ñ Prompt for AI Agents</summary>

```
In src/DistributedTraining/DDPOptimizer.cs around lines 109 to 123, the code
computes originalParams but never restores them before applying the averaged
gradients, causing a double-update; modify the flow to set the model's
parameters back to originalParams (i.e., restore the wrapped model or
localResult.BestSolution to originalParams) immediately after computing
originalParams and before calling Config.CommunicationBackend.AllReduce and
gradientOptimizer.ApplyGradients so that ApplyGradients operates on the
pre-update weights; finally ensure localResult.BestSolution is replaced with the
returned finalModel (the model with averaged gradients applied).
```

</details>

<!-- fingerprinting:phantom:medusa:sabertoothed -->

<!-- This is an auto-generated comment by CodeRabbit -->
============================================================

===== COMMENT 12 =====
ThreadID: PRRT_kwDOKSXUF85hNV-K
File: src/DistributedTraining/GlooCommunicationBackend.cs
Line: 137
Body:
_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_

**Fallback never engages when Gloo is present**

As soon as `Type.GetType("Gloo.Context, GlooSharp")` succeeds you set `_useNativeTCP = false` and immediately throw `NotImplementedException`. That propagates out of `OnInitialize`, so multi-rank initialization aborts instead of dropping into the TCP fallback. Anyone with Gloo assemblies deployed will be unable to start training. Keep the warning, but swallow the condition and mark `glooAvailable = false` so the code below can flip `_useNativeTCP = true` and continue with TCP.



```diff
-                Console.WriteLine($"GlooCommunicationBackend: Gloo library detected for {_worldSize} processes.");
-                ...
-                throw new NotImplementedException(
-                    "GlooCommunicationBackend with Gloo library support is not yet fully implemented.\n\n" +
-                    "...Using TCP fallback instead.");
+                Console.WriteLine($"GlooCommunicationBackend: Gloo library detected for {_worldSize} processes.");
+                Console.WriteLine("WARNING: Native Gloo support not yet implemented. Falling back to TCP.");
+                glooAvailable = false;
```

<details>
<summary>ü§ñ Prompt for AI Agents</summary>

```
In src/DistributedTraining/GlooCommunicationBackend.cs around lines 96 to 129,
the current logic detects the Gloo assembly then sets _useNativeTCP = false and
throws NotImplementedException which aborts initialization and prevents the TCP
fallback; instead, remove the throw, keep the diagnostic Console.WriteLine (or
replace it with a warning log), set glooAvailable = false (or explicitly
_useNativeTCP = true) so execution continues past the try/catch and the existing
TCP fallback code runs, and include a clear warning message that Gloo was
detected but native TCP will be used as a fallback.
```

</details>

<!-- fingerprinting:phantom:medusa:sabertoothed -->

<!-- This is an auto-generated comment by CodeRabbit -->
============================================================

===== COMMENT 13 =====
ThreadID: PRRT_kwDOKSXUF85hNV-L
File: src/DistributedTraining/GlooCommunicationBackend.cs
Line: 223
Body:
_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_

**Handshake closes sockets before you ever use them**

Both `ConnectToRank` and `AcceptConnectionFromRank` wrap `client.GetStream()` in a `using` block. Disposing the `NetworkStream` disposes the underlying socket, so the connection is dead before it is stored in `_tcpConnections`. Every subsequent collective will fail with `ObjectDisposedException` or ‚ÄúNo TCP connection‚Äù errors. Keep the handshake writers/readers but leave the stream open‚Äîe.g. request the stream once, create the reader/writer with `leaveOpen: true`, and dispose only the writer/reader.



```diff
-        using (var stream = client.GetStream())
-        using (var writer = new BinaryWriter(stream))
+        var stream = client.GetStream();
+        using (var writer = new BinaryWriter(stream, System.Text.Encoding.UTF8, leaveOpen: true))
         {
             writer.Write(_rank);
             writer.Flush();
         }
```

```diff
-        using (var stream = client.GetStream())
-        using (var reader = new BinaryReader(stream))
+        var stream = client.GetStream();
+        using (var reader = new BinaryReader(stream, System.Text.Encoding.UTF8, leaveOpen: true))
         {
             receivedRank = reader.ReadInt32();
         }
```


Also applies to: 244-248

<details>
<summary>ü§ñ Prompt for AI Agents</summary>

```
In src/DistributedTraining/GlooCommunicationBackend.cs around lines 197-215
(also apply same fix to 244-248), the current handshake disposes the
NetworkStream by using "using" on client.GetStream(), which closes the
underlying socket before storing the TcpClient in _tcpConnections; change the
pattern to obtain the NetworkStream without disposing it, create
BinaryWriter/BinaryReader with leaveOpen: true (or otherwise avoid disposing the
stream) so only the reader/writer is disposed, and ensure you do not wrap the
stream in a using that would close it; keep the writer/reader for the handshake,
call Flush, dispose the writer/reader (or use a using for them with
leaveOpen:true), and then store the TcpClient in _tcpConnections while leaving
the stream/socket open for future use.
```

</details>

<!-- fingerprinting:phantom:medusa:sabertoothed -->

<!-- This is an auto-generated comment by CodeRabbit -->
============================================================

===== COMMENT 14 =====
ThreadID: PRRT_kwDOKSXUF85hNV-M
File: src/DistributedTraining/GlooCommunicationBackend.cs
Line: 708
Body:
_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_

**Scatter chunk-length read is broken**

`ReceiveData(parentAbsolute, 1)` is invoked twice: once in the ternary condition and again inside `Convert.ToInt32(...)`. The first call consumes the single-element ‚Äúchunk size‚Äù message, so the second call blocks waiting for another length message and then blows up when it sees the following chunk payload (length ‚â† 1). Cache the first result and convert it; do not re-issue the read.



```diff
-            chunkSize = ReceiveData(parentAbsolute, 1)[0] != null ?
-                        Convert.ToInt32(ReceiveData(parentAbsolute, 1)[0]) : 0;
+            var sizeMessage = ReceiveData(parentAbsolute, 1);
+            chunkSize = Convert.ToInt32(sizeMessage[0]);
```

<details>
<summary>ü§ñ Prompt for AI Agents</summary>

```
In src/DistributedTraining/GlooCommunicationBackend.cs around lines 696 to 700,
the code calls ReceiveData(parentAbsolute, 1) twice which consumes the
single-element length message on the first call and blocks/errs on the second;
fix by calling ReceiveData(parentAbsolute, 1) once, store the returned array in
a local variable, check its element for null, convert that single cached element
to int to set chunkSize, then call ReceiveData(parentAbsolute, chunkSize) once
to read the payload.
```

</details>

<!-- fingerprinting:phantom:medusa:sabertoothed -->

<!-- This is an auto-generated comment by CodeRabbit -->
============================================================

===== COMMENT 15 =====
ThreadID: PRRT_kwDOKSXUF85hNV-O
File: src/DistributedTraining/GlooCommunicationBackend.cs
Line: 1002
Body:
_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_

**Send/receive helpers tear down the transport on first call**

`SendData`/`ReceiveData` also wrap `GetStream()` in `using` statements. The first collective disposes the `NetworkStream`, closing the socket and breaking every later operation. Switch to `leaveOpen: true` (or cache per-connection reader/writer instances) so the underlying TCP connection stays alive for the lifetime of the backend.



```diff
-        lock (_connectionLock)
-        {
-            var client = _tcpConnections[destRank];
-            using (var stream = client.GetStream())
-            using (var writer = new BinaryWriter(stream))
+        lock (_connectionLock)
+        {
+            var client = _tcpConnections[destRank];
+            var stream = client.GetStream();
+            using (var writer = new BinaryWriter(stream, System.Text.Encoding.UTF8, leaveOpen: true))
             {
                 writer.Write(data.Length);
                 ...
```

```diff
-        lock (_connectionLock)
-        {
-            var client = _tcpConnections[sourceRank];
-            using (var stream = client.GetStream())
-            using (var reader = new BinaryReader(stream))
+        lock (_connectionLock)
+        {
+            var client = _tcpConnections[sourceRank];
+            var stream = client.GetStream();
+            using (var reader = new BinaryReader(stream, System.Text.Encoding.UTF8, leaveOpen: true))
             {
                 int length = reader.ReadInt32();
                 ...
```

<!-- suggestion_start -->

<details>
<summary>üìù Committable suggestion</summary>

> ‚ÄºÔ∏è **IMPORTANT**
> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.

```suggestion
    /// <summary>
    /// Sends data to a specific rank via TCP.
    /// </summary>
    private void SendData(int destRank, T[] data)
    {
        if (_tcpConnections == null || !_tcpConnections.ContainsKey(destRank))
        {
            throw new InvalidOperationException($"No TCP connection to rank {destRank}");
        }

        lock (_connectionLock)
        {
            var client = _tcpConnections[destRank];
            var stream = client.GetStream();
            using (var writer = new BinaryWriter(stream, System.Text.Encoding.UTF8, leaveOpen: true))
            {
                // Send length header
                writer.Write(data.Length);

                // Send data elements
                foreach (var element in data)
                {
                    double value = Convert.ToDouble(element);
                    writer.Write(value);
                }
                writer.Flush();
            }
        }
    }

    /// <summary>
    /// Receives data from a specific rank via TCP.
    /// </summary>
    private T[] ReceiveData(int sourceRank, int expectedLength)
    {
        if (_tcpConnections == null || !_tcpConnections.ContainsKey(sourceRank))
        {
            throw new InvalidOperationException($"No TCP connection to rank {sourceRank}");
        }

        lock (_connectionLock)
        {
            var client = _tcpConnections[sourceRank];
            var stream = client.GetStream();
            using (var reader = new BinaryReader(stream, System.Text.Encoding.UTF8, leaveOpen: true))
            {
                // Read length header
                int length = reader.ReadInt32();
                if (length != expectedLength)
                {
                    throw new InvalidOperationException(
                        $"Rank {_rank}: Expected {expectedLength} elements from rank {sourceRank}, but received {length}");
                }

                // Read data elements
                var result = new T[length];
                for (int i = 0; i < length; i++)
                {
                    double value = reader.ReadDouble();
                    result[i] = (T)Convert.ChangeType(value, typeof(T));
                }
                return result;
            }
        }
    }
```

</details>

<!-- suggestion_end -->

<!-- fingerprinting:phantom:medusa:sabertoothed -->

<!-- This is an auto-generated comment by CodeRabbit -->
============================================================

===== COMMENT 16 =====
ThreadID: PRRT_kwDOKSXUF85hNV-S
File: src/DistributedTraining/GradientCompressionOptimizer.cs
Line: 151
Body:
_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_

**Avoid double-applying gradients after compression**

We undo the local step via `ComputeOriginalParameters`, but never restore those parameters before calling `ApplyGradients`. As written we apply the local update *and* the averaged compressed gradients, so every rank overshoots.

Reset the model to the reconstructed pre-update parameters before applying the synchronized gradients.

```diff
-            var originalParams = ComputeOriginalParameters(updatedParams, localGradients);
+            var originalParams = ComputeOriginalParameters(updatedParams, localGradients);
             var compressedGradients = CompressGradients(localGradients);
             Config.CommunicationBackend.AllReduce(compressedGradients, ReductionOperation.Average);
             var averagedGradients = DecompressGradients(compressedGradients, localGradients.Length);
-            var finalModel = gradientOptimizer.ApplyGradients(averagedGradients, localResult.BestSolution);
+            localResult.BestSolution.SetParameters(originalParams);
+            var finalModel = gradientOptimizer.ApplyGradients(averagedGradients, localResult.BestSolution);
```

<!-- suggestion_start -->

<details>
<summary>üìù Committable suggestion</summary>

> ‚ÄºÔ∏è **IMPORTANT**
> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.

```suggestion
            // Step 3: Get parameters BEFORE gradient application (reverse the local update)
            var updatedParams = localResult.BestSolution.GetParameters();
            var originalParams = ComputeOriginalParameters(updatedParams, localGradients);

            // Step 4: Compress local gradients
            var compressedGradients = CompressGradients(localGradients);

            // Step 5: Synchronize compressed gradients across all ranks and average them
            Config.CommunicationBackend.AllReduce(compressedGradients, ReductionOperation.Average);

            // Step 6: Decompress to get averaged gradients
            var averagedGradients = DecompressGradients(compressedGradients, localGradients.Length);

            // Step 7: Apply averaged compressed gradients to original parameters
            // This ensures all ranks converge using compressed gradients
            localResult.BestSolution.SetParameters(originalParams);
            var finalModel = gradientOptimizer.ApplyGradients(averagedGradients, localResult.BestSolution);

            // Step 8: Return result with model updated using averaged compressed gradients
            localResult.BestSolution = finalModel;
```

</details>

<!-- suggestion_end -->

<details>
<summary>ü§ñ Prompt for AI Agents</summary>

```
In src/DistributedTraining/GradientCompressionOptimizer.cs around lines 133 to
151, the code computes originalParams via ComputeOriginalParameters but never
restores them, so ApplyGradients is applied on a model that already has the
local update (causing double application); restore the model to the
reconstructed pre-update parameters before applying the averaged compressed
gradients‚Äîe.g., set or create a model instance using originalParams and pass
that to gradientOptimizer.ApplyGradients(averagedGradients, reconstructedModel),
then assign the returned finalModel back to localResult.BestSolution.
```

</details>

<!-- fingerprinting:phantom:medusa:sabertoothed -->

<!-- This is an auto-generated comment by CodeRabbit -->
============================================================

===== COMMENT 17 =====
ThreadID: PRRT_kwDOKSXUF85hNV-X
File: src/DistributedTraining/PipelineParallelOptimizer.cs
Line: 80
Body:
_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_

**Guarantee the closing barrier runs even on exceptions**

Any exception between the two barriers causes other stages to hang at the final barrier. Mirror the ZeRO2 fix: wrap the optimization block in `try/finally` and move the trailing barrier into the `finally` so all ranks either reach it or unwind safely.

<details>
<summary>ü§ñ Prompt for AI Agents</summary>

```
In src/DistributedTraining/PipelineParallelOptimizer.cs around lines 58 to 80,
the trailing barrier can be skipped if an exception occurs between the two
barriers; wrap the optimization and any per-stage logic in a try block and move
the final Config.CommunicationBackend.Barrier() into a finally block so the
barrier always executes; ensure any caught exception is not swallowed (rethrow
after finally or do not catch at all) so other ranks don't hang while exceptions
propagate.
```

</details>

<!-- fingerprinting:phantom:medusa:sabertoothed -->

<!-- This is an auto-generated comment by CodeRabbit -->
============================================================

===== COMMENT 18 =====
ThreadID: PRRT_kwDOKSXUF85hNV-a
File: src/DistributedTraining/ZeRO2Optimizer.cs
Line: None
Body:
_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_

**Wrap collectives in try/finally to avoid deadlock**

If `WrappedOptimizer.Optimize` (or any logic before the trailing barrier) throws on one rank, that rank skips the second barrier while its peers block forever. Please guard the collectives with `try/finally` so the trailing barrier (or a coordinated abort) always executes.

```diff
-        Config.CommunicationBackend.Barrier();
-
-        // Optimize on local data
-        var result = WrappedOptimizer.Optimize(inputData);
-        ...
-
-        Config.CommunicationBackend.Barrier();
-
-        return result;
+        Config.CommunicationBackend.Barrier();
+
+        try
+        {
+            // Optimize on local data
+            var result = WrappedOptimizer.Optimize(inputData);
+            ...
+            return result;
+        }
+        finally
+        {
+            Config.CommunicationBackend.Barrier();
+        }
```

<!-- suggestion_start -->

<details>
<summary>üìù Committable suggestion</summary>

> ‚ÄºÔ∏è **IMPORTANT**
> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.

```suggestion
        Config.CommunicationBackend.Barrier();

        try
        {
            // Optimize on local data
            var result = WrappedOptimizer.Optimize(inputData);

            // TODO: Implement ZeRO-2 gradient sharding
            // In ZeRO-2, we need to:
            // 1. Intercept gradients during backpropagation (before optimizer.Optimize() completes)
            // 2. Perform ReduceScatter to reduce gradients across processes and distribute shards
            //    var reducedGradientShard = Config.CommunicationBackend.ReduceScatter(gradients, ReductionOperation.Average);
            // 3. Map reducedGradientShard back to local parameter shard indices
            // 4. Apply gradient shard to update only local parameter shard and optimizer state
            // 5. Ensure optimizer momentum/exponential average states are updated for the shard
            // 6. Verify shard indices, sizes, and shapes match expected partitioning
            //
            // Current limitation: This framework's IOptimizer.Optimize() abstraction is a black box
            // that doesn't expose intermediate gradients. Proper ZeRO-2 implementation requires
            // either extending IOptimizer to expose gradients or integrating gradient hooks.
            //
            // For now, we synchronize parameters like ZeRO-1 (optimizer state is still sharded)
            if (Config.AutoSyncGradients && result.BestSolution != null)
            {
                SynchronizeParameters(result.BestSolution);
            }

            SynchronizeOptimizerState();

            return result;
        }
        finally
        {
            Config.CommunicationBackend.Barrier();
        }
```

</details>

<!-- suggestion_end -->

<!-- fingerprinting:phantom:medusa:sabertoothed -->

<!-- This is an auto-generated comment by CodeRabbit -->
============================================================

===== COMMENT 19 =====
ThreadID: PRRT_kwDOKSXUF85hNV-c
File: src/Optimizers/GradientBasedOptimizerBase.cs
Line: 126
Body:
_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_

**Do not double-step when applying distributed gradients**

`ApplyGradients` calls `UpdateSolution` on the model as-is. In the new DDP flow we call this right after `WrappedOptimizer.Optimize`, so `model` already contains the locally-updated parameters `P‚ÇÄ - Œ∑¬∑g_local`. Subtracting `gradients` again yields `P‚ÇÄ - Œ∑¬∑g_local - Œ∑¬∑g_avg`, i.e. we take two optimizer steps instead of replacing the local step with the averaged one. Please either change this method to operate on the pre-step parameter vector (e.g. accept it explicitly and feed that into `UpdateSolution`) or have the caller restore `ComputeOriginalParameters(...)` into the model before invoking it, otherwise distributed runs diverge from single-process results.

<!-- fingerprinting:phantom:medusa:sabertoothed -->

<!-- This is an auto-generated comment by CodeRabbit -->
============================================================

===== COMMENT 20 =====
ThreadID: PRRT_kwDOKSXUF85hNZT6
File: src/DistributedTraining/ElasticOptimizer.cs
Line: 136
Body:
_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_

**Critical barrier synchronization mismatch causes deadlock.**

The attempt to prevent deadlock introduced a different synchronization problem:

- **If `HandleWorkerChange()` throws**: Barrier called at line 129, then exception re-thrown (line 136 never reached)
- **If `HandleWorkerChange()` succeeds**: Barrier called at line 136

These are **two different barrier synchronization points**. In distributed training, all processes must reach the **same** barrier call. If one process calls the barrier in the catch block while others call the barrier at line 136, they're waiting at different synchronization points, causing deadlock.



**Solution options:**

1. **Unconditional barrier after try-catch** (recommended for this placeholder implementation):
```diff
     // Check for world size changes (workers joined/left)
+    InvalidOperationException? deferredError = null;
     if (DetectWorldSizeChange())
     {
         try
         {
             HandleWorkerChange();
         }
         catch (InvalidOperationException ex)
         {
-            // CRITICAL: Reach barrier even on validation failure to unblock other processes
-            // Otherwise, other workers waiting at the barrier will deadlock indefinitely
-            Config.CommunicationBackend.Barrier();
-            throw new InvalidOperationException(
-                $"Worker change validation failed: {ex.Message}. All workers have been synchronized.", ex);
+            // Defer throwing until after barrier to ensure all processes synchronize
+            deferredError = new InvalidOperationException(
+                $"Worker change validation failed: {ex.Message}", ex);
         }
     }

     // Barrier with current worker set - ALL processes must reach this point
     Config.CommunicationBackend.Barrier();
+    
+    // Throw deferred error after synchronization
+    if (deferredError != null)
+    {
+        throw deferredError;
+    }
```

2. **Collective validation** (better for production, but requires more infrastructure):
```csharp
// All workers must agree on whether validation succeeded
// Use AllReduce to determine if ANY worker failed validation
```

Note: In production elastic training, validation failures should be coordinated cluster-wide through the membership service, ensuring all workers agree on the new configuration before proceeding.

<details>
<summary>ü§ñ Prompt for AI Agents</summary>

```
In src/DistributedTraining/ElasticOptimizer.cs around lines 118 to 136, the
current code calls Barrier inside the catch and again after the try-catch,
creating mismatched synchronization points; change the flow to perform a single,
unconditional Barrier after the try-catch: remove the Barrier call from the
catch, capture any caught InvalidOperationException into a local variable, let
execution continue to the single Barrier call after the try-catch, and then
after that Barrier rethrow (or throw a wrapped) exception if one was captured so
all processes hit the same synchronization point before any process propagates
the error.
```

</details>

<!-- This is an auto-generated comment by CodeRabbit -->
============================================================

===== COMMENT 21 =====
ThreadID: PRRT_kwDOKSXUF85hNZT9
File: src/DistributedTraining/ElasticOptimizer.cs
Line: 288
Body:
_‚ö†Ô∏è Potential issue_ | _üü† Major_

**Overly strict validation prevents elastic restore.**

The deserialization validates that `savedWorldSize == WorldSize` (lines 260-265), which prevents restoring a checkpoint with a different number of workers. This contradicts the elastic training goal of supporting dynamic worker changes.



For elastic training, checkpoint restore should support different world sizes:

```diff
     // Read sharding configuration (for validation)
     int savedWorldSize = reader.ReadInt32();
     int savedRank = reader.ReadInt32();
     reader.ReadBoolean(); // AutoSyncGradients
     reader.ReadInt32(); // MinimumParameterGroupSize
     reader.ReadBoolean(); // EnableGradientCompression

-    if (savedWorldSize != WorldSize)
-    {
-        throw new InvalidOperationException(
-            $"World size mismatch. Optimizer was saved with {savedWorldSize} processes, " +
-            $"but current configuration has {WorldSize} processes.");
-    }
-
-    if (savedRank != Rank)
+    // Elastic training allows world size to change between save and restore
+    if (savedWorldSize != WorldSize || savedRank != Rank)
     {
-        throw new InvalidOperationException(
-            $"Rank mismatch. Optimizer was saved on rank {savedRank}, " +
-            $"but is being loaded on rank {Rank}. This could indicate a configuration error.");
+        // Log warning but allow restore
+        // Production would trigger re-sharding after restore
+        // For now, accept the mismatch and let DetectWorldSizeChange() handle it
     }
```

Additionally, consider restoring `_currentWorldSize = WorldSize` (current, not saved) to immediately reflect the current cluster state:

```diff
-    _currentWorldSize = savedCurrentWorldSize;
+    // Initialize to current world size so DetectWorldSizeChange() works correctly
+    // If world size changed since checkpoint, next Optimize() will trigger re-sharding
+    _currentWorldSize = WorldSize;
```

This aligns with the elastic pattern where checkpoints can be restored on differently-sized clusters.


> Committable suggestion skipped: line range outside the PR's diff.

<!-- This is an auto-generated comment by CodeRabbit -->
============================================================

===== COMMENT 22 =====
ThreadID: PRRT_kwDOKSXUF85hNZUB
File: src/DistributedTraining/FSDPModel.cs
Line: 155
Body:
_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_

**Critical: Gradient synchronization doesn't average across ranks.**

The current synchronization flow has a fundamental flaw when ranks train on different data:

1. After line 94, each rank has different full parameters (P‚ÇÄ, P‚ÇÅ, ...) from training on different mini-batches
2. Line 100 immediately extracts local shards: LocalShard‚ÇÄ = P‚ÇÄ[shard‚ÇÄ_range], LocalShard‚ÇÅ = P‚ÇÅ[shard‚ÇÅ_range]
3. These shards come from *different* parameter vectors
4. `SynchronizeGradients()` line 143 calls `AllGather(LocalShard)`, producing a "frankenstein" vector: [P‚ÇÄ[shard‚ÇÄ_range], P‚ÇÅ[shard‚ÇÅ_range]]
5. Line 147's `AllReduce` operates on identical vectors (AllGather broadcasts the same result to all ranks), making it a no-op
6. Result: Each rank retains its own shard from its own training‚Äîno averaging occurs

For correct data-parallel training, you must AllReduce the *full* parameter vectors *before* extracting shards:



Apply this restructuring to the Train method:

```diff
     // Train the wrapped model
     WrappedModel.Train(input, expectedOutput);
 
     // Get updated parameters
     var updatedParams = WrappedModel.GetParameters();
 
+    // Synchronize full parameters across ranks if auto-sync is enabled
+    if (Config.AutoSyncGradients)
+    {
+        Config.CommunicationBackend.AllReduce(updatedParams, ReductionOperation.Average);
+        WrappedModel.SetParameters(updatedParams);
+    }
+
     // Update local shard
     UpdateLocalShardFromFull(updatedParams);
-
-    // Invalidate cache immediately after local shard changes
-    InvalidateCache();
-
-    // Synchronize gradients if auto-sync is enabled
-    if (Config.AutoSyncGradients)
-    {
-        SynchronizeGradients();
-
-        // Apply synchronized parameters back to the model
-        fullParams = GatherFullParameters();
-        WrappedModel.SetParameters(fullParams);
-    }
 }
```

This ensures:
- Full parameter vectors are averaged *before* sharding
- All ranks extract consistent local shards from the same averaged parameters
- Proper data-parallel semantics: average(P - lr¬∑G‚ÇÄ, P - lr¬∑G‚ÇÅ, ‚Ä¶) = P - lr¬∑average(G‚ÇÄ, G‚ÇÅ, ‚Ä¶)

<details>
<summary>ü§ñ Prompt for AI Agents</summary>

```
In src/DistributedTraining/FSDPModel.cs around lines 86 to 155, the Train flow
averages shards incorrectly because it extracts and updates local shards before
performing AllReduce on the full parameter vectors; fix by gathering the full
parameters immediately after WrappedModel.Train, and if Config.AutoSyncGradients
is true perform the AllReduce (average) on that full parameter vector before
scattering: replace the current sequence so that after training you call
GatherFullParameters -> if AutoSyncGradients then
CommunicationBackend.AllReduce(fullParams, Average) ->
UpdateLocalShardFromFull(fullParams) -> InvalidateCache() ->
WrappedModel.SetParameters(fullParams) (and if not auto-sync keep the existing
local update behavior), ensuring CachedFullParameters is cleared when
appropriate.
```

</details>

<!-- This is an auto-generated comment by CodeRabbit -->
============================================================

===== COMMENT 23 =====
ThreadID: PRRT_kwDOKSXUF85hNZUE
File: src/DistributedTraining/ZeRO2Model.cs
Line: 57
Body:
_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_

**Expose gradient shard via property or accessor for optimizer integration.**

The `_gradientShard` field is private, but ZeRO-2 requires the optimizer to access sharded gradients to perform local updates. The XML documentation (line 48) references `ZeRO2Optimizer`, which must read this shard to implement the ZeRO-2 contract. Without a public property or method to retrieve the gradient shard, the optimizer cannot function correctly, rendering this implementation incomplete.



Add a public property to expose the gradient shard:

```diff
+    /// <summary>
+    /// Gets the local gradient shard for this rank after synchronization.
+    /// </summary>
+    public Vector<T>? GradientShard => _gradientShard;
+
     private Vector<T>? _gradientShard;
```

<!-- suggestion_start -->

<details>
<summary>üìù Committable suggestion</summary>

> ‚ÄºÔ∏è **IMPORTANT**
> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.

```suggestion
    /// <summary>
    /// Gets the local gradient shard for this rank after synchronization.
    /// </summary>
    public Vector<T>? GradientShard => _gradientShard;

    private Vector<T>? _gradientShard;
```

</details>

<!-- suggestion_end -->

<details>
<summary>ü§ñ Prompt for AI Agents</summary>

```
In src/DistributedTraining/ZeRO2Model.cs at line 57, the private field
`_gradientShard` is not accessible to the optimizer; add a public read-only
property (e.g., GradientShard) that returns the `Vector<T>?` so ZeRO2Optimizer
can read the local shard; implement it as a simple getter exposing
`_gradientShard` (optionally internal or protected if you need limited
visibility) and keep the field encapsulated while providing the required
accessor for optimizer integration.
```

</details>

<!-- This is an auto-generated comment by CodeRabbit -->
============================================================

===== COMMENT 24 =====
ThreadID: PRRT_kwDOKSXUF85hNZUK
File: src/DistributedTraining/ZeRO2Model.cs
Line: 113
Body:
_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_

**Fix ReduceScatter remainder distribution so sharded gradients are correct.**

The padding approach prevents the `ReduceScatter` crash but **incorrectly distributes remainder elements**. `ReduceScatter` divides the input into equal-sized chunks (`chunkSize = paddedLength / WorldSize`), so trimming afterward doesn't produce the intended uneven distribution.

**Example (10 params, 3 ranks):**
- **Expected:** Rank 0: indices [0:4] (4 elements), Rank 1: [4:7] (3), Rank 2: [7:10] (3)  
- **Actual after padding to 12:** ReduceScatter produces chunks [0:4], [4:8], [8:12]; trimming gives Rank 0: [0:4] ‚úì, Rank 1: [4:7] ‚úì, Rank 2: [8:10, PAD] ‚úó

Rank 2 (and others) stores **padding elements** in `_gradientShard` instead of the correct gradients, corrupting synchronization.



**Solution:** Manually implement uneven ReduceScatter using AllReduce + local slicing, or rework to use a scatter/gather approach that respects the remainder distribution:

```diff
     public override void SynchronizeGradients()
     {
-        var totalParams = LocalShard.Length;
-        var remainder = totalParams % WorldSize;
-
-        // Pad to satisfy ReduceScatter's divisibility requirement
-        Vector<T> reduceInput = LocalShard;
-        if (remainder != 0)
-        {
-            var paddedLength = totalParams + (WorldSize - remainder);
-            var padded = new T[paddedLength];
-            Array.Copy(LocalShard.ToArray(), padded, totalParams);
-            reduceInput = new Vector<T>(padded);
-        }
-
-        var reducedChunk = Config.CommunicationBackend.ReduceScatter(reduceInput, ReductionOperation.Average);
-
-        // Trim padding so each rank keeps only its logical shard
-        var shardLength = totalParams / WorldSize + (Rank < remainder ? 1 : 0);
-        var shardData = new T[shardLength];
-        Array.Copy(reducedChunk.ToArray(), 0, shardData, 0, shardLength);
-        _gradientShard = new Vector<T>(shardData);
+        // Use AllReduce to sum all gradients, then extract this rank's shard
+        var reducedGradients = Config.CommunicationBackend.AllReduce(LocalShard, ReductionOperation.Average);
+        
+        var totalParams = LocalShard.Length;
+        var baseShardSize = totalParams / WorldSize;
+        var remainder = totalParams % WorldSize;
+        
+        // Calculate this rank's shard boundaries
+        int shardStart = Rank * baseShardSize + Math.Min(Rank, remainder);
+        int shardLength = baseShardSize + (Rank < remainder ? 1 : 0);
+        
+        // Extract this rank's portion of the reduced gradients
+        var shardData = new T[shardLength];
+        Array.Copy(reducedGradients.ToArray(), shardStart, shardData, 0, shardLength);
+        _gradientShard = new Vector<T>(shardData);
 
         CachedFullParameters = null;
     }
```

**Note:** This uses `AllReduce` instead of `ReduceScatter` to ensure correct remainder handling. If `ReduceScatter` performance is critical, implement a custom variant that accepts per-rank shard sizes.

<!-- This is an auto-generated comment by CodeRabbit -->
============================================================

===== COMMENT 25 =====
ThreadID: PRRT_kwDOKSXUF85hNZUP
File: src/DistributedTraining/ZeRO2Optimizer.cs
Line: 103
Body:
_‚ö†Ô∏è Potential issue_ | _üü† Major_

**Class claims to implement ZeRO-2 but uses DDP-style gradient synchronization.**

The class name, XML docs (line 9: "shards optimizer states and gradients"), and remarks (lines 13-16) advertise ZeRO-2 gradient sharding via `ReduceScatter`. However, the implementation at lines 90-100 explicitly documents that true ZeRO-2 is not implemented and instead falls back to `AllReduce` (line 100), which averages the full gradient vector across all ranks‚Äîidentical to DDP behavior. This means gradients are replicated, not sharded, and users won't see the memory savings advertised in the documentation.



Either:
1. **Complete the ZeRO-2 implementation** as outlined in the TODO (lines 90-97): use `ReduceScatter` to distribute gradient shards, apply shards to corresponding parameter shards, and `AllGather` updated parameters, or
2. **Rename and document the limitation prominently**: rename to `ZeRO2OptimizerPartial` or add a prominent warning in the XML docs (e.g., `<para><b>Current Limitation:</b> Gradient sharding is not yet implemented; gradients are synchronized via AllReduce (DDP-style). Memory savings are limited to optimizer state sharding only.</para>`) to set accurate expectations.

<details>
<summary>ü§ñ Prompt for AI Agents</summary>

```
In src/DistributedTraining/ZeRO2Optimizer.cs around lines 78 to 103, the
implementation documents ZeRO-2 but falls back to DDP-style AllReduce (line
~100) which does not shard gradients; either implement the TODO
ReduceScatter/ApplyShard/AllGather flow or make the limitation explicit. To fix:
either (A) implement true ZeRO-2 by using
Config.CommunicationBackend.ReduceScatter(localGradients,
ReductionOperation.Average) to obtain this rank‚Äôs gradient shard, split
originalParams into matching parameter shards, apply the shard via a new
ApplyGradientShard method that updates only this shard, then use
CommunicationBackend.AllGather to reconstruct full parameters and set
localResult.BestSolution appropriately; or (B) rename the class/exports to
indicate partial ZeRO-2 (e.g., ZeRO2OptimizerPartial) and update the XML docs
and remarks to prominently state that gradients are synchronized via AllReduce
(DDP-style) and true gradient sharding is not implemented so memory savings are
limited to optimizer state sharding.
```

</details>

<!-- This is an auto-generated comment by CodeRabbit -->
============================================================

===== COMMENT 26 =====
ThreadID: PRRT_kwDOKSXUF85hNZUV
File: src/DistributedTraining/ZeRO2Optimizer.cs
Line: 141
Body:
_‚ö†Ô∏è Potential issue_ | _üî¥ Critical_

**Gradient reversal assumes SGD but constructor allows Adam, RMSprop, etc.**

`ComputeOriginalParameters` reverses the gradient update using the formula `params_old = params_new + lr * gradients` (lines 131-138), which only holds for vanilla SGD. However, the constructor (line 54) accepts any `IGradientBasedOptimizer`, including Adam (which uses momentum and adaptive learning rates: `params_new = params_old - lr * m_t / (sqrt(v_t) + epsilon)`) and RMSprop (which uses moving averages of squared gradients). For these optimizers, the reversal will compute incorrect original parameters, leading to incorrect gradient averaging and broken distributed training.



Either:
1. **Restrict to SGD only**: Update the constructor validation (line 54) to check for a specific SGD optimizer type and update the exception message, or
2. **Implement correct reversal for all optimizers**: Access the wrapped optimizer's internal state (momentum, variance, etc.) to correctly reverse the update for each optimizer type. This likely requires extending `IGradientBasedOptimizer` with a `ReverseUpdate` method or similar.

Option 1 is simpler and aligns with the current implementation; option 2 is more flexible but significantly more complex.

<details>
<summary>ü§ñ Prompt for AI Agents</summary>

```
In src/DistributedTraining/ZeRO2Optimizer.cs around lines 124 to 141 (and note
constructor check at line 54), the ComputeOriginalParameters method assumes
vanilla SGD when reversing updates; that is incorrect for Adam/RMSProp etc. Fix
by either (preferred simple fix) restricting the class to SGD: change the
constructor validation at line 54 to only accept the SGD optimizer type and
update the exception message to state ZeRO2Optimizer only supports SGD; or (if
you need broader support) extend IGradientBasedOptimizer with a ReverseUpdate
method (or similar) that exposes the optimizer-specific reverse logic, implement
that in each optimizer (Adam, RMSProp, SGD), and replace the current SGD-only
reversal in ComputeOriginalParameters with a call to
WrappedOptimizer.ReverseUpdate(updatedParams, gradients, options/state). Ensure
tests updated to reflect the new constraint or added reverse implementations.
```

</details>

<!-- This is an auto-generated comment by CodeRabbit -->
============================================================
